[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian Mills",
    "section": "",
    "text": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows\n        \n        \n        Learn how to install the Mamba package manager on Windows, set up a local Python environment, and install PyTorch and Jupyter for machine learning projects.\n        \n        \n            Getting Started with Google Colab\n        \n        \n        Learn the fundamentals of Google Colab, a free cloud-based Jupyter Notebook environment, to write, run, and share Python code in your browser without any setup or installation.\n        \n        \n            Code Walkthrough: Unity Barracuda Inference PoseNet Package\n        \n        \n        Walk through the code for the Unity Barracuda Inference PoseNet package, which extends the functionality of `unity-barracuda-inference-base` to perform 2D human pose estimation using PoseNet models.\n        \n        \n            Code Walkthrough: Unity Human Pose 2D Toolkit Package\n        \n        \n        Walk through the code for the Unity Human Pose 2D Toolkit package, which provides an easy-to-use and customizable solution to work with and visualize 2D human poses on a Unity canvas.\n        \n        \n            Code Walkthrough: Unity Barracuda Inference YOLOX Package\n        \n        \n        Walk through the code for the Unity Barracuda Inference YOLOX package, which extends the functionality of `unity-barracuda-inference-base` to perform object detection using YOLOX models.\n        \n\n\nNo matching items\n\n\n  \n\n\n\n\n\n        \n            A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO\n        \n        \n        In this three-part tutorial series, we will explore how to use IceVision and OpenVINO to perform end-to-end object detection in Unity.\n        \n        \n            Real-Time Object Detection in Unity With ONNX and DirectML\n        \n        \n        In this two-part tutorial series, I will show you how to implement real-time object detection in Unity using ONNX Runtime and DirectML.\n        \n        \n            TensorFlow.js in Unity\n        \n        \n        In this tutorial series, we explore how to create TensorFlow.js plugins for the Unity game engine.\n        \n        \n            Training a Mask R-CNN Model on a Custom Dataset With IceVision\n        \n        \n        Train a Mask R-CNN model on a custom dataset using the IceVision library and perform inference with ONNX Runtime.\n        \n        \n            Barracuda PoseNet Tutorial 2nd Edition\n        \n        \n        This tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library.\n        \n\n\nNo matching items\n\n\n\n\n\n        \n            Natural Language Processing with Transformers\n        \n        \n        My notes from the book Natural Language Processing with Transformers: Building Language Applications with Hugging Face by Lewis Tunstall, Leandro von Werra, and Thomas Wolf.\n        \n        \n            Deep Learning for Coders with fastai & PyTorch\n        \n        \n        My notes from the book Deep Learning for Coders with fastai & PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger.\n        \n        \n            Professional Growth\n        \n        \n        My notes from resources on professional growth.\n        \n        \n            Game Development\n        \n        \n        My notes from resources on game development.\n        \n        \n            Procedural Generation\n        \n        \n        My notes from resources on procedural generation.\n        \n\n\nNo matching items\n\n\n\n\nReady to elevate your project with deep learning expertise? Schedule a 30-minute consultation to:\n\nDiscuss your project’s vision, objectives, and challenges.\nExplore high-level deep-learning opportunities and roadmaps.\nDetermine if my services align with your needs.\n\nPlease note that this session provides an overview, not in-depth technical advice or solutions. If we decide to work together, we can create a tailored plan that addresses your project’s unique challenges and goals."
  },
  {
    "objectID": "index.html#book-a-free-consultation",
    "href": "index.html#book-a-free-consultation",
    "title": "Christian Mills",
    "section": "",
    "text": "Ready to elevate your project with deep learning expertise? Schedule a 30-minute consultation to:\n\nDiscuss your project’s vision, objectives, and challenges.\nExplore high-level deep-learning opportunities and roadmaps.\nDetermine if my services align with your needs.\n\nPlease note that this session provides an overview, not in-depth technical advice or solutions. If we decide to work together, we can create a tailored plan that addresses your project’s unique challenges and goals."
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "",
    "text": "Overview\nMotivations\nImplicit and Parametric Equations\nParametric Manipulations\nParametric Opportunities\nThe Big Idea\nThe Two Most Important Number Ranges\nNormalized Non-Linear Functions\nRange Mapping\nRelated Material"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#overview",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#overview",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Squirrel Eiserloh’s presentation covering how 1D nonlinear transformations can be used by game programmers."
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#motivations",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#motivations",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Motivations",
    "text": "Motivations\n\nJuice it or lose it talk: Makes the case for thinking about is something linear or non-linear, is it mechanical or organic\nThe art of screenshake:"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#implicit-versus-parametric-equations",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#implicit-versus-parametric-equations",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Implicit versus Parametric Equations",
    "text": "Implicit versus Parametric Equations\n\nImplicit equations are rules:\n\nEquation for a circle: \\(x^{2} + Y^{2} = 25\\)\n\nA point is either on the circle or not\n\n\nParametric functions\n\nYield an output for an input value\n\n\\(P_{x} = 5 \\cdot cos(2 \\pi \\cdot t)\\)\n\\(P_{y} = 5 \\cdot sin(2 \\pi \\cdot t)\\)\n\n\\(P(t) = ?\\)\n\\(P(t) = (t, t \\cdot cos(t), t*sin(t))\\)\n\n\\((x, y, z)\\)\nGenerates a spiral that increases in radius along the x axis\n\nAnything you can express in terms of a single float as input\nA common float input is “time”"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#parametric-manipulations",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#parametric-manipulations",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Parametric Manipulations",
    "text": "Parametric Manipulations\n\nDo NOT mess with the interpolation itself (e.g. color, position, AI disposition, etc.)\nInstead just mess the parameter"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#parametric-opportunities",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#parametric-opportunities",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Parametric Opportunities",
    "text": "Parametric Opportunities\n\nAnytime you have a single float to change\nAnytime you can express something in terms of a single float\nPretty much whenever you use time"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#the-big-idea",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#the-big-idea",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "The Big Idea",
    "text": "The Big Idea\n\nYou can make any parametric equation more interesting without modifying the function itself, without knowing anything about the function"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#the-two-most-important-number-ranges",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#the-two-most-important-number-ranges",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "The Two Most Important Number Ranges",
    "text": "The Two Most Important Number Ranges\n\n\\([0,1]\\)\n\nUseful for fractions\n\n% shadow\n% luminance\n% falloff\n% complete\n% damage\n% experience\n% cost\n% penalty\n% fog\n% AI aggression\n% chance to hit\n% chance to drop loot\n% time to complete\nFuzzy Logic\nMost anything parametric\n\n\n\\([-1,1]\\)\n\nUseful for deviations\n\nnoise\nperturbation\nterrain and map generation\nvariation\ndistribution\nsinusoidal\nAI response curves"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#normalized-non-linear-functions",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#normalized-non-linear-functions",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Normalized Non-Linear Functions",
    "text": "Normalized Non-Linear Functions\n\n\\([0,1]\\)\nFunctions for which:\n\n\\(P(0) = 0\\)\n\\(P(1) = 1\\)\n\\(P(t) \\ != t\\)\n\nExamples\n\nPosition over time\nScale over time\nAlpha over time\nColor over time\nStrength over time\nAggression over time\n\nAlso called\n\neasing functions\nfilter functions\nlerping functions\ntweening functions"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#range-mapping",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#range-mapping",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Range Mapping",
    "text": "Range Mapping\n\ncan be applied during middle of range-mapping\n\nout RangeMap(in, inStart, inEnd, outStart, outEnd)\n{\n    // Puts in [0, inEnd - inStart]\n    out = in - inStart;\n    // Puts in [0,1]\n    out /= (inEnd - inStart);\n    // in [0,1]\n    out = ApplySomeEasingFunction(out);\n    // Puts in [0, outRange]\n    out *= (outEnd - outStart);\n    // Puts in [outStart, outEnd]\n    return out + outStart\n}\n\nSmoothStart\n\n\\(SmoothStartN(t) = t^{n}\\)\nLarger exponents result in steeper curve\nWill always start and end at the same time, regardless of exponent value\nTechnique\n\nexponentiating\n\n\n\n\nSmoothStop\n\n\\(SmoothStopN(t) = 1 - (1 - t)^{n}\\)\nLarger exponents results in longer braking period at the end\nTechniques\n\nexponentiating\nflipping\n\n\n\n\n\\(Mix(a, b, weightB, t)= a + weightB(b-a)\\)\n\n\\(Mix(SmoothStart2, SmoothStop2, blend, t)\\)\n\\(SmoothStart2.2 = Mix(SmoothStart2, SmoothStart3, 0.2);\\)\n\nWay faster than using the pow() function\n\n\n\n\nCrossfade\n\nLike Mix, but use t itself as the mix weight\nAlso called SmoothStep\n\n\n\nScale\n\n\\(Scale(Function, t) = t \\cdot Function(t)\\)\n\n\n\nReverseScale\n\n\\(ReverseScale(Function, t) = (1-t) \\cdot Function(t)\\)\n\n\\(Arch2(t) = Scale(Flip(t)) = t \\cdot (1-t)\\)\n\\(SmoothStartArch3(t) = Scale(Arch2, t) = t^{2}(1-t)\\)\n\\(SmoothStopArch3(t) = ReverseScale(Arch2, t) = t(1-t)^{2}\\)\n\\(SmoothStepArch3(t) = ReverseScale(Scale(Arch2, t), t)\\)\n\\(BellCurve6(t) = SmoothStop3(t) \\cdot SmoothStart3(t)\\)"
  },
  {
    "objectID": "posts/1d-nonlinear-transformations-for-games-notes/index.html#related-material",
    "href": "posts/1d-nonlinear-transformations-for-games-notes/index.html#related-material",
    "title": "Notes on 1D Nonlinear Transformations for Games",
    "section": "Related Material",
    "text": "Related Material\nJuice it or lose it - a talk by Martin Jonasson & Petri Purho\nThe Art of Screenshake - Jan Willem Nijman - Vlambeer\nReferences:\n\nMath for Game Programmers: Fast and Funky 1D Nonlinear Transformations"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html",
    "href": "posts/advanced-git-tools-notes/index.html",
    "title": "Notes on Advanced Git Tools",
    "section": "",
    "text": "Overview\nInteractive Rebase\nCherry-Picking\nReflog\nSubmodules\nSearch and Find\nAdditional Resources"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#overview",
    "href": "posts/advanced-git-tools-notes/index.html#overview",
    "title": "Notes on Advanced Git Tools",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Tobias Gunther’s video covering advanced git tools such as interactive rebase, cherry-picking, reflog, submodules, and search and find."
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#interactive-rebase",
    "href": "posts/advanced-git-tools-notes/index.html#interactive-rebase",
    "title": "Notes on Advanced Git Tools",
    "section": "Interactive Rebase",
    "text": "Interactive Rebase\n\nThe swiss army knife of git commands\nA tool for optimizing and cleaning up your commit history\n\nChange a commit’s message\nDelete commits\nReorder commits\nCombine multiple commits into one\nEdit/split an existing commit into multiple new ones\n\nWarning Note\n\nInteractive rebase rewrites your commit history\nThe commits you manipulate will have new hash ID’s\nYou should not use interactive rebase on stuff you already pushed to a remote repository\n\nUse it for cleaning up your local commit history before merging it into a shard team branch\n\nExample: When you are done developing on a feature branch\n\nSteps\n\nWhat should be the “base” commit?\n\nAt least the parent commit of the one you want to manipulate\n\ngit rebase -i HEAD~3\nIn the editor, only determine which actions you want to perform. Don’t change commit data in this step.\n\nChange commit message\n\nMost recent commit\n\ngit commit ammend\n\nOlder commits\n\ngit rebase -i HEAD~&lt;number-of-commits-before-HEAD&gt;\n\nOpens editor window with commits in selected range\n\nMark up the line with the target commit with the desired action\n\nreword &lt;commit-hash&gt; &lt;commit-message&gt;\n\nSave and close editor\n\nNew editor window will open\n\nChange commit message\nSave and close editor\n\n\nCombine two commits\n\nDetermine the base commit\n\ngit rebase -i HEAD~&lt;number-of-commits-before-HEAD&gt;\n\nMark up the line with the target commit with the desired action\n\nsquash &lt;commit-hash&gt; &lt;commit-message&gt;\nWill combine commit in the marked line with the commit in the line above it\n\nSave and close editor\n\nNew editor window will open\n\nAdd commit message for new commit"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#cherry-picking",
    "href": "posts/advanced-git-tools-notes/index.html#cherry-picking",
    "title": "Notes on Advanced Git Tools",
    "section": "Cherry-Picking",
    "text": "Cherry-Picking\n\nIntegrating single, specific commits\n\nNormally, you should integrate commits on the branch level\n\nShould only be used for special situations\nMoving a commit to a different branch\n\nSwitch to the branch the commit will be moved to\n\ngit switch &lt;branch-name&gt;\n\ngit cherry-pick &lt;commit-hash&gt;\nClean up branch the commit was moved from\n\ngit switch &lt;commit-origin-branch&gt;\ngit reset --hard HEAD~1"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#reflog",
    "href": "posts/advanced-git-tools-notes/index.html#reflog",
    "title": "Notes on Advanced Git Tools",
    "section": "Reflog",
    "text": "Reflog\n\njournal where git logs every movement of the HEAD pointer\nRecovering Deleted Commits\n\nDelete commit\n\ngit reset --hard &lt;commit-hash&gt;\n\nOpen reflog\n\ngit reflog\nentries are ordered chronologically with the most recent at the top\n\nRestore state before deletion\n\ngit branch &lt;new-branch&gt; &lt;commit-hash-before-deletion&gt;\n\n\nRecovering deleted branches\n\nDelete branch\n\ngit branch -d &lt;branch-name&gt;\n\nOpen reflog\n\ngit reflog\n\nFind the commit hash before the deletion\n\n\nRestore branch\n\ngit branch &lt;branch-name&gt; &lt;commit-hash&gt;"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#submodules",
    "href": "posts/advanced-git-tools-notes/index.html#submodules",
    "title": "Notes on Advanced Git Tools",
    "section": "Submodules",
    "text": "Submodules\n\nA standard git repository that is nested inside another repository\n\nDon’t manually copy-paste third-party code\n\nmixes external code with your own files\nUpdating the external code is a manual process\n\n\nThe actual content of a submodule is not part of the parent git repository\n\nstored in a .gitmodules file, .gitconfig file and .git/.gitmodules file\n\nRemote URL\nLocal path\nChecked out revision\n\n\nAdding a submodule\n\nOpen your git project\nCreate a new folder for the submodule (e.g. lib)\nEnter new folder\ngit submodule add &lt;remote-repo-url&gt;\ncreates a new .gitmodules file\n\nNeed to commit to main repository\n\ngit commit -m \"commit message\"\n\nCloning a project with submodules\n\nClone project like normal\n\ngit clone &lt;remote-url&gt;\nsubmodule folders are empty by default\n\nInitialize submodules\n\ngit submodule update --init --recursize\ntriggers cloning processes\n\n\nCloning a project with submodules (single step)\n\ngit clone --recurse-submodules &lt;remote-repo-url&gt;\n\nCheck out revisions in submodule\n\nsubmodule repositories are checked out on a commit, not a branch"
  },
  {
    "objectID": "posts/advanced-git-tools-notes/index.html#search-and-find",
    "href": "posts/advanced-git-tools-notes/index.html#search-and-find",
    "title": "Notes on Advanced Git Tools",
    "section": "Search and Find",
    "text": "Search and Find\n\nFiltering your commit history\n\ncan use these in combination\nby date --before/--after\n\ngit log --after=\"2021-7-1\" --before=\"2021-7-5\"\n\nby message --grep\n\ngit log --grep=\"search-string\"\nsupports regex\n\nby author --author\n\ngit log --author=\"author-name\"\n\nby file -- &lt;filename&gt;\n\ngit log -- &lt;filename&gt;\nthe -- is to make sure it is not confused for a branch name\n\nby branch &lt;branch-name&gt;\n\ngit log &lt;branch-name&gt;\n\ncommits in one branch but not another one\n\ngit log &lt;branch-A&gt;..&lt;branch-B&gt;\n\n\n\n\nAdditional Resources\nAdvanced Git Kit\nReferences:\n\nAdvanced Git Tutorial - Interactive Rebase, Cherry-Picking, Reflog, Submodules and more"
  },
  {
    "objectID": "posts/ai-3d-industry-notes/index.html",
    "href": "posts/ai-3d-industry-notes/index.html",
    "title": "Notes on How A.I. Will Change the 3D Industry",
    "section": "",
    "text": "Overview\nAssets are unreasonably expensive\nMachine Creep\nNot in Presentation"
  },
  {
    "objectID": "posts/ai-3d-industry-notes/index.html#overview",
    "href": "posts/ai-3d-industry-notes/index.html#overview",
    "title": "Notes on How A.I. Will Change the 3D Industry",
    "section": "Overview",
    "text": "Overview\nI went back and watched the talk Andrew Price (Blender Guru) gave at Blender Conference 2018 on how A.I. will change the 3D industry. This time, I decided to take some notes.\nQuestion to consider: What is not going to change in the next 10 years?"
  },
  {
    "objectID": "posts/ai-3d-industry-notes/index.html#assets-are-unreasonably-expensive",
    "href": "posts/ai-3d-industry-notes/index.html#assets-are-unreasonably-expensive",
    "title": "Notes on How A.I. Will Change the 3D Industry",
    "section": "Assets are unreasonably expensive",
    "text": "Assets are unreasonably expensive\n\nCreating a building Asset:\n\nModeling: 12 hours\nTexturing: 10 hours\nFirst Pass total: 22 hours\nRevisions: x2-3\n\n\n\nProblem: static workflows\n\n\nSolution: procedural workflows\n\nPractical Procedural Generation for Everyone (GDC 2017)\nPractical Procedural Generation for Everyone\nProcedural Modeling Example: Procedural Lake Village by Anastasia Opera\nProcedural Lake Village\nHoudini Procedural Lake Houses Complete\nProcedural Texturing Example: Poliigon Substance Designer\npoliigon\npoliigon generators\nProcedural Texturing Example: Substance Painter\n\nBake → Smart Materials → Smart Masks\n\nsubstance3d\nProcedural Level Design: Houdini\nProcedural World Generation of Ubisoft’s Far Cry 5\n\nCreate an ecosystem\n\nSet rules to define where certain trees and plants would live\nOther factors\n\nOcclusion\nFlow\nSlope\nCurvature\nIllumination\nAltitude\nLatitude\nLongitude\nWind\n\nTools for customization like roads and buildings"
  },
  {
    "objectID": "posts/ai-3d-industry-notes/index.html#machine-creep",
    "href": "posts/ai-3d-industry-notes/index.html#machine-creep",
    "title": "Notes on How A.I. Will Change the 3D Industry",
    "section": "Machine Creep",
    "text": "Machine Creep\nTraditional Software: input (e.g. photo) → action (filter) → output\nMachine Learning: input (e.g. photo) → assess → appropriate action → compare → is it good? → output\n\nNeeds huge datasets and fast hardware\n\n\nMachine Learning Use Cases\n\nDe-noising\nSuper resolution\nMotion Capture\n\nDensepose\n\nAnimation\n\nMode-adaptive Neural Networks for motion control\n\n\n\n\nMachine-Assisted Creativity\n\nProblem: experimentation takes up 50%-70% of time\nBicycleGAN\n\n\n\n\n\n\nModel input: outline of an object in an image and the ground truth image\nModel output: generate variations of image\nGitHub Repository\nToward Multi-modal Image-to-image translation\n\nDivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network\n\nGitHub Repository\n\nSketch to Image\n\npix2pix\nImage-to-Image Demo - Affine Layer\n\nGitHub Repository\n\n\nStyleGAN2\n\nGitHub Repository\nGitHub Repository\n\nProgressive Growing of GANS (PGAN)\n\nGitHub Repository\n\nPyTorch\nText to Image\n\nStackGAN V2 (2017)\ntext2image (April 2021)\nTediGAN (March 2021)\nDF-GAN\n\nStyle Transfer\n\nA Style-Aware Content Loss for Real-time HD Style Transfer\n\nAdaptive Style Transfer (TensorFlow 2018)\ncolor-transform (PyTorch 2019)"
  },
  {
    "objectID": "posts/ai-3d-industry-notes/index.html#not-in-presentation",
    "href": "posts/ai-3d-industry-notes/index.html#not-in-presentation",
    "title": "Notes on How A.I. Will Change the 3D Industry",
    "section": "Not in Presentation",
    "text": "Not in Presentation\n\nRelated Works\n\nGANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds\nimaginaire/projects/gancraft at master · NVlabs/imaginaire\nUnsupervised 3D Neural Rendering of Minecraft Worlds\nNeRS: Neural Reflectance Surfaces for Sparse-View 3D Reconstruction in the Wild (October 2021)\n\nGitHub Repository\n\nPIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (2020)\n\nGitHub Repository\nGoogle Colaboratory\n\n\n3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations\n\n\n\n\n\nhttps://nv-tlabs.github.io/3DStyleNet/assets/animal-new.mp4\n3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations\nDISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction\n\n\n\n\n\n\nGitHub Repository (Tensorflow)\nGitHub Repository (PyTorch)\n\nLearning Linear Transformations for Fast Arbitrary Style Transfer\n\nGitHub Repository\n\nNeural Cages for Detail-Preserving 3D Deformations\n\nGitHub Repository\n\nTaming Transformers for High-Resolution Image Synthesis\n\nGitHub Repository\nGitHub Repository\n\n[Overview] Taming Transformers for High-Resolution Image Synthesis\nThese Neural Networks Have Superpowers! 💪\nRethinking Style Transfer: From Pixels to Parameterized Brushstrokes\n\nGitHub Repository (TensorFlow)\nGitHub Repository (PyTorch)\n\nNetwork-to-Network Translation with Conditional Invertible Neural Networks\n\nGitHub Repository\n\nArtistic Style Transfer with Internal-external Learning and Contrastive Learning\n\nGitHub Repository\nBased on: SANET\n\nSynthetic Silviculture: Multi-scale Modeling of Plant Ecosystems\nMakowski.etal-2019-Synthetic-Silviculture.pdf\nMakowski.etal-2019-Synthetic-SilvicultureSup.pdf\nSynthetic Silviculture: Multi-scale Modeling of Plant Ecosystems\nSimulating A Virtual World…For A Thousand Years! 🤯\nDualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes\n\nGitHub Repository\n\n\n\n\nOther Applications\nHow A.I will affect the art industry\nTakeaway: AI will handle more and more of the tedious manual work that humans don’t like doing (or is extremely time consuming)\nThis will reduce the cost of production, enabling more productions overall\n\nRotoscoping\nWhat is rotoscoping animation and how to do it\n\nsegmentaion\n\nRetopology\nWhat is Retopology? (A Complete Intro Guide For Beginners)\n\nAppearance-Driven Automatic 3D Model Simplification (2021)\n\nhttps://research.nvidia.com/publication/2021-04_Appearance-Driven-Automatic-3D\nGitHub Repository\n\n\nHuman provides general outline/concept and a model fills in technical details\n\nNVIDIA GauGAN2\nNVIDIA Research’s GauGAN AI Art Demo Responds to Words\nNVIDIA Canvas\nNVIDIA Canvas : Harness The Power Of AI\n\nFacial animations\n\nReferences:\n\nVideo: The Next Leap: How A.I. will change the 3D industry - Andrew Price\nSlides: Google Slides"
  },
  {
    "objectID": "posts/arc-a770-testing/part-1/index.html",
    "href": "posts/arc-a770-testing/part-1/index.html",
    "title": "Testing Intel’s Arc A770 GPU for Deep Learning Pt. 1",
    "section": "",
    "text": "Overview\nOpenVINO Inference\nONNX-DirectML Inference\nPyTorch-DirectML Training"
  },
  {
    "objectID": "posts/arc-a770-testing/part-1/index.html#overview",
    "href": "posts/arc-a770-testing/part-1/index.html#overview",
    "title": "Testing Intel’s Arc A770 GPU for Deep Learning Pt. 1",
    "section": "Overview",
    "text": "Overview\nLast week, I received an Arc A770 GPU from Intel as part of their Graphics Innovator program. I am primarily interested in the card for its deep-learning performance, so I tested it with some of my tutorial projects and attempted to train some models using the pytorch-directml package.\nDesktop Specs:\n\n\n\nOS\nCPU\nMemory\nGPU Driver\n\n\n\n\nWindows 10 Pro 21H2\ni7-11700K\n32 GB DDR4 3000MHz\n31.0.101.3490\n\n\n\nLibrary Versions:\n\n\n\nOpenVINO\nONNX-DirectML\npytorch-directml\n\n\n\n\n2022.1 and 2022.2\n1.12.1\n1.8.0a0.dev220506"
  },
  {
    "objectID": "posts/arc-a770-testing/part-1/index.html#openvino-inference",
    "href": "posts/arc-a770-testing/part-1/index.html#openvino-inference",
    "title": "Testing Intel’s Arc A770 GPU for Deep Learning Pt. 1",
    "section": "OpenVINO Inference",
    "text": "OpenVINO Inference\nI first tested the card’s performance in the Unity project from my End-to-End Object Detection for Unity With IceVision and OpenVINO tutorial. The project uses OpenVINO 2022.1, and I noticed an odd sensitivity to input resolution when using FP16 precision.\nI use a default resolution of 398x224 (for a 16:9 aspect ratio), which translates to a 384x224 (divisible by 32) input resolution for the YOLOX tiny model. At this resolution, the model detects the same hand gestures with the Arc card as the CPU. However, the confidence scores are much lower, and the bounding box dimensions are slightly different (but still usable).\n\nCPU (FP16)\n\nObjects Detected: Call 78.54%, No Gesture 83.2%\n\n\n\n\n\n\n\n\nA770 (FP16)\n\nObjects Detected: Call 23.25%, No Gesture 40.86%\n\n\n\n\n\n\nMoving to a higher resolution brought inconsistent improvements in accuracy and occasional crashes. The below sample is with an input resolution of 896x512 at FP16 precision.\nObjects Detected: Call 78.06%\n\n\n\n\n\nI later updated OpenVINO to the recent 2022.2 release, which resolved this issue.\nObjects Detected: Call 78.55%, No Gesture 83.23%"
  },
  {
    "objectID": "posts/arc-a770-testing/part-1/index.html#onnx-directml-inference",
    "href": "posts/arc-a770-testing/part-1/index.html#onnx-directml-inference",
    "title": "Testing Intel’s Arc A770 GPU for Deep Learning Pt. 1",
    "section": "ONNX-DirectML Inference",
    "text": "ONNX-DirectML Inference\nI used the project from my Object Detection for Unity With ONNX Runtime and DirectML tutorial to compare the inference speeds between the A770 and my Titan RTX. This project uses the same YOLOX tiny model and input resolution as the OpenVINO one but in FP32 precision.\nThe Titan RTX, essentially a 2080 Ti, hit around 145fps, while the A770 hovered around 120fps.\n\nA770\nObjects Detected: Call 78.64%, No Gesture 83.35%"
  },
  {
    "objectID": "posts/arc-a770-testing/part-1/index.html#pytorch-directml-training",
    "href": "posts/arc-a770-testing/part-1/index.html#pytorch-directml-training",
    "title": "Testing Intel’s Arc A770 GPU for Deep Learning Pt. 1",
    "section": "PyTorch-DirectML Training",
    "text": "PyTorch-DirectML Training\nAs far as I know, the only way to train models at the time of writing on an Arc card is with the pytorch-directml package (or tensorflow-directml package).\nTo test this, I set up a conda environment in WSL with the pytorch-directml package and downloaded the sample repo provided by Microsoft. The pytorch-directml package requires python 3.8, and the sample repo uses torchvision 0.9.0.\nI successfully trained a ResNet50 model on CIFAR-10 with the sample training script. GPU memory usage was volatile when using a batch size higher than 4. The ResNet50 training script used less than 3.6 GB of GPU memory at a batch size of 4.\n\n\n\n\n\nHowever, it spikes to using all 16 GB at a batch size of 8 and crashes the script. I was able to train at a batch size of 6, but barely.\nI then attempted to train the style transfer model included with the pytorch examples repo and hit the wall of unimplemented operators. Here is the PyTorch DirectML Operator Roadmap. Some of the missing operators are on the current roadmap, but not all of them.\nThe tensorflow-directml package recently received its first update since May, so hopefully, the PyTorch version will receive an update soon. I have no idea when the main PyTorch and TensorFlow libraries will gain support for Intel GPUs, but hopefully, that is not too far off either."
  },
  {
    "objectID": "posts/backtracking-notes/index.html",
    "href": "posts/backtracking-notes/index.html",
    "title": "Notes on Backtracking Problems in Python",
    "section": "",
    "text": "Overview\nBacktracking\nBacktracking Template\nToy Example\nSolve N-Queens\nSolve Sudoku"
  },
  {
    "objectID": "posts/backtracking-notes/index.html#overview",
    "href": "posts/backtracking-notes/index.html#overview",
    "title": "Notes on Backtracking Problems in Python",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Lynn Zheng’s video providing a walkthrough on solving backtracking problems."
  },
  {
    "objectID": "posts/backtracking-notes/index.html#backtracking",
    "href": "posts/backtracking-notes/index.html#backtracking",
    "title": "Notes on Backtracking Problems in Python",
    "section": "Backtracking",
    "text": "Backtracking\n\nGoal: Finding valid states that satisfy a set of problem constraints\nApproach: Recursively try to satisfy all constraints by testing potential solutions, step by step, and undoing steps when there are no valid potential next steps\nBrute force approach\nDepth First Search\n\nBacktracking Template\n\nGet initial state\nCheck if state is valid\nGet list of valid potential next steps\nTry each potential step, depth first\nBacktrack one step when there are no potential next steps\n\nCould be from reaching a valid state or from choosing an invalid step\n\n\n# Check if the current state is a valid soluion\ndef is_valid_state(state):\n        # check if is a valid solution\n        return True\n\n# Get list of potential next steps\ndef get_candidates(state):\n    return []\n\n# Recursively, perform a depth-first search to find valid solutions\ndef search(state, solutions):\n    # Check is the state is valid\n    if is_valid_state(state):\n        # Add a copy of the valid state to list of solutions\n        solutions.append(state.copy())\n        # return # uncomment if you only need to find one valid solution\n\n    # Iterate through the candidates that can be used\n    # to construct the next state\n    for candidate in get_candidates(state):\n        # Add candidate to the current state\n        state.add(candidate)\n        # Call search function with updated state\n        search(state, solutions)\n        # Remove the current candidate from the current state\n        state.remove(candidate)\n\n# Entry point to the program\n# responsible for returning the valid solutions\ndef solve():\n    # start with an empty list of solutions\n    solutions = []\n    # start with an empty state\n    state = set()\n    # initiate the recursive search\n    search(state, solutions)\n    # return the final list of solutions\n    return solutions"
  },
  {
    "objectID": "posts/backtracking-notes/index.html#toy-example",
    "href": "posts/backtracking-notes/index.html#toy-example",
    "title": "Notes on Backtracking Problems in Python",
    "section": "Toy Example",
    "text": "Toy Example\n\nThere are 3 students\n\nBoy 1\nBoy 2\nGirl 1\n\nThere is 1 row containing three seats\nConstraints\n\nall seats must be filled\na student can only sit in one seat at a time\n\nFind every possible seating arrangement\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\n\n\n\n\n\n\nPossible Seating Arrangements\n\n\n\n\n\nThere are six possible arrangements\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nBoy 1\nBoy 2\nGirl 1\n\n\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nBoy 1\nGirl 1\nBoy 2\n\n\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nBoy 2\nBoy 1\nGirl 1\n\n\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nBoy 2\nGirl 1\nBoy 1\n\n\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nGirl 1\nBoy 1\nBoy 2\n\n\n\n\n\n\nSeat\n1\n2\n3\n\n\n\n\n1\nGirl 1\nBoy 2\nBoy 1\n\n\n\nreplit: https://replit.com/@innominate817/backtracking-toy-example#main.py\nOPTIONS = {\"B1\", \"B2\", \"G1\"}\n\n\n# Check if the current state is a valid soluion\ndef is_valid_state(state):\n    # The current state is valid is there is a unique student in each seat\n    return len(state) == 3\n\n\n# Get list of potential next steps\ndef get_candidates(state):\n    # print(list(OPTIONS.difference(set(state))))\n    return list(OPTIONS.difference(set(state)))\n\n\n# Recursively, perform a depth-first search to find valid solutions\ndef search(state, solutions):\n    # Check is the state is valid\n    if is_valid_state(state):\n        # Add a copy of the valid state to list of solutions\n        solutions.append(state.copy())\n        print(f\"Valid State Found: {state}\")\n        # return # uncomment if you only need to find one valid solution\n\n    # Iterate through the candidates that can be used\n    # to construct the next state\n    for candidate in get_candidates(state):\n        # Add candidate to the current state\n        state.append(candidate)\n        # Call search function with updated state\n        search(state, solutions)\n        # Remove the current candidate from the current state\n        print(f\"backtracking from: {state}\")\n        state.remove(candidate)\n\n\n# Entry point to the program\n# responsible for returning the valid solutions\ndef solve():\n    solutions = []\n    state = []\n    search(state, solutions)\n    return solutions\n\n\nif __name__ == \"__main__\":\n    solutions = solve()\n    print(solutions)\nValid State Found: ['G1', 'B2', 'B1']\nbacktracking from: ['G1', 'B2', 'B1']\nbacktracking from: ['G1', 'B2']\nValid State Found: ['G1', 'B1', 'B2']\nbacktracking from: ['G1', 'B1', 'B2']\nbacktracking from: ['G1', 'B1']\nbacktracking from: ['G1']\nValid State Found: ['B2', 'G1', 'B1']\nbacktracking from: ['B2', 'G1', 'B1']\nbacktracking from: ['B2', 'G1']\nValid State Found: ['B2', 'B1', 'G1']\nbacktracking from: ['B2', 'B1', 'G1']\nbacktracking from: ['B2', 'B1']\nbacktracking from: ['B2']\nValid State Found: ['B1', 'G1', 'B2']\nbacktracking from: ['B1', 'G1', 'B2']\nbacktracking from: ['B1', 'G1']\nValid State Found: ['B1', 'B2', 'G1']\nbacktracking from: ['B1', 'B2', 'G1']\nbacktracking from: ['B1', 'B2']\nbacktracking from: ['B1']\n[\n    ['G1', 'B2', 'B1'], \n    ['G1', 'B1', 'B2'], \n    ['B2', 'G1', 'B1'], \n    ['B2', 'B1', 'G1'], \n    ['B1', 'G1', 'B2'], \n    ['B1', 'B2', 'G1']\n]\nAdditional Constraints\n\nThe girl cannot sit in the middle (Seat 2)\n\n\n\n\n\n\nreplit: https://replit.com/@innominate817/backtracking-toy-example-1#main.py\nOPTIONS = {\"B1\", \"B2\", \"G1\"}\n\n\n# Check if the current state is a valid soluion\ndef is_valid_state(state):\n    # The current state is valid is there is a unique student in each seat\n    # and the girl is not in the middle seat\n    return len(state) == 3\n\n\n# Get list of potential next steps\ndef get_candidates(state):\n    # Can only use students that are not already seated\n    # and the girl cannot be in the middle seat\n    if len(state) &gt; 1 and state[1] == \"G1\": return []\n    return list(OPTIONS.difference(set(state)))\n\n\n# Recursively, perform a depth-first search to find valid solutions\ndef search(state, solutions):\n    # Check is the state is valid\n    if is_valid_state(state):\n        # Add a copy of the valid state to list of solutions\n        solutions.append(state.copy())\n        print(f\"Valid State Found: {state}\")\n        # return # uncomment if you only need to find one valid solution\n\n    # Iterate through the candidates that can be used\n    # to construct the next state\n    for candidate in get_candidates(state):\n        # Add candidate to the current state\n        state.append(candidate)\n        # Call search function with updated state\n        search(state, solutions)\n        # Remove the current candidate from the current state\n        print(f\"backtracking from: {state}\")\n        state.remove(candidate)\n\n\n# Entry point to the program\n# responsible for returning the valid solutions\ndef solve():\n    solutions = []\n    state = []\n    search(state, solutions)\n    return solutions\n\n\nif __name__ == \"__main__\":\n    solutions = solve()\n    print(solutions)\nValid State Found: ['B1', 'B2', 'G1']\nbacktracking from: ['B1', 'B2', 'G1']\nbacktracking from: ['B1', 'B2']\nbacktracking from: ['B1', 'G1']\nbacktracking from: ['B1']\nValid State Found: ['B2', 'B1', 'G1']\nbacktracking from: ['B2', 'B1', 'G1']\nbacktracking from: ['B2', 'B1']\nbacktracking from: ['B2', 'G1']\nbacktracking from: ['B2']\nValid State Found: ['G1', 'B1', 'B2']\nbacktracking from: ['G1', 'B1', 'B2']\nbacktracking from: ['G1', 'B1']\nValid State Found: ['G1', 'B2', 'B1']\nbacktracking from: ['G1', 'B2', 'B1']\nbacktracking from: ['G1', 'B2']\nbacktracking from: ['G1']\n[\n    ['B1', 'B2', 'G1'], \n    ['B2', 'B1', 'G1'], \n    ['G1', 'B1', 'B2'], \n    ['G1', 'B2', 'B1']\n]"
  },
  {
    "objectID": "posts/backtracking-notes/index.html#solve-n-queens",
    "href": "posts/backtracking-notes/index.html#solve-n-queens",
    "title": "Notes on Backtracking Problems in Python",
    "section": "Solve N-Queens",
    "text": "Solve N-Queens\nArbitrary 4-Queens State\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n0\nQueen\n\nQueen\n\n\n\n1\n\n\n\nQueen\n\n\n2\n\nQueen\n\n\n\n\n3\n\n\n\n\n\n\n\nValid State\n\nThe queens must be in positions where they cannot attack each other\nA queen can move horizontally, vertically, and diagonally\n\nNo two queens can be on the same row, column, or diagonals\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n0\n\nQueen\n\n\n\n\n1\n\n\n\nQueen\n\n\n2\nQueen\n\n\n\n\n\n3\n\n\nQueen\n\n\n\n\nConstructing Valid States\n\nBuild up from previous states\nStart with blank board\nPlace first queen arbitrarily\nThis reduces the valid options for placing the second queen\n\nIf a position is in the same row, column, or diagonal as the queen(s) on the board, it is removed from the list of valid options\n\nArbitrarily pick from the remaining valid spots for the second queen\nThis further reduces the valid spots for the third queen\nArbitrarily place the third queen in one of the remaining valid spots\nRepeat for N queens\n\nLeetcode Problem: N-Queens - LeetCode\n\nCould represent board as a 2D array\n\nWould be wasteful as no two queens can be on the same row or column\n\nCan keep a 1D list that tracks the queen position in each row\n\nExample: [1, 3, 0, 2]\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n0\n\nQueen\n\n\n\n\n1\n\n\n\nQueen\n\n\n2\nQueen\n\n\n\n\n\n3\n\n\nQueen\n\n\n\n\n\n\nreplit: https://replit.com/@innominate817/backtracking-n-queens#main.py\nimport numpy as np\n\n\n# Check if the current state is a valid soluion\ndef is_valid_state(state, num_queens):\n    # Confirm the target number of queens\n    # are on the board\n    return len(state) == num_queens\n\n\n# Get list of potential next steps\ndef get_candidates(state, num_queens):\n    if not state: return range(num_queens)\n\n    # Get next index\n    position = len(state)\n\n    candidates = set(range(num_queens))\n\n    for row, col in enumerate(state):\n        # Remove column indices already occupied in previous rows\n        candidates.discard(col)\n\n        # Get the offset value for finding the column index in the\n        # next row that would be diagonal to the Queen\n        # in the current row index\n        dist = position - row\n\n        # Remove potential column indices that are diagonal\n        # to the current column index\n        candidates.discard(col + dist)\n        candidates.discard(col - dist)\n\n    return candidates\n\n\n# Recursively, perform a depth-first search to find valid solutions\ndef search(state, solutions, num_queens):\n    # Check is the state is valid\n    if is_valid_state(state, num_queens):\n        # Add a copy of the valid state to list of solutions\n        solutions.append(state.copy())\n        print(f\"Valid State Found: {state}\")\n        # return # uncomment if you only need to find one valid solution\n\n    # Iterate through the candidates that can be used\n    # to construct the next state\n    for candidate in get_candidates(state, num_queens):\n        # Add candidate to the current state\n        state.append(candidate)\n        # Call search function with updated state\n        search(state, solutions, num_queens)\n        # Remove the current candidate from the current state\n        print(f\"backtracking from: {state}\")\n        state.remove(candidate)\n\n\n# Entry point to the program\n# responsible for returning the valid solutions\ndef solve(num_queens):\n    solutions = []\n    state = []\n    search(state, solutions, num_queens)\n    return solutions\n\n\nif __name__ == \"__main__\":\n    num_queens = int(input(\"Enter number of queens: \"))\n    solutions = solve(num_queens)\n\n    for solution in solutions:\n        board = np.full((num_queens, num_queens), \"-\")\n        for row, col in enumerate(solution):\n            board[row][col] = 'Q'\n        print(f'\\nSolution: {solution}')\n        print(board)\nEnter number of queens: 4\nbacktracking from: [0, 2]\nbacktracking from: [0, 3, 1]\nbacktracking from: [0, 3]\nbacktracking from: [0]\nValid State Found: [1, 3, 0, 2]\nbacktracking from: [1, 3, 0, 2]\nbacktracking from: [1, 3, 0]\nbacktracking from: [1, 3]\nbacktracking from: [1]\nValid State Found: [2, 0, 3, 1]\nbacktracking from: [2, 0, 3, 1]\nbacktracking from: [2, 0, 3]\nbacktracking from: [2, 0]\nbacktracking from: [2]\nbacktracking from: [3, 0, 2]\nbacktracking from: [3, 0]\nbacktracking from: [3, 1]\nbacktracking from: [3]\n\nSolution: [1, 3, 0, 2]\n[['-' 'Q' '-' '-']\n ['-' '-' '-' 'Q']\n ['Q' '-' '-' '-']\n ['-' '-' 'Q' '-']]\n\nSolution: [2, 0, 3, 1]\n[['-' '-' 'Q' '-']\n ['Q' '-' '-' '-']\n ['-' '-' '-' 'Q']\n ['-' 'Q' '-' '-']]"
  },
  {
    "objectID": "posts/backtracking-notes/index.html#solve-sudoku",
    "href": "posts/backtracking-notes/index.html#solve-sudoku",
    "title": "Notes on Backtracking Problems in Python",
    "section": "Solve Sudoku",
    "text": "Solve Sudoku\nLeetCode Problem: Sudoku Solver - LeetCode\nSample Board\n\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n0\n\n\n9\n7\n4\n8\n\n\n\n\n\n1\n7\n\n\n\n\n\n\n\n\n\n\n2\n\n2\n\n1\n\n9\n\n\n\n\n\n3\n\n\n7\n\n\n\n2\n4\n\n\n\n4\n\n6\n4\n\n1\n\n5\n9\n\n\n\n5\n\n9\n8\n\n\n\n3\n\n\n\n\n6\n\n\n\n8\n\n3\n\n2\n\n\n\n7\n\n\n\n\n\n\n\n\n5\n\n\n8\n\n\n\n2\n7\n5\n9\n\n\n\n\n\nSolved Board\n\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n0\n5\n1\n9\n7\n4\n8\n6\n3\n2\n\n\n1\n7\n8\n3\n6\n5\n2\n4\n1\n9\n\n\n2\n4\n2\n6\n1\n3\n9\n8\n7\n5\n\n\n3\n3\n5\n7\n9\n8\n6\n2\n4\n1\n\n\n4\n2\n6\n4\n3\n1\n7\n5\n9\n8\n\n\n5\n1\n9\n8\n5\n2\n4\n3\n6\n7\n\n\n6\n9\n7\n5\n8\n6\n3\n1\n2\n4\n\n\n7\n8\n3\n2\n4\n9\n1\n7\n5\n6\n\n\n8\n6\n4\n1\n2\n7\n5\n9\n8\n3\n\n\n\nreplit: https://replit.com/@innominate817/backtracking-sudoku#main.py\nimport os\nimport numpy as np\nfrom time import sleep\n\nclear = lambda: os.system('clear')\n\n# Get the cartesian product\nfrom itertools import product\n\n# Blank board\n# BOARD = np.full((9,9), '.')\n\nBOARD = np.array([\n    [\".\", \".\", \"9\", \"7\", \"4\", \"8\", \".\", \".\", \".\"],\n    [\"7\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],\n    [\".\", \"2\", \".\", \"1\", \".\", \"9\", \".\", \".\", \".\"],\n    [\".\", \".\", \"7\", \".\", \".\", \".\", \"2\", \"4\", \".\"],\n    [\".\", \"6\", \"4\", \".\", \"1\", \".\", \"5\", \"9\", \".\"],\n    [\".\", \"9\", \"8\", \".\", \".\", \".\", \"3\", \".\", \".\"],\n    [\".\", \".\", \".\", \"8\", \".\", \"3\", \".\", \"2\", \".\"],\n    [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"6\"],\n    [\".\", \".\", \".\", \"2\", \"7\", \"5\", \"9\", \".\", \".\"],\n])\n\n# 9x9 board\nSHAPE = 9\n# 3x3 sub squares\nGRID = 3\n# Indicates board position is empty\nEMPTY = '.'\n# Digits 1-9 in string format\nDIGITS = set([str(num) for num in range(1, SHAPE + 1)])\n\n\n# Get the values in the kth row\ndef get_kth_row(board, k):\n    return board[k]\n\n\n# Get the values in the kth column\ndef get_kth_col(board, k):\n    return [board[row][k] for row in range(SHAPE)]\n\n\n# Get the sub square that contains the [row][col] index\ndef get_grid_at_row_col(board, row, col):\n    row = row // GRID * GRID\n    col = col // GRID * GRID\n\n    return [\n        board[r][c]\n        # Get every [row][col] index for the sub square\n        for r, c, in product(range(row, row + GRID), range(col, col + GRID))\n    ]\n\n\n# Get all rows\ndef get_rows(board):\n    for i in range(SHAPE):\n        yield board[i]\n\n\n# Get all columns\ndef get_cols(board):\n    for col in range(SHAPE):\n        ret = [board[row][col] for row in range(SHAPE)]\n        yield ret\n\n\n# Get all sub squares\ndef get_grids(board):\n    # Iterate over each row with a stride of GRID\n    for row in range(0, SHAPE, GRID):\n        # Iterate over each column with a stride of GRID\n        for col in range(0, SHAPE, GRID):\n            grid = [\n                board[r][c]\n                # Get every [row][col] index for the sub square\n                for r, c in product(range(row, row +\n                                          GRID), range(col, col + GRID))\n            ]\n            yield grid\n\n\n# Check if the current state is a valid soluion\ndef is_valid_state(state):\n\n    for row in get_rows(state):\n        if set(row) != DIGITS:\n            return False\n\n    for col in get_cols(state):\n        if set(col) != DIGITS:\n            return False\n\n    for grid in get_grids(state):\n        if set(grid) != DIGITS:\n            return False\n\n    return True\n\n\n# Get list of potential next steps\ndef get_candidates(state, row, col):\n\n    # Keep track of digits already in the current\n    # row, column, or sub square\n    used_digits = set()\n    # Get digits already in current row\n    used_digits.update(get_kth_row(state, row))\n    # Get digits already in current column\n    used_digits.update(get_kth_col(state, col))\n    # Get digits already in current sub square\n    used_digits.update(get_grid_at_row_col(state, row, col))\n    # Only try digits not already in current row, column, and square\n    return DIGITS - used_digits\n\n\n# Recursively, perform a depth-first search to find valid solutions\ndef search(state):\n    # Check is the state is valid\n    if is_valid_state(state):\n        # Ther is only one valid state\n        print(f\"Valid State Found:\\n{state}\\n\")\n        return True\n\n    for row_i, row in enumerate(state):\n        for col_i, val in enumerate(row):\n            # Only try values for empty spots\n            if val == EMPTY:\n                # Iterate through the candidates that can be used\n                # to construct the next state\n                for candidate in get_candidates(state, row_i, col_i):\n                    # Add candidate to the current state\n                    state[row_i][col_i] = candidate\n                    # Call search function with updated state\n                    if search(state):\n                        return True\n                    else:\n                        # Uncomment to see process\n                        # sleep(0.1)\n                        # clear()\n                        # print(f'Initial Board:\\n{BOARD}\\n')\n                        # print(f\"backtracking from:\\n{state}\\n\")\n                        \n                        # Remove the current candidate from the current state\n                        state[row_i][col_i] = EMPTY\n                # None of the current candidates led to a valid state\n                return False\n    # No empty spots\n    return True\n\n\n# Entry point to the program\n# responsible for returning the valid solutions\ndef solve(board):\n    search(board)\n\n\nif __name__ == \"__main__\":\n\n    print(f'Initial Board:\\n{BOARD}\\n')\n    solve(BOARD.copy())\nInitial Board:\n[['.' '.' '9' '7' '4' '8' '.' '.' '.']\n ['7' '.' '.' '.' '.' '.' '.' '.' '.']\n ['.' '2' '.' '1' '.' '9' '.' '.' '.']\n ['.' '.' '7' '.' '.' '.' '2' '4' '.']\n ['.' '6' '4' '.' '1' '.' '5' '9' '.']\n ['.' '9' '8' '.' '.' '.' '3' '.' '.']\n ['.' '.' '.' '8' '.' '3' '.' '2' '.']\n ['.' '.' '.' '.' '.' '.' '.' '.' '6']\n ['.' '.' '.' '2' '7' '5' '9' '.' '.']]\n\nValid State Found:\n[['5' '1' '9' '7' '4' '8' '6' '3' '2']\n ['7' '8' '3' '6' '5' '2' '4' '1' '9']\n ['4' '2' '6' '1' '3' '9' '8' '7' '5']\n ['3' '5' '7' '9' '8' '6' '2' '4' '1']\n ['2' '6' '4' '3' '1' '7' '5' '9' '8']\n ['1' '9' '8' '5' '2' '4' '3' '6' '7']\n ['9' '7' '5' '8' '6' '3' '1' '2' '4']\n ['8' '3' '2' '4' '9' '1' '7' '5' '6']\n ['6' '4' '1' '2' '7' '5' '9' '8' '3']]\nReferences:\n\nSolve Coding Interview Backtracking Problems - Crash Course\n[Algo] Backtracking Template & N-Queens Solution\n6 Introduction to Backtracking - Brute Force Approach - Abdul Bari"
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-1/index.html#background",
    "href": "posts/barracuda-pose-estimation-project-log/part-1/index.html#background",
    "title": "Barracuda Pose Estimation Project Log Pt. 1",
    "section": "Background",
    "text": "Background\nI’ve been learning how to get pose estimation working in Unity using their new inference library called Barracuda. I think the name might be a reference to Nvidia’s CUDA library. Apparently, barracuda are really fast swimmers so maybe inference speed is a priority for the library as well. I have difficulty coming up with names for things so I’m always looking for new methods.\nAnyways, I’ve made some decent progress and have actually managed to get a basic proof of concept working. I figured now would be a good time to start a blog and actually track my progress. I’ve never been good about keeping notes when working on projects. Hopefully, this project will provide sufficient motivation for changing that and get me to write more often (or at all really).\nI’ve already noticed a conflict between the desire to actually work on the project and writing about it. Maybe, I should try writing posts as I work on projects in the future. Well, I guess I should keep that in mind for later. For this post, I just want to outline the project and what I’ve accomplished so far."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-1/index.html#the-project",
    "href": "posts/barracuda-pose-estimation-project-log/part-1/index.html#the-project",
    "title": "Barracuda Pose Estimation Project Log Pt. 1",
    "section": "The project",
    "text": "The project\nI’m really interested in the potential applications for mapping a user’s movements to a virtual character in real-time. Traditionally, this required specialized hardware like motion capture suits. Fortunately, there are deep learning models that now let you accomplish the same thing with a regular webcam and sufficient computing power. These models perform what is called pose estimation.\nPose estimation is a technique where a model predicts the location of a person or object in an image or video. When tracking humans, a model is typically trained to predict the locations of key points on a person’s body (e.g. joints, nose, eyes, etc.). You can learn more about pose estimation here. I want to use these types of models inside a Unity application so I can combine the with all other features included in a modern real-time 2D/3D development platform.\n\nProject Goals\nMy goals for this project are evolving as I discover more of what’s possible. As a result, I don’t really a have a “definition of done”. I’m just seeing where it takes me for now. However, the basic goal for this project is to use the key point locations predicted by a pose estimation model to control a virtual character. For example, when the user moves their arm, the virtual character’s arm should move accordingly. Below are some more specific requirements that I’m currently working towards.\n\nRequirements:\n\nScale the estimated pose that the model outputs to account for differences in size between the target character sprite/model and the size of the image being fed into the model.\nHandle differences in body proportions between the user and the character.\nSeparate the estimated pose from the user’s location in the camera frame.\nSmooth out any choppiness in character movement due to updates in the pose predictions made by the model.\nKeep the frame rate high enough that the application isn’t jarring for the user."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-1/index.html#current-progress",
    "href": "posts/barracuda-pose-estimation-project-log/part-1/index.html#current-progress",
    "title": "Barracuda Pose Estimation Project Log Pt. 1",
    "section": "Current Progress",
    "text": "Current Progress\n\nProof of Concept\nAs mentioned previously, I have actually gotten a basic example working in Unity. I’ve also learned how to leverage compute shaders to perform the preprocessing steps on the GPU.\n\n\nSome Weak Points\nFiguring out how to process inputs and outputs for neural networks efficiently inside Unity has been the most irritating part of the project so far. It’s given me a new appreciation for all the great data science resources available in the Python ecosystem. Basic things like getting slices of arrays and matrices are such a pain in C# compared to Python. It really highlights the need to identify and learn how to leverage the strengths and weaknesses of the tools your working with. Compute shaders are definitely one of the more important strengths to leverage in Unity.\n\n\nSome Strong Points\nCompute shaders are awesome for doing the same thing to every element in a data structure on the GPU. They’re actually what the Barracuda library uses. There is a bit of a learning process for figuring out how to actually get the data you want to the shader, access the data within the shader, and how to get the output back. It took a lot of googling to figure out but it’s worth the effort.\nSo far, I’ve managed to get my whole preprocessing pipeline to run on the GPU. I can load an input image onto the GPU, crop it, resize it, normalize the values, and execute the model all without sending data back and forth between the CPU and GPU. Aside from being way faster, this also has the benefit of completely freeing up the CPU to do other things.\n\n\nCurrent Challenges\nI still need to figure out if I can get the post processing done on the GPU. That’s my last remaining bottleneck for fully utilizing my GPU during runtime. It also highlights just how much overhead there is when transferring data from the GPU back to the CPU. When testing different methods for preprocessing, I discovered that the simple act of downloading a tensor from the GPU to the CPU took longer than it did to actually run the model.\nUnfortunately, the Barracuda library does not make it intuitive for extracting information from a model’s output. This is an area that could really use improvement. My current method for processing the output in Unity is nested for loops. Part of the reason is that tensors can’t be accessed outside of the main thread. If I want to leverage parallel processing, I need to download the tensor data to an array on the CPU. The overhead from downloading the data seems to wipe out any performance benefits.\nSince I haven’t figured out how to efficiently access slices of arrays, I need iterate through the whole output tensor to find the most likely locations for key points in the input image. That’s not a big deal if the output tensor is small, but quickly becomes a problem when using larger input images. Unfortunately, the model architecture I’m using seems to require large input images to get more accurate pose estimations.\n\n\nPotential Roadblocks\nSomething else this project has highlighted is how much of a pain it still is to get machine learning models from the training environment to arbitrary production applications. Unity requires your model to either be in the native Barracuda or ONNX format. I decided to start with a pretrained TensorFlow model while I get the hang of using Barracuda. That meant I needed to convert the model to ONNX before I could begin working with it in Unity.\nUnfortunately, TensorFlow does not contain any built-in methods for exporting trained models to ONNX. They seem to prefer that you stay within their ecosystem. However, it also doesn’t contain a complete set of methods for converting between different TensorFlow libraries. For example, they provide a method to convert standard TensorFlow models to either TensorFlow Lite or Tensorflow.js format but not the other way around. This created a bit of a road block, since TensorFlow only provides tflite and TFJS versions of the pretrained PoseNet model I’m using. That combined with the lack of built in support for exporting to ONNX means that there aren’t any officially supported ways to get an arbitrary TensorFlow model into Unity. That’s a bit inconvenient since I wanted to start with a pretrained model so that I wouldn’t need to spend a bunch of time training my own model before even getting into Unity.\n\n\nPotential Solutions\nI eventually found a third-party library to convert a pretrained TFJS model to the standard TensorFlow SavedModel format. I then had to use another third-party library to convert the converted TensorFlow model into ONNX. It might seem like these third-party libraries completely resolve the missing functionality in TensorFlow. Unfortunately, this method requires that all third-party libraries used, implement support for whatever neural network layers are used in the model you want to convert. The PoseNet architecture is fully supported, but a lot of the the newer pretrained models released for TensorFlow contain new types of layers that are not yet supported by these libraries. Even if these libraries did support these new layer types, Unity would likely still need to implement support for them in Barracuda. This all introduced a bunch work that I had to get done before I could even begin making a proof of concept in Unity.\nWhile I was looking for solutions to the challenges in the previous paragraph, I came across some methods that others developed for manually converting models from one library to another. It involves using a neural network analysis tool to construct a JSON file that contains the network topology and then using that to construct the same topology using the target library. You then need to iterate through the trained model to get the weights for each layer and assign them to the appropriate layer in the new model.\nI haven’t tried out this method yet, but I’ll be sure to make a post describing how that goes. It definitely isn’t ideal, and it probably requires that all the layer types supported in the source library be implemented in the target library. For me, the source library would likely be a TensorFlow library and the target library would be PyTorch. PyTorch includes built in support for exporting models to ONNX so that should help streamline that part of the process. Even if I can’t get the trained weights to work in PyTorch, I should at least be able to get a model that I can then train on my own without having to work out the network topology from some research paper. I plan to do this after I get further along inside Unity anyways."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-1/index.html#next-steps",
    "href": "posts/barracuda-pose-estimation-project-log/part-1/index.html#next-steps",
    "title": "Barracuda Pose Estimation Project Log Pt. 1",
    "section": "Next Steps",
    "text": "Next Steps\nI want to see if I can get the post processing steps done on the GPU. I’d still need to get the processed output to the CPU to actually do anything with it. However, there would be much less data that would need to be transferred. That should hopefully minimize any overhead.\nThe next step is to figure out how to actually map the predicted key point locations to a character model/sprite. Before that, I’ll need to learn more about character animation in Unity. The pose estimation model I’m currently using only supports predictions in 2D space. Therefore, I’ll start with 2D characters to keep things simple.\nOnce I figure out how to map the model outputs to the virtual character, I’ll see if I can improve the model’s accuracy. The pretrained PoseNet models provided by TensorFlow work well enough but I doubt they’re as good as they could be. I also plan on training the models using custom datasets to see if I can improve the model’s performance. I’ve been learning about how you can make synthetic datasets using tools such as Blender and I’m really curious to see if I can use Blender’s built in automation capabilities to make a high quality dataset for pose estimation as well as other computer vision applications."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-1/index.html#conclusion",
    "href": "posts/barracuda-pose-estimation-project-log/part-1/index.html#conclusion",
    "title": "Barracuda Pose Estimation Project Log Pt. 1",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all for now. I’m feel like I’m forgetting to mention a bunch of stuff. It’s definitely not ideal to write about something weeks after the fact. Fortunately, I plan to make separate posts that go into further detail anyways. Hopefully those will fill in any blanks. I’ll probably update this post with images and other stuff as well. That way it won’t be just a wall of text."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-2/index.html",
    "href": "posts/barracuda-pose-estimation-project-log/part-2/index.html",
    "title": "Barracuda Pose Estimation Project Log Pt. 2",
    "section": "",
    "text": "Update 7/31/2021: Barracuda PoseNet Tutorial 2nd Edition\nI didn’t make much actual progress yesterday. Although, it was fairly educational. I spent the day trying to create a PyTorch model that would help speed up the post processing steps for the pose estimation model. The model outputs a heatmap and offsets for each of the 17 key points. Each element in a heatmap contains a confidence level for whether the relevant key point is in that cell. The offsets contain x and y vectors that are used to refine the coarse location from the heatmap. The part that seems to create a bottleneck is extracting the index value for the heatmap element that contains the highest confidence estimate. Currently, I need to iterate through every element in each of the heatmaps. This isn’t a huge issue when using a very small input image as the size of the heatmap is limited by input resolution. However, larger input images can yield more accurate predictions. Even when using 540p input images, the size of the heatmap increases quite a bit compared to a 300p image.\nRight now, post processing needs to be done on the main thread. Barracuda tensors either need to be manipulated on the GPU or on the main thread on the CPU. You need to download the data from the tensor to a float array if you want to perform the work in parallel on the CPU. Unfortunately, downloading the data from the GPU to the CPU seems to wipe out any performance gains from iterating over the data in parallel. That’s why I decided to spend a day beating my head against a wall trying to do the post processing on the GPU.\nMy plan was to simply use the argmax function available in PyTorch to extract the index with the highest confidence level for each of the 17 heatmaps. I thought this would be fine since the argmax operation is supported by ONNX. However, the dev team for the Barracuda library has not yet implemented the functionality to support the argmax operation yet. I confirmed this by double checking the current list of supported operations in the Barracuda documentation. Fortunately, the dev team is aware of the missing functionality and it’s on there todo list. But, that doesn’t really help me right now so I decided to spend the rest of the day trying to come up with a work around using a combination of supported operations. Basically nothing I tried really worked and the few things that did, didn’t actually improve performance.\nThe endeavor once again highlighted how much of a pain it is to access array slices in C#. Apparently, it gets slightly better in C# 8. Unfortunately, the only Unity version that supports C# 8 is the current beta build. I don’t need yet another source of debugging rabbit holes right now so I’ll wait to try that until it’s out of beta.\nIt’s funny, the odd things that crop up when you make a model in one framework, export it to another, and then load it into Unity that then converts it to it’s own internal representation. Barracuda apparently doesn’t like tensors being squeezed when loading an ONNX model as it apparently introduces some ambiguity. There was also some compute shader related issue that kept cropping up every time I tried to use a PyTorch model to permute the axes in a tensor. I don’t know if the error is because I did something wrong or if it’s just a bug in part of the Barracuda library. It’s probably a combination of both.\nI also tried to create a PyTorch model that split the different heatmaps into separate outputs. The goal of this was to make it possible to leverage the argmax helper function in Unity. The helper function isn’t super helpful for me since it needs to download the tensor data to the CPU to work. This is a problem since downloaded tensors get saved channel first. For example, a tensor containing an RGB image would be stored in an array pixel by pixel. In other words, every 3 elements contain the RGB values for a single pixel. For the heatmaps, this means that a sequence of 17 elements represent the confidence level for each key point for a single pixel rather than one entire heatmap after the other.\nI’m getting close to the limit of how much time I’m willing to spend on this issue before moving on. It wouldn’t be the end of the world if I can’t do post processing on the GPU but it definitely caps the max framerate. I want to try a few other things, but I have a feeling that I’ll need to wait until the Barracuda dev team implements support the the argmax operation."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-3/index.html#post-processing-notes",
    "href": "posts/barracuda-pose-estimation-project-log/part-3/index.html#post-processing-notes",
    "title": "Barracuda Pose Estimation Project Log Pt. 3",
    "section": "Post Processing Notes",
    "text": "Post Processing Notes\nUntil I can use an ONNX model that contains an argmax operation in Unity, I don’t think there’s much to be gained from making a PyTorch model to handle post processing. If I can’t reduce the size of the array that gets downloaded to the CPU, there isn’t a consistent or noticeable improvement in performance.\nI could try to make a compute shader that performs an argmax operation. I’m not sure what the best way would be to get the tensor data to the compute shader though. I also don’t know what I would do to keep track of the index with the highest value when everything is done in parallel.\nAfter some research, it seems that the best way to do this with a compute shader involves incrementally reducing the size of the RenderTexture by taking the max value of a group of neighboring pixels over and over. This seems like max pooling which could be done with a PyTorch model. I don’t know what the best way would be to keep track of the original index of the max value though. It seems that Barracuda only has partial support for max pooling. The MaxPool2d layer in PyTorch appears to have the option for returning the max indices along with the output. I have a weird feeling that Barracuda won’t support having that option enabled. Well, might as well give it a shot. If this doesn’t work, I’m going to move on for now. This is really only a problem for extracting every last bit of processing power from the GPU.\nAs expected, Barracuda did not like having the return_indices option enabled for the max pooling layer. At best then, I would still probably need to download the tensor data to the CPU to determine index that contains the returned values. Oh well, time to move on for now."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-3/index.html#key-point-location-scaling-notes",
    "href": "posts/barracuda-pose-estimation-project-log/part-3/index.html#key-point-location-scaling-notes",
    "title": "Barracuda Pose Estimation Project Log Pt. 3",
    "section": "Key Point Location Scaling Notes",
    "text": "Key Point Location Scaling Notes\nDepending on the model and input image resolution, the displayed key points seem to jump around a lot. Part of this is because the pose skeleton positions aren’t being updated smoothly. However the gaps in between positions seem rather large. I’m guessing this is due to the size of the heatmaps. I would have thought that the offset vectors would decrease the perceived gap between positions. Cranking up the input resolution to 720p does make this behavior less obvious but it’s still there. Lowering the input resolution to 256x256 makes the gaps rather absurd. It seems like the offset vectors are either wrong or they are not being scaled appropriately. Side note, there seems to be a cap in performance gained from lowering the input resolution. It seems that once you get to a point where the model can be executed instantaneously on the GPU, any post processing becomes a hard bottleneck.\nLooking at the skeleton drawn when using lower resolutions. I think the problem is in my implementation for scaling the model output back up to the original resolution. The gaps between the estimated key points are massive.\nSo, removing the offset vectors from the key point skeleton revealed some insights. The lower resolution heatmaps definitely contribute to the large gaps in key point locations. I need to take a closer look at how I’m determining which offset vectors to use as they seem off. Using an input resolution of 720p, the same as the source video, without offset vectors looks pretty spot on. The pose skeleton becomes increasingly blocky and less accurate the more you lower the input resolution. I wonder if I can upscale the heatmaps? Using a 720p input resolution with a ResNet50 is fine for a high-end desktop graphics card, but probably not for less powerful devices. However, it does seem that the resolution of the heatmaps is the most important factor for visual accuracy. Using an input resolution that’s higher than the source image doesn’t appear to be useful.\nIt’s definitely looking like a cut down ResNet50 model is still way better than a MobileNet. The MobileNet will require extra post processing to clean up the output from the heatmaps. Interestingly, MobileNet seems to do worse with higher resolution input images than lower resolutions.\nSince my desktop can handle using a 720p input resolution, I’ll stick with that for now and try to improve the heatmap accuracy later. Next, I’m going to focus on mapping the key point locations to a virtual character."
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-4/index.html#i-fixed-the-offset-vectors",
    "href": "posts/barracuda-pose-estimation-project-log/part-4/index.html#i-fixed-the-offset-vectors",
    "title": "Barracuda Pose Estimation Project Log Pt. 4",
    "section": "I Fixed the Offset Vectors…",
    "text": "I Fixed the Offset Vectors…\nWell I feel like an idiot. It turns out I’ve had the x and y offset vectors swapped this whole time. That explains why the gap problem with the heatmaps seemed so much worse when using lower resolutions. The offsets had to be bigger but they were going in the wrong directions.\nI finally stumbled across my mistake by pausing the project and clicking through frame by frame to see if the corresponding x and y values made sense. I can’t believe I didn’t check the values for those sooner. I’m guessing that I was just glad it was working at all at the time and moved on to other things. Well, lesson learned I guess.\nOn the bright side, the pose skeleton looks way better now! Comparing the pose skeletons with and without the offsets swapped makes it really apparent in hind sight. Also, this completely resolved the issues with the MobileNet models mentioned in the previous post. Their performance is still noticeably worse than the ResNet model but that’s expected.\nAnnoyingly, now that I’ve finally fixed this issue the post processing bottleneck is all the more noticeable. The MobileNet model is much more efficient but the current bottleneck means there isn’t much difference in framerate.\n\nBefore:\n\n\nVideo\n\n\n\n\nAfter:"
  },
  {
    "objectID": "posts/barracuda-pose-estimation-project-log/part-5/index.html",
    "href": "posts/barracuda-pose-estimation-project-log/part-5/index.html",
    "title": "Barracuda Pose Estimation Project Log Pt. 5",
    "section": "",
    "text": "Update 7/31/2021: Barracuda PoseNet Tutorial 2nd Edition\nIn further proof that you should never skip days when trying to form a new habit, I allowed myself to get completely distracted over past few weeks. There really are too many cool things to try out in the machine learning space these days. Although, if I’m being honest, I probably allowed myself to get distracted in the first place because I wasn’t sure what I wanted to work on next in the pose estimation project. I think it’s almost time to move on from 2D to 3D pose estimation models. However, there are some things I want to try out before making that jump. First things first though. I did actually make a bit more progress before getting sidetracked.\n\n\n2D Sprite Animation\nHaving fixed the swapped offsets, I’ve started working on mapping the estimated key point locations to a 2D sprite. Before I could do that though, I had to learn about 2D character rigging. Fortunately, Brackeys has a great video explaining how to do it. The sprite used in the video doesn’t seem to be available anymore so I also had learn how to setup my own. I decided to go with this one from the Mighty Heroes (Rogue) 2D Fantasy Characters Pack for no particular reason.\n\n\n\n2D_character_sprite\n\n\nIt turns out that 2D sprites are super tiny compared to the range of input resolutions I use for the pose estimation model. My current solution for resolving the size difference involves scaling key point locations based on the relative distances between joints. For example, I get the scale for the left wrist location by dividing the distance between the shoulder and the elbow for the sprite by the distance between the estimated location for the user’s shoulder and elbow.\nThere’s also the small issue that the estimated key point locations are dependent on the user’s location in the image. I decided to separate the estimated key point locations from the user’s location in frame by getting the relative distance between key points. I then used that information to offset the sprite’s key points.\nThis actually worked pretty well when combined with Unity’s 2D character rigging package. There is a bit of shakiness introduced from my current method of updating the key point locations. Right now, the game objects that are mapped to the processed output from the model basically teleport to the latest position. It should be easy to resolve the shakiness by using conventional methods for smoothly updating a game object’s position in Unity.\n\n\nMy Results:\n\n\n\n2D_sprite_animation\n\n\nThis 2D sprite turned out not to be ideal for testing out a full range of poses. It’s drawn facing towards one side of the screen which makes it a bit awkward when testing out the joint mappings. Ideally, the sprite would be facing directly towards the user.\n\n\nUpdates on Post Processing Optimization\nThere may be hope yet for doing the post processing on the GPU. Updates to Barracuda have fixed some of the bugs with custom models. I can now use a GlobalMaxPooling layer to get the max value for each of the key point heatmaps on the GPU. Unfortunately, I still need to iterate through the heatmap tensors on the CPU find the indices the max values are located at. There’s a very slight improvement in FPS since I only need to iterate up to the index with the with the matching value for each heatmap. However, this might not be necessary for much longer.\nThe current preview release for Barracuda adds a TopKIndices layer. According to the documentation, this layer retrieves the indices for the top-K largest (or smallest) elements along the specified axis. That sounds perfect! There is one slight problem. It doesn’t seem to work yet.\nIt could be that I haven’t figured out how to use the layer properly. After digging through the source code, it seems the TopKIndices layer expects another layer object as the k parameter. I’m not sure what to do with that. I would have thought that k would just be an integer. Perhaps it wants an input layer that specifies k? It’s also possible that the functionality for the layer hasn’t been fully implemented yet. This release does seem quite buggy. I think I’ll wait until it’s out of preview before spending more time with it."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 24, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library. We’ll be using a pretrained PoseNet model to estimate the 2D locations of key points on an individual’s body.\nThis post demonstrates how to play and view videos inside Unity. We’ll later perform pose estimation on individual frames while the video is playing. We can gauge the model’s accuracy by comparing the estimated key point locations to the source video."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#prerequisites",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#prerequisites",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Prerequisites",
    "text": "Prerequisites\nI recommend checking the following prerequisites if you want to follow along on your own computer.\n\nUnity\nThis tutorial assumes that you have Unity installed. You can get acquainted with Unity by clicking on one of the tutorials listed below.\n\n\nHow to Make a Game - Unity Beginner Tutorial\n\nUnity 2019.3\nUnity 2020.1\n\nNote: You can download the exact version of Unity used for this tutorial by clicking the link below.\n\nUnity 2019.4.13\n\n\n\nHardware\nWe’ll be performing inference on the GPU for this series. If possible, use a graphics card from a recent generation."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#create-a-new-project",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#create-a-new-project",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Create a New Project",
    "text": "Create a New Project\nFirst, we need to create a new Unity project. We’ll select the 2D template since the PoseNet model only estimates 2D poses."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#import-video-files",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#import-video-files",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Import Video Files",
    "text": "Import Video Files\nWe’ll be using these two videos available on Pexels, a free stock photos & videos site. The first one is easier for the PoseNet model. The second has some more challenging sections. Download the videos in Full HD resolution.\n\nTwo Young Men Doing a Boardslide Over a Railing\nNote: Renamed to pexels_boardslides\nWoman Dancing\nNote: Renamed to pexels_woman_dancing\n\n\nCreate the Videos Folder\nIn the Assets window, right-click an empty space, select the Create option, and click Folder. Name the folder Videos.\n\n\n\n\n\nDouble-click the Videos folder to open it.\n\n\nAdd Video Files\nDrag and drop the two video files from the file explorer into the Videos folder."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#create-the-video-player",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#create-the-video-player",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Create the Video Player",
    "text": "Create the Video Player\nIn the Hierarchy tab, right-click an empty area, select the Video section, and click Video Player. This will create a new GameObject called Video Player. The default name works well enough so we’ll leave it as is.\n\n\n\n\n\n\nSet Video Clip\nSelect the Video Player object in the Hierarchy tab. Then, drag and drop the pexels_boardslides file into the Video Clip parameter in the Inspector tab.\n\n\n\n\n\n\n\nMake the Video Loop\nTick the Loop checkbox in the Inspector tab to make the video repeat when the project is running."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#create-the-video-screen",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#create-the-video-screen",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Create the Video Screen",
    "text": "Create the Video Screen\nWe need to make a “screen” in Unity to watch the video. We’ll use a Render Texture to store the data for the current frame and attach it to the surface of a GameObject.\n\nCreate a Render Texture\nCreate a new folder in the Assets window and name it Textures.\n\n\n\n\n\nOpen the folder and right-click an empty space. Select Render Texture in the Create submenu and name it video_texture.\n\n\n\n\n\n\n\nResize the Render Texture\nSelect the video_texture asset and set the Size to 1920 x 1080 in the Inspector tab. This will match the video_texture to the resolution of our videos.\n\n\n\n\n\n\n\nAssign the Render Texture\nWith the resolution set, select the Video Player object in the Hierarchy tab again. Drag and drop the video_texture object into the Target Texture parameter in the Inspector tab.\n\n\n\n\n\n\n\nCreate the Screen GameObject\nNow, we need to create the screen itself. We’ll use a Quad object for the screen. Right click an empty space in the Hierarchy tab, select the 3D Object section and click Quad. We can just name it VideoScreen.\n\n\n\n\n\n\n\nResize the Screen\nWith the VideoScreen object selected, we need to adjust the Scale parameter in the Inspector tab. Set the X value to 1920 and the Y value to 1080. Leave the Z value at 1.\n\n\n\n\n\n\n\nSet the Screen Position\nNext, we’ll move VideoScreen to make things easier when processing output from the model. We want the bottom left corner to be at the origin. Set the X value for Position to half the X value for the Scale parameter. Do the same for the Y value. The new Position values should be X: 960 Y: 540 Z: 0.\n\n\n\n\n\n\n\nReset the Scene Perspective\nWe should center our perspective on the VideoScreen. We can do so by selecting the VideoScreen object and pressing the F key on our keyboard. You can zoom back in by scrolling up with your mouse wheel.\n\n\n\n\n\n\n\nApply the Render Texture to the Screen\nDrag and drop the video_texture asset onto the VideoScreen in the Scene tab. The VideoScreen object should turn completely black.\n\n\n\n\n\n\n\nMake Video Screen Unlit\nWith the VideoScreen object selected, click the Shader dropdown in the Inspector tab. Select the Unlit option and click Texture. This removes the need for a separate light source. The videos would look extremely dim with the Standard shader."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#camera-setup",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#camera-setup",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Camera Setup",
    "text": "Camera Setup\nBefore playing the video, we need to reposition and resize the Main Camera object.\n\nSet Camera Position\nSelect the Main Camera object in the Hierarchy tab and set the Position to same X: 960 Y: 540 as the VideoScreen object. Next, we need to set the Z value for the Position to the opposite of the X value.\n\n\n\n\n\n\n\nResize the Camera\nFinally, we need to adjust the Size parameter to 540 in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#test-the-video-player",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#test-the-video-player",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Test the Video Player",
    "text": "Test the Video Player\nNow we can finally click the play button and watch the video.\n\n\n\n\n\n\nResult"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe now have a video player that we can use to feed input to the PoseNet model. The next post covers how to prepare input for the model on the GPU.\n\nGitHub Repository - Version 1"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-1/index.html#next-part-2",
    "href": "posts/barracuda-posenet-tutorial/part-1/index.html#next-part-2",
    "title": "Barracuda PoseNet Tutorial Pt. 1 (Outdated)",
    "section": "Next: Part 2",
    "text": "Next: Part 2"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 25, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nThe PoseNet model we’ll be using has a ResNet-50 architecture and was created using TensorFlow. It takes a single RGB image as input. We need to perform some preprocessing operations on the RGB channel values before feeding an image to the model. We’ll first scale the values so that they are in the same range that the model was trained on. We then subtract the mean RGB values for the ImageNet dataset."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html#create-a-compute-shader",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html#create-a-compute-shader",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "Create a Compute Shader",
    "text": "Create a Compute Shader\nWe can perform the preprocessing steps more quickly on the GPU. In Unity, we accomplish this with compute shaders. Compute shaders are pieces of code that can run parallel tasks on the graphics card. This is beneficial since we need to perform the same operations on every pixel in an image. It also frees up the CPU.\n\nCreate the Asset File\nCreate a new folder in the Assets window and name it Shaders. Open the Shaders folder and right-click an empty space. Select Shader in the Create submenu and click Compute Shader. We’ll name it PoseNetShader.\n\n\n\n\n\n\n\nRemove the Default Code\nOpen the PoseNetShader in your code editor. By default, the ComputeShader will contain the following.\n\n\n\n\n\nDelete the CSMain function along with the #pragma kernel CSMain. Next, we need to add a Texture2D variable to store the input image. Name it InputImage and give it a data type of &lt;half4&gt;. Use the same data type for the Result variable as well.\n\n\n\n\n\n\n\nCreate PreprocessResNet Function\nWe need to make a new function to apply the ResNet preprocessing. Name the new function PreprocessResNet(). We’ll use the default [numthreads(8,8,1)].\n\n\n\n\n\nThe PreprocessResNet function scales the RGB channel values of every pixel in the InputImage by 255. By default, color values in Unity are in the range of [0,1]. The function then substracts the ImageNet mean specific to the RGB channels. The processed image is returned in the Result variable.\n\n\n\nChannel\nImageNet Mean\n\n\n\n\nRed\n123.15\n\n\nGreen\n115.90\n\n\nBlue\n103.06\n\n\n\n\n\n\n\n\nNow that we’ve created our ComputeShader, we need to execute it using a C# script."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html#create-the-posenet-script",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html#create-the-posenet-script",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "Create the PoseNet Script",
    "text": "Create the PoseNet Script\nWe need to make a new C# script to perform inference with the PoseNet model. When finished, this script will load the model, prepare the input, run the model, and process the output. For this post, we’ll implement the preprocessing functionality.\n\nCreate the Asset File\nCreate a new folder in the Assets window and name it Scripts. In the Scripts folder, right-click an empty space and select C# Script in the Create submenu.\n\n\n\n\n\nName the script PoseNet.\n\n\n\n\n\nOpen the script in your code editor.\n\n\nCreate videoTexture Variable\nAbove the start method, create a new public RenderTexture named videoTexture. This is the variable to which we’ll assign the video_texture that we made in part 1.\n\n\nCreate posenetShader Variable\nWe’ll also create a new public ComputeShader variable and name it posenetShader. We’ll assign the PoseNetShader to this variable in the Unity Editor.\n\n\n\n\n\n\n\nCreate PreprocessImage() Method\nNext, we need to make a new method to handle the preprocessing steps for the videoTexture. We’ll name this method PreprocessImage and define it below the Update method. The method will return a Texture2D that contains the preprocessed image.\n\n\n\n\n\n\nCreate a New Texture2D\nWe don’t want to alter the videoTexture directly, so we’ll make a copy of the current frame. Create a new Texture2D called imageTexture and give it the same dimensions as the videoTexture. We can use the Graphics.CopyTexture() method to copy the data from the RenderTexture directly on the GPU.\n\n\n\n\n\n\n\nResize the Image\nNow that we have our imageTexture, we need to resize it to a more practical resolution. Lowering the resolution does decrease the model’s accuracy. Unfortunately, using a higher resolution can significantly impact inference speed. We’ll examine this trade-off in a later post.\nFor now, we’ll use a resolution of 360 x 360. Create two new public int variables for the image height and width respectively. This will make it easier to experiment with different resolutions.\n\n\n\n\n\nWe’ll make a new method to handle the resizing process. The method will take in a Texture2D as well as the new height and width. It will return a Texture2D with the new resolution.\n\n\n\n\n\nThe Graphics.CopyTexture() method requires that the source and destination textures be the same size. That means we need to destroy the current imageTexture and make a temporary one with the smaller dimensions.\n\n\n\n\n\nNote: Resizing the image to 360 x 360 will squish our input image from a 16:9 aspect ratio to a square aspect ratio. We’ll need to account for this when we get to the postprocessing section.\n\n\nApply Model-Specific Preprocessing\nThis is where we’ll make use of the PoseNetShader we made earlier. We’ll create a new method to handle the execution process. Name the new method PreprocessResNet to match the function in the PoseNetShader. They don’t need to have the same name. It’s just personal preference.\nFor this method, we need to use HDR texture formats for the RenderTexture and Texture2D. This allows us to feed images into the model with color values outside of the standard range of [0,1]. The Barracuda library remaps non-HDR color values to [0,1]. Given that we’re scaling the values by 255, this is undesirable.\nYou can view the full PreprocessResNet method below.\n\n\n\n\n\nThe PreprocessResNet method returns a Texture2D with an HDR texture format. The switch to HDR texture formats means the tempTex variable is no longer compatible. Fortunately, we can reuse the imageTexture variable that we emptied.\nThe finished PreprocessImage method looks like this.\n\n\n\n\n\n\n\nCall the Method\nWe’ll call PreprocessImage() in the Update() method so that it runs every frame."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html#create-the-pose-estimator",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html#create-the-pose-estimator",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "Create the Pose Estimator",
    "text": "Create the Pose Estimator\nTo run the PoseNet script, we need to attach it to a GameObject in the Unity Editor.\n\nCreate an Empty GameObject\nIn the Hierarchy tab, right-click an empty space and select Create Empty from the menu. Name the empty GameObject PoseEstimator.\n\n\n\n\n\n\n\nAttach the PoseNet Script\nWith the PoseEstimator object selected, drag and drop the PoseNet script into the Inspector tab.\n\n\n\n\n\n\n\nAssign the video_texture\nNext, we need to assign the video_texture asset to the Video Texture parameter. With the PoseEstimator object selected, drag and drop the video_texture asset into the Video Texture spot in the Inspector tab.\n\n\nAssign the PoseNetShader\nWe also need to drag and drop the PoseNetShader asset into the Posenet Shader spot in the the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-2/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 2 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe’re now ready to feed video frames to our PoseNet model. In part 3, we’ll cover how to install the Barracuda library and perform inference with our model.\n\nGitHub Repository - Version 1\n\n\nNext: Part 2.5(Optional) Part 3"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 30, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nExamining the preprocessed input images can be a useful step when debugging. We can see how much the source image gets squished when resized to a square aspect ratio. The model can have a hard time accurately determining key point locations if the individual gets squished too much. We can also get a visual sense of how the preprocessing operations change the pixel values."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#make-a-new-screen",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#make-a-new-screen",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Make a New Screen",
    "text": "Make a New Screen\nWe can make a second screen to see what the preprocessed images look like before they get fed into the model.\n\nMake a Quad\nIn the Hierarchy tab, make an new Quad and name it InputScreen.\n\nSet the Scale\nWe’ll set both the X and Y scale values to 360 to match our current input resolution for the model.\n\n\nSet the Position\nWe also need to set the X and Y position values to 180. Set the Z position value to -1 so that it’s in front of the VideoScreen.\n\n\n\n\n\n\n\n\nMake a RenderTexture\nMake another RenderTexture and name it input_texture.\n\nSet the Size\nSet the Size to 360 x 360.\n\n\n\n\n\n\n\nApply the input_texture\nDrag and drop the input_texture onto the InputScreen in the Scene tab.\n\n\n\nMake the Shader Unlit\nSet the Shader for the InputScreen to Unlit/Texture just like the VideoScreen."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#update-the-posenet-script",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#update-the-posenet-script",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Update the PoseNet Script",
    "text": "Update the PoseNet Script\nNext, we need to create a few new public variables in the PoseNet script.\n\nAdd displayInput variable\nCreate a new public bool variable called displayInput. This will add a checkbox in the Inpsector tab that we can use to turn the InputScreen on and off.\n\n\nAdd inputScreen variable\nCreate a new public GameObject variable called inputScreen. We need to access the InputScreen object to activate and deactivate it.\n\n\nAdd inputTexture variable\nCreate a new public RenderTexture variable called inputTexture. We’ll assign the input_texture asset to this variable in the Unity Editor.\n\n\n\nModify the Update() Method\nWe can use the Graphics.Blit() method to copy the processedImage data to the inputTexture variable. We’ll use the inputScreen.SetActive() method to activate and deactivate the InputScreen."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#assign-inspector-variables",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#assign-inspector-variables",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Assign Inspector Variables",
    "text": "Assign Inspector Variables\nWith the PoseEstimator selected in the Hierarchy tab, drag and drop the InputScreen and input_texture into their respective variables in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#test-the-new-screen",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#test-the-new-screen",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Test the New Screen",
    "text": "Test the New Screen\nMake sure the Display Input checkbox is ticked in the Inspector tab. It will be easier to see the changes to the preprocessed images if we use a full color video. We can set the Video Clip for the Video Player to the pexels_woman_dancing file that we downloaded in Part 1.\n\n\nVideo"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#rescale-pixel-values",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#rescale-pixel-values",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Rescale Pixel Values",
    "text": "Rescale Pixel Values\nThe current result may not be the most accurate representation as pixel values are still in the the range of [0, 255] instead of Unity’s [0.0, 1.0]. We can rescale the pixel values by adding another function to our PoseNetShader.\n\nUpdate PoseNetShader\nWe’ll create another function called ScaleInputImage() that divides the RGB channel values by 255.0.\n\nNote: Rescaling the pixel values will have a small impact on performance. Make sure to untick the Display Input checkbox when you’re not examining the input image.\n\n\nCreate ScaleInputImage Method\nWe need to create a new method in the PoseNet script to execute the function just like for the PreprocessResNet function.\n\n\n\nCall ScaleInputImage Method\nFinally, we’ll call the new method in the if (displayInput) statement.\n\n\n\nResult"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-2-5/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-2-5/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 2.5 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe now have a little screen that we can use to view the processed input image before it gets fed to the model.\n\nGitHub Repository - Version 1\n\n\nNext: Part 3"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 30, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nWe can now start using the Barracuda library. We’ll first install the Barracuda package for our project and import the PoseNet model. We’re going to modify the model a bit to make postprocessing easier. Finally, we’ll perform inference using a preprocessed image as input."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#install-barracuda-package",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#install-barracuda-package",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nSelect the Package Manager tab in the Unity editor.\n\n\n\n\n\nType Barracuda into the search box. The version of the package used in the tutorial is 1.0.4.\n\n\n\n\n\nClick the Install button to install the package.\n\n\n\n\n\nWait for Unity to install the dependencies."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#import-posenet-model",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#import-posenet-model",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Import PoseNet Model",
    "text": "Import PoseNet Model\nNow we can import the model into Unity. The Barracuda dev team has focused on supporting the ONNX format for models. We aren’t able to directly import models from TensorFlow or PyTorch. I’ve already converted the PoseNet model to ONNX. You can check out my tutorial for converting TensorFlow SavedModels to ONNX (here). PyTorch provides built-in support for ONNX (link).\n\nDownload the ONNX File\nYou can download the converted PoseNet model from the link below.\n\nResNet50: (download)\n\n\n\nImport Model to Assets\nCreate a new folder in the Assets window and name it Models. Drag and drop the ONNX file into the Models folder.\nIf you select the resnet50 asset, you should see the following in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#load-the-model",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#load-the-model",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Load the Model",
    "text": "Load the Model\nNext, we need to implement the code for loading the model in the PoseNet script.\n\nAdd Unity.Barracuda Namespace\nOpen the PoseNet script and add the Unity.Barracuda namespace at the top of the script.\n\n\n\n\n\n\n\nCreate modelAsset Variable\nMake a new public NNModel variable called modelAsset. We’ll assign the resnet50 asset to this variable in the Unity Editor.\n\n\n\n\n\n\n\nCreate workerType Variable\nWe’ll also add a variable that let’s us choose which backend to use when performing inference. The options are divided into CPU and GPU. I believe there are plans to add support for specialized hardware such as Neural Processing Units in the future. Our preprocessing pipeline runs entirely on the GPU so we’ll be sticking with the GPU options for this tutorial series.\nMake a new public WorkerFactory.Type called workerType. Give it a default value of WorkerFactory.Type.Auto.\n\n\n\n\n\n\n\nCreate m_RuntimeModel Variable\nWe need to compile the modelAsset into a run-time model to perform inference. We’ll store the compiled model in a new private Model variable called m_RuntimeModel. This is the naming convention used in the Barracuda documentation.\n\n\nCreate engine Variable\nNext, we’ll create a new private IWorker variable to store our inference engine. Name the variable engine.\n\n\n\n\n\n\n\nCreate heatmapLayer Variable\nAdd a new private string variable to store the name of the heatmap layer in the resnet50 model. We’ll need the output of this layer to determine the location of key points (e.g. nose, elbows, knees, etc.) in the input image. We can find the name for the model’s output layers in the Inspector tab. For our model, the heatmap layer is named float_heamap.\n\n\n\n\n\nNote: The last two output layers, resnet_v1_50/displacement_bwd_2/BiasAd and resnet_v1_50/displacement_fwd_2/BiasAd, are used when estimating the pose of multiple people. We’ll be sticking to single pose estimation for this series.\n\n\nCreate offsetsLayer Variable\nWe’ll go ahead and create a variable for the float_short_offsets layer as well since we’ll need it later. The output from this layer is used to refine the estimated key point locations determined with the heatmap layer.\n\n\n\n\n\n\n\nCompile the Model\nWe need to get an object oriented representation of the model before we can work with it. We’ll do this in the Start() method and store it in the m_RuntimeModel.\n\n\n\n\n\n\n\nModify the Model\nWe need to add a Sigmoid layer to the end of the model before creating our inference engine. This will map the output values to the range [0,1]. We’ll use these values to measure the model’s confidence that a given key point is in a given spot in the input image. A value of 1 would indicate that the model is 100% confident the key point is in that location. We won’t be getting any 1’s.\nFirst, we need to make a new private string variable to store the name of this new layer. We’ll name the variable predictionLayer and name the layer heatmap_predictions.\n\n\n\n\n\nWe’ll add the new layer using a ModelBuilder.\n\n\n\n\n\n\n\nInitialize the Inference Engine\nNow we can create a worker to execute the modified model using the selected backend. We’ll do this using the WorkerFactory.CreateWorker() method.\n\n\n\n\n\n\n\nRelease Inference Engine Resources\nWe need to manually release the resources that get allocated for the inference engine. This should be one of the last actions performed. Therefore, we’ll do it in the OnDisable() method. This method gets called when the Unity project exits. We need to implement this method in the PoseNet script."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#set-inspector-variables",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#set-inspector-variables",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Set Inspector Variables",
    "text": "Set Inspector Variables\nNow we just need to set the values for the Model Asset and select the inference backend.\n\nAssign the Model Asset\nWith the PoseEstimator object selected, drag and drop the resnet50 asset into the Model Asset variable.\n\n\nSelect Inference Backend\nSet the backend to the Compute Precompiled option in the Worker Type drop-down. This is the most efficient GPU backend."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#perform-inference",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#perform-inference",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Perform Inference",
    "text": "Perform Inference\nFinally, we’ll add the code to perform inference in the Update() method.\n\nCreate the input Tensor\nWe need to convert the processedImage to a Tensor before we can feed it to the model. The Tensor constructor requires us to specify the number of channels in the image. We don’t need the alpha (transparency) channel so we’ll specify 3 for the RGB color channels.\n\n\n\n\n\n\n\nExecute the Model\nWe’ll use the engine.Execute() method to perform inference. This method takes in the input Tensor and schedules the network execution.\n\n\n\n\n\n\n\nRelease Input Tensor Resources\nWe’ll need to manually release the allocated resources for the Tensor with the input.Dispose() method.\n\n\n\n\n\nHere is the revised Update() method."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-3/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-3/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 3 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe’ve finally performed inference using a PoseNet model in Unity. However, we need to process the output from the model to determine the estimated key point locations. We’ll cover how to perform the postprocessing operations in part 4.\n\nGitHub Repository - Version 1\n\n\nNext: Part 4"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 30, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nThe post processing phase consists of a few main steps. We need to first determine the region of the image that the model estimates is most likely to contain a given key point. We’ll then refine this estimate using the output from the offsetsLayer. Lastly, we’ll account for any changes in aspect ratio and scale the key point locations up to the source resolution.\nSo far, major operations have been performed on the GPU. We’ll be performing the post processing steps on the CPU. Tensor elements need to be accessed on the main thread. Just reading the values from the model’s output layers forces the rest of the program to wait until the operation completes. Even if we perform the post processing on the GPU, we would still need to access the result on the CPU. I’m working on a way to avoid reading the values on the CPU. Unfortunately, it’s still too messy to include in this tutorial."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html#create-processoutput-method",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html#create-processoutput-method",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "Create ProcessOutput() Method",
    "text": "Create ProcessOutput() Method\nThe post processing steps will be handled in a new method called ProcessOutput(). The method will take in the output Tensors from the predictionLayer and the offsetsLayer.\n\n\n\n\n\nBefore filling out the function, we need to create a new constant and a new variable.\n\nCreate numKeypoints Constant\nThe PoseNet model estimates the 2D locations of 17 key points on a human body.\n\n\n\nIndex\nName\n\n\n\n\n0\nNose\n\n\n1\nLeft Eye\n\n\n2\nRight Eye\n\n\n3\nLeft Ear\n\n\n4\nRight Ear\n\n\n5\nLeft Shoulder\n\n\n6\nRight Shoulder\n\n\n7\nLeft Elbow\n\n\n8\nRight Elbow\n\n\n9\nLeft Wrist\n\n\n10\nRight Wrist\n\n\n11\nLeft Hip\n\n\n12\nRight Hip\n\n\n13\nLeft Knee\n\n\n14\nRight Knee\n\n\n15\nLeft Ankle\n\n\n16\nRight Ankle\n\n\n\nSince the number of key points never changes, we’ll store it in an int constant. Name the constant numKeypoints and set the value to 17.\n\n\n\n\n\n\n\nCreate keypointLocations Variable\nThe processed output from the model will be stored in a new variable called keypointLocations. This variable will contain the (x,y) coordinates for each key point. For this tutorial, the coordinates will be scaled to the original resolution of 1920x1080 for videoTexture.\nThis variable will also store the confidence values associated with the coordinates. The model predicts key point locations even when there isn’t a human in the input image. In such situations, the confidence values will likely be quite low. We can decide how to handle the latest coordinates based on a confidence threshold that we pick.\nThere are many ways we can store this information. For simplicity, we’ll stick with an array of arrays. The array will have 17 elements. Each element will contain the location information for the key point that matches their index.\n\n\n\n\n\n\n\nRetrieve Output Tenors\nCall ProcessOutput() after engine.Execute(input) in the Update() method. We’ll use the engine.PeekOutput() method to get a reference to the output Tensors from the model. Since they are just references, we don’t need to manually dispose of them.\n\n\n\n\n\nNow we can start filling out the ProcessOutput() method."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html#calculate-scaling-values",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html#calculate-scaling-values",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "Calculate Scaling Values",
    "text": "Calculate Scaling Values\nThe heatmaps generated by the model are much smaller than the input image fed into it. We’ll need to make some calculations to accurately scale the key point locations back up to the source resolution.\n\nCalculate Model Stride\nThe heatmap dimensions are dependent on both the size of the input image and a fixed integer value called the stride. The stride determines how much smaller the heatmaps will be than the input image. The model used in this tutorial has a stride of 32. The heatmap dimensions are equal to the ceiling of resolution/stride. With our default input resolution of 360 x 360, the size of the heatmaps are 12 x 12.\nSince we know the stride for this model, we could make it a constant value. However, calculating it is an easy way to make sure. This also makes it less of a hassle when switching between models with different stride values.\n\nModel with a Different Stride Value\n\nResNet50 Stride 16: (download)\n\nTo get the stride value, we’ll select a dimension of inputImage and subtract 1. We then divide that value by the same dimension of the heatmap with 1 subtracted as well. If we don’t subtract 1, we’ll undershoot the stride value.\nFor most input resolutions this will yield a value that is slightly above the actual stride. If we left it there, the key point locations would be offset from the videoTexture. To compensate, we’ll subtract the remainder of the calculated stride divided by 8. The stride for the PoseNet models provided in this tutorial series are all multiples of 8.\n\n\n\n\n\n\n\n\nCalculate Image Scale\nAfter scaling the output back to the inputImage resolution, we’ll need to scale the output up to the source resolution. We can use the dimensions of videoTexture to calculate this scale.\n\n\n\n\n\n\n\nCalculate Aspect Ratio Scale\nAs I noted in Part 2, we need to compensate for the change in aspect ratio that results from resizing the image. We can use the dimensions of the videoTexture to stretch the output to the original aspect ratio."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html#iterate-through-heatmaps",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html#iterate-through-heatmaps",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "Iterate Through Heatmaps",
    "text": "Iterate Through Heatmaps\nNow we can iterate through each of the heatmaps and determine the location of the associated key points.\n\n\n\n\n\n\nLocate Key Point Indices\nFor each heatmap, we’ll first need to locate the index with the highest confidence value. This indicates what region of the image the model thinks is most likely to contain that key point. We’ll create a separate method to handle this.\nThe new method will be called LocateKeyPointIndex() and take in the heatmaps and offsets tensors along with the current keypointIndex. It will return a Tuple containing the (x,y) coordinates from the heatmap index, the associated offset vector, and the confidence value at the heatmap index.\n\n\n\n\n\n\nCall the Method\nWe’ll call LocateKeyPointIndex() at the start of each iteration through the for loop in ProcessOutput().\n\n\n\n\n\n\n\n\nCalculate Key Point Positions\nNow we can calculate the estimated key point locations relative to the source videoTexture. We’ll first extract the output from the Tuple returned by LocateKeyPointIndex(). The offset vectors are based on the inputImage resolution so we’ll scale the (x,y) coordinates by the stride before adding them. We’ll then scale the coordinates up to the source videoTexture.\nOnly the x-axis position is scaled by the unsqueezeValue. This is specific to our current videoTexture aspect ratio. I will cover a more dynamic approach in a later post.\n\n\n\n\n\n\nStore Key Point Positions\nFinally, we’ll store the location data for the current key point at the corresponding index in the keypointLocations array."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-4/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-4/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 4 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe finally have the estimated key point locations relative to the source video. However, we still don’t have an easy means to gauge the model’s accuracy. In the next post, we’ll map each key point location to a GameObject. This will provide a quick way to determine if the model is outputting nonsense as well as what scenarios the model struggles with.\n\nGitHub Repository - Version 1\n\n\nNext: Part 5"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-5/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-5/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 5 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 30, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-5/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-5/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 5 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we’ll make use of the VideoScreen we made in Part 1 by checking if the estimated key point locations align with the actual locations in the video. We’ll know everything is working as intended if the objects get placed in front of the target key points. If not, we’ll at least have a visual debugging tool to work out where we things went wrong."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-5/index.html#create-key-points",
    "href": "posts/barracuda-posenet-tutorial/part-5/index.html#create-key-points",
    "title": "Barracuda PoseNet Tutorial Pt. 5 (Outdated)",
    "section": "Create Key Points",
    "text": "Create Key Points\nWe need to create a separate GameObject for each of the 17 key points.\n\nCreate Container\nIn the Hierarchy tab, create an empty GameObject and name it Key Points. We’ll store the key point objects in here to keep things organized.\nOptional: With Key Points selected, right-click the Transform component in the Inspector tab. Click Reset in the pop-up menu. This will reset the object’s position to the origin.\n\n\n\n\n\n\n\nCreate GameObjects\nRight-click the Key Points object and select Sphere under 3D Object. This will create a nested GameObject inside Key Points.\n\n\n\n\n\nSelect the new Sphere object and press Ctrl-d to duplicate it. We’ll need 17 spheres total.\nRename the Sphere objects according to the table below.\n\n\n\nIndex\nName\n\n\n\n\n0\nNose\n\n\n1\nLeft Eye\n\n\n2\nRight Eye\n\n\n3\nLeft Ear\n\n\n4\nRight Ear\n\n\n5\nLeft Shoulder\n\n\n6\nRight Shoulder\n\n\n7\nLeft Elbow\n\n\n8\nRight Elbow\n\n\n9\nLeft Wrist\n\n\n10\nRight Wrist\n\n\n11\nLeft Hip\n\n\n12\nRight Hip\n\n\n13\nLeft Knee\n\n\n14\nRight Knee\n\n\n15\nLeft Ankle\n\n\n16\nRight Ankle\n\n\n\n\nResult\n\n\n\n\n\n\n\n\nResize GameObjects\nNext, we’ll make the key point objects larger so that they’re easier to see. Select the Nose object in the Hierachy. Then, hold Shift and click RightAnkle to select all 17 objects at once.\n\n\n\n\n\nWe need to increase the X and Y values for the Scale parameter in the Inspector tab. Increasing them to 10 should be enough.\n\n\n\n\n\n\n\nChange GameObject Material\nThe default color for a GameObject doesn’t stand out much against the background. We’ll make the key point objects yellow since it’s apparently really easy for humans to spot.\n\nCreate Yellow Material\nOpen the Materials folder in the Assets window. Right-click an empty space and select Material in the the Create sub-menu. Name the new material Yellow.\n\n\n\n\n\n\n\nChange Material Color\nWith the Yellow material selected, click the small white box in the Inspector tab. A Color window should pop up.\n\nSet the value for B to 0 in the Color window. This will change the color to pure yellow.\n\n\n\n\n\n\n\nMake Material Unlit\nWe’ll change the Shader for the material to Unlit/Color.\n\n\n\n\n\n\n\nAssign Yellow Material\nSelect all the key point objects in the Hierarchy tab. Then, drag and drop the Yellow material into the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-5/index.html#map-key-point-locations",
    "href": "posts/barracuda-posenet-tutorial/part-5/index.html#map-key-point-locations",
    "title": "Barracuda PoseNet Tutorial Pt. 5 (Outdated)",
    "section": "Map Key Point Locations",
    "text": "Map Key Point Locations\nNow we can update the positions of the key point objects using the location data obtained from the PoseNet model. Ordinarily, we would implement this in a separate C# script. This script would access the keypointLocations[][] array in the PoseNet script. However, we’ll do it in the PoseNet script to keep things simple.\n\nCreate keypoints Variable\nOpen the PoseNet script and add a public GameObject array. Name the variable keypoints.\n\n\n\nAssign the Key Point Objects\nSelect the PoseEstimator object in the Hierarchy tab. Then, click the small lock icon above the Inspector tab. This will lock the current selected object in the Inspector tab.\n\nMake sure the Size value for the Keypoints variable is set to 0.\n\nSelect all the key point objects in the Hierarchy. Then, drag and drop them onto the Keypoints parameter in the Inspector tab.\n\n\n\n\n\nGo ahead and unlock the Inspector tab by clicking the lock icon again.\n\n\nCreate minConfidence Variable\nNext, we’ll add a public int variable. This variable will define the confidence threshold for deciding whether or not to display a given key point object. Name the variable minConfidence and set the default value to 70. You can add a Range attribute to create a slider in the Inspector tab. Set the range to [0, 100].\n\n\n\nCreate UpdateKeyPointPositions() Method\nWe need to define a new method to update the key point positions. Name the method UpdateKeyPointPositions().\n\n\n\nCall the Method\nWe’ll call the method in Update() just after ProcessOutput()."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-5/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-5/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 5 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe now have a useful debugging tool to gauge our model’s performance. In the next post, we’ll create a complete pose skeleton by drawing lines connecting the key point objects we made in this post.\n\nGitHub Repository - Version 1\n\n\nNext: Part 6"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Nov 30, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we’ll add some connecting lines between the key points to create a simple skeleton. This will improve visibility over the key point objects alone."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html#create-drawskeleton-script",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html#create-drawskeleton-script",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "Create DrawSkeleton Script",
    "text": "Create DrawSkeleton Script\nWe’ll complete our pose skeleton by drawing lines connecting the appropriate key points. Create a new C# script and name it DrawSkeleton.\n\n\n\n\n\n\nCreate keypoints Variable\nWe need to access the key point objects so make another public GameObject array just like in the PoseNet script.\n\n\n\nCreate lines Variable\nNext, create a private GameObject array to hold the lines themselves. Name the variable lines.\n\n\n\nCreate lineRenderers Variable\nWe’ll use LineRenderer components to draw the skeleton.\n\n\n\nCreate jointPairs Variable\nThe next variable will contain pairs of key point indices. The corresponding key points indicate the start and end points for the skeleton lines.\n\n\nJoint Pairs\n\n\n\n#\nStarting Key Point\nEnding Key Point\n\n\n\n\n1\nNose\nLeft Eye\n\n\n2\nNose\nRight Eye\n\n\n3\nLeft Eye\nLeft Ear\n\n\n4\nRight Eye\nRight Ear\n\n\n5\nLeft Shoulder\nRight Shoulder\n\n\n6\nLeft Shoulder\nLeft Hip\n\n\n7\nRight Shoulder\nRight Hip\n\n\n8\nLeft Shoulder\nRight Hip\n\n\n9\nRight Shoulder\nLeft Hip\n\n\n10\nLeft Hip\nRight Hip\n\n\n11\nLeft Shoulder\nLeft Elbow\n\n\n12\nLeft Elbow\nLeft Wrist\n\n\n13\nRight Shoulder\nRight Elbow\n\n\n14\nRight Elbow\nRight Wrist\n\n\n15\nLeft Hip\nLeft Knee\n\n\n16\nLeft Knee\nLeft Ankle\n\n\n17\nRight Hip\nRight Knee\n\n\n18\nRight Knee\nRight Ankle\n\n\n\n\n\n\nCreate lineWidth Variable\nThe last variable we’ll make defines the line width.\n\n\n\nInitialize Variables\nWe need to initialize the lines, lineRenderers, and jointPairs variables in the Start() method.\n\n\n\nCreate InitializeLine() Method\nWe’ll create a new method to set up each of the lines in the pose skeleton. The method will create an empty GameObject for a line and add a LineRenderer component to it. We won’t set the start and end positions as none of the key points will have updated yet.\n\n\n\nCreate InitializeSkeleton() Method\nNext, we need to call InitializeLine() in a new method for each line in the pose skeleton. We’ll give each region of the skeleton a different color.\n\n\nCall the method\nWe’ll initialize the pose skeleton lines in the Start() method.\n\n\n\n\nCreate RenderSkeleton() Method\nThe last method we need to define will handle updating the position of the each of the lines in the pose skeleton. The method will iterate through each of the joint pairs and update the start and end positions for the associated LineRenderer. We’ll only display a given line if both of the key point objects are currently active.\n\n\nCall the method\nWe’ll render the skeleton lines in the LateUpdate() method instead of Update(). This will ensure the PoseNet model has run for the latest frame before updating the pose skeleton."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html#create-skeletonrenderer",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html#create-skeletonrenderer",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "Create SkeletonRenderer",
    "text": "Create SkeletonRenderer\nWe’ll attach the DrawSkeleton script to a new GameObject. Create an empty GameObject in the Hierarchy tab and name it SkeletonRenderer.\n\n\n\n\n\n\nAttach the DrawSkeleton Script\nWith SkeletonRenderer selected in the Hierarchy, drag and drop the DrawSkeleton script into the Inspector tab.\n\n\n\n\n\n\n\nAssign Key Points\nDrag and drop the key point objects onto the Keypoints parameter just like with the PoseNet script."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html#try-it-out",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html#try-it-out",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "Try It Out",
    "text": "Try It Out\nIf you press the play button, you should see something like this.\n\n\n\nVariable\nValue\n\n\n\n\nimageHeight\n720\n\n\nimageWidth\n720\n\n\nminConfidence\n85\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-6/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-6/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 6 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe now have a complete pose skeleton that we can use for debugging or demos. In the next post, we’ll add the option to use a live webcam feed as input instead of a prerecorded video.\n\nGitHub Repository - Version 1\n\n\nNext: Part 7"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-7/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-7/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 7 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Dec 1, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-7/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-7/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 7 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nIt’s time to see how our model performs with a webcam. Prerecorded videos are great for testing, but most real-world applications will likely use a live video feed. We’ll set up our video feed for a front-facing camera that mirrors the user."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-7/index.html#modify-posenet-script",
    "href": "posts/barracuda-posenet-tutorial/part-7/index.html#modify-posenet-script",
    "title": "Barracuda PoseNet Tutorial Pt. 7 (Outdated)",
    "section": "Modify PoseNet Script",
    "text": "Modify PoseNet Script\nWe can add the option to use a webcam feed by making some modifications to the PoseNet script.\n\nCreate useWebcam Variable\nOpen the PoseNet script and create a new public bool variable. Name the variable useWebcam and set the default value to false. This will create a checkbox in the Inspector tab that we can use to enable and disable the webcam.\n\n\n\n\n\n\n\nCreate webcamTexture Variable\nWe’ll use a WebCamTexture variable to store the live video input from our webcam. Name the variable webcamTexture.\n\n\n\n\n\n\n\nSet Up Webcam Feed\nWe’ll prepare the webcam feed at the top of the Start() method. You can find the completed code below.\n\nInitialize the webcamTexture\nFirst, initialize the webcamTexture. We’ll use the first video input device Unity finds. If you have more than one webcam attached, you’ll need to specify the device name.\n\n\nFlip the VideoScreen\nNext, we need to adjust the rotation and scale of the VideoScreen object. The webcam feed doesn’t mirror the user by default. For example, the user’s right arm appears on the left side of the screen. This can be disorienting when looking at the generated pose skeleton. We’ll flip the VideoScreen to compensate.\n\n\nStart the Camera\nWe’ll use the webcamTexture.Play() method to start the camera.\n\n\nDeactivate the Video Player\nFinally, we’ll deactivate the Video Player as it’s not being used.\n\n\nCompleted Code\n\n\n\n\n\n\n\n\nGet webcamTexture Data\nWe’ll use the Graphics.Blit() method to update the videoTexture with the data from webcamTexture. Add the following code at the top of the Update() method.\n\n\n\n\n\n\n\nFlip Key Point Locations\nFlipping the VideoScreen does not flip the videoTexture itself. Therefore, the output of the model will not be flipped either. We can fix this by mirroring the xPos values for the calculated key point locations."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-7/index.html#set-inspector-variable",
    "href": "posts/barracuda-posenet-tutorial/part-7/index.html#set-inspector-variable",
    "title": "Barracuda PoseNet Tutorial Pt. 7 (Outdated)",
    "section": "Set Inspector Variable",
    "text": "Set Inspector Variable\nNow we can enable and disable the webcam from the Inspector tab.\n\n\n\n\n\nNote: Don’t toggle the useWebcam parameter during runtime with the code as it is."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-7/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-7/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 7 (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe can now perform pose estimation using either prerecorded or live video feeds. We’ll further increase our flexibility for input sources in the next post by adding the ability to handle input with different aspect ratios.\n\nGitHub Repository - Version 1\n\n\nNext: Part 8"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "",
    "text": "Version 2: Part 1\nLast Updated: Dec 1, 2020"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nSo far, we’ve only worked with video sources that have 16:9 aspect ratios. You might need to work with input that has a taller or wider aspect ratio depending on your application and equipment. In this post, we’ll cover how to adjust to the current source resolution at runtime."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#add-unityengine.video-namespace",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#add-unityengine.video-namespace",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Add UnityEngine.Video Namespace",
    "text": "Add UnityEngine.Video Namespace\nOpen the PoseNet script and add a new using directive at the top. We need the UnityEngine.Video namespace to work with the Video Player object."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#add-webcam-variables",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#add-webcam-variables",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Add Webcam Variables",
    "text": "Add Webcam Variables\nUnity defaults to a resolution of 640 x 480 for webcams. There is no built-in method that returns the frame rate. However, the default does not appear to be over 30fps. We can request a resolution and frame rate when initializing the webcamTexture. Unity should accept the requested settings as long as the camera supports them. Unity defaults to its closest preset if the requested settings are not supported.\n\nCreate webcamHeight Variable\nAdd a new public int variable so we can adjust the camera height from the Inspector tab. Name the variable webcamHeight. My webcam supports 720p at 60fps, so I’ve set the default value to 720.\n\n\n\n\n\n\n\nCreate webcamWidth Variable\nNext, create a variable for the camera’s width and name it webcamWidth. I’ve set the default value to 1280.\n\n\n\n\n\n\n\nCreate webcamFPS Variable\nWe’ll also add a variable to set the frame rate for the camera and name it webcamFPS. Set the default value to 60."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#add-video-resolution-variables",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#add-video-resolution-variables",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Add Video Resolution Variables",
    "text": "Add Video Resolution Variables\nNext, we need to create a couple of private int variables to store the dimensions of the video source. Name the variables videoHeight and videoWidth."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#update-start-method",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#update-start-method",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Update Start() Method",
    "text": "Update Start() Method\nWe’ll make some modifications to the Start() method to prepare for input with different aspect ratios.\n\nGet Reference to Video Player\nCreate a new GameObject variable to access the Video Player object. Name the variable videoPlayer and place it at the top of the Start() method. We’ll also move the Transform videoScreen variable outside of the if (useWebcam) statement.\n\n\n\n\n\n\n\nGet Webcam Resolution\nWe can’t access the webcam resolution until after the webcam has started. Therefore, we’ll update the values for videoHeight and videoWidth after webcamTexture.Play() has been called.\n\n\n\n\n\n\n\nGet Video Clip Dimensions\nWe need to get the video resolution from the Video Player object when we’re not using a webcam. We can get the height and width by accessing the VideoPlayer component.\n\n\n\n\n\n\n\nReplace videoTexture\nThe dimensions for videoTexture can’t be updated once it’s been created. We’ll need to replace it with a new RenderTexture that uses the new videoHeight and videoWidth values. We also need to set the new videoTexture as the targetTexture for the Video Player object.\n\n\n\n\n\n\n\nUpdate VideoScreen\nNext, we need to apply the new videoTexture to the VideoScreen object. We’ll also resize and reposition the VideoScreen to account for any changes in the videoTexture dimensions.\n\n\n\n\n\n\n\nAdjust Main Camera\nWe’ll also resize and reposition the Main Camera object to account for changes to the VideoScreen."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#update-processoutput-method",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#update-processoutput-method",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Update ProcessOutput() Method",
    "text": "Update ProcessOutput() Method\nFinally, we need to modify how we calculate the key point locations.\n\nUpdate Scaling Calculations\nWe’ll account for different aspect ratios by first determining whether the height or width of videoTexture is larger. We then use that information to calculate the scale and unsqueezeScale values.\n\n\n\n\n\n\n\nUpdate Key Point Calculations\nWe should only scale the xPos value by the unsqueezeScale if the width larger than the height. Remove that part from the initial xPos calculation. We also need to move the if (useWebcam) statement.\n\n\n\n\n\nWe’ll scale either xPos or yPos by unsqueezeScale depending on whether the width or height value for videoTexture is larger. Place the if (useWebcam) statement after this calculation."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#try-it-out",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#try-it-out",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Try It Out",
    "text": "Try It Out\nNow we can test the modifications on some vertical videos from Pexels.\n\nWoman Doing a Jump Rope Exercise\nMan Dancing Hip-Hop\n\n\n\nVideo"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial/part-8/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial/part-8/index.html#summary",
    "title": "Barracuda PoseNet Tutorial Pt. 8 (Outdated)",
    "section": "Summary",
    "text": "Summary\nNow we can use video sources with different aspect ratios without needing to manually change any parameters.\n\nGitHub Repository - Version 1"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nPrerequisites\nCreate a New Project\nInstall Barracuda Package\nImport Video Files\nImport ONNX Models\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#introduction",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#introduction",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library. We will be using a pretrained PoseNet model to estimate the 2D locations of key points on the bodies of one or more individuals in a video frame. We will then use the output from the model to control the locations of GameObjects in a scene.\n\nSingle Pose Demo\n\n\n\n\nMulti-Pose Demo"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Overview",
    "text": "Overview\nThis post covers the process for installing the Barracuda package as well as importing the required video files and PoseNet models into the project."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#prerequisites",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#prerequisites",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe following prerequisites are required to complete this tutorial.\n\nUnity\nThis tutorial assumes that Unity is already installed on the system. We will be using Unity 2020, and the exact version can be downloaded from the link below.\n\nUnity 2020.3.14\n\nAnyone who has never used Unity before can become acquainted with the basics by following the tutorial below. It will walk through the installation process all the way to making an Angry Birds clone.\n\nHow to Make a Game - Unity Beginner Tutorial\n\n\n\nHardware\nThere appears to be a known issue with playing videos in Unity on AMD GPUs. Therefore, an Intel or Nvidia GPU is recommended. However, webcams seem to work fine on AMD GPUs."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#create-a-new-project",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#create-a-new-project",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Create a New Project",
    "text": "Create a New Project\nFirst, we need to create a new Unity project. We can use the default 3D template.\n\nNote: There appears to currently be an issues with the 2D template where Barracuda does not work when the project is built."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#install-barracuda-package",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#install-barracuda-package",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nWe will start by installing the Barracuda package. This will allow us to import the PoseNet models into the project. Open the Window menu at the top of the Unity Editor and select Package Manager.\n\n\n\n\n\nThere might be a message in the console indicating that there is a new version of the Visual Studio Editor package.\n\n\n\n\n\nFeel free to update the package by selecting it in the Package Manager and clicking the Update button.\n\n\n\n\n\nWe will be using version 2.1.0 of the Barracuda package. Unity has this version marked as preview, so we will need to enable preview packages to install it. Click the small gear icon and select the Advanced Project Settings option.\n\n\n\n\n\nTick the Enable Preview Packages checkbox so that we can install the latest version of Barracuda.\n\n\n\n\n\nA popup window will appear, warning us that preview packages might not be ready for production. However, the latest version of Barracuda contains bug fixes that are not present in the Verified version, so click I understand in the popup window.\n\n\n\n\n\nEven though there is a verified version of Barracuda, it is not available in the package manager by default. We need to either install a package that has it as a dependency (e.g. ML Agents) or add it directly with a git URL. Click on the + icon in the upper-left corner and select Add package from git URL....\n\n\n\n\n\nEnter com.unity.barracuda into the search box and click Add. This will install the latest Verified version of the package. Unfortunately, there is a bug with this version that causes an error when performing inference on the CPU. This is resolved in later versions.\n\n\n\n\n\n\nNote: The version of Barracuda that we will be using in this tutorial is not available through the package manager in Unity 2021 at the time of writing. You will need to manually update the value for \"com.unity.barracuda\" in the Project_Folder/Packages/manifest.json file from \"1.0.4\" to \"2.1.0-preview\" as shown below. The package will be marked as Experimental in the editor.\n\n\"dependencies\": {\n    \"com.unity.barracuda\": \"2.1.0-preview\",\n    \"com.unity.collab-proxy\": \"1.5.7\",\n    \"com.unity.ide.rider\": \"2.0.7\",\n    \"com.unity.ide.visualstudio\": \"2.0.11\",\n    \"com.unity.ide.vscode\": \"1.2.3\",\nWe can view more recent versions of the package by clicking See other versions.\n\n\n\n\n\nScroll all the way up to version 2.1.0-preview and click the Update to 2.1.0-preview button in the bottom-right corner.\n\n\n\n\n\nDuring the installation process a popup window will appear indicating that the version of the Burst compiler has changed. Click OK to close the window. Once the installation process has finished, close Unity and then reopen the project.\n\n\n\n\n\nUnity seems to be concerned that anyone who jumps through the multiple hoops to install a preview package might forget that they are indeed using a preview package. To eliminate this possibility, they have added a reminder at the top of the editor that can not be permanently removed."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#import-video-files",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#import-video-files",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Import Video Files",
    "text": "Import Video Files\nWe will be using these two videos available on Pexels, a free stock photos & videos site. The first one is for testing single pose estimation and only has one person in frame at a time. The second video is meant for testing multipose estimation and has several individuals in frame at varying distances from the camera. Download the videos in Full HD resolution.\n\nTwo Young Men Doing a Boardslide Over a Railing\n\nNote: Renamed to pexels_boardslides\n\nTeens Riding Skateboard Doing Grind Rail\n\nNote: Renamed to pexels_teens_riding_skateboard_doing_grind_rail\n\n\n\nAdd Files to Assets\nIn the Assets section, right-click an empty space, select the Create option, and click Folder. Name the folder Videos. Double-click the Videos folder to open it.\n\n\n\n\n\nDrag and drop the two video files from the File Explorer into the Videos folder."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#import-onnx-models",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#import-onnx-models",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Import ONNX Models",
    "text": "Import ONNX Models\nWe will cover how to use two different versions of the PoseNet model. The MobileNet version is optimized to run efficiently on CPUs at the cost of some accuracy. The ResNet50 version is noticeably more accurate, but is more computationally demanding.\n\nDownload Files\nThe model files used in this tutorial series can be downloaded from the links below.\n\nMobileNet: (download)\nResNet50: (download)\n\n\n\nAdd Files to Assets\nBack in the Assets section, create a new folder called Models. Drag and drop the ONNX files from the File Explorer into the Models folder."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-1/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1",
    "section": "Summary",
    "text": "Summary\nThat takes care of the preliminary setup for the project. The next post will cover how to play and view videos inside Unity from both video files and a webcam.\nNext: Part 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "",
    "text": "Overview\nCreate the Video Player\nCreate PoseEstimator Script\nTest it Out\nSummary\nUpdate 7/6/2022: Fixed a code discrepancy between the blog post and the GitHub repository."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Overview",
    "text": "Overview\nThis post demonstrates how to play and view videos inside Unity from both video files and a webcam. We’ll later perform pose estimation on individual frames while the video is playing. We can gauge the model’s accuracy by comparing the estimated key point locations to the source video."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-the-video-player",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-the-video-player",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Create the Video Player",
    "text": "Create the Video Player\nTo start, we will create a new GameObject to play and view a video feed.\n\nCreate the Video Screen\nWe will use a Quad object for the screen. Right-click an empty space in the Hierarchy tab. Select the 3D Object section and click Quad. We can just name it VideoScreen.\n\n\n\n\n\nSince we are only working in 2D, we can switch the scene to 2D view by clicking the 2D button in the scene tab.\n\n\n\n\n\nThis will remove perspective from the scene view and align it with the VideoScreen.\n\n\n\n\n\nWe will be updating the VideoScreen dimensions in code based on the resolution of the video or webcam feed.\n\n\nAdd Video Player Component\nUnity has a Video Player component that provides the functionality to attach video files to the VideoScreen. With the VideoScreen object selected in the Hierarchy tab, click the Add Component button in the Inspector tab.\n\n\n\n\n\nType video into the search box and select Video Player from the search results.\n\n\n\n\n\n\n\nAssign Video Clip\nVideo files can be assigned by dragging them from the Assets section into the Video Clip spot in the Inspector tab. We will start with the pexels_boardslides file.\n\n\n\n\n\n\n\nMake the Video Loop\nTick the Loop checkbox in the Inspector tab to make the video repeat when the project is running."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Create PoseEstimator Script",
    "text": "Create PoseEstimator Script\nWe will be adjusting both the VideoScreen and Main Camera objects in the script where the PoseNet model will be executed.\nCreate a new folder in the Assets section and name it Scripts. Enter the Scripts folder and right-click an empty space. Select C# Script in the Create submenu and name it PoseEstimator.\n\n\n\n\n\nDouble-click the new script to open it in the code editor.\n\n\n\n\n\n\nAdd Required Namespace\nWe first need to add the UnityEngine.Video namespace to access the functionality for the Video Player component. Add the line using UnityEngine.Video; at the top of the script.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Video;\n\n\nDefine Public Variables\nWe can specify a desired resolution and framerate for webcams in Unity. If the provided resolution and framerate is not supported by the hardware, Unity will use a default resolution.\nWe will specify the desired webcam resolution using a public Vector2Int variable called webcamDims. Set the default values to 1280x720.\nNext, create a public int variable called webcamFPS and give it a default value of 60.\nWe will use a public bool variable to toggle between using a video file or webcam as input for the model. Set the default value to false as we will be starting with a video file.\nLastly, create a public Transform variable called videoScreen. We will use this variable to access the VideoScreen object and its Video Player component.\npublic class PoseEstimator : MonoBehaviour\n{\n    [Tooltip(\"The requested webcam dimensions\")]\n    public Vector2Int webcamDims = new Vector2Int(1280, 720);\n\n    [Tooltip(\"The requested webcam frame rate\")]\n    public int webcamFPS = 60;\n\n    [Tooltip(\"Use webcam feed as input\")]\n    public bool useWebcam = false;\n\n    [Tooltip(\"The screen for viewing preprocessed images\")]\n    public Transform videoScreen;\n\n\nDefine Private Variables\nWe need a private WebCamTexture variable to access the video feed from a webcam.\nWe will store the final dimensions from either the video or webcam feed in a private Vector2Int variable called videoDims.\nThe last variable we need is a private RenderTexture variable called videoTexture. This will store the pixel data for the current video or webcam frame.\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n\n// The dimensions of the current video source\nprivate Vector2Int videoDims;\n\n// The source video texture\nprivate RenderTexture videoTexture;\n\n\nCreate InitializeVideoScreen() Method\nWe will update the position, orientation, and size of the VideoScreen object in a new method called InitializeVideoScreen. The method will take in width and height value along with a bool to indicate whether to mirror the screen. When using a webcam, we need to mirror the VideoScreen object so that the user’s position is mirrored on screen (e.g. their right side is on the right side of the screen).\nFirst, we will set the video player component to render to a RenderTexture and set videoTexture as the target texture.\nWhen mirrorScreen is set to true the VideoScreen will be rotated 180 around the Y-Axis and scaled by -1 along the Z-Axis.\nThe default shader assigned to the VideoScreen object needs to be replaced with an Unlit/Texture shader. This will remove the need for the screen to be lit by an in-game light.\n\nImportant: By default, the Unlit/Texture shader is not included in project builds. We need to manually include it in the project settings\nOpen the Edit menu in the Unity Editor and select Project Settings\n\nIn the Project Settings window, select the Graphics submenu and scroll down to the Always Included Shaders section. Update the Size value to add an extra Element spot.\n\n\n\n\n\nSelect the new bottom shader spot.\n\n\n\n\n\nType Unlit/Texture shader into the Select Shader window and select Unlit/Texture from the available options. We can then close the Select Shader window.\n\n\n\n\n\nWe will also need the Unlit/Color shader later in this series so repeat these steps to add it as well.\n\n\n\n\n\n\nWe will then assign the videoTexture created earlier as the texture for the VideoScreen. This will allow us to access to pixel data for the current video frame.\nWe can adjust the dimensions of the VideoScreen object by updating it’s localScale attribute.\nThe last step is to reposition the screen based on the the new dimensions, so that the bottom left corner is at X:0, Y:0, Z:0. This will simplify the process for updating the positions of objects with the estimated key point locations.\n/// &lt;summary&gt;\n/// Prepares the videoScreen GameObject to display the chosen video source.\n/// &lt;/summary&gt;\n/// &lt;param name=\"width\"&gt;&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;&lt;/param&gt;\nprivate void InitializeVideoScreen(int width, int height, bool mirrorScreen)\n{\n    // Set the render mode for the video player\n    videoScreen.GetComponent&lt;VideoPlayer&gt;().renderMode = VideoRenderMode.RenderTexture;\n\n    // Use new videoTexture for Video Player\n    videoScreen.GetComponent&lt;VideoPlayer&gt;().targetTexture = videoTexture;\n    \n    if (mirrorScreen)\n    {\n        // Flip the VideoScreen around the Y-Axis\n        videoScreen.rotation = Quaternion.Euler(0, 180, 0);\n        // Invert the scale value for the Z-Axis\n        videoScreen.localScale = new Vector3(videoScreen.localScale.x, videoScreen.localScale.y, -1f);\n    }\n\n    // Apply the new videoTexture to the VideoScreen Gameobject\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.shader = Shader.Find(\"Unlit/Texture\");\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.SetTexture(\"_MainTex\", videoTexture);\n    // Adjust the VideoScreen dimensions for the new videoTexture\n    videoScreen.localScale = new Vector3(width, height, videoScreen.localScale.z);\n    // Adjust the VideoScreen position for the new videoTexture\n    videoScreen.position = new Vector3(width / 2, height / 2, 1);\n}\n\n\nCreate InitializeCamera() Method\nOnce the VideoScreen has been updated, we need to resize and reposition the in-game camera. We will do so in a new method called InitializeCamera.\nWe can access the Main Camera object with GameObject.Find(\"Main Camera\"). We will set the X and Y coordinates to the same as the VideoScreen position.\nThe camera also needs to be set to orthographic mode to remove perspective.\nLastly, we need to update the size of the camera. The orthographicSize attribute is actually the half size, so we need to divide videoDims.y (i.e. the height) by 2 as well.\n/// &lt;summary&gt;\n/// Resizes and positions the in-game Camera to accommodate the video dimensions\n/// &lt;/summary&gt;\nprivate void InitializeCamera()\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject mainCamera = GameObject.Find(\"Main Camera\");\n    // Adjust the camera position to account for updates to the VideoScreen\n    mainCamera.transform.position = new Vector3(videoDims.x / 2, videoDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    mainCamera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the VideoScreen\n    mainCamera.GetComponent&lt;Camera&gt;().orthographicSize = videoDims.y / 2;\n}\n\n\nModify Start() Method\nIn the Start method, we will first check if useWebcam is set to true. If it is, we will first limit the target framerate to the same as the target framerate for the webcam. We will then initialize the webcamTexture with the specified resolution and framerate. We will also disable the Video Player component. Lastly, we will update the values for videoDims with the final dimensions for the webcamTexture.\nIf we are not using a webcam, we will instead update videoDims with the dimensions from the Video Player component.\nNext, we need to initialize the videoTexture with the new dimensions and the ARGBHalf HDR texture format. We need to use an HDR texture format so that we can store color values outside the standard Unity range of [0,1]. The MobileNet version of the PoseNet model expects values to be in the range [-1,1] while the ResNet50 version expects values in the range [0,255].\nWe will then call the InitializeVideoScreen() and InitializeCamera() methods.\n// Start is called before the first frame update\nvoid Start()\n{\n    if (useWebcam)\n    {\n        // Limit application framerate to the target webcam framerate\n        Application.targetFrameRate = webcamFPS;\n        \n        // Create a new WebCamTexture\n        webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n\n        // Start the Camera\n        webcamTexture.Play();\n\n        // Deactivate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = false;\n\n        // Update the videoDims.y\n        videoDims.y = webcamTexture.height;\n        // Update the videoDims.x\n        videoDims.x = webcamTexture.width;\n    }\n    else\n    {\n        // Update the videoDims.y\n        videoDims.y = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().height;\n        // Update the videoDims.x\n        videoDims.x = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().width;\n    }\n\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(videoDims.x, videoDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(videoDims.x, videoDims.y, useWebcam);\n\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n}\n\n\nModify Update() Method\nFor now, the only thing we need to do in the Update method is to “copy” the pixel data from webcamTexture to videoTexture when using a webcam.\n// Update is called once per frame\nvoid Update()\n{\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam) Graphics.Blit(webcamTexture, videoTexture);\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-poseestimator-object",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#create-poseestimator-object",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Create PoseEstimator Object",
    "text": "Create PoseEstimator Object\nWith the required code completed, we just need to attach the script to a GameObject. Right-click an empty space in the Hierarchy tab and select Create Empty. Name the new object PoseEstimator.\n\n\n\n\n\n\nAttach PoseEstimator Script\nWith the PoseEstimator object selected in the Hierarchy tab, drag and drop the PoseEstimator script into the Inspector tab.\n\n\n\n\n\n\nAssign VideoScreen Object\nDrag and drop the VideoScreen object from the Hierarchy tab into the Video Screen spot in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#test-it-out",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#test-it-out",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Test it Out",
    "text": "Test it Out\nNow we can press the play button to test out the video player.\n\nNote: By default the Aspect for the Game view is set to Free Aspect, so the VideoScreen might not fill the entire view."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-2/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 2",
    "section": "Summary",
    "text": "Summary\nWe now have a video player that we can use to feed input to the PoseNet model. In the next post, we will implement the preprocessing steps for the PoseNet models.\nPrevious: Part 1\nNext: Part 3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "",
    "text": "Overview\nCreate Compute Shader\nCreate Utils Script\nUpdate PoseEstimator Script\nAssign PoseNetShader\nTest it Out\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Overview",
    "text": "Overview\nThe MobileNet and ResNet50 versions of the PoseNet model require different preprocessing steps. While it is more efficient to perform these steps on a GPU with a Compute shader, this may not be supported by the target platform. Therefore, we will also cover how to perform the preprocessing steps on the CPU as well.\n\nNote: We will be manually toggling between using the CPU and GPU in this tutorial. For real-world applications, we can determine if the target system supports compute shaders with the SystemInfo.supportsComputeShaders property."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#create-compute-shader",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#create-compute-shader",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Create Compute Shader",
    "text": "Create Compute Shader\nWe will start by implementing the preprocessing steps in a compute shader to execute them on a GPU. In the Assets section, create a new folder called Shaders. Enter the Shaders folder and right-click an empty space. Open the Create submenu and select Shader. Inside the Shader submenu, select Compute Shader. We can name the new shader PoseNetShader.\n\n\n\n\n\nDouble-click the new shader to open it in the code editor. By default, Compute shaders contain the following code. Go ahead and delete all the default code.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\n\nSpecify Function Names\nWe will first add the #pragma kernel lines to indicate what functions we want to be compiled. Without them, we can not access these functions from the PoseEstimator script. We’ll call the two functions PreprocessMobileNet and PreprocessResNet respectively.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel PreprocessMobileNet\n#pragma kernel PreprocessResNet\n\n\nDefine Variables\nWe will need a Texture2D variable to store the pixel data for the input image that will be passed from the PoseEstimator script. We will give it a data type of half4, which is a medium precision 4D vector. Each 4D Vector will contain the RGBA color and alpha values for a single pixel.\nWe also need a RWTexture2D so that we can write the processed image data back to a RenderTexture in the PoseEstimator script. Give it a data type of half4 as well.\n// The pixel data for the input image\nTexture2D&lt;half4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;half4&gt; Result;\n\n\nCreate PreprocessMobileNet Function\nNow we can define the functions we named earlier. We will stick with the default values for numthreads of (8,8,1).\nThe MobileNet version of the model expects color values to be in the range [-1,1]. By default color values in Unity are in the range [0,1]. The alpha channel is not used by the model, so the value does not matter.\n[numthreads(8, 8, 1)]\nvoid PreprocessMobileNet(uint3 id : SV_DispatchThreadID)\n{\n    // Normalize the color values to the range [-1,1]\n    //2 * (value - min) / (max - min) - 1\n    Result[id.xy] = half4(\n        2.0h * InputImage[id.xy].r / 1.0h - 1.0h,\n        2.0h * InputImage[id.xy].g / 1.0h - 1.0h,\n        2.0h * InputImage[id.xy].b / 1.0h - 1.0h, \n        InputImage[id.xy].a);\n}\n\n\nCreate PreprocessResNet Function\nThe ResNet50 version of the model expects color values to be in the range [0,255]. We also need to subtract the mean RGB color values for the ImageNet dataset to the pixel values.\n[numthreads(8, 8, 1)]\nvoid PreprocessResNet(uint3 id : SV_DispatchThreadID)\n{\n    // Scale each color value to the range [0,255]\n    // and add the ImageNet mean value\n    Result[id.xy] = half4(\n        InputImage[id.xy].r * 255.0h - 123.15h,\n        InputImage[id.xy].g * 255.0h - 115.90h,\n        InputImage[id.xy].b * 255.0h - 103.06h, \n        InputImage[id.xy].a);\n}\n\n\nFinal Code\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel PreprocessResNet\n#pragma kernel PreprocessMobileNet\n\n// The pixel data for the input image\nTexture2D&lt;half4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;half4&gt; Result;\n\n[numthreads(8, 8, 1)]\nvoid PreprocessMobileNet(uint3 id : SV_DispatchThreadID)\n{\n    // Normalize the color values to the range [-1,1]\n    //2 * (value - min) / (max - min) - 1\n    Result[id.xy] = half4(\n        2.0h * InputImage[id.xy].r / 1.0h - 1.0h,\n        2.0h * InputImage[id.xy].g / 1.0h - 1.0h,\n        2.0h * InputImage[id.xy].b / 1.0h - 1.0h, \n        InputImage[id.xy].a);\n}\n\n[numthreads(8, 8, 1)]\nvoid PreprocessResNet(uint3 id : SV_DispatchThreadID)\n{\n    // Scale each color value to the range [0,255]\n    // and add the ImageNet mean value\n    Result[id.xy] = half4(\n        InputImage[id.xy].r * 255.0h - 123.15h,\n        InputImage[id.xy].g * 255.0h - 115.90h,\n        InputImage[id.xy].b * 255.0h - 103.06h, \n        InputImage[id.xy].a);\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#create-utils-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#create-utils-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Create Utils Script",
    "text": "Create Utils Script\nWe will be placing the CPU preprocessing and postprocessing methods inside a separate C# script called Utils, to prevent the PoseEstimator script from getting too long.\n\nRemove MonoBehaviour Inheritance\nThe Utils class does not need to inherit from Monobehavior as it will not be directly attached to a GameObject.\npublic class Utils\n\n\nCreatePreprocessMobileNet Method\nThe Barracuda library uses Tensors to store data. These are like multidimensional arrays. We can download the data stored in a Tensor to a regular float array. We will pass this array as input to the preprocessing methods and then upload the new values to a Tensor.\n\nNote: Make sure to use the exact names for the methods as those in the Compute shader.\n\n/// &lt;summary&gt;\n/// Applies the preprocessing steps for the MobileNet model on the CPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"tensor\"&gt;Pixel data from the input tensor&lt;/param&gt;\npublic static void PreprocessMobileNet(float[] tensor)\n{\n    // Normaliz the values to the range [-1, 1]\n    System.Threading.Tasks.Parallel.For(0, tensor.Length, (int i) =&gt;\n    {\n        tensor[i] = (float)(2.0f * tensor[i] / 1.0f) - 1.0f;\n    });\n}\n\n\nCreate PreprocessResNet Method\nThe color data for pixels is stored sequentially in the tensor array. For example, the first three values in the array would be the red, green, and blue color values for the first pixel in the image. The tensor data will not have an alpha channel, so we do not need to account for it here.\n///// &lt;summary&gt;\n///// Applies the preprocessing steps for the ResNet50 model on the CPU\n///// &lt;/summary&gt;\n///// &lt;param name=\"tensor\"&gt;Pixel data from the input tensor&lt;/param&gt;\npublic static void PreprocessResNet(float[] tensor)\n{\n    System.Threading.Tasks.Parallel.For(0, tensor.Length / 3, (int i) =&gt;\n    {\n        tensor[i * 3 + 0] = (float)tensor[i * 3 + 0] * 255f - 123.15f;\n        tensor[i * 3 + 1] = (float)tensor[i * 3 + 1] * 255f - 115.90f;\n        tensor[i * 3 + 2] = (float)tensor[i * 3 + 2] * 255f - 103.06f;\n    });\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nNow we can call the preprocessing methods inside the PoseEstimator script. However, we first need to make some other additions.\n\nAdd Barracuda Namespace\nWe need to add the Unity.Barracuda namespace so that we can work with Tensors.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Video;\nusing Unity.Barracuda;\n\n\nAdd Public Variables\nWe can define a public enum for the two different model versions. We can use this to create a dropdown menu in the inspector tab to switch between the two options. We will name it ModelType.\npublic class PoseEstimator : MonoBehaviour\n{\n    public enum ModelType\n    {\n        MobileNet,\n        ResNet50\n    }\nNext, we will need a public ComputeShader variable so that we can access the PoseNetShader.\nWe can create a dropdown for selecting the model type by defining a public ModelType variable. We will set the default value to ModelType.ResNet50.\nWe also need a public bool variable to toggle between using the CPU and GPU for processing input.\nLastly, we need a public VectorInt variable to specify the dimensions of the input image. Using the original resolution of the video feed could significantly impact performance, so we will downscale the input image before feeding it to the model.\n[Tooltip(\"The ComputeShader that will perform the model-specific preprocessing\")]\npublic ComputeShader posenetShader;\n\n[Tooltip(\"The model architecture used\")]\npublic ModelType modelType = ModelType.ResNet50;\n\n[Tooltip(\"Use GPU for preprocessing\")]\npublic bool useGPU = true;\n\n[Tooltip(\"The dimensions of the image being fed to the model\")]\npublic Vector2Int imageDims = new Vector2Int(256, 256);\n\n\nAdd Private Variables\nWe will be maintaining the aspect ratio of the source video feed when downscaling the input image. We need to keep track of the current input dimensions so that we know when to calculate the new dimensions. Create a new private Vector2Int variable called targetDims.\nNext, create a private float variable called aspectRatioScale. This will store the scaling value to update the targetDims.\nThe pixel data for the input image will be stored in a new private RenderTexture variable called rTex.\nWe will be encapsulating the appropriate preprocessing method using the Action&lt;T&gt; delegate.\nThe last new variable we need is a Barracuda Tensor to store the input data for the model.\n// Target dimensions for model input\nprivate Vector2Int targetDims;\n\n// Used to scale the input image dimensions while maintaining aspect ratio\nprivate float aspectRatioScale;\n\n// The texture used to create input tensor\nprivate RenderTexture rTex;\n\n// The preprocessing function for the current model type\nprivate System.Action&lt;float[]&gt; preProcessFunction;\n\n// Stores the input data for the model\nprivate Tensor input;\n\n\nUpdate Start Method\nAt the bottom of the Start method, we need to adjust the input dimensions to maintain the source aspect ratio. We will use the height value to update the width for the input dimensions. We can then initialize rTex with the new input dimensions.\n// Adjust the input dimensions to maintain the source aspect ratio\naspectRatioScale = (float)videoTexture.width / videoTexture.height;\ntargetDims.x = (int)(imageDims.y * aspectRatioScale);\nimageDims.x = targetDims.x;\n\n// Initialize the RenderTexture that will store the processed input image\nrTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGBHalf);\n\nFinal Code\n// Start is called before the first frame update\nvoid Start()\n{\n    if (useWebcam)\n    {\n        // Limit application framerate to the target webcam framerate\n        Application.targetFrameRate = webcamFPS;\n\n        // Create a new WebCamTexture\n        webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n\n        // Start the Camera\n        webcamTexture.Play();\n\n        // Deactivate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = false;\n\n        // Update the videoDims.y\n        videoDims.y = webcamTexture.height;\n        // Update the videoDims.x\n        videoDims.x = webcamTexture.width;\n    }\n    else\n    {\n        // Update the videoDims.y\n        videoDims.y = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().height;\n        // Update the videoDims.x\n        videoDims.x = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().width;\n    }\n\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(videoDims.x, videoDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(videoDims.x, videoDims.y, useWebcam);\n\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n\n    // Adjust the input dimensions to maintain the source aspect ratio\n    aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n    targetDims.x = (int)(imageDims.y * aspectRatioScale);\n    imageDims.x = targetDims.x;\n\n    // Initialize the RenderTexture that will store the processed input image\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGBHalf);\n}\n\n\n\nCreate ProcessImageGPU Method\nNext, we’ll make a new method to execute the functions in our ComputeShader. This method will take in the image that needs to be processed as well as a function name to indicate which function we want to execute. As mentioned previously, we need to store the processed images in textures with HDR formats to use color values outside the default range of [0,1].\n\nMethod Steps\n\nGet the ComputeShader index for the specified function\nCreate a temporary RenderTexture with random write access enabled to store the processed image\nExecute the ComputeShader\nCopy the processed image back into the original RenderTexture\nRelease the temporary RenderTexture\n\n\n\nCode\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = posenetShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    posenetShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    posenetShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    posenetShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release the temporary RenderTexture\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\n\nCreate ProcessImage Method\nWe will call the preprocessing functions inside a new method called ProcessImage. The method will take in a RenderTexture and update the input Tensor data.\n\nMethod Steps\n\nCheck whether to use the GPU\n\nIf using GPU\n\nCall ProcessImageGPU() method using the name of the preProcessFunction\nInitialize input with pixel data from rTex\n\nIf using CPU\n\nInitialize input with pixel data from rTex\nDownload Tensor data to float array\nCall the appropriate preprocessing function for the current model type\nUpdate input with the new color values\n\n\n\n\n\nCode\n/// &lt;summary&gt;\n/// Calls the appropriate preprocessing function to prepare\n/// the input for the selected model and hardware\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;&lt;/param&gt;\nprivate void ProcessImage(RenderTexture image)\n{\n    if (useGPU)\n    {\n        // Apply preprocessing steps\n        ProcessImageGPU(image, preProcessFunction.Method.Name);\n        // Create a Tensor of shape [1, image.height, image.width, 3]\n        input = new Tensor(image, channels: 3);\n    }\n    else\n    {\n        // Create a Tensor of shape [1, image.height, image.width, 3]\n        input = new Tensor(image, channels: 3);\n        // Download the tensor data to an array\n        float[] tensor_array = input.data.Download(input.shape);\n        // Apply preprocessing steps\n        preProcessFunction(tensor_array);\n        // Update input tensor with new color data\n        input = new Tensor(input.shape.batch,\n                           input.shape.height,\n                           input.shape.width,\n                           input.shape.channels,\n                           tensor_array);\n    }\n}\n\n\n\nModify Update Method\nWe will update the input dimensions and process the input inside the Update method.\n\nClamp Input Dimensions\nThe model will not return useable output with input below 130px in size. There just isn’t enough for information for the model to work with at that low of a resolution. Also, the model downscales the input internally by a set amount and might error out if the input is too low. To prevent this, we will ensure the input dimensions are at least 130x130.\n// Prevent the input dimensions from going too low for the model\nimageDims.x = Mathf.Max(imageDims.x, 130);\nimageDims.y = Mathf.Max(imageDims.y, 130);\n\n\nCalculate Input Dimensions\nWe need to adjust the input dimensions to maintain the source aspect ratio whenever they are updated by the user. We will check if the values for inputDims have changed by comparing them to targetDims.\n// Update the input dimensions while maintaining the source aspect ratio\nif (imageDims.x != targetDims.x)\n{\n    aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n    targetDims.y = (int)(imageDims.x * aspectRatioScale);\n    imageDims.y = targetDims.y;\n    targetDims.x = imageDims.x;\n}\nif (imageDims.y != targetDims.y)\n{\n    aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n    targetDims.x = (int)(imageDims.y * aspectRatioScale);\n    imageDims.x = targetDims.x;\n    targetDims.y = imageDims.y;\n}\n\n\nUpdate rTex Dimensions\nWe will also need to update rTex with the new input dimensions and copy the pixel data from the source videoTexture to it.\n// Update the rTex dimensions to the new input dimensions\nif (imageDims.x != rTex.width || imageDims.y != rTex.height)\n{\n    RenderTexture.ReleaseTemporary(rTex);\n    // Assign a temporary RenderTexture with the new dimensions\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, rTex.format);\n}\n\n// Copy the src RenderTexture to the new rTex RenderTexture\nGraphics.Blit(videoTexture, rTex);\n\n\nCall ProcessImage Method\nThe preProcessFunction variable will be upated in a new function that will be covered in the next post. For now, we can add a temporary if/else statement to test the preprocessing functions. We will delete this statement in the next part of the tutorial.\nif (modelType == ModelType.MobileNet)\n{\n    preProcessFunction = Utils.PreprocessMobileNet;\n}\nelse\n{\n    preProcessFunction = Utils.PreprocessResNet;\n}\nFinally, we can call the ProcessImage method and pass rTex as input.\n// Prepare the input image to be fed to the selected model\nProcessImage(rTex);\n\n\nFinal Code\n// Update is called once per frame\nvoid Update()\n{\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam) Graphics.Blit(webcamTexture, videoTexture);\n\n    // Prevent the input dimensions from going too low for the model\n    imageDims.x = Mathf.Max(imageDims.x, 64);\n    imageDims.y = Mathf.Max(imageDims.y, 64);\n\n    // Update the input dimensions while maintaining the source aspect ratio\n    if (imageDims.x != targetDims.x)\n    {\n        aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n        targetDims.y = (int)(imageDims.x * aspectRatioScale);\n        imageDims.y = targetDims.y;\n        targetDims.x = imageDims.x;\n    }\n    if (imageDims.y != targetDims.y)\n    {\n        aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n        targetDims.x = (int)(imageDims.y * aspectRatioScale);\n        imageDims.x = targetDims.x;\n        targetDims.y = imageDims.y;\n    }\n\n    // Update the rTex dimensions to the new input dimensions\n    if (imageDims.x != rTex.width || imageDims.y != rTex.height)\n    {\n        RenderTexture.ReleaseTemporary(rTex);\n        // Assign a temporary RenderTexture with the new dimensions\n        rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, rTex.format);\n    }\n\n    // Copy the src RenderTexture to the new rTex RenderTexture\n    Graphics.Blit(videoTexture, rTex);\n\n\n    if (modelType == ModelType.MobileNet)\n    {\n        preProcessFunction = Utils.PreprocessMobileNet;\n    }\n    else\n    {\n        preProcessFunction = Utils.PreprocessResNet;\n    }\n\n    // Prepare the input image to be fed to the selected model\n    ProcessImage(rTex);\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#assign-posenetshader",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#assign-posenetshader",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Assign PoseNetShader",
    "text": "Assign PoseNetShader\nThe last step we need to take before pressing play is to assign the PoseNetShader asset. Select the PoseEstimator object in the Hierarchy tab. Then, drag and drop the PoseNetShader asset from the Assets section onto its spot in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#test-it-out",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#test-it-out",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Test it Out",
    "text": "Test it Out\nNow when we press play, we can see that the values for Image Dims get updated in the Inspector tab to maintain the source aspect ratio. We can change either the X or Y values and the other dimension will be automatically adjusted.\n\n\n\n\n\nWe can view the frame rate by pressing the stats button in the Game View. If we toggle Use GPU in the Inspector tab, we can see why it is preferrable to perform the preprocessing steps on the GPU. The frame rate drops significantly when using the CPU.\n\nGPU\n\n\n\n\n\n\n\nCPU"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-3/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 3",
    "section": "Summary",
    "text": "Summary\nNow that we have prepared the input, we are ready to feed it to the model. In the next post, we will cover how to initialize, modify, and execute the PoseNet models.\nPrevious: Part 2\nNext: Part 4\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "",
    "text": "Overview\nUpdate PoseEstimator Script\nAssign Model Assets\nTest it Out\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "Overview",
    "text": "Overview\nIn this post, we will cover how to load, modify, and execute the PoseNet models. We will also be comparing the relative efficiency of the ResNet50 and MobileNet versions of the model using the GPU and CPU backends for the Barracuda library."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nBefore we can execute the models, we need to add some new variables and add a new layer to the end of the selected model.\n\nAdd Public Variables\nThe ONNX files that we imported into the Assets section in part 1 are automatically converted into Barracuda model assets called NNModels. We need to add a couple public NNModel variables for the MobileNet and ResNet models.\nWe will also add a public WorkerFactory.Type variable so that we can switch between the available Barracuda backends while the project is running. Give this variable a default value of WorkerFactory.Type.Auto. This will automatically select the best backend for the platform.\n[Tooltip(\"The MobileNet model asset file to use when performing inference\")]\npublic NNModel mobileNetModelAsset;\n\n[Tooltip(\"The ResNet50 model asset file to use when performing inference\")]\npublic NNModel resnetModelAsset;\n\n[Tooltip(\"The backend to use when performing inference\")]\npublic WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\n\n\nAdd Private Variables\nTo perform inference with the Barracuda library, we first need to generate an object-orientated representation of the model. We then create an IWorker interface to handle model execution.\nIn order to switch between models or backends while the project is running, we will need to keep track of the current model and backend. Whenever we switch between models or backends, we will need to initialize the IWorker with the new model and backend.\nWe will define a new struct called Engine (for inference engine) to keep track of the current backend, model type, and IWorker.\nThe PoseNet model has four outputs: heatmaps, offsets, displacementFWDLayer, and displacementBWDLayer.\nThe heatmaps are basically low resolution versions of the input image where each pixel contains a value indicating how confident the model is that a given key point is in that spot. There is a heatmap for each key point predicted by the model.\nThe offsets are used to refine the rough locations from the heatmaps. There are two offsets for each key point. They correspond to the X and Y axes. These values are added to the coordinates (i.e. heatmap indices) estimated by the heatmaps to scale the coordinates back up to the input resolution and give a more accurate position.\nThe last two outputs are needed specifically for multi-pose estimation and are used to identify key points that belong to the same body in an image. These will be explored further in the post covering the post processing steps for multi-pose estimation.\nThe names of these output layers are different for the MobileNet and ResNet models so we will need to keep track of them as well.\nWe will also be adding a new layer to the model that will take the values from the heatmaps and remap them to the range [0,1]. This will make it easier to tell how confident the model is about its predictions. For example, a value of 0 would indicate the the model is certain that a given key point is not in that location. A value of 0.95 would indicate it is 95% certain the key point is there.\n/// &lt;summary&gt;\n/// Keeps track of the current inference backend, model execution interface, \n/// and model type\n/// &lt;/summary&gt;\nprivate struct Engine\n{\n    public WorkerFactory.Type workerType;\n    public IWorker worker;\n    public ModelType modelType;\n\n    public Engine(WorkerFactory.Type workerType, Model model, ModelType modelType)\n    {\n        this.workerType = workerType;\n        worker = WorkerFactory.CreateWorker(workerType, model);\n        this.modelType = modelType;\n    }\n}\n\n// The interface used to execute the neural network\nprivate Engine engine;\n\n// The name for the heatmap layer in the model asset\nprivate string heatmapLayer;\n\n// The name for the offsets layer in the model asset\nprivate string offsetsLayer;\n\n// The name for the forwards displacement layer in the model asset\nprivate string displacementFWDLayer;\n\n// The name for the backwards displacement layer in the model asset\nprivate string displacementBWDLayer;\n\n// The name for the Sigmoid layer that returns the heatmap predictions\nprivate string predictionLayer = \"heatmap_predictions\";\n\n\nCreate InitializeBarracuda Method\nWe will perform the initialization steps for Barracuda in a new method called InitializeBarracuda. This method will be called in the Start method and whenever the user switches models or backends.\n\nMethod Steps\n\nDeclare a new Model variable to store the object-oriented representation of the selected model asset.\nUpdate the values for the preprocessing method and output layers based on the selected model.\n\nNote: While the heatmap and offset layers are in the same order for both models, the two displacement layers are swapped.\n\nCreate a new ModelBuilder to modify the model\nAdd a Sigmoid layer to remap the output from the heatmaps to the range [0,1]\nConfirm whether the selected backend is supported by the current platform\nCreate a new instance of the Engine struct\n\nStore the backend type\nInitialize the IWorker with the selected backend and model\nStore the selected model type\n\n\n\n\nCode\n/// &lt;summary&gt;\n/// Updates the output layer names based on the selected model architecture\n/// and initializes the Barracuda inference engine witht the selected model.\n/// &lt;/summary&gt;\nprivate void InitializeBarracuda()\n{\n    // The compiled model used for performing inference\n    Model m_RunTimeModel;\n\n    if (modelType == ModelType.MobileNet)\n    {\n        preProcessFunction = Utils.PreprocessMobileNet;\n        // Compile the model asset into an object oriented representation\n        m_RunTimeModel = ModelLoader.Load(mobileNetModelAsset);\n        displacementFWDLayer = m_RunTimeModel.outputs[2];\n        displacementBWDLayer = m_RunTimeModel.outputs[3];\n    }\n    else\n    {\n        preProcessFunction = Utils.PreprocessResNet;\n        // Compile the model asset into an object oriented representation\n        m_RunTimeModel = ModelLoader.Load(resnetModelAsset);\n        displacementFWDLayer = m_RunTimeModel.outputs[3];\n        displacementBWDLayer = m_RunTimeModel.outputs[2];\n    }\n\n    heatmapLayer = m_RunTimeModel.outputs[0];\n    offsetsLayer = m_RunTimeModel.outputs[1];\n\n    // Create a model builder to modify the m_RunTimeModel\n    ModelBuilder modelBuilder = new ModelBuilder(m_RunTimeModel);\n\n    // Add a new Sigmoid layer that takes the output of the heatmap layer\n    modelBuilder.Sigmoid(predictionLayer, heatmapLayer);\n\n    // Validate if backend is supported, otherwise use fallback type.\n    workerType = WorkerFactory.ValidateType(workerType);\n\n    // Create a worker that will execute the model with the selected backend\n    engine = new Engine(workerType, modelBuilder.model, modelType);\n}\n\n\n\nModify Start Method\nWe will call the InitializeBarracuda at the bottom of the Start method.\n// Initialize the Barracuda inference engine based on the selected model\nInitializeBarracuda();\n\nFinal Code\n// Start is called before the first frame update\nvoid Start()\n{\n    if (useWebcam)\n    {\n        // Limit application framerate to the target webcam framerate\n        Application.targetFrameRate = webcamFPS;\n\n        // Create a new WebCamTexture\n        webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n\n        // Start the Camera\n        webcamTexture.Play();\n\n        // Deactivate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = false;\n\n        // Update the videoDims.y\n        videoDims.y = webcamTexture.height;\n        // Update the videoDims.x\n        videoDims.x = webcamTexture.width;\n    }\n    else\n    {\n        // Update the videoDims.y\n        videoDims.y = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().height;\n        // Update the videoDims.x\n        videoDims.x = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().width;\n    }\n\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(videoDims.x, videoDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(videoDims.x, videoDims.y, useWebcam);\n\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n\n    // Adjust the input dimensions to maintain the source aspect ratio\n    aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n    targetDims.x = (int)(imageDims.y * aspectRatioScale);\n    imageDims.x = targetDims.x;\n\n    // Initialize the RenderTexture that will store the processed input image\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the Barracuda inference engine based on the selected model\n    InitializeBarracuda();\n}\n\n\n\nModify Update Method\nIn the Update method we can delete the temporary if/else statement from the last part.\nif (modelType == ModelType.MobileNet)\n{\n    preProcessFunction = Utils.PreprocessMobileNet;\n}\nelse\n{\n    preProcessFunction = Utils.PreprocessResNet;\n}\nWe will replace it with a new if statement that will call the InitializeBarracuda method whenever the user switches models or backends. Before calling the InitializeBarracuda method, we need to release the resources allocated for the IWorker to avoid memory leaks.\n// Reinitialize Barracuda with the selected model and backend \nif (engine.modelType != modelType || engine.workerType != workerType)\n{\n    engine.worker.Dispose();\n    InitializeBarracuda();\n}\nThen, we can finally execute the model by calling the IWorker.Execute method with the input Tensor. Once we have executed the model, we need to release the system resources allocated for the input Tensor to avoid memory leaks.\n// Execute neural network with the provided input\nengine.worker.Execute(input);\n// Release resources allocated for the Tensor\ninput.Dispose();\n\nFinal Code\n// Update is called once per frame\nvoid Update()\n{\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam) Graphics.Blit(webcamTexture, videoTexture);\n\n    // Prevent the input dimensions from going too low for the model\n    imageDims.x = Mathf.Max(imageDims.x, 64);\n    imageDims.y = Mathf.Max(imageDims.y, 64);\n\n    // Update the input dimensions while maintaining the source aspect ratio\n    if (imageDims.x != targetDims.x)\n    {\n        aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n        targetDims.y = (int)(imageDims.x * aspectRatioScale);\n        imageDims.y = targetDims.y;\n        targetDims.x = imageDims.x;\n    }\n    if (imageDims.y != targetDims.y)\n    {\n        aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n        targetDims.x = (int)(imageDims.y * aspectRatioScale);\n        imageDims.x = targetDims.x;\n        targetDims.y = imageDims.y;\n    }\n\n    // Update the rTex dimensions to the new input dimensions\n    if (imageDims.x != rTex.width || imageDims.y != rTex.height)\n    {\n        RenderTexture.ReleaseTemporary(rTex);\n        // Assign a temporary RenderTexture with the new dimensions\n        rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, rTex.format);\n    }\n\n    // Copy the src RenderTexture to the new rTex RenderTexture\n    Graphics.Blit(videoTexture, rTex);\n\n    // Prepare the input image to be fed to the selected model\n    ProcessImage(rTex);\n\n    // Reinitialize Barracuda with the selected model and backend \n    if (engine.modelType != modelType || engine.workerType != workerType)\n    {\n        engine.worker.Dispose();\n        InitializeBarracuda();\n    }\n\n    // Execute neural network with the provided input\n    engine.worker.Execute(input);\n    // Release GPU resources allocated for the Tensor\n    input.Dispose();\n}\n\n\n\nDefine OnDisable Method\nWe need to add some cleanup code for when the application closes. As with calling the InitializeBarracuda method, we need to release the resources allocated for the IWorker to avoid memory leaks. We will do so in the OnDisable() method. This method is called when the MonoBehavior becomes disabled.\n// OnDisable is called when the MonoBehavior becomes disabled or inactive\nprivate void OnDisable()\n{\n    // Release the resources allocated for the inference engine\n    engine.worker.Dispose();\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#assign-model-assets",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#assign-model-assets",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "Assign Model Assets",
    "text": "Assign Model Assets\nNow we just need to assign the model assets in the Inspector tab. Open the Models folder in the Assets section. With the PoseEstimator object selected in the Hierarchy tab, drag the model assets onto their associated spots in the Inspector tab."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#test-it-out",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#test-it-out",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "Test it Out",
    "text": "Test it Out\nIf we press the Play button in the Game View, we can compare the inference speed of the models and backends.\n\nResNet50\nIf we look at the Inspector tab, we can see that the Compute Precompiled backend was automatically selected. This is the fastest GPU backend with the least CPU overhead.\n\nGPU Preprocessing and GPU Inference\nFor the best possible performance, both the preprocessing and inference should be performed on the GPU.\n\n\n\n\n\n\n\nCPU Preprocessing and GPU Inference\nIf we uncheck the Use GPU box, we can see that the frame rate drops significantly, even when using the same backend.\n\n\n\n\n\n\n\nGPU Preprocessing and CPU Inference\nThe ResNet50 model is not optimized for CPU inference and will not get playable frame rates on most CPUs.\n\n\n\n\n\n\n\n\nMobileNet\nAs it’s name suggests, the MobileNet model is optimized to run on mobile hardware. With a desktop GPU, performance is likely to be CPU bottlenecked.\n\nGPU Preprocessing and GPU Inference\nAs with the ResNet50 model, performing preprocessing and inference on the GPU yields the best performance.\n\n\n\n\n\n\n\nGPU Preprocessing and CPU Inference\nUnlike the Resnet50 model, the MobileNet model gets playable framerates on the CPU."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-4/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 4",
    "section": "Summary",
    "text": "Summary\nWe now have a general idea of how both models perform on GPU and CPU. In the next post we will implement the post processing steps for single pose estimation.\nPrevious: Part 3\nNext: Part 5\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-5/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-5/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 5",
    "section": "",
    "text": "Overview\nUpdate Utils Script\nUpdate PoseEstimator Script\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 5",
    "section": "Overview",
    "text": "Overview\nIn this post, we will cover how to implement the post processing steps for single pose estimation. This method is much simpler than what is required to perform multi-pose estimation. However, it should only be used when there is a single person in the input image."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#update-utils-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#update-utils-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 5",
    "section": "Update Utils Script",
    "text": "Update Utils Script\nWe will implement the methods for processing the model output in the Utils script.\n\nAdd Required Namespace\nFirst, we need to add the Unity.Barracuda namespace since we will be working with Tensors.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\n\n\nAdd Public Variables\nEach key point predicted by the model has a confidence score, position, and id number associated with it. For example a nose has the id number 0. We will define a new struct to keep track of these values for each key point.\n/// &lt;summary&gt;\n/// Stores the heatmap score, position, and partName index for a single keypoint\n/// &lt;/summary&gt;\npublic struct Keypoint\n{\n    public float score;\n    public Vector2 position;\n    public int id;\n\n    public Keypoint(float score, Vector2 position, int id)\n    {\n        this.score = score;\n        this.position = position;\n        this.id = id;\n    }\n}\n\n\nCreate GetOffsetVector Method\nNext, we will create a new method to obtain the offset values associated with a given heatmap coordinate. The method will take in the X and Y values for a heatmap coordinate, the current key point id number, and the offset values from the model output.\n/// &lt;summary&gt;\n/// Get the offset values for the provided heatmap indices\n/// &lt;/summary&gt;\n/// &lt;param name=\"y\"&gt;Heatmap column index&lt;/param&gt;\n/// &lt;param name=\"x\"&gt;Heatmap row index&lt;/param&gt;\n/// &lt;param name=\"keypoint\"&gt;Heatmap channel index&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;Offsets output tensor&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic static Vector2 GetOffsetVector(int y, int x, int keypoint, Tensor offsets)\n{\n    // Get the offset values for the provided heatmap coordinates\n    return new Vector2(offsets[0, y, x, keypoint + 17], offsets[0, y, x, keypoint]);\n}\n\n\nCreate GetImageCoords Method\nWe can calculate the estimated location of a key point in the input image by multiplying the heatmap coordinate by the stride value for the model and then adding the associated offset values. We will calculate the stride value for the current model in the PoseEstimator script.\n /// &lt;summary&gt;\n/// Calculate the position of the provided key point in the input image\n/// &lt;/summary&gt;\n/// &lt;param name=\"part\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic static Vector2 GetImageCoords(Keypoint part, int stride, Tensor offsets)\n{\n    // The accompanying offset vector for the current coords\n    Vector2 offsetVector = GetOffsetVector((int)part.position.y, (int)part.position.x,\n                                           part.id, offsets);\n\n    // Scale the coordinates up to the input image resolution\n    // Add the offset vectors to refine the key point location\n    return (part.position * stride) + offsetVector;\n}\n\n\nCreate DecodeSinglePose Method\nThis is the method that will be called from the PoseEstimator script after executing the model. It will take in the heatmaps and offsets from the model output along with the stride value for the model as input.\nFor single pose estimation, we will iterate through the heatmaps from the model output and keep track of the indices with the highest confidence value for each key point. Once we have the heatmap location with the highest confidence value, we can call the GetImageCoords method to calculate the position of the key point in the input image. We will store each key point in a Keypoint array.\n\nNote: This approach should only be used when there is a single person in the input image. It is unlikely that the key points with the highest confidence scores will belong to the same body when multiple people are visible.\n\n/// &lt;summary&gt;\n/// Determine the estimated key point locations using the heatmaps and offsets tensors\n/// &lt;/summary&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps that indicate the confidence levels for key point locations&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets that refine the key point locations determined with the heatmaps&lt;/param&gt;\n/// &lt;returns&gt;An array of keypoints for a single pose&lt;/returns&gt;\npublic static Keypoint[] DecodeSinglePose(Tensor heatmaps, Tensor offsets, int stride)\n{\n    Keypoint[] keypoints = new Keypoint[heatmaps.channels];\n\n    // Iterate through heatmaps\n    for (int c = 0; c &lt; heatmaps.channels; c++)\n    {\n        Keypoint part = new Keypoint();\n        part.id = c;\n\n        // Iterate through heatmap columns\n        for (int y = 0; y &lt; heatmaps.height; y++)\n        {\n            // Iterate through column rows\n            for (int x = 0; x &lt; heatmaps.width; x++)\n            {\n                if (heatmaps[0, y, x, c] &gt; part.score)\n                {\n                    // Update the highest confidence for the current key point\n                    part.score = heatmaps[0, y, x, c];\n\n                    // Update the estimated key point coordinates\n                    part.position.x = x;\n                    part.position.y = y;\n                }\n            }\n        }\n\n        // Calcluate the position in the input image for the current (x, y) coordinates\n        part.position = GetImageCoords(part, stride, offsets);\n\n        // Add the current keypoint to the list\n        keypoints[c] = part;\n    }\n\n    return keypoints;\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 5",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nIn the PoseEstimator script, we need to add some new variables before we can call the DecodeSinglePose method.\n\nAdd Public Variables\nFirst, we will define a new public enum so that we can choose whether to perform single or multi-pose estimation from the inspector tab.\npublic enum EstimationType\n{\n    MultiPose,\n    SinglePose\n}\n\n[Tooltip(\"The type of pose estimation to be performed\")]\npublic EstimationType estimationType = EstimationType.SinglePose;\n\n\nAdd Private Variables\nWe will store the Keypoint arrays returned by the post processing methods in an array of Keypoint arrays. There will only be one array stored for single pose estimation, but there will be several for multi-pose estimation.\n// Stores the current estimated 2D keypoint locations in videoTexture\nprivate Utils.Keypoint[][] poses;\n\n\nCreate ProcessOutput Method\nWe will call the postprocessing methods inside a new method called ProcessOutput. This method will take in the IWorker from engine.\n\nMethod Steps\n\nGet the four model outputs\nCalculate the stride for the current model\nCall the appropriate post processing method for the selected estimation type\n\nNote: We will fill in the else statement when we implement the post processing steps for multi-pose estimation.\n\nRelease the resources allocated for the output Tensors.\n\n/// &lt;summary&gt;\n/// Obtains the model output and either decodes single or mutlple poses\n/// &lt;/summary&gt;\n/// &lt;param name=\"engine\"&gt;&lt;/param&gt;\nprivate void ProcessOutput(IWorker engine)\n{\n    // Get the model output\n    Tensor heatmaps = engine.PeekOutput(predictionLayer);\n    Tensor offsets = engine.PeekOutput(offsetsLayer);\n    Tensor displacementFWD = engine.PeekOutput(displacementFWDLayer);\n    Tensor displacementBWD = engine.PeekOutput(displacementBWDLayer);\n\n    // Calculate the stride used to scale down the inputImage\n    int stride = (imageDims.y - 1) / (heatmaps.shape.height - 1);\n    stride -= (stride % 8);\n\n    if (estimationType == EstimationType.SinglePose)\n    {\n        // Initialize the array of Keypoint arrays\n        poses = new Utils.Keypoint[1][];\n\n        // Determine the key point locations\n        poses[0] = Utils.DecodeSinglePose(heatmaps, offsets, stride);\n    }\n    else\n    {\n        \n    }\n    \n    // Release the resources allocated for the output Tensors\n    heatmaps.Dispose();\n    offsets.Dispose();\n    displacementFWD.Dispose();\n    displacementBWD.Dispose();\n}\n\n\n\nModify Update Method\nWe will call the ProcessOutput method at the end of the Update method.\n// Decode the keypoint coordinates from the model output\nProcessOutput(engine.worker);\n\nFull Code\nvoid Update()\n{\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam) Graphics.Blit(webcamTexture, videoTexture);\n\n    // Prevent the input dimensions from going too low for the model\n    imageDims.x = Mathf.Max(imageDims.x, 64);\n    imageDims.y = Mathf.Max(imageDims.y, 64);\n\n    // Update the input dimensions while maintaining the source aspect ratio\n    if (imageDims.x != targetDims.x)\n    {\n        aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n        targetDims.y = (int)(imageDims.x * aspectRatioScale);\n        imageDims.y = targetDims.y;\n        targetDims.x = imageDims.x;\n    }\n    if (imageDims.y != targetDims.y)\n    {\n        aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n        targetDims.x = (int)(imageDims.y * aspectRatioScale);\n        imageDims.x = targetDims.x;\n        targetDims.y = imageDims.y;\n    }\n\n    // Update the rTex dimensions to the new input dimensions\n    if (imageDims.x != rTex.width || imageDims.y != rTex.height)\n    {\n        RenderTexture.ReleaseTemporary(rTex);\n        // Assign a temporary RenderTexture with the new dimensions\n        rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, rTex.format);\n    }\n\n    // Copy the src RenderTexture to the new rTex RenderTexture\n    Graphics.Blit(videoTexture, rTex);\n\n    // Prepare the input image to be fed to the selected model\n    ProcessImage(rTex);\n\n    // Reinitialize Barracuda with the selected model and backend \n    if (engine.modelType != modelType || engine.workerType != workerType)\n    {\n        engine.worker.Dispose();\n        InitializeBarracuda();\n    }\n\n    // Execute neural network with the provided input\n    engine.worker.Execute(input);\n    // Release GPU resources allocated for the Tensor\n    input.Dispose();\n\n    // Decode the keypoint coordinates from the model output\n    ProcessOutput(engine.worker);\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-5/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 5",
    "section": "Summary",
    "text": "Summary\nThat is all we need to perform pose estimation when there is a single person in the input image. In the next post, we will implement the post processing steps for multi-pose estimation.\nPrevious: Part 4\nPrevious: Part 6\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-6/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-6/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 6",
    "section": "",
    "text": "Overview\nUpdate Utils Script\nUpdate PoseEstimator Script\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 6",
    "section": "Overview",
    "text": "Overview\nIn this post, we will cover how to implement the post processing steps for multi-pose estimation. This method is more complex than what is required to perform single pose estimation. However, it can produce more reliable results.\n\nNote: The original JavaScript code for decoding multiple poses can be found in the official tfjs-models repository on GitHub. The code has been modified for this tutorial to better take advantage of functionality provided by Unity and .NET."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#update-utils-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#update-utils-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 6",
    "section": "Update Utils Script",
    "text": "Update Utils Script\nThere are a couple new variables and several methods that we will need to add to decode multiple poses from the model output.\n\nAdd Required Namespace\nFirst, we need to add the System namespace to access the Tuple class. We also need to access the System.Linq namespace to access classes and interfaces for querying data structures.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\nusing System;\nusing System.Linq;\n\n\nAdd Public Variables\nWhen iterating through the heatmaps from the model output, we will only be considering heatmap indices with the highest confidence score within a local radius called kLocalMaximumRadius. Naturally, setting this value to be larger than the dimensions of the heatmap would not do any good. The original code sets this radius to a constant value of 1 so we will do the same.\nWhen decoding the key point locations for a single body, we will need to traverse from the current key point to its neighboring key point. For example, the nose neighbors both the left eye and right eye. We will keep track of which key points neighbor each other in a TupleTuple&lt;int, int&gt; array, where the values are the key point id numbers.\n/// &lt;summary&gt;\n/// Defines the size of the local window in the heatmap to look for\n/// confidence scores higher than the one at the current heatmap coordinate\n/// &lt;/summary&gt;\nconst int kLocalMaximumRadius = 1;\n\n/// &lt;summary&gt;\n/// Defines the parent-&gt;child relationships used for multipose detection.\n/// &lt;/summary&gt;\npublic static Tuple&lt;int, int&gt;[] parentChildrenTuples = new Tuple&lt;int, int&gt;[]{\n    // Nose to Left Eye\n    Tuple.Create(0, 1),\n    // Left Eye to Left Ear\n    Tuple.Create(1, 3),\n    // Nose to Right Eye\n    Tuple.Create(0, 2),\n    // Right Eye to Right Ear\n    Tuple.Create(2, 4),\n    // Nose to Left Shoulder\n    Tuple.Create(0, 5),\n    // Left Shoulder to Left Elbow\n    Tuple.Create(5, 7),\n    // Left Elbow to Left Wrist\n    Tuple.Create(7, 9), \n    // Left Shoulder to Left Hip\n    Tuple.Create(5, 11),\n    // Left Hip to Left Knee\n    Tuple.Create(11, 13), \n    // Left Knee to Left Ankle\n    Tuple.Create(13, 15),\n    // Nose to Right Shoulder\n    Tuple.Create(0, 6), \n    // Right Shoulder to Right Elbow\n    Tuple.Create(6, 8),\n    // Right Elbow to Right Wrist\n    Tuple.Create(8, 10), \n    // Right Shoulder to Right Hip\n    Tuple.Create(6, 12),\n    // Right Hip to Right Knee\n    Tuple.Create(12, 14), \n    // Right Knee to Right Ankle\n    Tuple.Create(14, 16)\n};\n\n\nCreate GetStridedIndexNearPoint Method\nIn order to traverse from a key point to its neighboring key point, we will need to downscale the key point position back down to the heatmap resolution. We can calculate the nearest heatmap indices by dividing the position by the stride value for the model and clamping the result.\n/// &lt;summary&gt;\n/// Calculate the heatmap indices closest to the provided point\n/// &lt;/summary&gt;\n/// &lt;param name=\"point\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;A vector with the nearest heatmap coordinates&lt;/returns&gt;\nstatic Vector2Int GetStridedIndexNearPoint(Vector2 point, int stride, int height, int width)\n{\n    // Downscale the point coordinates to the heatmap dimensions\n    return new Vector2Int(\n        (int)Mathf.Clamp(Mathf.Round(point.x / stride), 0, width - 1),\n        (int)Mathf.Clamp(Mathf.Round(point.y / stride), 0, height - 1)\n    );\n}\n\n\nCreate GetDisplacement Method\nThe displacement layers from the model output are used to find the location of the nearest neighboring key point. Much like the offset layer, they provide vectors that we then add to the current key point position.\n/// &lt;summary&gt;\n/// Retrieve the displacement values for the provided point\n/// &lt;/summary&gt;\n/// &lt;param name=\"edgeId\"&gt;&lt;/param&gt;\n/// &lt;param name=\"point\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacements\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;A vector witht he displacement values for the provided point&lt;/returns&gt;\nstatic Vector2 GetDisplacement(int edgeId, Vector2Int point, Tensor displacements)\n{\n    // Calculate the number of edges for the pose skeleton\n    int numEdges = (int)(displacements.channels / 2);\n    // Get the displacement values for the provided heatmap coordinates\n    return new Vector2(\n        displacements[0, point.y, point.x, numEdges + edgeId],\n        displacements[0, point.y, point.x, edgeId]\n    );\n}\n\n\nCreate TraverseToTargetKeypoint Method\nWe can use the GetStridedIndexNearPoint and GetDisplacement methods to find the location of the neighboring key point for a given Keypoint.\n\nMethod Steps\n\nGet the nearest heatmap indices for the current key point position\nGet the displacement vector for the nearest heatmap indices\nCalculate the position for a neighboring key point using the displacement vector\nGet the nearest heatmap indices for the displaced point\nRefine the location key point location with the associated offset vector\nGet the confidence score for the neighboring key point\nReturn the neighboring Keypoint\n\n/// &lt;summary&gt;\n/// Get a new keypoint along the provided edgeId for the pose instance.\n/// &lt;/summary&gt;\n/// &lt;param name=\"edgeId\"&gt;&lt;/param&gt;\n/// &lt;param name=\"sourceKeypoint\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetKeypointId\"&gt;&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacements\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;A new keypoint with the displaced coordinates&lt;/returns&gt;\nstatic Keypoint TraverseToTargetKeypoint(\n    int edgeId, Keypoint sourceKeypoint, int targetKeypointId,\n    Tensor scores, Tensor offsets, int stride,\n    Tensor displacements)\n{\n    // Get heatmap dimensions\n    int height = scores.height;\n    int width = scores.width;\n\n    // Get neareast heatmap indices for source keypoint\n    Vector2Int sourceKeypointIndices = GetStridedIndexNearPoint(\n        sourceKeypoint.position, stride, height, width);\n    // Retrieve the displacement values for the current indices\n    Vector2 displacement = GetDisplacement(edgeId, sourceKeypointIndices, displacements);\n    // Add the displacement values to the keypoint position\n    Vector2 displacedPoint = sourceKeypoint.position + displacement;\n    // Get neareast heatmap indices for displaced keypoint\n    Vector2Int displacedPointIndices =\n        GetStridedIndexNearPoint(displacedPoint, stride, height, width);\n    // Get the offset vector for the displaced keypoint indices\n    Vector2 offsetVector = GetOffsetVector(\n        displacedPointIndices.y, displacedPointIndices.x, targetKeypointId,\n        offsets);\n    // Get the heatmap value at the displaced keypoint location\n    float score = scores[0, displacedPointIndices.y, displacedPointIndices.x, targetKeypointId];\n    // Calculate the position for the displaced keypoint\n    Vector2 targetKeypoint = (displacedPointIndices * stride) + offsetVector;\n\n    return new Keypoint(score, targetKeypoint, targetKeypointId);\n}\n\n\n\nCreate DecodePose Method\nWe don’t know which key point (e.g. nose, left shoulder, right wrist) we will start from when decoding a single pose. Therefore, we will need to traverse the list of neighboring key points both forwards and backwards to get all 17 of the key points for an individual in the input image.\n\nMethod Steps\n\nInitialize a new Keypoint array\nGet the input image coordinates for the starting key point\nStore the starting key point in the Keypoint array\nIterate upwards through the list of neighboring key points\n\nConfirm that the current child key point has already been found and that the parent key point has not already been found.\n\nCall the TraverseToTargetKeypoint method to obtain neighboring key points\nStore each neighboring key point in the Keypoint array according to its id number\n\n\nIterate downwards through the list of neighboring key points\n\nConfirm that the current parent key point has already been found and that the child key point has not already been found.\n\nCall the TraverseToTargetKeypoint method to obtain neighboring key points\nStore each neighboring key point in the Keypoint array according to its id number\n\n\nReturn the Keypoint array\n\n/// &lt;summary&gt;\n/// Follows the displacement fields to decode the full pose of the object\n/// instance given the position of a part that acts as root.\n/// &lt;/summary&gt;\n/// &lt;param name=\"root\"&gt;&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacementsFwd\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacementsBwd\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;An array of keypoints for a single pose&lt;/returns&gt;\nstatic Keypoint[] DecodePose(Keypoint root, Tensor scores, Tensor offsets,\n                             int stride, Tensor displacementsFwd, Tensor displacementsBwd)\n{\n\n    Keypoint[] instanceKeypoints = new Keypoint[scores.channels];\n\n    // Start a new detection instance at the position of the root.\n    Vector2 rootPoint = GetImageCoords(root, stride, offsets);\n\n    instanceKeypoints[root.id] = new Keypoint(root.score, rootPoint, root.id);\n\n    int numEdges = parentChildrenTuples.Length;\n\n    // Decode the part positions upwards in the tree, following the backward\n    // displacements.\n    for (int edge = numEdges - 1; edge &gt;= 0; --edge)\n    {\n        int sourceKeypointId = parentChildrenTuples[edge].Item2;\n        int targetKeypointId = parentChildrenTuples[edge].Item1;\n        if (instanceKeypoints[sourceKeypointId].score &gt; 0.0f &&\n            instanceKeypoints[targetKeypointId].score == 0.0f)\n        {\n            instanceKeypoints[targetKeypointId] = TraverseToTargetKeypoint(\n                edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores,\n                offsets, stride, displacementsBwd);\n        }\n    }\n\n    // Decode the part positions downwards in the tree, following the forward\n    // displacements.\n    for (int edge = 0; edge &lt; numEdges; ++edge)\n    {\n        int sourceKeypointId = parentChildrenTuples[edge].Item1;\n        int targetKeypointId = parentChildrenTuples[edge].Item2;\n        if (instanceKeypoints[sourceKeypointId].score &gt; 0.0f &&\n            instanceKeypoints[targetKeypointId].score == 0.0f)\n        {\n            instanceKeypoints[targetKeypointId] = TraverseToTargetKeypoint(\n                edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores,\n                offsets, stride, displacementsFwd);\n        }\n    }\n\n    return instanceKeypoints;\n}\n\n\n\nCreate ScoreIsMaximumInLocalWindow Method\nAs mentioned earlier, we only consider key points with the highest confidence score in their local area as potential starting key points. We will determine whether a given key point has the highest score in a new method called ScoreIsMaximumInLocalWindow.\n\nMethod Steps\n\nCalculate the starting and ending indices for the local heatmap window\nIterate through the heatmap indices within the local window\nCompare each confidence score in the local window to the score for the provided key point\n\nReturn false if any higher scores are found\n\n\n/// &lt;summary&gt;\n/// Compare the value at the current heatmap location to the surrounding values\n/// &lt;/summary&gt;\n/// &lt;param name=\"keypointId\"&gt;&lt;/param&gt;\n/// &lt;param name=\"score\"&gt;&lt;/param&gt;\n/// &lt;param name=\"heatmapY\"&gt;&lt;/param&gt;\n/// &lt;param name=\"heatmapX\"&gt;&lt;/param&gt;\n/// &lt;param name=\"localMaximumRadius\"&gt;&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;True if the value is the highest within a given radius&lt;/returns&gt;\nstatic bool ScoreIsMaximumInLocalWindow(int keypointId, float score, int heatmapY, int heatmapX,\n                                        int localMaximumRadius, Tensor heatmaps)\n{\n    bool localMaximum = true;\n    // Calculate the starting heatmap colummn index\n    int yStart = Mathf.Max(heatmapY - localMaximumRadius, 0);\n    // Calculate the ending heatmap colummn index\n    int yEnd = Mathf.Min(heatmapY + localMaximumRadius + 1, heatmaps.height);\n\n    // Iterate through calulated range of heatmap columns\n    for (int yCurrent = yStart; yCurrent &lt; yEnd; ++yCurrent)\n    {\n        // Calculate the starting heatmap row index\n        int xStart = Mathf.Max(heatmapX - localMaximumRadius, 0);\n        // Calculate the ending heatmap row index\n        int xEnd = Mathf.Min(heatmapX + localMaximumRadius + 1, heatmaps.width);\n\n        // Iterate through calulated range of heatmap rows\n        for (int xCurrent = xStart; xCurrent &lt; xEnd; ++xCurrent)\n        {\n            // Check if the score for at the current heatmap location\n            // is the highest within the specified radius\n            if (heatmaps[0, yCurrent, xCurrent, keypointId] &gt; score)\n            {\n                localMaximum = false; \n                break;\n            }\n        }\n        if (!localMaximum) break;\n    }\n    return localMaximum;\n}\n\n\n\nCreate BuildPartList Method\nThis is where we will build the list of potential starting key points that be passed to the DecodePose method.\nMuch like the DecodeSinglePose method, we need to iterate through the entire heatmap Tensor. This time, we will only consider heatmap indices with a value above the provided score threshold. When we get to an index with a value that meets this threshold, we will call the ScoreIsMaximumInLocalWindow method to confirm that it is the highest score in its local area. The heatmap indices with the highest local score will be added to a Keypoint List.\n/// &lt;summary&gt;\n/// Iterate through the heatmaps and create a list of indicies \n/// with the highest values within the provided radius.\n/// &lt;/summary&gt;\n/// &lt;param name=\"scoreThreshold\"&gt;&lt;/param&gt;\n/// &lt;param name=\"localMaximumRadius\"&gt;&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;A list of keypoints with the highest values in their local area&lt;/returns&gt;\nstatic List&lt;Keypoint&gt; BuildPartList(float scoreThreshold, int localMaximumRadius, Tensor heatmaps)\n{\n    List&lt;Keypoint&gt; list = new List&lt;Keypoint&gt;();\n\n    // Iterate through heatmaps\n    for (int c = 0; c &lt; heatmaps.channels; c++)\n    {\n        // Iterate through heatmap columns\n        for (int y = 0; y &lt; heatmaps.height; y++)\n        {\n            // Iterate through column rows\n            for (int x = 0; x &lt; heatmaps.width; x++)\n            {\n                float score = heatmaps[0, y, x, c];\n\n                // Skip parts with score less than the scoreThreshold\n                if (score &lt; scoreThreshold) continue;\n\n                // Only add keypoints with the highest score in a local window.\n                if (ScoreIsMaximumInLocalWindow(c, score, y, x, localMaximumRadius, heatmaps))\n                {\n                    list.Add(new Keypoint(score, new Vector2(x, y), c));\n                }\n            }\n        }\n    }\n\n    return list;\n}\n\n\nCreate WithinNmsRadiusOfCorrespondingPoint Method\nWe want to make sure that any key points that have already been assigned to a body do not get used again. We can prevent this by only sending key points to the DecodePose method that are not too close to any key points in an existing Keypoint array.\n/// &lt;summary&gt;\n/// Check if the provided image coordinates are too close to any keypoints in existing poses\n/// &lt;/summary&gt;\n/// &lt;param name=\"poses\"&gt;&lt;/param&gt;\n/// &lt;param name=\"squaredNmsRadius\"&gt;&lt;/param&gt;\n/// &lt;param name=\"vec\"&gt;&lt;/param&gt;\n/// &lt;param name=\"keypointId\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;True if there are any existing poses too close to the provided coords&lt;/returns&gt;\nstatic bool WithinNmsRadiusOfCorrespondingPoint(\n    List&lt;Keypoint[]&gt; poses, float squaredNmsRadius, Vector2 vec, int keypointId)\n{\n    // SquaredDistance\n    return poses.Any(pose =&gt; (vec - pose[keypointId].position).sqrMagnitude &lt;= squaredNmsRadius);\n}\n\n\nCreate DecodeMultiplePoses Method\nThis is the method that will be called from the PoseEstimator script after executing the model. It will take in all four output Tensors from the model output along with the stride value, max number poses to decode, a minimum confidence score threshold, and the radius for determining if a key point is too close to an existing pose.\n\nMethod Steps\n\nInitialize a new List of Keypoint arrays.\nSquare the provided radius value\nCall the BuildPartList method to get the List of potential starting key points\nSort the List in descending order based on the confidence scores for the key points\nIterate through the List of starting key points\n\nCreate a copy of the key point with the highest score\nRemove the key point from the List\nGet the input image coordinates for the key point\nSkip the key point if it is too close to an existing Keypoint array\nCall the DecodePose method with the key point as the starting key point\nAdd the new Keypoint array to the List\n\nReturn the List of Keypoint arrays as an array.\n\n/// &lt;summary&gt;\n/// Detects multiple poses and finds their parts from part scores and displacement vectors. \n/// &lt;/summary&gt;\n/// &lt;param name=\"heatmaps\"&gt;&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacementsFwd\"&gt;&lt;/param&gt;\n/// &lt;param name=\"displacementBwd\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;&lt;/param&gt;\n/// &lt;param name=\"maxPoseDetections\"&gt;&lt;/param&gt;\n/// &lt;param name=\"scoreThreshold\"&gt;&lt;/param&gt;\n/// &lt;param name=\"nmsRadius\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;An array of poses up to maxPoseDetections in size&lt;/returns&gt;\npublic static Keypoint[][] DecodeMultiplePoses(\n    Tensor heatmaps, Tensor offsets,\n    Tensor displacementsFwd, Tensor displacementBwd,\n    int stride, int maxPoseDetections,\n    float scoreThreshold = 0.5f, int nmsRadius = 20)\n{\n    // Stores the final poses\n    List&lt;Keypoint[]&gt; poses = new List&lt;Keypoint[]&gt;();\n    // \n    float squaredNmsRadius = (float)nmsRadius * nmsRadius;\n\n    // Get a list of indicies with the highest values within the provided radius.\n    List&lt;Keypoint&gt; list = BuildPartList(scoreThreshold, kLocalMaximumRadius, heatmaps);\n    // Order the list in descending order based on score\n    list = list.OrderByDescending(x =&gt; x.score).ToList();\n\n    // Decode poses until the max number of poses has been reach or the part list is empty\n    while (poses.Count &lt; maxPoseDetections && list.Count &gt; 0)\n    {\n        // Get the part with the highest score in the list\n        Keypoint root = list[0];\n        // Remove the keypoint from the list\n        list.RemoveAt(0);\n\n        // Calculate the input image coordinates for the current part\n        Vector2 rootImageCoords = GetImageCoords(root, stride, offsets);\n\n        // Skip parts that are too close to existing poses\n        if (WithinNmsRadiusOfCorrespondingPoint(\n            poses, squaredNmsRadius, rootImageCoords, root.id))\n        {\n            continue;\n        }\n\n        // Find the keypoints in the same pose as the root part\n        Keypoint[] keypoints = DecodePose(\n            root, heatmaps, offsets, stride, displacementsFwd,\n            displacementBwd);\n\n        // The current list of keypoints\n        poses.Add(keypoints);\n    }\n\n    return poses.ToArray();\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 6",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nNow we can complete the ProcessOutput method in the PoseEstimator script.\n\nAdd Public Variables\nFirst, we will add some public variables so that we can adjust the max number of poses to decode, score threshold, and radius for determining whether a key point is too close to an existing pose from the Inspector tab.\n[Tooltip(\"The maximum number of posees to estimate\")]\n[Range(1, 20)]\npublic int maxPoses = 20;\n\n[Tooltip(\"The score threshold for multipose estimation\")]\n[Range(0, 1.0f)]\npublic float scoreThreshold = 0.25f;\n\n[Tooltip(\"Non-maximum suppression part distance\")]\npublic int nmsRadius = 100;\n\n\nModify ProcessOutput Method\nWe will assign the output from the DecodeMultiplePoses to the poses variable.\n// Determine the key point locations\nposes = Utils.DecodeMultiplePoses(\n    heatmaps, offsets,\n    displacementFWD, displacementBWD,\n    stride: stride, maxPoseDetections: maxPoses,\n    scoreThreshold: scoreThreshold, \n    nmsRadius: nmsRadius);\n\nFull Code\n/// &lt;summary&gt;\n/// Obtains the model output and either decodes single or mutlple poses\n/// &lt;/summary&gt;\n/// &lt;param name=\"engine\"&gt;&lt;/param&gt;\nprivate void ProcessOutput(IWorker engine)\n{\n    // Get the model output\n    Tensor heatmaps = engine.PeekOutput(predictionLayer);\n    Tensor offsets = engine.PeekOutput(offsetsLayer);\n    Tensor displacementFWD = engine.PeekOutput(displacementFWDLayer);\n    Tensor displacementBWD = engine.PeekOutput(displacementBWDLayer);\n\n    // Calculate the stride used to scale down the inputImage\n    int stride = (imageDims.y - 1) / (heatmaps.shape.height - 1);\n    stride -= (stride % 8);\n\n    if (estimationType == EstimationType.SinglePose)\n    {\n        poses = new Utils.Keypoint[1][];\n\n        // Determine the key point locations\n        poses[0] = Utils.DecodeSinglePose(heatmaps, offsets, stride);\n    }\n    else\n    {\n        // Determine the key point locations\n        poses = Utils.DecodeMultiplePoses(\n            heatmaps, offsets,\n            displacementFWD, displacementBWD,\n            stride: stride, maxPoseDetections: maxPoses,\n            scoreThreshold: scoreThreshold, \n            nmsRadius: nmsRadius);\n    }\n\n    heatmaps.Dispose();\n    offsets.Dispose();\n    displacementFWD.Dispose();\n    displacementBWD.Dispose();\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-6/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 6",
    "section": "Summary",
    "text": "Summary\nWe now have everything need to perform pose estimation. However, we cannot currently gauge the accuracy of the estimated poses. In the next post, we will demonstrate how to add pose skeletons so that we can compare the estimated key point locations to the source video feed.\nPrevious: Part 5\nNext: Part 7\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-7/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/part-7/index.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 7",
    "section": "",
    "text": "Overview\nCreate PoseSkeleton Script\nUpdate PoseEstimator Script\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#overview",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 7",
    "section": "Overview",
    "text": "Overview\nIn this post, we will cover how to create pose skeletons so that we can compare the estimated key point locations to the source video feed."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#create-poseskeleton-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#create-poseskeleton-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 7",
    "section": "Create PoseSkeleton Script",
    "text": "Create PoseSkeleton Script\nWe will implement the functionality for creating pose skeletons in a new script. Open the Scripts folder in the Assets section and create a new C# script called PoseSkeleton. The PoseSkeleton class will handle creating a single pose skeleton and updating the positions of its key points. We will be creating as many PoseSkeleton instances as is specified by the maxPoses variable in the PoseEstimator script.\n\nAdd Required Namespace\nWe need to add the System namespace as we will once again be using the Tuple class.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing System;\n\n\nRemove MonoBehaviour Inheritance\nThe PoseSkeleton class does not need to be a MonoBehaviour so we can remove it.\npublic class PoseSkeleton\n\n\nAdd Variables\nWe will need a Transform array to keep track of the positions of the key point objects in the scene.\nWe also need a GameObject array to store the lines connecting the key point objects.\nNext, we will create a static string array to store the names of the key points predicted by the model. The names will be ordered based on their key point id number (e.g. nose is in index 0).\nThe number of key point predicted by the model will not change, so we will store the number in a static int variable.\nMuch like the parentChildrenTuples variable in the Utils script, we will create a Tuple array to keep track of which key points should be connected by lines. We could actually just use the pairs from parentChildrenTuples, but the skeleton would look a bit weird.\n\n\n\n\n\nInstead, we will make a pose skeleton that looks like this.\n\n\n\n\n\nTo help distinguish the different body areas, we will create a Color array so that we can specify what color we want each line to be.\nLastly, we need is a float variable to specify the line width for the pose skeleton lines.\n// The list of key point GameObjects that make up the pose skeleton\npublic Transform[] keypoints;\n\n// The GameObjects that contain data for the lines between key points\nprivate GameObject[] lines;\n\n// The names of the body parts that will be detected by the PoseNet model\nprivate static string[] partNames = new string[]{\n    \"nose\", \"leftEye\", \"rightEye\", \"leftEar\", \"rightEar\", \"leftShoulder\",\n    \"rightShoulder\", \"leftElbow\", \"rightElbow\", \"leftWrist\", \"rightWrist\",\n    \"leftHip\", \"rightHip\", \"leftKnee\", \"rightKnee\", \"leftAnkle\", \"rightAnkle\"\n};\n\nprivate static int NUM_KEYPOINTS = partNames.Length;\n\n// The pairs of key points that should be connected on a body\nprivate Tuple&lt;int, int&gt;[] jointPairs = new Tuple&lt;int, int&gt;[]{\n    // Nose to Left Eye\n    Tuple.Create(0, 1),\n    // Nose to Right Eye\n    Tuple.Create(0, 2),\n    // Left Eye to Left Ear\n    Tuple.Create(1, 3),\n    // Right Eye to Right Ear\n    Tuple.Create(2, 4),\n    // Left Shoulder to Right Shoulder\n    Tuple.Create(5, 6),\n    // Left Shoulder to Left Hip\n    Tuple.Create(5, 11),\n    // Right Shoulder to Right Hip\n    Tuple.Create(6, 12),\n    // Left Shoulder to Right Hip\n    Tuple.Create(5, 12),\n    // Rigth Shoulder to Left Hip\n    Tuple.Create(6, 11),\n    // Left Hip to Right Hip\n    Tuple.Create(11, 12),\n    // Left Shoulder to Left Elbow\n    Tuple.Create(5, 7),\n    // Left Elbow to Left Wrist\n    Tuple.Create(7, 9), \n    // Right Shoulder to Right Elbow\n    Tuple.Create(6, 8),\n    // Right Elbow to Right Wrist\n    Tuple.Create(8, 10),\n    // Left Hip to Left Knee\n    Tuple.Create(11, 13), \n    // Left Knee to Left Ankle\n    Tuple.Create(13, 15),\n    // Right Hip to Right Knee\n    Tuple.Create(12, 14), \n    // Right Knee to Right Ankle\n    Tuple.Create(14, 16)\n};\n\n// Colors for the skeleton lines\nprivate Color[] colors = new Color[] {\n    // Head\n    Color.magenta, Color.magenta, Color.magenta, Color.magenta,\n    // Torso\n    Color.red, Color.red, Color.red, Color.red, Color.red, Color.red,\n    // Arms\n    Color.green, Color.green, Color.green, Color.green,\n    // Legs\n    Color.blue, Color.blue, Color.blue, Color.blue\n};\n\n// The width for the skeleton lines\nprivate float lineWidth;\n\n// The material for the key point objects\nprivate Material keypointMat;\n\n\nCreate InitializeLine Method\nThe first method we will create will handle the initialization of a single line in the pose skeleton.\n\nMethod Steps\n\nGet the starting and ending joint pair indices to indicate what two key point are being connected\nUse the names of the two key points to create the name for the line object\nCreate a new standard GameObject\nAdd a LineRenderer component to the new GameObject\nCreate a new Material for the line with the appropriate color from the Color array\nIndicate that the line with only have two points\nSet the line width\n\n/// &lt;summary&gt;\n/// Create a line between the key point specified by the start and end point indices\n/// &lt;/summary&gt;\n/// &lt;param name=\"pairIndex\"&gt;&lt;/param&gt;\n/// &lt;param name=\"startIndex\"&gt;&lt;/param&gt;\n/// &lt;param name=\"endIndex\"&gt;&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;&lt;/param&gt;\n/// &lt;param name=\"color\"&gt;&lt;/param&gt;\nprivate void InitializeLine(int pairIndex, float width, Color color)\n{\n    int startIndex = jointPairs[pairIndex].Item1;\n    int endIndex = jointPairs[pairIndex].Item2;\n\n    // Create new line GameObject\n    string name = $\"{keypoints[startIndex].name}_to_{keypoints[endIndex].name}\";\n    lines[pairIndex] = new GameObject(name);\n\n    // Add LineRenderer component\n    LineRenderer lineRenderer = lines[pairIndex].AddComponent&lt;LineRenderer&gt;();\n    // Make LineRenderer Shader Unlit\n    lineRenderer.material = new Material(Shader.Find(\"Unlit/Color\"));\n    // Set the material color\n    lineRenderer.material.color = color;\n\n    // The line will consist of two points\n    lineRenderer.positionCount = 2;\n\n    // Set the width from the start point\n    lineRenderer.startWidth = width;\n    // Set the width from the end point\n    lineRenderer.endWidth = width;\n}\n\n\n\nCreate InitializeSkeleton Method\nWe will call the InitializeLine method for each joint pair in jointPairs.\n/// &lt;summary&gt;\n/// Initialize the pose skeleton\n/// &lt;/summary&gt;\nprivate void InitializeSkeleton()\n{\n    for (int i = 0; i &lt; jointPairs.Length; i++)\n    {\n        InitializeLine(i, lineWidth, colors[i]);\n    }\n}\n\n\nCreate Constructor\nNow we can define the class constructor that will initialize the pose skeleton.\n\nMethod Steps\n\nInitialize the keypoints array\nCreate a new material for the key point objects\nCreate a new GameObject for each key point\n\nCreate a sphere GameObject\nSet the position to the origin\nSet the size of the GameObject using the provided pointScale value\nAssign the new material\nSet the name for the object\n\nSet the lineWidth value\nInitialize the lines array\nCall the the InitializeSkeleton method\n\npublic PoseSkeleton(float pointScale = 10f, float lineWidth = 5f)\n{\n    this.keypoints = new Transform[NUM_KEYPOINTS];\n\n    Material keypointMat = new Material(Shader.Find(\"Unlit/Color\"));\n    keypointMat.color = Color.yellow;\n\n    for (int i = 0; i &lt; NUM_KEYPOINTS; i++)\n    {\n        this.keypoints[i] = GameObject.CreatePrimitive(PrimitiveType.Sphere).transform;\n        this.keypoints[i].position = new Vector3(0, 0, 0);\n        this.keypoints[i].localScale = new Vector3(pointScale, pointScale, 0);\n        this.keypoints[i].gameObject.GetComponent&lt;MeshRenderer&gt;().material = keypointMat;\n        this.keypoints[i].gameObject.name = partNames[i];\n    }\n\n    this.lineWidth = lineWidth;\n\n    // The number of joint pairs\n    int numPairs = keypoints.Length + 1;\n    // Initialize the lines array\n    lines = new GameObject[numPairs];\n\n    // Initialize the pose skeleton\n    InitializeSkeleton();\n}\n\n\n\nCreate ToggleSkeleton Method\nJust because we initialize a given number of pose skeletons based on the value for maxPoses does not mean that the model will find that many poses in an input image. We will need to hide the excess skeletons when they are not needed. We will hide the skeletons by deactivating the associated key point and line objects. We can use the same function to unhide the skeleton when it is needed. This method will be called from the PoseEstimator script, so it needs to be public.\n/// &lt;summary&gt;\n/// Toggles visibility for the skeleton\n/// &lt;/summary&gt;\n/// &lt;param name=\"show\"&gt;&lt;/param&gt;\npublic void ToggleSkeleton(bool show)\n{\n    for (int i= 0; i &lt; jointPairs.Length; i++)\n    {\n        lines[i].SetActive(show);\n        keypoints[jointPairs[i].Item1].gameObject.SetActive(show);\n        keypoints[jointPairs[i].Item2].gameObject.SetActive(show);\n    }\n}\n\n\nCreate Cleanup Method\nWhen we reduce the max number of poses to estimate, we should remove the skeletons that are no longer needed. This method is nearly identical to the ToggleSkeleton except that we will be destroying the objects rather than deactivating them.\n/// &lt;summary&gt;\n/// Clean up skeleton GameObjects\n/// &lt;/summary&gt;\npublic void Cleanup()\n{\n    for (int i = 0; i &lt; jointPairs.Length; i++)\n    {\n        GameObject.Destroy(lines[i]);\n        GameObject.Destroy(keypoints[jointPairs[i].Item1].gameObject);\n        GameObject.Destroy(keypoints[jointPairs[i].Item2].gameObject);\n    }\n}\n\n\nCreate UpdateKeyPointPositions Method\nWe will update the key point positions with the latest model output in a new method called UpdateKeyPointPositions.\nThis method will take in the following as input:\n\nKeypoint array for a single pose\nThe scale value to scale the key point positions from the input resolution to the source resolution\nThe source RenderTexture\nA bool to indicate whether to mirror the key point positions when using a webcam\nA float value to indicate the minimum confidence score a key point needs to have to be displayed\n\n\nMethod Steps\n\nIterate through the Keypoint array\nHide the key point objects that do not meet the minimum confidence score\nScale the key point positions from the input resolution up to the source resolution\nFlip the Y axis coordinates vertically to compensate for the difference between heatmap indices and scene coordinates\nMirror the X axis coordinates if using a webcam\nUpdate the key point object positions with the new coordinate values\n\n/// &lt;summary&gt;\n/// Update the positions for the key point GameObjects\n/// &lt;/summary&gt;\n/// &lt;param name=\"keypoints\"&gt;&lt;/param&gt;\n/// &lt;param name=\"sourceScale\"&gt;&lt;/param&gt;\n/// &lt;param name=\"sourceTexture\"&gt;&lt;/param&gt;\n/// &lt;param name=\"mirrorImage\"&gt;&lt;/param&gt;\n/// &lt;param name=\"minConfidence\"&gt;&lt;/param&gt;\npublic void UpdateKeyPointPositions(Utils.Keypoint[] keypoints,\n                                    float sourceScale, RenderTexture sourceTexture, bool mirrorImage, float minConfidence)\n{\n    // Iterate through the key points\n    for (int k = 0; k &lt; keypoints.Length; k++)\n    {\n        // Check if the current confidence value meets the confidence threshold\n        if (keypoints[k].score &gt;= minConfidence / 100f)\n        {\n            // Activate the current key point GameObject\n            this.keypoints[k].GetComponent&lt;MeshRenderer&gt;().enabled = true;\n        }\n        else\n        {\n            // Deactivate the current key point GameObject\n            this.keypoints[k].GetComponent&lt;MeshRenderer&gt;().enabled = false;\n        }\n\n        // Scale the keypoint position to the original resolution\n        Vector2 coords = keypoints[k].position * sourceScale;\n\n        // Flip the keypoint position vertically\n        coords.y = sourceTexture.height - coords.y;\n\n        // Mirror the x position if using a webcam\n        if (mirrorImage) coords.x = sourceTexture.width - coords.x;\n\n        // Update the current key point location\n        // Set the z value to -1f to place it in front of the video screen\n        this.keypoints[k].position = new Vector3(coords.x, coords.y, -1f);\n    }\n}\n\n\n\nCreate UpdateLines Method\nOnce we have update the positions of the key point objects in the scene, we need to update the starting and ending coordinates for the skeleton lines. We will do so in a new method called UpdateLines.\n\nMethod Steps\n\nGet references to the starting and ending key point objects\nCheck if both the starting and ending key point objects are visible\n\nIf true\n\nMake the line object active\nUpdate the starting position for the line\nUpdate the ending positions for the line\n\nif false, deactivate the line object\n\n\n/// &lt;summary&gt;\n/// Draw the pose skeleton based on the latest location data\n/// &lt;/summary&gt;\npublic void UpdateLines()\n{\n    // Iterate through the joint pairs\n    for (int i = 0; i &lt; jointPairs.Length; i++)\n    {\n        // Set the GameObject for the starting key point\n        Transform startingKeyPoint = keypoints[jointPairs[i].Item1];\n        // Set the GameObject for the ending key point\n        Transform endingKeyPoint = keypoints[jointPairs[i].Item2];\n\n        // Check if both the starting and ending key points are active\n        if (startingKeyPoint.GetComponent&lt;MeshRenderer&gt;().enabled &&\n            endingKeyPoint.GetComponent&lt;MeshRenderer&gt;().enabled)\n        {\n            // Activate the line\n            lines[i].SetActive(true);\n\n            LineRenderer lineRenderer = lines[i].GetComponent&lt;LineRenderer&gt;();\n            // Update the starting position\n            lineRenderer.SetPosition(0, startingKeyPoint.position);\n            // Update the ending position\n            lineRenderer.SetPosition(1, endingKeyPoint.position);\n        }\n        else\n        {\n            // Deactivate the line\n            lines[i].SetActive(false);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 7",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nBack in the PoseEstimator script, we need to add some new variables to use the PoseSkeleton class.\n\nAdd Public Variables\nWe will add a couple public float variables for setting the size of the key point objects and the width of the skeleton lines.\nWe will also add a public int variable to specify the minimum confidence value a key point need to have for it to be used for the pose skeleton.\n[Tooltip(\"The size of the pose skeleton key points\")]\npublic float pointScale = 10f;\n\n[Tooltip(\"The width of the pose skeleton lines\")]\npublic float lineWidth = 5f;\n\n[Tooltip(\"The minimum confidence level required to display the key point\")]\n[Range(0, 100)]\npublic int minConfidence = 70;\n\n\nAdd Private Variables\nLastly we will declare a PoseSkeleton array to store the pose skeletons.\n// Array of pose skeletons\nprivate PoseSkeleton[] skeletons;\n\n\nCreate InitializeSkeletons Method\nWe will create a new method called InitializeSkeletons to populate the skeletons array. When the performing single pose estimation, the max number of poses will be set to 1. This method will be called in the Start method as well as any time the maxPoses value gets updated.\n/// &lt;summary&gt;\n/// Initialize pose skeletons\n/// &lt;/summary&gt;\nprivate void InitializeSkeletons()\n{\n    // Initialize the list of pose skeletons\n    if (estimationType == EstimationType.SinglePose) maxPoses = 1;\n    skeletons = new PoseSkeleton[maxPoses];\n\n    // Populate the list of pose skeletons\n    for (int i = 0; i &lt; maxPoses; i++) skeletons[i] = new PoseSkeleton(pointScale, lineWidth);\n}\n\n\nModify Start Method\nWe will call the InitializeSkeletons method at the end of the Start method.\n// Initialize pose skeletons\nInitializeSkeletons();\n\nFull Code\n// Start is called before the first frame update\nvoid Start()\n{\n    if (useWebcam)\n    {\n        // Limit application framerate to the target webcam framerate\n        Application.targetFrameRate = webcamFPS;\n\n        // Create a new WebCamTexture\n        webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n\n        // Start the Camera\n        webcamTexture.Play();\n\n        // Deactivate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = false;\n\n        // Update the videoDims.y\n        videoDims.y = webcamTexture.height;\n        // Update the videoDims.x\n        videoDims.x = webcamTexture.width;\n    }\n    else\n    {\n        // Update the videoDims.y\n        videoDims.y = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().height;\n        // Update the videoDims.x\n        videoDims.x = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().width;\n    }\n\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(videoDims.x, videoDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(videoDims.x, videoDims.y, useWebcam);\n\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n\n    // Adjust the input dimensions to maintain the source aspect ratio\n    aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n    targetDims.x = (int)(imageDims.y * aspectRatioScale);\n    imageDims.x = targetDims.x;\n\n    // Initialize the RenderTexture that will store the processed input image\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    // Initialize the Barracuda inference engine based on the selected model\n    InitializeBarracuda();\n\n    // Initialize pose skeletons\n    InitializeSkeletons();\n}\n\n\n\nModify Update Method\nAt the end of the Update method, we need to first check if the maxPoses value has been updated.\nWe then need to calculate the scale value to upscale the key point positions from the input image resolution to the source video resolution.\nWe can then iterate through the pose skeletons in the skeleton array. If there are more pose skeletons than poses returned by the ProcessOutput method, we will hide the extra pose skeletons.\n// Reinitialize pose skeletons\nif (maxPoses != skeletons.Length)\n{\n    foreach (PoseSkeleton skeleton in skeletons)\n    {\n        skeleton.Cleanup();\n    }\n\n    // Initialize pose skeletons\n    InitializeSkeletons();\n}\n\n// The smallest dimension of the videoTexture\nint minDimension = Mathf.Min(videoTexture.width, videoTexture.height);\n\n// The value used to scale the key point locations up to the source resolution\nfloat scale = (float)minDimension / Mathf.Min(imageDims.x, imageDims.y);\n\n// Update the pose skeletons\nfor (int i = 0; i &lt; skeletons.Length; i++)\n{\n    if (i &lt;= poses.Length - 1)\n    {\n        skeletons[i].ToggleSkeleton(true);\n\n        // Update the positions for the key point GameObjects\n        skeletons[i].UpdateKeyPointPositions(poses[i], scale, videoTexture, useWebcam, minConfidence);\n        skeletons[i].UpdateLines();\n    }\n    else\n    {\n        skeletons[i].ToggleSkeleton(false);\n    }\n}\n\nFull Code\n// Update is called once per frame\nvoid Update()\n{\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam) Graphics.Blit(webcamTexture, videoTexture);\n\n    // Prevent the input dimensions from going too low for the model\n    imageDims.x = Mathf.Max(imageDims.x, 64);\n    imageDims.y = Mathf.Max(imageDims.y, 64);\n\n    // Update the input dimensions while maintaining the source aspect ratio\n    if (imageDims.x != targetDims.x)\n    {\n        aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n        targetDims.y = (int)(imageDims.x * aspectRatioScale);\n        imageDims.y = targetDims.y;\n        targetDims.x = imageDims.x;\n    }\n    if (imageDims.y != targetDims.y)\n    {\n        aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n        targetDims.x = (int)(imageDims.y * aspectRatioScale);\n        imageDims.x = targetDims.x;\n        targetDims.y = imageDims.y;\n    }\n\n    // Update the rTex dimensions to the new input dimensions\n    if (imageDims.x != rTex.width || imageDims.y != rTex.height)\n    {\n        RenderTexture.ReleaseTemporary(rTex);\n        // Assign a temporary RenderTexture with the new dimensions\n        rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, rTex.format);\n    }\n\n    // Copy the src RenderTexture to the new rTex RenderTexture\n    Graphics.Blit(videoTexture, rTex);\n\n    // Prepare the input image to be fed to the selected model\n    ProcessImage(rTex);\n\n    // Reinitialize Barracuda with the selected model and backend \n    if (engine.modelType != modelType || engine.workerType != workerType)\n    {\n        engine.worker.Dispose();\n        InitializeBarracuda();\n    }\n\n    // Execute neural network with the provided input\n    engine.worker.Execute(input);\n    // Release GPU resources allocated for the Tensor\n    input.Dispose();\n\n    // Decode the keypoint coordinates from the model output\n    ProcessOutput(engine.worker);\n    \n    // Reinitialize pose skeletons\n    if (maxPoses != skeletons.Length)\n    {\n        foreach (PoseSkeleton skeleton in skeletons)\n        {\n            skeleton.Cleanup();\n        }\n\n        // Initialize pose skeletons\n        InitializeSkeletons();\n    }\n\n    // The smallest dimension of the videoTexture\n    int minDimension = Mathf.Min(videoTexture.width, videoTexture.height);\n\n    // The value used to scale the key point locations up to the source resolution\n    float scale = (float)minDimension / Mathf.Min(imageDims.x, imageDims.y);\n\n    // Update the pose skeletons\n    for (int i = 0; i &lt; skeletons.Length; i++)\n    {\n        if (i &lt;= poses.Length - 1)\n        {\n            skeletons[i].ToggleSkeleton(true);\n\n            // Update the positions for the key point GameObjects\n            skeletons[i].UpdateKeyPointPositions(poses[i], scale, videoTexture, useWebcam, minConfidence);\n            skeletons[i].UpdateLines();\n        }\n        else\n        {\n            skeletons[i].ToggleSkeleton(false);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/part-7/index.html#summary",
    "title": "Barracuda PoseNet Tutorial 2nd Edition Pt. 7",
    "section": "Summary",
    "text": "Summary\nNow we can compare the estimated key point locations to the source video feed.\nPrevious: Part 6\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "",
    "text": "Overview\nDownload GitHub Project\nUpdate Barracuda Library\nRemove Video Player\nAdd Image Effect Shaders\nCreate Scriptable Objects\nUpdate PoseEstimator Script\nConfigure PoseEstimator Component\nBuild Project\nSummary"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#overview",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#overview",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Overview",
    "text": "Overview\nUntil recently, it was infeasible to use the Barracuda inference engine in WebGL builds. CPU inference was single-thread only, and we could not use the GPU at all. However, updates to the Barracuda library added a Pixel Shader worker type. This new worker type enables GPU inference even when the target platform does not support Compute Shaders.\nWe will cover how to leverage the Pixel Shader worker type to run the PoseNet project in a web browser.\nYou can try out a live demo at the link below.\n\nBarracuda PoseNet WebGL Demo"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#download-github-project",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#download-github-project",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Download GitHub Project",
    "text": "Download GitHub Project\nFirst, we need to clone the existing PoseNet project from GitHub. Once downloaded, open the project in the Unity Editor.\nGitHub Repository: Barracuda-PoseNet-Tutorial"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#update-barracuda-library",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#update-barracuda-library",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Update Barracuda Library",
    "text": "Update Barracuda Library\nOpen the package manager window and navigate to the Barracuda package. We can see there is a new 3.0.0 version available. At the time of writing, this was the most recent version of the package. Click the Update to 3.0.0 button to start the update process.\n\n\n\n\n\nA popup window will open stating we need to restart the editor. Click the OK button, then close and reopen the project."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#remove-video-player",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#remove-video-player",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Remove Video Player",
    "text": "Remove Video Player\nWebGL builds do not support playing local video files. The video files would need to be hosted on a web server and accessed via URL. Since real-world applications would likely take input from a webcam, we will focus on that instead. Select the VideoScreen object in the Hierarchy tab.\n\n\n\n\n\nRight-click the Video Player component in the Inspector tab and select Remove Component."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#add-image-effect-shaders",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#add-image-effect-shaders",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Add Image Effect Shaders",
    "text": "Add Image Effect Shaders\nThe new version of Barracuda also added support for creating a Tensor from a RenderTexture even when Compute Shaders are not available. We can implement the preprocessing steps using Image Effect Shaders to avoid downloading the texture data to the CPU.\n\nDefine Shaders\nOpen the Shaders folder in the Assets section. Right-click to open the context menu and select Create → Shader → Image Effect Shader.\n\n\n\n\n\nWe will need to create two of these shaders for the MobileNet and ResNet models, respectively. Name the two shaders ProcessMobileNet and ProcessResNet.\n\n\n\n\n\nWe will start by modifying the MobileNet shader. Open the file in the code editor. First, replace the default context menu string of Hidden/ProcessMobileNet with PoseNet/ProcessMobileNet.\nShader \"PoseNet/ProcessResNet\"\n{\n    Properties\n    {\n        _MainTex (\"Texture\", 2D) = \"white\" {}\n    }\nThen, replace the fixed4 frag function with the code below. This code implements the same steps as in the PoseNetShader Compute Shader.\nfloat4 frag(v2f i) : SV_Target\n{\n    float4 col = tex2D(_MainTex, i.uv);\n    col.r = 2.0 * col.r / 1.0 - 1.0;\n    col.g = 2.0 * col.g / 1.0 - 1.0;\n    col.b = 2.0 * col.b / 1.0 - 1.0;\n    return col;\n}\nWe can perform the same steps for the ResNet shader. This time, name the shader PoseNet/ProcessResNet and replace the fixed4 frag function with the code below.\nfloat4 frag(v2f i) : SV_Target\n{\n    float4 col = tex2D(_MainTex, i.uv);\n    col.r = col.r * 255.0 - 123.15;\n    col.g = col.g * 255.0 - 115.90;\n    col.b = col.b * 255.0 - 103.06;\n    return col;\n}\n\n\nCreate Materials\nWe need to attach the shaders to materials to apply them using the Graphics.Blit() function. Right-click to open the context menu and select Create → Material.\n\n\n\n\n\nWe need materials for both the MobileNet and ResNet models. We can use the same names as the shaders.\n\n\n\n\n\nSelect the MobileNet Material, then drag the MobileNet Shader into the Inspector Tab.\n\n\n\n\n\nDo the same for the ResNetMaterial."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#create-scriptable-objects",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#create-scriptable-objects",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Create Scriptable Objects",
    "text": "Create Scriptable Objects\nIt can be cumbersome to keep track of the individual requirements for the different versions of the PoseNet model. We can tidy things up a bit using Scriptable Objects.\n\nDefine PoseNetModel Scriptable Object\nOpen the Assets → Scripts folder and create a new C# script. Name it PoseNetModel and open it in the code editor.\n\n\n\n\n\nReplace the MonoBehaviour inheritance with ScriptableObject and delete the default methods. Add the Barracuda namespace as well.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\n\npublic class PoseNetModel : ScriptableObject\n{\n    \n}\nHere, we will keep track of the model asset, processing material, and output layer indices for individual versions of the PoseNet model.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\n\npublic class PoseNetModel : ScriptableObject\n{\n    [Tooltip(\"The ONNX model asset file to use when performing inference\")]\n    public NNModel modelAsset;\n\n    [Tooltip(\"The material with the required preprocessing shader\")]\n    public Material preprocessingMaterial;\n\n    [Tooltip(\"The index for the heatmap output layer\")]\n    public int heatmapLayerIndex = 0;\n    [Tooltip(\"The index for the offsets output layer\")]\n    public int offsetsLayerIndex = 1;\n    [Tooltip(\"The index for the forward displacement layer\")]\n    public int displacementFWDLayerIndex;\n    [Tooltip(\"The index for the backwared displacement layer\")]\n    public int displacementBWDLayerIndex;\n}\nLastly, We need to provide a name for the context menu. That way, we can create instances of this scriptable object in the Assets section.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\n\n[CreateAssetMenu(menuName = \"PoseNet Model\")]\npublic class PoseNetModel : ScriptableObject\n{\n    [Tooltip(\"The ONNX model asset file to use when performing inference\")]\n    public NNModel modelAsset;\n\n    [Tooltip(\"The material with the required preprocessing shader\")]\n    public Material preprocessingMaterial;\n\n    [Tooltip(\"The index for the heatmap output layer\")]\n    public int heatmapLayerIndex = 0;\n    [Tooltip(\"The index for the offsets output layer\")]\n    public int offsetsLayerIndex = 1;\n    [Tooltip(\"The index for the forward displacement layer\")]\n    public int displacementFWDLayerIndex;\n    [Tooltip(\"The index for the backwared displacement layer\")]\n    public int displacementBWDLayerIndex;\n}\n\n\nCreate PoseNetModel Assets\nBack in the Assets section, right-click to open the context menu and select Create → PoseNet Model to create a new instance of the Scriptable Object.\n\n\n\n\n\nName the .asset file MobileNet. With the asset selected, drag the Assets/Models/mobilenet.onnx file into the Model Asset field in the Inspector Tab.\n\n\n\n\n\nNext, drag the associated processing material into the Preprocessing Material field. Enter 2 and 3 for the forward and backward displacement layer indices, respectively.\n\n\n\n\n\nCreate a second instance called ResNet and assign the associated ONNX file and Material. This time, enter 3 for the forward displacement layer and 2 for the backward displacement layer."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#update-poseestimator-script",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#update-poseestimator-script",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Update PoseEstimator Script",
    "text": "Update PoseEstimator Script\nNow we can update the PoseEstimator script to work with WebGL. It might be easier to delete the current class content and start fresh.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\n\npublic class PoseEstimator : MonoBehaviour\n{\n    \n}\nWe will only go over the bare minimum changes required for using WebGL to prevent this post from getting too long. A more developed project is available at the link at the bottom of this page.\n\nDefine Public Variables\nMost of the public variables remain the same as the original project. The only new addition is public PoseNetModel[] models variable. This variable will store the instances of the PoseNetModel Scriptable Object we created earlier.\npublic enum EstimationType\n{\n    MultiPose,\n    SinglePose\n}\n\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n\n[Tooltip(\"The requested webcam frame rate\")]\npublic int webcamFPS = 60;\n\n[Tooltip(\"The screen for viewing the input source\")]\npublic Transform videoScreen;\n\n[Tooltip(\"Stores the models available for inference\")]\npublic PoseNetModel[] models;\n\n[Tooltip(\"The dimensions of the image being fed to the model\")]\npublic Vector2Int imageDims = new Vector2Int(256, 256);\n\n[Tooltip(\"The backend to use when performing inference\")]\npublic WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\n\n[Tooltip(\"The type of pose estimation to be performed\")]\npublic EstimationType estimationType = EstimationType.SinglePose;\n\n[Tooltip(\"The maximum number of posees to estimate\")]\n[Range(1, 20)]\npublic int maxPoses = 20;\n\n[Tooltip(\"The score threshold for multipose estimation\")]\n[Range(0, 1.0f)]\npublic float scoreThreshold = 0.25f;\n\n[Tooltip(\"Non-maximum suppression distance\")]\npublic int nmsRadius = 100;\n\n[Tooltip(\"The size of the pose skeleton key points\")]\npublic float pointScale = 10f;\n\n[Tooltip(\"The width of the pose skeleton lines\")]\npublic float lineWidth = 5f;\n\n[Tooltip(\"The minimum confidence level required to display a key point\")]\n[Range(0, 100)]\npublic int minConfidence = 70;\n\n\nDefine Private Variables\nWe only need to define one new private variable to keep track of the current PoseNetModel in use.\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n\n// The source video texture\nprivate RenderTexture videoTexture;\n\n// Target dimensions for model input\nprivate Vector2Int targetDims;\n\n// Used to scale the input image dimensions while maintaining aspect ratio\nprivate float aspectRatioScale;\n\n// The texture used to create the input tensor\nprivate RenderTexture rTex;\n\n// Stores the input data for the model\nprivate Tensor input;\n\n// The interface used to execute the neural network\nprivate IWorker engine;\n\n// The name for the heatmap layer in the model asset\nprivate string heatmapLayer;\n\n// The name for the offsets layer in the model asset\nprivate string offsetsLayer;\n\n// The name for the forwards displacement layer in the model asset\nprivate string displacementFWDLayer;\n\n// The name for the backwards displacement layer in the model asset\nprivate string displacementBWDLayer;\n\n// The name for the Sigmoid layer that returns the heatmap predictions\nprivate string predictionLayer = \"heatmap_predictions\";\n\n// Stores the current estimated 2D keypoint locations in videoTexture\nprivate Utils.Keypoint[][] poses;\n\n// Array of pose skeletons\nprivate PoseSkeleton[] skeletons;\n\n// Stores the PoseNetModel currently in use\nprivate PoseNetModel currentModel;\n\n\nDefine InitializeVideoScreen Method\nThe InitializeVideoScreen method is nearly identical to the original project. The only differences are that is no VideoPlayer component, and we will initialize the video texture inside this method instead of the start method.\n/// &lt;summary&gt;\n/// Prepares the videoScreen GameObject to display the chosen video source.\n/// &lt;/summary&gt;\n/// &lt;param name=\"width\"&gt;&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;&lt;/param&gt;\nprivate void InitializeVideoScreen(int width, int height, bool mirrorScreen)\n{\n    // Release temporary RenderTexture\n    RenderTexture.ReleaseTemporary(videoTexture);\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.ARGBHalf);\n\n    if (mirrorScreen)\n    {\n        // Flip the VideoScreen around the Y-Axis\n        videoScreen.rotation = Quaternion.Euler(0, 180, 0);\n        // Invert the scale value for the Z-Axis\n        videoScreen.localScale = new Vector3(videoScreen.localScale.x, videoScreen.localScale.y, -1f);\n    }\n\n    // Apply the new videoTexture to the VideoScreen Gameobject\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.shader = Shader.Find(\"Unlit/Texture\");\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.SetTexture(\"_MainTex\", videoTexture);\n    // Adjust the VideoScreen dimensions for the new videoTexture\n    videoScreen.localScale = new Vector3(width, height, videoScreen.localScale.z);\n    // Adjust the VideoScreen position for the new videoTexture\n    videoScreen.position = new Vector3(width / 2, height / 2, 1);\n}\n\n\nDefine InitializeCamera Method\nThe InitializeCamera method is nearly identical to the original project. The only difference is that we will use the video texture directly to adjust the camera position and size.\n/// &lt;summary&gt;\n/// Resizes and positions the in-game Camera to accommodate the video dimensions\n/// &lt;/summary&gt;\nprivate void InitializeCamera()\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject mainCamera = GameObject.Find(\"Main Camera\");\n    // Adjust the camera position to account for updates to the VideoScreen\n    mainCamera.transform.position = new Vector3(videoTexture.width / 2, videoTexture.height / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    mainCamera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the VideoScreen\n    mainCamera.GetComponent&lt;Camera&gt;().orthographicSize = videoTexture.height / 2;\n}\n\n\nDefine InitializeBarracuda Method\nThe steps for the InitializeBarracuda method are the same as the original project. We only need to update it to use the currentModel variable.\n/// &lt;summary&gt;\n/// Updates the output layer names based on the selected model architecture\n/// and initializes the Barracuda inference engine with the selected model.\n/// &lt;/summary&gt;\nprivate void InitializeBarracuda()\n{\n    // The compiled model used for performing inference\n    Model m_RunTimeModel;\n\n    // Compile the model asset into an object oriented representation\n    m_RunTimeModel = ModelLoader.Load(currentModel.modelAsset);\n\n    // Get output layer names\n    heatmapLayer = m_RunTimeModel.outputs[currentModel.heatmapLayerIndex];\n    offsetsLayer = m_RunTimeModel.outputs[currentModel.offsetsLayerIndex];\n    displacementFWDLayer = m_RunTimeModel.outputs[currentModel.displacementFWDLayerIndex];\n    displacementBWDLayer = m_RunTimeModel.outputs[currentModel.displacementBWDLayerIndex];\n\n    // Create a model builder to modify the m_RunTimeModel\n    ModelBuilder modelBuilder = new ModelBuilder(m_RunTimeModel);\n\n    // Add a new Sigmoid layer that takes the output of the heatmap layer\n    modelBuilder.Sigmoid(predictionLayer, heatmapLayer);\n\n    // Validate if backend is supported, otherwise use fallback type.\n    workerType = WorkerFactory.ValidateType(workerType);\n\n    // Create a worker that will execute the model with the selected backend\n    engine = WorkerFactory.CreateWorker(workerType, modelBuilder.model);\n}\n\n\nDefine InitializeSkeletons Method\nThe InitializeSkeletons method is unchanged from the original project.\n/// &lt;summary&gt;\n/// Initialize pose skeletons\n/// &lt;/summary&gt;\nprivate void InitializeSkeletons()\n{\n    // Initialize the list of pose skeletons\n    if (estimationType == EstimationType.SinglePose) maxPoses = 1;\n    skeletons = new PoseSkeleton[maxPoses];\n\n    // Populate the list of pose skeletons\n    for (int i = 0; i &lt; maxPoses; i++) skeletons[i] = new PoseSkeleton(pointScale, lineWidth);\n}\n\n\nDefine InitializeInputDims Method\nWe will implement the steps for updating the texture dimensions inside a new method rather than the Update method. That way, we can call this method when the input dimensions are updated using a GUI rather than the Inspector tab. The actual steps remain the same.\n/// &lt;summary&gt;\n/// Initialize the input dimensions for the model\n/// &lt;/summary&gt;\nprivate void InitializeInputDims()\n{\n    // Prevent the input dimensions from going too low for the model\n    imageDims.x = Mathf.Max(imageDims.x, 64);\n    imageDims.y = Mathf.Max(imageDims.y, 64);\n\n    // Update the input dimensions while maintaining the source aspect ratio\n    if (imageDims.y != targetDims.y)\n    {\n        aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n        targetDims.x = (int)(imageDims.y * aspectRatioScale);\n        imageDims.x = targetDims.x;\n        targetDims.y = imageDims.y;\n    }\n\n    if (imageDims.x != targetDims.x)\n    {\n        aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n        targetDims.y = (int)(imageDims.x * aspectRatioScale);\n        imageDims.y = targetDims.y;\n        targetDims.x = imageDims.x;\n    }\n\n    RenderTexture.ReleaseTemporary(rTex);\n    // Assign a temporary RenderTexture with the new dimensions\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGBHalf);\n}\n\n\nDefine InitializePoseEstimator Method\nWe will define a new function that calls the initialization methods.\n/// &lt;summary&gt;\n/// Perform initialization steps\n/// &lt;/summary&gt;\nprivate void InitializePoseEstimator()\n{\n    // Initialize the Barracuda inference engine based on the selected model\n    InitializeBarracuda();\n\n    // Initialize pose skeletons\n    InitializeSkeletons();\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(webcamTexture.width, webcamTexture.height, true);\n\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n\n    // Initialize input dimensions\n    InitializeInputDims();\n}\n\n\nDefine Start Method\nIn the Start method, we only need to create a new webcam texture, start the camera and initialize the currentModel variable. For real-world applications, we would update this value using a GUI. For now, we can just set it to the first element in the models array.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n    // Start the Camera\n    webcamTexture.Play();\n\n    // Default to the first PoseNetModel in the list\n    currentModel = models[0];\n}\n\n\nDefine ProcessImage Method\nFor the ProcessImage method, we no longer need to worry about processing input on the CPU. We only need to call the Graphics.Blit method using the Material for the current model.\n/// &lt;summary&gt;\n/// Calls the appropriate preprocessing function to prepare\n/// the input for the selected model\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;&lt;/param&gt;\nprivate void ProcessImage(RenderTexture image)\n{\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    RenderTexture.active = result;\n\n    // Apply preprocessing steps\n    Graphics.Blit(image, result, currentModel.preprocessingMaterial);\n\n    // Create a new Tensor\n    input = new Tensor(result, channels: 3);\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nDefine ProcessOutput Method\nThe ProcessOutput method is unchanged from the original project.\n/// &lt;summary&gt;\n/// Obtains the model output and either decodes single or mutlple poses\n/// &lt;/summary&gt;\n/// &lt;param name=\"engine\"&gt;&lt;/param&gt;\nprivate void ProcessOutput(IWorker engine)\n{\n    // Get the model output\n    Tensor heatmaps = engine.PeekOutput(predictionLayer);\n    Tensor offsets = engine.PeekOutput(offsetsLayer);\n    Tensor displacementFWD = engine.PeekOutput(displacementFWDLayer);\n    Tensor displacementBWD = engine.PeekOutput(displacementBWDLayer);\n\n    // Calculate the stride used to scale down the inputImage\n    int stride = (imageDims.y - 1) / (heatmaps.shape.height - 1);\n    stride -= (stride % 8);\n\n    if (estimationType == EstimationType.SinglePose)\n    {\n        // Initialize the array of Keypoint arrays\n        poses = new Utils.Keypoint[1][];\n\n        // Determine the key point locations\n        poses[0] = Utils.DecodeSinglePose(heatmaps, offsets, stride);\n    }\n    else\n    {\n        // Determine the key point locations\n        poses = Utils.DecodeMultiplePoses(\n            heatmaps, offsets,\n            displacementFWD, displacementBWD,\n            stride: stride, maxPoseDetections: maxPoses,\n            scoreThreshold: scoreThreshold,\n            nmsRadius: nmsRadius);\n    }\n\n    // Release the resources allocated for the output Tensors\n    heatmaps.Dispose();\n    offsets.Dispose();\n    displacementFWD.Dispose();\n    displacementBWD.Dispose();\n}\n\n\nDefine Update Method\nMost web browsers will likely require user permission to access the webcam. The webcam texture will not fully initialize until the user allows access. We will add a check at the top of the Update method to prevent the rest of the code from executing until the webcam texture initializes.\nThe default resolution for a webcam texture is 16x16. It seems reasonable to assume any webcam would have a higher resolution. Therefore we will use this as our condition to check when the webcam texture initializes.\nThe rest of the Update method is nearly identical to the original project with one critical addition. We need to call Resources.UnloadUnusedAssets at the end of the Update method. The UnloadUnusedAssets method will prevent GPU memory from filling up.\n// Update is called once per frame\nvoid Update()\n{\n    // Skip the rest of the method if the webcam is not initialized\n    if (webcamTexture.width &lt;= 16) return;\n\n    // Only perform initialization steps if the videoTexture has not been initialized\n    if (!videoTexture) InitializePoseEstimator();\n\n    // Copy webcamTexture to videoTexture\n    Graphics.Blit(webcamTexture, videoTexture);\n\n    // Copy the videoTexture data to rTex\n    Graphics.Blit(videoTexture, rTex);\n\n    // Prepare the input image to be fed to the selected model\n    ProcessImage(rTex);\n\n    // Execute neural network with the provided input\n    engine.Execute(input);\n    // Release GPU resources allocated for the Tensor\n    input.Dispose();\n\n    // Decode the keypoint coordinates from the model output\n    ProcessOutput(engine);\n\n    // Reinitialize pose skeletons\n    if (maxPoses != skeletons.Length)\n    {\n        foreach (PoseSkeleton skeleton in skeletons)\n        {\n            skeleton.Cleanup();\n        }\n\n        // Initialize pose skeletons\n        InitializeSkeletons();\n    }\n\n    // The smallest dimension of the videoTexture\n    int minDimension = Mathf.Min(videoTexture.width, videoTexture.height);\n\n    // The value used to scale the key point locations up to the source resolution\n    float scale = (float)minDimension / Mathf.Min(imageDims.x, imageDims.y);\n\n    // Update the pose skeletons\n    for (int i = 0; i &lt; skeletons.Length; i++)\n    {\n        if (i &lt;= poses.Length - 1)\n        {\n            skeletons[i].ToggleSkeleton(true);\n\n            // Update the positions for the key point GameObjects\n            skeletons[i].UpdateKeyPointPositions(poses[i], scale, videoTexture, true, minConfidence);\n            skeletons[i].UpdateLines();\n        }\n        else\n        {\n            skeletons[i].ToggleSkeleton(false);\n        }\n    }\n    Resources.UnloadUnusedAssets();\n}\n\n\nDefine OnDisable Method\nFor the OnDisable method, we only need to update the line to release the resources for the inference engine to engine.Dispose().\nprivate void OnDisable()\n{\n    // Release the resources allocated for the inference engine\n    engine.Dispose();\n}"
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#configure-poseestimator-component",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#configure-poseestimator-component",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Configure PoseEstimator Component",
    "text": "Configure PoseEstimator Component\nBefore we test the modified project, we need to assign the MobileNet and ResNet assets to the Pose Estimator component. Select the PoseEstimator object in the Hierarchy tab. Then drag the MobileNet and ResNet assets onto the Models field in the Inspector tab. Make sure to place the model you want to test in the Element 0 field. Lastly, open the Worker Type dropdown and select Pixel Shader."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#build-project",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#build-project",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Build Project",
    "text": "Build Project\nPress Ctrl+B to build and run the project. Select or create a Build folder in the popup window.\n\n\n\n\n\nThe first build attempt will likely fail, with Unity displaying lots of errors and warnings in the console. The console error messages all say that Bitwise integer instructions are not supported on GLES 2.\n\n\n\n\n\nHowever, if we press Ctrl+B again, the build will complete successfully.\n\n\n\n\n\nOnce the project build is complete, a new window will open, and the project will start.\n\n\n\n\n\nThere should be a prompt asking for permission to access the webcam. If not, try refreshing the page.\n\n\n\n\n\nThe project should automatically perform the initialization steps once the browser accesses the webcam."
  },
  {
    "objectID": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#summary",
    "href": "posts/barracuda-posenet-tutorial-v2/webgl/index.html#summary",
    "title": "Barracuda PoseNet WebGL Tutorial",
    "section": "Summary",
    "text": "Summary\nWe now have a way to use the Barracuda inference engine in WebGL builds.\nPrevious: Part 7\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "",
    "text": "Introduction\nSelect a Unity Project\nInstall Barracuda Package\nCreate Style Transfer Folder\nImport Models\nPrepare Render Textures\nCreate a Compute Shader\nCreate StyleTransfer Script\nSummary"
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#introduction",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#introduction",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Introduction",
    "text": "Introduction\nUnity has finally released the in-game style transfer project they’ve been teasing in the Barracuda documentation. Their implementation is slightly more polished than my early attempts. And by slightly, I mean they seem to have addressed every major complaint I had with my implementation. Be sure to check out their sample project as well as the accompanying blog post.\nIt’s exciting that Unity has started releasing projects that explore alternative uses for the Barracuda library. Hopefully, they’ll explore other deep learning applications in future projects. I would love to see projects that use GANs for dynamically generating in-game content.\nI plan to work on a more sophisticated implementation for in-game style transfer in the future, perhaps using some tricks from Unity’s implementation. However, I wanted to start with a basic implementation to serve as a baseline.\nThis tutorial will cover how to use trained models from the fast_neural_style project provided by PyTorch. The models take in regular images and return stylized versions. We’ll get our input images from the in-game camera and display the stylized output to the user.\nImportant: This is meant as a simple proof of concept and requires a powerful GPU to get playable frame rates. An RTX 20-series equivalent or newer is recommended."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#select-a-unity-project",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#select-a-unity-project",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Select a Unity Project",
    "text": "Select a Unity Project\nI’ll be using the Kinematica_Demo project provided by Unity for this tutorial. It provides a great character model for testing different styles. However, feel free to follow along with a different project. This one is a bit large and takes a while to open the first time.\n\nDownload Kinematica Demo\nYou can download the Unity project by clicking on the link below. The zipped folder is approximately 1.2 GB.\n\nKinematica_Demo_0.8.0-preview: (download)\n\n\n\nAdd Project to Unity Hub\nOnce downloaded, unzip the folder and add the project to Unity Hub using the Add button.\n\n\n\n\n\n\n\nSet the Unity Version\nSelect a Unity version from the drop-down menu. The demo project was made using Unity 2019.4.5f1. You can use a later 2019.4 release if you don’t have that version installed.\n\nUnity 2019.4.13: (download)\n\n\n\n\n\n\n\n\nOpen the Project\nNow we can open the project. We’ll be prompted to upgrade the project to the selected Unity version. Click Confirm in the popup to upgrade the project. As mentioned earlier, this project takes a while to load the first time."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#install-barracuda-package",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#install-barracuda-package",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nWe’ll install the Barracuda package once the project has finished loading. Select the Package Manager tab in the Unity editor and type Barracuda into the search box.\n\n\n\n\n\nClick the Install button to install the package."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#create-style-transfer-folder",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#create-style-transfer-folder",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Create Style Transfer Folder",
    "text": "Create Style Transfer Folder\nWe’ll place all our additions to the project in a new asset folder called Style_Transfer. This will help keep things organized."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#import-models",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#import-models",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Import Models",
    "text": "Import Models\nNext, we need to add some style transfer models. PyTorch models need to be exported to the ONNX format before being imported to Unity. Fortunately, PyTorch provides built-in support for exporting to ONNX (tutorial).\n\nDownload ONNX Files\nYou can download some exported style transfer models from the links below.\n\nMosaic: (download)\n\n\n\n\n\nVan Gogh Starry Night: (download)\n\n\n\n\n\n\n\n\nImport ONNX Files to Assets\nOpen the Style_Transfer folder and make a new folder called Models.\n\n\n\n\n\nDrag and drop the ONNX files into the Models folder."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#prepare-render-textures",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#prepare-render-textures",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Prepare Render Textures",
    "text": "Prepare Render Textures\nOur basic process will involve taking the current frame from the in-game camera, feeding it to the model, getting the output, and displaying the processed output to the user. We’ll store the current camera frame and processed output in separate render textures.\n\nCreate Textures Folder\nAdd a new folder called Textures in the Style_Transfer folder.\n\n\n\n\n\n\n\nCreate Asset Files\nOpen the Textures folder and create two new Render Texture assets.\n\n\n\n\n\nName the new assets CameraInput, ProcessedOutput.\n\n\n\n\n\n\n\nUpdate Size Parameters\nWe need to use a fairly low resolution to get playable frame rates. Click an empty space in the Textures folder and press Ctrl-a to select both render textures. Set the size the parameter to 720 x 540 in the Inspector tab. Feel free to try higher resolutions if you happen to have an RTX 30-series or equivalent GPU."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#create-a-compute-shader",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#create-a-compute-shader",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Create a Compute Shader",
    "text": "Create a Compute Shader\nWe can perform both the preprocessing and postprocessing operations on the GPU since both the input and output are images. We’ll implement these steps in a compute shader.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Shaders. Enter the Shaders folder and right-click an empty space. Select Shader in the Create submenu and click Compute Shader. We’ll name it StyleTransferShader.\n\n\n\n\n\n\n\nRemove the Default Code\nOpen the StyleTransferShader in your code editor. By default, the ComputeShader will contain the following.\n\n\n\n\n\nDelete the CSMain function along with the #pragma kernel CSMain. Next, we need to add a Texture2D variable to store the input image. Name it InputImage and give it a data type of &lt;half4&gt;. Use the same data type for the Result variable as well.\n\n\n\n\n\n\n\nCreate ProcessInput Function\nThe style transfer models expect RGB channel values to be in the range [0, 255]. Color values in Unity are in the range [0,1]. Therefore, we need to scale the three channel values for the InputImage by 255. We’ll perform this step in a new function called ProcessInput as shown below.\n\n\n\n\n\n\n\nCreate ProcessOutput Function\nThe models are supposed to output an image with RGB channel values in the range [0, 255]. However, it can sometimes return values a little outside that range. We can use the built-in clamp() method to make sure all values are in the correct range. We’ll then scale the values back down to [0, 1] for Unity. We’ll perform these steps in a new function called ProcessOutput as shown below.\n\n\n\n\n\nNow that we’ve created our ComputeShader, we need to execute it using a C# script."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#create-styletransfer-script",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#create-styletransfer-script",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Create StyleTransfer Script",
    "text": "Create StyleTransfer Script\nWe need to make a new C# script to perform inference with the style transfer models. This script will load the model, process the input, run the model, and process the output.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Scripts. In the Scripts folder, right-click an empty space and select C# Script in the Create submenu.\n\n\n\n\n\nName the script StyleTransfer.\n\n\n\n\n\n\n\nAdd Unity.Barracuda Namespace\nOpen the StyleTransfer script and add the Unity.Barracuda namespace at the top of the script.\n\n\n\n\n\n\n\nCreate RenderTexture Variables\nWe need to create some public variables that we can use to access our two render texture assets in the script.\n\n\n\n\n\n\n\nCreate StyleTransferShader Variable\nNext, we’ll add a public variable to access our compute shader.\n\n\n\n\n\n\n\nCreate Barracuda Variables\nNow we need to add a few variables to perform inference with the style transfer models.\n\nCreate modelAsset Variable\nMake a new public NNModel variable called modelAsset. We’ll assign one of the ONNX files to this variable in the Unity Editor.\n\n\n\n\n\n\n\nCreate workerType Variable\nWe’ll also add a variable that let’s us choose which backend to use when performing inference. The options are divided into CPU and GPU. Our preprocessing pipeline runs entirely on the GPU so we’ll be sticking with the GPU options for this tutorial series.\nMake a new public WorkerFactory.Type called workerType. Give it a default value of WorkerFactory.Type.Auto.\n\n\n\n\n\n\n\nCreate m_RuntimeModel Variable\nWe need to compile the modelAsset into a run-time model to perform inference. We’ll store the compiled model in a new private Model variable called m_RuntimeModel.\n\n\n\n\n\n\n\nCreate engine Variable\nNext, we’ll create a new private IWorker variable to store our inference engine. Name the variable engine.\n\n\n\n\n\n\n\n\nCompile the Model\nWe need to get an object oriented representation of the model before we can work with it. We’ll do this in the Start() method and store it in the m_RuntimeModel.\n\n\n\n\n\n\n\nInitialize Inference Engine\nNow we can create a worker to execute the modified model using the selected backend. We’ll do this using the WorkerFactory.CreateWorker() method.\n\n\n\n\n\n\n\nRelease Inference Engine Resources\nWe need to manually release the resources that get allocated for the inference engine. This should be one of the last actions performed. Therefore, we’ll do it in the OnDisable() method. This method gets called when the Unity project exits.\n\n\n\n\n\n\n\nCreate ToTexture2D() Method\nWe’ll make a new method to copy the data from a RenderTexture to a new Texture2D. We’ll need to call this method before performing both the preprocessing and postprocessing steps. The method will take in the source RenderTexture and the format for the new Texture2D.\n\n\n\n\n\n\n\nCreate ProcessImage() Method\nNext, we’ll make a new method to execute the ProcessInput() and ProcessOutput() functions in our ComputeShader. This method will take in the image that needs to be processed as well as a function name to indicate which function we want to execute. We’ll need to store the processed images in textures with HDR formats. This will allow us to use color values outside the default range of [0, 1]. As mentioned previously, the model expects values in the range of [0, 255].\n\n\n\n\n\n\n\nProcess Input Image\nNow we can process the current camera frame. We’ll call the ToTexture2D() method at the top of the Update method. The cameraInput is not an HDR texture so we’ll use an SDR format for the new Texture2D. We’ll then call the ProcessImage() method with new Texture2D as input.\n\n\n\n\n\n\n\nPerform Inference\nNext, we’ll feed the processedImage to the model and get the output. We first need to convert the processedImage to a Tensor.\n\n\n\n\n\nWe’ll then use the engine.Execute() method to run the model with the current input. We can store the raw output from the model in a new Tensor.\n\n\n\n\n\n\n\nProcess the Output\nWe need to process the raw output from the model before we can display it to the user. We’ll first copy the model output to a new HDR RenderTexture.\n\n\n\n\n\nWe’ll then copy the data to a Texture2D and pass it to the ProcessImage() method. This time we’ll be executing the ProcessOutput() function on the ComputeShader.\n\n\n\n\n\n\n\nDisplay the Processed Output\nWe can finally display the stylized image by using the Graphics.Blit() method to copy the final image to processedOutput.\n\n\n\n\n\nNext, we’ll need to modify the project scene to use the StyleTransfer script."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#open-the-biped-scene",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#open-the-biped-scene",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Open the Biped Scene",
    "text": "Open the Biped Scene\nIn the Assets window, open the Scenes folder and double-click on the Biped.unity asset. You don’t need to save the current scene if you get prompted to do so."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#create-style-converter",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#create-style-converter",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Create Style Converter",
    "text": "Create Style Converter\nTo run the StyleTransfer script, we need to attach it to a GameObject in the scene.\n\nCreate an Empty GameObject\nIn the Hierarchy tab, right-click an empty space and select Create Empty from the menu. Name the empty GameObject StyleConverter.\n\n\n\n\n\n\n\nAttach the StyleTransfer Script\nWith the StyleConverter object selected, drag and drop the StyleTransfer script into the Inspector tab.\n\n\n\n\n\n\n\nAssign the Assets\nWe need to assign the render textures, compute shader and one of the ONNX files to their respective parameters in the Inspector tab. I’ll start with the mosaic model. We’ll also set the Worker Type to Compute Precompiled."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#set-camera-target-texture",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#set-camera-target-texture",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Set Camera Target Texture",
    "text": "Set Camera Target Texture\nSelect the _Scene object in the Hierarchy tab. In the dropdown, select the Main Camera object."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#create-a-screen",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#create-a-screen",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Create a Screen",
    "text": "Create a Screen\nRight-click an empty space in the Hierarchy tab and select Raw Image in the UI submenu. Name it Screen.\n\n\n\n\n\n\nAdjust the Anchor Presets\nWith the Screen object selected, click on the anchor presets box in the Inspector tab outlined below.\n\n\n\n\n\nSelect the option in the bottom right corner that’s outlined below.\n\n\n\n\n\nNext we need to set all the Rect Transform values to zero. This will cause the Screen to take up the entire display.\n\n\n\n\n\n\n\nSet the Screen Texture\nWith the Screen object still selected, drag and drop the ProcessedOutput asset into the Texture parameter in the Inspector tab.\n\n\n\n\n\n\n\nAdjust the Game Tab\nOur last step is to set up the game tab for our chosen resolution.\n\nSet the Aspect Ratio\nMy chosen resolution of 720 x 540 has a 4:3 aspect ratio. You can change the aspect ratio in the drop-down menu.\n\n\n\n\n\n\n\nDisable Warning\nYou might see a warning saying that there isn’t a camera rendering. This would be because we set the camera to render to CameraInput. If you do, right-click the Game tab and uncheck the Warn if No Cameras Rendering option."
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#test-it-out",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#test-it-out",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Test it Out",
    "text": "Test it Out\nWe can finally press the play button and see how it looks.\nVideo"
  },
  {
    "objectID": "posts/basic-in-game-style-transfer-tutorial/index.html#summary",
    "href": "posts/basic-in-game-style-transfer-tutorial/index.html#summary",
    "title": "Basic In-Game Style Transfer Tutorial (Outdated)",
    "section": "Summary",
    "text": "Summary\nWe now have a basic implementation of in-game style transfer. It’s pretty inefficient and probably needs a seizure warning. I started with this model architecture for it’s relative simplicity but it was not designed for real-time video. I was surprised to get playable frame rates even at this low of a resolution.\nDespite it’s shortcomings, this little demo provides a glimpse at what’s possible. It can also serve as a decent testing environment for trying out different styles. It’s worth noting that the models used in this tutorial were trained on datasets of real-world photos and not video games. I might try making an training dataset using screenshots from video games and see what impact that has on the stylized images.\nI already have another style transfer project that I want to try to get working in Unity. This project does a great job of generating consistent video output (i.e. no seizure warning). In the mean time, I recommend checking out Unity’s sample project. They put a lot of work into optimizing it for playable frame rates at more reasonable resolutions.\n\nGitHub Repository"
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "",
    "text": "The Tensoflow.js library is great for leveraging machine learning directly in a web browser or Node.js application. However, TensorFlow does not currently provide any methods for converting TensorFlow.js models back into a standard TensorFlow format. This can be a problem if you need to change how your model is deployed at some point and don’t have access to a standard TensorFlow format. In addition, standard TensorFlow formats have not been made available for most of the pretrained TFJS models. Fortunately, there is a third-party library that provides this functionality. This post will cover how to use this library to convert a TFJS model to the standard SavedModel format."
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#motivation",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#motivation",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "",
    "text": "The Tensoflow.js library is great for leveraging machine learning directly in a web browser or Node.js application. However, TensorFlow does not currently provide any methods for converting TensorFlow.js models back into a standard TensorFlow format. This can be a problem if you need to change how your model is deployed at some point and don’t have access to a standard TensorFlow format. In addition, standard TensorFlow formats have not been made available for most of the pretrained TFJS models. Fortunately, there is a third-party library that provides this functionality. This post will cover how to use this library to convert a TFJS model to the standard SavedModel format."
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#about-the-tool",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#about-the-tool",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "About the Tool",
    "text": "About the Tool\nThe aptly named TensorFlow.js Graph Model Converter library allows you to convert a TFJS graph model to either a TensorFlow frozen graph or SavedModel format. Which format is best depends on your intended use case. However, the SavedModel format provides much more flexibility and can still be trained. The frozen graph format can only be used for inference. Therefore, we will use the SavedModel format for this tutorial.\nThe library can either be used as a command-line tool or accessed through an API within Python. The API provides more advanced functionality such as combining multiple TFJS models into a single SavedModel. However, using the command-line tool is faster for simple conversions. We will stick with the command-line tool for this tutorial."
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#requirements",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#requirements",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "Requirements",
    "text": "Requirements\nTo follow along with this example, you will need:\n\nA python environment with tensorflow 2.1+ and tensorflowjs 1.5.2+ installed\nA pretrained TFJS model (download)\n\nThe TFJS model used for this tutorial is a PoseNet model with a ResNet architecture. You can download the folder containing the TFJS model here. Once you’ve downloaded the zip file, extract the posenet-resnet-stride16 folder.\nA pretrained TFJS model consists of the following files:\n\nA JSON file that defines the model topology\nOne or more .bin files that contain the trained weights\n\nNote: When downloading a TFJS model from from somewhere like TensorFlow Hub, make sure the JSON file isn’t corrupted. If you open up the JSON file, you should see something like this:\n\"format\": \"graph-model\",\n    \"generatedBy\": \"1.13.1\",\n    \"convertedBy\": \"TensorFlow.js Converter v1.1.2\",\n    \"modelTopology\": {\n        \"node\": [{\n            \"name\": \"sub_2\",\n            \"op\": \"Placeholder\",\n            \"attr\": {\n                \"shape\": {\n                    \"shape\": {\n                        \"dim\": [{\n                            \"size\": \"1\"\n                        }, {\n                            \"size\": \"-1\"\n                        }, {\n                            \"size\": \"-1\"\n                        }, {\n                            \"size\": \"3\"\n                        }]\n                    }\n                },\n                \"dtype\": {\n                    \"type\": \"DT_FLOAT\"\n                }\n            }\n        }"
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#usage",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#usage",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nYou can install the library using pip:\npip install tfjs-graph-converter\n\n\nSteps\n\nOpen the posenet-resnet-stride16 folder in a terminal.\nTo convert the TFJS model into a SavedModel, you need to specify the path to the JSON file, the path to a folder that the SavedModel will be saved to, and the output format. The tool will create the folder if it doesn’t exist.\nFor example:\n\nJSON file: model-stride16.json\nSave Folder: savedmodel\nOutput Format: tf_saved_model\n\n\ntfjs_graph_converter ./model-stride16.json ./savedmodel --output_format tf_saved_model\nIf all goes well, you should see something like this:\nTensorFlow.js Graph Model Converter\n\nGraph model:    ./model-stride16.json\nOutput:         ./savedmodel\nTarget format:  tf_saved_model\n\nConverting.... Done.\nConversion took 2.778s\nNote: Some newer TFJS models released by Google use new types of layers in their Neural Network architecture that are not yet supported by the converter library at the time of writing.\nThe savedmodel folder should contain:\n\nA variables folder (which is empty for this example)\nA saved_model.pb file.\n\n(Optional) If you wish, you can examine the SavedModel using the following command:\nsaved_model_cli show --dir ./savedmodel --all\nThis command should return the following output:\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['sub_2'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (1, -1, -1, 3)\n        name: sub_2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['float_heatmaps'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (1, -1, -1, 17)\n        name: float_heatmaps:0\n    outputs['float_short_offsets'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (1, -1, -1, 34)\n        name: float_short_offsets:0\n    outputs['resnet_v1_50/displacement_bwd_2/BiasAdd'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (1, -1, -1, 32)\n        name: resnet_v1_50/displacement_bwd_2/BiasAdd:0\n    outputs['resnet_v1_50/displacement_fwd_2/BiasAdd'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (1, -1, -1, 32)\n        name: resnet_v1_50/displacement_fwd_2/BiasAdd:0\n  Method name is: tensorflow/serving/predict\nHere, you can see descriptions for the input and output layers of the model.\n(Optional) You can rename layers when converting the model by using the --rename option.\ntfjs_graph_converter ./model-stride16.json ./savedmodel --output_format tf_saved_model --rename float_short_offsets:offsets,float_heatmaps:heatmaps,sub_2:input"
  },
  {
    "objectID": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#next-steps",
    "href": "posts/convert-tfjs-to-tensorflow-savedmodel-tutorial/index.html#next-steps",
    "title": "How to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel",
    "section": "Next Steps",
    "text": "Next Steps\nBe sure to check out the GitHub repo if you want to learn what else you can do with this library or request (or add) support for new layer types.\nOnce you’ve successfully converted your TFJS model to standard TensorFlow, you have a lot more options for working with the model. A few of them are listed below.\n\nWork with the model in Python using standard TensorFlow.\nConvert the model to a TensorFlow Lite format and deploy it to mobile and IoT devices.\nConvert the model to non-TensorFlow formats such as ONNX."
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "",
    "text": "Overview\nStreamlit\nSimple Stock Price\nSimple Bioinformatics DNA Count\nEDA Basketball\nEDA Cryptocurrency\nClassification Iris Data\nRegression Boston Housing Data\nDeploy App to Heroku\nDeploy App to Streamlit Sharing"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#overview",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#overview",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Chanin Nantasenamat’s video on creating data science web apps with Streamlit."
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#streamlit",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#streamlit",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Streamlit",
    "text": "Streamlit\n\nStreamlit - The fastest way to build and share data apps\nTurns data scripts into shareable web apps\npip install streamlit\nTest Installation: streamlit hello\nRun apps: streamlit run main.py\nFormat text using Markdown"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#simple-stock-price",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#simple-stock-price",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Simple Stock Price",
    "text": "Simple Stock Price\nGet market data from Yahoo! Finance API\n\nyfinance python package\nHow to Get Stock Data Using Python\n\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nyfinance\n\npip install yfinance\n\n\nreplit: Simple_Stock_Price\nimport yfinance as yf\nimport streamlit as st\nimport pandas as pd\n\n# Write text in Markdown format\nst.write(\"\"\"\n# Simple Stock Price App\n\nShown are the stock closing price and volume of iPath Global Carbon ETN!\n\n\"\"\")\n\n# https://towardsdatascience.com/how-to-get-stock-data-using-python-c0de1df17e75\n# define the ticker symbol\ntickerSymbol = 'GRN'\n# get data on this ticker\ntickerData = yf.Ticker(tickerSymbol)\n\n# get the historical pricess for this ticker\n# Open High Low Close Volume Dividends Stock Splits\ntickerDf = tickerData.history(period='1d', start='2019-12-27', end='2021-12-27')\n\n# Create streamlit line charts\nst.write(\"\"\"\n## Closing Price\n\"\"\")\nst.line_chart(tickerDf.Close)\nst.write(\"\"\"\n## Trading Volume\n\"\"\")\nst.line_chart(tickerDf.Volume)"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#simple-bioinformatics-dna-count",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#simple-bioinformatics-dna-count",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Simple Bioinformatics DNA Count",
    "text": "Simple Bioinformatics DNA Count\nCount the number of nucleotides 'A', 'T', 'G', 'C' in entered in a text box\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nAltair\n\npip install altair\n\nPillow\n\npip install pillow\n\n\nreplit: Simple_Bioinformatics_DNA_Count\n# Import dependencies\nimport pandas as pd\nimport streamlit as st\nimport altair as alt\nfrom PIL import Image\n\n# Page Title\n# Add hero image\nimage = Image.open('dna-ge3ed05159_1920.jpg')\nst.image(image, use_column_width=True)\n\nst.write(\"\"\"\n# DNA Nucleotide Count Web App\nThis app counts the nucleotide composition of query DNA!\n\n***\n\"\"\")\n\n# Input Text Box\n#st.sidebar.header('Enter DNA sequence')\nst.header('Enter DNA sequence')\n\nsequence_input = \"&gt;DNA Query\\nGAACACGTGGAGGCAAACAGGAAGGTGAAGAAGAACTTATCCTATCAGGACGGAAGGTCCTGTGCTCGGG\\nATCTTCCAGACGTCGCGACTCTAAATTGCCCCCTCTGAGGTCAAGGAACACAAGATGGTTTTGGAAATGC\\nTGAACCCGATACATTATAACATCACCAGCATCGTGCCTGAAGCCATGCCTGCTGCCACCATGCCAGTCCT\"\n\nsequence = st.text_area(\"Sequence input\", sequence_input, height=250)\n# Split input text by line\nsequence = sequence.splitlines()\n# Skip the sequence name (first line)\nsequence = sequence[1:]\n# Concatenate list to string\nsequence = ''.join(sequence)\n\nst.write(\"\"\"\n***\n\"\"\")\n\n# Print the input DNA sequence\nst.header('INPUT (DNA Query)')\nsequence\n\n# DNA nucleotide count\nst.header('OUTPUT (DNA Nucleotide Count)')\n\n# 1. Print dictionary\nst.subheader('1. Prince dictionary')\ndef DNA_nucleotide_count(seq):\n  d = dict([\n    ('A', seq.count('A')),\n    ('T', seq.count('T')),\n    ('G', seq.count('G')),\n    ('C', seq.count('C'))\n  ])\n  return d\n\nX = DNA_nucleotide_count(sequence)\n\nX\n\n# 2. Print text\nst.subheader('2. Print text')\nst.write('There are ' + str(X['A']) + ' adenine (A)')\nst.write('There are ' + str(X['T']) + ' thymine (T)')\nst.write('There are ' + str(X['G']) + ' guanine (G)')\nst.write('There are ' + str(X['C']) + ' cytosine (C)')\n\n# 3. Display DataFrame\nst.subheader('3. Display DataFrame')\ndf = pd.DataFrame.from_dict(X, orient='index')\ndf = df.rename({0: 'count'}, axis='columns')\ndf.reset_index(inplace=True)\ndf = df.rename(columns={'index': 'nucleotide'})\nst.write(df)\n\n# 4. Display Bar Chart using Altair\nst.subheader('4. Display Bar chart')\np = alt.Chart(df).mark_bar().encode(\n  x='nucleotide',\n  y='count'\n)\n\np = p.properties(\n  # Controls width of bar\n  width=alt.Step(80)\n)\nst.write(p)"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#eda-basketball",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#eda-basketball",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "EDA Basketball",
    "text": "EDA Basketball\nScrape NBA player stats from a website and perform exploratory data analysis.\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nMatplotlib\n\npip install matplotlib\n\nSeaborn\n\npip install seaborn\n\nNumpy\n\npip install numpy\n\nlxml\n\npip install lxml\n\n\nData Source\nBasketball Statistics and History\nreplit: EDA_Basketball\nimport streamlit as st\nimport pandas as pd\nimport base64\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nst.title('NBA Player Stats Explorer')\n\nst.markdown(\"\"\"\nThis app performs simple websraping of NBA player stats data!\n* **Python libraries:** base64, pandas, streamlit\n* **Data source:** [Basketball-reference.com](https://www.basketball-reference.com/)\n\"\"\")\n\nst.sidebar.header('User Input Features')\nselected_year = st.sidebar.selectbox('Year', list(reversed(range(1950,2020))))\n\n# Web scraping of NBA player stats\n@st.cache\ndef load_data(year):\n  url = f'https://www.basketball-reference.com/leagues/NBA_{year}_per_game.html'\n  html = pd.read_html(url, header=0)\n  df = html[0]\n  # Delete repeating headers\n  raw = df.drop(df[df.Age == 'Age'].index)\n  # Fill missing data with 0\n  raw = raw.fillna(0)\n  # Convert int columns to float\n  raw['FG%'] = raw['FG%'].astype(float)\n  raw['3P%'] = raw['3P%'].astype(float)\n  raw['2P%'] = raw['2P%'].astype(float)\n  raw['eFG%'] = raw['eFG%'].astype(float)\n  raw['FT%'] = raw['FT%'].astype(float)\n  # Remove redundant index column\n  playerstats = raw.drop(['Rk'], axis=1)\n  return playerstats\n\nplayerstats = load_data(selected_year)\n\n# sidebar - Team selection\nsorted_unique_team = sorted(playerstats.Tm.unique())\nselected_team = st.sidebar.multiselect('Team', sorted_unique_team, sorted_unique_team)\n\n# Sidebar - Position selection\nunique_pos = ['C', 'PF', 'SF', 'PG', 'SG']\nselected_pos = st.sidebar.multiselect('Position', unique_pos, unique_pos)\n\n# Filtering data\ndf_selected_team = playerstats[(playerstats.Tm.isin(selected_team)) & (playerstats.Pos.isin(selected_pos))]\n\nst.header('Display Player Stats of Selected Team(s)')\nst.write('Data Dimension: ' + str(df_selected_team.shape[0]) + ' rows and ' + str(df_selected_team.shape[1]) + ' columns.')\nst.dataframe(df_selected_team)\n\n# Download NBA player stats data\n# https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806\ndef filedownload(df):\n  csv = df.to_csv(index=False)\n  # strings &lt;-&gt; bytes conversion\n  b64 = base64.b64encode(csv.encode()).decode()\n  href = f'&lt;a href=\"data:file/csv;base64,{b64}\" download=\"playerstats.csv\"&gt;Download CSV File&lt;/a&gt;'\n  return href\n\nst.markdown(filedownload(df_selected_team), unsafe_allow_html=True)\n\n# Heatmap\nif st.button('Intercorrelation Heatmap'):\n  st.header('Intercorrelation Matrix Heatmap')\n  df_selected_team.to_csv('output.csv', index=False)\n  df = pd.read_csv('output.csv')\n\n  corr = df.corr()\n  mask = np.zeros_like(corr)\n  mask[np.triu_indices_from(mask)] = True\n  fig = None\n  with sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(figsize=(7,5))\n    ax = sns.heatmap(corr, mask=mask, vmax=1, square=True)\n  st.pyplot(fig)"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#eda-cryptocurrency",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#eda-cryptocurrency",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "EDA Cryptocurrency",
    "text": "EDA Cryptocurrency\nUse the BeautifulSoup library to scrape data from CoinMarketCap and perform exploratory data analysis.\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nMatplotlib\n\npip install matplotlib\n\nlxml\n\npip install lxml\n\nBeautifulSoup\n\npip install beautifulsoup4\n\n\nreplit: EDA_Cryptocurrency\n# This app is for educational purpose only. Insights gained is not financial advice. Use at your own risk!\nimport streamlit as st\nfrom PIL import Image\nimport pandas as pd\nimport base64\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nimport time\n\n# ---------------------------------#\n# New feature (make sure to upgrade your streamlit library)\n# pip install --upgrade streamlit\n\n# ---------------------------------#\n# Page layout\n# Page expands to full width\nst.set_page_config(layout=\"wide\")\n# ---------------------------------#\n# Title\n\nimage = Image.open(\"pexels-worldspectrum-844124.jpg\")\n\nst.image(image, width=500)\n\nst.title(\"Crypto Price App\")\nst.markdown(\n    \"\"\"\nThis app retrieves cryptocurrency prices for the top 100 cryptocurrency from the **CoinMarketCap**!\n\n\"\"\"\n)\n# ---------------------------------#\n# About\nexpander_bar = st.expander(\"About\")\nexpander_bar.markdown(\n    \"\"\"\n* **Python libraries:** base64, pandas, streamlit, numpy, matplotlib, seaborn, BeautifulSoup, requests, json, time\n* **Data source:** [CoinMarketCap](http://coinmarketcap.com).\n* **Credit:** Web scraper adapted from the Medium article *[Web Scraping Crypto Prices With Python](https://towardsdatascience.com/web-scraping-crypto-prices-with-python-41072ea5b5bf)* written by [Bryan Feng](https://medium.com/@bryanf).\n\"\"\"\n)\n\n# ---------------------------------#\n# Page layout (continued)\n# Divide page to 3 columns (col1 = sidebar, col2 and col3 = page contents)\ncol1 = st.sidebar\ncol2, col3 = st.columns((2, 1))\n\n# ---------------------------------#\n# Sidebar + Main panel\ncol1.header(\"Input Options\")\n\n# Sidebar - Currency price unit\ncurrency_price_unit = col1.selectbox(\"Select currency for price\", (\"USD\", \"BTC\", \"ETH\"))\n\n# Web scraping of CoinMarketCap data\n@st.cache\ndef load_data():\n    cmc = requests.get(\"https://coinmarketcap.com\")\n    soup = BeautifulSoup(cmc.content, \"html.parser\")\n\n    data = soup.find(\"script\", id=\"__NEXT_DATA__\", type=\"application/json\")\n    coins = {}\n    coin_data = json.loads(data.contents[0])\n    listings = coin_data[\"props\"][\"initialState\"][\"cryptocurrency\"][\"listingLatest\"][\n        \"data\"\n    ]\n\n    attributes = listings[0][\"keysArr\"]\n    index_of_id = attributes.index(\"id\")\n    index_of_slug = attributes.index(\"slug\")\n\n    for i in listings[1:]:\n        coins[str(i[index_of_id])] = i[index_of_slug]\n\n    coin_name = []\n    coin_symbol = []\n    market_cap = []\n    percent_change_1h = []\n    percent_change_24h = []\n    percent_change_7d = []\n    price = []\n    volume_24h = []\n\n    index_of_slug = attributes.index(\"slug\")\n    index_of_symbol = attributes.index(\"symbol\")\n\n    index_of_quote_currency_price = attributes.index(\n        f\"quote.{currency_price_unit}.price\"\n    )\n    index_of_quote_currency_percent_change_1h = attributes.index(\n        f\"quote.{currency_price_unit}.percentChange1h\"\n    )\n    index_of_quote_currency_percent_change_24h = attributes.index(\n        f\"quote.{currency_price_unit}.percentChange24h\"\n    )\n    index_of_quote_currency_percent_change_7d = attributes.index(\n        f\"quote.{currency_price_unit}.percentChange7d\"\n    )\n    index_of_quote_currency_market_cap = attributes.index(\n        f\"quote.{currency_price_unit}.marketCap\"\n    )\n    index_of_quote_currency_volume_24h = attributes.index(\n        f\"quote.{currency_price_unit}.volume24h\"\n    )\n\n    for i in listings[1:]:\n        coin_name.append(i[index_of_slug])\n        coin_symbol.append(i[index_of_symbol])\n\n        price.append(i[index_of_quote_currency_price])\n        percent_change_1h.append(i[index_of_quote_currency_percent_change_1h])\n        percent_change_24h.append(i[index_of_quote_currency_percent_change_24h])\n        percent_change_7d.append(i[index_of_quote_currency_percent_change_7d])\n        market_cap.append(i[index_of_quote_currency_market_cap])\n        volume_24h.append(i[index_of_quote_currency_volume_24h])\n\n    df = pd.DataFrame(\n        columns=[\n            \"coin_name\",\n            \"coin_symbol\",\n            \"market_cap\",\n            \"percent_change_1h\",\n            \"percent_change_24h\",\n            \"percent_change_7d\",\n            \"price\",\n            \"volume_24h\",\n        ]\n    )\n    df[\"coin_name\"] = coin_name\n    df[\"coin_symbol\"] = coin_symbol\n    df[\"price\"] = price\n    df[\"percent_change_1h\"] = percent_change_1h\n    df[\"percent_change_24h\"] = percent_change_24h\n    df[\"percent_change_7d\"] = percent_change_7d\n    df[\"market_cap\"] = market_cap\n    df[\"volume_24h\"] = volume_24h\n    return df\n\ndf = load_data()\n\n# Sidebar - Cryptocurrency selections\nsorted_coin = sorted(df[\"coin_symbol\"])\nselected_coin = col1.multiselect(\"Cryptocurrency\", sorted_coin, sorted_coin)\n\ndf_selected_coin = df[(df[\"coin_symbol\"].isin(selected_coin))]  # Filtering data\n\n# Sidebar - Number of coins to display\nnum_coin = col1.slider(\"Display Top N Coins\", 1, 100, 100)\ndf_coins = df_selected_coin[:num_coin]\n\n# Sidebar - Percent change timeframe\npercent_timeframe = col1.selectbox(\"Percent change time frame\", [\"7d\", \"24h\", \"1h\"])\npercent_dict = {\n    \"7d\": \"percent_change_7d\",\n    \"24h\": \"percent_change_24h\",\n    \"1h\": \"percent_change_1h\",\n}\nselected_percent_timeframe = percent_dict[percent_timeframe]\n\n# Sidebar - Sorting values\nsort_values = col1.selectbox(\"Sort values?\", [\"Yes\", \"No\"])\n\ncol2.subheader(\"Price Data of Selected Cryptocurrency\")\ncol2.write(\n    \"Data Dimension: \"\n    + str(df_selected_coin.shape[0])\n    + \" rows and \"\n    + str(df_selected_coin.shape[1])\n    + \" columns.\"\n)\n\ncol2.dataframe(df_coins)\n\n# Download CSV data\n# https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806\ndef filedownload(df):\n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode()).decode()  # strings &lt;-&gt; bytes conversions\n    href = f'&lt;a href=\"data:file/csv;base64,{b64}\" download=\"crypto.csv\"&gt;Download CSV File&lt;/a&gt;'\n    return href\n\ncol2.markdown(filedownload(df_selected_coin), unsafe_allow_html=True)\n\n# ---------------------------------#\n# Preparing data for Bar plot of % Price change\ncol2.subheader(\"Table of % Price Change\")\ndf_change = pd.concat(\n    [\n        df_coins.coin_symbol,\n        df_coins.percent_change_1h,\n        df_coins.percent_change_24h,\n        df_coins.percent_change_7d,\n    ],\n    axis=1,\n)\ndf_change = df_change.set_index(\"coin_symbol\")\ndf_change[\"positive_percent_change_1h\"] = df_change[\"percent_change_1h\"] &gt; 0\ndf_change[\"positive_percent_change_24h\"] = df_change[\"percent_change_24h\"] &gt; 0\ndf_change[\"positive_percent_change_7d\"] = df_change[\"percent_change_7d\"] &gt; 0\ncol2.dataframe(df_change)\n\n# Conditional creation of Bar plot (time frame)\ncol3.subheader(\"Bar plot of % Price Change\")\n\nif percent_timeframe == \"7d\":\n    if sort_values == \"Yes\":\n        df_change = df_change.sort_values(by=[\"percent_change_7d\"])\n    col3.write(\"*7 days period*\")\n    plt.figure(figsize=(5, 25))\n    plt.subplots_adjust(top=1, bottom=0)\n    df_change[\"percent_change_7d\"].plot(\n        kind=\"barh\",\n        color=df_change.positive_percent_change_7d.map({True: \"g\", False: \"r\"}),\n    )\n    col3.pyplot(plt)\nelif percent_timeframe == \"24h\":\n    if sort_values == \"Yes\":\n        df_change = df_change.sort_values(by=[\"percent_change_24h\"])\n    col3.write(\"*24 hour period*\")\n    plt.figure(figsize=(5, 25))\n    plt.subplots_adjust(top=1, bottom=0)\n    df_change[\"percent_change_24h\"].plot(\n        kind=\"barh\",\n        color=df_change.positive_percent_change_24h.map({True: \"g\", False: \"r\"}),\n    )\n    col3.pyplot(plt)\nelse:\n    if sort_values == \"Yes\":\n        df_change = df_change.sort_values(by=[\"percent_change_1h\"])\n    col3.write(\"*1 hour period*\")\n    plt.figure(figsize=(5, 25))\n    plt.subplots_adjust(top=1, bottom=0)\n    df_change[\"percent_change_1h\"].plot(\n        kind=\"barh\",\n        color=df_change.positive_percent_change_1h.map({True: \"g\", False: \"r\"}),\n    )\n    col3.pyplot(plt)"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#classification-iris-data",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#classification-iris-data",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Classification Iris Data",
    "text": "Classification Iris Data\nUse scikit-learn to perform classification with a Random Forest Classifier.\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nScikit learn\n\npip install scikit-learn\n\n\nreplit: Classification_Iris_Data\nimport streamlit as st\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\n\nst.write(\"\"\"\n# Simple Iris Flower Prediction App\n\nThis app predicts the **Iris flower** typ:\n\"\"\")\n\nst.sidebar.header(\"User Input Parameters\")\n\ndef user_input_features():\n    sepal_length = st.sidebar.slider('Sepal length', 4.3, 7.9, 5.4)\n    sepal_width = st.sidebar.slider('Sepal width', 2.0, 4.4, 3.4)\n    petal_length = st.sidebar.slider('Petal length', 1.0, 6.9, 1.3)\n    petal_width = st.sidebar.slider('Petal width', 0.1, 2.5, 0.2)\n    data = {'sepal_length': sepal_length,\n            'sepal_width': sepal_width,\n            'petal_length': petal_length,\n            'petal_width': petal_width}\n    features = pd.DataFrame(data, index=[0])\n    return features\n\ndf = user_input_features()\n\nst.subheader('User Input parameters')\nst.write(df)\n\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\n\nclf = RandomForestClassifier()\nclf.fit(X, Y)\n\nprediction = clf.predict(df)\nprediction_proba = clf.predict_proba(df)\n\nst.subheader('Class labels and their corresponding index number')\nst.write(iris.target_names)\n\nst.subheader('Prediction')\nst.write(iris.target_names[prediction])\n#st.write(prediction)\n\nst.subheader('Prediction Probability')\nst.write(prediction_proba)"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#regression-boston-housing-data",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#regression-boston-housing-data",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Regression Boston Housing Data",
    "text": "Regression Boston Housing Data\nUse regression to predict housing prices.\nDependencies\n\nPandas\n\npip install pandas\n\nStreamlit\n\npip install streamlit\n\nScikit learn\n\npip install scikit-learn\n\nshap\n\npip install shap\n\nMatplotlib\n\npip install matplotlib\n\n\nreplit: Regression_Boston_Housing_Data\nimport streamlit as st\nimport pandas as pd\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\n\nst.write(\"\"\"\n# Boston House Price Prediction App\nThis app predicts the **Boston House Price**!\n\"\"\")\nst.write('---')\n\n# Loads the Boston House Price Dataset\nboston = datasets.load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\nY = pd.DataFrame(boston.target, columns=[\"MEDV\"])\n\n# Sidebar\n# Header of Specify Input Parameters\nst.sidebar.header('Specify Input Parameters')\n\ndef user_input_features():\n    CRIM = st.sidebar.slider('CRIM', float(X.CRIM.min()), float(X.CRIM.max()), float(X.CRIM.mean()))\n    ZN = st.sidebar.slider('ZN', float(X.ZN.min()), float(X.ZN.max()), float(X.ZN.mean()))\n    INDUS = st.sidebar.slider('INDUS', float(X.INDUS.min()), float(X.INDUS.max()), float(X.INDUS.mean()))\n    CHAS = st.sidebar.slider('CHAS', float(X.CHAS.min()), float(X.CHAS.max()), float(X.CHAS.mean()))\n    NOX = st.sidebar.slider('NOX', float(X.NOX.min()), float(X.NOX.max()), float(X.NOX.mean()))\n    RM = st.sidebar.slider('RM', float(X.RM.min()), float(X.RM.max()), float(X.RM.mean()))\n    AGE = st.sidebar.slider('AGE', float(X.AGE.min()), float(X.AGE.max()), float(X.AGE.mean()))\n    DIS = st.sidebar.slider('DIS', float(X.DIS.min()), float(X.DIS.max()), float(X.DIS.mean()))\n    RAD = st.sidebar.slider('RAD', float(X.RAD.min()), float(X.RAD.max()), float(X.RAD.mean()))\n    TAX = st.sidebar.slider('TAX', float(X.TAX.min()), float(X.TAX.max()), float(X.TAX.mean()))\n    PTRATIO = st.sidebar.slider('PTRATIO', float(X.PTRATIO.min()), float(X.PTRATIO.max()), float(X.PTRATIO.mean()))\n    B = st.sidebar.slider('B', float(X.B.min()), float(X.B.max()), float(X.B.mean()))\n    LSTAT = st.sidebar.slider('LSTAT', float(X.LSTAT.min()), float(X.LSTAT.max()), float(X.LSTAT.mean()))\n    data = {'CRIM': CRIM,\n            'ZN': ZN,\n            'INDUS': INDUS,\n            'CHAS': CHAS,\n            'NOX': NOX,\n            'RM': RM,\n            'AGE': AGE,\n            'DIS': DIS,\n            'RAD': RAD,\n            'TAX': TAX,\n            'PTRATIO': PTRATIO,\n            'B': B,\n            'LSTAT': LSTAT}\n    features = pd.DataFrame(data, index=[0])\n    return features\n\ndf = user_input_features()\n\n# Main Panel\n\n# Print specified input parameters\nst.header('Specified Input parameters')\nst.write(df)\nst.write('---')\n\n# Build Regression Model\nmodel = RandomForestRegressor()\nmodel.fit(X, Y)\n# Apply Model to Make Prediction\nprediction = model.predict(df)\n\nst.header('Prediction of MEDV')\nst.write(prediction)\nst.write('---')\n\n# Explaining the model's predictions using SHAP values\n# https://github.com/slundberg/shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n\nfig, ax = plt.subplots()\n\nst.header('Feature Importance')\nplt.title('Feature importance based on SHAP values')\nshap.summary_plot(shap_values, X)\nst.pyplot(fig, bbox_inches='tight')\nst.write('---')\n\nplt.title('Feature importance based on SHAP values (Bar)')\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\nst.pyplot(fig, bbox_inches='tight')"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#deploy-app-to-heroku",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#deploy-app-to-heroku",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Deploy App to Heroku",
    "text": "Deploy App to Heroku\nHeroku\nruntime.txt\n\ncontains the required python version\n\npython-3.7.9\nrequirements.txt\n\ncontains the required packages and version numbers\n\nstreamlit==0.61.0\npandas==0.25.3\nnumpy==1.18.1\nscikit-learn==0.22.1\nsetup.sh\n\ncontains the setup steps for the server on the Heroku dyno\n\nmkdir -p ~/.streamlit/\n\necho \"\\\n[server]\\n\\\nport = $PORT\\n\\\nenableCORS = false\\n\\\nheadless = true\\n\\\n\\n\\\n\" &gt; ~/.streamlit/config.toml\nProcfile\n\nruns the [setup.sh](http://setup.sh) file and starts the streamlit app\n\nweb: sh setup.sh && streamlit run app.py"
  },
  {
    "objectID": "posts/creating-data-science-apps-with-streamlit-notes/index.html#deploy-app-to-streamlit-sharing",
    "href": "posts/creating-data-science-apps-with-streamlit-notes/index.html#deploy-app-to-streamlit-sharing",
    "title": "Notes on Creating Data Science Apps With Streamlit",
    "section": "Deploy App to Streamlit Sharing",
    "text": "Deploy App to Streamlit Sharing\nYour Streamlit Apps\nStreamlit Cloud Example Apps\nrequirements.txt\n\ncontains the required packages and version numbers\n\nstreamlit==0.61.0\npandas==0.25.3\nnumpy==1.18.1\nscikit-learn==0.22.1\nReferences:\n\nBuild 12 Data Science Apps with Python and Streamlit - Full Course"
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html",
    "href": "posts/crop-images-on-gpu-tutorial/index.html",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "",
    "text": "Introduction\nCreate a 2D Unity Project\nCreate Crop Script\nCreate Screen GameObject\nCreate ImageCropper\nTest it Out\nConclusion"
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#introduction",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#introduction",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we’ll cover how to create a square crop of an image in Unity. The approach used in this tutorial can be adapted to crop other sections of an image as well."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#create-a-2d-unity-project",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#create-a-2d-unity-project",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Create a 2D Unity Project",
    "text": "Create a 2D Unity Project\nOpen the Unity Hub and create a new 2D project. I’m using Unity 2019.4.20f1, but you should be fine using other versions."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#create-crop-script",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#create-crop-script",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Create Crop Script",
    "text": "Create Crop Script\nIn Unity, right-click an empty space in the Assets folder and select C# Script in the Create submenu. Name the new script, Crop and open it in your code editor.\n\n\n\n\n\n\nDefine Variables\nCreate a public GameObject variable called screen. We’ll be using this screen to confirm our script is correctly cropping the test images. Add a public bool variable called cropImage as well. This will let us toggle whether to crop the image during runtime. Lastly, we’ll create a private RenderTexture called image to store a copy of the original test image.\n\n\n\n\n\n\n\nDefine Start() Method\nIn the Start() method, we’ll store a copy the original test image in the image RenderTexture. We can do so by getting a reference to the Texture attached to the screen and using the Graphics.Blit() method. We’ll also adjust the camera so that we can see the entire image.\n\n\n\n\n\n\n\nDefine Update() Method\nFirst, we need to make another copy of the original image so that we can edit it. We’ll store this copy in a temporary RenderTexture called rTex that will get released at the end of the method.\nWe can’t change the dimensions of a RenderTexture after it’s been created. Instead, we’ll create a cropped image by copying part of rTex to another temporary RenderTexture called tempTex that will be square. We can copy the square image to rTex after we release the current RenderTexture assigned to rTex and make a new square one.\nThe size of tempTex will depend on whether the original image is wider or taller. We want to use the smallest side of the original image.\nWe’ll determine what part of rTex we need to copy by calculating either (image.width - image.height) / 2f or (image.height - image.width) / 2f depending on whether the image is wider or taller.\nWe can copy part of rTex to tempTex using the Graphics.CopyTexture() method. We need to specify several parameters in order to use this method to crop images.\n\nsrc: The original image\nsrcElement: The source texture element, set to 0\n\nNot relevant for our use case\n\nsrcMip: The mipmap level for the image RenderTexture, set to 0\n\nNot relevant for our use case\n\nsrcX: The X coordinate of the top left corner of the center square of the original image\nsrcY: The Y coordinate of the top left corner of the center square of the original image\nsrcWidth: Width of the new square image\nsrcHeight: Height of the new square image\ndst: An empty square RenderTexture\ndstElement: The destination texture element, set to 0\n\nNot relevant for our use case\n\ndstMip: The mipmap level for destination texture, set to 0\n\nNot relevant for our use case\n\ndstX: The X coordinate of the top left corner of the new square image\ndstY: The Y coordinate of the top left corner of the new square image\n\nAfter we copy tempTex back to rTex we’ll update the Texture for the screen with the new square image and adjust the shape of the screen to fit the new image."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#create-screen-gameobject",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#create-screen-gameobject",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Create Screen GameObject",
    "text": "Create Screen GameObject\nBack in Unity, right-click an empty space in the Hierarchy tab and select Quad from the 3D Object submenu. Name the new object Screen. The size will be updated automatically by the Crop.cs script."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#create-imagecropper",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#create-imagecropper",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Create ImageCropper",
    "text": "Create ImageCropper\nRight-click an empty space in the Hierarchy tab and select Create Empty from the pop-up menu. Name the empty object ImageCropper\n\n\n\n\n\nWith the ImageCropper selected drag and drop the Crop.cs script into the Inspector tab.\n\n\n\n\n\nDrag and drop the Screen object from the Hierarchy tab onto the Screen parameter in the Inspector tab."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#test-it-out",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#test-it-out",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Test it Out",
    "text": "Test it Out\nWe’ll need some test images to try out the ImageCropper. You can use your own or download the ones I used for this tutorial.\n\nWide Image\nTall Image\n\nDrag and drop the test images into the Assets folder. Select one of the images and drag it onto the Screen in the Scene.\n\n\n\n\n\nNext, we need to set our Screen to use an Unlit shader. Otherwise it will be a bit dim. With the Screen object selected, open the Shader drop-down menu in the Inspector tab and select Unlit.\n\n\n\n\n\nSelect Texture from the Unlit submenu.\n\n\n\n\n\nNow we can click the Play button and toggle the Crop Image checkbox to confirm our script is working properly. If you check the performance stats, you should see that there is basically no performance hit from cropping the image."
  },
  {
    "objectID": "posts/crop-images-on-gpu-tutorial/index.html#conclusion",
    "href": "posts/crop-images-on-gpu-tutorial/index.html#conclusion",
    "title": "How to Crop Images With a GPU in Unity",
    "section": "Conclusion",
    "text": "Conclusion\nThat is one method to efficiently crop images on the GPU in Unity. As mentioned earlier, this method can be adapted to crop different parts of the image. You can do so by changing the values for the Graphics.CopyTexture() method to adjust what part of the source image gets copied and where in the target image it gets copied to.\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/customize-github-profile-notes/index.html",
    "href": "posts/customize-github-profile-notes/index.html",
    "title": "Notes on Customizing Your GitHub Profile",
    "section": "",
    "text": "Introduction\nBasic Custom Profile\nBlog Post Feed\nYouTube Channel Feed\nRecent Activity Feed\nGitHub Stats"
  },
  {
    "objectID": "posts/customize-github-profile-notes/index.html#introduction",
    "href": "posts/customize-github-profile-notes/index.html#introduction",
    "title": "Notes on Customizing Your GitHub Profile",
    "section": "Introduction",
    "text": "Introduction\nI finally got around to making my GitHub profile look less plain. I came across this video on YouTube covering several different customizations. Below are some notes I made for future reference.\n\nBasic Custom Profile\n\nCreate a repository that has the same name as your user name\nCreate a README.md file\n\nLink to website\n\nUse https://shields.io\n\n\n\nWebsite\n\n\n[![Website](https://img.shields.io/website?label=christianjmills.com&style=for-the-badge&url=https://christianjmills.com)](https://christianjmills.com)\n\nMaybe Twitter\n\nUse https://shields.io\n\n\n\nTwitter Follow\n\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/cdotjdotmills?color=1DA1F2&logo=twitter&style=for-the-badge)](https://twitter.com/intent/follow?original_referer=https://github.com/cj-mills&screen_name=cdotjdotmills)\n\nAbout Me section\nContacts info and social media\n\n\n[&lt;img align=\"left\" alt=\"Channel Name | YouTube\" width=\"22px\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/youtube.svg\" /&gt;][youtube_address]\n\nLanguages and Tools that you work with\n\nhttps://github.com/github/explore: Houses all of the community-curated content for GitHub Topics and Collections (e.g. icon images)\nCopy Download link\nVisual Studio\nexplore/visual-studio-code.png at main · github/explore\n\n\n[&lt;img align=\"left\" alt=\"Visual Studio Code\" width=\"26px\" src=\"https://github.com/github/explore/raw/main/topics/visual-studio-code/visual-studio-code.png\" /&gt;](https://code.visualstudio.com)\n\n\nCan have separate definitions for links\n\n[website]: https://christianjmills.com\n\n\nPush the repository to GitHub.\nMake sure the repository is public.\n\n\n\nBlog Post Feed\n\nRequires a link to an RSS feed.xml file\nUse GitHub Action\n\nhttps://github.com/gautamkrishnar/blog-post-workflow\nIn README.md\n# Blog posts\n&lt;!-- BLOG-POST-LIST:START --&gt;\n&lt;!-- BLOG-POST-LIST:END --&gt;\nCreate .github folder\n\nCreate workflows folder\n\nCreate blog-post-workflow.yml file\nname: Latest blog post workflow\non:\n  schedule: # Run workflow automatically\n    - cron: '0 * * * *' # Runs every hour, on the hour\n  workflow_dispatch: # Run workflow manually (without waiting for the cron to be called), through the Github Actions Workflow page directly\n\njobs:\n  update-readme-with-blog:\n    name: Update this repo's README with latest blog posts\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Pull in personal blog posts\n        uses: gautamkrishnar/blog-post-workflow@master\n        with:\n          feed_list: \"https://christianjmills.com/feed.xml\"\n\nAdditional options\nGitHub - gautamkrishnar/blog-post-workflow: Show your latest blog posts from any sources or StackOverflow activity or Youtube Videos on your GitHub profile/project readme automatically using the RSS feed\n\nInclude under jobs: → steps: → with:\n\n\n\n\nManually Update List\n\nGo to https://github.com/&lt;user-name&gt;/&lt;repo-name&gt;/actions/workflows/blog-post-workflow.yml\nClick on Run workflow drop-down menu\nClick on Run workflow button\n\n\n\n\n\nYouTube Channel Feed\n\nSame steps as for Blog posts\nChanges\n\nYouTube Channel Feed\n\nhttps://www.youtube.com/feeds/videos.xml?channel_id=&lt;channel-id&gt;\n\nREAME.md\n# Youtube Videos\n&lt;!-- YOUTUBE:START --&gt;\n&lt;!-- YOUTUBE:END --&gt;\n.github/workflows/youtube-workflow.yml\nname: Latest YouTube video workflow\non:\n  schedule: # Run workflow automatically\n    - cron: '0 * * * *' # Runs every hour, on the hour\n  workflow_dispatch: # Run workflow manually (without waiting for the cron to be called), through the Github Actions Workflow page directly\n\njobs:\n  update-readme-with-blog:\n    name: Update this repo's README with latest blog posts\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n      - name: Pull in personal blog posts\n        uses: gautamkrishnar/blog-post-workflow@master\n        with:\n                    comment_tag_name: \"YOUTUBE\"\n          feed_list: \"https://www.youtube.com/feeds/videos.xml?channel_id=UCDOTuz8In9mVs44WZMbYNGg\"\n\n\n\n\nRecent Activity Feed\n\nUse GitHub Action\n\nhttps://github.com/jamesgeorge007/github-activity-readme\n\nChanges\n\nREADME.md\n# Recent GitHub Activity\n&lt;!--START_SECTION:activity--&gt;\n.github/workflows/update-readme.yml\nname: GitHub Activity\n\non:\n  schedule:\n    - cron: '*/30 * * * *'\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    name: Update this repo's README with recent activity\n\n    steps:\n      - uses: actions/checkout@v2\n      - uses: jamesgeorge007/github-activity-readme@master\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n\n\n\nGitHub Stats\n\nUse GitHub Action\n\nhttps://github.com/anuraghazra/github-readme-stats\n\nAdd to README.md\n\nDefault\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;)\nHide Individual Stats\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;&hide=contribs,prs)\n\nOptions: &hide=stars,commits,prs,issues,contribs\n\nInclude Private Contributions\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;&count_private=true)\nShow Icons\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;&show_icons=true)\nHide Border\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;&hide_border=true)\nThemes\n![My GitHub stats](https://github-readme-stats.vercel.app/api?username=&lt;user-name&gt;&show_icons=true&theme=radical)\n\nBuilt-in\ngithub-readme-stats/README.md at master · anuraghazra/github-readme-stats\n\nCustomize\nGitHub - anuraghazra/github-readme-stats: Dynamically generated stats for your github readmes\nAdd Most Used Languages\n\nmarkdown       ![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=&lt;user-name&gt;&show_icons=true&hide_border=true)\n\n\n\nReferences:\n\nHow To Create An Amazing Profile ReadMe With GitHub Actions"
  },
  {
    "objectID": "posts/daily_recaps/recap-1/index.html",
    "href": "posts/daily_recaps/recap-1/index.html",
    "title": "Daily Recap",
    "section": "",
    "text": "Introduction\nWorking With the Blender Python API\nCompute Shader Course"
  },
  {
    "objectID": "posts/daily_recaps/recap-1/index.html#introduction",
    "href": "posts/daily_recaps/recap-1/index.html#introduction",
    "title": "Daily Recap",
    "section": "Introduction",
    "text": "Introduction\nWell I didn’t actually have much time for anything other than homework today, but I did manage to work on a little bit. I want to clear out some smaller projects on my list before updating the PoseNet tutorial."
  },
  {
    "objectID": "posts/daily_recaps/recap-1/index.html#working-with-the-blender-python-api",
    "href": "posts/daily_recaps/recap-1/index.html#working-with-the-blender-python-api",
    "title": "Daily Recap",
    "section": "Working With the Blender Python API",
    "text": "Working With the Blender Python API\nI’ve started cleaning up the code for the small motion graphics I made with the Blender Python API. The code was basically scratch paper so there was a lot to clean up. I found using arrays and for loops to set up a bunch of keyframes to be a lot easier. I’ll try to make posts going through full code for both of the motion graphics this week."
  },
  {
    "objectID": "posts/daily_recaps/recap-1/index.html#compute-shader-course",
    "href": "posts/daily_recaps/recap-1/index.html#compute-shader-course",
    "title": "Daily Recap",
    "section": "Compute Shader Course",
    "text": "Compute Shader Course\nI purchased the compute shader course on Udemy that I listed in the last weekly recap. Udemy was running one of their big sales so I got it for $11.99. I have only used compute shaders for basic image operations so far and I would like to learn how to do more. I have not come across many resources explaining how to fully utilize compute shaders so I’m looking forward to the course. I plan to make some tutorials based on what I learn from the course as well."
  },
  {
    "objectID": "posts/daily_recaps/recap-2/index.html",
    "href": "posts/daily_recaps/recap-2/index.html",
    "title": "Daily Recap",
    "section": "",
    "text": "Barracuda Background Execution"
  },
  {
    "objectID": "posts/daily_recaps/recap-2/index.html#barracuda-background-execution",
    "href": "posts/daily_recaps/recap-2/index.html#barracuda-background-execution",
    "title": "Daily Recap",
    "section": "Barracuda Background Execution",
    "text": "Barracuda Background Execution\nThe Barracuda library provides the option to execute models inside a coroutine. This is useful when running on hardware that is not fast enough to execute the model in a single frame. I have avoided using this approach for the PoseNet tutorial because I was concerned that the pose skeleton would become out of sync with the video or webcam feed. A reader confirmed that this occurred when they implemented scheduled execution on their own. The pose skeleton lagged slightly behind the video. This drawback is not necessarily an issue depending on the application so I decided to try it out so that I can include it in the updated tutorial.\nUnsurprisingly, the amount of lag depends on how demanding the model is to run. There was a negligible amount of lag when running either the ResNet or MobileNet versions of the model on the GPU. There was also a significant increase in frame rate. Executing the model in the background on the CPU results in a significant amount of lag. The ResNet model is basically unusable. Even with the MobileNet version, the input resolution still needs to lowered quite a bit for the pose skeleton to keep up with the video.\nIf the target application only needs to classify the estimated pose, say for a yoga application, then it should be fine to lag behind the source video a bit. However, it can be problematic if the target application requires real-time motion tracking. If the application needs to be run on a mobile device, this might just be a compromise you need to make for now. This approach does still have the benefit of allowing the rest of the application to run smoothly while the model executes.\nI believe a new backend is in the works for the Barracuda library that will make use of the Neural Processing Units (NPUs) that are in newer mobile devices. I don’t know how far along the development is, but that should allow models to be executed much more efficiently on supported devices."
  },
  {
    "objectID": "posts/daily_recaps/recap-3/index.html",
    "href": "posts/daily_recaps/recap-3/index.html",
    "title": "Daily Recap",
    "section": "",
    "text": "Unity GPU Targeting"
  },
  {
    "objectID": "posts/daily_recaps/recap-3/index.html#unity-gpu-targeting",
    "href": "posts/daily_recaps/recap-3/index.html#unity-gpu-targeting",
    "title": "Daily Recap",
    "section": "Unity GPU Targeting",
    "text": "Unity GPU Targeting\nI spent some time today trying to get Unity to use the integrated graphics on my CPU instead of my discrete graphics card. The goal was to see if the Barracuda library would perform inference on the integrated graphics, because of reasons. I was a bit surprised that the Unity Editor did not have a simple menu option to select a rendering device. In Blender, you can manually select which device is used to render scenes.\nAs one might imagine, most of the questions online asked how to get Unity to use a discrete graphics card instead of the integrated graphics. There seems to be a command line argument, -force-device-index, to make the Editor use a particular GPU device. However, it appears to be for macOS only and had no effect in Windows. I also could not find a setting in the Nvidia Control Panel to completely prevent Unity from using the graphics card.\nThe only way that I have been able to get the Barracuda library to execute models on the integrated graphics is to disable the Nvidia card in the Windows Device Manager. This method is not too much of a hassle as I did not have to restart the computer for it to take effect.\nI still find it a bit odd that their is no menu option in the Unity Editor. I wonder if Unreal Engine allows you to do this."
  },
  {
    "objectID": "posts/daily_recaps/recap-4/index.html",
    "href": "posts/daily_recaps/recap-4/index.html",
    "title": "Daily Recap",
    "section": "",
    "text": "Unity GPU Targeting"
  },
  {
    "objectID": "posts/daily_recaps/recap-4/index.html#unity-gpu-targeting",
    "href": "posts/daily_recaps/recap-4/index.html#unity-gpu-targeting",
    "title": "Daily Recap",
    "section": "Unity GPU Targeting",
    "text": "Unity GPU Targeting\nAfter a bit more testing, it turns out that the -force-device-index command line argument does work on Windows, but only for built games. The command line argument works even when there is no display cable plugged into the secondary GPU.\nSomething that I never really considered before is that it does not appear to be possible to switch which GPU an application is running on while the application is running. Every option I have found for manually selecting a GPU requires restarting the application to take effect."
  },
  {
    "objectID": "posts/daily_recaps/recap-5/index.html",
    "href": "posts/daily_recaps/recap-5/index.html",
    "title": "Daily Recap",
    "section": "",
    "text": "Targeted In-Game Style Transfer"
  },
  {
    "objectID": "posts/daily_recaps/recap-5/index.html#targeted-in-game-style-transfer",
    "href": "posts/daily_recaps/recap-5/index.html#targeted-in-game-style-transfer",
    "title": "Daily Recap",
    "section": "Targeted In-Game Style Transfer",
    "text": "Targeted In-Game Style Transfer\nI spent some time testing out a method for stylizing only specific GameObjects in Unity. My approach was to use a second camera along with layers and culling masks. It actually worked pretty well, with no real performance cost. One concern I have with my current approach is that it assumes the outline of the targeted GameObjects will be the same in the stylized output. This might not always be the case."
  },
  {
    "objectID": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html",
    "href": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html",
    "title": "Notes on Dungeon Generation via WaveFunctionCollapse",
    "section": "",
    "text": "Overview\nWaveFunctionCollapse\nWaveFunctionCollapse Textures and Tiles\nAdditional Reading"
  },
  {
    "objectID": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#overview",
    "href": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#overview",
    "title": "Notes on Dungeon Generation via WaveFunctionCollapse",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Brian Bucklew’s talk covering how to use the WaveFunctionCollapse algorithm to procedurally generate dungeon levels."
  },
  {
    "objectID": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#wavefunctioncollapse",
    "href": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#wavefunctioncollapse",
    "title": "Notes on Dungeon Generation via WaveFunctionCollapse",
    "section": "WaveFunctionCollapse",
    "text": "WaveFunctionCollapse\n\nDeveloped by Maxim Gumin and released as open source in 2016\nGitHub Repository\nCaves of Qud was the first commercial use, many others quickly followed."
  },
  {
    "objectID": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#wavefunctioncollapse-textures-and-tiles",
    "href": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#wavefunctioncollapse-textures-and-tiles",
    "title": "Notes on Dungeon Generation via WaveFunctionCollapse",
    "section": "WaveFunctionCollapse Textures and Tiles",
    "text": "WaveFunctionCollapse Textures and Tiles\n\nWFC hast two primary modes of function, tile maps and textures\nThe tilemap generation mode creates tile set solutions via propagation of defined tile adjacency constraints.\nCaves of Qud uses Texture Mode\n\n\nTexture Mode\n\nEasy training inputs (small (e.g. 16x16) training images that can be easily created in tools like mspaint)\nPowerful outputs (arbitrarily large output textures that are locally similar to the input)\n\n\n\n\n\n\n\n\nHow it Works\n\nThe input texture is divided into \\(NxN\\) (e.g. 3x3) subtextures (tiles) and their overlap with other tiles is calculated (e.g. overlap neighboring tiles by 1 on each side).\n\nCan rotate and reflect the input texture to increase the number of available tiles\n\nThe output is initialized with each pixel being a full superposition of possible output tiles.\n\nImagine placing the stack of subtexture tiles on each pixel in the output image\nThe (3x3) tiles overlap at the edges\n\nThe lowest entropy \\(NxN\\) (e.g. 3x3) area is selected from the output and one option is selected at random from the remaining possibilities.\n\nPick a random (or lowest entropy) pixel location in the output image\nPick a random tile from the stack for that pixel location\nThat final value for that pixel location in the output image is set to the value at the center of the (3x3) tile that was selected\nAll other potential tiles in the stack for that pixel location are discarded (i.e. the stack collapses)\n\nNew information based on that selection are propagated (like a wave) to adjacent areas, removing possibilities that won’t properly overlap.\n\nRemove the tiles from the stacks for the neighboring pixel locations that are not compatible with the selected tile for the finalized pixel location\nSelect the neighboring pixel location with the smallest remaining tile stack (i.e. lowest entropy)\nPick a random option from the remaining compatible tiles\nRepeat for the neighboring tiles around that pixel location\n\nIf any elements are still uncollapsed go to step 2.\n\nSelect the pixel location with the smallest remaining tile stack\n\n\n\n\nQuick Code Example\n// Input: the training image\n// N: How large of blocks NxN (e.g. 3x3) to sample from the input as input patterns.\n// Note: Higher N leads to rising CPU and memory cost)\n// Width: The output width\n// Height: The output height\n// periodicInput: Whether to sample the input across edges\n// perdiodic: Whether the output should be sampled across edges to create edge-wrapping output\n// symmetry: A value between 1..8, indicating how many reflection and rotation symmetries should be sampled from the input\nvar model = new OverlappingModel(input, N:3, width:48, height:48, periodicInput:true, periodic:false, symmetry:8, ground: 0);\nmodel.Run(random.Next, limit:0);\nmodel.Graphics().Save($\"output.png\");\nProblem 0 - Scaling\n\nThe execution time scales with the input resolution\nMemory usage scales with tile size\n\nProblem 1 - Homogeneity\n\nThe output just goes on forever in every direction\nThere is no inherent large scale structure\n\nSolution\n\nPerform a preprocess pass for map segmentation\n\nPartition large scale chunks (shapes) whose interior walls are generated by WFC.\n\nAdd additional details with subsequent passes\n\nFill in the chunks with different WFC pass using new input\nCan use algorithms like A* to find broken connectivity (e.g. find places to put doors)\n\n\nProblem 2 - Overfitting\n\nAdding more detail often results in overfitting small details, reducing the variability of the output\n\nSolution:\n\nUse WFC to create overall architecture\nCreate additional detail and connectivity (doors, etc) with follow-up passes"
  },
  {
    "objectID": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#additional-reading",
    "href": "posts/dungeon-generation-via-wavefunctioncollapse-notes/index.html#additional-reading",
    "title": "Notes on Dungeon Generation via WaveFunctionCollapse",
    "section": "Additional Reading",
    "text": "Additional Reading\n\n“WaveFunctionCollapse is Constraint Solving in the Wild”\nIsaackarth.com: WaveFunctionCollapse is Constraint Solving in the Wild\nWave Function Collapse Explained\nWave Function Collapse Explained\nwfc_python\nAn Intro To WaveFunctionCollapse\nOskar Stålberg on Twitter: “I regret to inform you that I’ve relapsed into procedural island generation pic.twitter.com/8v3DvNldBe / Twitter”\n\nReferences:\n\nBrian Bucklew - Dungeon Generation via Wave Function Collapse"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "",
    "text": "Introduction\nTrain the Model\nModify the Unity Project\nTest it Out\nConclusion"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#introduction",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#introduction",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "Introduction",
    "text": "Introduction\nWhen I was first planning out this tutorial series, I had intended to use a different style transfer model. This other model is designed to reduce flickering to better work with video. Unfortunately, I was unable to reliably get good results with that model at a playable frame rate. I figured I might as well make a tutorial for anyone who wants to mess around that model since I already dumped so much time into it.\nImportant: This post assumes that you have already gone through the previous parts of this tutorial series. You can go to the start of this series by clicking the link below.\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.1\n\nTraining this new model will require sample images from the target video game so checkout Part 1.5 of this tutorial series if you haven’t already. It shows how to use the Unity Recorder tool to capture in-game footage. We’ll split the video into images to generate our training data.\nWe’ll be using a modified version of the Google Colab notebook from Part 2 of this series. The new model requires some examples of stylized images for training. We’ll use the style transfer model from earlier in this series to generate the training samples. You can either use the final.pth checkpoint you made during Part 2 or train a new one in the Colab notebook for this post."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#train-the-model",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#train-the-model",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "Train the Model",
    "text": "Train the Model\nFirst, you need to get your own copy of the Colab Notebook. Open the notebook using the link below and save your own copy just like in Part 2.\n\nNotebook Link\n\n\nContinue in the Notebook\nFollow the directions in the notebook to train your own video style transfer model. Return to this post once you have exported the trained model."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#modify-the-unity-project",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#modify-the-unity-project",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "Modify the Unity Project",
    "text": "Modify the Unity Project\nNow we just need to updated our Unity project to support the new model. Unlike the training process, this part is pretty much identical to the other style transfer model.\n\nUpdate the ComputeShader\nThe only thing we need to add to the Unity project are some new image processing functions in the ComputeShader we made in Part 3. This time, we need to remap the RGB values from [0,1] to [-1,1] instead of [0,255]. You can either swap out the code for the existing processing functions or make new ones like in the image below.\n\n\n\n\n\nIf you make new functions, be sure to replace the function names in the StyleTransfer.cs script.\n\n\n\n\n\n\n\nReplace the modelAsset\nDownload the ONNX file from your Google Drive just like in Part 3 and drop it into the Models folder. You can also download the model I’ll be using from the link below.\n\nMosaic Style Transfer Model\n\nNow we just need to assign the ONNX file to the modelAsset variable in the Inspector tab. This model is less efficient than the one used earlier in this series so we’ll lower the targetHeight as well."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#test-it-out",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#test-it-out",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "Test it Out",
    "text": "Test it Out\nYou’ll probably notice that this model doesn’t preserve as much fine detail as the other model. This is because I had to significantly reduce the size of the model to get playable frame rates. The full size model is able to preserve much more detail, but is too large to get playable frame rates on any modern graphics card. Additionally, the model doesn’t completely get rid of the flickering effect."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#conclusion",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/addendum/index.html#conclusion",
    "title": "End-to-End In-Game Style Transfer Tutorial Addendum",
    "section": "Conclusion",
    "text": "Conclusion\nThis model works great for videos where the scene doesn’t change too drastically. Unfortunately, it does not seem well suited for a dynamic, real-time, environment. The default model is way too large to get playable frame rates and it quickly loses detail when its size is reduced. Still, the process of trying to make it work was educational at the very least."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "",
    "text": "Introduction\nSelect a Unity Project\nInstall Barracuda Package\nCreate Google Drive Project Folder\nConclusion"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#introduction",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#introduction",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial series covers how to train your own style transfer model and implement it in Unity using the Barracuda library. We’ll be using the PyTorch library to build and train the model. You will not need to set up PyTorch on your local machine to follow along. Instead, we’ll be using the free Google Colab service to train the model in a web browser. This does require you to have a Google account. You will also need some free space on Google Drive as we’ll be saving our model’s progress there.\nIn this first post, we’ll download our Unity project and install the Barracuda library. We’ll also create a folder for our project in Google Drive. This is where we’ll store our style images, test images and model checkpoints."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#select-a-unity-project",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#select-a-unity-project",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "Select a Unity Project",
    "text": "Select a Unity Project\nI’ll be using the Kinematica_Demo project provided by Unity for this tutorial. I was planning on using the lightweight FPS Microgame that’s available in Unity Hub. However, the environment ended up being so simple that there wasn’t much to stylize. Feel free to follow along with that project though. The steps are identical and it takes much less time to open.\n\n\n\n\n\n\nDownload Kinematica Demo\nYou can download the Kinematica project by clicking on the link below. The zipped folder is approximately 1.2 GB.\n\nKinematica_Demo_0.8.0-preview: (download)\n\n\n\nAdd Project to Unity Hub\nOnce downloaded, unzip the folder and add the project to Unity Hub using the Add button.\n\n\n\n\n\n\n\nSet the Unity Version\nSelect a Unity version from the drop-down menu. The demo project was made using Unity 2019.4.5f1. You can use a later 2019.4 release if you don’t have that version installed.\n\nUnity 2019.4.20: (download)\n\n\n\n\n\n\n\n\nOpen the Project\nNow we can open the project. We’ll be prompted to upgrade the project to the selected Unity version. Click Confirm in the popup to upgrade the project. This project takes a while to load the first time."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#install-barracuda-package",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#install-barracuda-package",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nWe’ll install the Barracuda package once the project has finished loading. Open the Package Manager window in the Unity editor.\n\n\n\n\n\nIn the Unity Registry section, type Barracuda into the search box. We’ll be using version 1.0.4.\n\n\n\n\n\nClick the Install button to install the package."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#create-google-drive-project-folder",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#create-google-drive-project-folder",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "Create Google Drive Project Folder",
    "text": "Create Google Drive Project Folder\nGoogle Colab environments provide the option to mount our Google Drive as a directory. We’ll be using this feature to automatically save our training progress.\n\nCreate a Project Folder\nWe’ll make a dedicated project folder to keep things organized. Open up your Google Drive and click the New button.\n\n\n\n\n\nSelect the Folder option.\n\n\n\n\n\nName the folder Style_Transfer_Project.\n\n\n\n\n\n\n\nCreate Folder for Style Images\nOpen the project folder and create a new folder for storing the style images you want to use.\n\n\n\n\n\n\nUpload Style Images\nYou can pick whatever images you want, but some will work better than others. Upload the images in their source resolution. You’ll have the option to resize them through code when training the model. If you don’t currently have any style images in mind you can download the one’s that I’ll use from the link below.\nNote: I recommend cropping the style images into squares. Not doing so can occasionally result in a border around the edges of the stylized output image.\n\n\nSample Style Images: (link)\n\n\n\n\n\n\n\n\nCreate Folder for Test Images\nWe’ll also need a test image to see how the well the model is stylizing images during training. This is especially important as it can take some experimentation to get the model to generate desirable results. It can often be clear early in a training session whether the model is learning as intended.\nYou can use this screenshot from the Kinematica demo for your test image.\n\nKinematica Demo Screenshot: (link)\n\nPlace your test images in a new folder called test_images."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#conclusion",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1/index.html#conclusion",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1",
    "section": "Conclusion",
    "text": "Conclusion\nThat takes care of the required setup. In the next post I’ll cover the optional step of recording in-game footage to add to your training dataset. This can help the model better adapt to the game’s specific environment. You can also skip ahead to part 2 where we’ll be training our style transfer model in Google Colab.\n\nNext: Part 1.5 (Optional) Part 2\nGitHub Repository"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "",
    "text": "Introduction\nInstall Unity Recorder\nOpen Game Scene\nOpen Unity Recorder\nAdd Movie Recorder\nRecord In-Game Footage\nUpload Video to Google Drive\nConclusion"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#introduction",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#introduction",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we’ll cover how to use the Unity Recorder tool to record in-game footage. We’ll be training our model using a pre-existing dataset of real-life images. This enables the model to learn to do a decent job stylizing arbitrary images. However, some games can look drastically different from real-life. In these situations, it can help to add additional training images from the target game. This should help the model learn to stylize frames from that specific environment."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#install-unity-recorder",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#install-unity-recorder",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Install Unity Recorder",
    "text": "Install Unity Recorder\nThe Unity Recorder tool is available through the Package Manger. In the Unity Registry section, type unity recorder in the search box and click Install."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#open-game-scene",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#open-game-scene",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Open Game Scene",
    "text": "Open Game Scene\nIn the Assets section of the Project tab, navigate to the Scene you want to record. For the Kinematica demo, open the Biped scene in the Scenes folder."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#open-unity-recorder",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#open-unity-recorder",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Open Unity Recorder",
    "text": "Open Unity Recorder\nThe Unity Recorder tool is accessible in the General sub-menu in the Windows section. The tool will open up in its own window."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#add-movie-recorder",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#add-movie-recorder",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Add Movie Recorder",
    "text": "Add Movie Recorder\nIn the Recorder window, click the + Add Recorder button.\n\n\n\n\n\nSelect the Movie option from the drop down.\n\n\n\n\n\nI recommend setting the Output Resolution to 1080p or higher to reduce the amount of compression artifacts."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#record-in-game-footage",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#record-in-game-footage",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Record In-Game Footage",
    "text": "Record In-Game Footage\nClick Start Recording and the game will begin automatically. Once the game is running, move around and try to capture different parts of the environment. The amount of footage required depends on the size the scene. The only thing to keep in mind is that you’ll need to upload the video file to Google Drive. The more footage you record, the longer it will take to upload.\n\n\n\n\n\nOnce you’re finished recording, you can open the Recordings folder by clicking the button outlined in the image below."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#upload-video-to-google-drive",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#upload-video-to-google-drive",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Upload Video to Google Drive",
    "text": "Upload Video to Google Drive\nThe last step is to drag and drop the video file into the project folder in Google Drive."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#conclusion",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-1-5/index.html#conclusion",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all that’s needed to generate custom training data from your Unity project. You can also use any other recording software that you prefer working with. This can be beneficial if you want more control over the quality of the video recordings. In the next post, we’ll cover how to split the recording into a folder of images and finally train a style transfer model in Google Colab.\n\nNext: Part 2\nGitHub Repository"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "",
    "text": "Introduction\nOpen Google Colab Notebook\nContinue in the Notebook\nConclusion"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#introduction",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#introduction",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Introduction",
    "text": "Introduction\nIn this post we’ll be using the free tier of Google Colab to train a style transfer model. Google Colab provides a virtual environment that allows anyone to write and execute arbitrary python code in their browser. This removes the need to setup a Python environment on your local machine. It also provides free access to GPUs.\nImportant: Google Colab restricts GPU allocation for free users to 12 hours at a time. You will get disconnected from the server if you leave a notebook running past that. You need to wait a while (probably 12 hours) for the time limit to reset."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#open-google-colab-notebook",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#open-google-colab-notebook",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Open Google Colab Notebook",
    "text": "Open Google Colab Notebook\nFirst, you need to get your own copy of the Colab Notebook. You can open my copy of the notebook by clicking the link below.\n\nNotebook Link\n\n\nCopy to Google Drive\nYou need to save the notebook to your Google Drive since you can’t make changes to my copy. To do so, click the Copy to Drive button.\n\n\n\n\n\nThat will reopen the notebook in a new tab where any changes you make can be saved to you Google Drive. Go ahead and close the original tab. The notebook should autosave progress, but you can manually save by pressing CTRL-s.\n\nColab Notebooks Folder\nIf you open your Google Drive, you should see a new folder named Colab Notebooks. This is where any notebooks your work on in Google Colab will be saved.\n\n\n\n\n\n\n\nInside Colab Notebooks Folder\nYou can open the new folder to see your copy of the notebook. If you double-click on the notebook file, you’ll be presented with the option to open it in a Google Colab environment. You can use this method to reopen the notebook in the future."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#using-a-colab-notebook",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#using-a-colab-notebook",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Using a Colab Notebook",
    "text": "Using a Colab Notebook\nColab Notebooks are primarily made up of code cells and text cells. Code cells can be executed in multiple ways. If you hover over or click on a code cell, a play button will appear on the left side of the cell. Clicking the play button will execute the code cell.\n\n\n\n\n\nThe other main ways are to either press CTRL-Enter or Shift-Enter. CTRL-Enter executes the code cell in place while Shift-Enter executes the code cell and moves to the next cell.\nYou can add more cells by hovering over either the top or bottom of an existing cell. You will be presented with the option to create either a code or text cell."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#connect-to-a-runtime-environment",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#connect-to-a-runtime-environment",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Connect to a Runtime Environment",
    "text": "Connect to a Runtime Environment\nWe need to connect to a runtime environment before we start using the notebook. Press the Connect button outlined below.\n\n\n\n\n\nOnce the notebook has connected to a runtime environment hover the RAM/Disk readout and make sure the notebook is using a GPU backend.\n\n\n\n\n\nIf it’s not, you need to manually set it to use a GPU. You can do so by opening the Notebook Settings under the Edit section.\n\n\n\n\n\nSelect GPU from the Hardware Accelerator dropdown and click Save."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#continue-in-the-notebook",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#continue-in-the-notebook",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Continue in the Notebook",
    "text": "Continue in the Notebook\nI recommend continuing this post in the Colab notebook itself. However, I have also included the notebook contents below if you’re only reading through the tutorial."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#install-the-fastai-library",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#install-the-fastai-library",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Install the fastai Library",
    "text": "Install the fastai Library\nFirst, we’ll install the fastai library which is built on top of PyTorch. We’ll be using pure PyTorch for training the model but the fastai library includes some convenience functions that we’ll use to download the training dataset.\n%%capture\n!pip install fastai==2.2.5"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#import-dependencies",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#import-dependencies",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nNext, we need to import the required python modules and packages.\n# Miscellaneous operating system interfaces\n# https://docs.python.org/3/library/os.html\nimport os\n# Time access and conversions\n# https://docs.python.org/3/library/time.html\nimport time\n# Object-oriented filesystem paths\n# https://docs.python.org/3/library/pathlib.html#pathlib.Path\nfrom pathlib import Path\n# Tuple-like objects that have named fields\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nfrom collections import namedtuple\n\n# A convenience function for downloading files from a url to a destination folder\n# https://docs.fast.ai/data.external.html#untar_data\nfrom fastai.data.external import untar_data\n\n# Provides image processing capabilities\n# https://pillow.readthedocs.io/en/stable/reference/Image.html\nfrom PIL import Image\n\n# The main PyTorch package\n# https://pytorch.org/docs/stable/torch.html\nimport torch\n\n# Used to iterate over the dataset during training \n# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\nfrom torch.utils.data import DataLoader\n\n# Contains definitions of models. We'll be downloading a pretrained VGG-19 model\n# to judge the performance of our style transfer model.\n# https://pytorch.org/vision/stable/models.html#torchvision.models.vgg19\nfrom torchvision.models import vgg19\n# Common image transforms that we'll use to process images before feeding them to the models\n# https://pytorch.org/vision/stable/transforms.html\nfrom torchvision import transforms\n# Loads images from a directory and applies the specified transforms\n# https://pytorch.org/vision/stable/datasets.html#imagefolder\nfrom torchvision.datasets import ImageFolder"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#utility-functions",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#utility-functions",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Utility Functions",
    "text": "Utility Functions\nWe’ll define some utility functions for making new directories, loading and saving images, and stylizing images using model checkpoints.\ndef make_dir(dir_name: str):\n    \"\"\"Create the specified directory if it doesn't already exist\"\"\"\n    dir_path = Path(dir_name)\n    try:\n        dir_path.mkdir()\n    except:\n        print(\"Directory already exists.\")\n\ndef load_image(filename: str, size: int=None, scale: float=None):\n    \"\"\"Load the specified image and return it as a PIL Image\"\"\"\n    img = Image.open(filename)\n    if size is not None:\n        img = img.resize((size, size), Image.ANTIALIAS)\n    elif scale is not None:\n        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n    return img\n\ndef save_image(filename: str, data: torch.Tensor):\n    \"\"\"Save the Tensor data to an image file\"\"\"\n    img = data.clone().clamp(0, 255).numpy()\n    img = img.transpose(1, 2, 0).astype(\"uint8\")\n    img = Image.fromarray(img)\n    img.save(filename)\n\ndef load_checkpoint(model_path):\n    state_dict = torch.load(model_path)\n    keys = [k for k in state_dict.keys()]\n    filters = set()\n    filters_list = [state_dict[k].shape[0] for k in keys if not (state_dict[k].shape[0] in filters or filters.add(state_dict[k].shape[0]))]\n    res_blocks = len(set(k.split('.')[1] for k in state_dict.keys() if 'resnets' in k))\n    model = TransformerNet(filters=filters_list[:-1], res_blocks=res_blocks) \n    model.load_state_dict(state_dict, strict=False)\n    return model\n\ndef stylize(model_path: str, input_image: str, output_image: str, content_scale: float=None, \n            device: str=\"cpu\", export_onnx: bool=None):\n    \"\"\"Load a TransformerNet checkpoint, stylize an image and save the output\"\"\"\n    device = torch.device(device)\n    content_image = load_image(input_image, scale=content_scale)\n    content_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255))\n    ])\n    content_image = content_transform(content_image)\n    content_image = content_image.unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        style_model = load_checkpoint(model_path)\n        style_model.to(device)\n         \n        if export_onnx:\n            assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n            output = torch.onnx._export(style_model, content_image, export_onnx, opset_version=9).cpu()\n        else:\n            output = style_model(content_image).cpu()\n    save_image(output_image, output[0])"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-style-transfer-model",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-style-transfer-model",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Define the Style Transfer Model",
    "text": "Define the Style Transfer Model\nNext, we’ll define the style transfer model itself. The model takes in an RGB image and generates a new image with the same dimensions. The features in the output image (e.g. color and texture) are then compared with the features of the style image and content image. The results of these comparisons are then used to update the parameters of the model so that it hopefully generates better images.\nI won’t go into detail about the model architecture as the goal of this tutorial is primarily showing how to use it.\nclass TransformerNet(torch.nn.Module):\n    \"\"\"TransformerNet\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L4\n    \"\"\"\n    \n    def __init__(self, filters=(32, 64, 128), res_blocks=5):\n        super(TransformerNet, self).__init__()\n        self.filters = filters\n        self.res_blocks = res_blocks if res_blocks &gt; 0 else 1\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, filters[0], kernel_size=9, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n        self.conv2 = ConvLayer(filters[0], filters[1], kernel_size=3, stride=2)\n        self.in2 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n        self.conv3 = ConvLayer(filters[1], filters[2], kernel_size=3, stride=2)\n        self.in3 = torch.nn.InstanceNorm2d(filters[2], affine=True)\n        # Residual layers\n        self.resnets = torch.nn.ModuleList()\n        for i in range(self.res_blocks):\n            self.resnets.append(ResidualBlock(filters[2]))\n        \n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(filters[2], filters[1], kernel_size=3, stride=1, upsample=2)\n        self.in4 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n        self.deconv2 = UpsampleConvLayer(filters[1], filters[0], kernel_size=3, stride=1, upsample=2)\n        self.in5 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n        self.deconv3 = ConvLayer(filters[0], 3, kernel_size=9, stride=1)\n        # Non-linearities\n        self.relu = torch.nn.ReLU()\n        \n    def forward(self, X):\n        conv1_y = self.relu(self.in1(self.conv1(X)))\n        conv2_y = self.relu(self.in2(self.conv2(conv1_y)))\n        conv3_y = self.relu(self.in3(self.conv3(conv2_y)))\n\n        y = self.resnets[0](conv3_y) + conv3_y\n        \n        for i in range(1, self.res_blocks):\n            y = self.resnets[i](y) + y\n\n        y = self.relu(self.in4(self.deconv1(conv3_y + y)))\n        y = self.relu(self.in5(self.deconv2(conv2_y + y)))\n        y = self.deconv3(conv1_y + y)\n        return y\n\n\nclass ConvLayer(torch.nn.Module):\n    \"\"\"ConvLayer\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L44\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    \"\"\"ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L57\n    \"\"\"\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.relu = torch.nn.ReLU()\n      \n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    \"\"\"UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L79\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n        \n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-vgg-19-model",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-vgg-19-model",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Define the VGG-19 Model",
    "text": "Define the VGG-19 Model\nNext, we’ll define the model that will be used to judge the quality of the output images from the style transfer model. This model has been pretrained a large image dataset. This means it’s already learned to recognize a wide variety of features in images. We’ll use this model to extract the features of the content image, style image, and stylized images.\nclass Vgg19(torch.nn.Module):\n    \"\"\"\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/vgg.py#L7\n    \"\"\"\n    \n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        self.feature_layers = [0, 3, 5]\n        self.vgg_pretrained_features = vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(4):\n            self.slice1.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(9, 18):\n            self.slice3.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(18, 27):\n            self.slice4.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(27, 36):\n            self.slice5.add_module(str(x), self.vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n            \n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n        return out"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-model-trainer",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#define-the-model-trainer",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Define the Model Trainer",
    "text": "Define the Model Trainer\nWe’ll define a new class to make training the style transfer model a bit easier. Along with training the model, this class will save the model’s current progress at set intervals. It will also generate sample images so we can see how the model is doing. This will allow us to determine if the model is actually improving or whether it’s already good enough that we can stop the training process early.\nclass Trainer(object):\n    def __init__(self, train_loader, style_transform, generator, opt_generator, style_criterion, perception_model, device):\n        self.train_loader = train_loader\n        self.style_transform = style_transform\n        self.generator = generator\n        self.opt_generator = opt_generator\n        self.style_criterion = style_criterion\n        self.perception_model = perception_model\n        self.device = device\n        self.generator.to(self.device)\n        \n    def gram_matrix(self, y: torch.Tensor):\n        \"\"\"Compute the gram matrix a PyTorch Tensor\"\"\"\n        (b, ch, h, w) = y.size()\n        features = y.view(b, ch, w * h)\n        features_t = features.transpose(1, 2)\n        gram = features.bmm(features_t) / (ch * h * w)\n        return gram\n\n    def normalize_batch(self, batch: torch.Tensor):\n        \"\"\"Normalize a batch of Tensors using the imagenet mean and std \"\"\"\n        mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n        std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n        batch = batch.div_(255.0)\n        return (batch - mean) / std\n\n    def get_gram_style(self, style_image: str, style_size: int):\n        \"\"\"Get the Gram Matrices for the style image\"\"\"\n        style = load_image(style_image, size=style_size)\n        style = self.style_transform(style)\n        style = style.repeat(self.train_loader.batch_size, 1, 1, 1).to(self.device)\n        features_style = self.perception_model(self.normalize_batch(style))\n        gram_style = [self.gram_matrix(y) for y in features_style]\n        return gram_style\n            \n    def save_checkpoint(self, path: str):\n        \"\"\"Save the current model weights at the specified path\"\"\"\n        self.generator.eval().cpu()\n        torch.save(self.generator.state_dict(), path)\n        print(f\"Checkpoint saved at {path}\")\n\n    def train(self, style_image, test_image, checkpoint_model_dir, epochs=5, content_weight=1e5, style_weight=1e10, \n                content_scale=None, style_size=None, log_interval=500, checkpoint_interval=500):\n        \"\"\"Train the style transfer model on the provided style image.\"\"\"\n        \n        gram_style = self.get_gram_style(style_image, style_size)\n\n        for e in range(epochs):\n            self.generator.train()\n            agg_content_loss = 0.\n            agg_style_loss = 0.\n            count = 0\n            for batch_id, (x, _) in enumerate(self.train_loader):\n                n_batch = len(x)\n                count += n_batch\n                self.opt_generator.zero_grad()\n                \n                x = x.to(self.device)\n                y = self.generator(x)\n\n                y = self.normalize_batch(y.clone())\n                x = self.normalize_batch(x.clone())\n                features_y = self.perception_model(y)\n                features_x = self.perception_model(x)\n\n                content_loss = content_weight * self.style_criterion(features_y.relu2_2, features_x.relu2_2)\n\n                style_loss = 0.\n                for ft_y, gm_s in zip(features_y, gram_style):\n                    gm_y = self.gram_matrix(ft_y)\n                    style_loss += self.style_criterion(gm_y, gm_s[:n_batch, :, :])\n                style_loss = style_loss * style_weight\n\n                total_loss = content_loss + style_loss\n                total_loss.backward()\n                self.opt_generator.step()\n\n                agg_content_loss += content_loss.item()\n                agg_style_loss += style_loss.item()\n\n                if (batch_id + 1) % log_interval == 0:\n                    mesg = f\"{' '.join(time.ctime().replace('  ', ' ').split(' ')[1:-1])}  \"\n                    mesg += f\"Epoch {e + 1}: [{count}/{len(self.train_loader.dataset)}]  \"\n                    mesg += f\"content: {(agg_content_loss / (batch_id + 1)):.4f}  \"\n                    mesg += f\"style: {(agg_style_loss / (batch_id + 1)):.4f}  \"\n                    mesg += f\"total: {((agg_content_loss + agg_style_loss) / (batch_id + 1)):.4f}\"\n                    print(mesg)\n\n                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n                    ckpt_base = f\"ckpt_epoch_{e}_batch_id_{batch_id + 1}\"\n                    ckpt_model_filename = ckpt_base + \".pth\"\n                    ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n                    self.save_checkpoint(ckpt_model_path)\n                    output_image = ckpt_base + \".png\"\n                    output_image_path = os.path.join(checkpoint_model_dir, output_image)\n                    stylize(ckpt_model_path, test_image, output_image_path)\n                    self.generator.to(self.device).train()\n                \n        print(\"Finished Training\")\n        ckpt_model_path = os.path.join(checkpoint_model_dir, 'final.pth')\n        self.save_checkpoint(ckpt_model_path)\n        output_image_path = os.path.join(checkpoint_model_dir, 'final.png')\n        stylize(ckpt_model_path, test_image, output_image_path)"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#mount-google-drive",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#mount-google-drive",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Mount Google Drive",
    "text": "Mount Google Drive\nBefore going any further, we need to mount out Google Drive so we can access our project folder. There is a python library that’s specifically made for working in Colab notebook that provides this functionality.\nfrom google.colab import drive\nWe’ll use the drive.mount() method to mount our whole Google Drive inside a new directory called drive.\nWhen you run the code cell below, you will be prompted to open a link to allow Google Colab to access your Drive.\nOnce you allow access you will be provided with an authorization code. Copy and paste the code into text box that appears in the output of the code cell and press Enter.\ndrive.mount('/content/drive')\nIf we look in the new drive folder, we can see that our main Drive folder is named MyDrive. All the folders and files in your Drive are accessible in MyDrive.\nIf you placed and named your project folder as shown in part 1 of this tutorial, it should be located at /content/drive/MyDrive/Style_Transfer_Project.\nWe’ll need that path to our project folder to store in Python variables in the next section."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#set-the-directories",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#set-the-directories",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Set the Directories",
    "text": "Set the Directories\nNow we need to create several variables to store the paths to various directories.\n\nThe dataset directory\nThe Google Drive style transfer project directory\nThe style images directory\nThe test image directory\nThe model checkpoint directory\n\nThe datset directory will be on the Google Colab environment while the rest will be on your Google Drive. This will allow you to keep all your progress while preventing the dataset from filling up your Drive storage.\nI recommend creating separate checkpoint directories for each training session. That makes it easier to compare results from experiments.\ndataset_dir = \"/content/dataset\"\n\nproject_dir = '/content/drive/MyDrive/Style_Transfer_Project'\nstyle_images_dir = f\"{project_dir}/style_images\"\ntest_images_dir = f\"{project_dir}/test_images\"\ncheckpoints_dir = f\"{project_dir}/checkpoints\"\nmake_dir(checkpoints_dir)"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#download-training-dataset",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#download-training-dataset",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Download Training Dataset",
    "text": "Download Training Dataset\nWe’ll be using the COCO train 2014 image dataset to train our model. It’s about 13.5 GB unzipped. That’s just high enough to trigger the disk space warning without actually using up the available disk space. You will likely get a disk space warning while the dataset is being unzipped. You can click ignore in the popup window. We’ll delete the zip file once the the folder is unzipped.\ncoco_url = 'http://images.cocodataset.org/zips/train2014.zip'\nuntar_data(coco_url, 'coco.zip', dataset_dir)\nif os.path.exists('coco.zip'): os.remove('coco.zip')"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#split-gameplay-video",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#split-gameplay-video",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Split Gameplay Video",
    "text": "Split Gameplay Video\nIn this section we’ll split the gameplay video if you made one. We’ll store the frames in a new sub-directory called video_frames in the dataset_dir.\n!mkdir ./dataset/video_frames/\nWe’ll use the ffmpeg command-line tool to split the video file. Google Colab should already have the tool installed.\nIn the code cell below replace /content/drive/MyDrive/Style_Transfer_Project/movie_001.mp4 with the path to your video file.\nIf you recorded a lot of footage, you might want to keep an eye on the available disk space and manually stop the code cell from running. This shouldn’t be a problem if you only recorded several minutes of gameplay.\n!ffmpeg -i /content/drive/MyDrive/Style_Transfer_Project/movie_001.mp4 ./dataset/video_frames/%05d.png -hide_banner"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#create-the-trainer-variables",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#create-the-trainer-variables",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Create the Trainer Variables",
    "text": "Create the Trainer Variables\nIn this section we’ll define the variables required to define a new Trainer.\n\nDefine the DataLoader\nWe need to define a DataLoader that will be responsible for iterating through the dataset during training.\nWe also need to specify the batch_size which indicates how many images will be fed to the model at a time.\nEvery image in a batch needs to be the same size. We’ll set the size using the image_size variable.\nImages need to be processed before being fed to the model. We’ll define the preprocessing steps using the transforms.Compose() method. Our preprocessing steps include the following:\n\nResize the images in the current batch to the target image_size\nCrop the images so that they are all square\nConvert the images to PyTorch Tensors\nMultiply the color channel values by 255\n\nWe then store the list of images in the dataset_dir along with the preprocessing steps in a new variable called train_dataset.\nFinally, we create our DataLoader using the train_dataset and specified batch_size\nbatch_size = 4\nimage_size = 256\ntransform = transforms.Compose([transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Lambda(lambda x: x.mul(255))\n                                ])\n\ntrain_dataset = ImageFolder(dataset_dir, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)\n\n\nSelect Compute Device\nWe’ll double that check a cuda GPU is available using the torch.cuda.is_available() method.\nuse_cuda = True\ndevice = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\nprint(f\"Using: {device}\")\n\n\nDefine Transforms for Style Image\nNext we’ll define the transforms used to process the style image before feeding it to the VGG-19 model. The processing steps are basically the same as for the training images accept the style image will have already been resized.\n\nConvert the image to a PyTorch Tensor\nMultiply the pixel values by 255\n\nstyle_transform = transforms.Compose([transforms.ToTensor(),\n                                      transforms.Lambda(lambda x: x.mul(255))\n                                      ])\n\n\nCreate the Style Transfer Model\nNext, we’ll create a new instance of the style transfer model. It’s here that you’ll be able to experiment with tradeoffs between performance and quality.\n\nTuning Model Inference Speed:\nThe easiest way to make the style transfer model faster is to make it smaller. We can easily tune the size of model by adjusting the size of the layers or by using fewer layers.\n\nResolution: 960x540\n\n\nFilters: (16, 32, 64)\n================================================================\nTotal params: 424,899\nTrainable params: 424,899\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 5.93\nForward/backward pass size (MB): 2210.61\nParams size (MB): 1.62\nEstimated Total Size (MB): 2218.17\n----------------------------------------------------------------\n\n\nResolution: 960x540\n\n\nFilters: (32, 64, 128)\n================================================================\nTotal params: 1,679,235\nTrainable params: 1,679,235\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 5.93\nForward/backward pass size (MB): 4385.35\nParams size (MB): 6.41\nEstimated Total Size (MB): 4397.69\n----------------------------------------------------------------\nBy default, the style transfer model uses the following values:\n\nfilters: (32, 64, 128)\nres_blocks: 5\n\nThe filters variable determines the size of the layers in the model. The resnet_blocks variable determines the number of ResidualBlocks that form the core of the model.\nI’ve found that setting filters to (8, 16, 32) and keeping keeping res_blocks at 5 significantly improves performance in Unity with minimal impact on quality.\nfilters = (8, 16, 32)\nres_blocks = 5\ngenerator = TransformerNet(filters=filters, res_blocks=res_blocks).to(device)\n\n\n\n\nCreate the Optimizer for the Style Transfer Model\nNext, we’ll define the optimizer for our model. The optimizer determines how the model gets updated during training. The optimizer takes in the model’s parameters and a learning rate. The learning rate determines how much the model gets updated after each batch of images.\nWe’ll use a learning rate of 1e-3 which is equivalent to 0.001.\nNotation Examples:\n\n1e-4 = 0.0001\n1e0 = 1.0\n1e5 = 100000.0\n5e10 = 50000000000.0\n\nlr = 1e-3\nopt_generator = torch.optim.Adam(generator.parameters(), lr)\n\n\nDefine How Model Performance Will Be Measured\nWe’ll be using Mean Squared Error (MSE) for comparing the difference between the features of the content image and stylized image and between the features of the stylized image and the target style image.\nstyle_criterion = torch.nn.MSELoss()\nNote: If you’re not familiar with MSE, take a look at the toy example below.\n\nMean Squared Error in Python\nx = [1, 2, 3, 4]\ny = [5, 6, 7, 8]\n\nsum_of_squares = 0\nfor i in range(len(x)):\n    error = x[i] - y[i]\n    squared_error = error**2\n    sum_of_squares += squared_error\n    \nmse = sum_of_squares / len(x)\nmse\n\n\nMean Squared Error in PyTorch\nx_t = torch.Tensor(x)\ny_t = torch.Tensor(y)\n\nmse_loss = torch.nn.MSELoss()\n\nmse_loss(x_t, y_t)\n\n\n\nCreate a New VGG-19 Perception Model\nNext, we’ll create a new vgg-19 model. The pretrained model will be downloaded the first time this cell is run.\nperception_model = Vgg19(requires_grad=False).to(device)"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#create-a-new-trainer",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#create-a-new-trainer",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Create a New Trainer",
    "text": "Create a New Trainer\nWe can now create a new trainer instance using the variables we defined above.\ntrainer = Trainer(train_loader=train_loader, \n                  style_transform=style_transform, \n                  generator=generator, \n                  opt_generator=opt_generator, \n                  style_criterion=style_criterion, \n                  perception_model=perception_model, \n                  device=device)\n\nTuning the Stylized Image\nThe stylized image will be influenced by the following:\n\nInfluence of the content image\nInfluence of the style image\nSize of the style image\n\nI recommend keeping the content_weight at 1e5 and adjusting the style_weight between 5e8 and 1e11. The ideal style_weight will vary depending on the style image. I recommend starting out low, training for 5-10 checkpoint intervals, and increasing the style weight as needed.\n# The file path for the target style image\nstyle_image = f\"{style_images_dir}/1.png\"\n# The file path for a sample input image for demonstrating the model's progress during training\ntest_image = f\"{test_images_dir}/011.png\" \n\n# The number of times to iterate through the entire training dataset\nepochs = 1\n\n# The influence from the input image on the stylized image\n# Default: 1e5\ncontent_weight = 1e5\n# The influence from the style image on the stylized image\n# Default: 1e10\nstyle_weight = 2e9\n\n# (test_image resolution) / content_scale\ncontent_scale = 1.0\n# Target size for style_image = (style_size, styl_size)\nstyle_size = 256\n\n# The number of training batches to wait before printing the progress of the model \nlog_interval = 500\n# The number of training to wait before saving the current model weights\ncheckpoint_interval = 500"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#train-the-model",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#train-the-model",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Train the Model",
    "text": "Train the Model\nOnce you execute the code cell below, open the checkpoints folder in Google Drive in another tab. You can view the model’s progress by looking at the sample style images that get generated with each checkpoint. You can stop the training process early by clicking the stop button where the play button normally is on the left side of the code cell.\ntrainer.train(style_image=style_image, \n              test_image=test_image, \n              checkpoint_model_dir=checkpoints_dir, \n              epochs=epochs, \n              content_weight=content_weight, \n              style_weight=style_weight,\n              content_scale=content_scale,\n              style_size=style_size,\n              log_interval=log_interval, \n              checkpoint_interval=checkpoint_interval)"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#export-the-model-to-onnx",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#export-the-model-to-onnx",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Export the model to ONNX",
    "text": "Export the model to ONNX\nWe can finally export the model to ONNX format. PyTorch exports models by feeding a sample input into the model and tracing what operators are used to compute the outputs.\nWe’ll use a (1, 3, 960, 540) Tensor with random values as our sample input. This is equivalent to feeding a 960x540 RGB image to the model. The resolution doesn’t matter as we can feed images with arbitrary resolutions once the model is exported.\nThe ONNX file will be saved to the project folder in Google Drive.\nNote: You will get a warning after running the code cell below recommending that you use ONNX opset 11 or above. Unity has prioritized support for opset 9 for Barracuda and higher opsets are not fully supported.\ncheckpoint_path = f\"{checkpoints_dir}/final.pth\"\nstyle_model = load_checkpoint(checkpoint_path)\nx = torch.randn(1, 3, 960, 540).cpu()\n\ntorch.onnx.export(style_model.cpu(),     #  Model being run\n                  x,                           # Sample input\n                  f\"{project_dir}/final.onnx\", # Path to save ONNX file\n                  export_params=True,          # Store trained weights\n                  opset_version=9,             # Which ONNX version to use\n                  do_constant_folding=True     # Replace operations that have all constant inputs with pre-computed nodes\n                 )"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#conclusion",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-2/index.html#conclusion",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.2",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s everything needed to train your own style transfer models. In the next post we’ll add the code to use the trained ONNX file in Unity.\n\nNext: Part 3\nGitHub Repository"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "",
    "text": "Introduction\nCreate Style Transfer Folder\nImport Model\nCreate Compute Shader\nCreate StyleTransfer Script\nAttach Script to Camera\nTest it Out\nConclusion"
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#introduction",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#introduction",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Introduction",
    "text": "Introduction\nIn this post we’ll implement our trained style transfer model in Unity."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-style-transfer-folder",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-style-transfer-folder",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Create Style Transfer Folder",
    "text": "Create Style Transfer Folder\nWe’ll place all our additions to the project in a new asset folder called Style_Transfer. This will help keep things organized."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#import-model",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#import-model",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Import Model",
    "text": "Import Model\nNext, we need to import the trained ONNX file that we created in Part 2.\n\nDownload ONNX Files\nRight-click the final.onnx in your Google Drive project folder and click Download.\n\n\n\n\n\nAlternatively, you can download the model that I’ll be using from the link below.\n\nMosaic Style Transfer Model\n\n\n\nImport ONNX Files to Assets\nOpen the Style_Transfer folder and make a new folder called Models.\n\n\n\n\n\nDrag and drop the ONNX file into the Models folder."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-compute-shader",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-compute-shader",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Create Compute Shader",
    "text": "Create Compute Shader\nWe can perform both the preprocessing and postprocessing operations on the GPU since both the input and output are images. We’ll implement these steps in a compute shader.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Shaders. Enter the Shaders folder and right-click an empty space. Select Shader in the Create submenu and click Compute Shader. We’ll name it StyleTransferShader.\n\n\n\n\n\n\n\nRemove the Default Code\nOpen the StyleTransferShader in your code editor. By default, the ComputeShader will contain the following.\n\n\n\n\n\nDelete the CSMain function along with the #pragma kernel CSMain. Next, we need to add a Texture2D variable to store the input image. Name it InputImage and give it a data type of &lt;half4&gt;. Use the same data type for the Result variable as well.\n\n\n\n\n\n\n\nCreate ProcessInput Function\nThe style transfer models expect RGB channel values to be in the range [0, 255]. Color values in Unity are in the range [0,1]. Therefore, we need to scale the three channel values for the InputImage by 255. We’ll perform this step in a new function called ProcessInput as shown below.\n\n\n\n\n\n\n\nCreate ProcessOutput Function\nThe models are supposed to output an image with RGB channel values in the range [0, 255]. However, it can sometimes return values a little outside that range. We can use the built-in clamp() method to make sure all values are in the correct range. We’ll then scale the values back down to [0, 1] for Unity. We’ll perform these steps in a new function called ProcessOutput as shown below.\n\n\n\n\n\nNow that we’ve created our ComputeShader, we need to execute it using a C# script."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-styletransfer-script",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#create-styletransfer-script",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Create StyleTransfer Script",
    "text": "Create StyleTransfer Script\nWe need to make a new C# script to perform inference with the style transfer model. This script will load the model, process the input, run the model, and process the output.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Scripts. In the Scripts folder, right-click an empty space and select C# Script in the Create submenu.\n\n\n\n\n\nName the script StyleTransfer.\n\n\n\n\n\n\n\nAdd Unity.Barracuda Namespace\nOpen the StyleTransfer script and add the Unity.Barracuda namespace at the top of the script.\n\n\n\n\n\n\n\nCreate StyleTransferShader Variable\nNext, we need to add a public variable to access our compute shader.\n\n\n\n\n\n\n\nCreate Style Transfer Toggle\nWe’ll also add a public bool variable to indicate whether we want to stylize the scene. This will create a checkbox in the Inspector tab that we can use to toggle the style transfer on and off while the game is running.\n\n\n\n\n\n\n\nCreate TargetHeight Variable\nGetting playable frame rates at higher resolutions can be difficult even when using a smaller model. We can help out our GPU by scaling down the camera input to a lower resolution before feeding it to the model. We would then scale the output image back up to the source resolution. This can also yield results closer to the test results during training if you trained the model with lower resolution images.\nCreate a new public int variable named targetHeight. We’ll set the default value to 540 which is the same as the test image used in the Colab Notebook.\n\n\n\n\n\n\n\nCreate Barracuda Variables\nNow we need to add a few variables to perform inference with the style transfer model.\n\nCreate modelAsset Variable\nMake a new public NNModel variable called modelAsset. We’ll assign the ONNX file to this variable in the Unity Editor.\n\n\n\n\n\n\n\nCreate workerType Variable\nWe’ll also add a variable that let’s us choose which backend to use when performing inference. The options are divided into CPU and GPU. Our image processing steps run entirely on the GPU so we’ll be sticking with the GPU options for this tutorial series.\nMake a new public WorkerFactory.Type called workerType. Give it a default value of WorkerFactory.Type.Auto.\n\n\n\n\n\n\n\nCreate m_RuntimeModel Variable\nWe need to compile the modelAsset into a run-time model to perform inference. We’ll store the compiled model in a new private Model variable called m_RuntimeModel.\n\n\n\n\n\n\n\nCreate engine Variable\nNext, we’ll create a new private IWorker variable to store our inference engine. Name the variable engine.\n\n\n\n\n\n\n\n\nCompile the Model\nWe need to get an object oriented representation of the model before we can work with it. We’ll do this in the Start() method and store it in the m_RuntimeModel.\n\n\n\n\n\n\n\nInitialize Inference Engine\nNow we can create a worker to execute the model using the selected backend. We’ll do this using the WorkerFactory.CreateWorker() method.\n\n\n\n\n\n\n\nRelease Inference Engine Resources\nWe need to manually release the resources that get allocated for the inference engine. This should be one of the last actions performed. Therefore, we’ll do it in the OnDisable() method. This method gets called when the Unity project exits.\n\n\n\n\n\n\n\nCreate ProcessImage() Method\nNext, we’ll make a new method to execute the ProcessInput() and ProcessOutput() functions in our ComputeShader. This method will take in the image that needs to be processed as well as a function name to indicate which function we want to execute. We’ll need to store the processed images in textures with HDR formats. This will allow us to use color values outside the default range of [0, 1]. As mentioned previously, the model expects values in the range of [0, 255].\n\nMethod Steps\n\nGet the ComputeShader index for the specified function\nCreate a temporary RenderTexture with random write access enabled to store the processed image\nExecute the ComputeShader\nCopy the processed image back into the original RenderTexture\nRelease the temporary RenderTexture\n\n\n\nMethod Code\n\n\n\n\n\n\n\n\nCreate StylizeImage() Method\nWe’ll create a new method to handle stylizing individual frames from the camera. This method will take in the src RenderTexture from the game camera and copy the stylized image back into that same RenderTexture.\n\nMethod Steps:\n\nResize the camera input to the targetHeight\nIf the height of src is larger than the targetHeight, we’ll calculate the new dimensions to downscale the camera input. We’ll then adjust the new dimensions to be multiples of 8. This is to make sure we don’t loose parts of the image after applying the processing steps with the Compute shader.\nApply preprocessing steps to the image\nWe’ll call the ProcessImage() method and pass rTex along with the name for the ProcessInput() function in the ComputeShader. The result will be stored in rTex.\nExecute the model\nWe’ll use the engine.Execute() method to run the model with the current input. We can store the raw output from the model in a new Tensor.\nApply the postprocessing steps to the model output\nWe’ll call the ProcessImage() method and pass rTex along with the name for the ProcessOutput() function in the ComputeShader. The result will be stored in rTex.\nCopy the stylized image to the src RenderTexture\nWe’ll use the Graphics.Blit() method to copy the final stylized image into the src RenderTexure.\nRelease the temporary RenderTexture\nFinally, we’ll release the temporary RenderTexture.\n\n\n\nMethod Code\n\n\n\n\n\n\n\n\nDefine OnRenderImage() Method\nWe’ll be calling the StylizeImage() method from the OnRenderImage() method instead of the Update() method. This gives us access to the RenderTexture for the game camera as well as the RenderTexture for the target display. We’ll only call the the StylizeImage() method if stylizeImage is set to true. You can delete the empty Update() method as it’s not needed in this tutorial.\n\nMethod Steps:\n\nStylize the RenderTexture for the game camera\nCopy the RenderTexture for the camera to the RenderTexture for the target display.\n\n\n\nMethod Code\n\n\n\n\n\nThat completes the StyleTransfer script. Next, we’ll attach it to the active camera in the scene."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#attach-script-to-camera",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#attach-script-to-camera",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Attach Script to Camera",
    "text": "Attach Script to Camera\nTo run the StyleTransfer script, we need to attach it to the active Camera in the scene.\n\nSelect the Camera\nOpen the Biped scene and expand the _Scene object in the Hierarchy tab. Select the Main Camera object from the dropdown list.\n\n\n\n\n\nNote: If you’re following along with the FPS Microgame, the Main Camera is a child of the Player object. However, the active camera is actually the WeaponCamera object which is a child of the Main Camera.\n\n\nAttach the StyleTransfer Script\nWith the Main Camera object still selected, drag and drop the StyleTransfer script into the bottom of the Inspector tab.\n\n\n\n\n\n\n\nAssign the Assets\nNow we just need to assign the ComputeShader and model assets as well as set the inference backend. Drag and drop the StyleTransferShader asset into the StyleTransferShader spot in the Inspector tab. Then, drag and drop the final.onnx asset into the Model Asset spot in the Inspector tab. Finally, select Compute Precompiled from the WorkerType dropdown.\n\n\n\n\n\n\n\nReduce Flickering\nThe style transfer model used in this tutorial series does not account for consistency between frames. This results in a flickering effect that can be distracting. Getting rid of this flickering entirely would require using a different (and likely less efficient) model. However, we can minimize flickering when the camera isn’t moving by disabling the Post-process Layer attached to the Main Camera object."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#test-it-out",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#test-it-out",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Test it Out",
    "text": "Test it Out\nAt last, we can press the play button and see how it runs. If you went through my previous in-game style transfer tutorial, you should see significantly higher frame rates this time."
  },
  {
    "objectID": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#conclusion",
    "href": "posts/end-to-end-in-game-style-transfer-tutorial/part-3/index.html#conclusion",
    "title": "End-to-End In-Game Style Transfer Tutorial Pt.3",
    "section": "Conclusion",
    "text": "Conclusion\nWe now have a complete workflow for implementing in-game style transfer in Unity. We can train as many style transfer models as we want using the Colab Notebook and drop them into our project. I wasn’t expecting to get such a significant improvement in frame rate compared to my first attempt at in-game style transfer. This implementation might actually be usable on non-flagship graphics cards.\nThere are still two major drawbacks to the approach covered in this tutorial. First, you need to train separate models for each specific style image. Second, the flickering is still present.\nThere is a more recent variant of the style transfer model used in this tutorial that can perform apply the style of arbitrary images. However, that version takes significantly more time to fully train. That version also still results in the same flickering effect.\nI had planned to use a style transfer model that provides more consistency between frames for this tutorial. Unfortunately, I wasn’t able to consistently get satisfying results and decided to exclude it. I still plan to make a stand alone post covering how to adapt the project from this tutorial series to use that model.\nGitHub Repository"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html",
    "href": "posts/fastai-book-notes/chapter-1/index.html",
    "title": "Notes on fastai Book Ch. 1",
    "section": "",
    "text": "A Brief History of Neural Networks\nHow to Learn Deep Learning\nWhat is Machine Learning?\nCreate an Image Classifier\nInspecting Deep Learning Models\nApplying Image Models to Non-Image Tasks\nOther Deep Learning Applications\nJargon\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#a-brief-history-of-neural-networks",
    "href": "posts/fastai-book-notes/chapter-1/index.html#a-brief-history-of-neural-networks",
    "title": "Notes on fastai Book Ch. 1",
    "section": "A Brief History of Neural Networks",
    "text": "A Brief History of Neural Networks\n\nThe Artificial Neuron\n\nDeveloped in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician\nA simplified model of a real neuron can be represented using simple addition and thresholding\nOriginal Paper: A Logical Calculus of the Ideas Immanent in Nervous Activity\n\nNeural events and the relations among them can be treated by means of propositional logic due to the “all-or-none” character of nervous activity\n\n\n\n\nMark 1 Perceptron\n\nThe first device based on the principles of artificial neurons\nDesigned for image recognition\n\nAble to recognize simple shapes\n\nDeveloped by a psychologist named Frank Rosenblatt\n\nGave the artificial neuron the ability to learn\nInvented the perceptron algorithm in 1958\nWrote The Design of an Intelligent Automaton\n\n\n\n\nPerceptrons\n\nWritten by Marvin Minsky, an MIT professor and Seymour Papert in 1969\nShowed that a single layer of Mark 1 Perceptron devices was unable to learn some simple but critical math functions like XOR\nAlso showed this limitation could be addressed by using multiple layers\n\nThis solution was not widely recognized by the global academic community\n\n\n\n\nParallel Distributed Processing (PDP)\n\nWritten by David Rumelhart, James McClelland, and the PDP Research Group\n\nReleased in 1986 by MIT Press\n\nPerhaps the most pivotal work in neural networks in the last 50 years\nPosited that traditional computer programs work very differently from brains and that this might be why computer programs were so bad at doing things that brains find easy\nAuthors claimed the PDP approach was closer to how the brain worked\nLaid out an approach that is very similar to today’s neural networks\nDefined parallel distributed processing as requiring the following:\n\nA set of processing units\nA state of activation\nAn output function for each unit\nA pattern of connectivity among units\nA propagation rule for propagating patterns of activities through the network of connectivities\nAn activation rule for combining the inputs impinging on a unit with the current state of the unit to produce an output for the unit\nA learning rule whereby patterns of connectivity are modified by experience\nAn environment within which the system must operate\n\nModern neural networks handle each of these requirements\nParallel distributed processing: Explorations in the microstructure of cognition\n\nVolume 1\nVolume 2\nPDF Scans\n\n\n\n\n1980s-1990s\n\nMost models were built with a second layer of neurons\nNeural networks were used for real, practical projects\nA misunderstanding of the theoretical issues held back the field\n\nIn theory, two-layer models could approximate any mathematical function\nIn practice, multiple layers are needed to get good, practical performance"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#how-to-learn-deep-learning",
    "href": "posts/fastai-book-notes/chapter-1/index.html#how-to-learn-deep-learning",
    "title": "Notes on fastai Book Ch. 1",
    "section": "How to Learn Deep Learning",
    "text": "How to Learn Deep Learning\n\nTeaching Approach\n\nTeach the whole game\n\nBuild a foundation of intuition through application then build on it with theory\nShow students how individuals pieces of theory are combined in a real-world application\n\nTeach through examples\n\nProvide a context and a purpose for abstract concepts\n\nMaking Learning Whole by David Perkins\n\nDescribes how teaching any subject at any level can be made more effective if students are introduced to the “whole game,” rather than isolated pieces of a discipline.\n\nA Mathematician’s Lament by Paul Lockhart\n\nImagines a nightmare world where music and art are taught the way math is taught\nStudents spend years doing rote memorization and learning dry, disconnected fundamentals that teachers claim will pay off later\n\n\n\n\nLearning Approach\n\nLearn by doing\nThe hardest part of deep learning is artisanal\n\nHow do you know if you have enough data?\nIs the data in the right format?\nIs your model trained properly?\nIf your model is not trained properly, what should you do about it?\n\nStart with small projects related to your hobbies and passions, before tackling a big project\nBe curious about problems you encounter\nFocus on underlying techniques and how to apply them over specific tools or software libraries\nExperiment constantly"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#what-is-machine-learning",
    "href": "posts/fastai-book-notes/chapter-1/index.html#what-is-machine-learning",
    "title": "Notes on fastai Book Ch. 1",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nanother way to get computers to complete a desired task\n\n\nTraditional Programming\n\nMust explicitly code the exact instructions required to complete a task\n\nDifficult when you do not know the exact steps\n\n\n\n\n\nA traditional program\n\n\n\n\nMachine Learning\n\nMust show the computer examples of the desired task and have it learn how to perform the task through experience\n\nDifficult when you do not have sufficient data\n\nInvented by IBM researcher Arthur Samuel in 1949\n\nCreated a checkers-player program that beat the Connecticut state champion in 1961\nWrote “Artificial Intelligence: A Frontier of Automation” in 1962\n\nComputers are giant morons and need to be told the exact steps required to solve a problem\nShow the computer examples of the problem to solve and let if figure out how to solve it itself\nArrange some automatic means of testing the effectiveness of any current weight assignment and provide a mechanism so as to maximize the actual performance\nIntroduced multiple concepts\n\nThe idea of “weight assignment”\n\nWeights are just variables\nA weight assignment is a particular choice of values for those variables\nWeight values define how the program will operate\n\nThe fact every weight assignment has some “actual performance”\n\nHow accurate is the image classifier?\nHow good is the checkers-player program at playing checkers?\n\nThe requirement there be an “automatic means” of testing that performance\n\nCompare performance with the current weight values to the desired result\n\nThe need for a “mechanism” for improving the performance by changing the weight assignments\n\nUpdate the weight values based on the performance with the current values\n\n\n\n\n\n\n\n\nA program using weight assignments\n\n\n\n\n\nTraining a machine learning model\n\n\n\n\n\nUsing a trained model as a program\n\n\n\n\nWhat is a Neural Network?\n\nA particular kind of machine learning model\nFlexible enough to solve a wide variety of problems just by finding the right weight values\nUniversal Approximation Theorem: shows that a neural network can, in theory, solve any problem to any level of accuracy\nStochastic Gradient Descent: a general way to automatically update the weights of a neural network to make it improve at any given task\n\n\n\nDeep learning\n\nA computer science technique to extract and transform data by using multiple layers of neural networks\nOutput from the previous layer serves as input for the next\nEach layer progressively refines their input\nA neural network learns to perform a specified task through training its layers using algorithms that minimize their errors and improve their accuracy\n\n\n\nInherent Limitations of Machine Learning\n\nA model cannot be created without data\nA model can learn to operate on only the patterns seen in the input data used to train it\nThis learning approach only creates predictions, not recommended actions\n\ncan result in a significant gap between organizational goals and model capabilities\n\nNeed labels for each example input in the training data\nThe way a model interacts with its environment can create feedback loops that amplify existing biases\n\nA predictive policing algorithm that is trained on data from past arrests will learn to predict arrests rather than to predict crime\nLaw enforcement officers using the model might decide to focus policing activity in areas where previous arrests were made, resulting in more arrests in those areas\nThe data from these new arrests are then fed into the model, increasing its bias towards those areas"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#create-an-image-classifier",
    "href": "posts/fastai-book-notes/chapter-1/index.html#create-an-image-classifier",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Create an Image Classifier",
    "text": "Create an Image Classifier\n\nImport Dependencies\n# Import fastai computer vision library\n# Includes functions and classes to create a wide variety of computer vision models\nfrom fastai.vision.all import *\n\n\nLoad Training Data\n# Download and extract the training dataset \n# base directory: '~/.fastai'\n# returns a Path object: https://docs.python.org/3/library/pathlib.html\npath = untar_data(URLs.PETS)/'images'\nprint(type(path))\nprint(path)\n&lt;class 'pathlib.PosixPath'&gt;\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images\nOxford-IIT Pet Dataset: contains 7,390 images of cats and dogs from 37 breeds\nURLs.PETS\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n!ls ~/.fastai/data\nannotations  coco_sample  oxford-iiit-pet\n\n!ls ~/.fastai/data/oxford-iiit-pet/images| head -5\nAbyssinian_100.jpg\nAbyssinian_100.mat\nAbyssinian_101.jpg\nAbyssinian_101.mat\nAbyssinian_102.jpg\nls: write error: Broken pipe\n\n# Returns true if the first letter in the string is upper case\ndef is_cat(x): return x[0].isupper()\nstr.isupper\n&lt;method 'isupper' of 'str' objects&gt;\n\nis_cat(\"word\")\nFalse\n\nis_cat(\"Word\")\nTrue\n\nis_cat(\"woRd\")\nFalse\n\n# Cat breeds are upper case\n!ls ~/.fastai/data/oxford-iiit-pet/images/[[:upper:]]* | head -5\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.mat\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Abyssinian_101.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Abyssinian_101.mat\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Abyssinian_102.jpg\nls: write error: Broken pipe\n\n# Dog breeds are lower case\n!ls ~/.fastai/data/oxford-iiit-pet/images/[[:lower:]]* | head -5\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/american_bulldog_100.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/american_bulldog_101.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/american_bulldog_102.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/american_bulldog_103.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/american_bulldog_104.jpg\nls: write error: Broken pipe\n\n# Create a dataloader to feed image files from dataset to the model\n# Reserves 20% of the available images for the validation set\n# Sets the random seed to get consistent results in each training session\n# Uses the is_cat() function to identify image classes\n# - Upper case will be the first class\n# Resizes and crops images to 224x224\n# Uses 8 cpu workers to load images during training\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224), num_workers=8)\n\nprint(len(get_image_files(path)))\nimg_files = get_image_files(path)\nfor i in range(5):\n    print(img_files[i])\n7390\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Birman_121.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/shiba_inu_131.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bombay_176.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bengal_199.jpg\n/home/innom-dt/.fastai/data/oxford-iiit-pet/images/beagle_41.jpg\n\ndls.after_item\nPipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -&gt; ToTensor\n\ndls.after_batch\nPipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\n\nTrain a Model\n\nRandomly Initialized Weights\nlearn = cnn_learner(dls, resnet34, metrics=accuracy, pretrained=False)\nlearn.fine_tune(1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.004878\n\n\n0.697246\n\n\n0.662382\n\n\n00:11\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.745436\n\n\n0.612512\n\n\n0.688769\n\n\n00:14\n\n\n\n\n\n\n\n\nPretrained Weights\n# removes the last layer of the of the pretrained resnet34 and\n# replaces it with a new output layer for the target dataset\nlearn = cnn_learner(dls, resnet34, metrics=accuracy, pretrained=True)\nlearn.fine_tune(1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.162668\n\n\n0.023766\n\n\n0.989851\n\n\n00:11\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.061914\n\n\n0.014920\n\n\n0.993234\n\n\n00:14\n\n\n\n\n\n\n\n# Build a convolutional neural network-style learner from the dataloader and model architecture\ncnn_learner\n&lt;function fastai.vision.learner.cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=&lt;function Adam at 0x7f0e87aa2040&gt;, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95), cut=None, n_in=3, init=&lt;function kaiming_normal_ at 0x7f0ed3b4f820&gt;, custom_head=None, concat_pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None)&gt;\n\n# ResNet-34 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n# the pretrained version has already been trained to recognize\n# a thousand different categories on 1.3 million images\nresnet34\n&lt;function torchvision.models.resnet.resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; torchvision.models.resnet.ResNet&gt;\n\n# based on https://github.com/sksq96/pytorch-summary/blob/master/torchsummary/torchsummary.py\ndef get_total_params(model, input_size, batch_size=-1, device='cuda', dtypes=None):\n    \n    def register_hook(module):\n\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n            module_idx = len(summary)\n\n            m_key = f\"{class_name}-{module_idx + 1}\"\n            summary[m_key] = OrderedDict()\n\n            params = 0\n            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][\"nb_params\"] = params\n\n        if (\n            not isinstance(module, nn.Sequential)\n            and not isinstance(module, nn.ModuleList)\n            and not (module == model)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    if device == \"cuda\" and torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n    else:\n        dtype = torch.FloatTensor\n\n    # multiple inputs to the network\n    if isinstance(input_size, tuple):\n        input_size = [input_size]\n\n    # batch_size of 2 for batchnorm\n    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n    \n    # create properties\n    summary = OrderedDict()\n    hooks = []\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass\n    model(*x)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    total_params = 0\n    for layer in summary:\n        total_params += summary[layer][\"nb_params\"]\n                \n    return total_params\ninput_shape = (3, 224, 224)\nprint(f\"ResNet18 Total params: {get_total_params(resnet18().cuda(), input_shape):,}\")\nprint(f\"ResNet34 Total params: {get_total_params(resnet34().cuda(), input_shape):,}\")\nprint(f\"ResNet50 Total params: {get_total_params(resnet50().cuda(), input_shape):,}\")\nprint(f\"ResNet101 Total params: {get_total_params(resnet101().cuda(), input_shape):,}\")\nprint(f\"ResNet152 Total params: {get_total_params(resnet152().cuda(), input_shape):,}\")\nResNet18 Total params: 11,689,512\nResNet34 Total params: 21,797,672\nResNet50 Total params: 25,557,032\nResNet101 Total params: 44,549,160\nResNet152 Total params: 60,192,808\n\n# 1 - accuracy\nerror_rate\n&lt;function fastai.metrics.error_rate(inp, targ, axis=-1)&gt;\n\n# 1 - error_rate\naccuracy\n&lt;function fastai.metrics.accuracy(inp, targ, axis=-1)&gt;\n\n\n\nUse Trained Model\n# Upload file(s) from browser to Python kernel as bytes\nuploader = widgets.FileUpload()\nuploader\nFileUpload(value={}, description='Upload')\n\n# Open an `Image` from path `fn`\n# Accepts pathlib.Path, str, torch.Tensor, numpy.ndarray and bytes objects\nimg = PILImage.create(uploader.data[0])\nprint(f\"Type: {type(img)}\")\nimg\nType: &lt;class 'fastai.vision.core.PILImage'&gt;\n\n\n\n\n\n# Prediction on `item`, fully decoded, loss function decoded and probabilities\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is this a cat?: {is_cat}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\nIs this a cat?: False.\nProbability it's a cat: 0.000024\n\nuploader = widgets.FileUpload()\nuploader\nFileUpload(value={}, description='Upload')\nimg = PILImage.create(uploader.data[0])\nimg\n\n\n\n\n\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is this a cat?: {is_cat}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\nIs this a cat?: True.\nProbability it's a cat: 1.000000"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#inspecting-deep-learning-models",
    "href": "posts/fastai-book-notes/chapter-1/index.html#inspecting-deep-learning-models",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Inspecting Deep Learning Models",
    "text": "Inspecting Deep Learning Models\n\nIt is possible to inspect deep learning models and get rich insights from them\n\ncan still be challenging to fully understand\n\nVisualizing and Understanding Convolutional Networks\n\npublished by PhD student Matt Zeiler and his supervisor Rob Fergus in 2013\nshowed how to visualize the neural network weights learned in each layer of a model\ndiscovered the early layers in a convolutional neural network recognize edges and simple patterns which are combined in later layers to detect more complex shapes\n\nvery similar to the basic visual machinery in the human eye"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#applying-image-models-to-non-image-tasks",
    "href": "posts/fastai-book-notes/chapter-1/index.html#applying-image-models-to-non-image-tasks",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Applying Image Models to Non-Image Tasks",
    "text": "Applying Image Models to Non-Image Tasks\n\nA lot of data can be represented as images\nSound can be converted to a spectrogram\n\nspectrogram: a chart that shows the amount of each frequency at each time in an audio file\n\nTime series can be converted to an image by plotting the time series on a graph\n\nVarious transformations available for time series data\n\nfast.ai student Ignacio Oguiza created images from time series data using Gramian Angular Difference Field (GADF)\n\nan image obtained from a time series, representing some temporal correlation between each time point\n\nfast.ai student Gleb Esman converted mouse movements and clicks to images for fraud detection\nMalware Classification with Deep Convolutional Neural Networks\n\nthe malware binary file is divided into 8-bit sequences which are then converted to equivalent decimal values\nthe decimal values are used to generate a grayscale image the represents the malware sample\n\n\n\nIt is often a good idea to represent your data in a way that makes it as easy as possible to pull out the most important components\n\nIn a time series, things like seasonality and anomalies are most likely to be of interest\n\nRule of thumb: if the human eye can recognize categories from images, then a deep learning model should be able to as well"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#other-deep-learning-applications",
    "href": "posts/fastai-book-notes/chapter-1/index.html#other-deep-learning-applications",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Other Deep Learning Applications",
    "text": "Other Deep Learning Applications\n\nImage Segmentation\n\ntraining a model to recognize the content of every single pixel in an image\n\n\npath = untar_data(URLs.CAMVID_TINY)\nprint(path)\n/home/innom-dt/.fastai/data/camvid_tiny\n\n!ls $path\ncodes.txt  images  labels\n\n# Basic wrapper around several `DataLoader`s with factory methods for segmentation problems\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str), seed=42, num_workers=8\n)\n\nimg_files = get_image_files(path/\"images\")\nprint(len(img_files))\nfor i in range(5):\n    print(img_files[i])\n100\n/home/innom-dt/.fastai/data/camvid_tiny/images/0016E5_08155.png\n/home/innom-dt/.fastai/data/camvid_tiny/images/Seq05VD_f03210.png\n/home/innom-dt/.fastai/data/camvid_tiny/images/Seq05VD_f03060.png\n/home/innom-dt/.fastai/data/camvid_tiny/images/Seq05VD_f03660.png\n/home/innom-dt/.fastai/data/camvid_tiny/images/0016E5_05310.png\n\npath/'labels'/f'{img_files[0].stem}_P{img_files[0].suffix}'\nPath('/home/innom-dt/.fastai/data/camvid_tiny/labels/0016E5_08155_P.png')\n\n# Build a unet learner\nlearn = unet_learner(dls, resnet34, pretrained=True)\nlearn.fine_tune(8)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.914330\n\n\n2.284680\n\n\n00:01\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.906939\n\n\n1.415985\n\n\n00:01\n\n\n\n\n1\n\n\n1.628720\n\n\n1.185889\n\n\n00:01\n\n\n\n\n2\n\n\n1.454888\n\n\n1.024575\n\n\n00:01\n\n\n\n\n3\n\n\n1.290813\n\n\n0.921251\n\n\n00:01\n\n\n\n\n4\n\n\n1.152427\n\n\n0.809383\n\n\n00:01\n\n\n\n\n5\n\n\n1.034114\n\n\n0.793250\n\n\n00:01\n\n\n\n\n6\n\n\n0.941492\n\n\n0.782535\n\n\n00:01\n\n\n\n\n7\n\n\n0.869773\n\n\n0.778228\n\n\n00:01\n\n\n\n\n\n\n\nlearn.show_results(max_n=6, figsize=(18,20))\n\n\n\n\n\n\n\nNatural Language Processing (NLP)\n\ngenerate text\ntranslate from one language to another\nanalyze comments\nlabel words in sentences\n\n\nfrom fastai.text.all import *\n\nIMDB Large Moview Review Dataset\n\nfrom Learning Word Vectors for Sentiment Analysis\n\n\npath = untar_data(URLs.IMDB)\npath\nPath('/home/innom-dt/.fastai/data/imdb')\n\n!ls $path\nimdb.vocab  README  test  tmp_clas  tmp_lm  train  unsup\n\n# Basic wrapper around several `DataLoader`s with factory methods for NLP problems\ndls = TextDataLoaders.from_folder(path, valid='test', bs=64, seed=42, num_workers=8)\n\nlen(dls.items)\n25000\n\ndls.after_iter\n&lt;bound method after_iter of &lt;fastai.text.data.SortedDL object at 0x7f0f493cb9d0&gt;&gt;\n\n# A `DataLoader` that goes throught the item in the order given by `sort_func`\nSortedDL\n\n!ls $path/train\nlabeledBow.feat  neg  pos  unsupBow.feat\n\n!ls $path/train/pos | wc -l\n12500\n\n!ls $path/train/pos | head -5\n0_9.txt\n10000_8.txt\n10001_10.txt\n10002_7.txt\n10003_8.txt\nls: write error: Broken pipe\n\n!cat $path/train/pos/0_9.txt\nBromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n\n!ls $path/train/neg | wc -l\n12500\n\n!ls $path/train/neg | head -5\n0_3.txt\n10000_4.txt\n10001_4.txt\n10002_1.txt\n10003_1.txt\nls: write error: Broken pipe\n\n!cat $path/train/neg/0_3.txt\nStory of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n\n# Create a `Learner` with a text classifier\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.460765\n\n\n0.407599\n\n\n0.815200\n\n\n01:25\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.316035\n\n\n0.262926\n\n\n0.895640\n\n\n02:46\n\n\n\n\n1\n\n\n0.250969\n\n\n0.223144\n\n\n0.908440\n\n\n02:48\n\n\n\n\n2\n\n\n0.186867\n\n\n0.187719\n\n\n0.926720\n\n\n02:48\n\n\n\n\n3\n\n\n0.146174\n\n\n0.190528\n\n\n0.927880\n\n\n02:50\n\n\n\n\n\n\n\nlearn.predict(\"I really liked that movie!\")\n('pos', TensorText(1), TensorText([5.5877e-04, 9.9944e-01]))\n\nlearn.predict(\"I really hated that movie!\")\n('neg', TensorText(0), TensorText([0.9534, 0.0466]))\n\n\nTabular Data\n\ndata that in in the form of a table\n\nspreedsheets\ndatabases\nComma-separated Values (CSV) files\n\nmodel tries to predict the value of one column based on information in other columns\n\n\nfrom fastai.tabular.all import *\nAdult Dataset * from the paper Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid * contains some demographic data about individuals\n\npath = untar_data(URLs.ADULT_SAMPLE)\nprint(path)\n/home/innom-dt/.fastai/data/adult_sample\n\n!ls $path\nadult.csv  export.pkl  models\n\nimport pandas as pd\n!cat $path/adult.csv | head -1\nage,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,hours-per-week,native-country,salary\ncat: write error: Broken pipe\n\npd.read_csv(f\"{path}/adult.csv\").head()\n\n\n\n\n\n\n\n\nage\n\n\nworkclass\n\n\nfnlwgt\n\n\neducation\n\n\neducation-num\n\n\nmarital-status\n\n\noccupation\n\n\nrelationship\n\n\nrace\n\n\nsex\n\n\ncapital-gain\n\n\ncapital-loss\n\n\nhours-per-week\n\n\nnative-country\n\n\nsalary\n\n\n\n\n\n\n0\n\n\n49\n\n\nPrivate\n\n\n101320\n\n\nAssoc-acdm\n\n\n12.0\n\n\nMarried-civ-spouse\n\n\nNaN\n\n\nWife\n\n\nWhite\n\n\nFemale\n\n\n0\n\n\n1902\n\n\n40\n\n\nUnited-States\n\n\n&gt;=50k\n\n\n\n\n1\n\n\n44\n\n\nPrivate\n\n\n236746\n\n\nMasters\n\n\n14.0\n\n\nDivorced\n\n\nExec-managerial\n\n\nNot-in-family\n\n\nWhite\n\n\nMale\n\n\n10520\n\n\n0\n\n\n45\n\n\nUnited-States\n\n\n&gt;=50k\n\n\n\n\n2\n\n\n38\n\n\nPrivate\n\n\n96185\n\n\nHS-grad\n\n\nNaN\n\n\nDivorced\n\n\nNaN\n\n\nUnmarried\n\n\nBlack\n\n\nFemale\n\n\n0\n\n\n0\n\n\n32\n\n\nUnited-States\n\n\n&lt;50k\n\n\n\n\n3\n\n\n38\n\n\nSelf-emp-inc\n\n\n112847\n\n\nProf-school\n\n\n15.0\n\n\nMarried-civ-spouse\n\n\nProf-specialty\n\n\nHusband\n\n\nAsian-Pac-Islander\n\n\nMale\n\n\n0\n\n\n0\n\n\n40\n\n\nUnited-States\n\n\n&gt;=50k\n\n\n\n\n4\n\n\n42\n\n\nSelf-emp-not-inc\n\n\n82297\n\n\n7th-8th\n\n\nNaN\n\n\nMarried-civ-spouse\n\n\nOther-service\n\n\nWife\n\n\nBlack\n\n\nFemale\n\n\n0\n\n\n0\n\n\n50\n\n\nUnited-States\n\n\n&lt;50k\n\n\n\n\n\n\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [\n        # Transform the categorical variables to something similar to `pd.Categorical`        \n        Categorify, \n        # Fill the missing values in continuous columns.\n        FillMissing,\n        # Normalize/denorm batch\n        Normalize\n    ], bs=64, seed=42, num_workers=8)\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.383882\n\n\n0.353906\n\n\n0.839834\n\n\n00:02\n\n\n\n\n1\n\n\n0.369853\n\n\n0.343141\n\n\n0.844134\n\n\n00:02\n\n\n\n\n2\n\n\n0.353572\n\n\n0.340899\n\n\n0.844441\n\n\n00:02\n\n\n\n\n\n\n\n\nRecommendation Systems\n\nmodel tries to predict the rating a user would give for something\n\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\nprint(path)\n/home/innom-dt/.fastai/data/movie_lens_sample\n\ndls = CollabDataLoaders.from_csv(path/'ratings.csv', bs=64, seed=42, num_workers=8)\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(10)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.503958\n\n\n1.418632\n\n\n00:00\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.376203\n\n\n1.359422\n\n\n00:00\n\n\n\n\n1\n\n\n1.252138\n\n\n1.180762\n\n\n00:00\n\n\n\n\n2\n\n\n1.017709\n\n\n0.880817\n\n\n00:00\n\n\n\n\n3\n\n\n0.798113\n\n\n0.741172\n\n\n00:00\n\n\n\n\n4\n\n\n0.688681\n\n\n0.708689\n\n\n00:00\n\n\n\n\n5\n\n\n0.648084\n\n\n0.697439\n\n\n00:00\n\n\n\n\n6\n\n\n0.631074\n\n\n0.693731\n\n\n00:00\n\n\n\n\n7\n\n\n0.608035\n\n\n0.691561\n\n\n00:00\n\n\n\n\n8\n\n\n0.609987\n\n\n0.691219\n\n\n00:00\n\n\n\n\n9\n\n\n0.607285\n\n\n0.691045\n\n\n00:00\n\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\nuserId\n\n\nmovieId\n\n\nrating\n\n\nrating_pred\n\n\n\n\n\n\n0\n\n\n63.0\n\n\n48.0\n\n\n5.0\n\n\n2.935433\n\n\n\n\n1\n\n\n70.0\n\n\n36.0\n\n\n4.0\n\n\n4.040259\n\n\n\n\n2\n\n\n65.0\n\n\n92.0\n\n\n4.5\n\n\n4.266208\n\n\n\n\n3\n\n\n47.0\n\n\n98.0\n\n\n5.0\n\n\n4.345912\n\n\n\n\n4\n\n\n4.0\n\n\n83.0\n\n\n3.5\n\n\n4.335626\n\n\n\n\n5\n\n\n4.0\n\n\n38.0\n\n\n4.5\n\n\n4.221953\n\n\n\n\n6\n\n\n59.0\n\n\n60.0\n\n\n5.0\n\n\n4.432968\n\n\n\n\n7\n\n\n86.0\n\n\n82.0\n\n\n4.0\n\n\n3.797124\n\n\n\n\n8\n\n\n55.0\n\n\n86.0\n\n\n4.5\n\n\n3.959364"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#validation-sets-and-test-sets",
    "href": "posts/fastai-book-notes/chapter-1/index.html#validation-sets-and-test-sets",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Validation Sets and Test Sets",
    "text": "Validation Sets and Test Sets\n\nchanges we make during the training process are influenced by the validation score\n\ncauses subsequent versions of the model to be indirectly shaped by the validation score\n\ntest set\n\na set of data that we the model trainers do not look at until we are finished training the model\nused to prevent us from over fitting our training process on the validation score\ncannot be used to improve the model\nespecially useful when hiring a 3rd-party to train a model\n\nkeep a test set hidden from the 3rd-party and test and use it to evaluate the final model\n\n\n\nUse Judgment in Defining Test Sets\n\nthe validation set and test set should be representative of the new data the model will encounter once deployed\na validation set for time series data should be a continuous section with the latest dates\ndetermine if your model will need to handle examples that are qualitatively different from the training set"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#jargon",
    "href": "posts/fastai-book-notes/chapter-1/index.html#jargon",
    "title": "Notes on fastai Book Ch. 1",
    "section": "Jargon",
    "text": "Jargon\n\narchitecture: the functional form of a model\n\npeople sometimes use model as a synonym for architecture\n\nblack box: something that gives predictions, but hides how it arrived at the prediction\n\ncommon misconception about deep learning models\n\nclassification model: attempts to predict a class or category\n\ndoes an image contain a cat or a dog\n\nConvolutional Neural Network: a type of neural network that works particularly well for computer vision tasks\nepoch\n\none complete pass through the dataset\n\nfinetune\n\nonly update parts of a pretrained model when training it on a new task\n\nhyperparameters\n\nparameters about parameters\nhigher-level choices that govern the meaning of the weight parameters\n\nindependent variable: the data not including the labels\nloss\n\na measure of performance that is used by the training system to update the model weights\nmeant for machine consumption\ndepends on the predictions and correct labels\n\nmetric\n\na function that measures the quality of the model’s prediction using the validation set\nmean for human consumption\n\nmodel: the combination of an architecture with a particular set of parameters\noverfitting\n\na large enough model can memorize the training set when allowed to train for long enough\nthe single most important and challenging issue when training a model\n\npredictions: output of the model\n\npredictions are calculated from the independent variable\n\npretrained model\n\na model that has weights which have already been trained on another dataset\nshould almost always use a pretrained model when available\n\nreduces training time for the target dataset\nreduces the amount of data needed for the new task\n\n\nregression model: attempts to predict one or more numeric quantities\n\nweather\nlocation\n\ntest set\n\na set of data that we the model trainers do not look at until we are finished training the model\nused to prevent us from over fitting our training process on the validation score\ncannot be used to improve the model\nespecially useful when hiring a 3rd-party to train a model\n\nvalidation set:\n\nused to measure the accuracy of the model during training\nhelps determine if the model is overfitting\nimportant to measure model performance on data that is not used to update its weight values\nwe want our model to perform well on previously unseen data, not just the examples it was trained on\n\ntransfer learning\n\ntraining a pre-trained model on a new dataset and task\nremoves the last layer of the pretrained model and replaces it with a new layer for the target dataset\nlast layer of the model is referred to as the head\n\nweights: are often called parameters"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-1/index.html#references",
    "href": "posts/fastai-book-notes/chapter-1/index.html#references",
    "title": "Notes on fastai Book Ch. 1",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nNext: Notes on fastai Book Ch. 2"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html",
    "href": "posts/fastai-book-notes/chapter-10/index.html",
    "title": "Notes on fastai Book Ch. 10",
    "section": "",
    "text": "NLP Deep Dive\nText Preprocessing\nTraining a Text Classifier\nDisinformation and Language Models\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html#nlp-deep-dive-rnns",
    "href": "posts/fastai-book-notes/chapter-10/index.html#nlp-deep-dive-rnns",
    "title": "Notes on fastai Book Ch. 10",
    "section": "NLP Deep Dive: RNNs",
    "text": "NLP Deep Dive: RNNs\n\nIn NLP, pretrained models are typically trained on a different type of task than your target task\nA language model is trained to predict the next word in a text (having read the ones before)\nWe do not feed the model labes, we just feed it lots of text\nthe model uses self-supervised learning to develop an understanding the underlying language of the text\nSelf-Supervised Learning: training a model using labels that are embedded in the independent variables, rather than requireing external labels\nSelf-supervised learning can also be used in other domains\n\nSelf-supervised learning and computer vision\n\nSelf-supervised learning is not usually used for the model that is trained directly\n\nused for pretraining a model that is then used for transfer learning\n\nA pretrained language model is often trained using a different body of text than the one you are targeting\n\ncan be useful to further pretrain the model on your target body of text\n\n\n\nUniversal Language Model Fine-tunine (ULMFiT)\n\nshowed that fine-tuning a language model on on the target body of text prior to transfer learning to a classification task, resulted in significantly better predictions"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html#text-preprocessing",
    "href": "posts/fastai-book-notes/chapter-10/index.html#text-preprocessing",
    "title": "Notes on fastai Book Ch. 10",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\n\ncan use an approach similar to preparing categorical variables\n\nMake a list of all possible levels of that categorical variable (called the vocab)\n\nTokenization: convert the text to a list of words\n\nReplace each level with its index in the vocab\n\nNumericalization: the process of mapping tokens to numbers\n\nList all of the unique words that appear, and convert eachword into a number by looking up its index in the vocab\n\n\nCreate an embedding matrix for this contianing a row for each item in the vocab\n\nLanguage model data loader creation: fastai provides an LMDataLoader that automatically handles creating a depdendent variable that is offset from the independent variable by one token. Also handles some important details such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\n\nUse this embedding matrix as the first layer of a neural network\n\na dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2\nLanguage Model Creation: need a special kind of model that can handle arbitrarily big or small input lists\n\n\nwe first concatenate all documents in our dataset into one big long string and split it into words (or tokens)\nour independent variable will be the sequence of words starting with the first word in our long list of words and ending with the second to last.\nour dependent variable will be the sequence of words starting with the second word and ending with the last word\nour vocab will consist of a mix of common words and new words specific to target dataset\nwe can use the corresponding rows in the embedding matrix for the pretrained model and only initialize new rows in the matrix for new words\n\n\nTokenization\ntoken: one element of a list created by the tokenization process. * could be a word, part of a word, or a single character\n\ntokenization is an active field of research, with new and improved tokenizers coming out all the time\n\n\nApproaches\n\nWord-based\n\nsplit a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces\npunctuation marks are typically split into separate tokens\nrelies on the assumption that spaces provide a useful separation of components of meaning in a sentence\nsome languages don’t have spaces or even a well-defined concept of a “word”\n\nSubword-based\n\nsplit words into smaller parts, based on the most commonly occurring sub-strings\nExample: “occasion” -&gt; “o c ca sion”\nhandles every human language without needing language specific algorithms to be develop\ncan handle other sequences like genomic sequences or MIDI music notation\n\nCharacter-based\n\nsplit a sentence into its individual characters\n\n\n\n\n\nWord Tokenization with fastai\n\nfastai provides a consistent interface to a range of tokenizers in external libraries\nThe default English word tokenizer for fastai uses spaCy\n\n\nfrom fastai.text.all import *\n\nURLs.IMDB\n'https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz'\n\npath = untar_data(URLs.IMDB)\npath\nPath('/home/innom-dt/.fastai/data/imdb')\n\nfastai get_text_files\n\nDocumentation\nGet text files in path recursively, only in folders, if specified.\n\n\nget_text_files\n&lt;function fastai.data.transforms.get_text_files(path, recurse=True, folders=None)&gt;\n\nprint_source(get_text_files)\ndef get_text_files(path, recurse=True, folders=None):\n    \"Get text files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=['.txt'], recurse=recurse, folders=folders)\n\nprint_source(get_files)\ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n    path = Path(path)\n    folders=L(folders)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n            else:                         d[:] = [o for o in d if not o.startswith('.')]\n            if len(folders) !=0 and i==0 and '.' not in folders: continue\n            res += _get_files(p, f, extensions)\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        res = _get_files(path, f, extensions)\n    return L(res)\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\nlen(files)\n100000\n\ntxt = files[0].open().read(); txt[:75]\n'This conglomeration fails so miserably on every level that it is difficult '\n\n\nfastai SpacyTokenizer\n\nDocumentation\n\n\nWordTokenizer\nfastai.text.core.SpacyTokenizer\n\nprint_source(WordTokenizer)\nclass SpacyTokenizer():\n    \"Spacy tokenizer for `lang`\"\n    def __init__(self, lang='en', special_toks=None, buf_sz=5000):\n        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n        nlp = spacy.blank(lang)\n        for w in self.special_toks: nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n        self.pipe,self.buf_sz = nlp.pipe,buf_sz\n\n    def __call__(self, items):\n        return (L(doc).attrgot('text') for doc in self.pipe(map(str,items), batch_size=self.buf_sz))\n\nfirst\n&lt;function fastcore.basics.first(x, f=None, negate=False, **kwargs)&gt;\n\nprint_source(first)\n&lt;function first at 0x7fdb8da3de50&gt;\ndef first(x, f=None, negate=False, **kwargs):\n    \"First element of `x`, optionally filtered by `f`, or None if missing\"\n    x = iter(x)\n    if f: x = filter_ex(x, f=f, negate=negate, gen=True, **kwargs)\n    return next(x, None)\n\ncoll_repr\n&lt;function fastcore.foundation.coll_repr(c, max_n=10)&gt;\n\nprint_source(coll_repr)\ndef coll_repr(c, max_n=10):\n    \"String repr of up to `max_n` items of (possibly lazy) collection `c`\"\n    return f'(#{len(c)}) [' + ','.join(itertools.islice(map(repr,c), max_n)) + (\n        '...' if len(c)&gt;max_n else '') + ']'\n\nspacy = WordTokenizer()\n\nspacy.buf_sz\n5000\n\nspacy.pipe\n&lt;bound method Language.pipe of &lt;spacy.lang.en.English object at 0x7fdb6545f1c0&gt;&gt;\n\nspacy.special_toks\n['xxunk',\n 'xxpad',\n 'xxbos',\n 'xxeos',\n 'xxfld',\n 'xxrep',\n 'xxwrep',\n 'xxup',\n 'xxmaj']\n\n# Wrap text in a list before feeding it to the tokenizer\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))\n(#174) ['This','conglomeration','fails','so','miserably','on','every','level','that','it','is','difficult','to','decide','what','to','say','.','It','does',\"n't\",'merit','one','line',',','much','less','ten',',','but'...]\n\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n\nTokenizer\nfastai.text.core.Tokenizer\n\nprint_source(Tokenizer)\nclass Tokenizer(Transform):\n    \"Provides a consistent `Transform` interface to tokenizers operating on `DataFrame`s and folders\"\n    input_types = (str, list, L, tuple, Path)\n    def __init__(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' '):\n        if isinstance(tok,type): tok=tok()\n        store_attr('tok,counter,lengths,mode,sep')\n        self.rules = defaults.text_proc_rules if rules is None else rules\n\n    @classmethod\n    @delegates(tokenize_df, keep=True)\n    def from_df(cls, text_cols, tok=None, rules=None, sep=' ', **kwargs):\n        if tok is None: tok = WordTokenizer()\n        res = cls(tok, rules=rules, mode='df')\n        res.kwargs,res.train_setup = merge({'tok': tok}, kwargs),False\n        res.text_cols,res.sep = text_cols,sep\n        return res\n\n    @classmethod\n    @delegates(tokenize_folder, keep=True)\n    def from_folder(cls, path, tok=None, rules=None, **kwargs):\n        path = Path(path)\n        if tok is None: tok = WordTokenizer()\n        output_dir = tokenize_folder(path, tok=tok, rules=rules, **kwargs)\n        res = cls(tok, counter=load_pickle(output_dir/fn_counter_pkl),\n                  lengths=load_pickle(output_dir/fn_lengths_pkl), rules=rules, mode='folder')\n        res.path,res.output_dir = path,output_dir\n        return res\n\n    def setups(self, dsets):\n        if not self.mode == 'df' or not isinstance(dsets.items, pd.DataFrame): return\n        dsets.items,count = tokenize_df(dsets.items, self.text_cols, rules=self.rules, **self.kwargs)\n        if self.counter is None: self.counter = count\n        return dsets\n\n    def encodes(self, o:Path):\n        if self.mode=='folder' and str(o).startswith(str(self.path)):\n            tok = self.output_dir/o.relative_to(self.path)\n            return L(tok.read_text(encoding='UTF-8').split(' '))\n        else: return self._tokenize1(o.read_text())\n\n    def encodes(self, o:str): return self._tokenize1(o)\n    def _tokenize1(self, o): return first(self.tok([compose(*self.rules)(o)]))\n\n    def get_lengths(self, items):\n        if self.lengths is None: return None\n        if self.mode == 'df':\n            if isinstance(items, pd.DataFrame) and 'text_lengths' in items.columns: return items['text_length'].values\n        if self.mode == 'folder':\n            try:\n                res = [self.lengths[str(Path(i).relative_to(self.path))] for i in items]\n                if len(res) == len(items): return res\n            except: return None\n\n    def decodes(self, o): return TitledStr(self.sep.join(o))\n\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n(#177) ['xxbos','xxmaj','this','conglomeration','fails','so','miserably','on','every','level','that','it','is','difficult','to','decide','what','to','say','.','xxmaj','it','does',\"n't\",'merit','one','line',',','much','less','ten'...]\n\n\nSpecial Tokens\n\ntokens that start with xx are special tokens\ndesigned to make it easier for a model to recognize the important parts of a sentence\nxxbos: Indicates the beginning of a text\nxxmaj: Indicates the next word begins with a capital letter (since everything is made lowercase)\nxxunk: Indicates the next word is unknown\n\n\n\nPreprocessing Rules\ndefaults.text_proc_rules\n[&lt;function fastai.text.core.fix_html(x)&gt;,\n &lt;function fastai.text.core.replace_rep(t)&gt;,\n &lt;function fastai.text.core.replace_wrep(t)&gt;,\n &lt;function fastai.text.core.spec_add_spaces(t)&gt;,\n &lt;function fastai.text.core.rm_useless_spaces(t)&gt;,\n &lt;function fastai.text.core.replace_all_caps(t)&gt;,\n &lt;function fastai.text.core.replace_maj(t)&gt;,\n &lt;function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;]\n\nfor rule in defaults.text_proc_rules:\n    print_source(rule)\ndef fix_html(x):\n    \"Various messy things we've seen in documents\"\n    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n        '#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('&lt;br /&gt;', \"\\n\").replace(\n        '\\\\\"', '\"').replace('&lt;unk&gt;',UNK).replace(' @.@ ','.').replace(' @-@ ','-').replace('...',' …')\n    return html.unescape(x)\n\ndef replace_rep(t):\n    \"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\n    def _replace_rep(m):\n        c,cc = m.groups()\n        return f' {TK_REP} {len(cc)+1} {c} '\n    return _re_rep.sub(_replace_rep, t)\n\ndef replace_wrep(t):\n    \"Replace word repetitions: word word word word -- TK_WREP 4 word\"\n    def _replace_wrep(m):\n        c,cc,e = m.groups()\n        return f' {TK_WREP} {len(cc.split())+2} {c} {e}'\n    return _re_wrep.sub(_replace_wrep, t)\n\ndef spec_add_spaces(t):\n    \"Add spaces around / and #\"\n    return _re_spec.sub(r' \\1 ', t)\n\ndef rm_useless_spaces(t):\n    \"Remove multiple spaces\"\n    return _re_space.sub(' ', t)\n\ndef replace_all_caps(t):\n    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n    def _replace_all_caps(m):\n        tok = f'{TK_UP} ' if len(m.groups()[1]) &gt; 1 else ''\n        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n    return _re_all_caps.sub(_replace_all_caps, t)\n\ndef replace_maj(t):\n    \"Replace tokens in Sentence Case by their lower version and add `TK_MAJ` before.\"\n    def _replace_maj(m):\n        tok = f'{TK_MAJ} ' if len(m.groups()[1]) &gt; 1 else ''\n        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n    return _re_maj.sub(_replace_maj, t)\n\ndef lowercase(t, add_bos=True, add_eos=False):\n    \"Converts `t` to lowercase\"\n    return (f'{BOS} ' if add_bos else '') + t.lower().strip() + (f' {EOS}' if add_eos else '')\n\n\nPostprocessing Rules\nfor rule in defaults.text_postproc_rules:\n    print_source(rule)\ndef replace_space(t):\n    \"Replace embedded spaces in a token with unicode line char to allow for split/join\"\n    return t.replace(' ', '_')\n\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\"\n\n\n\nSubword Tokenization\n\nProcess\n\nAnalyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab\nTokenize the corpus using this vocab of subword units.\n\n\ntxts = L(o.open().read() for o in files[:2000])\n\nSubwordTokenizer\nfastai.text.core.SentencePieceTokenizer\n\nprint_source(SubwordTokenizer)\nclass SentencePieceTokenizer():#TODO: pass the special tokens symbol to sp\n    \"SentencePiece tokenizer for `lang`\"\n    def __init__(self, lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000,\n                 model_type='unigram', char_coverage=None, cache_dir='tmp'):\n        try: from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n        except ImportError:\n            raise Exception('sentencepiece module is missing: run `pip install sentencepiece!=0.1.90,!=0.1.91`')\n        self.sp_model,self.cache_dir = sp_model,Path(cache_dir)\n        self.vocab_sz,self.max_vocab_sz,self.model_type = vocab_sz,max_vocab_sz,model_type\n        self.char_coverage = ifnone(char_coverage, 0.99999 if lang in eu_langs else 0.9998)\n        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n        if sp_model is None: self.tok = None\n        else:\n            self.tok = SentencePieceProcessor()\n            self.tok.Load(str(sp_model))\n        os.makedirs(self.cache_dir, exist_ok=True)\n\n    def _get_vocab_sz(self, raw_text_path):\n        cnt = Counter()\n        with open(raw_text_path, 'r') as f:\n            for line in f.readlines():\n                cnt.update(line.split())\n                if len(cnt)//4 &gt; self.max_vocab_sz: return self.max_vocab_sz\n        res = len(cnt)//4\n        while res%8 != 0: res+=1\n        return max(res,29)\n\n    def train(self, raw_text_path):\n        \"Train a sentencepiece tokenizer on `texts` and save it in `path/tmp_dir`\"\n        from sentencepiece import SentencePieceTrainer\n        vocab_sz = self._get_vocab_sz(raw_text_path) if self.vocab_sz is None else self.vocab_sz\n        spec_tokens = ['\\u2581'+s for s in self.special_toks]\n        SentencePieceTrainer.Train(\" \".join([\n            f\"--input={raw_text_path} --vocab_size={vocab_sz} --model_prefix={self.cache_dir/'spm'}\",\n            f\"--character_coverage={self.char_coverage} --model_type={self.model_type}\",\n            f\"--unk_id={len(spec_tokens)} --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2\",\n            f\"--user_defined_symbols={','.join(spec_tokens)} --hard_vocab_limit=false\"]))\n        raw_text_path.unlink()\n        return self.cache_dir/'spm.model'\n\n    def setup(self, items, rules=None):\n        from sentencepiece import SentencePieceProcessor\n        if rules is None: rules = []\n        if self.tok is not None: return {'sp_model': self.sp_model}\n        raw_text_path = self.cache_dir/'texts.out'\n        with open(raw_text_path, 'w') as f:\n            for t in progress_bar(maps(*rules, items), total=len(items), leave=False):\n                f.write(f'{t}\\n')\n        sp_model = self.train(raw_text_path)\n        self.tok = SentencePieceProcessor()\n        self.tok.Load(str(sp_model))\n        return {'sp_model': sp_model}\n\n    def __call__(self, items):\n        if self.tok is None: self.setup(items)\n        for t in items: yield self.tok.EncodeAsPieces(t)\n\n\nsentencepiece tokenizer\n\nGitHub Repository\nUnsupervised text tokenizer for Neural Network-based text generation.\n\n\ndef subword(sz):\n    # Initialize a tokenizer with the desired vocab size\n    sp = SubwordTokenizer(vocab_sz=sz)\n    # Generate vocab based on target body of text\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nPicking a vocab size\n\n\nprovides an easy way to scale between character tokenization and word tokenization\n\na smaller vocab size results in each token representing fewer characters\nan overly large vocab size results in most common words ending up in the vocab\n\nfewer tokens per sentence\nfaster training\nless memory\nless state for the model to remember\nlarger embedding matrices which require more data to learn\n\n\n\n# Use a vocab size of 1000\nsubword(1000)\nsentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false\n\"▁This ▁con g lo m er ation ▁fail s ▁so ▁mis er ably ▁on ▁every ▁level ▁that ▁it ▁is ▁di ff ic ul t ▁to ▁decide ▁what ▁to ▁say . ▁It ▁doesn ' t ▁me ri t ▁one ▁line ,\"\nNote: The _ character represents a space character in the original text\n# Use a vocab size of 200\nsubword(200)\n'▁ T h i s ▁c on g l o m er at ion ▁f a i l s ▁ s o ▁ m i s er a b ly ▁on ▁ e v er y ▁ le ve l'\n\n# Use a vocab size of 10000\nsubword(10000)\n\"▁This ▁con g l ome ration ▁fails ▁so ▁miserably ▁on ▁every ▁level ▁that ▁it ▁is ▁difficult ▁to ▁decide ▁what ▁to ▁say . ▁It ▁doesn ' t ▁merit ▁one ▁line , ▁much ▁less ▁ten , ▁but ▁to ▁adhere ▁to ▁the ▁rules\"\n\n\n\nNumericalization with fastai\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n(#177) ['xxbos','xxmaj','this','conglomeration','fails','so','miserably','on','every','level','that','it','is','difficult','to','decide','what','to','say','.','xxmaj','it','does',\"n't\",'merit','one','line',',','much','less','ten'...]\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n(#177) ['xxbos','xxmaj','this','conglomeration','fails','so','miserably','on','every','level'...]\n\nfastai Numericalize\n\nDocumentation\n\n\nNumericalize\nfastai.text.data.Numericalize\n\nprint_source(Numericalize)\nclass Numericalize(Transform):\n    \"Reversible transform of tokenized texts to numericalized ids\"\n    def __init__(self, vocab=None, min_freq=3, max_vocab=60000, special_toks=None):\n        store_attr('vocab,min_freq,max_vocab,special_toks')\n        self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)})\n\n    def setups(self, dsets):\n        if dsets is None: return\n        if self.vocab is None:\n            count = dsets.counter if getattr(dsets, 'counter', None) is not None else Counter(p for o in dsets for p in o)\n            if self.special_toks is None and hasattr(dsets, 'special_toks'):\n                self.special_toks = dsets.special_toks\n            self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab, special_toks=self.special_toks)\n            self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n\n    def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n    def decodes(self, o): return L(self.vocab[o_] for o_ in o)\n\nnum = Numericalize()\n# Generate vocab\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n\"(#1992) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','i','it','this'...]\"\n\nTensorText\nfastai.text.data.TensorText\n\nprint_source(TensorText)\nclass TensorText(TensorBase):   pass\n\nnums = num(toks)[:20]; nums\nTensorText([   2,    8,   19,    0,  585,   51, 1190,   36,  166,  586,   21,   18,   16,    0,   15,  663,   67,   15,  140,   10])\n\n' '.join(num.vocab[o] for o in nums)\n'xxbos xxmaj this xxunk fails so miserably on every level that it is xxunk to decide what to say .'\nNote: Special rules tokens appear first followed by tokens in order of frequency\n\n\n\nPutting Our Texts into Batches for a Language Model\nstream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\ntokens = tkn(stream)\ntokens\n(#90) ['xxbos','xxmaj','in','this','chapter',',','we','will','go','back'...]\n\n# Visualize 6 batches of 15 tokens\nbs,seq_len = 6,15\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\n\n\nxxbos\n\n\nxxmaj\n\n\nin\n\n\nthis\n\n\nchapter\n\n\n,\n\n\nwe\n\n\nwill\n\n\ngo\n\n\nback\n\n\nover\n\n\nthe\n\n\nexample\n\n\nof\n\n\nclassifying\n\n\n\n\nmovie\n\n\nreviews\n\n\nwe\n\n\nstudied\n\n\nin\n\n\nchapter\n\n\n1\n\n\nand\n\n\ndig\n\n\ndeeper\n\n\nunder\n\n\nthe\n\n\nsurface\n\n\n.\n\n\nxxmaj\n\n\n\n\nfirst\n\n\nwe\n\n\nwill\n\n\nlook\n\n\nat\n\n\nthe\n\n\nprocessing\n\n\nsteps\n\n\nnecessary\n\n\nto\n\n\nconvert\n\n\ntext\n\n\ninto\n\n\nnumbers\n\n\nand\n\n\n\n\nhow\n\n\nto\n\n\ncustomize\n\n\nit\n\n\n.\n\n\nxxmaj\n\n\nby\n\n\ndoing\n\n\nthis\n\n\n,\n\n\nwe\n\n\n’ll\n\n\nhave\n\n\nanother\n\n\nexample\n\n\n\n\nof\n\n\nthe\n\n\npreprocessor\n\n\nused\n\n\nin\n\n\nthe\n\n\ndata\n\n\nblock\n\n\nxxup\n\n\napi\n\n\n.\n\n\n\n\n\nxxmaj\n\n\nthen\n\n\nwe\n\n\n\n\nwill\n\n\nstudy\n\n\nhow\n\n\nwe\n\n\nbuild\n\n\na\n\n\nlanguage\n\n\nmodel\n\n\nand\n\n\ntrain\n\n\nit\n\n\nfor\n\n\na\n\n\nwhile\n\n\n.\n\n\n\n\n\n\n\n# 6 batches of 5 tokens\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\n\n\nxxbos\n\n\nxxmaj\n\n\nin\n\n\nthis\n\n\nchapter\n\n\n\n\nmovie\n\n\nreviews\n\n\nwe\n\n\nstudied\n\n\nin\n\n\n\n\nfirst\n\n\nwe\n\n\nwill\n\n\nlook\n\n\nat\n\n\n\n\nhow\n\n\nto\n\n\ncustomize\n\n\nit\n\n\n.\n\n\n\n\nof\n\n\nthe\n\n\npreprocessor\n\n\nused\n\n\nin\n\n\n\n\nwill\n\n\nstudy\n\n\nhow\n\n\nwe\n\n\nbuild\n\n\n\n\n\n\n\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\n\n\n,\n\n\nwe\n\n\nwill\n\n\ngo\n\n\nback\n\n\n\n\nchapter\n\n\n1\n\n\nand\n\n\ndig\n\n\ndeeper\n\n\n\n\nthe\n\n\nprocessing\n\n\nsteps\n\n\nnecessary\n\n\nto\n\n\n\n\nxxmaj\n\n\nby\n\n\ndoing\n\n\nthis\n\n\n,\n\n\n\n\nthe\n\n\ndata\n\n\nblock\n\n\nxxup\n\n\napi\n\n\n\n\na\n\n\nlanguage\n\n\nmodel\n\n\nand\n\n\ntrain\n\n\n\n\n\n\n\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\n\n\nover\n\n\nthe\n\n\nexample\n\n\nof\n\n\nclassifying\n\n\n\n\nunder\n\n\nthe\n\n\nsurface\n\n\n.\n\n\nxxmaj\n\n\n\n\nconvert\n\n\ntext\n\n\ninto\n\n\nnumbers\n\n\nand\n\n\n\n\nwe\n\n\n’ll\n\n\nhave\n\n\nanother\n\n\nexample\n\n\n\n\n.\n\n\n\n\n\nxxmaj\n\n\nthen\n\n\nwe\n\n\n\n\nit\n\n\nfor\n\n\na\n\n\nwhile\n\n\n.\n\n\n\n\n\n\n\nnums200 = toks200.map(num)\n\nLMDataLoader\nfastai.text.data.LMDataLoader\n\nprint_source(LMDataLoader)\n@delegates()\nclass LMDataLoader(TfmdDL):\n    \"A `DataLoader` suitable for language modeling\"\n    def __init__(self, dataset, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, **kwargs):\n        self.items = ReindexCollection(dataset, cache=cache, tfm=_maybe_first)\n        self.seq_len = seq_len\n        if lens is None: lens = _get_lengths(dataset)\n        if lens is None: lens = [len(o) for o in self.items]\n        self.lens = ReindexCollection(lens, idxs=self.items.idxs)\n        # The \"-1\" is to allow for final label, we throw away the end that's less than bs\n        corpus = round_multiple(sum(lens)-1, bs, round_down=True)\n        self.bl = corpus//bs #bl stands for batch length\n        self.n_batches = self.bl//(seq_len) + int(self.bl%seq_len!=0)\n        self.last_len = self.bl - (self.n_batches-1)*seq_len\n        self.make_chunks()\n        super().__init__(dataset=dataset, bs=bs, num_workers=num_workers, **kwargs)\n        self.n = self.n_batches*bs\n\n    def make_chunks(self): self.chunks = Chunks(self.items, self.lens)\n    def shuffle_fn(self,idxs):\n        self.items.shuffle()\n        self.make_chunks()\n        return idxs\n\n    def create_item(self, seq):\n        if seq is None: seq = 0\n        if seq&gt;=self.n: raise IndexError\n        sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len\n        st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len\n        txt = self.chunks[st : st+sl+1]\n        return LMTensorText(txt[:-1]),txt[1:]\n\n    @delegates(TfmdDL.new)\n    def new(self, dataset=None, seq_len=None, **kwargs):\n        lens = self.lens.coll if dataset is None else None\n        seq_len = self.seq_len if seq_len is None else seq_len\n        return super().new(dataset=dataset, lens=lens, seq_len=seq_len, **kwargs)\nNote: The order of separate documents is shuffled, not the order of the words inside them.\ndl = LMDataLoader(nums200)\n\nx,y = first(dl)\nx.shape,y.shape\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n' '.join(num.vocab[o] for o in x[0][:20])\n'xxbos xxmaj this xxunk fails so miserably on every level that it is xxunk to decide what to say .'\n\n' '.join(num.vocab[o] for o in y[0][:20])\n'xxmaj this xxunk fails so miserably on every level that it is xxunk to decide what to say . xxmaj'\nNote: The dependent variable is offset by one token, since the goal is to predict the next token in the sequence."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html#training-a-text-classifier",
    "href": "posts/fastai-book-notes/chapter-10/index.html#training-a-text-classifier",
    "title": "Notes on fastai Book Ch. 10",
    "section": "Training a Text Classifier",
    "text": "Training a Text Classifier\n\nFine-tune a language model pretrained on a standard corpus like Wikipedia on a target dataset\nUse the fine-tuned model to train a classifier\n\n\nLanguage Model Using DataBlock\n\nfastai automatically handles tokenization and numericalization when TextBlock is passed to DataBlock\nfastai saves the tokenized documents in a temporary fodler, so it does not have to tokenize them more than once\nfastai runs multiple tokenization processes in parallel\n\n\nTextBlock\nfastai.text.data.TextBlock\n\nprint_source(TextBlock)\nclass TextBlock(TransformBlock):\n    \"A `TransformBlock` for texts\"\n    @delegates(Numericalize.__init__)\n    def __init__(self, tok_tfm, vocab=None, is_lm=False, seq_len=72, backwards=False, **kwargs):\n        type_tfms = [tok_tfm, Numericalize(vocab, **kwargs)]\n        if backwards: type_tfms += [reverse_text]\n        return super().__init__(type_tfms=type_tfms,\n                                dl_type=LMDataLoader if is_lm else SortedDL,\n                                dls_kwargs={'seq_len': seq_len} if is_lm else {'before_batch': Pad_Chunk(seq_len=seq_len)})\n\n    @classmethod\n    @delegates(Tokenizer.from_df, keep=True)\n    def from_df(cls, text_cols, vocab=None, is_lm=False, seq_len=72, backwards=False, min_freq=3, max_vocab=60000, **kwargs):\n        \"Build a `TextBlock` from a dataframe using `text_cols`\"\n        return cls(Tokenizer.from_df(text_cols, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n\n    @classmethod\n    @delegates(Tokenizer.from_folder, keep=True)\n    def from_folder(cls, path, vocab=None, is_lm=False, seq_len=72, backwards=False, min_freq=3, max_vocab=60000, **kwargs):\n        \"Build a `TextBlock` from a `path`\"\n        return cls(Tokenizer.from_folder(path, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n\n# Define how to get dataset items\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\n\n\n\ntext\n\n\ntext_\n\n\n\n\n\n\n0\n\n\nxxbos i think xxmaj dark xxmaj angel is great ! xxmaj first season was excellent , and had a good plot . xxmaj with xxunk xxmaj alba ) as an escaped xxup xxunk , manticore creation , trying to adapt to a normal life , but still ” saving the world ” . xxmaj and being hunted by manticore throughout the season which gives the series some extra spice . xxmaj the second season though suddenly became a bit\n\n\ni think xxmaj dark xxmaj angel is great ! xxmaj first season was excellent , and had a good plot . xxmaj with xxunk xxmaj alba ) as an escaped xxup xxunk , manticore creation , trying to adapt to a normal life , but still ” saving the world ” . xxmaj and being hunted by manticore throughout the season which gives the series some extra spice . xxmaj the second season though suddenly became a bit odd\n\n\n\n\n1\n\n\ncheating boyfriend xxmaj nick xxmaj gordon planning to drop her for the much younger and sexier xxmaj sally xxmaj higgins . xxmaj sally ’s boyfriend xxmaj jerry had earlier participated in a payroll robbery with xxmaj nick where he and a security guard were shot and killed . xxmaj now seeing that there ’s a future , in crime , for her with xxmaj nick xxmaj sally willingly replaced xxmaj mimi as xxmaj nick ’s new squeeze . xxmaj mad\n\n\nboyfriend xxmaj nick xxmaj gordon planning to drop her for the much younger and sexier xxmaj sally xxmaj higgins . xxmaj sally ’s boyfriend xxmaj jerry had earlier participated in a payroll robbery with xxmaj nick where he and a security guard were shot and killed . xxmaj now seeing that there ’s a future , in crime , for her with xxmaj nick xxmaj sally willingly replaced xxmaj mimi as xxmaj nick ’s new squeeze . xxmaj mad as\n\n\n\n\n\n\n\n\nFine-Tuning the Language Model\n\nUse embeddings to convert the integer word indices into activations that we can use for our neural network\nFeed those embeddings to a Recurrent Neural Nerwork (RNN), using an architecture called AWD-LSTM\n\n\nThis process is handled automatically inside language_model_learner\n\n\nlanguage_model_learner\n&lt;function fastai.text.learner.language_model_learner(dls, arch, config=None, drop_mult=1.0, backwards=False, pretrained=True, pretrained_fnames=None, loss_func=None, opt_func=&lt;function Adam at 0x7fdb8123e430&gt;, lr=0.001, splitter=&lt;function trainable_params at 0x7fdb8d9171f0&gt;, cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))&gt;\n\nprint_source(language_model_learner)\n@delegates(Learner.__init__)\ndef language_model_learner(dls, arch, config=None, drop_mult=1., backwards=False, pretrained=True, pretrained_fnames=None, **kwargs):\n    \"Create a `Learner` with a language model from `dls` and `arch`.\"\n    vocab = _get_text_vocab(dls)\n    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n    meta = _model_meta[arch]\n    learn = LMLearner(dls, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n    url = 'url_bwd' if backwards else 'url'\n    if pretrained or pretrained_fnames:\n        if pretrained_fnames is not None:\n            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n        else:\n            if url not in meta:\n                warn(\"There are no pretrained weights for that architecture yet!\")\n                return learn\n            model_path = untar_data(meta[url] , c_key='model')\n            try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n            except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n        learn = learn.load_pretrained(*fnames)\n    return learn\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()\n\nPerplexity Metric\n\nthe exponential of the loss (i.e. torch.exp(cross_entropy))\noften used in NLP for language models\n\n\nPerplexity\nfastai.metrics.Perplexity\n\nprint_source(Perplexity)\nclass Perplexity(AvgLoss):\n    \"Perplexity (exponential of cross-entropy loss) for Language Models\"\n    @property\n    def value(self): return torch.exp(self.total/self.count) if self.count != 0 else None\n    @property\n    def name(self):  return \"perplexity\"\n\n# Train only the embeddings (the only part of the model that contains randomly initialize weights)\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\nperplexity\n\n\ntime\n\n\n\n\n\n\n0\n\n\n4.011688\n\n\n3.904507\n\n\n0.300504\n\n\n49.625618\n\n\n09:21\n\n\n\n\n\n\n\n\n\nSaving and Loading Models\nlearn.save\n&lt;bound method Learner.save of &lt;fastai.text.learner.LMLearner object at 0x7fdb64fd4190&gt;&gt;\n\nprint_source(learn.save)\n@patch\n@delegates(save_model)\ndef save(self:Learner, file, **kwargs):\n    \"Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\"\n    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n    save_model(file, self.model, getattr(self,'opt',None), **kwargs)\n    return file\n\nsave_model\n&lt;function fastai.learner.save_model(file, model, opt, with_opt=True, pickle_protocol=2)&gt;\n\nprint_source(save_model)\ndef save_model(file, model, opt, with_opt=True, pickle_protocol=2):\n    \"Save `model` to `file` along with `opt` (if available, and if `with_opt`)\"\n    if rank_distrib(): return # don't save if child proc\n    if opt is None: with_opt=False\n    state = get_model(model).state_dict()\n    if with_opt: state = {'model': state, 'opt':opt.state_dict()}\n    torch.save(state, file, pickle_protocol=pickle_protocol)\n\nprint_source(rank_distrib)\ndef rank_distrib():\n    \"Return the distributed rank of this process (if applicable).\"\n    return int(os.environ.get('RANK', 0))\n\nprint_source(get_model)\ndef get_model(model):\n    \"Return the model maybe wrapped inside `model`.\"\n    return model.module if isinstance(model, (DistributedDataParallel, nn.DataParallel)) else model\n\nprint_source(torch.save)\ndef save(obj, f: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n         pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True) -&gt; None:\n    # Reference: https://github.com/pytorch/pytorch/issues/54354\n    # The first line of this docstring overrides the one Sphinx generates for the\n    # documentation. We need it so that Sphinx doesn't leak `pickle`s path from\n    # the build environment (e.g. `&lt;module 'pickle' from '/leaked/path').\n\n    \"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\n\n    Saves an object to a disk file.\n\n    See also: :ref:`saving-loading-tensors`\n\n    Args:\n        obj: saved object\n        f: a file-like object (has to implement write and flush) or a string or\n           os.PathLike object containing a file name\n        pickle_module: module used for pickling metadata and objects\n        pickle_protocol: can be specified to override the default protocol\n\n    .. note::\n        A common PyTorch convention is to save tensors using .pt file extension.\n\n    .. note::\n        PyTorch preserves storage sharing across serialization. See\n        :ref:`preserve-storage-sharing` for more details.\n\n    .. note::\n        The 1.6 release of PyTorch switched ``torch.save`` to use a new\n        zipfile-based file format. ``torch.load`` still retains the ability to\n        load files in the old format. If for any reason you want ``torch.save``\n        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n\n    Example:\n        &gt;&gt;&gt; # Save to file\n        &gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])\n        &gt;&gt;&gt; torch.save(x, 'tensor.pt')\n        &gt;&gt;&gt; # Save to io.BytesIO buffer\n        &gt;&gt;&gt; buffer = io.BytesIO()\n        &gt;&gt;&gt; torch.save(x, buffer)\n    \"\"\"\n    _check_dill_version(pickle_module)\n\n    with _open_file_like(f, 'wb') as opened_file:\n        if _use_new_zipfile_serialization:\n            with _open_zipfile_writer(opened_file) as opened_zipfile:\n                _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n                return\n        _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n\nlearn.save('1epoch')\nPath('/home/innom-dt/.fastai/data/imdb/models/1epoch.pth')\n\nlearn.load\n&lt;bound method TextLearner.load of &lt;fastai.text.learner.LMLearner object at 0x7fdb64fd4190&gt;&gt;\n\nprint_source(learn.load)\n    @delegates(load_model_text)\n    def load(self, file, with_opt=None, device=None, **kwargs):\n        if device is None: device = self.dls.device\n        if self.opt is None: self.create_opt()\n        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n        load_model_text(file, self.model, self.opt, device=device, **kwargs)\n        return self\n\nload_model_text\n&lt;function fastai.text.learner.load_model_text(file, model, opt, with_opt=None, device=None, strict=True)&gt;\n\nprint_source(load_model_text)\ndef load_model_text(file, model, opt, with_opt=None, device=None, strict=True):\n    \"Load `model` from `file` along with `opt` (if available, and if `with_opt`)\"\n    distrib_barrier()\n    if isinstance(device, int): device = torch.device('cuda', device)\n    elif device is None: device = 'cpu'\n    state = torch.load(file, map_location=device)\n    hasopt = set(state)=={'model', 'opt'}\n    model_state = state['model'] if hasopt else state\n    get_model(model).load_state_dict(clean_raw_keys(model_state), strict=strict)\n    if hasopt and ifnone(with_opt,True):\n        try: opt.load_state_dict(state['opt'])\n        except:\n            if with_opt: warn(\"Could not load the optimizer state.\")\n    elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n\ndistrib_barrier\n&lt;function fastai.torch_core.distrib_barrier()&gt;\n\nprint_source(distrib_barrier)\ndef distrib_barrier():\n    \"Place a synchronization barrier in distributed training\"\n    if num_distrib() &gt; 1 and torch.distributed.is_initialized(): torch.distributed.barrier()\n\nlearn = learn.load('1epoch')\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\nperplexity\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.767039\n\n\n3.763731\n\n\n0.316231\n\n\n43.108986\n\n\n09:44\n\n\n\n\n1\n\n\n3.692761\n\n\n3.705623\n\n\n0.323240\n\n\n40.675396\n\n\n09:29\n\n\n\n\n2\n\n\n3.634718\n\n\n3.654817\n\n\n0.328937\n\n\n38.660458\n\n\n09:31\n\n\n\n\n3\n\n\n3.563724\n\n\n3.624163\n\n\n0.332917\n\n\n37.493317\n\n\n09:32\n\n\n\n\n4\n\n\n3.486968\n\n\n3.600153\n\n\n0.335374\n\n\n36.603825\n\n\n09:36\n\n\n\n\n5\n\n\n3.435516\n\n\n3.585277\n\n\n0.337806\n\n\n36.063351\n\n\n09:30\n\n\n\n\n6\n\n\n3.363010\n\n\n3.575442\n\n\n0.339413\n\n\n35.710388\n\n\n09:18\n\n\n\n\n7\n\n\n3.300442\n\n\n3.574242\n\n\n0.340387\n\n\n35.667561\n\n\n09:22\n\n\n\n\n8\n\n\n3.247055\n\n\n3.576924\n\n\n0.340627\n\n\n35.763359\n\n\n09:14\n\n\n\n\n9\n\n\n3.210976\n\n\n3.581657\n\n\n0.340366\n\n\n35.933022\n\n\n09:18\n\n\n\n\n\n\n\nEncoder\n\nthe model not including the task-specific final layer(s)\ntypically used to refer to the body of NLP and generative models\n\n\nlearn.save_encoder\n&lt;bound method TextLearner.save_encoder of &lt;fastai.text.learner.LMLearner object at 0x7fdb64fd4190&gt;&gt;\n\nprint_source(learn.save_encoder)\n    def save_encoder(self, file):\n        \"Save the encoder to `file` in the model directory\"\n        if rank_distrib(): return # don't save if child proc\n        encoder = get_model(self.model)[0]\n        if hasattr(encoder, 'module'): encoder = encoder.module\n        torch.save(encoder.state_dict(), join_path_file(file, self.path/self.model_dir, ext='.pth'))\n\nlearn.save_encoder('finetuned')\n\n\n\nText Generation\n\nTraining the model to predict the next word of a sentence enables it to generate new reviews\n\n\n# Prompt\nTEXT = \"I liked this movie because\"\n# Generate 40 new words\nN_WORDS = 40\nN_SENTENCES = 2\n# Add some randomness (temperature) to prevent generating the same review twice \npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\n\nprint(\"\\n\".join(preds))\ni liked this movie because it was a very well done film . Lee Bowman does a wonderful job in his role . Being a big Astaire fan , i think this movie is worth seeing for the musical numbers .\ni liked this movie because it was based on a true story . The script was excellent as it was great . i would recommend this movie to anyone interested in history and the history of the holocaust . It was great to\nNote: The model has learned a lot about English sentences, despite not having any explicitely programmed knowledge.\n\n\nCreating the Classifier DataLoaders\n\nvery similar to the DataBlocks used for the image classification datasets\ndata augmentation has not been well-explored\nneed to pad smaller documents when creating mini-batches\n\nbatches are padded based on the largest document in a given batch\n\nthe data block API automatically handles sorting and padding when using TextBlock with is_lm=False\n\n\nGrandparentSplitter\n&lt;function fastai.data.transforms.GrandparentSplitter(train_name='train', valid_name='valid')&gt;\n\nprint_source(GrandparentSplitter)\ndef GrandparentSplitter(train_name='train', valid_name='valid'):\n    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n    def _inner(o):\n        return _grandparent_idxs(o, train_name),_grandparent_idxs(o, valid_name)\n    return _inner\n\ndls_clas = DataBlock(\n    # Pass in the vocab used by the language model\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\ndls_clas.show_batch(max_n=3)\n\n\n\n\n\n\n\n\ntext\n\n\ncategory\n\n\n\n\n\n\n0\n\n\nxxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\n\n\npos\n\n\n\n\n1\n\n\nxxbos xxmaj by now you ’ve probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki ’s classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released ” kiki ’s xxmaj delivery xxmaj service ” on video which included a preview of the xxmaj laputa dub saying it was due out in ” 1 xxrep 3 9 ” . xxmaj it ’s obviously way past that year now , but the dub has been finally completed . xxmaj and it ’s not ” laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky ” , just ” castle xxmaj in xxmaj the xxmaj sky ” for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times\n\n\npos\n\n\n\n\n2\n\n\nxxbos xxmaj heavy - handed moralism . xxmaj writers using characters as mouthpieces to speak for themselves . xxmaj predictable , plodding plot points ( say that five times fast ) . a child ’s imitation of xxmaj britney xxmaj spears . xxmaj this film has all the earmarks of a xxmaj lifetime xxmaj special reject . i honestly believe that xxmaj jesus xxmaj xxunk and xxmaj julia xxmaj xxunk set out to create a thought - provoking , emotional film on a tough subject , exploring the idea that things are not always black and white , that one who is a criminal by definition is not necessarily a bad human being , and that there can be extenuating circumstances , especially when one puts the well - being of a child first . xxmaj however , their earnestness ends up being channeled into preachy dialogue and trite\n\n\nneg\n\n\n\n\n\n\n\nnums_samp = toks200[:10].map(num)\n\nnums_samp.map(len)\n(#10) [177,562,211,125,125,425,421,1330,196,278]\n\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\nlearn = learn.load_encoder('finetuned')\n\n\nFine-Tuning the Classifier\n\nNLP classifiers benefit from gradually unfreezing a few layers at a time\n\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.242196\n\n\n0.178359\n\n\n0.931280\n\n\n00:29\n\n\n\n\n\n\n\n# Freeze all except the last two parameter groups\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.226292\n\n\n0.162955\n\n\n0.938840\n\n\n00:35\n\n\n\n\n\n\n\n# Freeze all except the last two parameter groups\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.150115\n\n\n0.144669\n\n\n0.947280\n\n\n00:46\n\n\n\n\n\n\n\n# Unfreeze all layers\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.160042\n\n\n0.149997\n\n\n0.945320\n\n\n00:54\n\n\n\n\n1\n\n\n0.146106\n\n\n0.148102\n\n\n0.945320\n\n\n00:54\n\n\n\n\n\n\nNote: We can further improve the accuracy by training another model on all the texts read backward and averaging the predictions of the two models."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html#disinformation-and-language-models",
    "href": "posts/fastai-book-notes/chapter-10/index.html#disinformation-and-language-models",
    "title": "Notes on fastai Book Ch. 10",
    "section": "Disinformation and Language Models",
    "text": "Disinformation and Language Models\n\nEven simple algorithms based on rules could be used to create fraudulent accounts and try influence policymakers\nMore than a Million Pro-Repeal Net Neutrality Comments were Likely Faked\n\nJeff Kao discovered a large cluster of comments opposing net neutrality that seemed to have been generated by some sort of Mad Libs-style mail merge.\nestimated that less than 800,000 of the 22M+ comments could be considered unique\nmore than 99% of the truly unique comments were in favor of net neutrality\n\nThe same type of language model as trained above could be used to generate context-appropriate, believable text"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-10/index.html#references",
    "href": "posts/fastai-book-notes/chapter-10/index.html#references",
    "title": "Notes on fastai Book Ch. 10",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 9\nNext: Notes on fastai Book Ch. 11"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-11/index.html",
    "href": "posts/fastai-book-notes/chapter-11/index.html",
    "title": "Notes on fastai Book Ch. 11",
    "section": "",
    "text": "Going Deeper into fastai’s Layered API\nTfmdLists and Datasets: Transformed Collections\nApplying the Mid-Level Data API: SiamesePair\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-11/index.html#going-deeper-into-fastais-layered-api",
    "href": "posts/fastai-book-notes/chapter-11/index.html#going-deeper-into-fastais-layered-api",
    "title": "Notes on fastai Book Ch. 11",
    "section": "Going Deeper into fastai’s Layered API",
    "text": "Going Deeper into fastai’s Layered API\n\nfastai is built on a layered API\n\n\nfrom fastai.text.all import *\n\nprint_source(TextDataLoaders.from_folder)\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_folder(cls, path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, text_vocab=None, is_lm=False,\n                    tok_tfm=None, seq_len=72, backwards=False, **kwargs):\n        \"Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)\"\n        splitter = GrandparentSplitter(train_name=train, valid_name=valid) if valid_pct is None else RandomSplitter(valid_pct, seed=seed)\n        blocks = [TextBlock.from_folder(path, text_vocab, is_lm, seq_len, backwards, tok=tok_tfm)]\n        if not is_lm: blocks.append(CategoryBlock(vocab=vocab))\n        get_items = partial(get_text_files, folders=[train,valid]) if valid_pct is None else get_text_files\n        dblock = DataBlock(blocks=blocks,\n                           get_items=get_items,\n                           splitter=splitter,\n                           get_y=None if is_lm else parent_label)\n        return cls.from_dblock(dblock, path, path=path, seq_len=seq_len, **kwargs)\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nNote: The above from_folder method expects the dataset to be arranged in a certain way.\npath = untar_data(URLs.IMDB)\npath\nPath('/home/innom-dt/.fastai/data/imdb')\n\nMide-Level API\n\ncontains functionality for creating DataLoaders\nHas a callback system which allows us to customize the training loop, along with the general optimizer\n\n\nData Block API\n\nAllows us to define custom approaches for loading data from datasets\n\n\nprint_source(DataBlock)\n@docs\n@funcs_kwargs\nclass DataBlock():\n    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n    get_x=get_items=splitter=get_y = None\n    blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n    _methods = 'get_items splitter get_y get_x'.split()\n    _msg = \"If you wanted to compose several transforms in your getter don't forget to wrap them in a `Pipeline`.\"\n    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs):\n        blocks = L(self.blocks if blocks is None else blocks)\n        blocks = L(b() if callable(b) else b for b in blocks)\n        self.type_tfms = blocks.attrgot('type_tfms', L())\n        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n        for b in blocks:\n            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n        if dl_type is not None: self.dl_type = dl_type\n        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n\n        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n        if self.get_x:\n            if len(L(self.get_x)) != self.n_inp:\n                raise ValueError(f'get_x contains {len(L(self.get_x))} functions, but must contain {self.n_inp} (one for each input)\\n{self._msg}')\n            self.getters[:self.n_inp] = L(self.get_x)\n        if self.get_y:\n            n_targs = len(self.getters) - self.n_inp\n            if len(L(self.get_y)) != n_targs:\n                raise ValueError(f'get_y contains {len(L(self.get_y))} functions, but must contain {n_targs} (one for each target)\\n{self._msg}')\n            self.getters[self.n_inp:] = L(self.get_y)\n\n        if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n        self.new(item_tfms, batch_tfms)\n\n    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n        lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n\n    def new(self, item_tfms=None, batch_tfms=None):\n        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n        return self\n\n    @classmethod\n    def from_columns(cls, blocks=None, getters=None, get_items=None, **kwargs):\n        if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n        get_items = _zip if get_items is None else compose(get_items, _zip)\n        return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n\n    def datasets(self, source, verbose=False):\n        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n        splits = (self.splitter or RandomSplitter())(items)\n        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n\n    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n        dsets = self.datasets(source, verbose=verbose)\n        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n\n    _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n                 datasets=\"Create a `Datasets` object from `source`\",\n                 dataloaders=\"Create a `DataLoaders` object from `source`\")\n\nprint_source(TextBlock.from_folder)\n    @classmethod\n    @delegates(Tokenizer.from_folder, keep=True)\n    def from_folder(cls, path, vocab=None, is_lm=False, seq_len=72, backwards=False, min_freq=3, max_vocab=60000, **kwargs):\n        \"Build a `TextBlock` from a `path`\"\n        return cls(Tokenizer.from_folder(path, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n\nprint_source(CategoryBlock)\ndef CategoryBlock(vocab=None, sort=True, add_na=False):\n    \"`TransformBlock` for single-label categorical targets\"\n    return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na))\n\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)\n\n\n\nTransforms\n\nA Transform object implements any encoding, setup steps for an inner state, and decoding steps required for a type of data\nNot every type of Transform needs to implement setup steps\nData augmentation transforms do not need a decode method\nTransforms always get applied over tuples\n\ndata is almost always in a tuple of (input, target)\n\n\n\nTransform\nfastcore.transform.Transform\n\nprint_source(Transform)\nclass Transform(metaclass=_TfmMeta):\n    \"Delegates (`__call__`,`decode`,`setup`) to (&lt;code&gt;encodes&lt;/code&gt;,&lt;code&gt;decodes&lt;/code&gt;,&lt;code&gt;setups&lt;/code&gt;) if `split_idx` matches\"\n    split_idx,init_enc,order,train_setup = None,None,0,None\n    def __init__(self, enc=None, dec=None, split_idx=None, order=None):\n        self.split_idx = ifnone(split_idx, self.split_idx)\n        if order is not None: self.order=order\n        self.init_enc = enc or dec\n        if not self.init_enc: return\n\n        self.encodes,self.decodes,self.setups = TypeDispatch(),TypeDispatch(),TypeDispatch()\n        if enc:\n            self.encodes.add(enc)\n            self.order = getattr(enc,'order',self.order)\n            if len(type_hints(enc)) &gt; 0: self.input_types = first(type_hints(enc).values())\n            self._name = _get_name(enc)\n        if dec: self.decodes.add(dec)\n\n    @property\n    def name(self): return getattr(self, '_name', _get_name(self))\n    def __call__(self, x, **kwargs): return self._call('encodes', x, **kwargs)\n    def decode  (self, x, **kwargs): return self._call('decodes', x, **kwargs)\n    def __repr__(self): return f'{self.name}:\\nencodes: {self.encodes}decodes: {self.decodes}'\n\n    def setup(self, items=None, train_setup=False):\n        train_setup = train_setup if self.train_setup is None else self.train_setup\n        return self.setups(getattr(items, 'train', items) if train_setup else items)\n\n    def _call(self, fn, x, split_idx=None, **kwargs):\n        if split_idx!=self.split_idx and self.split_idx is not None: return x\n        return self._do_call(getattr(self, fn), x, **kwargs)\n\n    def _do_call(self, f, x, **kwargs):\n        if not _is_tuple(x):\n            if f is None: return x\n            ret = f.returns(x) if hasattr(f,'returns') else None\n            return retain_type(f(x, **kwargs), x, ret)\n        res = tuple(self._do_call(f, x_, **kwargs) for x_ in x)\n        return retain_type(res, x)\n\nTypeDispatch\nfastcore.dispatch.TypeDispatch\n\nprint_source(TypeDispatch)\nclass TypeDispatch:\n    \"Dictionary-like object; `__getitem__` matches keys of types using `issubclass`\"\n    def __init__(self, funcs=(), bases=()):\n        self.funcs,self.bases = _TypeDict(),L(bases).filter(is_not(None))\n        for o in L(funcs): self.add(o)\n        self.inst = None\n        self.owner = None\n\n    def add(self, f):\n        \"Add type `t` and function `f`\"\n        if isinstance(f, staticmethod): a0,a1 = _p2_anno(f.__func__)\n        else: a0,a1 = _p2_anno(f)\n        t = self.funcs.d.get(a0)\n        if t is None:\n            t = _TypeDict()\n            self.funcs.add(a0, t)\n        t.add(a1, f)\n\n    def first(self):\n        \"Get first function in ordered dict of type:func.\"\n        return self.funcs.first().first()\n\n    def returns(self, x):\n        \"Get the return type of annotation of `x`.\"\n        return anno_ret(self[type(x)])\n\n    def _attname(self,k): return getattr(k,'__name__',str(k))\n    def __repr__(self):\n        r = [f'({self._attname(k)},{self._attname(l)}) -&gt; {getattr(v, \"__name__\", type(v).__name__)}'\n             for k in self.funcs.d for l,v in self.funcs[k].d.items()]\n        r = r + [o.__repr__() for o in self.bases]\n        return '\\n'.join(r)\n\n    def __call__(self, *args, **kwargs):\n        ts = L(args).map(type)[:2]\n        f = self[tuple(ts)]\n        if not f: return args[0]\n        if isinstance(f, staticmethod): f = f.__func__\n        elif self.inst is not None: f = MethodType(f, self.inst)\n        elif self.owner is not None: f = MethodType(f, self.owner)\n        return f(*args, **kwargs)\n\n    def __get__(self, inst, owner):\n        self.inst = inst\n        self.owner = owner\n        return self\n\n    def __getitem__(self, k):\n        \"Find first matching type that is a super-class of `k`\"\n        k = L(k)\n        while len(k)&lt;2: k.append(object)\n        r = self.funcs.all_matches(k[0])\n        for t in r:\n            o = t[k[1]]\n            if o is not None: return o\n        for base in self.bases:\n            res = base[k]\n            if res is not None: return res\n        return None\n\n# Get list of individual text files\nfiles = get_text_files(path, folders = ['train', 'test'])\n# Extract contents of text files\ntxts = L(o.open().read() for o in files[:2000])\nprint_source(Tokenizer.from_folder)\n    @classmethod\n    @delegates(tokenize_folder, keep=True)\n    def from_folder(cls, path, tok=None, rules=None, **kwargs):\n        path = Path(path)\n        if tok is None: tok = WordTokenizer()\n        output_dir = tokenize_folder(path, tok=tok, rules=rules, **kwargs)\n        res = cls(tok, counter=load_pickle(output_dir/fn_counter_pkl),\n                  lengths=load_pickle(output_dir/fn_lengths_pkl), rules=rules, mode='folder')\n        res.path,res.output_dir = path,output_dir\n        return res\n\nprint_source(tokenize_folder)\n@delegates(_tokenize_files)\ndef tokenize_folder(path, extensions=None, folders=None, output_dir=None, skip_if_exists=True, **kwargs):\n    \"Tokenize text files in `path` in parallel using `n_workers`\"\n    path,extensions = Path(path),ifnone(extensions, ['.txt'])\n    files = get_files(path, extensions=extensions, recurse=True, folders=folders)\n    def _f(i,output_dir): return output_dir/files[i].relative_to(path)\n    return _tokenize_files(_f, files, path, skip_if_exists=skip_if_exists, **kwargs)\n\ntok = Tokenizer.from_folder(path)\ntok.setup(txts)\ntoks = txts.map(tok)\ntoks[0]\n(#177) ['xxbos','xxmaj','this','conglomeration','fails','so','miserably','on','every','level'...]\n\n# Initialize the numericalizer\nnum = Numericalize()\n# Set up vocab\nnum.setup(toks)\n# Get number for tokens\nnums = toks.map(num)\nnums[0][:10]\nTensorText([   2,    8,   20,    0,  615,   49, 1689,   35,  195,  599])\n\n# Decode numbers to tokens\nnums_dec = num.decode(nums[0][:10]); nums_dec\n(#10) ['xxbos','xxmaj','this','xxunk','fails','so','miserably','on','every','level']\n\n# Decode numericalized list of tokens to a single string of words\ntok.decode(nums_dec)\n'xxbos xxmaj this xxunk fails so miserably on every level'\nNote: The default WordTokenizer cannot decode special tokens back to the original text.\n# Tokenize a single tuple of (input, target)\ntok((txts[0], txts[1]))\n((#177) ['xxbos','xxmaj','this','conglomeration','fails','so','miserably','on','every','level'...],\n (#562) ['xxbos','xxmaj','jacqueline','xxmaj','susann','wrote','several','novels','all','involving'...])\n\n\nWriting Your Own Transform\n\neasiest way is to write a function\n\nspecify a type in the function signature so the transform is only applied to the target data types\n\nTransforms that require either setup or decode, need to be implemented as a subclass of Transform\n\n\n# Simple transform function that adds 1 to an int\ndef f(x:int): return x+1\n# Initialize a Transform object with the transform function\ntfm = Transform(f)\n# Try to apply the transform to an in and a float\ntfm(2),tfm(2.0)\n(3, 2.0)\n\nPython Decorators\n\nspecial syntax for passing a function to another function (or something like a function)\nused by prepending a callable with a @ and placing it before a function definition\nPEP 318 – Decorators for Functions and Methods\nPrimer on Python Decorators\n\n\n\nPython typing.Callable\n\nDocumentation\nSomething that can be called\na function or something like a function\n\n\n@Transform\ndef f(x:int): return x+1\nf(2),f(2.0)\n(3, 2.0)\n\nclass NormalizeMean(Transform):\n    # Calculate the mean for the dataset and maintain it as inner state\n    def setups(self, items): self.mean = sum(items)/len(items)\n    # Encode items by substracting the mean\n    def encodes(self, x): return x-self.mean\n    # Decode items by adding the mean\n    def decodes(self, x): return x+self.mean\n\n# Instantiate the custom transform\ntfm = NormalizeMean()\n# Initialize the inner state for the transform\n# (i.e. calculate the mean for the dataset)\ntfm.setup([1,2,3,4,5])\nstart = 2\n# Apply transform to the value 2\ny = tfm(start)\nz = tfm.decode(y)\ntfm.mean,y,z\n(3.0, -1.0, 2.0)\n\n\n\nPipeline\n\ncompose several transforms together\npass in a list of Transforms\ncalls the Transforms in sequential order\nNeed to use a TfmdLists to properly setup a pipeline of transforms on some data.\n\n\nPipeline\nfastcore.transform.Pipeline\n\nprint_source(Pipeline)\nclass Pipeline:\n    \"A pipeline of composed (for encode/decode) transforms, setup with types\"\n    def __init__(self, funcs=None, split_idx=None):\n        self.split_idx,self.default = split_idx,None\n        if funcs is None: funcs = []\n        if isinstance(funcs, Pipeline): self.fs = funcs.fs\n        else:\n            if isinstance(funcs, Transform): funcs = [funcs]\n            self.fs = L(ifnone(funcs,[noop])).map(mk_transform).sorted(key='order')\n        for f in self.fs:\n            name = camel2snake(type(f).__name__)\n            a = getattr(self,name,None)\n            if a is not None: f = L(a)+f\n            setattr(self, name, f)\n\n    def setup(self, items=None, train_setup=False):\n        tfms = self.fs[:]\n        self.fs.clear()\n        for t in tfms: self.add(t,items, train_setup)\n\n    def add(self,ts, items=None, train_setup=False):\n        if not is_listy(ts): ts=[ts]\n        for t in ts: t.setup(items, train_setup)\n        self.fs+=ts\n        self.fs = self.fs.sorted(key='order')\n\n    def __call__(self, o): return compose_tfms(o, tfms=self.fs, split_idx=self.split_idx)\n    def __repr__(self): return f\"Pipeline: {' -&gt; '.join([f.name for f in self.fs if f.name != 'noop'])}\"\n    def __getitem__(self,i): return self.fs[i]\n    def __setstate__(self,data): self.__dict__.update(data)\n    def __getattr__(self,k): return gather_attrs(self, k, 'fs')\n    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'fs')\n\n    def decode  (self, o, full=True):\n        if full: return compose_tfms(o, tfms=self.fs, is_enc=False, reverse=True, split_idx=self.split_idx)\n        #Not full means we decode up to the point the item knows how to show itself.\n        for f in reversed(self.fs):\n            if self._is_showable(o): return o\n            o = f.decode(o, split_idx=self.split_idx)\n        return o\n\n    def show(self, o, ctx=None, **kwargs):\n        o = self.decode(o, full=False)\n        o1 = (o,) if not _is_tuple(o) else o\n        if hasattr(o, 'show'): ctx = o.show(ctx=ctx, **kwargs)\n        else:\n            for o_ in o1:\n                if hasattr(o_, 'show'): ctx = o_.show(ctx=ctx, **kwargs)\n        return ctx\n\n    def _is_showable(self, o):\n        if hasattr(o, 'show'): return True\n        if _is_tuple(o): return all(hasattr(o_, 'show') for o_ in o)\n        return False\n\ncamel2snake\n&lt;function fastcore.basics.camel2snake(name)&gt;\n\nprint_source(camel2snake)\ndef camel2snake(name):\n    \"Convert CamelCase to snake_case\"\n    s1   = re.sub(_camel_re1, r'\\1_\\2', name)\n    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n\n# Create a pipeline that first tokenizes and then numericalizes a text dataset\ntfms = Pipeline([tok, num])\nt = tfms(txts[0]); t[:20]\nTensorText([   2,    8,   20,    0,  615,   49, 1689,   35,  195,  599,   21,   17,   16, 1088,   15, 1177,   65,   15,  145,   10])\n\ntfms.decode(t)[:100]\n'xxbos xxmaj this xxunk fails so miserably on every level that it is difficult to decide what to say '"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-11/index.html#tfmdlists-and-datasets-transformed-collections",
    "href": "posts/fastai-book-notes/chapter-11/index.html#tfmdlists-and-datasets-transformed-collections",
    "title": "Notes on fastai Book Ch. 11",
    "section": "TfmdLists and Datasets: Transformed Collections",
    "text": "TfmdLists and Datasets: Transformed Collections\n\nyour data is usually a set of raw items to which you want to apply a succession of transformations\n\n\nTfmdLists\n\ngroups your pipeline with your raw items\nuse when you have manually written a Transform that performs all your preprocessing at once, which turns raw items into a tuple with inputs and targets\npass in items and a list of Transforms\nautomatically calls the setup method of each Transform in order when initialized\ncan index into the TfmdLists to get the result of the Pipeline on any raw element\nknows how to properly decode transformed items\ncan handle a training and validation set\ncan directly convert a TfmdLists to a DataLoaders object with the dataloaders method (inherited from FilteredBase)\n\n\nNote: You will typically have two or more parallel pipelines of transforms: * one for processing your raw items into inputs * one to process your raw items into targets\n\nTfmdLists\nfastai.data.core.TfmdLists\n\nprint_source(TfmdLists)\nclass TfmdLists(FilteredBase, L, GetAttr):\n    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n    _default='tfms'\n    def __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True,\n                 splits=None, types=None, verbose=False, dl_type=None):\n        super().__init__(items, use_list=use_list)\n        if dl_type is not None: self._dl_type = dl_type\n        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n        if isinstance(tfms,Pipeline): do_setup=False\n        self.tfms = Pipeline(tfms, split_idx=split_idx)\n        store_attr('types,split_idx')\n        if do_setup:\n            pv(f\"Setting up {self.tfms}\", verbose)\n            self.setup(train_setup=train_setup)\n\n    def _new(self, items, split_idx=None, **kwargs):\n        split_idx = ifnone(split_idx,self.split_idx)\n        return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs)\n    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n    def _after_item(self, o): return self.tfms(o)\n    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n    def __iter__(self): return (self[i] for i in range(len(self)))\n    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n    def new_empty(self): return self._new([])\n\n    def setup(self, train_setup=True):\n        self.tfms.setup(self, train_setup)\n        if len(self) != 0:\n            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n            self.types = []\n            for f in self.tfms.fs:\n                self.types.append(getattr(f, 'input_types', type(x)))\n                x = f(x)\n            self.types.append(type(x))\n        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n\n    def infer_idx(self, x):\n        # TODO: check if we really need this, or can simplify\n        idx = 0\n        for t in self.types:\n            if isinstance(x, t): break\n            idx += 1\n        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n        assert idx &lt; len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n        return idx\n\n    def infer(self, x):\n        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n\n    def __getitem__(self, idx):\n        res = super().__getitem__(idx)\n        if self._after_item is None: return res\n        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)\n\nprint_source(FilteredBase)\nclass FilteredBase:\n    \"Base class for lists with subsets\"\n    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n    def __init__(self, *args, dl_type=None, **kwargs):\n        if dl_type is not None: self._dl_type = dl_type\n        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def n_subsets(self): return len(self.splits)\n    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n    def subset(self): raise NotImplemented\n\n    def dataloaders(self, bs=64, shuffle_train=None, shuffle=True, val_shuffle=False,n=None, path='.', dl_type=None, dl_kwargs=None,\n                    device=None,drop_last=None,val_bs=None, **kwargs):\n        if shuffle_train is not None:\n            shuffle=shuffle_train\n            warnings.warn('`shuffle_train` is deprecated. Use `shuffle` instead.',DeprecationWarning)\n        if device is None: device=default_device()\n        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n        if dl_type is None: dl_type = self._dl_type\n        if drop_last is None: drop_last = shuffle\n        val_kwargs={k[4:]:v for k,v in kwargs.items() if k.startswith('val_')}\n        def_kwargs = {'bs':bs,'shuffle':shuffle,'drop_last':drop_last,'n':n,'device':device}\n        dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))\n        def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}\n        dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n                      for i in range(1, self.n_subsets)]\n        return self._dbunch_type(*dls, path=path, device=device)\n\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])\nt = tls[0]; t[:20]\nTensorText([    2,     8,    20, 33649,  1033,    52,  3265,    35,   193,   649,    21,    17,    16,   890,    15,  1156,    65,    15,   159,    10])\n\ntls.decode(t)[:100]\n'xxbos xxmaj this conglomeration fails so miserably on every level that it is difficult to decide wha'\nNote: The sentence is properly decoded including the word “conglomeration”.\ntls.show(t)\nxxbos xxmaj this conglomeration fails so miserably on every level that it is difficult to decide what to say . xxmaj it does n't merit one line , much less ten , but to adhere to the rules of imdb , here goes and i probably wo n't succeed the first time around and have to type some more to make up for this submission to be accepted . xxup lol \n\n xxmaj if i had seen this schlock during the ' 70s while i was going through my mushroom phase , i would have still considered it unimaginative and shallow . xxmaj the most exciting shot for me was the long shot when the elevator door opened and xxunk was on the edge of my seat . \n\n xxmaj one person on here wrote that he had met the creator of this mess , as if that were a red letter day in his life . xxmaj one can only pray that something far more exciting occurs in that posters xxunk a grip , amigo .\n\n# Define how many files to keep in the training set\ncut = int(len(files)*0.8)\ncut\n40000\n\n# Split the files into a training and validation set\nsplits = [list(range(cut)), list(range(cut,len(files)))]\nlen(splits[0]), len(splits[1])\n(40000, 10000)\n\n# Initialize a TfmdLists with indices for a training and validation set\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], \n                splits=splits)\n\ntls.valid[0][:20]\nTensorText([   2,    8,    9,  107,   77,   19,   87,    9, 1836,   14, 1438,   15,   86,   20,   45,   31, 1524,  393,   25,  164])\n\nparent_label\n&lt;function fastai.data.transforms.parent_label(o)&gt;\n\nprint_source(parent_label)\ndef parent_label(o):\n    \"Label `item` with the parent folder name.\"\n    return Path(o).parent.name\n\n# Label files based on the parent folder name\nlbls = files.map(parent_label)\nlbls\n(#50000) ['neg','neg','neg','neg','neg','neg','neg','neg','neg','neg'...]\n\nCategorize\nfastai.data.transforms.Categorize\n\nprint_source(Categorize)\nclass Categorize(DisplayedTransform):\n    \"Reversible transform of category string to `vocab` id\"\n    loss_func,order=CrossEntropyLossFlat(),1\n    def __init__(self, vocab=None, sort=True, add_na=False):\n        if vocab is not None: vocab = CategoryMap(vocab, sort=sort, add_na=add_na)\n        store_attr()\n\n    def setups(self, dsets):\n        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets, sort=self.sort, add_na=self.add_na)\n        self.c = len(self.vocab)\n\n    def encodes(self, o):\n        try:\n            return TensorCategory(self.vocab.o2i[o])\n        except KeyError as e:\n            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n    def decodes(self, o): return Category      (self.vocab    [o])\n\n# Grab all unique items and build a vocab\ncat = Categorize()\ncat.setup(lbls)\ncat.vocab, cat(lbls[0])\n\n# Create a TfmdLists that labels items using the parent folder name and build a vocab\ntls_y = TfmdLists(files, [parent_label, Categorize()])\ntls_y[0]\nTensorCategory(0)\n\n\nDatasets\n\napplies two (or more) pipelines in parallel to the same raw object and build a tuple with the result\nautomatically performs the setup steps\ncan index into a Datasets to get a tuple with the results of each pipeline\ncan pass along a splits value for a training and validation split\ncan decode any processed tuple\ncan directly convert a Datasets to a DataLoaders object with the dataloaders method (inherited from FilteredBase)\n\n\nDatasets\nfastai.data.core.Datasets\n\nprint_source(Datasets)\n@docs\n@delegates(TfmdLists)\nclass Datasets(FilteredBase):\n    \"A dataset that creates a tuple from each `tfms`\"\n    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n        super().__init__(dl_type=dl_type)\n        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n        self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1))\n\n    def __getitem__(self, it):\n        res = tuple([tl[it] for tl in self.tls])\n        return res if is_indexer(it) else list(zip(*res))\n\n    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n    def __len__(self): return len(self.tls[0])\n    def __iter__(self): return (self[i] for i in range(len(self)))\n    def __repr__(self): return coll_repr(self)\n    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n    def new_empty(self): return type(self)(tls=[tl.new_empty() for tl in self.tls], n_inp=self.n_inp)\n    @property\n    def splits(self): return self.tls[0].splits\n    @property\n    def split_idx(self): return self.tls[0].tfms.split_idx\n    @property\n    def items(self): return self.tls[0].items\n    @items.setter\n    def items(self, v):\n        for tl in self.tls: tl.items = v\n\n    def show(self, o, ctx=None, **kwargs):\n        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n        return ctx\n\n    @contextmanager\n    def set_split_idx(self, i):\n        old_split_idx = self.split_idx\n        for tl in self.tls: tl.tfms.split_idx = i\n        try: yield self\n        finally:\n            for tl in self.tls: tl.tfms.split_idx = old_split_idx\n\n    _docs=dict(\n        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n        show=\"Show item `o` in `ctx`\",\n        dataloaders=\"Get a `DataLoaders`\",\n        overlapping_splits=\"All splits that are in more than one split\",\n        subset=\"New `Datasets` that only includes subset `i`\",\n        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n    )\n\n# Tokenize and numericalize inputs\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\n# Label and categorize targets\ny_tfms = [parent_label, Categorize()]\n# Initialize a Datasets\ndsets = Datasets(files, [x_tfms, y_tfms])\nx,y = dsets[0]\nx[:20],y\n(TensorText([    2,     8,    20, 33649,  1033,    52,  3265,    35,   193,   649,    21,    17,    16,   890,    15,  1156,    65,    15,   159,    10]),\n TensorCategory(0))\n\n# Tokenize and numericalize inputs\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\n# Label and categorize targets\ny_tfms = [parent_label, Categorize()]\n# Initialize a Datasets with a splits\ndsets = Datasets(files, [x_tfms, y_tfms], splits=splits)\nx,y = dsets.valid[0]\nx[:20],y\n(TensorText([   2,    8,    9,  107,   77,   19,   87,    9, 1836,   14, 1438,   15,   86,   20,   45,   31, 1524,  393,   25,  164]),\n TensorCategory(1))\n\nt = dsets.valid[0]\ndsets.decode(t)\n('xxbos xxmaj the first time i had the window of opportunity to see this all but forgotten classic was back in the early xxunk , at one of our art houses as a revival . xxmaj as i watched this fever dream of an exercise in 1930 \\'s sexuality , i thought xxup yowza ! xxmaj they got away with murder in xxmaj europe back in the day . xxmaj unfortunately , this film was heavily cut in it \\'s original xxup u.s . release by the blue nosed xxmaj hayes xxmaj office ( the staunch government censorship board , started by the \" holier than thou \" xxmaj bible thumper , xxmaj will xxmaj hayes … a former xxmaj post xxmaj office official , if you can believe xxunk to it \\'s overall theme of human sexuality ( heaven \\'s forbid humans actually had sex in the 1930 \\'s ) . xxmaj the plot of xxmaj ecstasy concerns a young woman ( played by xxmaj hedy xxmaj xxunk marries a much older man , and later regrets it . xxmaj she ( xxunk a handsome younger man & has an affair with him , resulting in a divorce from previous husband ( another no no in xxmaj hollywood movies back then xxrep 3 - xxmaj divorce ! ) . xxmaj despite the fact that the film was produced in 1933 , it was probably the director \\'s first time working in the sound format ( i.e. the film seems to possess techniques that were used mostly in silent films xxrep 3 - i.e. 1920 \\'s expressionism \\'s ) . xxmaj it \\'s still worth searching out for a window into early xxmaj european talking pictures , along with xxmaj luis xxmaj bunuels l\\'age xxmaj dor ( xxunk xxmaj karl xxmaj gustav xxmaj dryer \\'s \\' vampyre \\' ( 1931 ) . xxmaj not rated , but contains that infamous nude swimming scene & some thinly veiled sexual references , which would fare little more than a xxup pg-13 by today \\'s standards ( but would have easily landed the dreaded \\' xxunk in the xxunk , if it existed then )',\n 'pos')\n\nprint(type(pad_input))\npad_input\n&lt;class 'fastai.text.data.Pad_Input'&gt;\nPad_Input:\nencodes: (object,object) -&gt; encodes\ndecodes: (TensorText,object) -&gt; decodes\n\nprint_source(Pad_Input)\nclass Pad_Input(ItemTransform):\n    def encodes(self,samples, pad_idx=1, pad_fields=0, pad_first=False, backwards=False):\n        \"Function that collect `samples` and adds padding\"\n        self.pad_idx = pad_idx\n        pad_fields = L(pad_fields)\n        max_len_l = pad_fields.map(lambda f: max([len(s[f]) for s in samples]))\n        if backwards: pad_first = not pad_first\n        def _f(field_idx, x):\n            if field_idx not in pad_fields: return x\n            idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed\n            sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x))\n            pad =  x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx\n            x1 = torch.cat([pad, x] if pad_first else [x, pad])\n            if backwards: x1 = x1.flip(0)\n            return retain_type(x1, x)\n        return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples]\n    def decodes(self, o:TensorText):\n        pad_idx = self.pad_idx if hasattr(self,'pad_idx') else 1\n        return o[o != pad_idx]\n\n\nDataLoader\n\nfastai’s DataLoader expands on the PyTorch DataLoader class\nresponsible for collating the items from a dataset into batches\n\nafter_item:\n\napplied on each item after grabbing it inside the dataset\nequivalent of item_tfms in DataBlock\n\nbefore_batch:\n\napplied on the list of items before they are collated.\nideal place to pad items ot the same size\n\nafter_batch:\n\napplied on the batch as a whole after its construction\nequivalent of batch_tfms in DataBlock\n\n\nDataLoader\nfastai.data.load.DataLoader\n\nprint_source(DataLoader)\n@funcs_kwargs\nclass DataLoader(GetAttr):\n    _noop_methods = 'wif before_iter after_item before_batch after_batch after_iter'.split()\n    for o in _noop_methods: exec(f\"def {o}(self, x=None, *args, **kwargs): return x\")\n    _methods = _noop_methods + 'create_batches create_item create_batch retain \\\n        get_idxs sample shuffle_fn do_batch create_batch'.split()\n    _default = 'dataset'\n    def __init__(self, dataset=None, bs=None, num_workers=0, pin_memory=False, timeout=0, batch_size=None,\n                 shuffle=False, drop_last=False, indexed=None, n=None, device=None, persistent_workers=False, **kwargs):\n        if batch_size is not None: bs = batch_size # PyTorch compatibility\n        assert not (bs is None and drop_last)\n        if indexed is None: indexed = (hasattr(dataset,'__getitem__')\n                                       and not isinstance(dataset, IterableDataset))\n        if not indexed and shuffle: raise ValueError(\"Can only shuffle an indexed dataset (not an iterable one).\")\n        if n is None:\n            try: n = len(dataset)\n            except TypeError: pass\n        store_attr('dataset,bs,shuffle,drop_last,indexed,n,pin_memory,timeout,device')\n        self.rng,self.num_workers,self.offs = random.Random(random.randint(0,2**32-1)),1,0\n        if sys.platform == \"win32\" and IN_NOTEBOOK and num_workers &gt; 0:\n            print(\"Due to IPython and Windows limitation, python multiprocessing isn't available now.\")\n            print(\"So `number_workers` is changed to 0 to avoid getting stuck\")\n            num_workers = 0\n        self.fake_l = _FakeLoader(self, pin_memory, num_workers, timeout, persistent_workers=persistent_workers)\n\n    def __len__(self):\n        if self.n is None: raise TypeError\n        if self.bs is None: return self.n\n        return self.n//self.bs + (0 if self.drop_last or self.n%self.bs==0 else 1)\n\n    def get_idxs(self):\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(itertools.islice(idxs, self.n))\n        if self.shuffle: idxs = self.shuffle_fn(idxs)\n        return idxs\n\n    def sample(self):\n        return (b for i,b in enumerate(self.__idxs) if i//(self.bs or 1)%self.num_workers==self.offs)\n\n    def __iter__(self):\n        self.randomize()\n        self.before_iter()\n        self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)\n        for b in _loaders[self.fake_l.num_workers==0](self.fake_l):\n            if self.device is not None: b = to_device(b, self.device)\n            yield self.after_batch(b)\n        self.after_iter()\n        if hasattr(self, 'it'): del(self.it)\n\n    def create_batches(self, samps):\n        if self.dataset is not None: self.it = iter(self.dataset)\n        res = filter(lambda o:o is not None, map(self.do_item, samps))\n        yield from map(self.do_batch, self.chunkify(res))\n\n    def new(self, dataset=None, cls=None, **kwargs):\n        if dataset is None: dataset = self.dataset\n        if cls is None: cls = type(self)\n        cur_kwargs = dict(dataset=dataset, num_workers=self.fake_l.num_workers, pin_memory=self.pin_memory, timeout=self.timeout,\n                          bs=self.bs, shuffle=self.shuffle, drop_last=self.drop_last, indexed=self.indexed, device=self.device)\n        for n in self._methods:\n            o = getattr(self, n)\n            if not isinstance(o, MethodType): cur_kwargs[n] = o\n        return cls(**merge(cur_kwargs, kwargs))\n\n    @property\n    def prebatched(self): return self.bs is None\n    def do_item(self, s):\n        try: return self.after_item(self.create_item(s))\n        except SkipItemException: return None\n    def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last)\n    def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))\n    def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1))\n    def retain(self, res, b):  return retain_types(res, b[0] if is_listy(b) else b)\n    def create_item(self, s):\n        if self.indexed: return self.dataset[s or 0]\n        elif s is None:  return next(self.it)\n        else: raise IndexError(\"Cannot index an iterable dataset numerically - must use `None`.\")\n    def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b)\n    def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b)\n    def to(self, device): self.device = device\n    def one_batch(self):\n        if self.n is not None and len(self)==0: raise ValueError(f'This DataLoader does not contain any batches')\n        with self.fake_l.no_multiproc(): res = first(self)\n        if hasattr(self, 'it'): delattr(self, 'it')\n        return res\n\nGetAttr\nfastcore.basics.GetAttr\n\nprint_source(GetAttr)\nclass GetAttr:\n    \"Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`\"\n    _default='default'\n    def _component_attr_filter(self,k):\n        if k.startswith('__') or k in ('_xtra',self._default): return False\n        xtra = getattr(self,'_xtra',None)\n        return xtra is None or k in xtra\n    def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)]\n    def __getattr__(self,k):\n        if self._component_attr_filter(k):\n            attr = getattr(self,self._default,None)\n            if attr is not None: return getattr(attr,k)\n        raise AttributeError(k)\n    def __dir__(self): return custom_dir(self,self._dir())\n#     def __getstate__(self): return self.__dict__\n    def __setstate__(self,data): self.__dict__.update(data)\n\n# Create a DataLoaders object from the Datasets object\n# Add padding to input before batch\ndls = dsets.dataloaders(bs=64, before_batch=pad_input)\n\nSortedDL\n\nconstructs batches by putting samples of roughly the same lengths into batches\n\n\nprint_source(SortedDL)\n@delegates(TfmdDL)\nclass SortedDL(TfmdDL):\n    \"A `DataLoader` that goes throught the item in the order given by `sort_func`\"\n    def __init__(self, dataset, sort_func=None, res=None, **kwargs):\n        super().__init__(dataset, **kwargs)\n        self.sort_func = _default_sort if sort_func is None else sort_func\n        if res is None and self.sort_func == _default_sort: res = _get_lengths(dataset)\n        self.res = [self.sort_func(self.do_item(i)) for i in range_of(self.dataset)] if res is None else res\n        if len(self.res) &gt; 0: self.idx_max = np.argmax(self.res)\n\n    def get_idxs(self):\n        idxs = super().get_idxs()\n        if self.shuffle: return idxs\n        return sorted(idxs, key=lambda i: self.res[i], reverse=True)\n\n    def shuffle_fn(self,idxs):\n        idxs = np.random.permutation(len(self.dataset))\n        idx_max = np.where(idxs==self.idx_max)[0][0]\n        idxs[0],idxs[idx_max] = idxs[idx_max],idxs[0]\n        sz = self.bs*50\n        chunks = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n        chunks = [sorted(s, key=lambda i: self.res[i], reverse=True) for s in chunks]\n        sort_idx = np.concatenate(chunks)\n\n        sz = self.bs\n        batches = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n        sort_idx = np.concatenate(np.random.permutation(batches[1:-1])) if len(batches) &gt; 2 else np.array([],dtype=np.int)\n        sort_idx = np.concatenate((batches[0], sort_idx) if len(batches)==1 else (batches[0], sort_idx, batches[-1]))\n        return iter(sort_idx)\n\n    @delegates(TfmdDL.new)\n    def new(self, dataset=None, **kwargs):\n        if 'val_res' in kwargs and kwargs['val_res'] is not None: res = kwargs['val_res']\n        else: res = self.res if dataset is None else None\n        return super().new(dataset=dataset, res=res, **kwargs)\n\n# Full code necessary to prepare the data for text classification\ntfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]]\nfiles = get_text_files(path, folders = ['train', 'test'])\nsplits = GrandparentSplitter(valid_name='test')(files)\ndsets = Datasets(files, tfms, splits=splits)\ndls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-11/index.html#applying-the-mid-level-data-api-siamesepair",
    "href": "posts/fastai-book-notes/chapter-11/index.html#applying-the-mid-level-data-api-siamesepair",
    "title": "Notes on fastai Book Ch. 11",
    "section": "Applying the Mid-Level Data API: SiamesePair",
    "text": "Applying the Mid-Level Data API: SiamesePair\n\nSiamese Model\n\ntakes two images and has to determine whether they are of the same class\n\n\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.PETS)\npath\nPath('/home/innom-dt/.fastai/data/oxford-iiit-pet')\n\nfiles = get_image_files(path/\"images\")\nfiles\n(#7390) [Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Birman_121.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/shiba_inu_131.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bombay_176.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bengal_199.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/beagle_41.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/beagle_27.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/great_pyrenees_181.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bengal_100.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/keeshond_124.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/havanese_115.jpg')...]\n\nfastuple\nfastcore.basics.fastuple\n\nprint_source(fastuple)\nclass fastuple(tuple):\n    \"A `tuple` with elementwise ops and more friendly __init__ behavior\"\n    def __new__(cls, x=None, *rest):\n        if x is None: x = ()\n        if not isinstance(x,tuple):\n            if len(rest): x = (x,)\n            else:\n                try: x = tuple(iter(x))\n                except TypeError: x = (x,)\n        return super().__new__(cls, x+rest if rest else x)\n\n    def _op(self,op,*args):\n        if not isinstance(self,fastuple): self = fastuple(self)\n        return type(self)(map(op,self,*map(cycle, args)))\n\n    def mul(self,*args):\n        \"`*` is already defined in `tuple` for replicating, so use `mul` instead\"\n        return fastuple._op(self, operator.mul,*args)\n\n    def add(self,*args):\n        \"`+` is already defined in `tuple` for concat, so use `add` instead\"\n        return fastuple._op(self, operator.add,*args)\n\n# Custom type to allow us to show siamese image pairs\n# Tracks whether the two images belong to the same class\nclass SiameseImage(fastuple):\n    # Implement a custom show method to display the image pair\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        # Convert to Tensors\n        if not isinstance(img1, Tensor):\n            # Resize images to the same size\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            # Reorder color channels\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        # Create a separating line between the two images\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n\nimg = PILImage.create(files[0])\n# Create a siamese pair of the same image\ns = SiameseImage(img, img, True)\ns.show();\n\n\n\n\n\n\nimg1 = PILImage.create(files[1])\ns1 = SiameseImage(img, img1, False)\ns1.show();\n\n\n\n\n\n\nResize\nfastai.vision.augment.Resize\n\nprint_source(Resize)\n@delegates()\nclass Resize(RandTransform):\n    split_idx,mode,mode_mask,order = None,Image.BILINEAR,Image.NEAREST,1\n    \"Resize image to `size` using `method`\"\n    def __init__(self, size, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection,\n                 resamples=(Image.BILINEAR, Image.NEAREST), **kwargs):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n        self.mode,self.mode_mask = resamples\n\n    def before_call(self, b, split_idx):\n        if self.method==ResizeMethod.Squish: return\n        self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random())\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n        orig_sz = _get_sz(x)\n        if self.method==ResizeMethod.Squish:\n            return x.crop_pad(orig_sz, fastuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\n\n        w,h = orig_sz\n        op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad]\n        m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1]\n        cp_sz = (int(m*self.size[0]),int(m*self.size[1]))\n        tl = fastuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1])))\n        return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\n\ns2 = Resize(224)(s1)\ns2.show();\n\n\n\n\n\n\n# Label images by filename\ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        # Generate list of unique labels\n        self.labels = files.map(label_func).unique()\n        # Create a dictionary to match labels to filenames\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) \n                          for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        # Create siamese image pair\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        # 50/50 chance of generating a pair of the same class\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: \n            cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n\nprint_source(RandomSplitter)\ndef RandomSplitter(valid_pct=0.2, seed=None):\n    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n    def _inner(o):\n        if seed is not None: torch.manual_seed(seed)\n        rand_idx = L(list(torch.randperm(len(o)).numpy()))\n        cut = int(valid_pct * len(o))\n        return rand_idx[cut:],rand_idx[:cut]\n    return _inner\n\nhelp(torch.randperm)\nHelp on built-in function randperm:\n\nrandperm(...)\n    randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor\n    \n    Returns a random permutation of integers from ``0`` to ``n - 1``.\n    \n    Args:\n        n (int): the upper bound (exclusive)\n    \n    Keyword args:\n        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n        out (Tensor, optional): the output tensor.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n            Default: ``torch.int64``.\n        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n            Default: ``torch.strided``.\n        device (:class:`torch.device`, optional): the desired device of returned tensor.\n            Default: if ``None``, uses the current device for the default tensor type\n            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n            for CPU tensor types and the current CUDA device for CUDA tensor types.\n        requires_grad (bool, optional): If autograd should record operations on the\n            returned tensor. Default: ``False``.\n        pin_memory (bool, optional): If set, returned tensor would be allocated in\n            the pinned memory. Works only for CPU tensors. Default: ``False``.\n    \n    Example::\n    \n        &gt;&gt;&gt; torch.randperm(4)\n        tensor([2, 1, 0, 3])\n\nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntfm(files[0]).show();\n\n\n\n\n\n\ntls = TfmdLists(files, tfm, splits=splits)\nshow_at(tls.valid, 0);\n\n\n\n\n\n\nToTensor\nfastai.data.transforms.ToTensor\n\nprint_source(ToTensor)\nclass ToTensor(Transform):\n    \"Convert item to appropriate tensor class\"\n    order = 5\n\nIntToFloatTensor\nfastai.data.transforms.IntToFloatTensor\n\nprint_source(IntToFloatTensor)\nclass IntToFloatTensor(DisplayedTransform):\n    \"Transform image to float tensor, optionally dividing by 255 (e.g. for images).\"\n    order = 10 #Need to run after PIL transforms on the GPU\n    def __init__(self, div=255., div_mask=1): store_attr()\n    def encodes(self, o:TensorImage): return o.float().div_(self.div)\n    def encodes(self, o:TensorMask ): return o.long() // self.div_mask\n    def decodes(self, o:TensorImage): return ((o.clamp(0., 1.) * self.div).long()) if self.div else o\n\nNormalize.from_stats\n&lt;bound method Normalize.from_stats of &lt;class 'fastai.data.transforms.Normalize'&gt;&gt;\n\nprint_source(Normalize.from_stats)\n    @classmethod\n    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n\nbroadcast_vec\n&lt;function fastai.data.transforms.broadcast_vec(dim, ndim, *t, cuda=True)&gt;\n\nprint_source(broadcast_vec)\ndef broadcast_vec(dim, ndim, *t, cuda=True):\n    \"Make a vector broadcastable over `dim` (out of `ndim` total) by prepending and appending unit axes\"\n    v = [1]*ndim\n    v[dim] = -1\n    f = to_device if cuda else noop\n    return [f(tensor(o).view(*v)) for o in t]\n\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-11/index.html#references",
    "href": "posts/fastai-book-notes/chapter-11/index.html#references",
    "title": "Notes on fastai Book Ch. 11",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 10\nNext: Notes on fastai Book Ch. 12"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html",
    "href": "posts/fastai-book-notes/chapter-12/index.html",
    "title": "Notes on fastai Book Ch. 12",
    "section": "",
    "text": "The Data\nOur First Language Model from Scratch\nImproving the RNN\nMultilayer RNNs\nLSTM\nRegularizing an LSTM\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#a-language-model-from-scratch",
    "href": "posts/fastai-book-notes/chapter-12/index.html#a-language-model-from-scratch",
    "title": "Notes on fastai Book Ch. 12",
    "section": "A Language Model from Scratch",
    "text": "A Language Model from Scratch"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#the-data",
    "href": "posts/fastai-book-notes/chapter-12/index.html#the-data",
    "title": "Notes on fastai Book Ch. 12",
    "section": "The Data",
    "text": "The Data\n\ntry to think of the simplest useable dataset when starting on a new problem\nthe starter dataset should allow you to quickly and easily try out methods and interpret the results\none of the most common practical mistakes is failing to use appropriate datasets at appropriate times during the analysis process\n\nmost people tend to start with datasets that are too big and too complicated\n\n\n\nfrom fastai.text.all import *\n\nfastai Human Numbers Dataset\n\nA synthetic dataset consisting of human number counts in text such as one, two, three, four..\nUseful for experimenting with Language Models\n\n\nURLs.HUMAN_NUMBERS\n'https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz'\n\npath = untar_data(URLs.HUMAN_NUMBERS)\npath\nPath('/home/innom-dt/.fastai/data/human_numbers')\n\npath.ls()\n(#2) [Path('/home/innom-dt/.fastai/data/human_numbers/train.txt'),Path('/home/innom-dt/.fastai/data/human_numbers/valid.txt')]\n\ntrain_file = path.ls()[0]\ncat $train_file | head -5\none \ntwo \nthree \nfour \nfive \ncat: write error: Broken pipe\n\nvalid_file = path.ls()[1]\ncat $valid_file | head -5\neight thousand one \neight thousand two \neight thousand three \neight thousand four \neight thousand five \ncat: write error: Broken pipe\n\nlines = L()\n# Combine the training and validation sets into a single List\nwith open(path/'train.txt') as f: lines += L(*f.readlines())\nwith open(path/'valid.txt') as f: lines += L(*f.readlines())\nlines\n(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]\n\n# Remove the '\\n' new line characters and separate the words with a '.'\ntext = ' . '.join([l.strip() for l in lines])\ntext[:100]\n'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'\n\n# Separate the words into a list\ntokens = text.split(' ')\ntokens[:10]\n['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']\n\n# Generate unique vocab\nvocab = L(*tokens).unique()\nvocab\n(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]\n\npd.DataFrame(list(vocab))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\none\n\n\n\n\n1\n\n\n.\n\n\n\n\n2\n\n\ntwo\n\n\n\n\n3\n\n\nthree\n\n\n\n\n4\n\n\nfour\n\n\n\n\n5\n\n\nfive\n\n\n\n\n6\n\n\nsix\n\n\n\n\n7\n\n\nseven\n\n\n\n\n8\n\n\neight\n\n\n\n\n9\n\n\nnine\n\n\n\n\n10\n\n\nten\n\n\n\n\n11\n\n\neleven\n\n\n\n\n12\n\n\ntwelve\n\n\n\n\n13\n\n\nthirteen\n\n\n\n\n14\n\n\nfourteen\n\n\n\n\n15\n\n\nfifteen\n\n\n\n\n16\n\n\nsixteen\n\n\n\n\n17\n\n\nseventeen\n\n\n\n\n18\n\n\neighteen\n\n\n\n\n19\n\n\nnineteen\n\n\n\n\n20\n\n\ntwenty\n\n\n\n\n21\n\n\nthirty\n\n\n\n\n22\n\n\nforty\n\n\n\n\n23\n\n\nfifty\n\n\n\n\n24\n\n\nsixty\n\n\n\n\n25\n\n\nseventy\n\n\n\n\n26\n\n\neighty\n\n\n\n\n27\n\n\nninety\n\n\n\n\n28\n\n\nhundred\n\n\n\n\n29\n\n\nthousand\n\n\n\n\n\n\n\n# Map words to their vocab indices\nword2idx = {w:i for i,w in enumerate(vocab)}\n# Numericalize dataset\nnums = L(word2idx[i] for i in tokens)\nnums\n(#63095) [0,1,2,1,3,1,4,1,5,1...]"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#our-first-language-model-from-scratch",
    "href": "posts/fastai-book-notes/chapter-12/index.html#our-first-language-model-from-scratch",
    "title": "Notes on fastai Book Ch. 12",
    "section": "Our First Language Model from Scratch",
    "text": "Our First Language Model from Scratch\n# Create a list of (input, target) tuples\n# input: the previous three words\n# target: the next word\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]\n\n# # Create a list of (input, target) tuples\n# input: a tensor containing the numericalized forms of previous three words\n# target: the numericalized form of the next word\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\n(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]\n\nDataLoaders.from_dsets\n&lt;bound method DataLoaders.from_dsets of &lt;class 'fastai.data.core.DataLoaders'&gt;&gt;\n\nprint_source(DataLoaders.from_dsets)\n    @classmethod\n    def from_dsets(cls, *ds, path='.',  bs=64, device=None, dl_type=TfmdDL, **kwargs):\n        default = (True,) + (False,) * (len(ds)-1)\n        defaults = {'shuffle': default, 'drop_last': default}\n        tfms = {k:tuple(Pipeline(kwargs[k]) for i in range_of(ds)) for k in _batch_tfms if k in kwargs}\n        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items() if k not in _batch_tfms}, tfms)\n        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n        return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device)\n\nbs = 64\n# Split data between train and valid 80/20\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\ndls.one_batch()[0].shape, dls.one_batch()[1].shape\n(torch.Size([64, 3]), torch.Size([64]))\n\ndls.one_batch()[0][0], dls.one_batch()[1][0]\n(tensor([0, 1, 2]), tensor(1))\n\nOur Language Model in PyTorch\n\nEvery word is interpreted in the information context of any words preceding it\n\n\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        # Input to hidden\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        # Hidden to hidden\n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        # Hidden to output\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        # First input word\n        # Pass embedding for first word to first linear layer\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        # Second input word\n        # Add embedding for second word to previous output\n        h = h + self.i_h(x[:,1])\n        # Pass to first linear layer\n        h = F.relu(self.h_h(h))\n        # Third input word\n        # Add embeddingfor third word to previous output\n        h = h + self.i_h(x[:,2])\n        # Pass to first linear layer\n        h = F.relu(self.h_h(h))\n        # Pass output to second linear layer\n        return self.h_o(h)\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.813438\n\n\n1.944979\n\n\n0.466603\n\n\n00:01\n\n\n\n\n1\n\n\n1.405106\n\n\n1.702907\n\n\n0.474447\n\n\n00:01\n\n\n\n\n2\n\n\n1.427549\n\n\n1.650981\n\n\n0.489898\n\n\n00:00\n\n\n\n\n3\n\n\n1.380016\n\n\n1.685956\n\n\n0.470882\n\n\n00:01\n\n\n\n\n\n\n\nrange_of\n&lt;function fastcore.basics.range_of(a, b=None, step=None)&gt;\n\nprint_source(range_of)\ndef range_of(a, b=None, step=None):\n    \"All indices of collection `a`, if `a` is a collection, otherwise `range`\"\n    if is_coll(a): a = len(a)\n    return list(range(a,b,step) if step is not None else range(a,b) if b is not None else range(a))\n\n# Get the number of occurrences of each unique vocab item in the validation set\nn,counts = 0,torch.zeros(len(vocab))\nn, counts\nfor x,y in dls.valid:\n    n += y.shape[0]\n    # Keep track of \n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n# Get the index for the most common token in the validation set\nidx = torch.argmax(counts)\n# Print the most common index\n(idx, \n# Print the corresponding word for the index\nvocab[idx.item()], \n# Calculate the likelihood of randomly picking the most common word\ncounts[idx].item()/n)\n(tensor(29), 'thousand', 0.15165200855716662)\nNote: This indicates the model is performing much better than picking a word at random.\n\n\nOur First Recurrent Neural Network (a.k.a A Looping Network)\n\nreplace the hardcoded forward function in the LMModel1 with a for loop\n\n\nHidden State:\n\nthe activations that are updated at each step of a recurrent neural network\n\n\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        # Initialize the hidden state\n        h = 0\n        for i in range(3):\n            # Update the hidden state\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.790029\n\n\n1.993387\n\n\n0.463038\n\n\n00:01\n\n\n\n\n1\n\n\n1.389832\n\n\n1.836371\n\n\n0.466603\n\n\n00:01\n\n\n\n\n2\n\n\n1.422808\n\n\n1.669952\n\n\n0.487045\n\n\n00:01\n\n\n\n\n3\n\n\n1.380381\n\n\n1.681706\n\n\n0.459472\n\n\n00:01"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#improving-the-rnn",
    "href": "posts/fastai-book-notes/chapter-12/index.html#improving-the-rnn",
    "title": "Notes on fastai Book Ch. 12",
    "section": "Improving the RNN",
    "text": "Improving the RNN\n\nthe above LMModel2 version resets the hidden state for every new input sequence\n\nthrowing away all the information we have about the sentences we have seen so far\n\nthe above LMModel2 version only tries to predict the fourth word\n\n\nMaintaining the State of an RNN\nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        # Maintain the same hidden state across input sequences\n        self.h = 0\n        \n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        # Detach the hidden state from the pytorch computation graph\n        self.h = self.h.detach()\n        return out\n    \n    def reset(self): self.h = 0\n\nBackpropogation Through Time (BPTT)\n\nTreating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way\nusually use Truncated BPTT which detaches the history of computation steps in the hidden state every few time steps.\n\n\nm = len(seqs)//bs\nm,bs,len(seqs)\n(328, 64, 21031)\n\ndef group_chunks(ds, bs):\n    # Calculate the number of groups\n    m = len(ds) // bs\n    # Initialize new dataset container\n    new_ds = L()\n    # Group dataset into chunks\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n# Split dataset 80/20 into training and validation\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, \n    # Drop the last batch that does not have the shape of bs\n    drop_last=True, \n    shuffle=False)\n\nModelResetter\nfastai.callback.rnn.ModelResetter\n\nprint_source(ModelResetter)\n@docs\nclass ModelResetter(Callback):\n    \"`Callback` that resets the model at each validation/training step\"\n    def before_train(self):    self.model.reset()\n    def before_validate(self): self.model.reset()\n    def after_fit(self):       self.model.reset()\n    _docs = dict(before_train=\"Reset the model before training\",\n                 before_validate=\"Reset the model before validation\",\n                 after_fit=\"Reset the model after fitting\")\n\n\nfastai Callbacks\n\nDocumentation\nafter_create: called after the Learner is created\nbefore_fit: called before starting training or inference, ideal for initial setup.\nbefore_epoch: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\nbefore_train: called at the beginning of the training part of an epoch.\nbefore_batch: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\nafter_pred: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss.\nafter_loss: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\nbefore_backward: called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)\nbefore_step: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\nafter_step: called after the step and before the gradients are zeroed.\nafter_batch: called at the end of a batch, for any clean-up before the next one.\nafter_train: called at the end of the training phase of an epoch.\nbefore_validate: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\nafter_validate: called at the end of the validation part of an epoch.\nafter_epoch: called at the end of an epoch, for any clean-up before the next one.\nafter_fit: called at the end of training, for final clean-up.\n\n\nCallback\nfastai.callback.core.Callback\n\nprint_source(Callback)\n@funcs_kwargs(as_method=True)\nclass Callback(Stateful,GetAttr):\n    \"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\n    order,_default,learn,run,run_train,run_valid = 0,'learn',None,True,True,True\n    _methods = _events\n\n    def __init__(self, **kwargs): assert not kwargs, f'Passed unknown events: {kwargs}'\n    def __repr__(self): return type(self).__name__\n\n    def __call__(self, event_name):\n        \"Call `self.{event_name}` if it's defined\"\n        _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or\n               (self.run_valid and not getattr(self, 'training', False)))\n        res = None\n        if self.run and _run: res = getattr(self, event_name, noop)()\n        if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit\n        return res\n\n    def __setattr__(self, name, value):\n        if hasattr(self.learn,name):\n            warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n        super().__setattr__(name, value)\n\n    @property\n    def name(self):\n        \"Name of the `Callback`, camel-cased and with '*Callback*' removed\"\n        return class2attr(self, 'Callback')\n\nlearn = Learner(dls, \n                LMModel3(len(vocab), 64), \n                loss_func=F.cross_entropy,\n                metrics=accuracy, \n                # reset the model at the beginning of each epoch and before each validation phase\n                cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.695570\n\n\n1.837262\n\n\n0.474519\n\n\n00:01\n\n\n\n\n1\n\n\n1.316114\n\n\n1.939660\n\n\n0.366346\n\n\n00:01\n\n\n\n\n2\n\n\n1.102734\n\n\n1.578932\n\n\n0.469471\n\n\n00:01\n\n\n\n\n3\n\n\n1.017313\n\n\n1.470766\n\n\n0.552163\n\n\n00:01\n\n\n\n\n4\n\n\n0.961458\n\n\n1.568437\n\n\n0.551923\n\n\n00:01\n\n\n\n\n5\n\n\n0.920572\n\n\n1.632755\n\n\n0.574519\n\n\n00:01\n\n\n\n\n6\n\n\n0.932616\n\n\n1.634864\n\n\n0.588221\n\n\n00:01\n\n\n\n\n7\n\n\n0.848161\n\n\n1.668468\n\n\n0.587500\n\n\n00:01\n\n\n\n\n8\n\n\n0.802442\n\n\n1.698610\n\n\n0.591827\n\n\n00:01\n\n\n\n\n9\n\n\n0.794550\n\n\n1.716233\n\n\n0.594952\n\n\n00:01\n\n\n\n\n\n\n\n\n\nCreating More Signal\n\nwe can increase the amount of signal for updating the model weights by predicting the next word after every single word, rather than every three words\n\n\n# Define the sequence length\nsl = 16\n# Update the dependent variable to include each of the words \n# that follow each of the words in the independent variable\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\n# Define the split for the training and validation set\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\n[L(vocab[o] for o in s) for s in seqs[0]]\n[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]\n\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            # Store the output for each word in the current sequence\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        # stack the output for each word in the current sequence\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\n# Define custom loss function that flattens the output before calculating cross entropy\ndef loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.229987\n\n\n3.069768\n\n\n0.249756\n\n\n00:00\n\n\n\n\n1\n\n\n2.291759\n\n\n1.903835\n\n\n0.468018\n\n\n00:00\n\n\n\n\n2\n\n\n1.719411\n\n\n1.769336\n\n\n0.469157\n\n\n00:00\n\n\n\n\n3\n\n\n1.444394\n\n\n1.729377\n\n\n0.459554\n\n\n00:00\n\n\n\n\n4\n\n\n1.273674\n\n\n1.625678\n\n\n0.531169\n\n\n00:00\n\n\n\n\n5\n\n\n1.141202\n\n\n1.762818\n\n\n0.545898\n\n\n00:00\n\n\n\n\n6\n\n\n1.037926\n\n\n1.575556\n\n\n0.573812\n\n\n00:00\n\n\n\n\n7\n\n\n0.939284\n\n\n1.470020\n\n\n0.614095\n\n\n00:00\n\n\n\n\n8\n\n\n0.858596\n\n\n1.532887\n\n\n0.628255\n\n\n00:00\n\n\n\n\n9\n\n\n0.784250\n\n\n1.495697\n\n\n0.655843\n\n\n00:00\n\n\n\n\n10\n\n\n0.739764\n\n\n1.539676\n\n\n0.666423\n\n\n00:00\n\n\n\n\n11\n\n\n0.693413\n\n\n1.550242\n\n\n0.662191\n\n\n00:00\n\n\n\n\n12\n\n\n0.661127\n\n\n1.519285\n\n\n0.680908\n\n\n00:00\n\n\n\n\n13\n\n\n0.635551\n\n\n1.523878\n\n\n0.676921\n\n\n00:00\n\n\n\n\n14\n\n\n0.621697\n\n\n1.531653\n\n\n0.684408\n\n\n00:00"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#multilayer-rnns",
    "href": "posts/fastai-book-notes/chapter-12/index.html#multilayer-rnns",
    "title": "Notes on fastai Book Ch. 12",
    "section": "Multilayer RNNs",
    "text": "Multilayer RNNs\n\npass the activations from one RNN into another RNN\n\n\nThe Model\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n    \n    def reset(self): self.h.zero_()\n\nLMModel5(len(vocab), 64, 2)\nLMModel5(\n  (i_h): Embedding(30, 64)\n  (rnn): RNN(64, 64, num_layers=2, batch_first=True)\n  (h_o): Linear(in_features=64, out_features=30, bias=True)\n)\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.070420\n\n\n2.586252\n\n\n0.460775\n\n\n00:00\n\n\n\n\n1\n\n\n2.154392\n\n\n1.760734\n\n\n0.471680\n\n\n00:00\n\n\n\n\n2\n\n\n1.709090\n\n\n1.851027\n\n\n0.327311\n\n\n00:00\n\n\n\n\n3\n\n\n1.523287\n\n\n1.790196\n\n\n0.412028\n\n\n00:00\n\n\n\n\n4\n\n\n1.364664\n\n\n1.816422\n\n\n0.468262\n\n\n00:00\n\n\n\n\n5\n\n\n1.247051\n\n\n1.796951\n\n\n0.493001\n\n\n00:00\n\n\n\n\n6\n\n\n1.156087\n\n\n1.907447\n\n\n0.489095\n\n\n00:00\n\n\n\n\n7\n\n\n1.073325\n\n\n2.014389\n\n\n0.499268\n\n\n00:00\n\n\n\n\n8\n\n\n0.995001\n\n\n2.056770\n\n\n0.501139\n\n\n00:00\n\n\n\n\n9\n\n\n0.927453\n\n\n2.080244\n\n\n0.503743\n\n\n00:00\n\n\n\n\n10\n\n\n0.874861\n\n\n2.084781\n\n\n0.502441\n\n\n00:00\n\n\n\n\n11\n\n\n0.837194\n\n\n2.102611\n\n\n0.514974\n\n\n00:00\n\n\n\n\n12\n\n\n0.812340\n\n\n2.111124\n\n\n0.512126\n\n\n00:00\n\n\n\n\n13\n\n\n0.797198\n\n\n2.110253\n\n\n0.513346\n\n\n00:00\n\n\n\n\n14\n\n\n0.789102\n\n\n2.108808\n\n\n0.513997\n\n\n00:00\n\n\n\n\n\n\nNote: The multi-layer RNN performs worse than the single-layer RNN\n\n\nExploding or Disappearing Activations\n\ndeeper models are more difficult to train\n\nperforming matrix multiplication so many times can cause numbers to get extremely big or extremely small\nfloating point numbers get less accurate the further away they get from zero\n\nWhat you never wanted to know about floating point but will be forced to find out\nTwo types of layers are frequently used to avoid exploding activations in RNNs\n\nGated Recurrent Units (GRUs)\nLong short-term memory (LSTM)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#lstm",
    "href": "posts/fastai-book-notes/chapter-12/index.html#lstm",
    "title": "Notes on fastai Book Ch. 12",
    "section": "LSTM",
    "text": "LSTM\n\nintroduced in 1997 by Jürgen Schmidhuber and Sepp Hochreiter\nNormal RNNs are realy bad at retaining memory of what happened much earlier in a sentence\nLSTMs maintain two hidden states to address this\n\ncell state: responsible for keeping long short-term memory\nhidden state: focuses on the next token to predict\n\n\n\n\n\nlstm-cell\n\n\n\n\\(x_{t}\\) the current input\n\\((h_{t-1})\\): the previous hidden state\n\\((c_{t-1})\\): the previous hidden state\n\\(\\sigma\\): sigmoid function\n\\(tanh\\): a sigmoid function rescaled to the range \\([-1,1]\\)\n\\(tanh(x) = \\frac{e^{x}+e^{-x}}{e^{x}-e^{-x}} = 2\\sigma(2x)-1\\)\nfour neural nets (orange) called gates (left to right):\n\nforget gate: a linear layer followed by a sigmoid (i.e. output will be scalars [0,1])\n\nmultipy output by cell state to determine which information to keep\ngives the LSTM the ability to forget things about its long-term state\n\ninput gate: works with the third gate (tanh) to update the cell state\n\ndecided which elements of the cell state to updates (values close to 1)\n\ncell gate: determines what the updated values are for the cell state\noutput gate: determines which information from the cell state to use to generate the new hidden state\n\n\n\n2*torch.sigmoid(2*tensor(0.5)) - 1\ntensor(0.4621)\n\ntorch.tanh(tensor(0.5))\ntensor(0.4621)\n\nBuilding an LSTM from Scratch\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.cat([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = out * torch.tanh(c)\n        return h, (h,c)\nNote: It is better for performance reasons to do one big matrix multiplication than four smaller ones\n\nlaunch the special fast kernel on the GPU only once\ngive the GPU more work to do in parallel\n\n\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\ntorch chunk\n\nDocumentation\n\n\nhelp(torch.chunk)\nHelp on built-in function chunk:\n\nchunk(...)\n    chunk(input, chunks, dim=0) -&gt; List of Tensors\n    \n    Attempts to split a tensor into the specified number of chunks. Each chunk is a view of\n    the input tensor.\n        \n    .. note::\n        This function may return less then the specified number of chunks!\n    \n    .. seealso::\n    \n        :func:`torch.tensor_split` a function that always returns exactly the specified number of chunks\n    \n    If the tensor size along the given dimesion :attr:`dim` is divisible by :attr:`chunks`,\n    all returned chunks will be the same size.\n    If the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`chunks`,\n    all returned chunks will be the same size, except the last one.\n    If such division is not possible, this function may return less\n    than the specified number of chunks.\n    \n    Arguments:\n        input (Tensor): the tensor to split\n        chunks (int): number of chunks to return\n        dim (int): dimension along which to split the tensor\n    \n    Example::\n        &gt;&gt;&gt; torch.arange(11).chunk(6)\n        (tensor([0, 1]),\n         tensor([2, 3]),\n         tensor([4, 5]),\n         tensor([6, 7]),\n         tensor([8, 9]),\n         tensor([10]))\n        &gt;&gt;&gt; torch.arange(12).chunk(6)\n        (tensor([0, 1]),\n         tensor([2, 3]),\n         tensor([4, 5]),\n         tensor([6, 7]),\n         tensor([8, 9]),\n         tensor([10, 11]))\n        &gt;&gt;&gt; torch.arange(13).chunk(6)\n        (tensor([0, 1, 2]),\n         tensor([3, 4, 5]),\n         tensor([6, 7, 8]),\n         tensor([ 9, 10, 11]),\n         tensor([12]))\n\nt = torch.arange(0,10); t\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nt.chunk(2)\n(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n\n\n\nTraining a Language Model Using LSTMs\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\n# Using a two-layer LSTM\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.model\nLMModel6(\n  (i_h): Embedding(30, 64)\n  (rnn): LSTM(64, 64, num_layers=2, batch_first=True)\n  (h_o): Linear(in_features=64, out_features=30, bias=True)\n)\n\nlearn.fit_one_cycle(15, 1e-2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.013088\n\n\n2.705310\n\n\n0.417074\n\n\n00:01\n\n\n\n\n1\n\n\n2.215323\n\n\n1.904673\n\n\n0.406657\n\n\n00:01\n\n\n\n\n2\n\n\n1.622977\n\n\n1.772446\n\n\n0.438232\n\n\n00:01\n\n\n\n\n3\n\n\n1.319893\n\n\n1.853711\n\n\n0.519613\n\n\n00:00\n\n\n\n\n4\n\n\n1.096065\n\n\n1.868788\n\n\n0.554118\n\n\n00:01\n\n\n\n\n5\n\n\n0.872888\n\n\n1.679482\n\n\n0.609375\n\n\n00:01\n\n\n\n\n6\n\n\n0.590291\n\n\n1.355017\n\n\n0.661458\n\n\n00:01\n\n\n\n\n7\n\n\n0.385917\n\n\n1.319989\n\n\n0.667887\n\n\n00:01\n\n\n\n\n8\n\n\n0.284691\n\n\n1.221118\n\n\n0.689290\n\n\n00:01\n\n\n\n\n9\n\n\n0.228731\n\n\n1.181922\n\n\n0.730632\n\n\n00:01\n\n\n\n\n10\n\n\n0.172228\n\n\n1.250237\n\n\n0.727946\n\n\n00:01\n\n\n\n\n11\n\n\n0.124468\n\n\n1.155407\n\n\n0.754720\n\n\n00:01\n\n\n\n\n12\n\n\n0.090831\n\n\n1.183195\n\n\n0.749674\n\n\n00:01\n\n\n\n\n13\n\n\n0.071399\n\n\n1.179867\n\n\n0.750081\n\n\n00:01\n\n\n\n\n14\n\n\n0.061995\n\n\n1.168421\n\n\n0.753499\n\n\n00:01\n\n\n\n\n\n\nNote: We were able to use a higher learning rate and achieve a much higher accuracy than the multi-layer RNN. Note: There is still some overfitting."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#regularizing-an-lstm",
    "href": "posts/fastai-book-notes/chapter-12/index.html#regularizing-an-lstm",
    "title": "Notes on fastai Book Ch. 12",
    "section": "Regularizing an LSTM",
    "text": "Regularizing an LSTM\n\nRegularizing and Optimizing LSTM Language Models\n\nused an LSTM with dropout, activation regularization, and temporal activation regularization to beat state-of-the-art results that previously required much more complicated models\ncalled the combination an AWD-LSTM\n\n\n\nDropout\n\nImproving neural networks by preventing co-adaptation of feature detectors\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\nrandomly change some activations to zero at training time\nmakes activations more noisy\nmakes sure all neurons actively work toward the output\nmakes the model more robust\nneed to rescale activations after applying dropout\n\ndivide activations by \\(1-p\\) where p is the probability to keep an activation\n\nusing dropout before passing the output of our LSTM to the final layer will help reduce overfitting\nmake sure to turn off dropout during inference\n\n\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)\n\nhelp(torch.bernoulli)\nHelp on built-in function bernoulli:\n\nbernoulli(...)\n    bernoulli(input, *, generator=None, out=None) -&gt; Tensor\n    \n    Draws binary random numbers (0 or 1) from a Bernoulli distribution.\n    \n    The :attr:`input` tensor should be a tensor containing probabilities\n    to be used for drawing the binary random number.\n    Hence, all values in :attr:`input` have to be in the range:\n    :math:`0 \\leq \\text{input}_i \\leq 1`.\n    \n    The :math:`\\text{i}^{th}` element of the output tensor will draw a\n    value :math:`1` according to the :math:`\\text{i}^{th}` probability value given\n    in :attr:`input`.\n    \n    .. math::\n        \\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})\n    \n    The returned :attr:`out` tensor only has values 0 or 1 and is of the same\n    shape as :attr:`input`.\n    \n    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating\n    point ``dtype``.\n    \n    Args:\n        input (Tensor): the input tensor of probability values for the Bernoulli distribution\n    \n    Keyword args:\n        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n        out (Tensor, optional): the output tensor.\n    \n    Example::\n    \n        &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n        &gt;&gt;&gt; a\n        tensor([[ 0.1737,  0.0950,  0.3609],\n                [ 0.7148,  0.0289,  0.2676],\n                [ 0.9456,  0.8937,  0.7202]])\n        &gt;&gt;&gt; torch.bernoulli(a)\n        tensor([[ 1.,  0.,  0.],\n                [ 0.,  0.,  0.],\n                [ 1.,  1.,  1.]])\n    \n        &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n        &gt;&gt;&gt; torch.bernoulli(a)\n        tensor([[ 1.,  1.,  1.],\n                [ 1.,  1.,  1.],\n                [ 1.,  1.,  1.]])\n        &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n        &gt;&gt;&gt; torch.bernoulli(a)\n        tensor([[ 0.,  0.,  0.],\n                [ 0.,  0.,  0.],\n                [ 0.,  0.,  0.]])\n\n\nActivation Regularization and Temporal Activation Regularization\n\nboth are similar to weight decay (AR)\nactivation regularization: try to make the final activations produced by the LSTM as small as possible\n\nloss += alpha * activations.pow(2).mean()\noften applied on dropped-out activations to not penalize the activations set to zero\n\ntemporal activation regularization (TAR)\n\nlinked to the fact we are predicting tokens in a sentence\nthe outputs of our LSTMs should somewhat make sense when we read them in order\nTAR encourages that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n\nalpha and beta are tunable hyperparameters\n\napplied to non-dropped-out activations (because the zeros in the dropped-out activations create big differences)\n\n\n\n\nTraining a Weight-Tied Regularized LSTM\n\nneed to return the normal output from the LSTM, the dropped-out activations, and the activations from the LSTMs\n\n\nWeight Tying\n\nin a language model, the input embeddings represent a mapping from English words to activations and the output hidden layer represents a mapping from activations to English words\n\nthese mappings could be the same\n\nintroduced in AWD-LSTM paper\nself.h_o.weight = self.i_h.weight\n\n\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])\n\nRNNRegularizer\nfastai.callback.rnn.RNNRegularizer\n\nprint_source(RNNRegularizer)\nclass RNNRegularizer(Callback):\n    \"Add AR and TAR regularization\"\n    order,run_valid = RNNCallback.order+1,False\n    def __init__(self, alpha=0., beta=0.): store_attr()\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha: self.learn.loss_grad += self.alpha * self.rnn.out.float().pow(2).mean()\n        if self.beta:\n            h = self.rnn.raw_out\n            if len(h)&gt;1: self.learn.loss_grad += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.model\nLMModel7(\n  (i_h): Embedding(30, 64)\n  (rnn): LSTM(64, 64, num_layers=2, batch_first=True)\n  (drop): Dropout(p=0.4, inplace=False)\n  (h_o): Linear(in_features=64, out_features=30, bias=True)\n)\n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.620218\n\n\n1.797085\n\n\n0.484294\n\n\n00:01\n\n\n\n\n1\n\n\n1.622718\n\n\n1.452620\n\n\n0.652181\n\n\n00:01\n\n\n\n\n2\n\n\n0.864787\n\n\n0.726230\n\n\n0.773275\n\n\n00:01\n\n\n\n\n3\n\n\n0.434755\n\n\n0.699705\n\n\n0.828613\n\n\n00:01\n\n\n\n\n4\n\n\n0.225359\n\n\n0.579946\n\n\n0.842855\n\n\n00:01\n\n\n\n\n5\n\n\n0.126518\n\n\n0.571510\n\n\n0.850911\n\n\n00:01\n\n\n\n\n6\n\n\n0.076041\n\n\n0.444107\n\n\n0.874349\n\n\n00:01\n\n\n\n\n7\n\n\n0.051340\n\n\n0.366569\n\n\n0.882487\n\n\n00:01\n\n\n\n\n8\n\n\n0.037389\n\n\n0.547799\n\n\n0.854818\n\n\n00:01\n\n\n\n\n9\n\n\n0.027291\n\n\n0.392787\n\n\n0.880615\n\n\n00:01\n\n\n\n\n10\n\n\n0.022100\n\n\n0.354383\n\n\n0.889648\n\n\n00:01\n\n\n\n\n11\n\n\n0.018304\n\n\n0.380172\n\n\n0.885417\n\n\n00:01\n\n\n\n\n12\n\n\n0.015668\n\n\n0.384031\n\n\n0.885010\n\n\n00:01\n\n\n\n\n13\n\n\n0.013562\n\n\n0.389092\n\n\n0.884033\n\n\n00:01\n\n\n\n\n14\n\n\n0.012376\n\n\n0.383106\n\n\n0.885254\n\n\n00:01\n\n\n\n\n\n\nNote: This performance is significantly better than the regular LSTM."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-12/index.html#references",
    "href": "posts/fastai-book-notes/chapter-12/index.html#references",
    "title": "Notes on fastai Book Ch. 12",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 11\nNext: Notes on fastai Book Ch. 13"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html",
    "href": "posts/fastai-book-notes/chapter-13/index.html",
    "title": "Notes on fastai Book Ch. 13",
    "section": "",
    "text": "The Magic of Convolutions\nOur First Convolutional Neural Network\nImproving Training Stability\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#convolutional-neural-networks",
    "href": "posts/fastai-book-notes/chapter-13/index.html#convolutional-neural-networks",
    "title": "Notes on fastai Book Ch. 13",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#the-magic-of-convolutions",
    "href": "posts/fastai-book-notes/chapter-13/index.html#the-magic-of-convolutions",
    "title": "Notes on fastai Book Ch. 13",
    "section": "The Magic of Convolutions",
    "text": "The Magic of Convolutions\n\nfeature engineering\n\ncreating new transformations of the input data in order to make it easier to the model\none of the most powerful tools machine learning practitioners have at their disposal\n\na feature is a transformation of the data that is designed to make it easier to the model\n\n\nConvolution\n\napplies a kernel across an image\n\nmultiplies each element of an \\(NxN\\) size kernel by each element of an \\(NxN\\) block of an image and adds the results together\n\nkernel: a little matrix\n\n\n\nA guide to convolution arithmetic for deep learning\n\nprovides many great diagrams showing how image kernels can be applied\n\n\n# A convolutional kernel that finds top edges (i.e. dark on bottom, light on top)\ntop_edge = tensor([[-1,-1,-1],\n                   [ 0, 0, 0],\n                   [ 1, 1, 1]]).float()\n\npath = untar_data(URLs.MNIST_SAMPLE)\npath\nPath('/home/innom-dt/.fastai/data/mnist_sample')\n\nim3 = Image.open(path/'train'/'3'/'12.png')\nshow_image(im3);\n\n\n\n\n\n\nshow_image\n&lt;function fastai.torch_core.show_image(im, ax=None, figsize=None, title=None, ctx=None, cmap=None, norm=None, *, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs)&gt;\n\nprint_source(show_image)\n@delegates(plt.Axes.imshow, keep=True, but=['shape', 'imlim'])\ndef show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n    \"Show a PIL or PyTorch image on `ax`.\"\n    # Handle pytorch axis order\n    if hasattrs(im, ('data','cpu','permute')):\n        im = im.data.cpu()\n        if im.shape[0]&lt;5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=array(im)\n    # Handle 1-channel images\n    if im.shape[-1]==1: im=im[...,0]\n\n    ax = ifnone(ax,ctx)\n    if figsize is None: figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.axis('off')\n    return ax\n\nim3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\ntensor([[-0., -0., -0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n(im3_t[0:3,0:3] * top_edge).sum()\ntensor(0.)\n\ndf = pd.DataFrame(im3_t[:10,:20])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n0\n\n\n0\n\n\n0\n\n\n12\n\n\n99\n\n\n91\n\n\n142\n\n\n155\n\n\n246\n\n\n182\n\n\n155\n\n\n155\n\n\n155\n\n\n155\n\n\n131\n\n\n52\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n0\n\n\n0\n\n\n0\n\n\n138\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n254\n\n\n252\n\n\n210\n\n\n122\n\n\n33\n\n\n0\n\n\n\n\n7\n\n\n0\n\n\n0\n\n\n0\n\n\n220\n\n\n254\n\n\n254\n\n\n254\n\n\n235\n\n\n189\n\n\n189\n\n\n189\n\n\n189\n\n\n150\n\n\n189\n\n\n205\n\n\n254\n\n\n254\n\n\n254\n\n\n75\n\n\n0\n\n\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n35\n\n\n74\n\n\n35\n\n\n35\n\n\n25\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n13\n\n\n224\n\n\n254\n\n\n254\n\n\n153\n\n\n0\n\n\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n90\n\n\n254\n\n\n254\n\n\n247\n\n\n53\n\n\n0\n\n\n\n\n\n\n\ndf = pd.DataFrame(im3_t[4:7,6:9])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n\n\n0\n\n\n1\n\n\n2\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n142\n\n\n155\n\n\n246\n\n\n\n\n2\n\n\n254\n\n\n254\n\n\n254\n\n\n\n\n\n\n\n(im3_t[4:7,6:9] * top_edge).sum()\ntensor(762.)\nNote: Returns a high number because the \\(3x3\\) pixel square represents a top edge.\ndf = pd.DataFrame(im3_t[7:10,17:20])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n\n\n0\n\n\n1\n\n\n2\n\n\n\n\n\n\n0\n\n\n254\n\n\n75\n\n\n0\n\n\n\n\n1\n\n\n254\n\n\n153\n\n\n0\n\n\n\n\n2\n\n\n247\n\n\n53\n\n\n0\n\n\n\n\n\n\n\n(im3_t[7:10,17:20] * top_edge).sum()\ntensor(-29.)\nNote: Returns a low number because the \\(3x3\\) pixel square does not represent a top edge.\n# Center coords of the 3x3 matrix will be (row,col)\ndef apply_kernel(row, col, kernel):\n    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()\n\napply_kernel(5,7,top_edge)\ntensor(762.)\n\n\nMapping a Convolution Kernel\n# Nested list comprehension to generate a list of coordinates\n[[(i,j) for j in range(1,5)] for i in range(1,5)]\n[[(1, 1), (1, 2), (1, 3), (1, 4)],\n [(2, 1), (2, 2), (2, 3), (2, 4)],\n [(3, 1), (3, 2), (3, 3), (3, 4)],\n [(4, 1), (4, 2), (4, 3), (4, 4)]]\n\nrng = range(1,27)\n# Map top edge kernel to the generated list of coordinates\ntop_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n\nshow_image(top_edge3);\n\n\n\n\n\nNote: Top edges are black and bottom edges are white.\nleft_edge = tensor([[-1,1,0],\n                    [-1,1,0],\n                    [-1,1,0]]).float()\n\nleft_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n\nshow_image(left_edge3);\n\n\n\n\n\nright_edge = tensor([[0,1,-1],\n                     [0,1,-1],\n                     [0,1,-1]]).float()\n\nright_edge3 = tensor([[apply_kernel(i,j,right_edge) for j in rng] for i in rng])\n\nshow_image(right_edge3);\n\n\n\n\n\nbottom_edge = tensor([[0,0,0],\n                      [1,1,1],\n                      [-1,-1,-1]]).float()\n\nbottom_edge3 = tensor([[apply_kernel(i,j,bottom_edge) for j in rng] for i in rng])\n\nshow_image(bottom_edge3);\n\n\n\n\n\n\n\nConvolutions in PyTorch\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, right_edge, top_edge, bottom_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\ntorch.Size([6, 3, 3])\n\nprint_source(first)\ndef first(x, f=None, negate=False, **kwargs):\n    \"First element of `x`, optionally filtered by `f`, or None if missing\"\n    x = iter(x)\n    if f: x = filter_ex(x, f=f, negate=negate, gen=True, **kwargs)\n    return next(x, None)\n\nmnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), \n                  get_items=get_image_files, \n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n\ndls = mnist.dataloaders(path)\nxb,yb = first(dls.valid)\nxb.shape\ntorch.Size([64, 1, 28, 28])\n\n# Move to CPU\nxb,yb = to_cpu(xb),to_cpu(yb)\n\nedge_kernels.shape,edge_kernels.unsqueeze(1).shape\n(torch.Size([6, 3, 3]), torch.Size([6, 1, 3, 3]))\n\nedge_kernels = edge_kernels.unsqueeze(1)\nedge_kernels\ntensor([[[[[-1.,  1.,  0.],\n           [-1.,  1.,  0.],\n           [-1.,  1.,  0.]]]],\n\n    \n    \n            [[[[ 0.,  1., -1.],\n               [ 0.,  1., -1.],\n               [ 0.,  1., -1.]]]],\n\n\n    \n    \n            [[[[-1., -1., -1.],\n               [ 0.,  0.,  0.],\n               [ 1.,  1.,  1.]]]],\n\n\n    \n    \n            [[[[ 0.,  0.,  0.],\n               [ 1.,  1.,  1.],\n               [-1., -1., -1.]]]],\n\n\n    \n    \n            [[[[ 0., -1.,  1.],\n               [-1.,  1.,  0.],\n               [ 1.,  0.,  0.]]]],\n\n\n    \n    \n            [[[[ 1., -1.,  0.],\n               [ 0.,  1., -1.],\n               [ 0.,  0.,  1.]]]]])\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\ntorch.Size([64, 6, 26, 26])\n\nhelp(F.conv2d)\nHelp on built-in function conv2d:\n\nconv2d(...)\n    conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor\n    \n    Applies a 2D convolution over an input image composed of several input\n    planes.\n    \n    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.\n    \n    See :class:`~torch.nn.Conv2d` for details and output shape.\n    \n    Note:\n        In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n\n\n\n        \n        Args:\n            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n            weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n            bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n            stride: the stride of the convolving kernel. Can be a single number or a\n              tuple `(sH, sW)`. Default: 1\n            padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n              single number or a tuple `(padH, padW)`. Default: 0\n              ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n              the input so the output has the shape as the input. However, this mode\n              doesn't support any stride values other than 1.\n        \n\n          .. warning::\n              For ``padding='same'``, if the ``weight`` is even-length and\n              ``dilation`` is odd in any dimension, a full :func:`pad` operation\n              may be needed internally. Lowering performance.\n    \n        dilation: the spacing between kernel elements. Can be a single number or\n          a tuple `(dH, dW)`. Default: 1\n        groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n          number of groups. Default: 1\n    \n    Examples::\n    \n        &gt;&gt;&gt; # With square kernels and equal stride\n        &gt;&gt;&gt; filters = torch.randn(8, 4, 3, 3)\n        &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)\n        &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)\n\nfor i in range(6):\n    show_image(batch_features[0,i]);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrides and Padding\n\nappropriate padding ensures the output activation map is the same size as the original image\nthe necessary padding for an \\(ksxks\\) size kernel (where \\(ks\\) is an odd number) is ks//2\n\nalmost never use even size kernels\n\n\n\nStride\n\nthe amount of pixels the kernel moves across the image at each step\nstride-1 convolutions (with appropriate padding) maintain the same image size\nstride-2 convolutions are usefult for reducing the size of the output\n\n\n\n\nUnderstanding the Convolution Equations\n\nCNNs from different viewpoints\n\nshows different visualizations for convolutions\n\nA convolution can be represented as a special kind of matrix multiplication with two constraints\n\nsome elements are always zero\nsome elements are forced to have the same value\n\nThese constraints enforce a certain pattern of connectivity"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#our-first-convolutional-neural-network",
    "href": "posts/fastai-book-notes/chapter-13/index.html#our-first-convolutional-neural-network",
    "title": "Notes on fastai Book Ch. 13",
    "section": "Our First Convolutional Neural Network",
    "text": "Our First Convolutional Neural Network\n\nthe kernels for the convolutions are learned during training\n\nthe model will learn what features are useful for classification\n\n\n\nCreating the CNN\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nsimple_net\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\nbroken_cnn = sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,1, kernel_size=3, padding=1)\n)\n\nbroken_cnn\nSequential(\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(30, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\nbroken_cnn(xb).shape\ntorch.Size([64, 1, 28, 28])\nNote: We don’t need to specify the input dimensions for convolutional layers because they are automatically applied over each pixel\nNote: We can use stride-2 convolutions to progressively decrease the size down to a single output for classification. * It is common to increase the number of features at the same time, to maintain the same amount of computation\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\nsimple_cnn = sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,32),           #2x2\n    conv(32,2, act=False), #1x1\n    # Flatten output to a single dimension\n    Flatten(),\n)\n\nsimple_cnn\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Conv2d(32, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): Flatten(full=False)\n)\n\nsimple_cnn(xb).shape\ntorch.Size([64, 2])\n\nlearn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\nlearn.summary()\nSequential (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 4 x 14 x 14    \nConv2d                                    40         True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 8 x 7 x 7      \nConv2d                                    296        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 16 x 4 x 4     \nConv2d                                    1168       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 32 x 2 x 2     \nConv2d                                    4640       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 2 x 1 x 1      \nConv2d                                    578        True      \n____________________________________________________________________________\n                     64 x 2              \nFlatten                                                        \n____________________________________________________________________________\n\nTotal params: 6,722\nTotal trainable params: 6,722\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7f576a7d3430&gt;\nLoss function: &lt;function cross_entropy at 0x7f57b69003a0&gt;\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n\nlearn.fit_one_cycle(2, 0.01)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.063063\n\n\n0.045171\n\n\n0.987242\n\n\n00:02\n\n\n\n\n1\n\n\n0.023533\n\n\n0.026628\n\n\n0.991168\n\n\n00:01\n\n\n\n\n\n\n\n\nUnderstanding Convolution Arithmetic\nm = learn.model[0]\nm\nSequential(\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n1 input channel, four output channels, and a 3x3 kernel\nm[0].weight.shape\ntorch.Size([4, 1, 3, 3])\n\n4*1*3*3\n36\n\nm[0].bias.shape\ntorch.Size([4])\n\n\nReceptive Fields\n\nthe area of an image that is involved in the calculation of a layer\n\n\n\nA Note About Twitter\n\nMany of the top people in deep learning today are Twitter regulars\nOne of the main ways to stay up to date with interesting papers, software releases, and other deep learning news"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#color-images",
    "href": "posts/fastai-book-notes/chapter-13/index.html#color-images",
    "title": "Notes on fastai Book Ch. 13",
    "section": "Color Images",
    "text": "Color Images\n\na color image is a rank-3 tensor\nwe don’t use the same convolutional kernel for all three color channels\nkernel has a size of ch_in x 3 x 3 where ch_in is the number of input channels (e.g. 3 for RGB)\n\n\nimage2tensor\n&lt;function fastai.vision.core.image2tensor(img)&gt;\n\nprint_source(image2tensor)\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n\n(&lt;function fastai.vision.core.image2tensor(img)&gt;, None)\n\nim = image2tensor(Image.open(image_bear()))\nim.shape\ntorch.Size([3, 1000, 846])\n\nshow_image(im);\n\n\n\n\n\n\n_,axs = subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n    show_image(255-bear, ax=ax, cmap=color)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#improving-training-stability",
    "href": "posts/fastai-book-notes/chapter-13/index.html#improving-training-stability",
    "title": "Notes on fastai Book Ch. 13",
    "section": "Improving Training Stability",
    "text": "Improving Training Stability\npath = untar_data(URLs.MNIST)\npath\nPath('/home/innom-dt/.fastai/data/mnist_png')\n\npath.ls()\n(#2) [Path('/home/innom-dt/.fastai/data/mnist_png/testing'),Path('/home/innom-dt/.fastai/data/mnist_png/training')]\n\nPath(path/'training').ls()\n(#10) [Path('/home/innom-dt/.fastai/data/mnist_png/training/2'),Path('/home/innom-dt/.fastai/data/mnist_png/training/4'),Path('/home/innom-dt/.fastai/data/mnist_png/training/1'),Path('/home/innom-dt/.fastai/data/mnist_png/training/6'),Path('/home/innom-dt/.fastai/data/mnist_png/training/5'),Path('/home/innom-dt/.fastai/data/mnist_png/training/9'),Path('/home/innom-dt/.fastai/data/mnist_png/training/3'),Path('/home/innom-dt/.fastai/data/mnist_png/training/0'),Path('/home/innom-dt/.fastai/data/mnist_png/training/8'),Path('/home/innom-dt/.fastai/data/mnist_png/training/7')]\n\ndef get_dls(bs=64):\n    return DataBlock(\n        blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n        get_items=get_image_files, \n        splitter=GrandparentSplitter('training','testing'),\n        get_y=parent_label,\n        batch_tfms=Normalize()\n    ).dataloaders(path, bs=bs)\n\ndls = get_dls()\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\n\n\nA Simple Baseline\n\nmore convolutional filters are likely required since there are more numbers to recognize\nit is important to keep the number of filters smaller than the number of pixels in the kernel size\n\nthis forces the neural network to extract useful features\n\n\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\ndef simple_cnn():\n    return sequential(\n        # Increate starting kernel size and number of filters\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        Flatten(),\n    )\n\nfrom fastai.callback.hook import *\n\ndef fit(epochs=1):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit(epochs, 0.06)\n    return learn\n\nfastai ActivationStats\n\nprovides som handy utilities for plotting the activations during training\n\n\nActivationStats\nfastai.callback.hook.ActivationStats\n\nprint_source(ActivationStats)\n@delegates()\nclass ActivationStats(HookCallback):\n    \"Callback that record the mean and std of activations.\"\n    order=-20\n    def __init__(self, with_hist=False, **kwargs):\n        super().__init__(**kwargs)\n        self.with_hist = with_hist\n\n    def before_fit(self):\n        \"Initialize stats.\"\n        super().before_fit()\n        self.stats = L()\n\n    def hook(self, m, i, o):\n        if isinstance(o, tuple): return self.hook_multi_ouput(o)\n        o = o.float()\n        res = {'mean': o.mean().item(), 'std': o.std().item(),\n               'near_zero': (o&lt;=0.05).long().sum().item()/o.numel()}\n        if self.with_hist: res['hist'] = o.histc(40,0,10)\n        return res\n\n    def hook_multi_ouput(self,o_tuple):\n        \"For outputs of RNN which are [nested] tuples of tensors\"\n        res = []\n        for o in self._flatten_tuple(o_tuple):\n            if not(isinstance(o, Tensor)): continue\n            res.append(self.hook(None, None, o))\n        return res\n\n    def _flatten_tuple(self, o_tuple):\n        \"Recursively flatten a [nested] tuple\"\n        res = []\n        for it in o_tuple:\n            if isinstance(it, tuple): res += self._flatten_tuple(it)\n            else: res += [it]\n        return tuple(res)\n\n    def after_batch(self):\n        \"Take the stored results and puts it in `self.stats`\"\n        if self.training and (self.every is None or self.train_iter%self.every == 0):\n            self.stats.append(self.hooks.stored)\n        super().after_batch()\n\n    def layer_stats(self, idx):\n        lstats = self.stats.itemgot(idx)\n        return L(lstats.itemgot(o) for o in ('mean','std','near_zero'))\n\n    def hist(self, idx):\n        res = self.stats.itemgot(idx).itemgot('hist')\n        return torch.stack(tuple(res)).t().float().log1p()\n\n    def color_dim(self, idx, figsize=(10,5), ax=None):\n        \"The 'colorful dimension' plot\"\n        res = self.hist(idx)\n        if ax is None: ax = subplots(figsize=figsize)[1][0]\n        ax.imshow(res, origin='lower')\n        ax.axis('off')\n\n    def plot_layer_stats(self, idx):\n        _,axs = subplots(1, 3, figsize=(12,3))\n        for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n            ax.plot(o)\n            ax.set_title(title)\n\nlearn = fit()\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.617736\n\n\n0.533550\n\n\n0.831100\n\n\n00:08\n\n\n\n\n\n\n\nlearn.activation_stats.plot_layer_stats(0)\n\n\n\n\n\nNote: Generally, the model should have a consisten (or at least smooth) mean and standard deviation of layer activations during training. * Activations near zero indicate we have computation in the model that is doing nothing at all * zeros in one layer generally carry over to the next layer, which will then create more zeros\n\n# The penultimate layer\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\nNote: The problems got wors toward the end of the network.\n\n\n\nIncrease Batch Size\n\na larger batch size can make training more stable\nlarger batches have more accurate gradients, since they are calculated from more data\nlarger batch sizes mean fewer batches per epoch, meaning fewer opportunities for your model to update weights\n\n\ndls = get_dls(512)\nlearn = fit()\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.444612\n\n\n0.259085\n\n\n0.916200\n\n\n00:05\n\n\n\n\n\n\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\nNote: Still a high number of activations near zero.\n\n\n1cycle Training\n\nit is dangerous to begin training with a high learning rate as the initial random weights are not well suited to the target task\ndon’t want to end with a high learning rate either\nwant to start with a smaller learning rate, then gradually increase it, then gradually decrease it again towards the end of training\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n\ndesigned a schedule for learning rate separated into two phases\n\nwarmup: the learning rate grows from the minimum value to the maximum value\nannealing: the learning rate decreases back to the minimum value\n\n\n1cycle training allows us to use higher learning rates\n\nallows us to train faster and reduces\nresults in less overfitting\n\nwe skip over sharp local minima in the loss landscape\nwe end up in a smoother, more generalizable part of the loss landscape\n\n\na model that generalizes well is one whose loss would not change much if you changed the input a little\n\n\nMomentum\n\na technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps\nA disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay\n\ncyclical momentum: the momentum varies in the opposite direction of the learning rate\n\nhigh learning rate, low momentum\nlow learning rate, high momentum\n\n\n\n\ndef fit(epochs=1, lr=0.06):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit_one_cycle(epochs, lr)\n    return learn\n\n\nfastai fit_one_cycle\n\nuses cosine annealing instead of linear annealing\nlr_max: the highest learning rate that will be used during training\n\nsingle number for all layers\na list specifying learning rates for each layer group\na Python slice object containing learning rates for the first and last layer group\n\ndiv: How much to divide lr_max by to get the starting learning rate\ndiv_final: How much to divide lr_max by to get the ending learning rate\npct_start: What percentage of the batches to use for warmup\nmoms: a tuple (mom1,mom2,mom3)\n\nmom1: the initial momentum\nmom2: the minimum momentum\nmom3: the final momentum\n\n\n\nLearner.fit_one_cycle\n&lt;function fastai.callback.schedule.Learner.fit_one_cycle(self: fastai.learner.Learner, n_epoch, lr_max=None, div=25.0, div_final=100000.0, pct_start=0.25, wd=None, moms=None, cbs=None, reset_opt=False)&gt;\n\nprint_source(Learner.fit_one_cycle)\n@patch\ndef fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n                  moms=None, cbs=None, reset_opt=False):\n    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)\n\nprint_source(ParamScheduler)\n@docs\nclass ParamScheduler(Callback):\n    \"Schedule hyper-parameters according to `scheds`\"\n    order,run_valid = 60,False\n\n    def __init__(self, scheds): self.scheds = scheds\n    def before_fit(self): self.hps = {p:[] for p in self.scheds.keys()}\n    def before_batch(self): self._update_val(self.pct_train)\n\n    def _update_val(self, pct):\n        for n,f in self.scheds.items(): self.opt.set_hyper(n, f(pct))\n\n    def after_batch(self):\n        for p in self.scheds.keys(): self.hps[p].append(self.opt.hypers[-1][p])\n\n    def after_fit(self):\n        if hasattr(self.learn, 'recorder') and hasattr(self, 'hps'): self.recorder.hps = self.hps\n\n    _docs = {\"before_fit\": \"Initialize container for hyper-parameters\",\n             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n             \"after_batch\": \"Record hyper-parameters of this batch\",\n             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one\"}\n\nlearn = fit()\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.191914\n\n\n0.067390\n\n\n0.979200\n\n\n00:05\n\n\n\n\n\n\n\nlearn.recorder.plot_sched()\n\n\n\n\n\n\nlearn.recorder\nRecorder\n\nRecorder\nfastai.learner.Recorder\n\n\nfastai Recorder\n\nDocumentaion\nrecords everything that happens during training including\n\nlosses\nmetrics\nhyperparameters\n\nlearning rate\nmomentum\n\n\n\n\nprint_source(Recorder)\nclass Recorder(Callback):\n    \"Callback that registers statistics (lr, loss and metrics) during training\"\n    _stateattrs=('lrs','iters','losses','values')\n    remove_on_fetch,order = True,50\n\n    def __init__(self, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n        store_attr('add_time,train_metrics,valid_metrics')\n        self.loss,self.smooth_loss = AvgLoss(),AvgSmoothLoss(beta=beta)\n\n    def before_fit(self):\n        \"Prepare state for training\"\n        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n        names = self.metrics.attrgot('name')\n        if self.train_metrics and self.valid_metrics:\n            names = L('loss') + names\n            names = names.map('train_{}') + names.map('valid_{}')\n        elif self.valid_metrics: names = L('train_loss', 'valid_loss') + names\n        else: names = L('train_loss') + names\n        if self.add_time: names.append('time')\n        self.metric_names = 'epoch'+names\n        self.smooth_loss.reset()\n\n    def after_batch(self):\n        \"Update all metrics and records lr and smooth loss in training\"\n        if len(self.yb) == 0: return\n        mets = self._train_mets if self.training else self._valid_mets\n        for met in mets: met.accumulate(self.learn)\n        if not self.training: return\n        self.lrs.append(self.opt.hypers[-1]['lr'])\n        self.losses.append(self.smooth_loss.value)\n        self.learn.smooth_loss = self.smooth_loss.value\n\n    def before_epoch(self):\n        \"Set timer if `self.add_time=True`\"\n        self.cancel_train,self.cancel_valid = False,False\n        if self.add_time: self.start_epoch = time.time()\n        self.log = L(getattr(self, 'epoch', 0))\n\n    def before_train   (self): self._train_mets[1:].map(Self.reset())\n    def before_validate(self): self._valid_mets.map(Self.reset())\n    def after_train   (self): self.log += self._train_mets.map(_maybe_item)\n    def after_validate(self): self.log += self._valid_mets.map(_maybe_item)\n    def after_cancel_train(self):    self.cancel_train = True\n    def after_cancel_validate(self): self.cancel_valid = True\n\n    def after_epoch(self):\n        \"Store and log the loss/metric values\"\n        self.learn.final_record = self.log[1:].copy()\n        self.values.append(self.learn.final_record)\n        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n        self.logger(self.log)\n        self.iters.append(self.smooth_loss.count)\n\n    @property\n    def _train_mets(self):\n        if getattr(self, 'cancel_train', False): return L()\n        return L(self.smooth_loss) + (self.metrics if self.train_metrics else L())\n\n    @property\n    def _valid_mets(self):\n        if getattr(self, 'cancel_valid', False): return L()\n        return (L(self.loss) + self.metrics if self.valid_metrics else L())\n\n    def plot_loss(self, skip_start=5, with_valid=True):\n        plt.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n        if with_valid:\n            idx = (np.array(self.iters)&lt;skip_start).sum()\n            valid_col = self.metric_names.index('valid_loss') - 1\n            plt.plot(self.iters[idx:], L(self.values[idx:]).itemgot(valid_col), label='valid')\n            plt.legend()\n\nRecorder.plot_sched\n&lt;function fastai.callback.schedule.Recorder.plot_sched(self: fastai.learner.Recorder, keys=None, figsize=None)&gt;\n\nprint_source(Recorder.plot_sched)\n@patch\ndef plot_sched(self:Recorder, keys=None, figsize=None):\n    keys = self.hps.keys() if keys is None else L(keys)\n    rows,cols = (len(keys)+1)//2, min(2, len(keys))\n    figsize = figsize or (6*cols,4*rows)\n    _, axs = plt.subplots(rows, cols, figsize=figsize)\n    axs = axs.flatten() if len(keys) &gt; 1 else L(axs)\n    for p,ax in zip(keys, axs):\n        ax.plot(self.hps[p])\n        ax.set_ylabel(p)\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\nNote: The percentage of non-zero weight is better, but still high.\nlearn.activation_stats\nActivationStats\n\nprint_source(ActivationStats)\n@delegates()\nclass ActivationStats(HookCallback):\n    \"Callback that record the mean and std of activations.\"\n    order=-20\n    def __init__(self, with_hist=False, **kwargs):\n        super().__init__(**kwargs)\n        self.with_hist = with_hist\n\n    def before_fit(self):\n        \"Initialize stats.\"\n        super().before_fit()\n        self.stats = L()\n\n    def hook(self, m, i, o):\n        if isinstance(o, tuple): return self.hook_multi_ouput(o)\n        o = o.float()\n        res = {'mean': o.mean().item(), 'std': o.std().item(),\n               'near_zero': (o&lt;=0.05).long().sum().item()/o.numel()}\n        if self.with_hist: res['hist'] = o.histc(40,0,10)\n        return res\n\n    def hook_multi_ouput(self,o_tuple):\n        \"For outputs of RNN which are [nested] tuples of tensors\"\n        res = []\n        for o in self._flatten_tuple(o_tuple):\n            if not(isinstance(o, Tensor)): continue\n            res.append(self.hook(None, None, o))\n        return res\n\n    def _flatten_tuple(self, o_tuple):\n        \"Recursively flatten a [nested] tuple\"\n        res = []\n        for it in o_tuple:\n            if isinstance(it, tuple): res += self._flatten_tuple(it)\n            else: res += [it]\n        return tuple(res)\n\n    def after_batch(self):\n        \"Take the stored results and puts it in `self.stats`\"\n        if self.training and (self.every is None or self.train_iter%self.every == 0):\n            self.stats.append(self.hooks.stored)\n        super().after_batch()\n\n    def layer_stats(self, idx):\n        lstats = self.stats.itemgot(idx)\n        return L(lstats.itemgot(o) for o in ('mean','std','near_zero'))\n\n    def hist(self, idx):\n        res = self.stats.itemgot(idx).itemgot('hist')\n        return torch.stack(tuple(res)).t().float().log1p()\n\n    def color_dim(self, idx, figsize=(10,5), ax=None):\n        \"The 'colorful dimension' plot\"\n        res = self.hist(idx)\n        if ax is None: ax = subplots(figsize=figsize)[1][0]\n        ax.imshow(res, origin='lower')\n        ax.axis('off')\n\n    def plot_layer_stats(self, idx):\n        _,axs = subplots(1, 3, figsize=(12,3))\n        for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n            ax.plot(o)\n            ax.set_title(title)\n\n\nfastai color_dim\n\nDetailed Explanation\ndeveloped with fast.ai student Stefano Giomo\nexpress with colors the mean and standard deviation of activations for each batch during training\nvertical axis represents a group (bin) of activation values\neach column in the horizontal axis is a batch\nthe colors represent how many activations for that batch have a value in that bin\n\n\n# Set matplotlib color map\nmatplotlib.rcParams['image.cmap'] = 'viridis'\nlearn.activation_stats.color_dim(-2)\n\n\n\n\n\nNote: This shows the classic picture of “bad training”: * Starts with nearly all activations at zero * The number of nonzero activations increases exponentially over the first few batches * Then it goes to far and collapses with most activations returning to zero or near-zero * This cycle repeats a few times before we see a spread of activations throughout the range * This can be addressed with batch normalization\n\n\n\nBatch Normalization\n\ntake the average of the mean and standard deviations of the activations of a layer and use those to normalize that activations\n\nthis by itself can cause problems if the network wants some activations to be really high in order to make accurate predictions\n\nresolved by adding two learnable parameters, gamma and beta\n\ngamma*y + beta where y is a vector of normalize activations\n\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\ntraining deep neural networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as parameters of the previous layers change\n\ncalled covariate shift\nslows down training by requiring lower learning rates and careful parameter initialization\nresolved by normalizing layer inputs (each mini-batch)\n\n\nbatch normalization allows much higher learning rates and is less sensitive to parameter initialization\nDifferent behavior during training and validation\n\ntraining: use the mean and standard deviation of the batch to normalize the data\nvalidation: use a running mean of the statistics calculated during training\n\nmodels with batch normalization layers tend to generalize better\n\n\\(y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\)\n\nnn.BatchNorm2d\ntorch.nn.modules.batchnorm.BatchNorm2d\n\nprint_source(nn.BatchNorm2d)\nclass BatchNorm2d(_BatchNorm):\n    r\"\"\"Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\n    with additional channel dimension) as described in the paper\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`__ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n    of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n    to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\n    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n\n    Also by default, during training this layer keeps running estimates of its\n    computed mean and variance, which are then used for normalization during\n    evaluation. The running estimates are kept with a default :attr:`momentum`\n    of 0.1.\n\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n    keep running estimates, and batch statistics are instead used during\n    evaluation time as well.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Because the Batch Normalization is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, H, W)`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples::\n\n        &gt;&gt;&gt; # With Learnable Parameters\n        &gt;&gt;&gt; m = nn.BatchNorm2d(100)\n        &gt;&gt;&gt; # Without Learnable Parameters\n        &gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)\n        &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)\n        &gt;&gt;&gt; output = m(input)\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n\ndef conv(ni, nf, ks=3, act=True):\n    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n    layers.append(nn.BatchNorm2d(nf))\n    if act: layers.append(nn.ReLU())\n    return nn.Sequential(*layers)\n\nlearn = fit()\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.135761\n\n\n0.058451\n\n\n0.985700\n\n\n00:06\n\n\n\n\n\n\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\n\nNote: Shows a smooth development of activations, with no crashes.\n# Try training for longer and at a higher learning rate\nlearn = fit(5, lr=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.185239\n\n\n0.153986\n\n\n0.951600\n\n\n00:06\n\n\n\n\n1\n\n\n0.083529\n\n\n0.110004\n\n\n0.965800\n\n\n00:06\n\n\n\n\n2\n\n\n0.052301\n\n\n0.048957\n\n\n0.984400\n\n\n00:07\n\n\n\n\n3\n\n\n0.034640\n\n\n0.032938\n\n\n0.988600\n\n\n00:06\n\n\n\n\n4\n\n\n0.017389\n\n\n0.024644\n\n\n0.991700\n\n\n00:06\n\n\n\n\n\n\n\nlearn = fit(5, lr=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.187077\n\n\n0.099310\n\n\n0.969900\n\n\n00:06\n\n\n\n\n1\n\n\n0.077691\n\n\n0.089945\n\n\n0.972400\n\n\n00:06\n\n\n\n\n2\n\n\n0.050960\n\n\n0.061807\n\n\n0.980500\n\n\n00:06\n\n\n\n\n3\n\n\n0.033020\n\n\n0.030316\n\n\n0.989600\n\n\n00:06\n\n\n\n\n4\n\n\n0.017050\n\n\n0.023186\n\n\n0.992000\n\n\n00:06"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-13/index.html#references",
    "href": "posts/fastai-book-notes/chapter-13/index.html#references",
    "title": "Notes on fastai Book Ch. 13",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 12\nNext: Notes on fastai Book Ch. 14"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-14/index.html",
    "href": "posts/fastai-book-notes/chapter-14/index.html",
    "title": "Notes on fastai Book Ch. 14",
    "section": "",
    "text": "ResNets\nGoing Back to Imagenette\nBuilding a Modern CNN: ResNet\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-14/index.html#resnets",
    "href": "posts/fastai-book-notes/chapter-14/index.html#resnets",
    "title": "Notes on fastai Book Ch. 14",
    "section": "ResNets",
    "text": "ResNets\n\nDeep Residual Learning for Image Recognition\n\nintroduced the concept of residual (skip) connections"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-14/index.html#going-back-to-imagenette",
    "href": "posts/fastai-book-notes/chapter-14/index.html#going-back-to-imagenette",
    "title": "Notes on fastai Book Ch. 14",
    "section": "Going Back to Imagenette",
    "text": "Going Back to Imagenette\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    print(path)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\nURLs.IMAGENETTE_160\n'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz'\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n\nAverage Pooling\n\ntake the average of activations across a convolutional grid\nan alternative to the approach of using stride-2 layers to downscale input dimensions to the a single output vector\nused in fully convolutional networks to allow the model to be used on image sizes other than what the model was trained on\n\nNote: A fully convolutional network is a good choice when the objects you want to classify don’t have a single correct orientation or size (e.g. most natural photos) * It would not be a good choice for applications like MNIST where their is a fixed correct orientation for each character\n\n# Take the mean over the x and y axes\n# Will always convert a grid of activations into a single activation per image\ndef avg_pool(x): return x.mean((2,3))\ndef block(ni, nf): return ConvLayer(ni, nf, stride=2)\ndef get_model():\n    return nn.Sequential(\n        block(3, 16),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten(),\n        nn.Linear(256, dls.c))\n\nConvLayer\nfastai.layers.ConvLayer\n\nprint_source(ConvLayer)\nclass ConvLayer(nn.Sequential):\n    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and `norm_type` layers.\"\n    @delegates(nn.Conv2d)\n    def __init__(self, ni, nf, ks=3, stride=1, padding=None, bias=None, ndim=2, norm_type=NormType.Batch, bn_1st=True,\n                 act_cls=defaults.activation, transpose=False, init='auto', xtra=None, bias_std=0.01, **kwargs):\n        if padding is None: padding = ((ks-1)//2 if not transpose else 0)\n        bn = norm_type in (NormType.Batch, NormType.BatchZero)\n        inn = norm_type in (NormType.Instance, NormType.InstanceZero)\n        if bias is None: bias = not (bn or inn)\n        conv_func = _conv_func(ndim, transpose=transpose)\n        conv = conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding, **kwargs)\n        act = None if act_cls is None else act_cls()\n        init_linear(conv, act, init=init, bias_std=bias_std)\n        if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n        elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n        layers = [conv]\n        act_bn = []\n        if act is not None: act_bn.append(act)\n        if bn: act_bn.append(BatchNorm(nf, norm_type=norm_type, ndim=ndim))\n        if inn: act_bn.append(InstanceNorm(nf, norm_type=norm_type, ndim=ndim))\n        if bn_1st: act_bn.reverse()\n        layers += act_bn\n        if xtra: layers.append(xtra)\n        super().__init__(*layers)\n\nAdaptive Average Pooling\n\naverages a grid of activations into whatever sized destination you require\n\n\nnn.AdaptiveAvgPool2d\ntorch.nn.modules.pooling.AdaptiveAvgPool2d\n\ninput: \\((N, C, H_{in}, W_{in})\\) or \\((C, H_{in}, W_{in})\\)\noutput: \\((N, C, S_{0}, S_{1})\\) or \\((C, S_{0}, S_{1})\\), where \\(S=output_size\\)\n\n\nprint_source(nn.AdaptiveAvgPool2d)\nclass AdaptiveAvgPool2d(_AdaptiveAvgPoolNd):\n    r\"\"\"Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n    The output is of size H x W, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size of the image of the form H x W.\n                     Can be a tuple (H, W) or a single H for a square image H x H.\n                     H and W can be either a ``int``, or ``None`` which means the size will\n                     be the same as that of the input.\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, S_{0}, S_{1})` or :math:`(C, S_{0}, S_{1})`, where\n          :math:`S=\\text{output\\_size}`.\n\n    Examples:\n        &gt;&gt;&gt; # target output size of 5x7\n        &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))\n        &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)\n        &gt;&gt;&gt; output = m(input)\n        &gt;&gt;&gt; # target output size of 7x7 (square)\n        &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)\n        &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n        &gt;&gt;&gt; output = m(input)\n        &gt;&gt;&gt; # target output size of 10x7\n        &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((None, 7))\n        &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n        &gt;&gt;&gt; output = m(input)\n\n    \"\"\"\n\n    output_size: _size_2_opt_t\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return F.adaptive_avg_pool2d(input, self.output_size)\n\ndef get_learner(m):\n    return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy\n                  ).to_fp16()\n\nlearn = get_learner(get_model())\n\nlearn.model\nSequential(\n  (0): ConvLayer(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (1): ConvLayer(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (2): ConvLayer(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (3): ConvLayer(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (4): ConvLayer(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (5): AdaptiveAvgPool2d(output_size=1)\n  (6): Flatten(full=False)\n  (7): Linear(in_features=256, out_features=10, bias=True)\n)\n\nlearn.lr_find()\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.885349\n\n\n2.287517\n\n\n0.341401\n\n\n00:03\n\n\n\n\n1\n\n\n1.525377\n\n\n2.245459\n\n\n0.395159\n\n\n00:03\n\n\n\n\n2\n\n\n1.290083\n\n\n1.217583\n\n\n0.609172\n\n\n00:03\n\n\n\n\n3\n\n\n1.145256\n\n\n1.164249\n\n\n0.631338\n\n\n00:03\n\n\n\n\n4\n\n\n1.045142\n\n\n1.064749\n\n\n0.668025\n\n\n00:03\n\n\n\n\n\n\nNote: We won’t necessarily get better results simply by adding more layers.\nlearn = get_learner(\n    nn.Sequential(\n        block(3, 16),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256),\n        block(256, 512),\n        block(512, 1024),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten(),\n        nn.Linear(1024, dls.c)))\nlearn.lr_find()\nSuggestedLRs(valley=0.0002754228771664202)\n\n\n\n\n\n\nlearn.fit_one_cycle(10, 0.03)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.826138\n\n\n3.406225\n\n\n0.314395\n\n\n00:03\n\n\n\n\n1\n\n\n1.752539\n\n\n3.521486\n\n\n0.296051\n\n\n00:03\n\n\n\n\n2\n\n\n1.531295\n\n\n2.125122\n\n\n0.382420\n\n\n00:03\n\n\n\n\n3\n\n\n1.322462\n\n\n1.528130\n\n\n0.530955\n\n\n00:04\n\n\n\n\n4\n\n\n1.180348\n\n\n1.409803\n\n\n0.559236\n\n\n00:03\n\n\n\n\n5\n\n\n1.112325\n\n\n1.155497\n\n\n0.638217\n\n\n00:03\n\n\n\n\n6\n\n\n0.977593\n\n\n0.918580\n\n\n0.706242\n\n\n00:03\n\n\n\n\n7\n\n\n0.870830\n\n\n1.019607\n\n\n0.680000\n\n\n00:03\n\n\n\n\n8\n\n\n0.780924\n\n\n0.779768\n\n\n0.750573\n\n\n00:03\n\n\n\n\n9\n\n\n0.706074\n\n\n0.767961\n\n\n0.759490\n\n\n00:03\n\n\n\n\n\n\nNote: Needed to train for more epochs to achieve the same accuracy."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-14/index.html#building-a-modern-cnn-resnet",
    "href": "posts/fastai-book-notes/chapter-14/index.html#building-a-modern-cnn-resnet",
    "title": "Notes on fastai Book Ch. 14",
    "section": "Building a Modern CNN: ResNet",
    "text": "Building a Modern CNN: ResNet\n\nSkip Connections\n\nskip connections make the network easier to train with SGD\n\nx + conv2(conv1(x))\n\nIdentity Mapping: returning the input without changing it at all\nresidual: prediction minus target\nThe authors of the ResNet paper noticed that a network with more layers performed worse than a network with fewer layers, all other factors being equal\n\nthis was true on both the training and validation sets\n\nKey Insight:\n\nstart with a model with fewer layers that is trained well, and add more layers to it that do nothing at all the result should be a larger network that does exactly the same thing as the smaller network, proving there are always deep networks that should be at least as good as a more shallow network\nhowever, SGD was not finding these\n\nUsing skip connections also helps smooth the loss function, which makes training easier\n\nNote: Use zero for the initial value of gamma in batch normalization * allows training at higher learning rates * recall: \\(y*gamma + beta\\)\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf):\n        self.convs = nn.Sequential(\n            ConvLayer(ni,nf),\n            ConvLayer(nf,nf, norm_type=NormType.BatchZero))\n        \n    def forward(self, x): return x + self.convs(x)\nNote: The above implementation requires the stride to be 1 and ni==nf in order for x + self.convs(x) to work. * Need to change the shape of x to match the result of self.convs(x) * Can use an average pooling layer with a stride of 2 to change the shape * Can use a 1x1 convolution to change the number of channels\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf, stride=stride),\n        ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero))\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf, stride=1):\n        self.convs = _conv_block(ni,nf,stride)\n        self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None)\n        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return F.relu(self.convs(x) + self.idconv(self.pool(x)))\n\nnoop\n&lt;function fastai.imports.noop(x=None, *args, **kwargs)&gt;\n\nprint_source(noop)\ndef noop (x=None, *args, **kwargs):\n    \"Do nothing\"\n    return x\n\ndef block(ni,nf): return ResBlock(ni, nf, stride=2)\nlearn = get_learner(get_model())\n\nlearn.model\nSequential(\n  (0): ResBlock(\n    (convs): Sequential(\n      (0): ConvLayer(\n        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (1): ConvLayer(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (idconv): ConvLayer(\n      (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (1): ResBlock(\n    (convs): Sequential(\n      (0): ConvLayer(\n        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (1): ConvLayer(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (idconv): ConvLayer(\n      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (2): ResBlock(\n    (convs): Sequential(\n      (0): ConvLayer(\n        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (1): ConvLayer(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (idconv): ConvLayer(\n      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (3): ResBlock(\n    (convs): Sequential(\n      (0): ConvLayer(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (1): ConvLayer(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (idconv): ConvLayer(\n      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (4): ResBlock(\n    (convs): Sequential(\n      (0): ConvLayer(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (1): ConvLayer(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (idconv): ConvLayer(\n      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (5): AdaptiveAvgPool2d(output_size=1)\n  (6): Flatten(full=False)\n  (7): Linear(in_features=256, out_features=10, bias=True)\n)\n\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.975520\n\n\n1.854834\n\n\n0.346242\n\n\n00:04\n\n\n\n\n1\n\n\n1.658158\n\n\n1.478215\n\n\n0.517197\n\n\n00:04\n\n\n\n\n2\n\n\n1.377669\n\n\n1.307936\n\n\n0.585478\n\n\n00:04\n\n\n\n\n3\n\n\n1.162053\n\n\n1.106804\n\n\n0.642548\n\n\n00:04\n\n\n\n\n4\n\n\n1.027879\n\n\n1.009977\n\n\n0.674140\n\n\n00:04\n\n\n\n\n\n\n\n# Try training with a model that is twice as deep\ndef block(ni, nf):\n    return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf))\n\nlearn = get_learner(get_model())\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.940119\n\n\n1.815366\n\n\n0.372484\n\n\n00:05\n\n\n\n\n1\n\n\n1.602748\n\n\n1.461486\n\n\n0.526879\n\n\n00:05\n\n\n\n\n2\n\n\n1.296337\n\n\n1.346131\n\n\n0.571210\n\n\n00:05\n\n\n\n\n3\n\n\n1.052998\n\n\n0.996216\n\n\n0.678471\n\n\n00:05\n\n\n\n\n4\n\n\n0.926301\n\n\n0.901209\n\n\n0.713121\n\n\n00:05\n\n\n\n\n\n\n\n\nA State-of-the-Art ResNet\n\nBag of Tricks for Image Classification with Convolutional Neural Networks\n\nstudies variations of the ResNet architecture that come at almost no additional cost in terms of number of parameters or computation\nused a tweaked ResNet-50 architecture and Mixup to achieve a 94.6% top-5 accuracy on ImageNet compared to 92.2% with a regular ResNet-50 without Mixup\n\n\n\nTop-5 Accuracy\n\na metric testing how often the label we want is in the top-5 predictions of our model\nused in ImageNet competition because many of the images contained multiple objects, or objects that could be easily confused or were mislabeled\n\n\n\nStem\n\nthe first few layers of a CNN\nthe stem has a different structure than the main body of the CNN\nthe vast majority of computation in a deep convolutional network occurs in the early layers\n\nwe should keep the early layers as fast and simple as possible\n\nthe vast majority of parameters are in the last layers\na ResNet block takes more computation than a plain convolutional block\n\n\ndef _resnet_stem(*sizes):\n    return [\n        ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1)\n            for i in range(len(sizes)-1)\n    ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n\n_resnet_stem(3,32,32,64)\n[ConvLayer(\n   (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ),\n ConvLayer(\n   (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ),\n ConvLayer(\n   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ),\n MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)]\n\nclass ResNet(nn.Sequential):\n    def __init__(self, n_out, layers, expansion=1):\n        stem = _resnet_stem(3,32,32,64)\n        self.block_szs = [64, 64, 128, 256, 512]\n        for i in range(1,5): self.block_szs[i] *= expansion\n        blocks = [self._make_layer(*o) for o in enumerate(layers)]\n        super().__init__(*stem, *blocks,\n                         nn.AdaptiveAvgPool2d(1), Flatten(),\n                         nn.Linear(self.block_szs[-1], n_out))\n    \n    def _make_layer(self, idx, n_layers):\n        stride = 1 if idx==0 else 2\n        ch_in,ch_out = self.block_szs[idx:idx+2]\n        return nn.Sequential(*[\n            ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1)\n            for i in range(n_layers)\n        ])\n\nrn = ResNet(dls.c, [2,2,2,2])\n\nlearn = get_learner(rn)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.676334\n\n\n3.144195\n\n\n0.332229\n\n\n00:05\n\n\n\n\n1\n\n\n1.330461\n\n\n1.301046\n\n\n0.601274\n\n\n00:05\n\n\n\n\n2\n\n\n1.081648\n\n\n1.654932\n\n\n0.530701\n\n\n00:05\n\n\n\n\n3\n\n\n0.883710\n\n\n0.889572\n\n\n0.725096\n\n\n00:06\n\n\n\n\n4\n\n\n0.752461\n\n\n0.770025\n\n\n0.751592\n\n\n00:05\n\n\n\n\n\n\nNote: The optimized stem kept training time just as fast as before despite the model having more channels.\n\n\n\nBottleneck Layers\n\ninstead of stacking two convolutions with a kernel size of 3, bottleneck layers use three convolutions\n\ntwo \\(1x1\\) (at the beginning and the end) and one \\(3x3\\)\n\n\\(1x1\\) convolutions are much faster so the block executes faster than the earlier type of ResNet block above\n\nallows us to use more convolutional filters\n\n\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf//4, 1),\n        ConvLayer(nf//4, nf//4, stride=stride), \n        ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero))\n\n# Use bigger images\ndls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224)\n/home/innom-dt/.fastai/data/imagenette2-320\n\nrn = ResNet(dls.c, [3,4,6,3], 4)\nNote: Deeper networks like this take more epochs to show improvements in accuracy.\nlearn = get_learner(rn)\n\nlearn.lr_find()\nSuggestedLRs(valley=2.2908675418875646e-06)\n\n\n\n\n\n\nlearn.fit_one_cycle(20, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.631725\n\n\n1.667390\n\n\n0.491720\n\n\n00:26\n\n\n\n\n1\n\n\n1.396282\n\n\n1.772770\n\n\n0.492739\n\n\n00:26\n\n\n\n\n2\n\n\n1.244810\n\n\n1.644604\n\n\n0.520764\n\n\n00:26\n\n\n\n\n3\n\n\n1.147434\n\n\n2.000461\n\n\n0.433121\n\n\n00:26\n\n\n\n\n4\n\n\n1.061841\n\n\n1.668560\n\n\n0.537580\n\n\n00:26\n\n\n\n\n5\n\n\n0.971388\n\n\n1.221623\n\n\n0.634395\n\n\n00:26\n\n\n\n\n6\n\n\n0.875570\n\n\n1.315725\n\n\n0.606369\n\n\n00:26\n\n\n\n\n7\n\n\n0.773697\n\n\n1.347997\n\n\n0.625987\n\n\n00:26\n\n\n\n\n8\n\n\n0.693710\n\n\n1.044750\n\n\n0.662166\n\n\n00:26\n\n\n\n\n9\n\n\n0.642479\n\n\n0.847296\n\n\n0.715669\n\n\n00:26\n\n\n\n\n10\n\n\n0.597677\n\n\n0.815032\n\n\n0.728662\n\n\n00:26\n\n\n\n\n11\n\n\n0.554460\n\n\n1.310289\n\n\n0.632357\n\n\n00:26\n\n\n\n\n12\n\n\n0.504132\n\n\n0.645502\n\n\n0.797707\n\n\n00:26\n\n\n\n\n13\n\n\n0.444430\n\n\n0.553372\n\n\n0.839490\n\n\n00:26\n\n\n\n\n14\n\n\n0.404554\n\n\n0.524731\n\n\n0.840510\n\n\n00:26\n\n\n\n\n15\n\n\n0.363680\n\n\n0.430417\n\n\n0.869299\n\n\n00:26\n\n\n\n\n16\n\n\n0.326445\n\n\n0.468357\n\n\n0.858854\n\n\n00:26\n\n\n\n\n17\n\n\n0.291472\n\n\n0.398314\n\n\n0.882038\n\n\n00:26\n\n\n\n\n18\n\n\n0.273819\n\n\n0.441020\n\n\n0.865987\n\n\n00:26\n\n\n\n\n19\n\n\n0.266643\n\n\n0.429735\n\n\n0.868280\n\n\n00:26\n\n\n\n\n\n\n\n# Try training for much longer with MixUp\nrn = ResNet(dls.c, [3,4,6,3], 4)\nlearn = Learner(dls, rn, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixUp).to_fp16()\n\nlearn.cbs\n(#5) [TrainEvalCallback,Recorder,ProgressCallback,MixUp,MixedPrecision]\n\nlearn.lr_find()\nSuggestedLRs(valley=0.002511886414140463)\n\n\n\n\n\n\nlearn.fit_one_cycle(100, 1e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.357731\n\n\n2.383324\n\n\n0.130191\n\n\n00:27\n\n\n\n\n1\n\n\n2.336724\n\n\n2.336634\n\n\n0.139108\n\n\n00:27\n\n\n\n\n2\n\n\n2.307106\n\n\n2.287395\n\n\n0.160764\n\n\n00:26\n\n\n\n\n3\n\n\n2.270103\n\n\n2.233956\n\n\n0.206369\n\n\n00:27\n\n\n\n\n4\n\n\n2.230594\n\n\n2.175580\n\n\n0.248408\n\n\n00:27\n\n\n\n\n5\n\n\n2.186367\n\n\n2.114939\n\n\n0.285605\n\n\n00:26\n\n\n\n\n6\n\n\n2.138676\n\n\n2.052963\n\n\n0.314904\n\n\n00:26\n\n\n\n\n7\n\n\n2.093770\n\n\n1.997618\n\n\n0.340892\n\n\n00:26\n\n\n\n\n8\n\n\n2.050327\n\n\n1.941644\n\n\n0.362038\n\n\n00:26\n\n\n\n\n9\n\n\n2.008452\n\n\n1.897457\n\n\n0.381146\n\n\n00:26\n\n\n\n\n10\n\n\n1.976088\n\n\n1.857476\n\n\n0.393376\n\n\n00:26\n\n\n\n\n11\n\n\n1.942721\n\n\n1.811248\n\n\n0.414013\n\n\n00:26\n\n\n\n\n12\n\n\n1.913029\n\n\n1.795007\n\n\n0.415796\n\n\n00:26\n\n\n\n\n13\n\n\n1.887761\n\n\n1.753944\n\n\n0.427771\n\n\n00:26\n\n\n\n\n14\n\n\n1.867352\n\n\n1.726478\n\n\n0.446369\n\n\n00:26\n\n\n\n\n15\n\n\n1.850482\n\n\n1.703274\n\n\n0.458089\n\n\n00:26\n\n\n\n\n16\n\n\n1.820035\n\n\n1.656702\n\n\n0.478471\n\n\n00:26\n\n\n\n\n17\n\n\n1.804909\n\n\n1.627078\n\n\n0.489172\n\n\n00:26\n\n\n\n\n18\n\n\n1.777827\n\n\n1.597916\n\n\n0.509045\n\n\n00:26\n\n\n\n\n19\n\n\n1.754695\n\n\n1.580777\n\n\n0.500637\n\n\n00:26\n\n\n\n\n20\n\n\n1.730523\n\n\n1.532980\n\n\n0.521274\n\n\n00:26\n\n\n\n\n21\n\n\n1.719565\n\n\n1.506606\n\n\n0.532229\n\n\n00:26\n\n\n\n\n22\n\n\n1.699022\n\n\n1.465760\n\n\n0.549045\n\n\n00:26\n\n\n\n\n23\n\n\n1.671000\n\n\n1.451764\n\n\n0.551083\n\n\n00:26\n\n\n\n\n24\n\n\n1.642783\n\n\n1.409297\n\n\n0.568408\n\n\n00:26\n\n\n\n\n25\n\n\n1.624837\n\n\n1.379675\n\n\n0.586752\n\n\n00:26\n\n\n\n\n26\n\n\n1.601221\n\n\n1.365560\n\n\n0.586752\n\n\n00:26\n\n\n\n\n27\n\n\n1.586046\n\n\n1.327966\n\n\n0.597452\n\n\n00:26\n\n\n\n\n28\n\n\n1.569099\n\n\n1.349460\n\n\n0.584713\n\n\n00:26\n\n\n\n\n29\n\n\n1.547596\n\n\n1.318505\n\n\n0.599236\n\n\n00:27\n\n\n\n\n30\n\n\n1.545876\n\n\n1.287586\n\n\n0.611465\n\n\n00:26\n\n\n\n\n31\n\n\n1.516503\n\n\n1.269668\n\n\n0.616815\n\n\n00:26\n\n\n\n\n32\n\n\n1.510151\n\n\n1.259776\n\n\n0.620892\n\n\n00:26\n\n\n\n\n33\n\n\n1.499429\n\n\n1.274317\n\n\n0.611720\n\n\n00:26\n\n\n\n\n34\n\n\n1.492611\n\n\n1.246700\n\n\n0.622675\n\n\n00:26\n\n\n\n\n35\n\n\n1.470544\n\n\n1.241083\n\n\n0.624204\n\n\n00:27\n\n\n\n\n36\n\n\n1.453925\n\n\n1.201963\n\n\n0.639236\n\n\n00:27\n\n\n\n\n37\n\n\n1.450406\n\n\n1.166396\n\n\n0.649172\n\n\n00:27\n\n\n\n\n38\n\n\n1.444372\n\n\n1.220421\n\n\n0.629045\n\n\n00:26\n\n\n\n\n39\n\n\n1.447028\n\n\n1.173165\n\n\n0.648153\n\n\n00:26\n\n\n\n\n40\n\n\n1.425553\n\n\n1.183527\n\n\n0.636433\n\n\n00:26\n\n\n\n\n41\n\n\n1.415466\n\n\n1.161390\n\n\n0.647643\n\n\n00:26\n\n\n\n\n42\n\n\n1.405419\n\n\n1.155593\n\n\n0.648662\n\n\n00:26\n\n\n\n\n43\n\n\n1.411261\n\n\n1.179044\n\n\n0.636688\n\n\n00:26\n\n\n\n\n44\n\n\n1.404317\n\n\n1.118105\n\n\n0.665733\n\n\n00:26\n\n\n\n\n45\n\n\n1.392309\n\n\n1.119027\n\n\n0.663949\n\n\n00:26\n\n\n\n\n46\n\n\n1.382899\n\n\n1.081154\n\n\n0.674395\n\n\n00:26\n\n\n\n\n47\n\n\n1.374947\n\n\n1.107488\n\n\n0.661911\n\n\n00:26\n\n\n\n\n48\n\n\n1.364989\n\n\n1.132930\n\n\n0.655287\n\n\n00:26\n\n\n\n\n49\n\n\n1.371344\n\n\n1.078784\n\n\n0.674904\n\n\n00:27\n\n\n\n\n50\n\n\n1.365992\n\n\n1.086367\n\n\n0.673885\n\n\n00:26\n\n\n\n\n51\n\n\n1.355748\n\n\n1.102273\n\n\n0.663185\n\n\n00:26\n\n\n\n\n52\n\n\n1.346135\n\n\n1.069236\n\n\n0.672357\n\n\n00:26\n\n\n\n\n53\n\n\n1.342680\n\n\n1.059305\n\n\n0.681019\n\n\n00:26\n\n\n\n\n54\n\n\n1.342354\n\n\n1.056270\n\n\n0.683057\n\n\n00:26\n\n\n\n\n55\n\n\n1.348057\n\n\n1.054993\n\n\n0.680000\n\n\n00:26\n\n\n\n\n56\n\n\n1.327710\n\n\n1.040115\n\n\n0.682293\n\n\n00:26\n\n\n\n\n57\n\n\n1.331265\n\n\n1.082385\n\n\n0.669299\n\n\n00:26\n\n\n\n\n58\n\n\n1.328200\n\n\n1.027992\n\n\n0.686879\n\n\n00:26\n\n\n\n\n59\n\n\n1.328389\n\n\n1.039981\n\n\n0.681529\n\n\n00:26\n\n\n\n\n60\n\n\n1.319880\n\n\n1.033769\n\n\n0.682293\n\n\n00:27\n\n\n\n\n61\n\n\n1.323757\n\n\n1.039335\n\n\n0.681783\n\n\n00:27\n\n\n\n\n62\n\n\n1.308027\n\n\n1.014351\n\n\n0.690191\n\n\n00:27\n\n\n\n\n63\n\n\n1.317649\n\n\n1.047797\n\n\n0.677197\n\n\n00:27\n\n\n\n\n64\n\n\n1.291757\n\n\n1.013592\n\n\n0.690955\n\n\n00:27\n\n\n\n\n65\n\n\n1.303231\n\n\n1.045248\n\n\n0.680255\n\n\n00:26\n\n\n\n\n66\n\n\n1.296039\n\n\n1.027358\n\n\n0.684331\n\n\n00:26\n\n\n\n\n67\n\n\n1.298419\n\n\n1.015950\n\n\n0.692994\n\n\n00:27\n\n\n\n\n68\n\n\n1.295830\n\n\n1.029434\n\n\n0.684841\n\n\n00:26\n\n\n\n\n69\n\n\n1.301385\n\n\n1.021460\n\n\n0.687898\n\n\n00:26\n\n\n\n\n70\n\n\n1.297332\n\n\n1.003877\n\n\n0.695032\n\n\n00:26\n\n\n\n\n71\n\n\n1.304215\n\n\n1.015531\n\n\n0.689936\n\n\n00:26\n\n\n\n\n72\n\n\n1.301384\n\n\n0.997708\n\n\n0.696051\n\n\n00:26\n\n\n\n\n73\n\n\n1.299483\n\n\n0.999720\n\n\n0.696561\n\n\n00:26\n\n\n\n\n74\n\n\n1.279976\n\n\n0.983441\n\n\n0.702420\n\n\n00:26\n\n\n\n\n75\n\n\n1.283374\n\n\n0.998377\n\n\n0.694522\n\n\n00:26\n\n\n\n\n76\n\n\n1.297397\n\n\n0.997266\n\n\n0.693248\n\n\n00:26\n\n\n\n\n77\n\n\n1.292927\n\n\n0.989717\n\n\n0.697580\n\n\n00:27"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-14/index.html#references",
    "href": "posts/fastai-book-notes/chapter-14/index.html#references",
    "title": "Notes on fastai Book Ch. 14",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 13\nNext: Notes on fastai Book Ch. 15"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html",
    "href": "posts/fastai-book-notes/chapter-15/index.html",
    "title": "Notes on fastai Book Ch. 15",
    "section": "",
    "text": "Application Architectures Deep Dive\nComputer Vision\nNatural Language Processing\nTabular\nConclusion\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#application-architectures-deep-dive",
    "href": "posts/fastai-book-notes/chapter-15/index.html#application-architectures-deep-dive",
    "title": "Notes on fastai Book Ch. 15",
    "section": "Application Architectures Deep Dive",
    "text": "Application Architectures Deep Dive"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#computer-vision",
    "href": "posts/fastai-book-notes/chapter-15/index.html#computer-vision",
    "title": "Notes on fastai Book Ch. 15",
    "section": "Computer Vision",
    "text": "Computer Vision\n\ncnn_learner\n\nTransfer Learning\n\nthe head (the final layers) of the pretrained model needs to be cut off and replaced\nfastai stores where to cut the included pretrained models in the model_meta dictionary\n\n\n\nHead\n\nthe part that is specialized for a particular task\ngenerally the part after the adaptive average pooling layer\n\n\n\nBody\n\neverything other than the head\nincludes the stem\n\n\npd.DataFrame(model_meta)\n\n\n\n\n\n\n\n\n&lt;function xresnet18 at 0x7f8668ee5310&gt;\n\n\n&lt;function xresnet34 at 0x7f8668ee53a0&gt;\n\n\n&lt;function xresnet50 at 0x7f8668ee5430&gt;\n\n\n&lt;function xresnet101 at 0x7f8668ee54c0&gt;\n\n\n&lt;function xresnet152 at 0x7f8668ee5550&gt;\n\n\n&lt;function resnet18 at 0x7f866b0d0670&gt;\n\n\n&lt;function resnet34 at 0x7f866b0d0700&gt;\n\n\n&lt;function resnet50 at 0x7f866b0d0790&gt;\n\n\n&lt;function resnet101 at 0x7f866b0d0820&gt;\n\n\n&lt;function resnet152 at 0x7f866b0d08b0&gt;\n\n\n&lt;function squeezenet1_0 at 0x7f866b0d7790&gt;\n\n\n&lt;function squeezenet1_1 at 0x7f866b0d7820&gt;\n\n\n&lt;function densenet121 at 0x7f866a7b78b0&gt;\n\n\n&lt;function densenet169 at 0x7f866a7b79d0&gt;\n\n\n&lt;function densenet201 at 0x7f866a7b7a60&gt;\n\n\n&lt;function densenet161 at 0x7f866a7b7940&gt;\n\n\n&lt;function vgg11_bn at 0x7f866b0d7040&gt;\n\n\n&lt;function vgg13_bn at 0x7f866b0d7160&gt;\n\n\n&lt;function vgg16_bn at 0x7f866b0d7280&gt;\n\n\n&lt;function vgg19_bn at 0x7f866b0d73a0&gt;\n\n\n&lt;function alexnet at 0x7f866b0c2d30&gt;\n\n\n\n\n\n\ncut\n\n\n-4\n\n\n-4\n\n\n-4\n\n\n-4\n\n\n-4\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n-1\n\n\n-1\n\n\n-1\n\n\n-1\n\n\n-1\n\n\n-1\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n-2\n\n\n\n\nsplit\n\n\n&lt;function _xresnet_split at 0x7f8662906ee0&gt;\n\n\n&lt;function _xresnet_split at 0x7f8662906ee0&gt;\n\n\n&lt;function _xresnet_split at 0x7f8662906ee0&gt;\n\n\n&lt;function _xresnet_split at 0x7f8662906ee0&gt;\n\n\n&lt;function _xresnet_split at 0x7f8662906ee0&gt;\n\n\n&lt;function _resnet_split at 0x7f8662906f70&gt;\n\n\n&lt;function _resnet_split at 0x7f8662906f70&gt;\n\n\n&lt;function _resnet_split at 0x7f8662906f70&gt;\n\n\n&lt;function _resnet_split at 0x7f8662906f70&gt;\n\n\n&lt;function _resnet_split at 0x7f8662906f70&gt;\n\n\n&lt;function _squeezenet_split at 0x7f866290e040&gt;\n\n\n&lt;function _squeezenet_split at 0x7f866290e040&gt;\n\n\n&lt;function _densenet_split at 0x7f866290e0d0&gt;\n\n\n&lt;function _densenet_split at 0x7f866290e0d0&gt;\n\n\n&lt;function _densenet_split at 0x7f866290e0d0&gt;\n\n\n&lt;function _densenet_split at 0x7f866290e0d0&gt;\n\n\n&lt;function _vgg_split at 0x7f866290e160&gt;\n\n\n&lt;function _vgg_split at 0x7f866290e160&gt;\n\n\n&lt;function _vgg_split at 0x7f866290e160&gt;\n\n\n&lt;function _vgg_split at 0x7f866290e160&gt;\n\n\n&lt;function _alexnet_split at 0x7f866290e1f0&gt;\n\n\n\n\nstats\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\n\n\n\n\nmodel_meta[resnet50]\n{'cut': -2,\n 'split': &lt;function fastai.vision.learner._resnet_split(m)&gt;,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}\n\nprint_source(model_meta[resnet50]['split'])\ndef  _resnet_split(m): return L(m[0][:6], m[0][6:], m[1:]).map(params)\n\ncreate_head(20,2)\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): Flatten(full=False)\n  (2): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=40, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\nNote: fastai add two linear layers by default for transfer learning * using just one linear layer is unlikely to be enough when transferring a pretrained model to very different domains\n\ncreate_head\n&lt;function fastai.vision.learner.create_head(nf, n_out, lin_ftrs=None, ps=0.5, concat_pool=True, first_bn=True, bn_final=False, lin_first=False, y_range=None)&gt;\n\nprint_source(create_head)\ndef create_head(nf, n_out, lin_ftrs=None, ps=0.5, concat_pool=True, first_bn=True, bn_final=False,\n                lin_first=False, y_range=None):\n    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n    if concat_pool: nf *= 2\n    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n    bns = [first_bn] + [True]*len(lin_ftrs[1:])\n    ps = L(ps)\n    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n    layers = [pool, Flatten()]\n    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n    for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n        layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n    if y_range is not None: layers.append(SigmoidRange(*y_range))\n    return nn.Sequential(*layers)\n\n\nOne Last Batchnorm\n\nbn_final: setting this to True will cause a batchnorm layher to be added as the final layer\ncan be useful in helping your model scale appropriately for your output activations\n\n\nAdaptiveConcatPool2d\nfastai.layers.AdaptiveConcatPool2d\n\nprint_source(AdaptiveConcatPool2d)\nclass AdaptiveConcatPool2d(Module):\n    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n    def __init__(self, size=None):\n        self.size = size or 1\n        self.ap = nn.AdaptiveAvgPool2d(self.size)\n        self.mp = nn.AdaptiveMaxPool2d(self.size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n\n\n\nunet_learner\n\nused for generative vision models\nuse a custom head which progressively increases the dimensions back to the same as the source image\n\ncan use nearest neighbor interpolation\ncan use a transposed convolution\n\nzero padding is inserted between all the pixels in the input before performing a convolution\nalso known as a stride half convolution\nfastai ConvLayer(transpose=True)\n\n\nunets use skip connections pass information from the encoding layers in the body to the decoding layers in the head\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\nTasks\n\nsegmentation\nsuper resolution\ncolorization\nstyle transfer\n\n\n\n\nA Siamese Network\n#hide\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.PETS)\npath\nPath('/home/innom-dt/.fastai/data/oxford-iiit-pet')\n\nfiles = get_image_files(path/\"images\")\nfiles\n(#7390) [Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Birman_121.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/shiba_inu_131.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bombay_176.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bengal_199.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/beagle_41.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/beagle_27.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/great_pyrenees_181.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/Bengal_100.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/keeshond_124.jpg'),Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images/havanese_115.jpg')...]\n\n# Custom type to allow us to show siamese image pairs\n# Tracks whether the two images belong to the same class\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n\ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        # Generate list of unique labels\n        self.labels = files.map(label_func).unique()\n        # Create a dictionary to match labels to filenames\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) \n                          for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        # Create siamese image pair\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        # 50/50 chance of generating a pair of the same class\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: \n            cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n\nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntls = TfmdLists(files, tfm, splits=splits)\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n    \n    def forward(self, x1, x2):\n        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n        return self.head(ftrs)\n\nencoder = create_body(resnet34, cut=-2)\n\ncreate_body\n&lt;function fastai.vision.learner.create_body(arch, n_in=3, pretrained=True, cut=None)&gt;\n\nprint_source(create_body)\ndef create_body(arch, n_in=3, pretrained=True, cut=None):\n    \"Cut off the body of a typically pretrained `arch` as determined by `cut`\"\n    model = arch(pretrained=pretrained)\n    _update_first_layer(model, n_in, pretrained)\n    #cut = ifnone(cut, cnn_config(arch)['cut'])\n    if cut is None:\n        ll = list(enumerate(model.children()))\n        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n    if   isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n    elif callable(cut): return cut(model)\n    else: raise NameError(\"cut must be either integer or a function\")\n\nhead = create_head(512*2, 2, ps=0.5)\nhead\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): Flatten(full=False)\n  (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=2048, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\nmodel = SiameseModel(encoder, head)\n\ndef loss_func(out, targ):\n    return nn.CrossEntropyLoss()(out, targ.long())\n\n# Tell fastai how to split the model into parameter groups\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n\nlearn = Learner(dls, model, loss_func=loss_func, \n                splitter=siamese_splitter, metrics=accuracy)\nlearn.freeze()\n\nLearner.freeze\n&lt;function fastai.learner.Learner.freeze(self: fastai.learner.Learner)&gt;\n\nprint_source(Learner.freeze)\n@patch\ndef freeze(self:Learner): self.freeze_to(-1)\n\nprint_source(Learner.freeze_to)\n@patch\ndef freeze_to(self:Learner, n):\n    if self.opt is None: self.create_opt()\n    self.opt.freeze_to(n)\n    self.opt.clear_state()\n\nlearn.fit_one_cycle(4, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.530529\n\n\n0.281408\n\n\n0.887686\n\n\n00:29\n\n\n\n\n1\n\n\n0.377506\n\n\n0.224826\n\n\n0.912043\n\n\n00:29\n\n\n\n\n2\n\n\n0.276916\n\n\n0.195273\n\n\n0.928958\n\n\n00:29\n\n\n\n\n3\n\n\n0.242797\n\n\n0.170715\n\n\n0.933018\n\n\n00:29\n\n\n\n\n\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-6,1e-4))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.249987\n\n\n0.160208\n\n\n0.939784\n\n\n00:38\n\n\n\n\n1\n\n\n0.236774\n\n\n0.157880\n\n\n0.941137\n\n\n00:38\n\n\n\n\n2\n\n\n0.222469\n\n\n0.151024\n\n\n0.945196\n\n\n00:38\n\n\n\n\n3\n\n\n0.218679\n\n\n0.160581\n\n\n0.939107\n\n\n00:38"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#natural-language-processing",
    "href": "posts/fastai-book-notes/chapter-15/index.html#natural-language-processing",
    "title": "Notes on fastai Book Ch. 15",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\nWe can convert an AWD-LSTM language model into a transfer learning classifier by selecting stack RNN for the encoder\nUniversal Language Model Fine-tuning for Text Classification\n\ndivide the document into fixed-length batches of size b\nthe model is initialized at the beginning of each batch with the final state of the previous batch\nkeep track of the hidden states for mean and max-pooling\ngradients are backpropogated to the batches whose hidden states contributed to the final prediction\n\nuse variable lenght backpropogation sequences\n\n\nthe classifier contains a for-loop, which loops over each batch of a sequence\n\nneed to gather data in batches\neach text needs to be treated separately as they have their own labels\nit is likely the texts will not all be the same length\n\nwe won’t be able to put them all in the same array\nneed to use padding\n\nwhen grabbing a bunch of texts, determine which one has the greatest length\nfill the ones that are shorter with the special character xxpad.\nmake sure texts of similar sizes are put together to minimize excess padding\n\n\n\nthe state is maintained across batches\nthe activations of each batch are stored\nat the end, we use the same average and max concatenated pooling trick used for computer vision models"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#tabular",
    "href": "posts/fastai-book-notes/chapter-15/index.html#tabular",
    "title": "Notes on fastai Book Ch. 15",
    "section": "Tabular",
    "text": "Tabular\nfrom fastai.tabular.all import *\n\nTabularModel\nfastai.tabular.model.TabularModel\n\nprint_source(TabularModel)\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0.,\n                 y_range=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True),\n                 lin_first=True):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\n\ndef forward(self, x_cat, x_cont=None):\n    # Check if there are any embeddings to deal with\n    if self.n_emb != 0:\n        # Get the activations of each embedding matrix\n        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n        # Concatenate embeddings to a single tensor\n        x = torch.cat(x, 1)\n        # Apply dropout\n        x = self.emb_drop(x)\n    # Check if there are any continuous variables to deal with \n    if self.n_cont != 0:\n        # Pass continuous variables through batch normalization layer\n        if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n        # Concatenate continuous variables with embedding activations\n        x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n    # Pass concatenated input through linear layers\n    return self.layers(x)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#conclusion",
    "href": "posts/fastai-book-notes/chapter-15/index.html#conclusion",
    "title": "Notes on fastai Book Ch. 15",
    "section": "Conclusion",
    "text": "Conclusion\n\ndeep learning can be challenging because your data, memory, and time are typically limited\ntrain a smaller model when memory is limited\nif you are not able to overfit your model to your data, you are not taking advantage of the capacity of your model\nYou should first get to a point where you can overfit\nSteps to reduce overfitting in order of priority\n\nMore data\n\n\nadd more labels to data you already have\nfind additional tasks your model could be asked to solve\ncreate additional synthetic data by using more or different augmentation techniques\n\n\nData augmentation\n\nMixup\n\nGeneralizable architecture\n\nAdd batch normalization\n\nRegularization\n\nAdding dropout to the last layer or two is often sufficient\nAdding dropout of different types throughout your model can help even more\n\nReduce architecture complexity\n\nShould be the last thing you try"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-15/index.html#references",
    "href": "posts/fastai-book-notes/chapter-15/index.html#references",
    "title": "Notes on fastai Book Ch. 15",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 14\nNext: Notes on fastai Book Ch. 16"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html",
    "href": "posts/fastai-book-notes/chapter-16/index.html",
    "title": "Notes on fastai Book Ch. 16",
    "section": "",
    "text": "Establishing a Baseline\nA Generic Optimizer\nMomentum\nRMSProp\nAdam\nDecoupled Weight Decay\nCallbacks\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#establishing-a-baseline",
    "href": "posts/fastai-book-notes/chapter-16/index.html#establishing-a-baseline",
    "title": "Notes on fastai Book Ch. 16",
    "section": "Establishing a Baseline",
    "text": "Establishing a Baseline\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\n\nURLs.IMAGENETTE_160\n'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz'\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\ndef get_learner(**kwargs):\n    return cnn_learner(dls, resnet34, pretrained=False,\n                    metrics=accuracy, **kwargs).to_fp16()\n\n# Default fastai optimizer\nlearn = get_learner()\nlearn.fit_one_cycle(3, 0.003)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.597476\n\n\n7.834398\n\n\n0.205096\n\n\n00:07\n\n\n\n\n1\n\n\n2.075519\n\n\n2.908207\n\n\n0.393121\n\n\n00:06\n\n\n\n\n2\n\n\n1.739347\n\n\n1.434073\n\n\n0.544713\n\n\n00:06\n\n\n\n\n\n\n\nlearn.opt_func\n&lt;function fastai.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)&gt;\n\n# Use plain SGD for baseline\nlearn = get_learner(opt_func=SGD)\n\nlearn.lr_find()\nSuggestedLRs(valley=0.004365158267319202)\n\n\n\n\n\n\n# Disable momentum for baseline\nlearn.fit_one_cycle(3, 0.03, moms=(0,0,0))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.968830\n\n\n2.996733\n\n\n0.235159\n\n\n00:06\n\n\n\n\n1\n\n\n2.414340\n\n\n1.804736\n\n\n0.403057\n\n\n00:06\n\n\n\n\n2\n\n\n2.157545\n\n\n1.676913\n\n\n0.430318\n\n\n00:06\n\n\n\n\n\n\nNote: Plain SGD is training slower."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#a-generic-optimizer",
    "href": "posts/fastai-book-notes/chapter-16/index.html#a-generic-optimizer",
    "title": "Notes on fastai Book Ch. 16",
    "section": "A Generic Optimizer",
    "text": "A Generic Optimizer\n\nneed a flexible optimizer foundation to easily implement new improvements\noptimizer callbacks: small pieces of code that we can compose, mix, and match in an optimizer to build the optimizer step\n\n\nOptimizer\nfastai.optimizer.Optimizer\n\nprint_source(Optimizer)\n    class Optimizer(_BaseOptimizer):\n        \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n        _keep_on_clear = ['force_train', 'do_wd']\n        def __init__(self, params, cbs, train_bn=True, **defaults):\n            params = L(params)\n            self.cbs,self.state,self.train_bn = L(cbs),defaultdict(dict),train_bn\n            defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n            self.param_lists = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n            self.hypers = L({} for _ in range_of(self.param_lists))\n            self.set_hypers(**defaults)\n            self.frozen_idx = 0\n    \n        def zero_grad(self):\n            for p,*_ in self.all_params(with_grad=True):\n                p.grad.detach_()\n                p.grad.zero_()\n    \n        def step(self):\n            for p,pg,state,hyper in self.all_params(with_grad=True):\n                for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))\n                self.state[p] = state\n    \n        def clear_state(self):\n            for p,pg,state,hyper in self.all_params():\n                self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n    \n        def state_dict(self):\n            state = [self.state[p] for p,*_ in self.all_params()]\n            return {'state': state, 'hypers': self.hypers}\n    \n        def load_state_dict(self, sd):\n            assert len(sd[\"hypers\"]) == len(self.param_lists)\n            assert len(sd[\"state\"])  == sum([len(pg) for pg in self.param_lists])\n            self.hypers = sd['hypers']\n            self.state = {p: s for p,s in zip(self.all_params().itemgot(0), sd['state'])}\n\n# Custom optimizer callback that performs a single SGD step\n# when Tensor.add_ in PyTorch is passed two parameters, they are multiplied together before the addition\ndef sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)\ntorch.add: \\(out_{i} = input_{i} + alpha \\times other_{i}\\)\nhelp(torch.add)\n    Help on built-in function add:\n    \n    add(...)\n        add(input, other, *, alpha=1, out=None) -&gt; Tensor\n        \n        Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.\n        \n        .. math::\n            \\text{{out}}_i = \\text{{input}}_i + \\text{{alpha}} \\times \\text{{other}}_i\n\n\n        \n        Supports :ref:`broadcasting to a common shape &lt;broadcasting-semantics&gt;`,\n        :ref:`type promotion &lt;type-promotion-doc&gt;`, and integer, float, and complex inputs.\n        \n        Args:\n            input (Tensor): the input tensor.\n            other (Tensor or Number): the tensor or number to add to input.\n        \n        Keyword arguments:\n            alpha (Number): the multiplier for :attr:`other`.\n            out (Tensor, optional): the output tensor.\n        \n        Examples::\n        \n            &gt;&gt;&gt; a = torch.randn(4)\n            &gt;&gt;&gt; a\n            tensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n            &gt;&gt;&gt; torch.add(a, 20)\n            tensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n        \n            &gt;&gt;&gt; b = torch.randn(4)\n            &gt;&gt;&gt; b\n            tensor([-0.9732, -0.3497,  0.6245,  0.4022])\n            &gt;&gt;&gt; c = torch.randn(4, 1)\n            &gt;&gt;&gt; c\n            tensor([[ 0.3743],\n                    [-1.7724],\n                    [-0.5811],\n                    [-0.8017]])\n            &gt;&gt;&gt; torch.add(b, c, alpha=10)\n            tensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n                    [-18.6971, -18.0736, -17.0994, -17.3216],\n                    [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n                    [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n\nopt_func = partial(Optimizer, cbs=[sgd_cb])\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit(3, 0.03)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.582404\n\n\n1.843338\n\n\n0.379618\n\n\n00:06\n\n\n\n\n1\n\n\n2.124360\n\n\n1.591867\n\n\n0.468280\n\n\n00:06\n\n\n\n\n2\n\n\n1.824502\n\n\n1.510853\n\n\n0.498089\n\n\n00:06"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#momentum",
    "href": "posts/fastai-book-notes/chapter-16/index.html#momentum",
    "title": "Notes on fastai Book Ch. 16",
    "section": "Momentum",
    "text": "Momentum\n\nuse a moving average, instead of only the current gradient\nused to skip over little bumps in the loss landscape\nhigher momentum will skip over bigger bumps\nworks particularly well if the loss function has narrow canyons that would cause vanilla SGD to bounce around\n\n\nweight.avg = beta * weight.avg + (1-beta) * weight.grad\nnew_weight = weight - lr * weight.avg\n\nbeta: defines how much momentum to use\n\nbeta = 0: no momentum\n\nneed to track the moving averages for each parameter in the model\n\n\nNoisy Data for a Single Parameter with Different Levels of Momentum\n# Get 100 evenly spaced input values over the interval [-4,4]\nx = np.linspace(-4, 4, 100)\n# Function to plot\ny = 1 - (x/3) ** 2\n# Add noise to both input and output\nx1 = x + np.random.randn(100) * 0.1\ny1 = y + np.random.randn(100) * 0.1\n_,axs = plt.subplots(2,2, figsize=(12,8))\n# List of beta values\nbetas = [0.5,0.7,0.9,0.99]\nidx = x1.argsort()\nfor beta,ax in zip(betas, axs.flatten()):\n    ax.scatter(x1,y1)\n    avg,res = 0,[]\n    for i in idx:\n        avg = beta * avg + (1-beta) * y1[i]\n        res.append(avg)#/(1-beta**(i+1)))\n    ax.plot(x1[idx],np.array(res), color='red');\n    ax.set_title(f'beta={beta}')\n\n\n\n\n\nNote: A beta value that is too high causes the overall changes in the gradient to be ignored.\n\nA beta value of 0.9 is often used for SGD with Momentum\nfit_one_cycle starts with a beta value of 0.95, gradually adjusts to 0.85, then gradually moves back to 0.95\n\n\nhelp(np.linspace)\n    Help on function linspace in module numpy:\n    \n    linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)\n        Return evenly spaced numbers over a specified interval.\n        \n        Returns `num` evenly spaced samples, calculated over the\n        interval [`start`, `stop`].\n        \n        The endpoint of the interval can optionally be excluded.\n        \n        .. versionchanged:: 1.16.0\n            Non-scalar `start` and `stop` are now supported.\n        \n        .. versionchanged:: 1.20.0\n            Values are rounded towards ``-inf`` instead of ``0`` when an\n            integer ``dtype`` is specified. The old behavior can\n            still be obtained with ``np.linspace(start, stop, num).astype(int)``\n        \n        Parameters\n        ----------\n        start : array_like\n            The starting value of the sequence.\n        stop : array_like\n            The end value of the sequence, unless `endpoint` is set to False.\n            In that case, the sequence consists of all but the last of ``num + 1``\n            evenly spaced samples, so that `stop` is excluded.  Note that the step\n            size changes when `endpoint` is False.\n        num : int, optional\n            Number of samples to generate. Default is 50. Must be non-negative.\n        endpoint : bool, optional\n            If True, `stop` is the last sample. Otherwise, it is not included.\n            Default is True.\n        retstep : bool, optional\n            If True, return (`samples`, `step`), where `step` is the spacing\n            between samples.\n        dtype : dtype, optional\n            The type of the output array.  If `dtype` is not given, the data type\n            is inferred from `start` and `stop`. The inferred dtype will never be\n            an integer; `float` is chosen even if the arguments would produce an\n            array of integers.\n        \n            .. versionadded:: 1.9.0\n        \n        axis : int, optional\n            The axis in the result to store the samples.  Relevant only if start\n            or stop are array-like.  By default (0), the samples will be along a\n            new axis inserted at the beginning. Use -1 to get an axis at the end.\n        \n            .. versionadded:: 1.16.0\n        \n        Returns\n        -------\n        samples : ndarray\n            There are `num` equally spaced samples in the closed interval\n            ``[start, stop]`` or the half-open interval ``[start, stop)``\n            (depending on whether `endpoint` is True or False).\n        step : float, optional\n            Only returned if `retstep` is True\n        \n            Size of spacing between samples.\n\n\n​        \n        See Also\n        --------\n        arange : Similar to `linspace`, but uses a step size (instead of the\n                 number of samples).\n        geomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n                    scale (a geometric progression).\n        logspace : Similar to `geomspace`, but with the end points specified as\n                   logarithms.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; np.linspace(2.0, 3.0, num=5)\n        array([2.  , 2.25, 2.5 , 2.75, 3.  ])\n        &gt;&gt;&gt; np.linspace(2.0, 3.0, num=5, endpoint=False)\n        array([2. ,  2.2,  2.4,  2.6,  2.8])\n        &gt;&gt;&gt; np.linspace(2.0, 3.0, num=5, retstep=True)\n        (array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n        \n        Graphical illustration:\n        \n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; N = 8\n        &gt;&gt;&gt; y = np.zeros(N)\n        &gt;&gt;&gt; x1 = np.linspace(0, 10, N, endpoint=True)\n        &gt;&gt;&gt; x2 = np.linspace(0, 10, N, endpoint=False)\n        &gt;&gt;&gt; plt.plot(x1, y, 'o')\n        [&lt;matplotlib.lines.Line2D object at 0x...&gt;]\n        &gt;&gt;&gt; plt.plot(x2, y + 0.5, 'o')\n        [&lt;matplotlib.lines.Line2D object at 0x...&gt;]\n        &gt;&gt;&gt; plt.ylim([-0.5, 1])\n        (-0.5, 1)\n        &gt;&gt;&gt; plt.show()\n\n# Custom callback to calculate the average gradient\ndef average_grad(p, mom, grad_avg=None, **kwargs):\n    # Set starting average to zero for all parameters\n    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n    return {'grad_avg': grad_avg*mom + p.grad.data}\n\n# Custom callback that performs a single SGD with momentum step\ndef momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg)\n\n# Start training with a beta value of 0.9\nopt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9)\n\n# Learner automatically schedules the momentum and learning rate\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.03)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.720397\n\n\n1.938127\n\n\n0.330446\n\n\n00:06\n\n\n\n\n1\n\n\n2.461116\n\n\n1.865412\n\n\n0.390828\n\n\n00:06\n\n\n\n\n2\n\n\n2.155982\n\n\n1.645647\n\n\n0.451720\n\n\n00:06\n\n\n\n\n\n\nNote: Still not getting great results\nlearn.recorder.plot_sched()"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#rmsprop",
    "href": "posts/fastai-book-notes/chapter-16/index.html#rmsprop",
    "title": "Notes on fastai Book Ch. 16",
    "section": "RMSProp",
    "text": "RMSProp\n\nIntroduced by Geoffrey Hinton in Overview of mini-batch gradient descent\n\nDivide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight\n\nUses an adaptive learning rate\n\neach parameter gets its own learning rate controlled by a global learning rate\n\nCan speed up training by giving a higher learning rate to weights that need to change a lot and a lower learning rate to weights that are good enough\nParameters whose gradients have been close to zero for a while will need a higher learning rate, because the loss is flat\nParameters whose gradients have been erratic will need a lower learning rate to avoid divergence\nUse a moving average of the gradients squared\n\n\n# If the moving average is low, the effective learning rate will be higher\nw.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2)\nnew_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)\n\neps (epsilon): added for numerial stability\n\ndefault value is 1e-8\n\nalpha: default value is usually 0.99\n\n\n# Custom callback that calculated the moving average of the gradients squared\ndef average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs):\n    # Set starting average to zero for all parameters\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    return {'sqr_avg': sqr_mom*sqr_avg + (1-sqr_mom)*p.grad.data**2}\n\n# Custom callback that perform a single rmsprop step\ndef rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n    denom = sqr_avg.sqrt().add_(eps)\n    p.data.addcdiv_(-lr, p.grad, denom)\n\ntorch.addcdiv: \\(\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}\\)\nhelp(torch.addcdiv)\n    Help on built-in function addcdiv:\n    \n    addcdiv(...)\n        addcdiv(input, tensor1, tensor2, *, value=1, out=None) -&gt; Tensor\n        \n        Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,\n        multiply the result by the scalar :attr:`value` and add it to :attr:`input`.\n        \n        .. warning::\n            Integer division with addcdiv is no longer supported, and in a future\n            release addcdiv will perform a true division of tensor1 and tensor2.\n            The historic addcdiv behavior can be implemented as\n            (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\n            for integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\n            The future addcdiv behavior is just the latter implementation:\n            (input + value * tensor1 / tensor2), for all dtypes.\n        \n        .. math::\n            \\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}\n\n\n​        \n        The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be\n        :ref:`broadcastable &lt;broadcasting-semantics&gt;`.\n        \n        For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be\n        a real number, otherwise an integer.\n        \n        Args:\n            input (Tensor): the tensor to be added\n            tensor1 (Tensor): the numerator tensor\n            tensor2 (Tensor): the denominator tensor\n        \n        Keyword args:\n            value (Number, optional): multiplier for :math:`\\text{tensor1} / \\text{tensor2}`\n            out (Tensor, optional): the output tensor.\n        \n        Example::\n        \n            &gt;&gt;&gt; t = torch.randn(1, 3)\n            &gt;&gt;&gt; t1 = torch.randn(3, 1)\n            &gt;&gt;&gt; t2 = torch.randn(1, 3)\n            &gt;&gt;&gt; torch.addcdiv(t, t1, t2, value=0.1)\n            tensor([[-0.2312, -3.6496,  0.1312],\n                    [-1.0428,  3.4292, -0.1030],\n                    [-0.5369, -0.9829,  0.0430]])\n\nopt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step], sqr_mom=0.99, eps=1e-7)\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.003)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.763567\n\n\n2.188202\n\n\n0.278981\n\n\n00:07\n\n\n\n\n1\n\n\n2.208868\n\n\n1.775339\n\n\n0.385223\n\n\n00:07\n\n\n\n\n2\n\n\n1.771404\n\n\n1.440212\n\n\n0.531720\n\n\n00:07\n\n\n\n\n\n\nNote: Higher final accuracy"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#adam",
    "href": "posts/fastai-book-notes/chapter-16/index.html#adam",
    "title": "Notes on fastai Book Ch. 16",
    "section": "Adam",
    "text": "Adam\n\nMixes the ideas of SGD with momentum and RMSProp together\nUses the moving average of the gradients as a direction and divides by the square root of the moving average of the gradients squared to give an adaptive learnig rate to each parameter\n\ntakes the unbiased moving average\n\n\nw.avg = beta * w.avg + (1-beta) * w.grad\nunbias_avg = w.avg / (1 - beta**(i+1))\ni: the i-th iteration, starting at 0 (1 - beta**(i+1): makes sure the unbiased average look more like the gradients at the beginning\n\nfull update step\n\n\nw.avg = beta * w.avg + (1-beta) * w.grad\nunbias_avg = w.avg / (1 - beta**(i+1))\nw.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\nnew_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)\n\nfastai default values\n\neps: 1e-5\n\nhigher eps value limits the maximum value of the adjusted learning rate\n\nbeta1: 0.9\nbeta2: 0.99\n\nset with moms in fit_one_cycle\n\n\n\n\nAdam\n&lt;function fastai.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)&gt;\n\nprint_source(Adam)\ndef Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True):\n    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n    cbs = [weight_decay] if decouple_wd else [l2_reg]\n    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n\nprint_source(step_stat)\ndef step_stat(p, step=0, **kwargs):\n    \"Register the number of steps done in `state` for `p`\"\n    step += 1\n    return {'step' : step}\n\nprint_source(adam_step)\ndef adam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n    \"Step for Adam with `lr` on `p`\"\n    debias1 = debias(mom,     1-mom,     step)\n    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n    p.data.addcdiv_(grad_avg, (sqr_avg/debias2).sqrt() + eps, value = -lr / debias1)\n    return p\n\nprint_source(debias)\ndef debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#decoupled-weight-decay",
    "href": "posts/fastai-book-notes/chapter-16/index.html#decoupled-weight-decay",
    "title": "Notes on fastai Book Ch. 16",
    "section": "Decoupled Weight Decay",
    "text": "Decoupled Weight Decay\n\nDecoupled Weight Decay Regularization\neach weight is decayed by a factor of lr * wd\n\n\nnew_weight = weight - lr*weight.grad - lr*wd*weight\n\nalso called L2 regularization\n\n\nprint_source(weight_decay)\ndef weight_decay(p, lr, wd, do_wd=True, **kwargs):\n    \"Weight decay as decaying `p` with `lr*wd`\"\n    if do_wd and wd!=0: p.data.mul_(1 - lr*wd)\n\nprint_source(l2_reg)\ndef l2_reg(p, lr, wd, do_wd=True, **kwargs):\n    \"L2 regularization as adding `wd*p` to `p.grad`\"\n    if do_wd and wd!=0: p.grad.data.add_(p.data, alpha=wd)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#callbacks",
    "href": "posts/fastai-book-notes/chapter-16/index.html#callbacks",
    "title": "Notes on fastai Book Ch. 16",
    "section": "Callbacks",
    "text": "Callbacks\n\nAllow users to insert code at any part of the training loop in a consistent, well-defined way\ncallback: a piece of code you write and inject into another piece of code at a predefined point\nThe typical approach to customizing the training loop by making a copy and inserting changes is problematic\n\nhundreds of changes can be made to a training loop, meaning their are billions of possible permutations\n\n\n\nBasic Training Loop\nfor xb,yb in dl:\n    loss = loss_func(model(xb),yb)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\nLearner._do_one_batch\n&lt;function fastai.learner.Learner._do_one_batch(self)&gt;\n\nprint_source(Learner._do_one_batch)\n    def _do_one_batch(self):\n        self.pred = self.model(*self.xb)\n        self('after_pred')\n        if len(self.yb):\n            self.loss_grad = self.loss_func(self.pred, *self.yb)\n            self.loss = self.loss_grad.clone()\n        self('after_loss')\n        if not self.training or not len(self.yb): return\n        self('before_backward')\n        self.loss_grad.backward()\n        self._with_events(self.opt.step, 'step', CancelStepException)\n        self.opt.zero_grad()\nNote: The self('') calls are where the callbacks are called. * The callback will receive the entire state of training and can modify it\n\n\nCreating a Callback\n\nafter_create: called after the Learner is created\nbefore_fit: called before starting training or inference, ideal for initial setup.\nbefore_epoch: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\nbefore_train: called at the beginning of the training part of an epoch.\nbefore_batch: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\nafter_pred: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss.\nafter_loss: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\nbefore_backward: called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)\nbefore_step: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\nafter_step: called after the step and before the gradients are zeroed.\nafter_batch: called at the end of a batch, for any clean-up before the next one.\nafter_train: called at the end of the training phase of an epoch.\nbefore_validate: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\nafter_validate: called at the end of the validation part of an epoch.\nafter_epoch: called at the end of an epoch, for any clean-up before the next one.\nafter_fit: called at the end of training, for final clean-up.\n\n\n# List of available events\nhelp(event)\n    Help on class event in module fastcore.basics:\n    \n    class event(builtins.object)\n     |  event(*args, **kwargs)\n     |  \n     |  All possible events as attributes to get tab-completion and typo-proofing\n     |  \n     |  Methods defined here:\n     |  \n     |  __eq__ = _eq(self, b)\n     |  \n     |  __init__ = _init(self, *args, **kwargs)\n     |  \n     |  __repr__ = _f(self)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  __hash__ = None\n     |  \n     |  after_batch = 'after_batch'\n     |  \n     |  after_cancel_batch = 'after_cancel_batch'\n     |  \n     |  after_cancel_epoch = 'after_cancel_epoch'\n     |  \n     |  after_cancel_fit = 'after_cancel_fit'\n     |  \n     |  after_cancel_step = 'after_cancel_step'\n     |  \n     |  after_cancel_train = 'after_cancel_train'\n     |  \n     |  after_cancel_validate = 'after_cancel_validate'\n     |  \n     |  after_create = 'after_create'\n     |  \n     |  after_epoch = 'after_epoch'\n     |  \n     |  after_fit = 'after_fit'\n     |  \n     |  after_loss = 'after_loss'\n     |  \n     |  after_pred = 'after_pred'\n     |  \n     |  after_step = 'after_step'\n     |  \n     |  after_train = 'after_train'\n     |  \n     |  after_validate = 'after_validate'\n     |  \n     |  before_backward = 'before_backward'\n     |  \n     |  before_batch = 'before_batch'\n     |  \n     |  before_epoch = 'before_epoch'\n     |  \n     |  before_fit = 'before_fit'\n     |  \n     |  before_step = 'before_step'\n     |  \n     |  before_train = 'before_train'\n     |  \n     |  before_validate = 'before_validate'\n\nModelResetter, print_source(ModelResetter)\n@docs\nclass ModelResetter(Callback):\n    \"`Callback` that resets the model at each validation/training step\"\n    def before_train(self):    self.model.reset()\n    def before_validate(self): self.model.reset()\n    def after_fit(self):       self.model.reset()\n    _docs = dict(before_train=\"Reset the model before training\",\n                 before_validate=\"Reset the model before validation\",\n                 after_fit=\"Reset the model after fitting\")\n(fastai.callback.rnn.ModelResetter, None)\n\nclass ModelResetter(Callback):\n    def begin_train(self):    self.model.reset()\n    def begin_validate(self): self.model.reset()\n\nRNNRegularizer, print_source(RNNRegularizer)\nclass RNNRegularizer(Callback):\n    \"Add AR and TAR regularization\"\n    order,run_valid = RNNCallback.order+1,False\n    def __init__(self, alpha=0., beta=0.): store_attr()\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha: self.learn.loss_grad += self.alpha * self.rnn.out.float().pow(2).mean()\n        if self.beta:\n            h = self.rnn.raw_out\n            if len(h)&gt;1: self.learn.loss_grad += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n(fastai.callback.rnn.RNNRegularizer, None)\n\nclass RNNRegularizer(Callback):\n    def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta\n\n    def after_pred(self):\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.learn.pred = self.pred[0]\n\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha != 0.:\n            self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean()\n        if self.beta != 0.:\n            h = self.raw_out[-1]\n            if len(h)&gt;1:\n                self.learn.loss += self.beta * (h[:,1:] - h[:,:-1]\n                                               ).float().pow(2).mean()\nNote: A callback will always try to get an attribute it does not have inside the Learner associated with it.\n\nLearner attributes available to callbacks\n\nmodel: the model used for training/validation\ndls: the underlying DataLoaders\nloss_func: the loss function used\nopt: the optimizer used to update the model parameters\nopt_func: the function used to create the optimizer\ncbs: the list containing all Callbacks\ndl: current DataLoader used for iteration\nx/xb: last input drawn from self.dl (potentially modified by callbacks). xb is always a tuple (potentially with one element) and x is detuplified. You can only assign to xb.\ny/yb: last target drawn from self.dl (potentially modified by callbacks). yb is always a tuple (potentially with one element) and y is detuplified. You can only assign to yb.\npred: last predictions from self.model (potentially modified by callbacks)\nloss_grad: last computed loss (potentially modified by callbacks)\nloss: clone of loss_grad used for logging\nn_epoch: the number of epochs in this training\nn_iter: the number of iterations in the current self.dl\nepoch: the current epoch index (from 0 to n_epoch-1)\niter: the current iteration index in self.dl (from 0 to n_iter-1)\n\n\n\n\nCallback Ordering and Exceptions\n\nA callback sometimes need to tell fastai to skip over a batch or an epoch, or stop training all together\n\n\nTerminateOnNaNCallback, print_source(TerminateOnNaNCallback)\nclass TerminateOnNaNCallback(Callback):\n    \"A `Callback` that terminates training if loss is NaN.\"\n    order=-9\n    def after_batch(self):\n        \"Test if `last_loss` is NaN and interrupts training.\"\n        if torch.isinf(self.loss) or torch.isnan(self.loss): raise CancelFitException\n(fastai.callback.tracker.TerminateOnNaNCallback, None)\n\nclass TerminateOnNaNCallback(Callback):\n    run_before=Recorder\n    def after_batch(self):\n        if torch.isinf(self.loss) or torch.isnan(self.loss):\n            raise CancelFitException\n\nfrom fastai.callback.core import _ex_docscs\n\n_ex_docs\n{'CancelBatchException': 'Skip the rest of this batch and go to `after_batch`',\n 'CancelTrainException': 'Skip the rest of the training part of the epoch and go to `after_train`',\n 'CancelValidException': 'Skip the rest of the validation part of the epoch and go to `after_validate`',\n 'CancelEpochException': 'Skip the rest of this epoch and go to `after_epoch`',\n 'CancelStepException': 'Skip stepping the optimizer',\n 'CancelFitException': 'Interrupts training and go to `after_fit`'}\nNote: Can Detect one the above exceptions and execute code right after * after_cancel_batch: reached immediately after a CancelBatchException before proceeding to after_batch * after_cancel_train: reached immediately after a CancelTrainException before proceeding to after_epoch * after_cancel_valid: reached immediately after a CancelValidException before proceeding to after_epoch * after_cancel_epoch: reached immediately after a CancelEpochException before proceeding to after_epoch * after_cancel_fit: reached immediately after a CancelFitException before proceeding to after_fit"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-16/index.html#references",
    "href": "posts/fastai-book-notes/chapter-16/index.html#references",
    "title": "Notes on fastai Book Ch. 16",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 15\nNext: Notes on fastai Book Ch. 17"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html",
    "href": "posts/fastai-book-notes/chapter-17/index.html",
    "title": "Notes on fastai Book Ch. 17",
    "section": "",
    "text": "A Neural Net from the Foundations\nBuilding a Neural Net Layer from Scratch\nThe Forward and Backward Passes\nConclusion\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html#a-neural-net-from-the-foundations",
    "href": "posts/fastai-book-notes/chapter-17/index.html#a-neural-net-from-the-foundations",
    "title": "Notes on fastai Book Ch. 17",
    "section": "A Neural Net from the Foundations",
    "text": "A Neural Net from the Foundations"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html#building-a-neural-net-layer-from-scratch",
    "href": "posts/fastai-book-notes/chapter-17/index.html#building-a-neural-net-layer-from-scratch",
    "title": "Notes on fastai Book Ch. 17",
    "section": "Building a Neural Net Layer from Scratch",
    "text": "Building a Neural Net Layer from Scratch\n\nModeling a Neuron\n\na neuron receives a given number of inputs and has an internal weight for each of them\nthe neuron sums the weighted inputs to produce an output and adds an inner bias\n\\(out = \\sum_{i=1}^{n}{x_{i}w_{i}+b}\\), where \\((x_{1},\\ldots,x_{n})\\) are inputs, \\((w_{1},\\ldots,w_{n})\\) are the weights, and \\(b\\) is the bias\n  output = sum([x*w for x,w in zip(inputs, weights)]) + bias\nthe output of the neuron is fed to a nonlinear function called an activation function\nthe output of the nonlinear activation function is fed as input to another neuron\nRectified Linear Unit (ReLU) activation function:\n  def relu(x): return x if x &gt;= 0 else 0\na deep learning model is build by stacking a lot of neurons in successive layers\na linear layer: all inputs are linked to each neuron in the layer\n\nneed to compute the dot product for each input and each neuron with a given weight\npython     sum([x*w for x,w in zip(input,weight)]\n\n\n\nThe output of a fully connected layer\n\n\\(y_{i,j} = \\sum_{k=1}^{n}{x_{i,k}w_{k,j}+b_{j}}\\)\n  y[i,j] = sum([a*b for a,b in zip(x[i,:],w[j,:])]) + b[j]\n  y = x @ w.t() + b\nx: a matrix containing the inputs with a size of batch_size by n_inputs\nw: a matrix containing the weights for the neurons with a size of n_neurons by n_inputs\nb: a vector containing the biases for the neurons with a size of n_neurons\ny: the output of the fully connected layer of size batch_size by n_neurons\n@: a matrix multiplication\nw.t(): the transpose matrix of w\n\n\n\n\nMatrix Multiplication from Scratch\n\nNeed three nested loops\n\nfor the row indices\nfor the column indices\nfor the inner sum\n\n\n\nimport torch\nfrom torch import tensor\n\ndef matmul(a,b):\n    # Get the number of rows and columns for the two matrices\n    ar,ac = a.shape\n    br,bc = b.shape\n    # The number of columns in the first matrix need to be\n    # the same as the number of rows in the second matrix\n    assert ac==br\n    # Initialize the output matrix\n    c = torch.zeros(ar, bc)\n    # For each row in the first matrix\n    for i in range(ar):\n        # For each column in the second matrix\n        for j in range(bc):\n            # For each column in the first matrix\n            # Element-wise multiplication\n            # Sum the products\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n\nUsing nested for-loops\n%time t1=matmul(m1, m2)\n    CPU times: user 329 ms, sys: 0 ns, total: 329 ms\n    Wall time: 328 ms\n\n%timeit -n 20 t1=matmul(m1, m2)\n    325 ms ± 801 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\nNote: Using loops is extremely inefficient!!! Avoid loops whenever possible.\n\n\nUsing PyTorch’s built-in matrix multiplication operator\n\nwritten in C++ to make it fast\nneed to vectorize operations on tensors to take advantage of speed of PyTorch\n\nuse element-wise arithmetic and broadcasting\n\n\n\n%time t2=m1@m2\n    CPU times: user 190 µs, sys: 0 ns, total: 190 µs\n    Wall time: 132 µs\n\n%timeit -n 20 t2=m1@m2\n    The slowest run took 9.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n    6.42 µs ± 8.4 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n\n\n\nElementwise Arithmetic\n\naddition: +\nsubtraction: -\nmultiplication: *\ndivision: /\ngreater than: &gt;\nless than: &lt;\nequal to: ==\n\nNote: Both tensors need to have the same shape to perform element-wise arithmetic.\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b\n    tensor([12., 14.,  3.])\n\na &lt; b\n    tensor([False,  True,  True])\n\nReduction Operators\n\nreturn tensors with only one element\nall: Tests if all elements evaluate to True.\nsum: Returns the sum of all elements in the tensor.\nmean: Returns the mean value of all elements in the tensor.\n\n\n# Check if every element in matrix a is less than the corresponding element in matrix b\n((a &lt; b).all(), \n # Check if every element in matrix a is equal to the corresponding element in matrix b\n (a==b).all())\n    (tensor(False), tensor(False))\n\n# Convert tensor to a plain python number or boolean\n(a + b).mean().item()\n    9.666666984558105\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m\n    tensor([[ 1.,  4.,  9.],\n            [16., 25., 36.],\n            [49., 64., 81.]])\n\n# Attempt to perform element-wise arithmetic on tensors with different shapes\nn = tensor([[1., 2, 3], [4,5,6]])\nm*n\n    ---------------------------------------------------------------------------\n    \n    RuntimeError                              Traceback (most recent call last)\n    \n    /tmp/ipykernel_38356/3763285369.py in &lt;module&gt;\n          1 n = tensor([[1., 2, 3], [4,5,6]])\n    ----&gt; 2 m*n\n\n\n    RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n# Replace the inner-most for-loop with element-wise arithmetic\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n\n%timeit -n 20 t3 = matmul(m1,m2)\n    488 µs ± 159 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\nNote: Just replacing one of the for loops with PyTorch element-wise arithmetic dramatically improved performance.\n\n\n\nBroadcasting\n\ndescribes how tensors of different ranks are treated during arithmetic operations\ngives specific rules to codify when shapes are compatible when trying to do an element-wise operation, and how the tensor of the smaller shape is expanded to match the tensor of bigger shape\n\n\nBroadcasting with a scalar\n\nthe scalar is “virtually” expanded to the same shape as the tensor where every element contains the original scalar value\n\n\na = tensor([10., 6, -4])\na &gt; 0\n    tensor([ True,  True, False])\nNote: Broadcasting with a scalar is useful when normalizing a dataset by subtracting the mean and dividing by the standard deviation.\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73\n    tensor([[-1.4652, -1.0989, -0.7326],\n            [-0.3663,  0.0000,  0.3663],\n            [ 0.7326,  1.0989,  1.4652]])\n\n\nBroadcasting a vector to a matrix\n\nthe vector is virtually expanded to the same shape as the tensor, by duplicating the rows/columns as needed\nPyTorch uses the expand_as method to expand the vector to the same size as the higher-rank tensor\n\ncreates a new view on the existing vector tensor without allocating new memory\n\nIt is only possible to broadcast a vector of size n by a matrix of size m by n.\n\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape\n    (torch.Size([3, 3]), torch.Size([3]))\n\nm + c\n    tensor([[11., 22., 33.],\n            [14., 25., 36.],\n            [17., 28., 39.]])\n\nc.expand_as(m)\n    tensor([[10., 20., 30.],\n            [10., 20., 30.],\n            [10., 20., 30.]])\n\n\nhelp(torch.Tensor.expand_as)\n    Help on method_descriptor:\n    \n    expand_as(...)\n        expand_as(other) -&gt; Tensor\n        \n        Expand this tensor to the same size as :attr:`other`.\n        ``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``.\n        \n        Please see :meth:`~Tensor.expand` for more information about ``expand``.\n        \n        Args:\n            other (:class:`torch.Tensor`): The result tensor has the same size\n                as :attr:`other`.\n\nhelp(torch.Tensor.expand)\n    Help on method_descriptor:\n    \n    expand(...)\n        expand(*sizes) -&gt; Tensor\n        \n        Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n        to a larger size.\n        \n        Passing -1 as the size for a dimension means not changing the size of\n        that dimension.\n        \n        Tensor can be also expanded to a larger number of dimensions, and the\n        new ones will be appended at the front. For the new dimensions, the\n        size cannot be set to -1.\n        \n        Expanding a tensor does not allocate new memory, but only creates a\n        new view on the existing tensor where a dimension of size one is\n        expanded to a larger size by setting the ``stride`` to 0. Any dimension\n        of size 1 can be expanded to an arbitrary value without allocating new\n        memory.\n        \n        Args:\n            *sizes (torch.Size or int...): the desired expanded size\n        \n        .. warning::\n        \n            More than one element of an expanded tensor may refer to a single\n            memory location. As a result, in-place operations (especially ones that\n            are vectorized) may result in incorrect behavior. If you need to write\n            to the tensors, please clone them first.\n        \n        Example::\n        \n            &gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])\n            &gt;&gt;&gt; x.size()\n            torch.Size([3, 1])\n            &gt;&gt;&gt; x.expand(3, 4)\n            tensor([[ 1,  1,  1,  1],\n                    [ 2,  2,  2,  2],\n                    [ 3,  3,  3,  3]])\n            &gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension\n            tensor([[ 1,  1,  1,  1],\n                    [ 2,  2,  2,  2],\n                    [ 3,  3,  3,  3]])\nNote: Expanding the vector does not increase the amount of data stored.\nt = c.expand_as(m)\nt.storage()\n     10.0\n     20.0\n     30.0\n    [torch.FloatStorage of size 3]\n\nhelp(torch.Tensor.storage)\n    Help on method_descriptor:\n    \n    storage(...)\n        storage() -&gt; torch.Storage\n        \n        Returns the underlying storage.\nNote: PyTorch accomplishes this by giving the new dimension a stride of 0 * When PyTorch looks for the next row by adding the stride, it will stay at the same row\n\nt.stride(), t.shape\n    ((0, 1), torch.Size([3, 3]))\n\nc + m\n    tensor([[11., 22., 33.],\n            [14., 25., 36.],\n            [17., 28., 39.]])\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n    tensor([[11., 22., 33.],\n            [14., 25., 36.]])\n\n# Attempt to broadcast a vector with an incompatible matrix\nc = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n\n    ---------------------------------------------------------------------------\n    \n    RuntimeError                              Traceback (most recent call last)\n    \n    /tmp/ipykernel_38356/3928136702.py in &lt;module&gt;\n          1 c = tensor([10.,20])\n          2 m = tensor([[1., 2, 3], [4,5,6]])\n    ----&gt; 3 c+m\n\n\n    RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n# Expand the vector to broadcast across a different dimension\nc = c.unsqueeze(1)\nm.shape,c.shape, c\n    (torch.Size([3, 3]),\n     torch.Size([3, 1]),\n     tensor([[10.],\n             [20.],\n             [30.]]))\n\nc.expand_as(m)\n    tensor([[10., 10., 10.],\n            [20., 20., 20.],\n            [30., 30., 30.]])\n\nc+m\n    tensor([[11., 12., 13.],\n            [24., 25., 26.],\n            [37., 38., 39.]])\n\nt = c.expand_as(m)\nt.storage()\n     10.0\n     20.0\n     30.0\n    [torch.FloatStorage of size 3]\n\nt.stride(), t.shape\n    ((1, 0), torch.Size([3, 3]))\nNote: By default, new broadcast dimensions are added at the beginning using c.unsqueeze(0) behind the scenes.\nc = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape\n    (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\nNote: The unsqueeze command can be replaced by None indexing.\nc.shape, c[None,:].shape,c[:,None].shape\n    (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\nNote: * You can omit training columns when indexing * ... means all preceding dimensions\n\nc[None].shape,c[...,None].shape\n    (torch.Size([1, 3]), torch.Size([3, 1]))\n\nc,c.unsqueeze(-1)\n    (tensor([10., 20., 30.]),\n     tensor([[10.],\n             [20.],\n             [30.]]))\n\n# Replace the second for loop with broadcasting\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n    return c\n\nm1.shape, m1.unsqueeze(-1).shape\n    (torch.Size([5, 784]), torch.Size([5, 784, 1]))\n\nm1[0].unsqueeze(-1).expand_as(m2).shape\n    torch.Size([784, 10])\n\n%timeit -n 20 t4 = matmul(m1,m2)\n    414 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\nNote: Even faster still, though the improvement is not as dramatic.\n\n\nBroadcasting rules\n\nwhen operating on two tensors, PyTorch compares their shapes element-wise\n\nstarts with the trailing dimensions and works with its way backward\nadds 1 when it meets and empty dimension\n\ntwo dimensions are compatible when one of the following is true\n\nThey are equal\nOne of them is 1, in which case that dimension is broadcast to make it the same as the other\n\narrays do not need to have the same number of dimensions\n\n\n\n\nEinstein Summation\n\na compact representation for combining products and sums in a general way\n\\(ik,kj \\rightarrow ij\\)\nlefthand side represents the operands dimensions, separated by commas\nrighthand side represents the result dimensions\na practical way of expressing operations involving indexing and sum of products\n\n\nNotaion Rules\n\nRepeated indices are implicitly summed over.\nEach index can appear at most twice in any term.\nEach term must contain identical nonrepeated indices.\n\n\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij', a, b)\n\n%timeit -n 20 t5 = matmul(m1,m2)\n    The slowest run took 10.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n    26.9 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\nNote: It is extremely fast even compared to the earlier broadcast implementation. * einsum is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA * still not as fast as carefully optimizes CUDA code\nAdditional Einsum Notation Examples\nx = torch.randn(2, 2)\nprint(x)\n# Transpose\ntorch.einsum('ij-&gt;ji', x)\n    tensor([[ 0.7541, -0.8633],\n            [ 2.2312,  0.0933]])\n\n\n\n\n\n    tensor([[ 0.7541,  2.2312],\n            [-0.8633,  0.0933]])\n\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\nz = torch.randn(2, 2)\nprint(x)\nprint(y)\nprint(z)\n# Return a vector of size b where the k-th coordinate is the sum of x[k,i] y[i,j] z[k,j] \ntorch.einsum('bi,ij,bj-&gt;b', x, y, z)\n    tensor([[-0.2458, -0.7571],\n            [ 0.0921,  0.5496]])\n    tensor([[-1.2792, -0.0648],\n            [-0.2263, -0.1153]])\n    tensor([[-0.2433,  0.4558],\n            [ 0.8155,  0.5406]])\n\n\n\n\n\n    tensor([-0.0711, -0.2349])\n\n# trace\nx = torch.randn(2, 2)\nx, torch.einsum('ii', x)\n    (tensor([[ 1.4828, -0.7057],\n             [-0.6288,  1.3791]]),\n     tensor(2.8619))\n\n# diagonal\nx = torch.randn(2, 2)\nx, torch.einsum('ii-&gt;i', x)\n    (tensor([[-1.0796,  1.1161],\n             [ 2.2944,  0.6225]]),\n     tensor([-1.0796,  0.6225]))\n\n# outer product\nx = torch.randn(3)\ny = torch.randn(2)\nf\"x: {x}\", f\"y: {y}\", torch.einsum('i,j-&gt;ij', x, y)\n    ('x: tensor([ 0.1439, -1.8456, -1.5355])',\n     'y: tensor([-0.7276, -0.5566])',\n     tensor([[-0.1047, -0.0801],\n             [ 1.3429,  1.0273],\n             [ 1.1172,  0.8547]]))\n\n# batch matrix multiplication\nAs = torch.randn(3,2,5)\nBs = torch.randn(3,5,4)\ntorch.einsum('bij,bjk-&gt;bik', As, Bs)\n    tensor([[[ 1.9657,  0.5904,  2.8094, -2.2607],\n             [ 0.7610, -2.0402,  0.7331, -2.2257]],\n    \n            [[-1.5433, -2.9716,  1.3589,  0.1664],\n             [ 2.7327,  4.4207, -1.1955,  0.5618]],\n    \n            [[-1.7859, -0.8143, -0.8410, -0.2257],\n             [-3.4942, -1.9947,  0.7098,  0.5964]]])\n\n# with sublist format and ellipsis\ntorch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n    tensor([[[ 1.9657,  0.5904,  2.8094, -2.2607],\n             [ 0.7610, -2.0402,  0.7331, -2.2257]],\n    \n            [[-1.5433, -2.9716,  1.3589,  0.1664],\n             [ 2.7327,  4.4207, -1.1955,  0.5618]],\n    \n            [[-1.7859, -0.8143, -0.8410, -0.2257],\n             [-3.4942, -1.9947,  0.7098,  0.5964]]])\n\n# batch permute\nA = torch.randn(2, 3, 4, 5)\ntorch.einsum('...ij-&gt;...ji', A).shape\n    torch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\nA = torch.randn(3,5,4)\nl = torch.randn(2,5)\nr = torch.randn(2,4)\ntorch.einsum('bn,anm,bm-&gt;ba', l, A, r)\n    tensor([[ 1.1410, -1.7888,  4.7315],\n            [ 3.8092,  3.0976,  2.2764]])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html#the-forward-and-backward-passes",
    "href": "posts/fastai-book-notes/chapter-17/index.html#the-forward-and-backward-passes",
    "title": "Notes on fastai Book Ch. 17",
    "section": "The Forward and Backward Passes",
    "text": "The Forward and Backward Passes\n\nDefining and Initializing a Layer\n# Linear layer\ndef lin(x, w, b): return x @ w + b\n\n# Initialize random input values\nx = torch.randn(200, 100)\n# Initialize random target values\ny = torch.randn(200)\n\n# Initialize layer 1 weights\nw1 = torch.randn(100,50)\n# Initialize layer 1 biases\nb1 = torch.zeros(50)\n# Initialize layer 2 weights\nw2 = torch.randn(50,1)\n# Initialize layer 2 biases\nb2 = torch.zeros(1)\n\n# Get a batch of hidden state\nl1 = lin(x, w1, b1)\nl1.shape\n    torch.Size([200, 50])\n\nl1.mean(), l1.std()\n    (tensor(-0.0385), tensor(10.0544))\nNote: Having with activations with a high standard deviation is a problem since the values can scale to numbers that can’t be represented by a computer by the end of the model.\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n    tensor([[nan, nan, nan, nan, nan],\n            [nan, nan, nan, nan, nan],\n            [nan, nan, nan, nan, nan],\n            [nan, nan, nan, nan, nan],\n            [nan, nan, nan, nan, nan]])\nNote: Having activations that are too small can cause all the activations at the end of the model to go to zero.\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n    tensor([[0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.]])\nNote: Need to scale the weight matrices so the standard deviation of the activations stays at 1 * Understanding the difficulty of training deep feedforward neural networks * the right scale for a given layer is \\(\\frac{1}{\\sqrt{n_{in}}}\\), where \\(n_{in}\"\\) represents the number of inputs.\nNote: For a layer with 100 inputs, \\(\\frac{1}{\\sqrt{100}}=0.1\\)\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n    tensor([[-1.7695,  0.5923,  0.3357, -0.7702, -0.8877],\n            [ 0.6093, -0.8594, -0.5679,  0.4050,  0.2279],\n            [ 0.4312,  0.0497,  0.1795,  0.3184, -1.7031],\n            [-0.7370,  0.0251, -0.8574,  0.6826,  2.0615],\n            [-0.2335,  0.0042, -0.1503, -0.2087, -0.0405]])\n\nx.std()\n    tensor(1.0150)\nNote: Even a slight variation from \\(0.1\\) will dramatically change the values\n# Redefine inputs and targets\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\nfrom math import sqrt\n# Scale the weights based on the number of inputs to the layers\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n    (tensor(-0.0062), tensor(1.0231))\n\n# Define non-linear activation function\ndef relu(x): return x.clamp_min(0.)\n\nl2 = relu(l1)\nl2.mean(),l2.std()\n    (tensor(0.3758), tensor(0.6150))\nNote: The activation function ruined the mean and standard deviation. * The \\(\\frac{1}{\\sqrt{n_{in}}}\\) weight initialization method used not account for the ReLU function.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n    tensor([[1.2172e-08, 0.0000e+00, 0.0000e+00, 7.1241e-09, 5.9308e-09],\n            [1.9647e-08, 0.0000e+00, 0.0000e+00, 9.2189e-09, 7.1026e-09],\n            [1.8266e-08, 0.0000e+00, 0.0000e+00, 1.1150e-08, 7.0774e-09],\n            [1.8673e-08, 0.0000e+00, 0.0000e+00, 1.0574e-08, 7.3020e-09],\n            [2.1829e-08, 0.0000e+00, 0.0000e+00, 1.1662e-08, 1.0466e-08]])\n\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n\nthe article that introduced ResNet\nIntroduced Kaiming initialization:\n\n\\(\\sqrt{\\frac{2}{n_{in}}}\\), where \\(n_{in}\\) is the number of inputs of our model\n\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\ntensor([[0.0000, 0.0000, 0.1001, 0.0358, 0.0000],\n        [0.0000, 0.0000, 0.1612, 0.0164, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.1764, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.1331, 0.0000, 0.0000]])\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\nw1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)\n\nl1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()\n(tensor(0.5720), tensor(0.8259))\n\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n\nout = model(x)\nout.shape\ntorch.Size([200, 1])\n\n# Squeeze the output to get rid of the trailing dimension\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\nloss = mse(out, y)\n\n\n\nGradients and the Backward Pass\n\nthe gradients are computed in the backward pass using the chain rule from calculus\nchain rule: \\((g \\circ f)'(x) = g'(f(x)) f'(x)\\)\nour loss if a big composition of different functions\n  loss = mse(out,y) = mse(lin(l2, w2, b2), y)\nchain rule: \\[\\frac{\\text{d} loss}{\\text{d} b_{2}} = \\frac{\\text{d} loss}{\\text{d} out} \\times \\frac{\\text{d} out}{\\text{d} b_{2}} = \\frac{\\text{d}}{\\text{d} out} mse(out, y) \\times \\frac{\\text{d}}{\\text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})\\]\nTo compute all the gradients we need for the update, we need to begin from the output of the model and work our way backward, one layer after the other.\nWe can automate this process by having each function we implemented provided its backward step\n\n\nGradient of the loss function\n\nundo the squeeze in the mse function\ncalculate the derivative of \\(x^{2}\\): \\(2x\\)\ncalculate the derivative of the mean: \\(\\frac{1}{n}\\) where \\(n\\) is the number of elements in the input\n\n\ndef mse_grad(inp, targ): \n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n\n\nGradient of the ReLU activation function\ndef relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp&gt;0).float() * out.g\n\n\nGradient of a linear layer\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = inp.t() @ out.g\n    b.g = out.g.sum(0)\n\n\n\nSymPy\n\na library for symbolic computation that is extremely useful when working with calculus\nSymbolic computation deals with the computation of mathematical objects symbolically\n\nthe mathematical objects are represented exactly, not approximately, and mathematical expressions with unevaluated variables are left in symbolic form\n\n\n\nfrom sympy import symbols,diff\nsx,sy = symbols('sx sy')\n# Calculate the derivative of sx**2\ndiff(sx**2, sx)\n\n2*sx\n\nDefine Forward and Backward Pass\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n    \n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n\n\n\nRefactoring the Model\n\ndefine classes for each function that include their own forward and backward pass functions\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n    \n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nmodel = Model(w1, b1, w2, b2)\n\nloss = model(x, y)\n\nmodel.backward()\n\n\nGoing to PyTorch\n# Define a base class for all functions in the model\nclass LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n\nclass Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g\n\nclass Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)\n\nclass Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): \n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\ntorch.autograd.Function\n\nIn PyTorch, each basic function we need to differentiate is written as a torch.autograd.Function that has a forward and backward method\n\n\nfrom torch.autograd import Function\nclass MyRelu(Function):\n    # Performs the operation\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n    \n    # Defines a formula for differentiating the operation with \n    # backward mode automatic differentiation\n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i&gt;0).float()\n\nhelp(staticmethod)\n    Help on class staticmethod in module builtins:\n    \n    class staticmethod(object)\n     |  staticmethod(function) -&gt; method\n     |  \n     |  Convert a function to be a static method.\n     |  \n     |  A static method does not receive an implicit first argument.\n     |  To declare a static method, use this idiom:\n     |  \n     |       class C:\n     |           @staticmethod\n     |           def f(arg1, arg2, ...):\n     |               ...\n     |  \n     |  It can be called either on the class (e.g. C.f()) or on an instance\n     |  (e.g. C().f()). Both the class and the instance are ignored, and\n     |  neither is passed implicitly as the first argument to the method.\n     |  \n     |  Static methods in Python are similar to those found in Java or C++.\n     |  For a more advanced concept, see the classmethod builtin.\n     |  \n     |  Methods defined here:\n     |  \n     |  __get__(self, instance, owner, /)\n     |      Return an attribute of instance, which is of type owner.\n     |  \n     |  __init__(self, /, *args, **kwargs)\n     |      Initialize self.  See help(type(self)) for accurate signature.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Static methods defined here:\n     |  \n     |  __new__(*args, **kwargs) from builtins.type\n     |      Create and return a new object.  See help(type) for accurate signature.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |  \n     |  __func__\n     |  \n     |  __isabstractmethod__\n\n\ntorch.nn.Module\n\nthe base structure for all models in PyTorch\n\nImplementation Steps 1. Make sure the superclass __init__ is called first when you initialize it. 2. Define any parameters of the model as attributes with nn.Parameter. 3. Define a forward function that returns the output of your model.\n\nimport torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n    \n    def forward(self, x): return x @ self.weight.t() + self.bias\n\nlin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape\n(torch.Size([2, 10]), torch.Size([2]))\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\nNote: fsatai provides its own variant of Module that is identical to nn.Module, but automatically calls super().__init__().\nclass Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html#conclusion",
    "href": "posts/fastai-book-notes/chapter-17/index.html#conclusion",
    "title": "Notes on fastai Book Ch. 17",
    "section": "Conclusion",
    "text": "Conclusion\n\nA neural net is a bunch of matrix multiplications with nonlinearities in between\nVectorize and take advantage of techniques such as element-wise arithmetic and broadcasting when possible\nTwo tensors are broadcastable if the dimensions starting from the end and going backward match\n\nMay need to add dimensions of size one to make tensors broadcastable\n\nProperly initializing a neural net is crucial to get training started\n\nUse Kaiming initialization when using ReLU\n\nThe backward pass is the chain rule applied multiple times, computing the gradient from the model output and going back, one layer at a time"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-17/index.html#references",
    "href": "posts/fastai-book-notes/chapter-17/index.html#references",
    "title": "Notes on fastai Book Ch. 17",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 16\nNext: Notes on fastai Book Ch. 18"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-18/index.html",
    "href": "posts/fastai-book-notes/chapter-18/index.html",
    "title": "Notes on fastai Book Ch. 18",
    "section": "",
    "text": "CNN Interpretation with CAM\nCAM and Hooks\nGradient CAM\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-18/index.html#cnn-interpretation-with-cam",
    "href": "posts/fastai-book-notes/chapter-18/index.html#cnn-interpretation-with-cam",
    "title": "Notes on fastai Book Ch. 18",
    "section": "CNN Interpretation with CAM",
    "text": "CNN Interpretation with CAM"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-18/index.html#cam-and-hooks",
    "href": "posts/fastai-book-notes/chapter-18/index.html#cam-and-hooks",
    "title": "Notes on fastai Book Ch. 18",
    "section": "CAM and Hooks",
    "text": "CAM and Hooks\n\nClass Activation Map (CAM)\n\nLearning Deep Features for Discriminative Localization\nuses the output of the last convolutional layer together with the predictions to generate a heatmap visualization of why the model made its decision\nat each position of our final convolutional layer, we have as many filters as in the last linear layer\n\nwe can compute the dot product of the activations with the final weights to get the score of the feature that was used to make a decision for each location in our feature map\n\nneed a way to get access to the activations inside the model while it is training\n\ncan use PyTorch hooks\n\n\n\n\nPyTorch Hooks\n\nPyTorch’s equivalent of fastai’s callbacks\nallow you to inject code into the forward and backward calculations themselves\ncan attach a hook to any layer of the model, and it will be executed when we compute the outputs (forward hook) or during backpropogation (backward hook)\nforward hook:\n\na function that takes a module, its input, and its output\ncan perform any behavior you want\n\n\n\nHookCallback\n    fastai.callback.hook.HookCallback\n\nprint_source(HookCallback)\n    @funcs_kwargs\n    class HookCallback(Callback):\n        \"`Callback` that can be used to register hooks on `modules`\"\n        _methods = [\"hook\"]\n        hook = noops\n        def __init__(self, modules=None, every=None, remove_end=True, is_forward=True, detach=True, cpu=True, include_paramless=False , **kwargs):\n            store_attr('modules,every,remove_end,is_forward,detach,cpu, include_paramless')\n            assert not kwargs\n    \n        def before_fit(self):\n            \"Register the `Hooks` on `self.modules`.\"\n            if self.modules is None: self.modules = [m for m in flatten_model(self.model) if self.include_paramless or has_params(m)]\n            if self.every is None: self._register()\n    \n        def before_batch(self):\n            if self.every is None: return\n            if self.training and self.train_iter%self.every==0: self._register()\n    \n        def after_batch(self):\n            if self.every is None: return\n            if self.training and self.train_iter%self.every==0: self._remove()\n    \n        def after_fit(self):\n            \"Remove the `Hooks`.\"\n            if self.remove_end: self._remove()\n    \n        def _register(self): self.hooks = Hooks(self.modules, self.hook, self.is_forward, self.detach, self.cpu)\n        def _remove(self):\n            if getattr(self, 'hooks', None): self.hooks.remove()\n    \n        def __del__(self): self._remove()\n\n\npath = untar_data(URLs.PETS)/'images'\npath\n    Path('/home/innom-dt/.fastai/data/oxford-iiit-pet/images')\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\n# Train a cat classifier with the default settings\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.127009\n\n\n0.042651\n\n\n0.011502\n\n\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.045776\n\n\n0.023368\n\n\n0.007442\n\n\n00:19\n\n\n\n\n\n\n\nprint_source(image_cat)\n    def image_cat (): return BytesIO(pkgutil.get_data('fastbook', 'images/cat.jpg'))\n\n# Load an image of a cat\nimg = PILImage.create(image_cat())\n# Get a batch of data from the test set\nx, = first(dls.test_dl([img]))\n\n# Define a hook that stores a copy of the output\nclass Hook():\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n\n# Instantiate a hook\nhook_output = Hook()\n# Attach the hook to the last layer in the model\nhook = learn.model[0].register_forward_hook(hook_output.hook_func)\n\n# Perform inference on the test batch\nwith torch.no_grad(): output = learn.model.eval()(x)\n\n# Get the copy of the model activations from the hook\nact = hook_output.stored[0]\nact.shape\n    torch.Size([512, 7, 7])\n\n# Check the model predictions\nF.softmax(output, dim=-1)\n    TensorBase([[4.0876e-09, 1.0000e+00]], device='cuda:0')\nNote: The model is confident the image contains a cat.\ndls.vocab\n    [False, True]\n\nx.shape\n    torch.Size([1, 3, 224, 224])\n\nlearn.model[1]\n    Sequential(\n      (0): AdaptiveConcatPool2d(\n        (ap): AdaptiveAvgPool2d(output_size=1)\n        (mp): AdaptiveMaxPool2d(output_size=1)\n      )\n      (1): Flatten(full=False)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Dropout(p=0.25, inplace=False)\n      (4): Linear(in_features=1024, out_features=512, bias=False)\n      (5): ReLU(inplace=True)\n      (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (7): Dropout(p=0.5, inplace=False)\n      (8): Linear(in_features=512, out_features=2, bias=False)\n    )\n\nlearn.model[1][-1]\n    Linear(in_features=512, out_features=2, bias=False)\n\n# Calculate the dot product the weight matrix for the last layer with the activations\ncam_map = torch.einsum('ck,kij-&gt;cij', learn.model[1][-1].weight, act)\ncam_map.shape\n    torch.Size([2, 7, 7])\nNote: The result of the dot product is a \\(7x7\\) feature map that shows where the activations were higher and lower.\n# Get the first image from the test batch\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\n# Overlay the feature map for the cat class on top of the cat image\nax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\nNote: The bright yellow spots correspond to high activations. * The head and front paw seem to have had the most influence on the model’s prediction.\n\n# Remove the hook to avoid memory leaks\nhook.remove()\n\nprint_source(hook.remove)\n        def remove(self) -&gt; None:\n            hooks_dict = self.hooks_dict_ref()\n            if hooks_dict is not None and self.id in hooks_dict:\n                del hooks_dict[self.id]\n\n# Update the custom hook to be a context manager\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)   \n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    # Automatically register the hook when entering it\n    def __enter__(self, *args): return self\n    # Automatically remove the hook when exiting it\n    def __exit__(self, *args): self.hook.remove()\n\n# Pass the model to the `__enter__()` method for the Hook\nwith Hook(learn.model[0]) as hook:\n    with torch.no_grad(): output = learn.model.eval()(x.cuda())\n    act = hook.stored\nNote: This method only works for the last layer."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-18/index.html#gradient-cam",
    "href": "posts/fastai-book-notes/chapter-18/index.html#gradient-cam",
    "title": "Notes on fastai Book Ch. 18",
    "section": "Gradient CAM",
    "text": "Gradient CAM\n\nGrad-CAM: Why did you say that?\n\nuses the final activation for the desired class\ncan use Grad-CAM on any layer\nrecall the gradients of the output of the last layers with respect to the input of that layer are equal to the layer weights\n\n\n\n# Define a hook that stores a copy of the gradients calculated by PyTorch during the backward pass\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)   \n    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n# The class index for the cat class\ncls = 1\nwith HookBwd(learn.model[0]) as hookg:\n    with Hook(learn.model[0]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    # Get the gradients for the cat class for the first image in the test set\n    output[0,cls].backward()\n    grad = hookg.stored\n\n# Calculate the average of the gradients across the feature map\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\n\n# Get the gradients for the output of the second to last ResNet group\nwith HookBwd(learn.model[0][-2]) as hookg:\n    with Hook(learn.model[0][-2]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\n\nprint_source(ActivationStats)\n    @delegates()\n    class ActivationStats(HookCallback):\n        \"Callback that record the mean and std of activations.\"\n        order=-20\n        def __init__(self, with_hist=False, **kwargs):\n            super().__init__(**kwargs)\n            self.with_hist = with_hist\n    \n        def before_fit(self):\n            \"Initialize stats.\"\n            super().before_fit()\n            self.stats = L()\n    \n        def hook(self, m, i, o):\n            if isinstance(o, tuple): return self.hook_multi_ouput(o)\n            o = o.float()\n            res = {'mean': o.mean().item(), 'std': o.std().item(),\n                   'near_zero': (o&lt;=0.05).long().sum().item()/o.numel()}\n            if self.with_hist: res['hist'] = o.histc(40,0,10)\n            return res\n    \n        def hook_multi_ouput(self,o_tuple):\n            \"For outputs of RNN which are [nested] tuples of tensors\"\n            res = []\n            for o in self._flatten_tuple(o_tuple):\n                if not(isinstance(o, Tensor)): continue\n                res.append(self.hook(None, None, o))\n            return res\n    \n        def _flatten_tuple(self, o_tuple):\n            \"Recursively flatten a [nested] tuple\"\n            res = []\n            for it in o_tuple:\n                if isinstance(it, tuple): res += self._flatten_tuple(it)\n                else: res += [it]\n            return tuple(res)\n    \n        def after_batch(self):\n            \"Take the stored results and puts it in `self.stats`\"\n            if self.training and (self.every is None or self.train_iter%self.every == 0):\n                self.stats.append(self.hooks.stored)\n            super().after_batch()\n    \n        def layer_stats(self, idx):\n            lstats = self.stats.itemgot(idx)\n            return L(lstats.itemgot(o) for o in ('mean','std','near_zero'))\n    \n        def hist(self, idx):\n            res = self.stats.itemgot(idx).itemgot('hist')\n            return torch.stack(tuple(res)).t().float().log1p()\n    \n        def color_dim(self, idx, figsize=(10,5), ax=None):\n            \"The 'colorful dimension' plot\"\n            res = self.hist(idx)\n            if ax is None: ax = subplots(figsize=figsize)[1][0]\n            ax.imshow(res, origin='lower')\n            ax.axis('off')\n    \n        def plot_layer_stats(self, idx):\n            _,axs = subplots(1, 3, figsize=(12,3))\n            for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n                ax.plot(o)\n                ax.set_title(title)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-18/index.html#references",
    "href": "posts/fastai-book-notes/chapter-18/index.html#references",
    "title": "Notes on fastai Book Ch. 18",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 17\nNext: Notes on fastai Book Ch. 19"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html",
    "href": "posts/fastai-book-notes/chapter-19/index.html",
    "title": "Notes on fastai Book Ch. 19",
    "section": "",
    "text": "A fastai Learner from Scratch\nData\nModule and Parameter\nLoss\nLearner\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#a-fastai-learner-from-scratch",
    "href": "posts/fastai-book-notes/chapter-19/index.html#a-fastai-learner-from-scratch",
    "title": "Notes on fastai Book Ch. 19",
    "section": "A fastai Learner from Scratch",
    "text": "A fastai Learner from Scratch"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#data",
    "href": "posts/fastai-book-notes/chapter-19/index.html#data",
    "title": "Notes on fastai Book Ch. 19",
    "section": "Data",
    "text": "Data\nprint_source(untar_data)\n    def untar_data(url, archive=None, data=None, c_key='data', force_download=False):#, extract_func=file_extract, timeout=4):\n        \"Download `url` to `fname` if `dest` doesn't exist, and extract to folder `dest`\"\n        d = FastDownload(fastai_cfg(), module=fastai.data, archive=archive, data=data, base='~/.fastai')\n        return d.get(url, force=force_download, extract_key=c_key)\n\nprint_source(fastai_cfg)\n    @lru_cache(maxsize=None)\n    def fastai_cfg():\n        \"`Config` object for fastai's `config.ini`\"\n        return Config(Path(os.getenv('FASTAI_HOME', '~/.fastai')), 'config.ini', create=dict(\n            data = 'data', archive = 'archive', storage = 'tmp', model = 'models'))\n\nfastdownload library\n\nDocumentation\neasily download, verify, and extract archives\n\n\nfrom fastdownload import FastDownload\n\nprint_source(FastDownload)\n    class FastDownload:\n        def __init__(self, cfg=None, base='~/.fastdownload', archive=None, data=None, module=None):\n            base = Path(base).expanduser().absolute()\n            default = {'data':(data or 'data'), 'archive':(archive or 'archive')}\n            self.cfg = Config(base, 'config.ini', create=default) if cfg is None else cfg\n            self.module = checks_module(module)\n            if data is not None: self.cfg['data'] = data\n            if archive is not None: self.cfg['archive'] = archive\n    \n        def arch_path(self):\n            \"Path to archives\"\n            return self.cfg.path('archive')\n    \n        def data_path(self, extract_key='data', arch=None):\n            \"Path to extracted data\"\n            path = self.cfg.path(extract_key)\n            return path if arch is None else path/remove_suffix(arch.stem, '.tar')\n    \n        def check(self, url, fpath):\n            \"Check whether size and hash of `fpath` matches stored data for `url` or data is missing\"\n            checks = read_checks(self.module).get(url)\n            return not checks or path_stats(fpath)==checks\n    \n        def download(self, url, force=False):\n            \"Download `url` to archive path, unless exists and `self.check` fails and not `force`\"\n            self.arch_path().mkdir(exist_ok=True, parents=True)\n            return download_and_check(url, urldest(url, self.arch_path()), self.module, force)\n    \n        def rm(self, url, rm_arch=True, rm_data=True, extract_key='data'):\n            \"Delete downloaded archive and extracted data for `url`\"\n            arch = urldest(url, self.arch_path())\n            if rm_arch: arch.delete()\n            if rm_data: self.data_path(extract_key, arch).delete()\n    \n        def update(self, url):\n            \"Store the hash and size in `download_checks.py`\"\n            update_checks(urldest(url, self.arch_path()), url, self.module)\n    \n        def extract(self, url, extract_key='data', force=False):\n            \"Extract archive already downloaded from `url`, overwriting existing if `force`\"\n            arch = urldest(url, self.arch_path())\n            if not arch.exists(): raise Exception(f'{arch} does not exist')\n            dest = self.data_path(extract_key)\n            dest.mkdir(exist_ok=True, parents=True)\n            return untar_dir(arch, dest, rename=True, overwrite=force)\n    \n        def get(self, url, extract_key='data', force=False):\n            \"Download and extract `url`, overwriting existing if `force`\"\n            if not force:\n                data = self.data_path(extract_key, urldest(url, self.arch_path()))\n                if data.exists(): return data\n            self.download(url, force=force)\n            return self.extract(url, extract_key=extract_key, force=force)\n\nimport fastdownload\n\nprint_source(fastdownload.download_and_check)\n    def download_and_check(url, fpath, fmod, force):\n        \"Download `url` to `fpath`, unless exists and `check` fails and not `force`\"\n        if not force and fpath.exists():\n            if check(fmod, url, fpath): return fpath\n            else: print(\"Downloading a new version of this dataset...\")\n        res = download_url(url, fpath)\n        if not check(fmod, url, fpath): raise Exception(\"Downloaded file is corrupt or not latest version\")\n        return res\n\n\nprint_source(fastdownload.download_url)\n    def download_url(url, dest=None, timeout=None, show_progress=True):\n        \"Download `url` to `dest` and show progress\"\n        pbar = progress_bar([])\n        def progress(count=1, bsize=1, tsize=None):\n            pbar.total = tsize\n            pbar.update(count*bsize)\n        return urlsave(url, dest, reporthook=progress if show_progress else None, timeout=timeout)\n\nurlsave\n    &lt;function fastcore.net.urlsave(url, dest=None, reporthook=None, timeout=None)&gt;\n\nprint_source(urlsave)\n    def urlsave(url, dest=None, reporthook=None, timeout=None):\n        \"Retrieve `url` and save based on its name\"\n        dest = urldest(url, dest)\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        nm,msg = urlretrieve(url, dest, reporthook, timeout=timeout)\n        return nm\n\nurlretrieve\n    &lt;function fastcore.net.urlretrieve(url, filename=None, reporthook=None, data=None, timeout=None)&gt;\n\nprint_source(urlretrieve)\n    def urlretrieve(url, filename=None, reporthook=None, data=None, timeout=None):\n        \"Same as `urllib.request.urlretrieve` but also works with `Request` objects\"\n        with contextlib.closing(urlopen(url, data, timeout=timeout)) as fp:\n            headers = fp.info()\n            if filename: tfp = open(filename, 'wb')\n            else:\n                tfp = tempfile.NamedTemporaryFile(delete=False)\n                filename = tfp.name\n    \n            with tfp:\n                bs,size,read,blocknum = 1024*8,-1,0,0\n                if \"content-length\" in headers: size = int(headers[\"Content-Length\"])\n                if reporthook: reporthook(blocknum, bs, size)\n                while True:\n                    block = fp.read(bs)\n                    if not block: break\n                    read += len(block)\n                    tfp.write(block)\n                    blocknum += 1\n                    if reporthook: reporthook(blocknum, bs, size)\n    \n        if size &gt;= 0 and read &lt; size:\n            raise ContentTooShortError(f\"retrieval incomplete: got only {read} out of {size} bytes\", headers)\n        return filename,headers\n\nimport contextlib\n\nprint_source(contextlib.closing)\n    class closing(AbstractContextManager):\n        \"\"\"Context to automatically close something at the end of a block.\n    \n        Code like this:\n    \n            with closing(&lt;module&gt;.open(&lt;arguments&gt;)) as f:\n                &lt;block&gt;\n    \n        is equivalent to this:\n    \n            f = &lt;module&gt;.open(&lt;arguments&gt;)\n            try:\n                &lt;block&gt;\n            finally:\n                f.close()\n    \n        \"\"\"\n        def __init__(self, thing):\n            self.thing = thing\n        def __enter__(self):\n            return self.thing\n        def __exit__(self, *exc_info):\n            self.thing.close()\n\npath = untar_data(URLs.IMAGENETTE_160)\npath\n    Path('/home/innom-dt/.fastai/data/imagenette2-160')\n\nt = get_image_files(path)\nt[0]\n    Path('/home/innom-dt/.fastai/data/imagenette2-160/val/n03028079/n03028079_2470.JPEG')\n\nprint_source(get_image_files)\n    def get_image_files(path, recurse=True, folders=None):\n        \"Get image files in `path` recursively, only in `folders`, if specified.\"\n        return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)\n\nimage_extensions\n    {'.art',\n     '.bmp',\n     '.cdr',\n     '.cdt',\n     '.cpt',\n     '.cr2',\n     '.crw',\n     '.djv',\n     '.djvu',\n     '.erf',\n     '.gif',\n     '.ico',\n     '.ief',\n     '.jng',\n     '.jp2',\n     '.jpe',\n     '.jpeg',\n     '.jpf',\n     '.jpg',\n     '.jpg2',\n     '.jpm',\n     '.jpx',\n     '.nef',\n     '.orf',\n     '.pat',\n     '.pbm',\n     '.pcx',\n     '.pgm',\n     '.png',\n     '.pnm',\n     '.ppm',\n     '.psd',\n     '.ras',\n     '.rgb',\n     '.svg',\n     '.svgz',\n     '.tif',\n     '.tiff',\n     '.wbmp',\n     '.xbm',\n     '.xpm',\n     '.xwd'}\n\nprint_source(get_files)\n    def get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n        \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n        path = Path(path)\n        folders=L(folders)\n        extensions = setify(extensions)\n        extensions = {e.lower() for e in extensions}\n        if recurse:\n            res = []\n            for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n                if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n                else:                         d[:] = [o for o in d if not o.startswith('.')]\n                if len(folders) !=0 and i==0 and '.' not in folders: continue\n                res += _get_files(p, f, extensions)\n        else:\n            f = [o.name for o in os.scandir(path) if o.is_file()]\n            res = _get_files(path, f, extensions)\n        return L(res)\n\nhelp(os.walk)\n    Help on function walk in module os:\n    \n    walk(top, topdown=True, onerror=None, followlinks=False)\n        Directory tree generator.\n        \n        For each directory in the directory tree rooted at top (including top\n        itself, but excluding '.' and '..'), yields a 3-tuple\n        \n            dirpath, dirnames, filenames\n        \n        dirpath is a string, the path to the directory.  dirnames is a list of\n        the names of the subdirectories in dirpath (excluding '.' and '..').\n        filenames is a list of the names of the non-directory files in dirpath.\n        Note that the names in the lists are just names, with no path components.\n        To get a full path (which begins with top) to a file or directory in\n        dirpath, do os.path.join(dirpath, name).\n        \n        If optional arg 'topdown' is true or not specified, the triple for a\n        directory is generated before the triples for any of its subdirectories\n        (directories are generated top down).  If topdown is false, the triple\n        for a directory is generated after the triples for all of its\n        subdirectories (directories are generated bottom up).\n        \n        When topdown is true, the caller can modify the dirnames list in-place\n        (e.g., via del or slice assignment), and walk will only recurse into the\n        subdirectories whose names remain in dirnames; this can be used to prune the\n        search, or to impose a specific order of visiting.  Modifying dirnames when\n        topdown is false has no effect on the behavior of os.walk(), since the\n        directories in dirnames have already been generated by the time dirnames\n        itself is generated. No matter the value of topdown, the list of\n        subdirectories is retrieved before the tuples for the directory and its\n        subdirectories are generated.\n        \n        By default errors from the os.scandir() call are ignored.  If\n        optional arg 'onerror' is specified, it should be a function; it\n        will be called with one argument, an OSError instance.  It can\n        report the error to continue with the walk, or raise the exception\n        to abort the walk.  Note that the filename is available as the\n        filename attribute of the exception object.\n        \n        By default, os.walk does not follow symbolic links to subdirectories on\n        systems that support them.  In order to get this functionality, set the\n        optional argument 'followlinks' to true.\n        \n        Caution:  if you pass a relative pathname for top, don't change the\n        current working directory between resumptions of walk.  walk never\n        changes the current directory, and assumes that the client doesn't\n        either.\n        \n        Example:\n        \n        import os\n        from os.path import join, getsize\n        for root, dirs, files in os.walk('python/Lib/email'):\n            print(root, \"consumes\", end=\"\")\n            print(sum(getsize(join(root, name)) for name in files), end=\"\")\n            print(\"bytes in\", len(files), \"non-directory files\")\n            if 'CVS' in dirs:\n                dirs.remove('CVS')  # don't visit CVS directories\nNote: os.walk() is faster than glob.\nsetify\n    &lt;function fastcore.basics.setify(o)&gt;\n\nprint_source(setify)\n    def setify(o):\n        \"Turn any list like-object into a set.\"\n        return o if isinstance(o,set) else set(listify(o))\n\nfrom glob import glob\n\nhelp(glob)\n    Help on function glob in module glob:\n    \n    glob(pathname, *, recursive=False)\n        Return a list of paths matching a pathname pattern.\n        \n        The pattern may contain simple shell-style wildcards a la\n        fnmatch. However, unlike fnmatch, filenames starting with a\n        dot are special cases that are not matched by '*' and '?'\n        patterns.\n        \n        If recursive is true, the pattern '**' will match any files and\n        zero or more directories and subdirectories.\n\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n    Path('/home/innom-dt/.fastai/data/imagenette2-160/val/n03028079/n03028079_2470.JPEG')\n\nim = Image.open(files[0])\nim\n\n\n\n\n\n\nhelp(Image.open)\n    Help on function open in module PIL.Image:\n    \n    open(fp, mode='r', formats=None)\n        Opens and identifies the given image file.\n        \n        This is a lazy operation; this function identifies the file, but\n        the file remains open and the actual image data is not read from\n        the file until you try to process the data (or call the\n        :py:meth:`~PIL.Image.Image.load` method).  See\n        :py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n        \n        :param fp: A filename (string), pathlib.Path object or a file object.\n           The file object must implement ``file.read``,\n           ``file.seek``, and ``file.tell`` methods,\n           and be opened in binary mode.\n        :param mode: The mode.  If given, this argument must be \"r\".\n        :param formats: A list or tuple of formats to attempt to load the file in.\n           This can be used to restrict the set of formats checked.\n           Pass ``None`` to try all supported formats. You can print the set of\n           available formats by running ``python3 -m PIL`` or using\n           the :py:func:`PIL.features.pilinfo` function.\n        :returns: An :py:class:`~PIL.Image.Image` object.\n        :exception FileNotFoundError: If the file cannot be found.\n        :exception PIL.UnidentifiedImageError: If the image cannot be opened and\n           identified.\n        :exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n           instance is used for ``fp``.\n        :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\n\nim_t = tensor(im)\nim_t.shape\n    torch.Size([213, 160, 3])\n\nhelp(Path.parent)\n    Help on property:\n    \n        The logical parent of the path.\n\nlbls = files.map(Self.parent.name()).unique(); lbls\n    (#10) ['n03028079','n03445777','n03417042','n02102040','n03425413','n03000684','n01440764','n03394916','n03888257','n02979186']\n\n!ls $path/'train'\n    n01440764  n02979186  n03028079  n03417042  n03445777\n    n02102040  n03000684  n03394916  n03425413  n03888257\n\nval2idx\n    &lt;function fastcore.basics.val2idx(x)&gt;\n\nprint_source(val2idx)\n    def val2idx(x):\n        \"Dict from value to index\"\n        return {v:k for k,v in enumerate(x)}\n\nv2i = lbls.val2idx(); v2i\n    {'n03028079': 0,\n     'n03445777': 1,\n     'n03417042': 2,\n     'n02102040': 3,\n     'n03425413': 4,\n     'n03000684': 5,\n     'n01440764': 6,\n     'n03394916': 7,\n     'n03888257': 8,\n     'n02979186': 9}\n\n\nDataset\n# Define a dataset that returns tuples of inputs and targets from a list filenames\nclass Dataset:\n    def __init__(self, fns): self.fns=fns\n    def __len__(self): return len(self.fns)\n    def __getitem__(self, i):\n        im = Image.open(self.fns[i]).resize((64,64)).convert('RGB')\n        # the index mapping for the target value\n        y = v2i[self.fns[i].parent.name]\n        # scale the input image to the range [0,1]\n        return tensor(im).float()/255, tensor(y)\nNote: __getitem__ lets you index using the [] syntax.\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\n\ntrain_filt\n    (#13394) [False,False,False,False,False,False,False,False,False,False...]\n\n~train_filt\n    (#13394) [True,True,True,True,True,True,True,True,True,True...]\nNote: __invert__: Return the bitwise inverse of the number obj. This is equivalent to ~obj.\nprint_source(L.__invert__)\n        def __invert__(self): return self._new(not i for i in self)\n\nprint_source(L._new)\n        def _new(self, items, *args, **kwargs): return type(self)(items, *args, use_list=None, **kwargs)\n\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n    (9469, 3925)\n\ntrain_ds,valid_ds = Dataset(train),Dataset(valid)\nx,y = train_ds[0]\nx.shape,y\n    (torch.Size([64, 64, 3]), tensor(0))\n\nshow_image(x, title=lbls[y]);\n\n\n\n\n\n\nshow_image\n    &lt;function fastai.torch_core.show_image(im, ax=None, figsize=None, title=None, ctx=None, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, *, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs)&gt;\n\nprint_source(show_image)\n    @delegates(plt.Axes.imshow, keep=True, but=['shape', 'imlim'])\n    def show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n        \"Show a PIL or PyTorch image on `ax`.\"\n        # Handle pytorch axis order\n        if hasattrs(im, ('data','cpu','permute')):\n            im = im.data.cpu()\n            if im.shape[0]&lt;5: im=im.permute(1,2,0)\n        elif not isinstance(im,np.ndarray): im=array(im)\n        # Handle 1-channel images\n        if im.shape[-1]==1: im=im[...,0]\n    \n        ax = ifnone(ax,ctx)\n        if figsize is None: figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n        if ax is None: _,ax = plt.subplots(figsize=figsize)\n        ax.imshow(im, **kwargs)\n        if title is not None: ax.set_title(title)\n        ax.axis('off')\n        return ax\n\n# Define a function to stack individual tuples of independent and dependent variables\n# into a mini-batch\ndef collate(idxs, ds): \n    xb,yb = zip(*[ds[i] for i in idxs])\n    return torch.stack(xb),torch.stack(yb)\n\nhelp(torch.stack)\n    Help on built-in function stack:\n    \n    stack(...)\n        stack(tensors, dim=0, *, out=None) -&gt; Tensor\n        \n        Concatenates a sequence of tensors along a new dimension.\n        \n        All tensors need to be of the same size.\n        \n        Arguments:\n            tensors (sequence of Tensors): sequence of tensors to concatenate\n            dim (int): dimension to insert. Has to be between 0 and the number\n                of dimensions of concatenated tensors (inclusive)\n        \n        Keyword args:\n            out (Tensor, optional): the output tensor.\n\n# Create a mini-batch with a batch size of 2\nx,y = collate([1,2], train_ds)\nx.shape,y\n    (torch.Size([2, 64, 64, 3]), tensor([0, 0]))\n\nclass DataLoader:\n    def __init__(self, ds, bs=128, shuffle=False, n_workers=1):\n        self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers\n\n    def __len__(self): return (len(self.ds)-1)//self.bs+1\n\n    def __iter__(self):\n        idxs = L.range(self.ds)\n        if self.shuffle: idxs = idxs.shuffle()\n        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]\n        with ProcessPoolExecutor(self.n_workers) as ex:\n            yield from ex.map(collate, chunks, ds=self.ds)\n\nProcessPoolExecutor\n    fastcore.parallel.ProcessPoolExecutor\n\nprint_source(ProcessPoolExecutor)\n    class ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n        \"Same as Python's ProcessPoolExecutor, except can pass `max_workers==0` for serial execution\"\n        def __init__(self, max_workers=defaults.cpus, on_exc=print, pause=0, **kwargs):\n            if max_workers is None: max_workers=defaults.cpus\n            store_attr()\n            self.not_parallel = max_workers==0\n            if self.not_parallel: max_workers=1\n            super().__init__(max_workers, **kwargs)\n    \n        def map(self, f, items, *args, timeout=None, chunksize=1, **kwargs):\n            if not parallelable('max_workers', self.max_workers, f): self.max_workers = 0\n            self.not_parallel = self.max_workers==0\n            if self.not_parallel: self.max_workers=1\n    \n            if self.not_parallel == False: self.lock = Manager().Lock()\n            g = partial(f, *args, **kwargs)\n            if self.not_parallel: return map(g, items)\n            _g = partial(_call, self.lock, self.pause, self.max_workers, g)\n            try: return super().map(_g, items, timeout=timeout, chunksize=chunksize)\n            except Exception as e: self.on_exc(e)\n\nprint_source(parallelable)\n    def parallelable(param_name, num_workers, f=None):\n        f_in_main = f == None or sys.modules[f.__module__].__name__ == \"__main__\"\n        if sys.platform == \"win32\" and IN_NOTEBOOK and num_workers &gt; 0 and f_in_main:\n            print(\"Due to IPython and Windows limitation, python multiprocessing isn't available now.\")\n            print(f\"So `{param_name}` has to be changed to 0 to avoid getting stuck\")\n            return False\n        return True\n\nPython ProcessPoolExecutor\n\nDocumentaion\nexecutes calls asynchronously using a pool of processes\n\nNote: A parallel data loader is critical because opening and decompressing a JPEG image is slow.\ndefaults\n    namespace(cpus=16,\n              use_cuda=None,\n              activation=torch.nn.modules.activation.ReLU,\n              callbacks=[fastai.callback.core.TrainEvalCallback,\n                         fastai.learner.Recorder,\n                         fastai.callback.progress.ProgressCallback],\n              lr=0.001)\n\n# Only use 16 workers at most\nn_workers = min(16, defaults.cpus)\ntrain_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)\nvalid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)\nxb,yb = first(train_dl)\nxb.shape,yb.shape,len(train_dl)\n    (torch.Size([128, 64, 64, 3]), torch.Size([128]), 74)\nNote: This dataloader is nearly as fast as the one provided by PyTorch.\n# Calculate the mean and standard deviation for the mini-batch\nstats = [xb.mean((0,1,2)),xb.std((0,1,2))]\nstats\n    [tensor([0.4697, 0.4648, 0.4382]), tensor([0.2758, 0.2752, 0.2963])]\n\n# Define a class to store and apply the mean and standard deviation for the data\nclass Normalize:\n    def __init__(self, stats): self.stats=stats\n    def __call__(self, x):\n        if x.device != self.stats[0].device:\n            self.stats = to_device(self.stats, x.device)\n        return (x-self.stats[0])/self.stats[1]\nnorm = Normalize(stats)\n# Normalize the input and swap convert from HWC to CHW\ndef tfm_x(x): return norm(x).permute((0,3,1,2))\nRecall: PIL uses HWC axis order for images while PyTorch uses NCHW axis order.\nt = tfm_x(x)\nt.mean((0,2,3)),t.std((0,2,3))\n    (tensor([-0.1890, -0.2993, -0.4721]), tensor([0.6051, 0.5759, 0.7124]))"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#module-and-parameter",
    "href": "posts/fastai-book-notes/chapter-19/index.html#module-and-parameter",
    "title": "Notes on fastai Book Ch. 19",
    "section": "Module and Parameter",
    "text": "Module and Parameter\n# The parameter class just sets requires_grad_ to True for its tensor\nclass Parameter(Tensor):\n    def __new__(self, x): return Tensor._make_subclass(Parameter, x, True)\n    def __init__(self, *args, **kwargs): self.requires_grad_()\nNote: __new__: Called to create a new instance of class cls. * Called before __init__\n\nParameter(tensor(3.))\n    tensor(3., requires_grad=True)\n\nclass Module:\n    def __init__(self):\n        self.hook,self.params,self.children,self._training = None,[],[],False\n        \n    def register_parameters(self, *ps): self.params += ps\n    def register_modules   (self, *ms): self.children += ms\n        \n    @property\n    def training(self): return self._training\n    @training.setter\n    def training(self,v):\n        self._training = v\n        for m in self.children: m.training=v\n            \n    def parameters(self):\n        # Get parameters for the Module and any child Modules recursively\n        return self.params + sum([m.parameters() for m in self.children], [])\n\n    # Called anytime Python sets an attribute for the class\n    def __setattr__(self,k,v):\n        super().__setattr__(k,v)\n        # Check if the new value is a Parameter\n        if isinstance(v,Parameter): self.register_parameters(v)\n        # Check if the new value is a Module\n        if isinstance(v,Module):    self.register_modules(v)\n        \n    def __call__(self, *args, **kwargs):\n        # The forward method needs to be implemented by a subclass\n        res = self.forward(*args, **kwargs)\n        # Call any attached hooks after the forward pass\n        if self.hook is not None: self.hook(res, args)\n        return res\n    \n    def cuda(self):\n        # Move all parameters to the GPU\n        for p in self.parameters(): p.data = p.data.cuda()\nsum([[4],[5],[6]],[1,2,3])\n    [1, 2, 3, 4, 5, 6]\n\n# Inherit from the custom Module class\nclass ConvLayer(Module):\n    def __init__(self, ni, nf, stride=1, bias=True, act=True):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni,3,3))\n        self.b = Parameter(torch.zeros(nf)) if bias else None\n        self.act,self.stride = act,stride\n        init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_\n        init(self.w)\n    # Implement the forward function\n    def forward(self, x):\n        x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1)\n        if self.act: x = F.relu(x)\n        return x\n\n# A single 3-channel 28x28 image\ninp = torch.randn(1, 3, 28, 28)\n# A single 3x3 convolutional kernel for a 3-channel image\nw = torch.randn(1, 3, 3, 3)\n\n# Extracts sliding local blocks\ninp_unf = torch.nn.functional.unfold(inp, (3, 3))\ninp_unf.shape\n    torch.Size([1, 27, 676])\nNote: No padding and a stride of \\(1\\) so we go from \\(28x28\\) to \\(26x26\\) (\\(26x26=676\\)).\ninp[0][0][0][:9]\n    tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047, -0.7521])\n\ninp_unf[0][0][:9]\n    tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047, -0.7521])\n\ninp[0][0][1][:9]\n    tensor([-0.2316,  0.0418, -0.2516,  0.8599, -1.3847, -0.8712, -0.2234,  1.7174,  0.3189])\n\ninp_unf[0][3][:9]\n    tensor([-0.2316,  0.0418, -0.2516,  0.8599, -1.3847, -0.8712, -0.2234,  1.7174,  0.3189])\n\ninp_unf.transpose(1, 2).shape\n    torch.Size([1, 676, 27])\n\nw.view(w.size(0), -1).t().shape\n    torch.Size([27, 1])\n\nout_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\nout_unf.shape\n    torch.Size([1, 1, 676])\n\nout_unf.view(1,1,inp.shape[2]-2,inp.shape[3]-2)[0][0][0]\n    tensor([  2.1036,   3.9867, -11.0306,  -4.8908,  -5.1374,   0.4219,   6.1278,  -1.9733,   4.0213,   0.3114,  -8.5303,   5.3541,   6.5859,  -4.0149,   4.5094,  -5.4661,   2.4601,  -4.3120,   1.3568,\n             -1.8379,   0.1750,  -1.6839,   4.5438,  -1.2521,  -1.9825,  -4.2498])\n\ntorch.nn.functional.conv2d(inp, w).shape, torch.nn.functional.conv2d(inp, w)[0][0][0]\n    (torch.Size([1, 1, 26, 26]),\n     tensor([  2.1036,   3.9867, -11.0306,  -4.8908,  -5.1374,   0.4219,   6.1278,  -1.9733,   4.0213,   0.3114,  -8.5303,   5.3541,   6.5859,  -4.0149,   4.5094,  -5.4661,   2.4601,  -4.3120,   1.3568,\n              -1.8379,   0.1750,  -1.6839,   4.5438,  -1.2521,  -1.9825,  -4.2498]))\n\nl = ConvLayer(3, 4)\nlen(l.parameters())\n    2\n\nxbt = tfm_x(xb)\nr = l(xbt)\nr.shape\n    torch.Size([128, 4, 64, 64])\n\nclass Linear(Module):\n    def __init__(self, ni, nf):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni))\n        self.b = Parameter(torch.zeros(nf))\n        nn.init.xavier_normal_(self.w)\n    \n    def forward(self, x): return x@self.w.t() + self.b\nl = Linear(4,2)\nr = l(torch.ones(3,4))\nr.shape\n    torch.Size([3, 2])\n\n# A test class to confirm that mu;tiple parameters are properly registered\nclass T(Module):\n    def __init__(self):\n        super().__init__()\n        self.c,self.l = ConvLayer(3,4),Linear(4,2)\n\nt = T()\nlen(t.parameters())\n    4\nNote: A weight and bias parameter for both of the layers should add up to for total parameters.\n# Verify the cuda() method moves all the parameters to the GPU\nt.cuda()\n# Check the device for the linear layer weights parameter\nt.l.w.device\n    device(type='cuda', index=0)\n\nSimple CNN\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n        # A Sequential Module will contain a list of other Modules\n        self.register_modules(*layers)\n\n    # Call each layer in sequential order\n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\nclass AdaptivePool(Module):\n    # Pool input values to a 1x1 output\n    def forward(self, x): return x.mean((2,3))\n\ndef simple_cnn():\n    return Sequential(\n        ConvLayer(3 ,16 ,stride=2), #32\n        ConvLayer(16,32 ,stride=2), #16\n        ConvLayer(32,64 ,stride=2), # 8\n        ConvLayer(64,128,stride=2), # 4\n        AdaptivePool(),\n        Linear(128, 10)\n    )\n\nm = simple_cnn()\nlen(m.parameters())\n    10\nNote: Five layers each with a weights parameter and a bias parameter. (plus a pooling layer)\n# Add a hook to print the mean and standard deviation of the activations for each ConvLayer\ndef print_stats(outp, inp): print (outp.mean().item(),outp.std().item())\nfor i in range(4): m.layers[i].hook = print_stats\n\nr = m(xbt)\nr.shape\n    0.5312516689300537 0.9105479121208191\n    0.4804598093032837 0.872667133808136\n    0.4145401120185852 0.7496744990348816\n    0.440979540348053 0.7261776924133301\n\n    torch.Size([128, 10])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#loss",
    "href": "posts/fastai-book-notes/chapter-19/index.html#loss",
    "title": "Notes on fastai Book Ch. 19",
    "section": "Loss",
    "text": "Loss\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nsm = log_softmax(r); sm[0][0]\n    tensor(-2.5062, grad_fn=&lt;AliasBackward0&gt;)\n\nloss = nll(sm, yb)\nloss\n    tensor(2.6062, grad_fn=&lt;AliasBackward0&gt;)\n\nx = torch.rand(5)\na = x.max()\nx.exp().sum().log() == a + (x-a).exp().sum().log()\n    tensor(False)\n\nLogSumExp Trick\n\nExplanation\na more stable way (for computers) to compute the log of the sum of exponentials \\[\\log{\\left( \\sum^{n}_{j=1}{e^{x_{j}}} \\right)} = \\log{\\left(e^{a} \\sum^{n}_{j=1}{e^{x_{j}-a}} \\right)} = a + \\log{\\left(\\sum^{n}_{j=1}{e^{x_{j}-a}} \\right)}\\]\nwhere \\(a\\) is the maximum of \\(x_{j}\\)\n\n\nx = torch.rand(5)\na = x.max()\nx.exp().sum().log() == a + (x-a).exp().sum().log()\n    tensor(True)\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nlogsumexp(r)[0]\n    tensor(2.6922, grad_fn=&lt;AliasBackward0&gt;)\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nsm = log_softmax(r); sm[0][0]\n    tensor(-2.5062, grad_fn=&lt;AliasBackward0&gt;)\n\ndef cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean()"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#learner",
    "href": "posts/fastai-book-notes/chapter-19/index.html#learner",
    "title": "Notes on fastai Book Ch. 19",
    "section": "Learner",
    "text": "Learner\nclass SGD:\n    def __init__(self, params, lr, wd=0.): store_attr()\n    def step(self):\n        for p in self.params:\n            # SGD plus weight decay\n            p.data -= (p.grad.data + p.data*self.wd) * self.lr\n            p.grad.data.zero_()\n\nprint_source(store_attr)\n    def store_attr(names=None, self=None, but='', cast=False, store_args=None, **attrs):\n        \"Store params named in comma-separated `names` from calling context into attrs in `self`\"\n        fr = sys._getframe(1)\n        args = argnames(fr, True)\n        if self: args = ('self', *args)\n        else: self = fr.f_locals[args[0]]\n        if store_args is None: store_args = not hasattr(self,'__slots__')\n        if store_args and not hasattr(self, '__stored_args__'): self.__stored_args__ = {}\n        anno = annotations(self) if cast else {}\n        if names and isinstance(names,str): names = re.split(', *', names)\n        ns = names if names is not None else getattr(self, '__slots__', args[1:])\n        added = {n:fr.f_locals[n] for n in ns}\n        attrs = {**attrs, **added}\n        if isinstance(but,str): but = re.split(', *', but)\n        attrs = {k:v for k,v in attrs.items() if k not in but}\n        return _store_attr(self, anno, **attrs)\n\nhelp(sys._getframe)\n    Help on built-in function _getframe in module sys:\n    \n    _getframe(depth=0, /)\n        Return a frame object from the call stack.\n        \n        If optional integer depth is given, return the frame object that many\n        calls below the top of the stack.  If that is deeper than the call\n        stack, ValueError is raised.  The default for depth is zero, returning\n        the frame at the top of the call stack.\n        \n        This function should be used for internal and specialized purposes\n        only.\n\nprint_source(argnames)\n    def argnames(f, frame=False):\n        \"Names of arguments to function or frame `f`\"\n        code = getattr(f, 'f_code' if frame else '__code__')\n        return code.co_varnames[:code.co_argcount+code.co_kwonlyargcount]\n\nprint_source(annotations)\n    def annotations(o):\n        \"Annotations for `o`, or `type(o)`\"\n        res = {}\n        if not o: return res\n        res = type_hints(o)\n        if not res: res = type_hints(getattr(o,'__init__',None))\n        if not res: res = type_hints(type(o))\n        return res\n\n# A class that just stores the training and validation datasets\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls\n\ndls = DataLoaders(train_dl,valid_dl)\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD):\n        store_attr()\n        # Let each callback know what Learner it is being used in\n        for cb in cbs: cb.learner = self\n\n    def one_batch(self):\n        # Call the before_batch callback\n        self('before_batch')\n        xb,yb = self.batch\n        self.preds = self.model(xb)\n        self.loss = self.loss_func(self.preds, yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n        # Call the after_batch callback\n        self('after_batch')\n\n    def one_epoch(self, train):\n        self.model.training = train\n        # Call the before_epoch callback\n        self('before_epoch')\n        # Select dataset based on whether we are in training or validation mode\n        dl = self.dls.train if train else self.dls.valid\n        # Iterate through the current dataset one batch at a time\n        for self.num,self.batch in enumerate(progress_bar(dl, leave=False)):\n            self.one_batch()\n        # Call the after_epoch callback\n        self('after_epoch')\n    \n    def fit(self, n_epochs):\n        # Call the before_fit callback\n        self('before_fit')\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        try:\n            for self.epoch in range(n_epochs):\n                # Complete a full pass through the training dataset\n                self.one_epoch(True)\n                # Complete a full pass through the validation dataset\n                self.one_epoch(False)\n        except CancelFitException: pass\n        # Call the after_fit callback\n        self('after_fit')\n        \n    def __call__(self,name):\n        for cb in self.cbs: getattr(cb,name,noop)()\n\nCallbacks\n# Automatically pass requests to change attributes that don't exist to the `_default`\nclass Callback(GetAttr): _default='learner'\n\nGetAttr\n    fastcore.basics.GetAttr\n\nprint_source(GetAttr)\n    class GetAttr:\n        \"Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`\"\n        _default='default'\n        def _component_attr_filter(self,k):\n            if k.startswith('__') or k in ('_xtra',self._default): return False\n            xtra = getattr(self,'_xtra',None)\n            return xtra is None or k in xtra\n        def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)]\n        def __getattr__(self,k):\n            if self._component_attr_filter(k):\n                attr = getattr(self,self._default,None)\n                if attr is not None: return getattr(attr,k)\n            raise AttributeError(k)\n        def __dir__(self): return custom_dir(self,self._dir())\n    #     def __getstate__(self): return self.__dict__\n        def __setstate__(self,data): self.__dict__.update(data)\n\nhelp(getattr)\n    Help on built-in function getattr in module builtins:\n    \n    getattr(...)\n        getattr(object, name[, default]) -&gt; value\n        \n        Get a named attribute from an object; getattr(x, 'y') is equivalent to x.y.\n        When a default argument is given, it is returned when the attribute doesn't\n        exist; without it, an exception is raised in that case.\nNote: __dir__: Called when dir() is called on an object\ndir(Callback)\n    ['__class__',\n     '__delattr__',\n     '__dict__',\n     '__dir__',\n     '__doc__',\n     '__eq__',\n     '__format__',\n     '__ge__',\n     '__getattr__',\n     '__getattribute__',\n     '__gt__',\n     '__hash__',\n     '__init__',\n     '__init_subclass__',\n     '__le__',\n     '__lt__',\n     '__module__',\n     '__ne__',\n     '__new__',\n     '__reduce__',\n     '__reduce_ex__',\n     '__repr__',\n     '__setattr__',\n     '__setstate__',\n     '__sizeof__',\n     '__str__',\n     '__subclasshook__',\n     '__weakref__',\n     '_component_attr_filter',\n     '_default',\n     '_dir']\n\n# A callback that moves all model parameters to the GPU at the start of the Learner.fit() method\nclass SetupLearnerCB(Callback):\n    def before_batch(self):\n        xb,yb = to_device(self.batch)\n        self.learner.batch = tfm_x(xb),yb\n\n    def before_fit(self): self.model.cuda()\n\n# A callback to track training progress\nclass TrackResults(Callback):\n    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n        \n    def after_epoch(self):\n        n = sum(self.ns)\n        print(self.epoch, self.model.training,\n              sum(self.losses).item()/n, sum(self.accs).item()/n)\n        \n    def after_batch(self):\n        xb,yb = self.batch\n        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n        self.accs.append(acc)\n        n = len(xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n# Test the learner\ncbs = [SetupLearnerCB(),TrackResults()]\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)\nlearn.model\n    &lt;__main__.Sequential at 0x7f15bd1d2bb0&gt;\n\nlearn.cbs\n    [&lt;__main__.SetupLearnerCB at 0x7f15bfee94c0&gt;,\n     &lt;__main__.TrackResults at 0x7f15bfee9430&gt;]\n\nlearn.cbs[0].model\n    &lt;__main__.Sequential at 0x7f15bd1d2bb0&gt;\n\nlearn.fit(1)\n    0 True 2.124386154820995 0.23592776428345127\n\n    0 False 2.206350766321656 0.2140127388535032\n\n\nScheduling the Learning Rate\nclass LRFinder(Callback):\n    def before_fit(self):\n        self.losses,self.lrs = [],[]\n        self.learner.lr = 1e-6\n        \n    def before_batch(self):\n        if not self.model.training: return\n        self.opt.lr *= 1.2\n\n    def after_batch(self):\n        if not self.model.training: return\n        if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException\n        self.losses.append(self.loss.item())\n        self.lrs.append(self.opt.lr)\n\nlrfind = LRFinder()\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])\nlearn.fit(2)\n    0 True 2.3831190100591404 0.12894709050586123\n\n    0 False 3.06156648089172 0.0889171974522293\n\nplt.plot(lrfind.lrs[:-2],lrfind.losses[:-2])\nplt.xscale('log')\n\n\n\n\n\n\nclass OneCycle(Callback):\n    def __init__(self, base_lr): self.base_lr = base_lr\n    def before_fit(self): self.lrs = []\n\n    def before_batch(self):\n        if not self.model.training: return\n        n = len(self.dls.train)\n        bn = self.epoch*n + self.num\n        mn = self.n_epochs*n\n        pct = bn/mn\n        pct_start,div_start = 0.25,10\n        if pct&lt;pct_start:\n            pct /= pct_start\n            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n        else:\n            pct = (pct-pct_start)/(1-pct_start)\n            lr = (1-pct)*self.base_lr\n        self.opt.lr = lr\n        self.lrs.append(lr)\n\nonecyc = OneCycle(0.1)\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc])\n\nlearn.fit(8)\n\n    0 True 2.260071103733235 0.1608406378709473\n    \n    0 False 2.204750696656051 0.22038216560509555\n    \n    1 True 2.0790218410074983 0.25873904319357904\n    \n    1 False 2.020204518312102 0.267515923566879\n    \n    2 True 1.9341191074163058 0.32041398246910974\n    \n    2 False 1.837548765923567 0.36178343949044584\n    \n    3 True 1.8241042728244798 0.3763861020171085\n    \n    3 False 1.780602980692675 0.381656050955414\n    \n    4 True 1.7337240270883938 0.403210476291055\n    \n    4 False 1.7259341411226115 0.41095541401273883\n    \n    5 True 1.6540151167625938 0.4310909282923223\n    \n    5 False 1.6680330662818472 0.41859872611464966\n    \n    6 True 1.5874014671229804 0.4634069067483367\n    \n    6 False 1.6006179090366242 0.4601273885350318\n    \n    7 True 1.5361670525200655 0.4771359172035062\n    \n    7 False 1.5802234275477707 0.4682802547770701\nplt.plot(onecyc.lrs);"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-19/index.html#references",
    "href": "posts/fastai-book-notes/chapter-19/index.html#references",
    "title": "Notes on fastai Book Ch. 19",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 18"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html",
    "href": "posts/fastai-book-notes/chapter-2/index.html",
    "title": "Notes on fastai Book Ch. 2",
    "section": "",
    "text": "The Practice of Deep Learning\nGathering Data\nFrom Data to DataLoaders\nUsing a Model to Clean Your Data\nTurn Your Model into an Online Application\nHow to Avoid Disaster\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#the-practice-of-deep-learning",
    "href": "posts/fastai-book-notes/chapter-2/index.html#the-practice-of-deep-learning",
    "title": "Notes on fastai Book Ch. 2",
    "section": "The Practice of Deep Learning",
    "text": "The Practice of Deep Learning\n\nKeep an open mind\nUnderestimating the constraints and overestimating the capabilities of deep learning may lead to poor results\nOverestimating the constraints and underestimating the capabilities of deep learning may lead prevent you from exploring solvable problems\nDesign a process through which you can find the specific capabilities and constraints related to your particular problem\n\n\nStarting Your Project\n\nData availability is the most important consideration when selecting a deep learning project\nDo not attempt to find the “perfect” dataset\n\nJust get started and iterate\n\nIterate from end-to-end\n\nDon’t spend months fine-tuning your model, polishing the GUI, or labeling the perfect dataset\nComplete every step as well as you can in a reasonable amount of time\nIf your final goal is an application that runs on a mobile phone, that should be what you have after each iteration\nCan take shortcuts in early iterations like running the model on a remote server rather than on device\nIteration exposes the trickiest bits and which bits make the biggest difference to the final result\nGives you a better understanding of how much data you really need\nGives you a working prototype to demo your project\n\nThe secret to getting good organizational buy-in for a project\n\n\nIt is easiest to get started on a project for which you already have available data\nCan sometimes find a relevant dataset created for a previous machine learning project\nStart with projects in areas that deep learning has already been shown to work\n\n\n\nThe State of Deep Learning (Early 2020)\n\nComputer Vision\n\nDeep learning has been shown to recognize items in an image at least as well as people in nearly every domain\n\nKnown as object recognition\n\nDeep learning is good at locating objects in an image\n\nKnown as object detection\nLabeling images for object detection can be slow and expensive\n\nDeep learning models are generally not good at recognizing images that are significantly different in structure or style from those used to train the model\n\nColor images vs black and white images\nReal images vs hand drawn images\n\nIt is often possible to represent data for a non-computer vision problem as an image\n\n\n\nText (Natural Language Processing)\n\nDeep learning is good at classifying both short and long documents based on categories\n\nSpam vs not spam\nPositive vs negative reviews\nAuthor\nSource website\n\nDeep learning is good at generating context-appropriate text\n\nReplies to social media posts\nImitating a particular author’s style\n\nDeep learning is not good at generating correct responses\nWe do not have a reliable way to combine a knowledge base with a deep learning model to generate factually accurate responses\nDanger of deep learning models being used at scale to generate context appropriate, highly compelling responses on social media to spread disinformation, create unrest and encourage conflict\n\nText generation models will always be a bit ahead of models for automatically recognizing generated text\nModels for automatically recognizing generated text can be used to improve text generation models\n\nDeep learning has many applications in NLP\n\nTranslating text between languages\n\nThe translation might include completely incorrect information\n\nSummarizing long documents\n\nThe translation might include completely incorrect information\n\nFind all mentions of a concept of interest\n\nAvoid using deep learning as an entirely automated process when it is generating text that needs to be accurate\n\nInstead use is as part of a process in which the model and a human user closely interact\n\n\n\n\nCombining Text and Images\n\nDeep learning models can combine both text and images\n\nGenerate captions based on an input image\n\n\n\n\nTabular Data\n\nDeep learning has made significant improvements but is still used as part of an ensemble of multiple types of model\nGreatly increases the variety of columns that you can include\n\nColumns containing natural language\n\nBook titles\nReviews\n\nHigh-cardinality categorical columns\n\nSomething that contains a large number of discrete choices\n\nZip code\nProduct id\n\n\n\nDeep learning models generally take longer to train than more traditional methods like random forests or gradient boosting machines\n\nThis is changing thanks to libraries such as RAPIDS which provides GPU acceleration\n\n\n\n\nRecommendation Systems\n\nA special type of tabular data\n\nGenerally have a high-cardinality categorical variable representing users and another representing things to recommend (e.g. products)\n\nDeep learning models are good at handling recommendation systems since they are good at handling high-cardinality categorical variables\nNearly all machine learning approaches have the downside that they tell you only which products a particular user might like rather than what recommendation would be helpful to a user\n\nA recommendation system might recommend nothing but hammers because you recently bought a hammer\n\n\n\n\nOther Data Types\n\nDomain-specific data types often fit well into existing categories\n\nProtein chains look a lot like natural language documents\n\nLong sequences of discrete tokens with complex relationships and meaning throughout the sequence\n\nSounds can be represented in image format as spectrograms\n\n\n\n\n\nThe Drivetrain Approach\n\nDesigning great data products\nData scientists need a systematic design approach to build increasingly sophisticated products\n\nWe use data to produce actionable outcomes\nPractical implementation of models requires a lot more than just training the model\n\nDefined Objective → Levers → Data → Models\nSteps\n\nDefined Objective\n\nConsider your objective\n\nLevers\n\nWhat inputs can we control?\nThink about what actions you can take to meet that objective\n\nData\n\nWhat inputs can we collect?\nThink about what data you have or can acquire that can help\n\nModels\n\nBuild a model that you can use to determine the best actions to take to get the best results in terms of your objective\nThe models we can build are determined by the objective, available levers and available data"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#gathering-data",
    "href": "posts/fastai-book-notes/chapter-2/index.html#gathering-data",
    "title": "Notes on fastai Book Ch. 2",
    "section": "Gathering Data",
    "text": "Gathering Data\n\nCan often find the data you need for a project online\nBing Web Search API\n\nCan be used to download images from web searches\n\nModels can only reflect the data used to train them\n\nThink carefully about the types of data that you expect to see in production and make sure all these types are reflected in your model’s source data\n\nActionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products\n\n\nimport pandas as pd\nimport os\nkey = os.environ.get('AZURE_SEARCH_KEY', 'f4be28837a074dfa90a1b72900a971ef')\n\nsearch_images_bing\n&lt;function fastbook.search_images_bing(key, term, min_sz=128, max_images=150)&gt;\n\nresults = search_images_bing(key, 'grizzly bear')\n\nprint(type(results))\nprint(f\"Number of results: {len(results)}\")\n&lt;class 'fastcore.foundation.L'&gt;\nNumber of results: 150\n\npd.DataFrame(results).head()\n\n\n\n\n\n\n\n\nwebSearchUrl\n\n\nname\n\n\nthumbnailUrl\n\n\ndatePublished\n\n\nisFamilyFriendly\n\n\ncontentUrl\n\n\nhostPageUrl\n\n\ncontentSize\n\n\nencodingFormat\n\n\nhostPageDisplayUrl\n\n\nwidth\n\n\nheight\n\n\nhostPageFavIconUrl\n\n\nhostPageDomainFriendlyName\n\n\nhostPageDiscoveredDate\n\n\nthumbnail\n\n\nimageInsightsToken\n\n\ninsightsMetadata\n\n\nimageId\n\n\naccentColor\n\n\ncreativeCommons\n\n\n\n\n\n\n0\n\n\nhttps://www.bing.com/images/search?view=detailv2&FORM=OIIRPO&q=grizzly+bear&id=4FE226180F7071D1B3F36B29C2EA074B00E3CBEC&simid=607989609245709244\n\n\nGrizzly Bear Basic Facts And New Pictures | The Wildlife\n\n\nhttps://tse1.mm.bing.net/th?id=OIP.Mw_Mi-jVWv9_0SNTuiGaSQHaE8&pid=Api\n\n\n2012-10-15T12:00:00.0000000Z\n\n\nTrue\n\n\nhttp://2.bp.blogspot.com/-NjMTuklENdE/UHzVv_8dIxI/AAAAAAAAA-U/tNBsQDn8kFI/s1600/Grizzly+Bear+Pic.jpg\n\n\nhttp://wildlifeanimalz.blogspot.com/2012/10/Grizzly-Bear.html\n\n\n332689 B\n\n\njpeg\n\n\nwildlifeanimalz.blogspot.com/2012/10/Grizzly-Bear.html\n\n\n1600\n\n\n1068\n\n\nhttps://www.bing.com/th?id=ODF.kCKFU1-d0l3Elu2Vvbpmew&pid=Api\n\n\nblogspot.com\n\n\n2012-10-15T12:00:00.0000000Z\n\n\n{‘width’: 474, ‘height’: 316}\n\n\nccid_Mw/Mi+jVcp_433560BB77158852D3AB8C7F61E351BFmid_4FE226180F7071D1B3F36B29C2EA074B00E3CBECsimid_607989609245709244thid_OIP.Mw!_Mi-jVWv9!_0SNTuiGaSQHaE8\n\n\n{‘recipeSourcesCount’: 0, ‘pagesIncludingCount’: 37, ‘availableSizesCount’: 22}\n\n\n4FE226180F7071D1B3F36B29C2EA074B00E3CBEC\n\n\n8C623F\n\n\nNaN\n\n\n\n\n1\n\n\nhttps://www.bing.com/images/search?view=detailv2&FORM=OIIRPO&q=grizzly+bear&id=753BAE8D7E8284DA0D378113DB437A15800814ED&simid=608022027642737229\n\n\nThe Legacy of Big Boy the Grizzly Bear | Blog | Nature | PBS\n\n\nhttps://tse4.mm.bing.net/th?id=OIP.P710tottl5nl_DmTEEDv-gHaEK&pid=Api\n\n\n2018-08-22T02:23:00.0000000Z\n\n\nTrue\n\n\nhttp://www.pbs.org/wnet/nature/files/2018/07/Bear133.jpg\n\n\nhttp://www.pbs.org/wnet/nature/blog/legacy-big-boy-grizzly-bear/\n\n\n631006 B\n\n\njpeg\n\n\nwww.pbs.org/wnet/nature/blog/legacy-big-boy-grizzly-bear\n\n\n1920\n\n\n1080\n\n\nhttps://www.bing.com/th?id=ODF.6ks36T98eL13adVA4nwLZA&pid=Api\n\n\nPBS\n\n\n2018-08-21T00:00:00.0000000Z\n\n\n{‘width’: 474, ‘height’: 266}\n\n\nccid_P710tottcp_0F0D46773348FABEE8C97ACBB9037F98mid_753BAE8D7E8284DA0D378113DB437A15800814EDsimid_608022027642737229thid_OIP.P710tottl5nl!_DmTEEDv-gHaEK\n\n\n{‘pagesIncludingCount’: 21, ‘availableSizesCount’: 10}\n\n\n753BAE8D7E8284DA0D378113DB437A15800814ED\n\n\n717C4F\n\n\nNaN\n\n\n\n\n2\n\n\nhttps://www.bing.com/images/search?view=detailv2&FORM=OIIRPO&q=grizzly+bear&id=CB17B3DC66E2F4684C9CA770135FF2F3D1B2070D&simid=608018286723871513\n\n\nIdaho Grizzly Bears are Waking Up, Emerging from Dens\n\n\nhttps://tse4.mm.bing.net/th?id=OIP.H3h0vO_c6L61im-L99pEegHaE8&pid=Api\n\n\n2015-04-22T09:38:00.0000000Z\n\n\nTrue\n\n\nhttps://tetonvalleylodge.com/wp-content/uploads/2015/04/grizzly-bear-idaho.jpg\n\n\nhttps://tetonvalleylodge.com/grizzly-bears-idaho-are-waking-up/\n\n\n477917 B\n\n\njpeg\n\n\nhttps://tetonvalleylodge.com/grizzly-bears-idaho-are-waking-up\n\n\n3150\n\n\n2100\n\n\nNaN\n\n\nNaN\n\n\n2015-04-22T09:38:24.0000000Z\n\n\n{‘width’: 474, ‘height’: 316}\n\n\nccid_H3h0vO/ccp_9AD6DD7A0450257A1DD35511F3C7D0B5mid_CB17B3DC66E2F4684C9CA770135FF2F3D1B2070Dsimid_608018286723871513thid_OIP.H3h0vO!_c6L61im-L99pEegHaE8\n\n\n{‘recipeSourcesCount’: 0, ‘pagesIncludingCount’: 6, ‘availableSizesCount’: 3}\n\n\nCB17B3DC66E2F4684C9CA770135FF2F3D1B2070D\n\n\n846847\n\n\nNaN\n\n\n\n\n3\n\n\nhttps://www.bing.com/images/search?view=detailv2&FORM=OIIRPO&q=grizzly+bear&id=92DD5424F3ECF22292A0A370FE25492F4057F1C6&simid=608029569602050472\n\n\nGrizzly Bear forum Ellen downtown Bozeman – March 2, 2019\n\n\nhttps://tse3.mm.bing.net/th?id=OIP.zMGLZFixVpyMGWZOH8oe3QHaEN&pid=Api\n\n\n2019-03-02T12:00:00.0000000Z\n\n\nTrue\n\n\nhttps://bozone.com/site/wp-content/uploads/2019/03/GRIZZLY.jpeg\n\n\nhttps://bozone.com/grizzly-bear-forum-ellen-downtown-bozeman-march-2-2019/\n\n\n655514 B\n\n\njpeg\n\n\nhttps://bozone.com/grizzly-bear-forum-ellen-downtown-bozeman-march-2-2019\n\n\n2951\n\n\n1680\n\n\nNaN\n\n\nNaN\n\n\n2019-03-02T00:00:00.0000000Z\n\n\n{‘width’: 474, ‘height’: 269}\n\n\nccid_zMGLZFixcp_C07AA690D0C3FEF983C6BB52766E8160mid_92DD5424F3ECF22292A0A370FE25492F4057F1C6simid_608029569602050472thid_OIP.zMGLZFixVpyMGWZOH8oe3QHaEN\n\n\n{‘recipeSourcesCount’: 0, ‘pagesIncludingCount’: 13, ‘availableSizesCount’: 6}\n\n\n92DD5424F3ECF22292A0A370FE25492F4057F1C6\n\n\n818843\n\n\nNaN\n\n\n\n\n4\n\n\nhttps://www.bing.com/images/search?view=detailv2&FORM=OIIRPO&q=grizzly+bear&id=861CAB82AE35F116E2E1A1AAE921DEDFCD75B4BB&simid=608013171422092302\n\n\nGrisly outlook: Bears kill more and more livestock as their population grows | TSLN.com\n\n\nhttps://tse3.mm.bing.net/th?id=OIP.G7bi9KTwAG79A0emOjD8IAHaE6&pid=Api\n\n\n2019-04-11T21:24:00.0000000Z\n\n\nTrue\n\n\nhttps://www.tsln.com/wp-content/uploads/2018/10/bears-tsln-101318-1240x823.jpg\n\n\nhttps://www.tsln.com/news/wyoming-grizzly-bear-update/\n\n\n225897 B\n\n\njpeg\n\n\nhttps://www.tsln.com/news/wyoming-grizzly-bear-update\n\n\n1240\n\n\n823\n\n\nhttps://www.bing.com/th?id=ODF.naaPSqp1Pz37bdCNwXhx_Q&pid=Api\n\n\nTri-State Livestock News\n\n\n2018-10-13T00:00:00.0000000Z\n\n\n{‘width’: 474, ‘height’: 314}\n\n\nccid_G7bi9KTwcp_268C8D29BC3775D345836BFB181C377Dmid_861CAB82AE35F116E2E1A1AAE921DEDFCD75B4BBsimid_608013171422092302thid_OIP.G7bi9KTwAG79A0emOjD8IAHaE6\n\n\n{‘recipeSourcesCount’: 0, ‘pagesIncludingCount’: 67, ‘availableSizesCount’: 50}\n\n\n861CAB82AE35F116E2E1A1AAE921DEDFCD75B4BB\n\n\n91633A\n\n\nNaN\n\n\n\n\n\n\n\nims = results.attrgot('contentUrl')\nlen(ims)\n150\n\ndest = 'images/grizzly.jpg'\n# Download `url` to `dest`\ndownload_url(ims[0], dest)\nPath('images/grizzly.jpg')\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n# Define the parent directory for the dataset\ndatasets_dir = \"/mnt/980SSD/Datasets\"\n# Define the main directory for the dataset\npath = Path(f'{datasets_dir}/bears')\nprint(path)\n# Define the class subdirectories for the dataset\nbear_types = 'grizzly','black','teddy'\nfor b in bear_types:\n    print(f\"{path}/{b}\")\n/mnt/980SSD/Datasets/bears\n/mnt/980SSD/Datasets/bears/grizzly\n/mnt/980SSD/Datasets/bears/black\n/mnt/980SSD/Datasets/bears/teddy\n\n# Check if the path exists\nif not path.exists():\n    # Create a new directory at this given path\n    print(f\"Creating new directory: {path}\")\n    path.mkdir()\n    for o in bear_types:\n        # Define subdirectory name for bear type        \n        dest = (path/o)\n        # Create subdirectory for bear type\n        print(f\"\\tCreating subdirectory: {dest}\")\n        dest.mkdir(exist_ok=True)\n        # Search for images of bear type\n        search_query = f'{o} bear'\n        results = search_images_bing(key, search_query)\n        # Download images from URL results\n        print(f\"\\t\\tDownloading results for search query: {search_query}\")\n        download_images(dest, urls=results.attrgot('contentUrl'))\nfns = get_image_files(path)\nprint(fns[:5])\n[Path('/mnt/980SSD/Datasets/bears/black/00000000.jpg'), Path('/mnt/980SSD/Datasets/bears/black/00000001.jpg'), Path('/mnt/980SSD/Datasets/bears/black/00000002.jpg'), Path('/mnt/980SSD/Datasets/bears/black/00000003.png'), Path('/mnt/980SSD/Datasets/bears/black/00000004.jpg')]\n\n# Find images in `fns` that can't be opened\nfailed = verify_images(fns)\nprint(failed[:5])\n/home/innom-dt/miniconda3/envs/fastbook/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:822: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n  warnings.warn(str(msg))\n[]\n\n# Remove image files that failed verification\nfailed.map(Path.unlink);"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#from-data-to-dataloaders",
    "href": "posts/fastai-book-notes/chapter-2/index.html#from-data-to-dataloaders",
    "title": "Notes on fastai Book Ch. 2",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\n\nDataLoaders\n\nA thin fastai class that just stores whatever DataLoader objects passed to it and makes them available as the properties train and valid\nProvides the data for your model\nInformation needed to turn downloaded data into DataLoaders objects\n\nThe kind of data we are working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\n\n\n\nDataLoader\n\nA class that provides batches of a few items at a time to the GPU\n\n\n\nData block API\n\nA flexible system to fully customize every stage of the creation of your DataLoaders\nData block: a template for creating a DataLoaders object\nIndependent variable: the thing we are using to make predictions\nDependent variable: the target variable to predict\nTraining data is fed to a model in batches\n\nEach image in a batch needs to be the same size\n\n\n\n# Generic container to quickly build `Datasets` and `DataLoaders`\nbears = DataBlock(\n    # Define blocks for the data and labels\n    blocks=(\n        # A `TransformBlock` for images\n        ImageBlock, \n        # A `TransformBlock` for single-label categorical targets\n        CategoryBlock\n    ), \n    # Get image files in `path` recursively\n    get_items=get_image_files, \n    # Create function that splits `items` between train/val with `valid_pct` randomly\n    splitter=RandomSplitter(\n        # Use 20% of data for validation set\n        valid_pct=0.2, \n        # Set random seed to get the same split across different training sessions\n        seed=42\n    ),\n    # Label `item` with the parent folder name\n    get_y=parent_label,\n    # Resize and crop image to 128x128\n    item_tfms=Resize(128))\n# Create a `DataLoaders` object from `path`\ndls = bears.dataloaders(path)\n# Show some samples from the validation set\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n# Create a new `DataBlock` that resizes and squishes images to 128x128\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n# Create a new `DataBlock` that pads each image to squares with black pixels and resizes to 128x128\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n# Create a new `DataBlock` that picks a random scaled crop of an image and resize it to 128x128\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\n# Show some unique random crops of a single sample from the validation set\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\n\n\n\nData Augmentation\n\nRefers to creating random variations of our input data, such that they appear different but do not change the meaning of the data\nCommon Types of Data Augmentation for Images\n\nrotation\nflipping\nperspective warping\nbrightness changes\ncontrast changes\n\n\n\n# Create a new `DataBlock` that crops and resizes each image to 128x128\n# and applies a list of data augmentations including flip, rotate, zoom, warp, lighting transforms\n# to each batch on the GPU\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n/home/innom-dt/miniconda3/envs/fastbook/lib/python3.9/site-packages/torch/_tensor.py:1051: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\ntorch.linalg.solve has its arguments reversed and does not return the LU factorization.\nTo get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\nX = torch.solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n  ret = func(*args, **kwargs)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#using-a-model-to-clean-your-data",
    "href": "posts/fastai-book-notes/chapter-2/index.html#using-a-model-to-clean-your-data",
    "title": "Notes on fastai Book Ch. 2",
    "section": "Using a Model to Clean Your Data",
    "text": "Using a Model to Clean Your Data\n\nCleaning data and getting it ready for your model are two of the biggest challenges for data scientists\n\nData scientists say it takes 90% of their time\n\nUsing the model for data cleaning\n\nTrain the model on the current dataset\nExamine the incorrectly classified images with the highest confidence score\n\nThere might be images that were incorrectly labeled\n\nExamine the incorrectly labeled images with the lowest confidence scores\n\nThere might be poor quality images in the training set\n\nMove any misplaced images to the correct folder\nRemove any poor quality images\nRetrain model on updated dataset\n\n\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.493479\n\n\n0.147736\n\n\n0.057471\n\n\n00:05\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.272248\n\n\n0.107368\n\n\n0.057471\n\n\n00:05\n\n\n\n\n1\n\n\n0.173436\n\n\n0.091117\n\n\n0.034483\n\n\n00:05\n\n\n\n\n2\n\n\n0.151810\n\n\n0.106020\n\n\n0.034483\n\n\n00:05\n\n\n\n\n3\n\n\n0.122778\n\n\n0.110871\n\n\n0.034483\n\n\n00:05\n\n\n\n\n\n\n\n# Contains interpretation methods for classification models\ninterp = ClassificationInterpretation.from_learner(learn)\n# Plot the confusion matrix\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\nVBox(children=(Dropdown(options=('black', 'grizzly', 'teddy'), value='black'), Dropdown(options=('Train', 'Val…\n\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#turn-your-model-into-an-online-application",
    "href": "posts/fastai-book-notes/chapter-2/index.html#turn-your-model-into-an-online-application",
    "title": "Notes on fastai Book Ch. 2",
    "section": "Turn Your Model into an Online Application",
    "text": "Turn Your Model into an Online Application\n\ninference: using a trained model to make predictions on new data\n\n\nCreating a Notebook App From the Model\n\nIPython Widgets\n\nGUI components that bring together JavaScript and Python functionality in a web browser\ncan be created and used within a Jupyter Notebook\n\nVoila\n\nA system for making applications consisting of IPython widgets available to end users without them having to use Jupyter\n\n\n\n\nDeploying Your App\n\nUse a CPU for inference when a GPU is not required\nNeed to be careful with managing GPU memory in production\nCPU inference is much cheaper than GPU\nThere are often free CPU servers available for demoing prototype application\nRun your model on a server instead of an edge device when possible"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#how-to-avoid-disaster",
    "href": "posts/fastai-book-notes/chapter-2/index.html#how-to-avoid-disaster",
    "title": "Notes on fastai Book Ch. 2",
    "section": "How to Avoid Disaster",
    "text": "How to Avoid Disaster\n\nA deep learning model will be just one piece of a larger production system\nBuilding a data product requires thinking about the end-to-end process, from conception to use in production\nManaging deployed data products\n\nManaging multiple versions of models\nA/B testing\nCanarying\nRefreshing the data\n\nShould we just continue adding to our datasets or should we regularly remove some of the old data?\n\nHandling data labeling\nMonitoring everything\nDetecting model rot\netc.\n\nBuilding Machine Learning Powered Applications\nUnderstanding and testing the behavior of a deep learning model is much more difficult than with most other code you write\n\nWith normal software development, you can analyze the exact steps that the software is taking\nWith a neural network, the behavior emerges from the model’s attempt to match the training data, rather than being exactly defined\n\nA common problems with training a models on images people upload to the internet\n\nThe kinds of photos people upload are the kinds of photos that do a good job of clearly and artistically displaying their subject matter\n\nThis is not the kind of input a system is most likely going to encounter\n\n\nOut-of-domain data\n\nThere may be data that our model sees in production that is very different from what it saw during training\nThere is not a complete technical solution to this problem\nNeed to be careful about our approach to rolling out the model\n\nDomain shift\n\nThe type of data that our model sees changes over time, making the original training data irrelevant\n\nYou can never fully understand all the possible behaviors of a neural network\n\nA natural downside to their inherent flexibility\n\n\n\nDeployment Process\n\nManual Process\n\nRun a model in parallel, but do not use directly to drive any actions\nHumans check all predictions\n\nlook at the deep learning outputs and check whether they make sense\n\n\nLimited scope deployment\n\nCareful human supervision\nTime or geography limited\n\nGradual Expansion\n\nGood reporting systems needed\n\nMake sure you are aware of any significant changes to the actions being taken compared to your manual process\n\nConsider what could go wrong\n\nThink about what measure or report or picture could reflect that problem and ensure that your regular reporting includes that information\n\n\n\n\n\nUnforeseen Consequences and Feedback Loops\n\nOne of the biggest challenges in rolling out a model is that your model may change the behavior of the system it is part of\nWhen bias is present, feedback loops can result in negative implications of that bias getting worse and worse\nQuestions to consider when rolling out a significant machine learning system\n\nWhat would happen if it went really, really well?\nWhat if the predictive power was extremely high and its ability to influence behavior extremely significant?\nWho would be most impacted?\nWhat would the most extreme results potentially look like?\nHow would you know what was really going on?\n\nMake sure that reliable and resilient communication channels exist so that the right people will be aware of issues and will have the power to fix them"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-2/index.html#references",
    "href": "posts/fastai-book-notes/chapter-2/index.html#references",
    "title": "Notes on fastai Book Ch. 2",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 1\nNext: Notes on fastai Book Ch. 3"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html",
    "href": "posts/fastai-book-notes/chapter-3/index.html",
    "title": "Notes on fastai Book Ch. 3",
    "section": "",
    "text": "Data Ethics\nKey Examples for Data Ethics\nIntegrating Machine Learning with Product Design\nTopics in Data Ethics\nIdentifying and Addressing Ethical Issues\nRole of Policy\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#data-ethics",
    "href": "posts/fastai-book-notes/chapter-3/index.html#data-ethics",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Data Ethics",
    "text": "Data Ethics\n\nSometimes machine learning models can go wrong\n\nthe can have bugs\nthey can be presented with data that they have not seen before and behave in ways we don’t expect\nthey can work exactly as designed but be used for malicious purposes\n\nthe power of deep learning makes it important for us to consider the consequences of our choices\n\n\nEthics\n\nthe philosophical study of right and wrong, including how we can define those terms, reognize right and wrong actions, and understand the connection between actions and consequences\nno one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good and bad, etc.\nWhat is Ethics? by Markkula Center for Applied Ethics\n\nthe term ethics refers to\n\nWell-founded standards of right and wrong that prescribe what humans should do\nThe study and development of one’s ethical standards\n\n\nethics is complicated and context-dependent\ninvolves the perspectives of many stakeholders\n\n\n\nWhat is Data Ethics?\n\na subfiled of ethics\nbeing used to define policy in many jurisdictions\nbeing used in companies to consider how best to ensure good societal outcomes from product development\nbeing used by researchers who want to make sure the work they are doing is used for good\ndeep learning practitioners will likely face situations where they need to consider data ethics"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#key-examples-for-data-ethics",
    "href": "posts/fastai-book-notes/chapter-3/index.html#key-examples-for-data-ethics",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Key Examples for Data Ethics",
    "text": "Key Examples for Data Ethics\n\nBugs and Recourse: Buggy Algorithm Used for Healthcare Benefits\n\nAkansas’s buggy healthcare system left patients stranded\nWhat happens when an algorithm cuts your health care\n\nThe Verge investigated software used in over half of the US states to determine how much healthcare people receive\nhundreds of people, many with severe disabilities, had their healthcare drastically cut.\n\na woman with cerebral-palsy how needs constant assistance had her hours of help suddenly reduced by 20 hours per week\n\nshe could not get any explanation for why\na court case revealed there were mistakes made in the software implementation of the algorithm\n\n\n\n\n\n\nFeedback Loops: YouTube’s Recommendation System\n\nfeedback loops can occur when your model is influences future input data\nYouTube’s recommendation system helped unleash a conspiracy theory boom\n\nThe algorithm was designed to optimize watch time\nresponsible for 70% of the content that is watched\n\nYouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?\nrecommendation systems have a lot of power over what people see\n\n\n\nBias: Professor Layanya Sweeney “Arrested”\n\nWhen a traditionally African American name is searched for on Google, is displays ads for criminal background checks\nDr. Latanya Sweeney is a professor at Harvard and director of the university’s data privacy lab\n\ndiscovered that Googling her name resulted in adverstisements saying “Latanya Sweeney Arrested?”, despite being the only Latanya Sweeney and never having been arrested\ndiscovered that historically Black names received advertisements suggesting the person had a criminal record\n\n\n\n\nWhy Does This Matter?\n\neverybody who is training models needs to consider how their models will be used and consider how to ensure they are used positively\n\nHow would you feel if you discovered that you had been part of system that ended up hurting society?\n\nWould you be open to finding out?\nHow can you help make sure this doesn’t happen?\n\n\nIBM and Nazi Germany\n\nIBM and the Holocaust by Edwin Black\n\n“To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM’s technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.”\n\nIBM supplied the Nazis with the data tabulation products used to track the extermination of Jews and other groups on a massive scale\n\nThis was drive from the top of the company, with marketing to Hitler and his leadership team\nCompany President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews\nHitler awared Company President Thomas Watson with a special “Service to the Reich” medal in 1937\nIBM and its subsidiaries provied regular training and maintenance onsite at the concentration camps\n\nprinting off cards\nconfiguring machines\nrepariing machines\n\nIBM set up categorizations on its punch card system for\n\nthe way each person was killed\nwhic group they were assigned to\nthe logistical information necessary to track them through the vast Holocaust system\n\n\n\nVolkswagen Diesel Scandal\n\nthe care company was revealed to have cheated on its diesel emissions tests\nthe first person who was jailed was one of the engineers who just did what he was told\n\nData scientists need to consider the entire pipeline of steps that occurs between the development of a model and the point at which the model is used to make a decision\n\ninform everyone involved in this chain about the capabilities, constraints, and details of your work\nmake sure the right issues are being considered\n\nneed to know when to refuse to do a piece of work"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#integrating-machine-learning-with-product-design",
    "href": "posts/fastai-book-notes/chapter-3/index.html#integrating-machine-learning-with-product-design",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Integrating Machine Learning with Product Design",
    "text": "Integrating Machine Learning with Product Design\n\nLots of decisions are involved when collecting your data and developing your model\n\nWhat level of aggregation will you store your data at?\nWhat loss function should you use?\nWhat validation and training sets should you use?\nShould you focus on simplicity of implementation, speed of inference, or accuracy of the model?\nHow will your model handle out-of-domain data items?\nCan it be fine-tuned, or must it be retrained from scratch over time?\n\nWhoever ends up developing and using the system that implements your model will not be well-placed to understand the decisions you made when training the model\nData scientists need to be part of a tightly integrated, cross-disciplinary team\nResearchers need to work closely with the kinds of people who will end up using their research.\nIdeally, domain experts should learn enough to be able to train and debug some models themselves"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#topics-in-data-ethics",
    "href": "posts/fastai-book-notes/chapter-3/index.html#topics-in-data-ethics",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Topics in Data Ethics",
    "text": "Topics in Data Ethics\n\nRecourse and Accountability\n\na lack of responsibility and accountability leads to bad results\nmechanisms for data audits and error corrections are crucial\n\ndata often contains errors\n\nmachine learning practitioners have a responsibility to understand how their algorithms end up being implemented in practice\n\n\n\nFeedback Loops\n\nbe aware of the centrality of metrics in driving a fincancially important system\n\nan algorithm will do everything it can to optimize a target metric\n\ncan lead to unforeseen edge cases\nhumans ineracting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage\n\ntransparancy is important for uncovering problems\n\nthere can also be feedback loops without humans\nconsider what data features and metrics might lead to feedback loops\n\nthe most optimal algorithm might not be the best one to deploy to production\n\nhave mechanisms in place to spot runaway feedback loops and tak positive steps to break them when they occur\n\n\n\nBias\n\nA Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle\nSix Types of Bias in Machine Learning\n\nHistorical Bias\n\nbias from people, processes, and society\nhistorical bias is a fundamental, structural issue with the first step of the data generation process\ncan exist even given perfect sampling and feature selection\nany dataset involving humans can have historical bias\n\nmedical data\nsales data\nhousing data\npolitical data\n\nfixing problems in machine learning systems is hard when the input data has problems\nunder representation of certain groups in the training data can result in a model that performs worse for those subgroups\n\nNo Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\n\nthe vast majority of images in popular benchmark datasets like ImageNet were found to be from the US and other Western countries\n\nDoes Object Recognition Work for Everyone?\n\nobject recognition models trained on popular benckmark datasets performed poorly on items from lower-income countries\n\n\n\nRepresentation Bias\n\nBias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting\nmodels can not only reflect representation imbalances, but also amplify them\n\nMeasurement Bias\n\nbias from measuring the wrong thing, measuring something in the wrong way, or incorporating a measurement into the model innapropriately\nDoes Machine Learning Automate Moral Hazard and Error?\n\nAggregation Bias\n\noccurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interactions terms, nonlinearities, or so forth\nlikely to occur in medical settings\n\nEvaluation Bias\nDeployment Bias\n\n\n\n\nAddressing different types of bias\n\ndifferent types of bias require different approaches for mitigations\nCreate better documentation\n\ndecisions\ncontext\nspecifics about how and why a particular dataset was created\nwhat scenerios it is appropriate to use in\nwhat the limitations are\n\nAlgorithms are used differently than human decision makers\n\nin practice, machine learning is often implemented because it is cheaper and more efficient, not because it leads to better outcomes\nWeapons of Math Destruction\n\ndescribes a pattern in which the priviledged are processed by people, whereas the poor are processed by algorithms\n\nPeople are more likely to assume algorithms are objective or error-free\nAlgorithms are more likely to be implemented with no appeals process in place\nAlgorithms are often used at scale\nAlgorithmic systems are often cheap\n\n\n\n\nDisinformation\n\noften used to sow disharmony and uncertainty, and to get people to give up on seeking the truth\n\nreceiving conflicting accounts can lead people to assume they can’t trust anyting\n\noften contains seeds of truth, or half truths taken out of context\nMost propaganda campaigns are a carefully designed mixture of facts, half-truths, exagerrations, and deliberate lies.\n\nA Houston protest, organized by Russian trolls\n\noften involves coordinated campaigns of inauthentic behavior\n\nfraufulent accounts may try to make it seem like many people hold a particular viewpoint\n\ndisinformation through autogenerated texsst is a significant issue enabled by deep learning models\none proposed solution is to develope some form of digital signiture\nHow Will We Prevent AI-Based Forgery?\n\n“AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society.”"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#identifying-and-addressing-ethical-issues",
    "href": "posts/fastai-book-notes/chapter-3/index.html#identifying-and-addressing-ethical-issues",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Identifying and Addressing Ethical Issues",
    "text": "Identifying and Addressing Ethical Issues\n\nmake finding and dealing with mistakes part of the design of any system that includes machine learning\n\n\nAnalyze a Project You Are Working On\n\nShould we even be doing this?\nWhat bias is in the data?\nCan the code be audited?\nWhat are the error rates for different subgroups?\nWhat is the accuracy of a simple rule-based alternative?\nWhat processes are in place to handle appeals or mistakes?\nHow diverse is the team that built it?\nWhat data are you collecting and storing?\n\ndata often ends up being used for different purposes than the original intent\n\ncensus data has been used to target minorities\nHow Capitalism Betrayed Privacy\n\n\n\n\n\nImplement processes at your company to find and address ethical risks\n\nAn Ethical Toolkit for Engineering/Design Practice\n\nincludes concrete practices to implement at your company\n\nregularly scheduled sweeps to proactively search for ethical risks\nexpanding the ethical circle to include the perspectives of a variety of stakeholders\nconsidering how bad actors can abuse, steal, misinterpret, hack, destroy, or weaponize what your are building\n\nWhose interests, desires, skills, experiences, and values have we simply assumed, rather than actually consulted?\nWho are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are?\nWho/which groups and individuals will be indirectly affected in significant ways?\nWho might use this product the we did not expect to use it, or for purposes we did not initially intend?\n\n\nEthical lenses\n\ndifferent foundational ethical lenses can help identify concrete issues\nThe rights approach\n\nWhich option best respects the rights of all who have a stake?\n\nThe justics approach\n\nWhich option treats people equally or proportionately?\n\nThe utilitarian approach\n\nWhich option will produce the most good and do the least harm?\n\nThe common good approach\n\nWhich option best serves the community as a whole, not just some members?\n\nThe virtue approach\n\nWhich option leads me to act as the sort of person I want to be?\n\nConsquences\n\nWho will be directly affected bt this project? Who will be indirectly affected?\nWill the effects in aggregate likely create more good than harm, and what types of good and harm?\nAre we thinking about all relevant types of harm/benefit?\nHow might future generations be affected by this project?\nDo the risks of harm from this project fall disproportionately on the least powerful? Will the benefits go disproportionately go to the well-off?\nHave we adequately considered “dual-use” and unintended downstream effects?\n\nDeontological perspective\n\nWhat rights of other and duties to others must we respect?\nHow might the dignity and autonomy of each stakeholder be impacted by this project?\nWhat considerations of trust and of justice are relevant to this design/project?\nDoes this project involve any conflicting moral duties to other, or conflicting stakeholder rights? Howe can we prioritize these?\n\n\n\n\nThe Power of Diversity\n\nteam members from similar backgrounds are likely to have similar blindspots around ethical risks\ndiversity can lead to problems being identified earlie and a wider range of solutions being considered\n\n\n\nFairness, Accountability, and Transparency\n\nFairness and machine learning book\n\ngives a perspective on machine learning that treats fairness as a central concern rather than an afterthought\n\nExercise\n\nCome up with a process, definition, set of questions, etc. which is designed to resolve a problem\nTry to come up with an example in which that apparant solution results in a proposal that no one would consider acceptable\nThis can then lead to further refinement of the solution"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#role-of-policy",
    "href": "posts/fastai-book-notes/chapter-3/index.html#role-of-policy",
    "title": "Notes on fastai Book Ch. 3",
    "section": "Role of Policy",
    "text": "Role of Policy\n\npurely technical solutions are not sufficient to address the underlying problems that have led to our current state.\n\nas long as it is profitable to create addictive technology, companies will continue to do so\n\n\n\nThe Effectiveness of Regulation\n\ncorporations are more reactive to the threat of significant financial penalty than to the systematic destruction of an ethnic minority\n\n\n\nRights and Policy\n\nmany harms resulting from unintended consequences of misuses of technology involve public goods, such as a polluted information environment or deteriorated ambient privacy\nthere are societal impacts to widespread survilance\nhuman rights issues need to be addressed through law\n\n\n\nCars: A Historical Precedent\n\nThe movement to increase car safety sets a historical precedent for addressing problems in data ethics\nDatasheets for Datasets\nThe Nut Behind the Wheel"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-3/index.html#references",
    "href": "posts/fastai-book-notes/chapter-3/index.html#references",
    "title": "Notes on fastai Book Ch. 3",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 2\nNext: Notes on fastai Book Ch. 4"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html",
    "href": "posts/fastai-book-notes/chapter-4/index.html",
    "title": "Notes on fastai Book Ch. 4",
    "section": "",
    "text": "Tenacity and Deep Learning\nThe Foundations of Computer Vision\nPixels\nPixel Similarity\nComputing Metrics Using Broadcasting\nStochastic Gradient Descent\nThe MNIST Loss Function\nPutting It All Together\nAdding a Nonlinearity\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#tenacity-and-deep-learning",
    "href": "posts/fastai-book-notes/chapter-4/index.html#tenacity-and-deep-learning",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Tenacity and Deep Learning",
    "text": "Tenacity and Deep Learning\n\nDeep learning practitioners need to be tenacious\nOnly a handful of researchers kept trying to make neural networks work through the 1990s and 2000s.\n\nYann Lecun, Yoshua Bengio, and Geoffrey Hinton were not awarded the Turing Award until 2018\n\nAcademic Papers for neural networks were rejected by top journals and conferences, despite showing dramatically better results than anything previously published\nJurgen Schmidhuber\n\npioneered many important ideas\nworked with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture\n\nLSTMs are now widely used for speech recognition and other text modelling tasks\n\n\nPaul Werbos\n\nInvented backpropagation for neural networks in 1974\n\nconsidered the most important foundation of modern AI"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#the-foundations-of-computer-vision",
    "href": "posts/fastai-book-notes/chapter-4/index.html#the-foundations-of-computer-vision",
    "title": "Notes on fastai Book Ch. 4",
    "section": "The Foundations of Computer Vision",
    "text": "The Foundations of Computer Vision\n\nMNIST Database\n\ncontains images of handwritten digits, collected by the National Institute of Standards and Technology\ncreated in 1998\n\nLeNet-5\n\nA convolutional neural network structure proposed by Yann Lecun and his colleagues\nDemonstrated the first practically useful recognition of handwritten digit sequences in 1998\nOne of the most important breakthroughs in the history of AI"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#pixels",
    "href": "posts/fastai-book-notes/chapter-4/index.html#pixels",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Pixels",
    "text": "Pixels\n\nMNIST_SAMPLE\n\nA sample of the famous MNIST dataset consisting of handwritten digits.\ncontains training data for the digits 3 and 7\nimages are in 1-dimensional grayscale format\nalready split into training and validation sets\n\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n\nprint(URLs.MNIST_SAMPLE)\npath = untar_data(URLs.MNIST_SAMPLE)\nprint(path)\nhttps://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz\n/home/innom-dt/.fastai/data/mnist_sample\n\n# Set base path to mnist_sample directory\nPath.BASE_PATH = path\n\n# A custom fastai method that returns the contents of path as a list\npath.ls()\n(#3) [Path('labels.csv'),Path('train'),Path('valid')]\n\nfastcore L Class\n\nhttps://fastcore.fast.ai/foundation.html#L\nBehaves like a list of items but can also index with list of indices or masks\nDisplays the number of items before printing the items\n\n\ntype(path.ls())\nfastcore.foundation.L\n\n(path/'train').ls()\n(#2) [Path('train/3'),Path('train/7')]\n\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\n\nim3_path = threes[1]\nprint(im3_path)\nim3 = Image.open(im3_path)\nim3\n/home/innom-dt/.fastai/data/mnist_sample/train/3/10000.png\n\n\n\n\n\n\n\n\nPIL Image Module\n\nhttps://pillow.readthedocs.io/en/stable/reference/Image.html\nprovides a class with the same name which is used to represent a PIL image\nprovides a number of factory functions, including functions to load images from files, and to create new images\n\n\nprint(type(im3))\nprint(im3.size)\n&lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;\n(28, 28)\n\n# Slice of the image from index 4 up to, but not including, index 10\narray(im3)[4:10,4:10]\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\nNumPy Arrays and PyTorch Tensors\n\nNumPy\n\nthe most widely used library for scientific and numeric programming in Python\ndoes not support using GPUs or calculating gradients\n\nPython is slow compared to many languages\n\nanything fast in Python is likely to be a wrapper for a compiled object written and optimized in another language like C\nNumPy arrays and PyTorch tensors can finish computations many thousands of times than using pure Python\n\nNumPy array\n\na multidimensional table of data\nall items are the same type\ncan use any type, including arrays, for the array type\nsimple types are stored as a compact C data structure in memory\n\nPyTorch tensor\n\nnearly identical to NumPy arrays\ncan only use a single basic numeric type for all elements\nnot as flexible as a genuine array of arrays\n\nmust always be a regularly shaped multi-dimensional rectangular structure\n\ncannot be jagged\n\n\nsupports using GPUs\nPyTorch can automatically calculate derivatives of operations performed with tensors\n\nimpossible to do deep learning without this capability\n\n\nperform operations directly on arrays or tensors as much as possible instead of using loops\n\n\ndata = [[1,2,3],[4,5,6]]\narr = array (data)\ntns = tensor(data)\n\narr  # numpy\narray([[1, 2, 3],\n       [4, 5, 6]])\n\ntns  # pytorch\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n# select a row\ntns[1]\ntensor([4, 5, 6])\n\n# select a column\ntns[:,1]\ntensor([2, 5])\n\n# select a slice\ntns[1,1:3]\ntensor([5, 6])\n\n# Perform element-wise addition\ntns+1\ntensor([[2, 3, 4],\n        [5, 6, 7]])\n\ntns.type()\n'torch.LongTensor'\n\n# Perform element-wise multiplication\ntns*1.5\ntensor([[1.5000, 3.0000, 4.5000],\n        [6.0000, 7.5000, 9.0000]])\n\n\nNumPy Array Objects\n\nhttps://numpy.org/doc/stable/reference/arrays.html\nan N-dimensional array type, the ndarray, which describes a collection of “items” of the same type\n\n\nnumpy.array function\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.array.html\ncreates an array\n\n\nprint(type(array(im3)[4:10,4:10]))\narray\n&lt;class 'numpy.ndarray'&gt;\n&lt;function numpy.array&gt;\n\nprint(array(im3)[4:10,4:10][0].data)\nprint(array(im3)[4:10,4:10][0].dtype)\n&lt;memory at 0x7f3c13a20dc0&gt;\nuint8\n\n\n\nPyTorch Tensor\n\nhttps://pytorch.org/docs/stable/tensors.html\na multi-dimensional matrix containing elements of a single data type\n\n\nfastai tensor function\n\nhttps://docs.fast.ai/torch_core.html#tensor\nLike torch.as_tensor, but handle lists too, and can pass multiple vector elements directly.\n\n\nprint(type(tensor(im3)[4:10,4:10][0]))\ntensor\n&lt;class 'torch.Tensor'&gt;\n&lt;function fastai.torch_core.tensor(x, *rest, dtype=None, device=None, requires_grad=False, pin_memory=False)&gt;\n\nprint(tensor(im3)[4:10,4:10][0].data)\nprint(tensor(im3)[4:10,4:10][0].dtype)\ntensor([0, 0, 0, 0, 0, 0], dtype=torch.uint8)\ntorch.uint8\n\n\n\nPandas DataFrame\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data\n\n\n# Full Image\npd.DataFrame(tensor(im3))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n29\n\n\n150\n\n\n195\n\n\n254\n\n\n255\n\n\n254\n\n\n176\n\n\n193\n\n\n150\n\n\n96\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n48\n\n\n166\n\n\n224\n\n\n253\n\n\n253\n\n\n234\n\n\n196\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n233\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n7\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n93\n\n\n244\n\n\n249\n\n\n253\n\n\n187\n\n\n46\n\n\n10\n\n\n8\n\n\n4\n\n\n10\n\n\n194\n\n\n253\n\n\n253\n\n\n233\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n107\n\n\n253\n\n\n253\n\n\n230\n\n\n48\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n192\n\n\n253\n\n\n253\n\n\n156\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n20\n\n\n20\n\n\n15\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n43\n\n\n224\n\n\n253\n\n\n245\n\n\n74\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n10\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n249\n\n\n253\n\n\n245\n\n\n126\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n11\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n14\n\n\n101\n\n\n223\n\n\n253\n\n\n248\n\n\n124\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n12\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n11\n\n\n166\n\n\n239\n\n\n253\n\n\n253\n\n\n253\n\n\n187\n\n\n30\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n13\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n16\n\n\n248\n\n\n250\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n232\n\n\n213\n\n\n111\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n14\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n43\n\n\n98\n\n\n98\n\n\n208\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n187\n\n\n22\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n15\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n9\n\n\n51\n\n\n119\n\n\n253\n\n\n253\n\n\n253\n\n\n76\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n16\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n183\n\n\n253\n\n\n253\n\n\n139\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n17\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n182\n\n\n253\n\n\n253\n\n\n104\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n18\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n85\n\n\n249\n\n\n253\n\n\n253\n\n\n36\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n19\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n60\n\n\n214\n\n\n253\n\n\n253\n\n\n173\n\n\n11\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n20\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n98\n\n\n247\n\n\n253\n\n\n253\n\n\n226\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n21\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n42\n\n\n150\n\n\n252\n\n\n253\n\n\n253\n\n\n233\n\n\n53\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n22\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n42\n\n\n115\n\n\n42\n\n\n60\n\n\n115\n\n\n159\n\n\n240\n\n\n253\n\n\n253\n\n\n250\n\n\n175\n\n\n25\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n23\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n187\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n197\n\n\n86\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n24\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n103\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n232\n\n\n67\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n25\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n26\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n27\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n\ntensor(im3).shape\ntorch.Size([28, 28])\n\nim3_t = tensor(im3)\n# Create a pandas DataFrame from image slice\ndf = pd.DataFrame(im3_t[4:15,4:22])\n# Set defined CSS-properties to each ``&lt;td&gt;`` HTML element within the given subset.\n# Color-code the values using a gradient\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n29\n\n\n150\n\n\n195\n\n\n254\n\n\n255\n\n\n254\n\n\n176\n\n\n193\n\n\n150\n\n\n96\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n48\n\n\n166\n\n\n224\n\n\n253\n\n\n253\n\n\n234\n\n\n196\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n233\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n93\n\n\n244\n\n\n249\n\n\n253\n\n\n187\n\n\n46\n\n\n10\n\n\n8\n\n\n4\n\n\n10\n\n\n194\n\n\n253\n\n\n253\n\n\n233\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n107\n\n\n253\n\n\n253\n\n\n230\n\n\n48\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n192\n\n\n253\n\n\n253\n\n\n156\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n0\n\n\n3\n\n\n20\n\n\n20\n\n\n15\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n43\n\n\n224\n\n\n253\n\n\n245\n\n\n74\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n249\n\n\n253\n\n\n245\n\n\n126\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n7\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n14\n\n\n101\n\n\n223\n\n\n253\n\n\n248\n\n\n124\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n11\n\n\n166\n\n\n239\n\n\n253\n\n\n253\n\n\n253\n\n\n187\n\n\n30\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n16\n\n\n248\n\n\n250\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n232\n\n\n213\n\n\n111\n\n\n2\n\n\n0\n\n\n0\n\n\n\n\n10\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n43\n\n\n98\n\n\n98\n\n\n208\n\n\n253\n\n\n253\n\n\n253\n\n\n253\n\n\n187\n\n\n22\n\n\n0"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#pixel-similarity",
    "href": "posts/fastai-book-notes/chapter-4/index.html#pixel-similarity",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Pixel Similarity",
    "text": "Pixel Similarity\n\nEstablish a baseline to compare against your model\n\na simple model that you are confident should perform reasonably well\nshould be simple to implement and easy to test\nhelps indicate whether your super-fancy models are any good\n\n\n\nMethod\n\nCalculate the average values for each pixel location across all images for each digit\n\nThis will generate a blurry image of the target digit\n\nCompare the values for each pixel location in a new image to the average\n\n\n# Store all images of the digit 7 in a list of tensors\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\n# Store all iamges of the digit 3 in a list of tensors\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors),len(seven_tensors)\n(6131, 6265)\n\nfastai show_image function\n\nhttps://docs.fast.ai/torch_core.html#show_image\nDisplay tensor as an image\n\n\nshow_image(three_tensors[1]);\n\n\n\n\n\n\n\nPyTorch Stack Function\n\nhttps://pytorch.org/docs/stable/generated/torch.stack.html\nConcatenates a sequence of tensors along a new dimension\n\n\n# Stack all images for each digit into a single tensor\n# and scale pixel values from the range [0,255] to [0,1]\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\ntorch.Size([6131, 28, 28])\n\n\n\n\n\n\npython len(stacked_threes.shape)\n\n\n\nstacked_threes.ndim\n3\n\n# Calculate the mean values for each pixel location across all images of the digit 3\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\n\n\n\n\n# Calculate the mean values for each pixel location across all images of the digit 7\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\n\n\n# Pick a single image to compare to the average\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\n\n\n# Calculate the Mean Absolute Error between the single image and the mean pixel values\ndist_3_abs = (a_3 - mean3).abs().mean()\n# Calculate the Root Mean Squared Error between the single image and the mean pixel values\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\nprint(f\"MAE: {dist_3_abs}\")\nprint(f\"RMSE: {dist_3_sqr}\")\nMAE: 0.11143654584884644\nRMSE: 0.20208320021629333\nKhan Academy: Understanding Square Roots\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\nprint(f\"MAE: {dist_7_abs}\")\nprint(f\"RMSE: {dist_7_sqr}\")\nMAE: 0.15861910581588745\nRMSE: 0.30210891366004944\n\nNote: The error is larger when comparing the image of a 3 to the average pixel values for the digit 7\n\n\n\ntorch.nn.functional\n\nhttps://pytorch.org/docs/stable/nn.functional.html\nProvides access to a variety of functions in PyTorch\n\n\nF\n&lt;module 'torch.nn.functional' from '/home/innom-dt/miniconda3/envs/fastbook/lib/python3.9/site-packages/torch/nn/functional.py'&gt;\n\n\nPyTorch l1_loss function\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss\ntakes the mean element-wise absolute value difference\n\n\n\nPyTorch mse_loss function\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss\nMeasures the element-wise mean squared error\nPenalizes bigger mistakes more heavily\n\n\n# Calculate the Mean Absolute Error aka L1 norm\nprint(F.l1_loss(a_3.float(),mean7))\n# Calculate the Root Mean Squared Error aka L2 norm\nprint(F.mse_loss(a_3,mean7).sqrt())\ntensor(0.1586)\ntensor(0.3021)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#computing-metrics-using-broadcasting",
    "href": "posts/fastai-book-notes/chapter-4/index.html#computing-metrics-using-broadcasting",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\nbroadcasting\n\nautomatically expanding a tensor with a smaller rank to have the same size one with a larger rank to perform an operation\nan important capability that makes tensor code much easier to write\nPyTorch does not allocate additional memory for broadcasting\n\nit does not actually create multiple copies of the smaller tensor\n\nPyTorch performs broadcast calculations in C on the CPU and CUDA on the GPU\n\ntens of thousands of times faster than pure Python\nup to millions of times faster on GPU\n\n\n\n\n# Create tensors for the validation set for the digit 3\n# and stack them into a single tensor\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\n# Scale pixel values from [0,255] to [0,1]\nvalid_3_tens = valid_3_tens.float()/255\n\n# Create tensors for the validation set for the digit 7\n# and stack them into a single tensor\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\n# Scale pixel values from [0,255] to [0,1]\nvalid_7_tens = valid_7_tens.float()/255\n\nvalid_3_tens.shape,valid_7_tens.shape\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n# Calculate Mean Absolute Error using broadcasting\n# Subtraction operation is performed using broadcasting\n# Absolute Value operation is performed elementwise\n# Mean operation is performed over the values indexed by the height and width axes\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n# Calculate MAE for two single images\nmnist_distance(a_3, mean3)\ntensor(0.1114)\n\n# Calculate MAE between a single image and a vector of images\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n(tensor([0.1422, 0.1230, 0.1055,  ..., 0.1244, 0.1188, 0.1103]),\n torch.Size([1010]))\n\ntensor([1,2,3]) + tensor([1,1,1])\ntensor([2, 3, 4])\n\n(valid_3_tens-mean3).shape\ntorch.Size([1010, 28, 28])\n\n# Compare the MAE value between the single and the mean values for the digits 3 and 7\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\nis_3(a_3), is_3(a_3).float()\n(tensor(True), tensor(1.))\n\nis_3(valid_3_tens)\ntensor([ True,  True,  True,  ..., False,  True,  True])\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\nprint(f\"Correct 3s: {accuracy_3s * valid_3_tens.shape[0]:.0f}\")\nprint(f\"Incorrect 3s: {(1 - accuracy_3s) * valid_3_tens.shape[0]:.0f}\")\nCorrect 3s: 926\nIncorrect 3s: 84\n\nprint(f\"Correct 7s: {accuracy_7s * valid_7_tens.shape[0]:.0f}\")\nprint(f\"Incorrect 7s: {(1 - accuracy_7s) * valid_7_tens.shape[0]:.0f}\")\nCorrect 7s: 1013\nIncorrect 7s: 15"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#stochastic-gradient-descent",
    "href": "posts/fastai-book-notes/chapter-4/index.html#stochastic-gradient-descent",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nthe key to having a model that can improve\nneed to represent a task such that their are weight assignments that can be evaluated and updated\nSample function:\n\nassign a weight value to each pixel location\nX is the image represented as a vector\n\nall of the rows are stacked up end to end into a single long line\n\nW contains the weights for each pixel\n\n\n\ndef pr_eight(x,w) = (x*w).sum()\n\n\n\ndef f(x): return x**2\n\nplot_function\n\nhttps://github.com/fastai/fastbook/blob/e57e3155824c81a54f915edf9505f64d5ccdad84/utils.py#L70\n\n\nplot_function(f, 'x', 'x**2')\n\n\n\n\n\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\n\n\n\n\n\nCalculating Gradients\n\nthe gradients tell us how much we need to change each weight to make our model better\n\n\\(\\frac{rise}{run} = \\frac{the \\ change \\ in \\ value \\ of \\ the \\ function}{the \\ change \\ in \\ the \\ value \\ of \\ the \\ parameter}\\)\n\nderivative of a function\n\ntells you how much a change in its parameters will change its result\nKhan Academy: Basic Derivatives\n\nwhen we know how our function will change, we know how to make it smaller\n\nthe key to machine learning\n\nPyTorch is able to automatically compute the derivative of nearly any function\nThe gradient only tells us the slope of the function\n\nit does not indicate exactly how far to adjust the parameters\nif the slope is large, more adjustments may be required\nif the slope is small, we may be close to the optimal value\n\n\n\nTensor.requires_grad\n\nhttps://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html\nis True if gradients need to be computed for the Tensor\nhere gradient refers to the value of a function’s derivative at a particular argument value\nThe PyTorch API puts the focus onto the argument, not the function\n\n\nxt = tensor(3.).requires_grad_()\n\nyt = f(xt)\nyt\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\nTensor.grad_fn\n\nhttps://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html#tensors-that-track-history\nreferences a function that has created a function\n\n\nyt.grad_fn\n&lt;PowBackward0 at 0x7f91e90a6670&gt;\n\n\nTensor.backward()\n\nhttps://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward\nComputes the gradient of current tensor w.r.t. graph leaves.\n\nuses the chain rule\n\nbackward refers to backpropagation\n\nthe process of calculating the derivative for each layer\n\n\n\nyt.backward()\nThe derivative of f(x) = x**2 is 2x, so the derivative at x=3 is 6\nxt.grad\ntensor(6.)\nDerivatives should be 6, 8, 20\nxt = tensor([3.,4.,10.]).requires_grad_()\nxt\ntensor([ 3.,  4., 10.], requires_grad=True)\n\ndef f(x): return (x**2).sum()\n\nyt = f(xt)\nyt\ntensor(125., grad_fn=&lt;SumBackward0&gt;)\n\nyt.backward()\nxt.grad\ntensor([ 6.,  8., 20.])\n\n\n\nStepping with a Learning Rate\n\nnearly all approaches to updating model parameters start with multiplying the gradient by some small number called the learning rate\nLearning rate is often a number between 0.001 and 0.1\n\ncould be value\n\nstepping: adjusting your model parameters\n\nsize of step is determined by the learning rate\npicking a learning rate that is too small means more steps are needed to reach the optimal parameter values\npicking a learning rate that is too big can result in the loss getting worse or bouncing around the same range of values\n\n\n\n\nAn End-to-End SGD Example\n\nSteps to turn function into classifier\n\nInitialize the weights\n\ninitialize parameters to random values\n\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (it loss)\n\n“testing the effectiveness of any current weight assignment in terms of actual performance”\nneed a function that will return a number that is small when performance is good\nstandard convention is to treat a small loss as good and a large loss as bad\n\nCalculate the gradient, which measures for each weight how changing that weight would change the loss\n\nuse calculus to determine whether to increase or decrease individual weight values\n\nStep (update) the weights based on that calculation\nGo back to step 2 and repeat the process\nIterate until you decide to stop the training process\n\nuntil either the model is good enough, the model accuracy starts to decrease or you don’t want to wait any longer\n\n\n\nScenario: build a model of how the speed of a rollercoaster changes over time\n\ntorch.arange()\n\nhttps://pytorch.org/docs/stable/generated/torch.arange.html?highlight=arange#torch.arange\nReturns a 1-D tensor of size \\(\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\\) with values from the interval [start, end) taken with common difference step beginning from start.\n\n\ntime = torch.arange(0,20).float();\nprint(time)\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n\n\ntorch.randn()\n\nhttps://pytorch.org/docs/stable/generated/torch.randn.html?highlight=randn#torch.randn\nReturns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution)\n\n\n\nmatplotlib.pyplot.scatter()\n\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\nA scatter plot of y vs. x with varying marker size and/or color.\n\n\n# Add some random noise to mimic manually measuring the speed\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\n\n\n\n\n# A quadratic function with trainable parameters\ndef f(t, params):\n    a,b,c = params\n    return a*(t**2) + (b*t) + c\n\ndef mse(preds, targets): return ((preds-targets)**2).mean().sqrt()\n\n\nStep 1: Initialize the parameters\n# Initialize trainable parameters with random values\n# Let PyTorch know that we want to track the gradients\nparams = torch.randn(3).requires_grad_()\nparams\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n#hide\norig_params = params.clone()\n\n\nStep 2: Calculate the predictions\npreds = f(time, params)\nprint(preds.shape)\npreds\ntorch.Size([20])\ntensor([ 1.3525e+00, -1.6391e-01, -3.2121e+00, -7.7919e+00, -1.3903e+01, -2.1547e+01, -3.0721e+01, -4.1428e+01, -5.3666e+01, -6.7436e+01, -8.2738e+01, -9.9571e+01, -1.1794e+02, -1.3783e+02,\n        -1.5926e+02, -1.8222e+02, -2.0671e+02, -2.3274e+02, -2.6029e+02, -2.8938e+02], grad_fn=&lt;AddBackward0&gt;)\n\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\nshow_preds(preds)\n\n\n\n\n\n\n\nStep 3: Calculate the loss\n\ngoal is to minimize this value\n\n\nloss = mse(preds, speed)\nloss\ntensor(160.6979, grad_fn=&lt;SqrtBackward0&gt;)\n\n\nStep 4: Calculate the gradients\nloss.backward()\nparams.grad\ntensor([-165.5151,  -10.6402,   -0.7900])\n\n# Set learning rate to 0.00001\nlr = 1e-5\n\n# Multiply the graients by the learning rate\nparams.grad * lr\ntensor([-1.6552e-03, -1.0640e-04, -7.8996e-06])\n\nparams\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\nStep 5: Step the weights.\n# Using a learning rate of 0.0001 for larger steps\nlr = 1e-4\n# Update the parameter values\nparams.data -= lr * params.grad.data\n# Reset the computed gradients\nparams.grad = None\n\n# Test the updated parameter values\npreds = f(time,params)\nmse(preds, speed)\ntensor(157.9476, grad_fn=&lt;SqrtBackward0&gt;)\n\nshow_preds(preds)\n\n\n\n\n\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n\nStep 6: Repeat the process\nfor i in range(10): apply_step(params)\n157.9476318359375\n155.1999969482422\n152.45513916015625\n149.71319580078125\n146.97434997558594\n144.23875427246094\n141.50660705566406\n138.77809143066406\n136.05340576171875\n133.33282470703125\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\nMany steps later…\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\n\n\nStep 7: Stop\n\nWatch the training and validation losses and our metrics to decide when to stop\n\n\n\n\nSummarizing Gradient Descent\n\nInitial model weights can be randomly initialized or from a pretrained model\nCompare the model output with our labeled training data using a loss function\nThe loss function returns a number that we want to minimize by improving the model weights\nWe change the weights a little bit to make the model slightly better based on gradients calculated using calculus\n\nthe magnitude of the gradients indicate how big of a step needs to be taken\n\nMultiply the gradients by a learning rate to control how big of a change to make for each update\nIterate"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#the-mnist-loss-function",
    "href": "posts/fastai-book-notes/chapter-4/index.html#the-mnist-loss-function",
    "title": "Notes on fastai Book Ch. 4",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\nKhan Academy: Intro to Matrix Multiplication\nAccuracy is not useful as a loss function\n\naccuracy only changes when prediction changes from a 3 to a 7 or vice versa\nits derivative is 0 almost everywhere\n\nneed a loss function that gives a slightly better loss when our weights result in slightly better prediction\n\n\ntorch.cat()\n\nhttps://pytorch.org/docs/stable/generated/torch.cat.html\nConcatenates a given sequence of tensors in the specified dimension\nAll tensor must have the same shape except in the specified dimension\n\n\n\nTensor.view()\n\nhttps://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view\nReturns a new tensor with the same data as the self tensor but of a different shape.\n\n\n# 1. Concatenate all independent variables into a single tensor\n# 2. Flatten each image matrix into a vector\n#    -1: auto adjust axis to maintain fit all the data \ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\ntrain_x.shape\ntorch.Size([12396, 784])\n\n# Label 3s as `1` and label 7s as `0`\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_y.shape\ntorch.Size([12396, 1])\n\n# Combine independent and dependent variables into a dataset\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n(torch.Size([784]), tensor([1]))\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\n# Randomly initialize parameters\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n# Initialize weight values\nweights = init_params((28*28,1))\n\n# Initialize bias values\nbias = init_params(1)\n\n# Calculate a prediction for a single image\n(train_x[0]*weights.T).sum() + bias\ntensor([-6.2330], grad_fn=&lt;AddBackward0&gt;)\n\n\nMatrix Multiplication\n# Matrix multiplication using loops\ndef mat_mul(m1, m2):\n    result = []\n    for m1_r in range(len(m1)):\n        for m2_r in range(len(m2[0])):\n            sum_val = 0\n            for c in range(len(m1[0])):\n                sum_val += m1[m1_r][c] * m2[c][m2_r]\n            result += [sum_val]\n    return result\n\n# Create copies of the tensors that don't require gradients\ntrain_x_clone = train_x.clone().detach()\nweights_clone = weights.clone().detach()\n\n%%time\n# Matrix multiplication using @ operator\n(train_x_clone@weights_clone)[:5]\nCPU times: user 2.35 ms, sys: 4.15 ms, total: 6.5 ms\nWall time: 5.29 ms\n\ntensor([[ -6.5802],\n        [-10.9860],\n        [-21.2337],\n        [-18.2173],\n        [ -1.7079]], device='cuda:0')\n\n%%time\n# This is why you should avoid using loops\nmat_mul(train_x_clone, weights_clone)[:5]\nCPU times: user 1min 37s, sys: 28 ms, total: 1min 37s\nWall time: 1min 37s\n\n[tensor(-6.5802, device='cuda:0'),\n tensor(-10.9860, device='cuda:0'),\n tensor(-21.2337, device='cuda:0'),\n tensor(-18.2173, device='cuda:0'),\n tensor(-1.7079, device='cuda:0')]\n\n# Move tensor copies to GPU\ntrain_x_clone = train_x_clone.to('cuda');\nweights_clone = weights_clone.to('cuda');\n\n%%time\n(train_x_clone@weights_clone)[:5]\nCPU times: user 2.19 ms, sys: 131 µs, total: 2.32 ms\nWall time: 7.78 ms\n\ntensor([[ -6.5802],\n        [-10.9860],\n        [-21.2337],\n        [-18.2173],\n        [ -1.7079]], device='cuda:0')\n\n# Over 86,000 times faster on GPU\nprint(f\"{(44.9 * 1e+6) / 522:,.2f}\")\n86,015.33\n\n# Define a linear layer\n# Matrix-multiply xb and weights and add the bias\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\ntensor([[ -6.2330],\n        [-10.6388],\n        [-20.8865],\n        ...,\n        [-15.9176],\n        [ -1.6866],\n        [-11.3568]], grad_fn=&lt;AddBackward0&gt;)\n\n# Determine which predictions were correct\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\ntensor([[False],\n        [False],\n        [False],\n        ...,\n        [ True],\n        [ True],\n        [ True]])\n\n# Calculate the current model accuracy\ncorrects.float().mean().item()\n0.5379961133003235\n\n# Test a small change in the weights\nwith torch.no_grad():\n    weights[0] *= 1.0001\n\npreds = linear1(train_x)\n((preds&gt;0.0).float() == train_y).float().mean().item()\n0.5379961133003235\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\ntorch.where(condition, x, y)\n\nhttps://pytorch.org/docs/stable/generated/torch.where.html\nReturn a tensor of elements selected from either x or y, depending on condition\n\n\n# Measures how distant each prediction is from 1 if it should be one\n# and how distant it is from 0 if it should be 0 and take the mean of those distances\n# returns a lower number when predictions are more accurate\n# Assumes that all predictions are between 0 and 1\ndef mnist_loss(predictions, targets):\n    # return \n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\ntorch.where(trgts==1, 1-prds, prds)\ntensor([0.1000, 0.4000, 0.8000])\n\nmnist_loss(prds,trgts)\ntensor(0.4333)\n\nmnist_loss(tensor([0.9, 0.4, 0.8]),trgts)\ntensor(0.2333)\n\n\n\nSigmoid Function\n\nalways returns a value between 0 and 1\nfunction is a smooth curve only goes up\n\nmakes it easier for SGD to find meaningful gradients\n\n\n\ntorch.exp(x)\nhttps://pytorch.org/docs/stable/generated/torch.exp.html  returns \\(e^{x}\\) where \\(e\\) is [Euler’s number](https://en.wikipedia.org/wiki/E_(mathematical_constant) * \\(e \\approx 2.7183\\)\n\nprint(torch.exp(tensor(1)))\nprint(torch.exp(tensor(2)))\ntensor(2.7183)\ntensor(7.3891)\n\n# Always returns a number between 0 and 1\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4);\n\n\n\n\n\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nSGD and Mini-Batches\n\ncalculating the loss for the entire dataset would take a lot of time\n\nthe full dataset is also unlikely to fit in memory\n\ncalculating the loss for single data item would result in an imprecise and unstable gradient\nwe can compromise by calculating the loss for a few data items at a time\nmini-batch: a subset of data items\nbatch size: the number of data items in a mini-batch\n\nlarger batch-size\n\ntypically results in a more accurate and stable estimate of your dataset’s gradient from the loss function\ntakes longer per mini-batch\nfewer mini-batches processed per epoch\n\nthe batch size is limited by the amount of available memory for the CPU or GPU\nideal batch-size is context dependent\n\naccelerators like GPUs work best when they have lots of work to do at a time\n\ntypically want to use the largest batch-size that will fit in GPU memory\n\ntypically want to randomly shuffle the contents of mini-batches for each epoch\nDataLoader\n\nhandles shuffling and mini-batch collation\ncan take any Python collection and turn it into an iterator over many batches\n\nPyTorch Dataset: a collection that contains tuples of independent and dependent variables\n\nIn-Place Operations:\n\nmethods in PyTorch that end in an underscore modify their objects in place\n\n\nPyTorch DataLoader:\n\nhttps://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\nCombines a dataset and a sampler, and provides an iterable over the given dataset.\nsupports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning\n\n\n\nPyTorch Dataset:\n\nhttps://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\nan abstract class representing a dataset\n\n\n\nMap-style datasets:\n\nimplements the __getitem__() and __len__() protocols, and represents a map from indices/keys to data samples\n\n\n\nIterable-style datasets:\n\nan instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples\nparticularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data\n\n\n\nfastai DataLoader:\n\nhttps://docs.fast.ai/data.load.html#DataLoader\nAPI compatible with PyTorch DataLoader, with a lot more callbacks and flexibility\n\n\nDataLoader\nfastai.data.load.DataLoader\n\n# Sample collection \ncoll = range(15)\nrange(0, 15)\n\n# Sample collection \ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n[tensor([ 0,  7,  4,  5, 11]),\n tensor([ 9,  3,  8, 14,  6]),\n tensor([12,  2,  1, 10, 13])]\n\n# Sample dataset of independent and dependent variables\nds = L(enumerate(string.ascii_lowercase))\nds\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n[(tensor([20, 18, 21,  5,  6,  9]), ('u', 's', 'v', 'f', 'g', 'j')),\n (tensor([13, 19, 12, 16, 25,  3]), ('n', 't', 'm', 'q', 'z', 'd')),\n (tensor([15,  1,  0, 24, 10, 23]), ('p', 'b', 'a', 'y', 'k', 'x')),\n (tensor([11, 22,  2,  4, 14, 17]), ('l', 'w', 'c', 'e', 'o', 'r')),\n (tensor([7, 8]), ('h', 'i'))]"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#putting-it-all-together",
    "href": "posts/fastai-book-notes/chapter-4/index.html#putting-it-all-together",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Putting It All Together",
    "text": "Putting It All Together\n# Randomly initialize parameters\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n# Create data loader for training dataset\ndl = DataLoader(dset, batch_size=256)\n\nfastcore first():\n\nhttps://fastcore.fast.ai/basics.html#first\nFirst element of x, optionally filtered by f, or None if missing\n\n\nfirst\n&lt;function fastcore.basics.first(x, f=None, negate=False, **kwargs)&gt;\n\n# Get the first mini-batch from the data loader \nxb,yb = first(dl)\nxb.shape,yb.shape\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n# Create data loader for validation dataset\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n# Smaller example mini-batch for testing\nbatch = train_x[:4]\nbatch.shape\ntorch.Size([4, 784])\n\n# Test model smaller mini-batch\npreds = linear1(batch)\npreds\ntensor([[ -9.2139],\n        [-20.0299],\n        [-16.8065],\n        [-14.1171]], grad_fn=&lt;AddBackward0&gt;)\n\n# Calculate the loss\nloss = mnist_loss(preds, train_y[:4])\nloss\ntensor(1.0000, grad_fn=&lt;MeanBackward0&gt;)\n\n# Compute the gradients\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n(torch.Size([784, 1]), tensor(-3.5910e-06), tensor([-2.5105e-05]))\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n(tensor(-7.1820e-06), tensor([-5.0209e-05]))\n\nNote: loss.backward() adds the gradients of loss to any gradients that are currently stored. This means we need to zero the gradients first\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n(tensor(-1.0773e-05), tensor([-7.5314e-05]))\n\nweights.grad.zero_()\nbias.grad.zero_();\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            # Assign directly to the data attribute to prevent \n            # PyTorch from taking the gradient of that step\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n# Calculate accuracy using broadcasting\n(preds&gt;0.0).float() == train_y[:4]\ntensor([[False],\n        [False],\n        [False],\n        [False]])\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\nbatch_accuracy(linear1(batch), train_y[:4])\ntensor(0.)\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nvalidate_epoch(linear1)\n0.3407\n\nlr = 1.\nparams = weights,bias\n# Train for one epoch\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n0.6138\n\n# Train for twenty epochs\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n0.7358 0.9052 0.9438 0.9575 0.9638 0.9692 0.9726 0.9741 0.975 0.976 0.9765 0.9765 0.9765 0.9779 0.9784 0.9784 0.9784 0.9784 0.9789 0.9784 \nNote: Accuracy improves from 0.7358 to 0.9784\n\n\nCreating an Optimizer\nWhy we need Non-Linear activation functions\n\na series of any number of linear layers in a row can be replaced with a single linear layer with different parameters\nadding a non-linear layer between linear layers helps decouple the linear layers from each other so they can learn separate features\n\n\ntorch.nn:\n\nhttps://pytorch.org/docs/stable/nn.html\nprovides the basic building blocks for building PyTorch models\n\n\n\nnn.Linear():\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.Linear.html\nApplies a linear transformation to the incoming data: \\(y=xA^{T}+b\\)\ncontains both the weights and biases in a single class\ninherits from nn.Module()\n\n\n\nnn.Module():\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\nBase class for all neural network modules\nany PyTorch models should subclass this class\nmodules can contain other modules\nsubmodules can be assigned as regular attributes\n\n\nnn.Linear\ntorch.nn.modules.linear.Linear\n\nlinear_model = nn.Linear(28*28,1)\nlinear_model\nLinear(in_features=784, out_features=1, bias=True)\n\n\nnn.Parameter():\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\nA Tensor sublcass\nA kind of Tensor that is to be considered a module parameter.\n\n\nw,b = linear_model.parameters()\nw.shape,b.shape\n(torch.Size([1, 784]), torch.Size([1]))\n\nprint(type(w))\nprint(type(b))\n&lt;class 'torch.nn.parameter.Parameter'&gt;\n&lt;class 'torch.nn.parameter.Parameter'&gt;\n\nb\nParameter containing:\ntensor([0.0062], requires_grad=True)\n\n# Implements the basic optimization steps used earlier for use with a PyTorch Module\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\n# PyTorch optimizers need a reference to the target model parameters\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nvalidate_epoch(linear_model)\n0.4673\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\ntrain_model(linear_model, 20)\n0.4932 0.8193 0.8467 0.9155 0.935 0.9477 0.956 0.9629 0.9653 0.9682 0.9697 0.9731 0.9741 0.9751 0.9761 0.9765 0.9775 0.978 0.9785 0.9785 \nNote: The PyTorch version arrives at almost exactly the same accuracy as the hand-crafted version\n\n\nfastai SGD():\n\nhttps://docs.fast.ai/optimizer.html#SGD\nAn Optimizer for SGD with lr and mom and params\nby default does the same thing as BasicOptim\n\n\nSGD\n&lt;function fastai.optimizer.SGD(params, lr, mom=0.0, wd=0.0, decouple_wd=True)&gt;\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n0.4932 0.8135 0.8481 0.916 0.9341 0.9487 0.956 0.9634 0.9653 0.9673 0.9692 0.9717 0.9746 0.9751 0.9756 0.9765 0.9775 0.9775 0.978 0.978 \n\ndls = DataLoaders(dl, valid_dl)\n\n\nfastai Learner:\n\nhttps://docs.fast.ai/learner.html#Learner\nGroup together a model, some data loaders, an optimizer and a loss function to handle training\n\n\nLearner\nfastai.learner.Learner\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nfastai Learner.fit:\n\nhttps://docs.fast.ai/learner.html#Learner.fit\nfit a model for a specifed number of epochs using a specified learning rate\n\n\nlr\n1.0\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nbatch_accuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.635737\n\n\n0.503216\n\n\n0.495584\n\n\n00:00\n\n\n\n\n1\n\n\n0.443481\n\n\n0.246651\n\n\n0.777723\n\n\n00:00\n\n\n\n\n2\n\n\n0.165904\n\n\n0.159723\n\n\n0.857704\n\n\n00:00\n\n\n\n\n3\n\n\n0.074277\n\n\n0.099495\n\n\n0.918057\n\n\n00:00\n\n\n\n\n4\n\n\n0.040486\n\n\n0.074255\n\n\n0.934740\n\n\n00:00\n\n\n\n\n5\n\n\n0.027243\n\n\n0.060227\n\n\n0.949951\n\n\n00:00\n\n\n\n\n6\n\n\n0.021766\n\n\n0.051380\n\n\n0.956330\n\n\n00:00\n\n\n\n\n7\n\n\n0.019304\n\n\n0.045439\n\n\n0.962709\n\n\n00:00\n\n\n\n\n8\n\n\n0.018036\n\n\n0.041227\n\n\n0.965653\n\n\n00:00\n\n\n\n\n9\n\n\n0.017262\n\n\n0.038097\n\n\n0.968106\n\n\n00:00"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#adding-a-nonlinearity",
    "href": "posts/fastai-book-notes/chapter-4/index.html#adding-a-nonlinearity",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Adding a Nonlinearity",
    "text": "Adding a Nonlinearity\ndef simple_net(xb): \n    # Linear layer    \n    res = xb@w1 + b1\n    # ReLU activation layer\n    res = res.max(tensor(0.0))\n    # Linear layer\n    res = res@w2 + b2\n    return res\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nPyTorch F.relu:\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu\nApplies the rectified linear unit function element-wise.\n\\(\\text{ReLU}(x) = (x)^+ = \\max(0, x)\\)\n\n\nF.relu\n&lt;function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -&gt; torch.Tensor&gt;\n\nplot_function(F.relu)\n\n\n\n\n\n\n\nnn.Sequential:\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\nA sequential container.\nTreats the whole container as a single module\nouputs from the previous layer are fed as input to the next layer in the list\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\nsimple_net\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nbatch_accuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.259396\n\n\n0.417702\n\n\n0.504416\n\n\n00:00\n\n\n\n\n1\n\n\n0.128176\n\n\n0.216283\n\n\n0.818449\n\n\n00:00\n\n\n\n\n2\n\n\n0.073893\n\n\n0.111460\n\n\n0.920020\n\n\n00:00\n\n\n\n\n3\n\n\n0.050328\n\n\n0.076076\n\n\n0.941119\n\n\n00:00\n\n\n\n\n4\n\n\n0.039086\n\n\n0.059598\n\n\n0.958292\n\n\n00:00\n\n\n\n\n5\n\n\n0.033148\n\n\n0.050273\n\n\n0.964671\n\n\n00:00\n\n\n\n\n6\n\n\n0.029618\n\n\n0.044374\n\n\n0.966634\n\n\n00:00\n\n\n\n\n7\n\n\n0.027258\n\n\n0.040340\n\n\n0.969087\n\n\n00:00\n\n\n\n\n8\n\n\n0.025527\n\n\n0.037404\n\n\n0.969578\n\n\n00:00\n\n\n\n\n9\n\n\n0.024172\n\n\n0.035167\n\n\n0.971541\n\n\n00:00\n\n\n\n\n10\n\n\n0.023068\n\n\n0.033394\n\n\n0.972522\n\n\n00:00\n\n\n\n\n11\n\n\n0.022145\n\n\n0.031943\n\n\n0.973503\n\n\n00:00\n\n\n\n\n12\n\n\n0.021360\n\n\n0.030726\n\n\n0.975466\n\n\n00:00\n\n\n\n\n13\n\n\n0.020682\n\n\n0.029685\n\n\n0.974975\n\n\n00:00\n\n\n\n\n14\n\n\n0.020088\n\n\n0.028779\n\n\n0.975466\n\n\n00:00\n\n\n\n\n15\n\n\n0.019563\n\n\n0.027983\n\n\n0.975957\n\n\n00:00\n\n\n\n\n16\n\n\n0.019093\n\n\n0.027274\n\n\n0.976448\n\n\n00:00\n\n\n\n\n17\n\n\n0.018670\n\n\n0.026638\n\n\n0.977920\n\n\n00:00\n\n\n\n\n18\n\n\n0.018285\n\n\n0.026064\n\n\n0.977920\n\n\n00:00\n\n\n\n\n19\n\n\n0.017933\n\n\n0.025544\n\n\n0.978901\n\n\n00:00\n\n\n\n\n20\n\n\n0.017610\n\n\n0.025069\n\n\n0.979392\n\n\n00:00\n\n\n\n\n21\n\n\n0.017310\n\n\n0.024635\n\n\n0.979392\n\n\n00:00\n\n\n\n\n22\n\n\n0.017032\n\n\n0.024236\n\n\n0.980373\n\n\n00:00\n\n\n\n\n23\n\n\n0.016773\n\n\n0.023869\n\n\n0.980373\n\n\n00:00\n\n\n\n\n24\n\n\n0.016531\n\n\n0.023529\n\n\n0.980864\n\n\n00:00\n\n\n\n\n25\n\n\n0.016303\n\n\n0.023215\n\n\n0.981354\n\n\n00:00\n\n\n\n\n26\n\n\n0.016089\n\n\n0.022923\n\n\n0.981354\n\n\n00:00\n\n\n\n\n27\n\n\n0.015887\n\n\n0.022652\n\n\n0.981354\n\n\n00:00\n\n\n\n\n28\n\n\n0.015695\n\n\n0.022399\n\n\n0.980864\n\n\n00:00\n\n\n\n\n29\n\n\n0.015514\n\n\n0.022164\n\n\n0.981354\n\n\n00:00\n\n\n\n\n30\n\n\n0.015342\n\n\n0.021944\n\n\n0.981354\n\n\n00:00\n\n\n\n\n31\n\n\n0.015178\n\n\n0.021738\n\n\n0.981354\n\n\n00:00\n\n\n\n\n32\n\n\n0.015022\n\n\n0.021544\n\n\n0.981845\n\n\n00:00\n\n\n\n\n33\n\n\n0.014873\n\n\n0.021363\n\n\n0.981845\n\n\n00:00\n\n\n\n\n34\n\n\n0.014731\n\n\n0.021192\n\n\n0.981845\n\n\n00:00\n\n\n\n\n35\n\n\n0.014595\n\n\n0.021031\n\n\n0.982336\n\n\n00:00\n\n\n\n\n36\n\n\n0.014464\n\n\n0.020879\n\n\n0.982826\n\n\n00:00\n\n\n\n\n37\n\n\n0.014338\n\n\n0.020735\n\n\n0.982826\n\n\n00:00\n\n\n\n\n38\n\n\n0.014217\n\n\n0.020599\n\n\n0.982826\n\n\n00:00\n\n\n\n\n39\n\n\n0.014101\n\n\n0.020470\n\n\n0.982336\n\n\n00:00\n\n\n\n\n\n\n\n\nmatplotlib.pyplot.plot:\n\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\nPlot y versus x as lines and/or markers\n\n\nplt.plot\n&lt;function matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)&gt;\n\n\nfastai learner.Recorder:\n\nhttps://docs.fast.ai/learner.html#Recorder\nCallback that registers statistics (lr, loss and metrics) during training\n\n\nlearn.recorder\nRecorder\n\nRecorder\nfastai.learner.Recorder\n\n\nfastcore L.itemgot():\n\nhttps://fastcore.fast.ai/foundation.html#L.itemgot\nCreate new L with item idx of all items\n\n\nL.itemgot\n&lt;function fastcore.foundation.L.itemgot(self, *idxs)&gt;\n\nplt.plot(L(learn.recorder.values).itemgot(2));\n\n\n\n\n\n\nlearn.recorder.values[-1][2]\n0.98233562707901\n\n\nGoing Deeper\n\ndeeper models: models with more layers\ndeeper models are more difficult to optimize the more layers\ndeeper models require fewer parameters\nwe can use smaller matrices with more layers\nwe can train the model more quickly using less memory\ntypically perform better\n\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.066122\n\n\n0.008277\n\n\n0.997547\n\n\n00:04\n\n\n\n\n\n\n\nlearn.model\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#jargon-recap",
    "href": "posts/fastai-book-notes/chapter-4/index.html#jargon-recap",
    "title": "Notes on fastai Book Ch. 4",
    "section": "Jargon Recap",
    "text": "Jargon Recap\n\nneural networks contain two types of numbers\n\nParameters: numbers that are randomly initialized and optimized\n\ndefine the model\n\nActivations: numbers that are calculated using the parameter values\n\ntensors\n\nregularly-shaped arrays like a matrix\nhave rows and columns\n\ncalled the axes or dimensions\n\n\nrank: the number of dimensions of a tensor\n\nRank-0: scalar\nRank-1: vector\nRank-2: matrix\n\na neural network contains a number of linear and non-linear layers\nnon-linear layers are referred to as activation layers\nReLU: a function that sets any negative values to zero\nMini-batch: a small group of inputs and labels gathered together in two arrays to perform gradient descent\nForward pass: Applying the model to some input and computing the predictions\nLoss: A value that represents how the model is doing\nGradient: The derivative of the loss with respect to all model parameters\nGradient descent: Taking a step in the direction opposite to the gradients to make the model parameters a little bit better\nLearning rate: The size of the step we take when applying SGD to update the parameters of the model"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-4/index.html#references",
    "href": "posts/fastai-book-notes/chapter-4/index.html#references",
    "title": "Notes on fastai Book Ch. 4",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 3\nNext: Notes on fastai Book Ch. 5"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html",
    "href": "posts/fastai-book-notes/chapter-5/index.html",
    "title": "Notes on fastai Book Ch. 5",
    "section": "",
    "text": "Image Classification\nFrom Dogs and Cats to Pet Breeds\nPresizing\nCross-Entropy Loss\nModel Interpretation\nImproving Our Model\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#image-classification",
    "href": "posts/fastai-book-notes/chapter-5/index.html#image-classification",
    "title": "Notes on fastai Book Ch. 5",
    "section": "Image Classification",
    "text": "Image Classification\n\nThere are a lot of details you need to get right for your models to be accurate and reliable\nYou must be able to look inside your neural network as it trains and as it makes predictions, find possible problems and know how to fix them"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#from-dogs-and-cats-to-pet-breeds",
    "href": "posts/fastai-book-notes/chapter-5/index.html#from-dogs-and-cats-to-pet-breeds",
    "title": "Notes on fastai Book Ch. 5",
    "section": "From Dogs and Cats to Pet Breeds",
    "text": "From Dogs and Cats to Pet Breeds\n\nIn real-life\n\nstart with a dataset that we know nothing about\nfigure out how it is put together\nfigure out how to extract the data we need from it\nfigure out what the data looks like\n\nData is usually provided in one of two ways\n\nIndividual files representing items of data, possibly organized into folder or with filenames representing information about those items\n\ntext documents\nimages\n\nA table of data in which each row is an item and may include filenames providing connections between the data in the table and data in other formats\n\nCSV files\n\nExceptions:\n\nDomains like Genomics\n\nbinary database formats\nnetwork streams\n\n\n\n\n\nfrom fastai.vision.all import *\nmatplotlib.rc('image', cmap='Greys')\n\nThe Oxford-IIIT Pet Dataset\n\nhttps://www.robots.ox.ac.uk/~vgg/data/pets/\na 37 category pet dataset with roughly 200 images for each class\nimages have a large variations in scale, pose and lighting\nall images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation\n\n\npath = untar_data(URLs.PETS)\npath\nPath('/home/innom-dt/.fastai/data/oxford-iiit-pet')\n\n#hide\nPath.BASE_PATH = path\npath\nPath('.')\n\npath.ls()\n(#2) [Path('images'),Path('annotations')]\n\n# associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation\n# Not needed for classification\n(path/\"annotations\").ls()\n(#7) [Path('annotations/trimaps'),Path('annotations/xmls'),Path('annotations/._trimaps'),Path('annotations/list.txt'),Path('annotations/test.txt'),Path('annotations/README'),Path('annotations/trainval.txt')]\n\n(path/\"images\").ls()\n(#7393) [Path('images/Birman_121.jpg'),Path('images/shiba_inu_131.jpg'),Path('images/Bombay_176.jpg'),Path('images/Bengal_199.jpg'),Path('images/beagle_41.jpg'),Path('images/beagle_27.jpg'),Path('images/great_pyrenees_181.jpg'),Path('images/Bengal_100.jpg'),Path('images/keeshond_124.jpg'),Path('images/havanese_115.jpg')...]\nPet breed and species is indicated in the file name for each image * Cat breeds have capitalized file names and dog breeds have lowercase file names\n\nfname = (path/\"images\").ls()[0]\nfname\nPath('images/Birman_121.jpg')\n\n\nRegular Expressions\n\na special string written in the regular expression language, which specifies a general rule for deciding whether another string passes a test\nextremely useful for pattern matching and extracting sections of text\nPython Regular Expressions Cheat Sheet\nOrigin:\n\noriginally examples of a “regular” language\n\nthe lowest rung within the Chomsky hierarchy\n\nChomsky hierarchy\n\na grammar classification developed by linguist Noam Chomsky\n\nNoam Chomsky\n\nalso wrote Syntactic Structures (pdf)\n\nthe pioneering work searching for the formal grammar underlying human language\n\n\n\n\n\n# Matches all instances of any group of characters before a sequence of digits right before '.jpg'\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n['Birman']\n\nfastai RegexLabeller:\n\nhttps://docs.fast.ai/data.transforms.html#RegexLabeller\nlabel items using a regular expression pattern\n\nRegexLabeller\nfastai.data.transforms.RegexLabeller\n\nget_image_files\n&lt;function fastai.data.transforms.get_image_files(path, recurse=True, folders=None)&gt;\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n\ndls.c\n37\n\nimport pandas as pd\n\n\nfastai Categorize:\n\nhttps://docs.fast.ai/data.transforms.html#Categorize\nreversible transform of a category string to a vocab id\ninherits from DisplayedTransform\n\n\n\nfastai DisplayedTransform\n\nhttps://fastcore.fast.ai/transform#DisplayedTransform\nA transform that shows its attributes\n\n\npd.DataFrame(dls.categorize.vocab)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nAbyssinian\n\n\n\n\n1\n\n\nBengal\n\n\n\n\n2\n\n\nBirman\n\n\n\n\n3\n\n\nBombay\n\n\n\n\n4\n\n\nBritish_Shorthair\n\n\n\n\n5\n\n\nEgyptian_Mau\n\n\n\n\n6\n\n\nMaine_Coon\n\n\n\n\n7\n\n\nPersian\n\n\n\n\n8\n\n\nRagdoll\n\n\n\n\n9\n\n\nRussian_Blue\n\n\n\n\n10\n\n\nSiamese\n\n\n\n\n11\n\n\nSphynx\n\n\n\n\n12\n\n\namerican_bulldog\n\n\n\n\n13\n\n\namerican_pit_bull_terrier\n\n\n\n\n14\n\n\nbasset_hound\n\n\n\n\n15\n\n\nbeagle\n\n\n\n\n16\n\n\nboxer\n\n\n\n\n17\n\n\nchihuahua\n\n\n\n\n18\n\n\nenglish_cocker_spaniel\n\n\n\n\n19\n\n\nenglish_setter\n\n\n\n\n20\n\n\ngerman_shorthaired\n\n\n\n\n21\n\n\ngreat_pyrenees\n\n\n\n\n22\n\n\nhavanese\n\n\n\n\n23\n\n\njapanese_chin\n\n\n\n\n24\n\n\nkeeshond\n\n\n\n\n25\n\n\nleonberger\n\n\n\n\n26\n\n\nminiature_pinscher\n\n\n\n\n27\n\n\nnewfoundland\n\n\n\n\n28\n\n\npomeranian\n\n\n\n\n29\n\n\npug\n\n\n\n\n30\n\n\nsaint_bernard\n\n\n\n\n31\n\n\nsamoyed\n\n\n\n\n32\n\n\nscottish_terrier\n\n\n\n\n33\n\n\nshiba_inu\n\n\n\n\n34\n\n\nstaffordshire_bull_terrier\n\n\n\n\n35\n\n\nwheaten_terrier\n\n\n\n\n36\n\n\nyorkshire_terrier"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#presizing",
    "href": "posts/fastai-book-notes/chapter-5/index.html#presizing",
    "title": "Notes on fastai Book Ch. 5",
    "section": "Presizing",
    "text": "Presizing\n\nwe need our images to have the same dimensions before we collate them into tensors\nwe should compose our augmentation transforms into fewer transformations and transform the images into uniform sizes\n\n\nSteps for presizing\n\nResize images to relatively “large” dimensions compared to the target training dimensions\n\nLarger images have some spare margin for augmentations that might result in empty zones in the image\naugmented images are then cropped and resized to a square\n\nthe crop area is chosen randomly on the training set\n\n\nCompose all the common augmentation operations into one, and perform a combined operation on the GPU\n\nall potentially destructive operations are performed together with a single interpolation at the end\n\n\ndblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_y=parent_label,\n                   item_tfms=Resize(460))\n\n(Path.cwd()/'images'/'grizzly.jpg')\nPath('/media/innom-dt/Samsung_T3/Projects/Current_Projects/fastbook/clean/images/grizzly.jpg')\n\n# Create a test DataLoaders object with 100 copies of the same image\ndls1 = dblock1.dataloaders([(Path.cwd()/'images'/'grizzly.jpg')]*100, bs=8)\nprint(len(dls1.items))\nprint(dls1.categorize.vocab)\n80\n['images']\n\n# Return elements from the iterable until it is exhausted.\ndls1.train.get_idxs = lambda: Inf.ones\n\nitertools.cycle()\n\nhttps://docs.python.org/3/library/itertools.html#itertools.cycle\nMake an iterator returning elements from the iterable and saving a copy of each\n\n\ntype(Inf.ones)\nitertools.cycle\n\n\nfastai DataLoader.one_batch:\n\nhttps://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/data/load.py#L146\nReturn one batch from DataLoader\n\n\nDataLoader.one_batch\n&lt;function fastai.data.load.DataLoader.one_batch(self)&gt;\n\nx,y = dls1.one_batch()\nprint(x.shape)\nprint(y.shape)\nprint(y)\ntorch.Size([8, 3, 460, 460])\ntorch.Size([8])\nTensorCategory([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n\nprint(TensorImage)\n&lt;class 'fastai.torch_core.TensorImage'&gt;\n\n\nfastai TensorImage:\n\nhttps://docs.fast.ai/torch_core.html#TensorImage\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\n\n\nTensorImage.affine_coord:\n\nhttps://docs.fast.ai/vision.augment.html#AffineCoordTfm\nhttps://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/vision/augment.py#L310\n\n\n\nTensorImage.rotate:\n\nhttps://docs.fast.ai/vision.augment.html#Rotate\nhttps://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/vision/augment.py#L549\n\n\n\nTensorImage.zoom:\n\nhttps://docs.fast.ai/vision.augment.html#Zoom\nhttps://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/vision/augment.py#L582\n\n\n\nTensorImage.warp:\n\nhttps://docs.fast.ai/vision.augment.html#Warp\nhttps://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/vision/augment.py#L656\n\n\nprint(TensorImage)\nprint(TensorImage.affine_coord)\nprint(TensorImage.rotate)\nprint(TensorImage.zoom)\nprint(TensorImage.warp)\n&lt;class 'fastai.torch_core.TensorImage'&gt;\n&lt;function TensorImage.affine_coord at 0x7fb370ba0940&gt;\n&lt;function TensorImage.rotate at 0x7fb370ba8dc0&gt;\n&lt;function TensorImage.zoom at 0x7fb370bb2160&gt;\n&lt;function TensorImage.warp at 0x7fb370bb2820&gt;\n\n\nfastcore Pipeline:\n\nhttps://fastcore.fast.ai/transform.html#Pipeline\nA pipeline of composed (for encode/decode) transforms, setup with types\na wrapper for compose_tfms\n\n\nPipeline\nfastcore.transform.Pipeline\n\n_,axs = subplots(1, 2)\n\nx1 = TensorImage(x.clone())\nx1 = x1.affine_coord(sz=224)\nx1 = x1.rotate(draw=30, p=1.)\nx1 = x1.zoom(draw=1.2, p=1.)\nx1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)\n\n# Go through transforms and combine together affine/coord or lighting transforms\ntfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224),\n                       Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)])\nx = Pipeline(tfms)(x)\n#x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode)\nTensorImage(x[0]).show(ctx=axs[0])\nTensorImage(x1[0]).show(ctx=axs[1]);\n\n\n\n\n\n\n\n\nChecking and Debugging a DataBlock\n\nalways check your data when creating a new DataBlock\n\nmake sure the augmentations work as intended\n\nonce your data looks right, run it through a simple model\n\nstart getting feedback as soon as possible\n\n\n\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n\n\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\n\nDataBlock.summary():\n\nhttps://docs.fast.ai/data.block.html#DataBlock.summary\nSteps through the transform pipeline for one batch\n\n\nDataBlock.summary\n&lt;function fastai.data.block.DataBlock.summary(self: fastai.data.block.DataBlock, source, bs=4, show_batch=False, **kwargs)&gt;\n\npets1.summary(path/\"images\")\nSetting-up type transforms pipelines\nCollecting items from /home/innom-dt/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /home/innom-dt/.fastai/data/oxford-iiit-pet/images/great_pyrenees_182.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=400x500\n  Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /home/innom-dt/.fastai/data/oxford-iiit-pet/images/great_pyrenees_182.jpg\n    applying partial gives\n      great_pyrenees\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(21)\n\nFinal sample: (PILImage mode=RGB size=400x500, TensorCategory(21))\n\nCollecting items from /home/innom-dt/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=400x500, TensorCategory(21))\n    applying ToTensor gives\n      (TensorImage of size 3x500x400, TensorCategory(21))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following shapes\ntorch.Size([3, 500, 400]),torch.Size([3, 334, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 375])\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.530514\n\n\n0.347316\n\n\n0.115697\n\n\n00:19\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.556219\n\n\n0.296193\n\n\n0.098782\n\n\n00:23\n\n\n\n\n1\n\n\n0.340186\n\n\n0.199035\n\n\n0.066982\n\n\n00:23"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#cross-entropy-loss",
    "href": "posts/fastai-book-notes/chapter-5/index.html#cross-entropy-loss",
    "title": "Notes on fastai Book Ch. 5",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\n\nthe combination of taking the softmax and then the log likelihood\nworks even when our dependent variable has more than two categories\nresults in faster and more reliable training\n\n\nViewing Activations and Labels\nx,y = dls.one_batch()\n\ny.shape\ntorch.Size([64])\n\ny\nTensorCategory([31, 30,  5, 17,  6,  7,  4, 22,  4, 27,  2, 19, 12, 14, 11,  8,  5, 26, 14, 11, 28, 25, 35,  4, 22, 36, 31,  9, 27, 20, 23, 33,  2, 27,  0, 18, 12, 22, 17, 21, 25, 13, 16, 15, 33, 14, 20, 15,\n         8, 18, 36, 32,  7, 26,  4, 20, 36, 14, 25, 32,  4, 14, 25, 17], device='cuda:0')\n\nLearner.get_preds:\n\nhttps://docs.fast.ai/learner.html#Learner.get_preds\nGet the predictions and targets using a dataset index or an iterator of batches\n\n\nLearner.get_preds\n&lt;function fastai.learner.Learner.get_preds(self, ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None, inner=False, reorder=True, cbs=None, save_preds=None, save_targs=None, concat_dim=0)&gt;\n\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\nTensorBase([2.8917e-05, 1.6130e-08, 3.2022e-04, 1.7541e-05, 1.5420e-05, 3.5346e-06, 7.1617e-06, 1.0242e-04, 4.3236e-05, 7.5608e-05, 3.9862e-05, 7.9690e-07, 7.0372e-07, 4.5139e-08, 2.7499e-06, 4.2070e-06,\n        9.9794e-08, 1.6574e-06, 4.6330e-07, 5.7135e-06, 8.5598e-07, 8.1175e-02, 7.1706e-05, 1.1809e-05, 7.3426e-05, 9.2441e-06, 7.5984e-07, 2.6505e-06, 1.1533e-04, 4.2089e-07, 4.4916e-06, 9.1771e-01,\n        3.1806e-06, 7.8490e-06, 4.9332e-07, 1.4727e-04, 3.1404e-07])\n\nlen(preds[0]),preds[0].sum()\n(37, TensorBase(1.0000))\n\n\n\nSoftmax\n\n\\(\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\)\ntakes in an n-dimensional input Tensor and rescales them so that the elements are all in the range [0,1] and sum to 1.\nthe multi-category equivalent of sigmoid\nslightly bigger activation values are amplified, making the highest value closer to 1\nthe first part of cross-entropy loss\nideal for when we know each image has a definite correct label\n\nduring training\n\nless ideal if we want our model to tell us it does not recognize what is in the image\n\nduring inference\nmight be better to use multiple binary output columns, each using a sigmoid activation\n\n\n\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n# set random seed to get same results across sessions\ntorch.random.manual_seed(42);\n\n# Create a random set of test activations for a binary classification problem\nacts = torch.randn((6,2))*2\nacts\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\nacts.sigmoid()\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n(acts[:,0]-acts[:,1]).sigmoid()\ntensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])\n\\(\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\)\n\ndef softmax(x): return 2.718**x / (2.718**x).sum(dim=1, keepdim=True)\nsoftmax(acts)\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\nprint(softmax(acts)[0])\nsoftmax(acts)[0].sum()\ntensor([0.6025, 0.3975])\ntensor(1.)\n\ntorch.softmax\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\nApplies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n\n\ntorch.softmax\n&lt;function _VariableFunctionsClass.softmax&gt;\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nLog Likelihood\n\\(y_{i} = \\log_{e}{x_{i}}\\)\n\\(\\log{(a*b)} = \\log{(a)} + \\log{(b)}\\)\n\nlogarithms increase linearly when the underlying signal increases exponentially or multiplicatively\nmeans multiplication can be replaced with addition, which is easier for working with very larger or very small numbers\n\nThe gradient of cross_entropy(a,b) is softmax(a)-b\n\nthe gradient is proportional to the difference between the prediction and the target\n\nthe same as mean squared error in regression, since the gradient of (a-b)**2 is 2*(a-b)\nsince the gradient is linear, we will not see any sudden jumps or exponential increases in gradients\n\n\n\n# Sample labels for testing\ntarg = tensor([0,1,0,1,1,0])\n\nsm_acts\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\nidx = range(6)\nsm_acts[idx, targ]\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targ'] = targ\ndf['idx'] = idx\ndf['loss'] = sm_acts[range(6), targ]\ndf\n\n\n\n\n\n\n\n\n3\n\n\n7\n\n\ntarg\n\n\nidx\n\n\nloss\n\n\n\n\n\n\n0\n\n\n0.602469\n\n\n0.397531\n\n\n0\n\n\n0\n\n\n0.602469\n\n\n\n\n1\n\n\n0.502065\n\n\n0.497935\n\n\n1\n\n\n1\n\n\n0.497935\n\n\n\n\n2\n\n\n0.133188\n\n\n0.866811\n\n\n0\n\n\n2\n\n\n0.133188\n\n\n\n\n3\n\n\n0.996640\n\n\n0.003360\n\n\n1\n\n\n3\n\n\n0.003360\n\n\n\n\n4\n\n\n0.595949\n\n\n0.404051\n\n\n1\n\n\n4\n\n\n0.404051\n\n\n\n\n5\n\n\n0.366118\n\n\n0.633882\n\n\n0\n\n\n5\n\n\n0.366118\n\n\n\n\n\n\n\n-sm_acts[idx, targ]\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\nF.nll_loss\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss\nThe negative log likelihood (nll) loss.\ndoes not actually take the log, because it assumes you have already taken the log\ndesigned to be used after log_softmax\n\n\nF.nll_loss\n&lt;function torch.nn.functional.nll_loss(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean') -&gt; torch.Tensor&gt;\n\nF.nll_loss(sm_acts, targ, reduction='none')\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nTaking the Log\n\ntorch.log\n\nhttps://pytorch.org/docs/stable/generated/torch.log.html\nReturns a new tensor with the natural logarithm of the elements of input.\n\\(y_{i} = \\log_{e}{(x_{i})}\\)\n\\(e = 2.718\\)\n\n\ntorch.log\n&lt;function _VariableFunctionsClass.log&gt;\n\nplot_function(torch.log, min=0,max=4)\n\n\n\n\n\n\n\nnn.CrossEntropyLoss\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\ncomputes the cross entropy loss between input and target\ntakes the mean of the loss of all items by default\n\ncan be disabled with reduction='none'\n\n\n\nloss_func = nn.CrossEntropyLoss()\n\nloss_func(acts, targ)\ntensor(1.8045)\n\nF.cross_entropy(acts, targ)\ntensor(1.8045)\n\n-torch.log(-F.nll_loss(sm_acts, targ, reduction='none'))\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n# Do not take the mean\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#model-interpretation",
    "href": "posts/fastai-book-notes/chapter-5/index.html#model-interpretation",
    "title": "Notes on fastai Book Ch. 5",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nIt is very hard to interpret loss functions directly as they are optimized for differentiation and optimization, not human consumption\n\n\nClassificationInterpretation\n\nhttps://docs.fast.ai/interpret.html#Interpretation\nInterpretation base class for exploring predictions from trained models\n\n\n\nClassificationInterpretation.from_learner\n\nhttps://docs.fast.ai/interpret.html#Interpretation.from_learner\nConstruct interpretation object from a learner\n\n\nClassificationInterpretation\nfastai.interpret.ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\n\n\n\n\ninterp.most_confused(min_val=4)\n[('Birman', 'Ragdoll', 4),\n ('Ragdoll', 'Birman', 4),\n ('Siamese', 'Birman', 4),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 4)]"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#improving-our-model",
    "href": "posts/fastai-book-notes/chapter-5/index.html#improving-our-model",
    "title": "Notes on fastai Book Ch. 5",
    "section": "Improving Our Model",
    "text": "Improving Our Model\n\nThe Learning Rate Finder\n\npicking the right learning rate is one of the most important things we can doe when training a model\n\na learning rate that is too small can take many, many epochs, increasing both training time and the risk of overfitting\na learning rate that is too big can prevent the model from improving at all\n\nNaval researcher, Leslie Smith, came up with the idea of a learning rate finder in 2015\n\nstart with a very, very small learning rate\nuse the small learning rate for one mini-batch\nfind what the losses are after that one mini-batch\nincrease the learning rate by a certain percentage\nrepeat steps 2-4 until the loss gets worse\nselect a learning rate that is a bit lower than the highest useful learning rate\n\nEither one order of magnitude less than where the minimum loss was achieved or the last point where the loss was clearly decreasing\n\n\n\n\n# Test using a very high learning rate\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.648785\n\n\n4.358732\n\n\n0.443843\n\n\n00:19\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n4.398980\n\n\n3.214994\n\n\n0.839648\n\n\n00:23\n\n\n\n\n\n\nUsing a very high learning rate resulted in an increasing error rate\n\nLearner.lr_find\n\nhttps://docs.fast.ai/callback.schedule.html#Learner.lr_find\nLaunch a mock training to find a good learning rate and return suggestions as a named tuple\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\nNote: The plot has a logarithmic scale\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\nMinimum/10: 1.00e-02, steepest point: 6.31e-03\n\nlr_steep\n0.0063095735386013985\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=lr_steep)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.086781\n\n\n0.335212\n\n\n0.107578\n\n\n00:19\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.733380\n\n\n0.517203\n\n\n0.146143\n\n\n00:23\n\n\n\n\n1\n\n\n0.400132\n\n\n0.270925\n\n\n0.085250\n\n\n00:23\n\n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.264121\n\n\n0.360774\n\n\n0.117727\n\n\n00:19\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.544009\n\n\n0.415368\n\n\n0.131935\n\n\n00:23\n\n\n\n\n1\n\n\n0.332703\n\n\n0.216870\n\n\n0.066306\n\n\n00:23\n\n\n\n\n\n\n\n\n\nUnfreezing and Transfer Learning\n\nfreezing: only updating the weights in newly added layers while leaving the rest of a pretrained model unchanged\nProcess\n\nadd new layers to pretrained model\nfreeze pretrained layers\ntrain for a few epochs where only the new layers get updated\nunfreeze the pretrained layers\ntrain for a more epochs\n\n\n\nLearner.fine_tune\n\nhttps://docs.fast.ai/callback.schedule.html#Learner.fine_tune\nFine tune with Learner.freeze for freeze_epochs, then with Learner.unfreeze for epochs, using discriminative LR.\n\n\nLearner.fine_tune\n&lt;function fastai.callback.schedule.Learner.fine_tune(self: fastai.learner.Learner, epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False)&gt;\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\n# Train new layers for 3 epochs\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.154504\n\n\n0.279982\n\n\n0.083897\n\n\n00:19\n\n\n\n\n1\n\n\n0.528465\n\n\n0.244664\n\n\n0.079161\n\n\n00:19\n\n\n\n\n2\n\n\n0.313210\n\n\n0.205661\n\n\n0.066306\n\n\n00:19\n\n\n\n\n\n\n\n\nLearner.unfreeze()\n\nhttps://docs.fast.ai/learner.html#Learner.unfreeze\nUnfreeze the entire model\n\nlearn.unfreeze()\n\nlr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\nlr_min\n1.3182566908653825e-05\n\nlr_steep\n6.918309736647643e-06\n\nlearn.fit_one_cycle(6, lr_max=1e-5)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.280055\n\n\n0.198210\n\n\n0.065629\n\n\n00:23\n\n\n\n\n1\n\n\n0.259113\n\n\n0.193244\n\n\n0.066306\n\n\n00:24\n\n\n\n\n2\n\n\n0.228144\n\n\n0.190782\n\n\n0.063599\n\n\n00:24\n\n\n\n\n3\n\n\n0.209694\n\n\n0.186441\n\n\n0.064276\n\n\n00:24\n\n\n\n\n4\n\n\n0.203076\n\n\n0.189319\n\n\n0.064276\n\n\n00:23\n\n\n\n\n5\n\n\n0.180903\n\n\n0.186041\n\n\n0.062246\n\n\n00:23\n\n\n\n\n\n\n\n\n\nDiscriminative Learning Rates\n\nthe earliest layers our pretrained model might not need as a high of a learning rate as the last ones\nbased on insights developed by Jason Yosinski et al.\n\nHow transferable are features in deep neural networks?\nshowed that with transfer learning, different layers of a neural network should train at different speeds\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\n# Set the learning rate for the earliest layer to 1e-6\n# Set the learning rate for the last layer to 1e-4\n# Scale the learning rate for the in-between layers to gradually increase from 1e-6 up to 1e-4\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.158203\n\n\n0.300560\n\n\n0.092693\n\n\n00:19\n\n\n\n\n1\n\n\n0.516345\n\n\n0.242830\n\n\n0.073072\n\n\n00:19\n\n\n\n\n2\n\n\n0.335896\n\n\n0.207630\n\n\n0.065629\n\n\n00:19\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.257385\n\n\n0.204113\n\n\n0.068336\n\n\n00:23\n\n\n\n\n1\n\n\n0.266140\n\n\n0.203935\n\n\n0.065629\n\n\n00:23\n\n\n\n\n2\n\n\n0.240853\n\n\n0.194436\n\n\n0.060893\n\n\n00:23\n\n\n\n\n3\n\n\n0.218652\n\n\n0.189227\n\n\n0.062246\n\n\n00:23\n\n\n\n\n4\n\n\n0.196062\n\n\n0.192026\n\n\n0.063599\n\n\n00:24\n\n\n\n\n5\n\n\n0.173631\n\n\n0.184970\n\n\n0.060217\n\n\n00:23\n\n\n\n\n6\n\n\n0.159832\n\n\n0.185538\n\n\n0.061570\n\n\n00:23\n\n\n\n\n7\n\n\n0.151429\n\n\n0.180841\n\n\n0.061570\n\n\n00:23\n\n\n\n\n8\n\n\n0.136421\n\n\n0.182115\n\n\n0.062246\n\n\n00:23\n\n\n\n\n9\n\n\n0.133766\n\n\n0.175982\n\n\n0.058187\n\n\n00:24\n\n\n\n\n10\n\n\n0.133599\n\n\n0.178821\n\n\n0.056834\n\n\n00:24\n\n\n\n\n11\n\n\n0.128872\n\n\n0.176038\n\n\n0.058863\n\n\n00:24\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nNote: Accuracy may continue to improve, even when the validation loss starts to get worse * validation loss can get worse when your model gets overconfident, not just when it starts to memorize the training data\n\n\nSelecting the Number of Epochs\n\nyou will often find that you are limited by time, rather than generalization and accuracy\n\n\nyou should start with picking a number of epochs that will train in the amount of time that you are happy to wait for\nthen look at the training and validation loss plots, and your metrics\nyou will know that you have not trained for too long if they are still getting better even in your final epochs\n\n\n\nDeeper Architectures\n\na model with more parameters can generally model your data more accurately\n\nlots of caveats to this generalization\ndepends on the specifics of the architectures you are using\ntry a smaller model first\n\nmore likely to suffer from overfitting\nrequires more GPU memory\n\nmight need to lower the batch size\n\ntake longer to train\n\n\nLearner.to_fp16\n\nhttps://docs.fast.ai/callback.fp16.html#Learner.to_fp16\nMixed Precision Training\n\n\nfrom fastai.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.251300\n\n\n0.289129\n\n\n0.081867\n\n\n00:18\n\n\n\n\n1\n\n\n0.567936\n\n\n0.275442\n\n\n0.083221\n\n\n00:18\n\n\n\n\n2\n\n\n0.440096\n\n\n0.237322\n\n\n0.071719\n\n\n00:18\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.271949\n\n\n0.198495\n\n\n0.065629\n\n\n00:21\n\n\n\n\n1\n\n\n0.322747\n\n\n0.302487\n\n\n0.093369\n\n\n00:21\n\n\n\n\n2\n\n\n0.256723\n\n\n0.226659\n\n\n0.071042\n\n\n00:21\n\n\n\n\n3\n\n\n0.166247\n\n\n0.190719\n\n\n0.064953\n\n\n00:21\n\n\n\n\n4\n\n\n0.092291\n\n\n0.155199\n\n\n0.050744\n\n\n00:21\n\n\n\n\n5\n\n\n0.060924\n\n\n0.141513\n\n\n0.048038\n\n\n00:21"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-5/index.html#references",
    "href": "posts/fastai-book-notes/chapter-5/index.html#references",
    "title": "Notes on fastai Book Ch. 5",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 4\nNext: Notes on fastai Book Ch. 6"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-6/index.html",
    "href": "posts/fastai-book-notes/chapter-6/index.html",
    "title": "Notes on fastai Book Ch. 6",
    "section": "",
    "text": "Multi-Label Classification\nRegression\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-6/index.html#multi-label-classification",
    "href": "posts/fastai-book-notes/chapter-6/index.html#multi-label-classification",
    "title": "Notes on fastai Book Ch. 6",
    "section": "Multi-Label Classification",
    "text": "Multi-Label Classification\n\nthe problem of identifying the categories of objects in images that may not contain exactly one type of object\n\nthere may be more than one kind of object or none at all that belong to the target classes\n\nsingle-label classifiers cannot properly handle input that either does not contain an object of a target class or contains multiple objects of a different target classes\n\na single-label classifier trained to recognize cats and dogs could not handle an image that contains both cats and dogs\n\nmodels deployed in production are more likely to encounter input with zero matches or more than one match\n\n\nThe Data\nfrom fastai.vision.all import *\n\nThe PASCAL Visual Object Classes Challenge 2007 Dataset\n\nhttp://host.robots.ox.ac.uk/pascal/VOC/voc2007/\ncontains twenty classes\nmultiple classes may be present in the same image\nclassification labels are stored in a CSV file\n\n\npath = untar_data(URLs.PASCAL_2007)\npath\nPath('/home/innom-dt/.fastai/data/pascal_2007')\n\npath.ls()\n(#8) [Path('/home/innom-dt/.fastai/data/pascal_2007/segmentation'),Path('/home/innom-dt/.fastai/data/pascal_2007/test'),Path('/home/innom-dt/.fastai/data/pascal_2007/train.csv'),Path('/home/innom-dt/.fastai/data/pascal_2007/valid.json'),Path('/home/innom-dt/.fastai/data/pascal_2007/train'),Path('/home/innom-dt/.fastai/data/pascal_2007/train.json'),Path('/home/innom-dt/.fastai/data/pascal_2007/test.csv'),Path('/home/innom-dt/.fastai/data/pascal_2007/test.json')]\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfname\n\n\nlabels\n\n\nis_valid\n\n\n\n\n\n\n0\n\n\n000005.jpg\n\n\nchair\n\n\nTrue\n\n\n\n\n1\n\n\n000007.jpg\n\n\ncar\n\n\nTrue\n\n\n\n\n2\n\n\n000009.jpg\n\n\nhorse person\n\n\nTrue\n\n\n\n\n3\n\n\n000012.jpg\n\n\ncar\n\n\nFalse\n\n\n\n\n4\n\n\n000016.jpg\n\n\nbicycle\n\n\nTrue\n\n\n\n\n\n\nClass lables are stored in a space-delimited string\n\n\n\nPandas and DataFrames\n\nhttps://pandas.pydata.org/docs/index.html\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n\n\n# Access rows and columns using the `iloc` property \ndf.iloc[:,0]\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\ndf.iloc[0,:]\n# Trailing :s are always optional (in numpy, pytorch, pandas, etc.),\n#   so this is equivalent:\ndf.iloc[0]\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n# Get a column by name\ndf['fname']\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n# Initialize a new data frame using a dictionary\ntmp_df = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ntmp_df\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\n\n\n\n\n0\n\n\n1\n\n\n3\n\n\n\n\n1\n\n\n2\n\n\n4\n\n\n\n\n\n\n\n# Perform calculations using columns\ntmp_df['c'] = tmp_df['a']+tmp_df['b']\ntmp_df\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\nc\n\n\n\n\n\n\n0\n\n\n1\n\n\n3\n\n\n4\n\n\n\n\n1\n\n\n2\n\n\n4\n\n\n6\n\n\n\n\n\n\n\n\nConstructing a DataBlock\n\nDataset: a collection that returns a tuple of your independent and dependent variable for a single item\nDataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\nDatasets: an iterator that contains a training Dataset and a validation Dataset\nDataLoaders: an object that contains a training DataLoader and a validation DataLoader\nBy default, a DataBlock assumes we have an input and a target\n\n\nPython Lambda Functions\n\ngreat for quickly iterating\nnot compatible with serialization\nHow to Use Python Lambda Functions\n\nOne-hot encoding: using a vector of 0s, with a 1 in each location that is represented in the data\n\n# Start with a data block created with no parameters\ndblock = DataBlock()\n\n# Add a Datasets object using the DataFrame\ndsets = dblock.datasets(df)\n\nlen(dsets.train),len(dsets.valid)\n(4009, 1002)\n\n# Grabs the same thing twice\n# Need to specify an input and a target\nx,y = dsets.train[0]\nx,y\n(fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object,\n fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object)\n\nx['fname']\n'008663.jpg'\n\n# Tell the DataBlock how to extract the input and target from the DataFrame\n# Using lamda functions\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n('005620.jpg', 'aeroplane')\nNote: Do not use lambda functions if you need to export the Learner\n\n# Tell the DataBlock how to extract the input and target from the DataFrame\n# Using standard functions\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n('002549.jpg', 'tvmonitor')\nNote: Need the full file path for the dependent variable and need to split the dependent variables on the space character\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n(Path('/home/innom-dt/.fastai/data/pascal_2007/train/002844.jpg'), ['train'])\n\n\nImageBlock\n\nhttps://docs.fast.ai/vision.data.html#ImageBlock\nA TransformBlock for images\n\n\n\nMultiCategoryBlock\n\nhttps://docs.fast.ai/data.block.html#MultiCategoryBlock\nA TransformBlock for multi-label categorical targets\nUses One-hot encoding\nExpects to receive a list of strings\n\n\nImageBlock\n&lt;function fastai.vision.data.ImageBlock(cls=&lt;class 'fastai.vision.core.PILImage'&gt;)&gt;\n\nMultiCategoryBlock\n&lt;function fastai.data.block.MultiCategoryBlock(encoded=False, vocab=None, add_na=False)&gt;\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n# Check which object class is represented by the above one-hot encoding\nidxs = torch.where(dsets.train[0][1]==1.)[0]\ndsets.train.vocab[idxs]\n(#1) ['dog']\n\n# Define a function to split the dataset based on the is_valid column\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n\n\ndblock.summary(df)\nSetting-up type transforms pipelines\nCollecting items from            fname          labels  is_valid\n0     000005.jpg           chair      True\n1     000007.jpg             car      True\n2     000009.jpg    horse person      True\n3     000012.jpg             car     False\n4     000016.jpg         bicycle      True\n...          ...             ...       ...\n5006  009954.jpg    horse person      True\n5007  009955.jpg            boat      True\n5008  009958.jpg  person bicycle      True\n5009  009959.jpg             car     False\n5010  009961.jpg             dog     False\n\n[5011 rows x 3 columns]\nFound 5011 items\n2 datasets of sizes 2501,2510\nSetting up Pipeline: get_x -&gt; PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n\nBuilding one sample\n  Pipeline: get_x -&gt; PILBase.create\n    starting from\n      fname       000012.jpg\nlabels             car\nis_valid         False\nName: 3, dtype: object\n    applying get_x gives\n      /home/innom-dt/.fastai/data/pascal_2007/train/000012.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=500x333\n  Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n    starting from\n      fname       000012.jpg\nlabels             car\nis_valid         False\nName: 3, dtype: object\n    applying get_y gives\n      [car]\n    applying MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorMultiCategory([6])\n    applying OneHotEncode -- {'c': None} gives\n      TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\nFinal sample: (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\nCollecting items from            fname          labels  is_valid\n0     000005.jpg           chair      True\n1     000007.jpg             car      True\n2     000009.jpg    horse person      True\n3     000012.jpg             car     False\n4     000016.jpg         bicycle      True\n...          ...             ...       ...\n5006  009954.jpg    horse person      True\n5007  009955.jpg            boat      True\n5008  009958.jpg  person bicycle      True\n5009  009959.jpg             car     False\n5010  009961.jpg             dog     False\n\n[5011 rows x 3 columns]\nFound 5011 items\n2 datasets of sizes 2501,2510\nSetting up Pipeline: get_x -&gt; PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\nSetting up after_item: Pipeline: RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    applying RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} gives\n      (PILImage mode=RGB size=128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    applying ToTensor gives\n      (TensorImage of size 3x128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20)\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20)\n\n\n\nBinary Cross-Entropy\n\nGetting Model Activations\n\nit is important to know how to manually get a mini-batch, pass it into a model, and look at the activations\n\nCan’t directly use nll_loss or softmax for a one-hot-encoded dependent variable\n\nsoftmax requires all predictions sum to 1 and tends to push one activation to be much larger than all the other\n\nnot desirable when there may be multiple objects or none at all in a single image\n\nnll_loss returns the value of just one activation\n\nbinary cross-entropy combines mnist_loss with log\n\n\nlearn = cnn_learner(dls, resnet18)\n\nto_cpu(b)\n\nhttps://docs.fast.ai/torch_core.html#to_cpu\nRecursively map lists of tensors in b to the cpu.\n\n\nto_cpu\n&lt;function fastai.torch_core.to_cpu(b)&gt;\n\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\ntorch.Size([64, 20])\n\nactivs[0]\nTensorBase([ 0.5674, -1.2013,  4.5409, -1.5284, -0.6600,  0.0999, -2.4757, -0.8773, -0.2934, -1.4746, -0.1738,  2.1763, -3.4473, -1.1407,  0.1783, -1.6922, -2.3396,  0.7602, -1.4213, -0.4334],\n       grad_fn=&lt;AliasBackward0&gt;)\nNote: The raw model activations are not scaled between [0,1]\n\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nbinary_cross_entropy(activs, y)\nTensorMultiCategory(1.0367, grad_fn=&lt;AliasBackward0&gt;)\n\n\nnn.BCELoss\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss\nmeasures the binary cross entropy between the predictions and target\n\n\n\nnn.BCEWithLogitsLoss\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\ncombines a sigmoid layer and the BCELoss in a single class\n\n\nnn.BCEWithLogitsLoss\ntorch.nn.modules.loss.BCEWithLogitsLoss\n\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\nTensorMultiCategory(1.0367, grad_fn=&lt;AliasBackward0&gt;)\n\n\nPython Partial Functions\n\nhttps://docs.python.org/3/library/functools.html#functools.partial\nreturn a new partial object that will behave like a function with the positional and keyword arguments\nallows us to bind a function with some arguments or keyword arguments\n\n\npartial\nfunctools.partial\n\ndef say_hello(name, say_what=\"Hello\"): return f\"{say_what} {name}.\"\nsay_hello('Jeremy'),say_hello('Jeremy', 'Ahoy!')\n('Hello Jeremy.', 'Ahoy! Jeremy.')\n\nf = partial(say_hello, say_what=\"Bonjour\")\nf(\"Jeremy\"),f(\"Sylvain\")\n('Bonjour Jeremy.', 'Bonjour Sylvain.')\n\n\naccuracy_multi\n\nhttps://docs.fast.ai/metrics.html#accuracy_multi\ncompute accuracy using a threshold value\n\n\naccuracy_multi\n&lt;function fastai.metrics.accuracy_multi(inp, targ, thresh=0.5, sigmoid=True)&gt;\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy_multi\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.942860\n\n\n0.704590\n\n\n0.234223\n\n\n00:06\n\n\n\n\n1\n\n\n0.821557\n\n\n0.550972\n\n\n0.293825\n\n\n00:06\n\n\n\n\n2\n\n\n0.604402\n\n\n0.202164\n\n\n0.813645\n\n\n00:06\n\n\n\n\n3\n\n\n0.359336\n\n\n0.122809\n\n\n0.943466\n\n\n00:06\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy_multi\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.135016\n\n\n0.122502\n\n\n0.944601\n\n\n00:07\n\n\n\n\n1\n\n\n0.118378\n\n\n0.107208\n\n\n0.950478\n\n\n00:07\n\n\n\n\n2\n\n\n0.098511\n\n\n0.103568\n\n\n0.951613\n\n\n00:07\n\n\n\n\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n(#2) [0.10356765240430832,0.9294222593307495]\n\nlearn.metrics = partial(accuracy_multi, thresh=0.99)\nlearn.validate()\n(#2) [0.10356765240430832,0.9427291750907898]\n\npreds,targs = learn.get_preds()\n\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\nTensorBase(0.9566)\n\n# Try a few different threshold values to see which works best\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-6/index.html#regression",
    "href": "posts/fastai-book-notes/chapter-6/index.html#regression",
    "title": "Notes on fastai Book Ch. 6",
    "section": "Regression",
    "text": "Regression\n\na model is defined by it independent and dependent variables, along with its loss function\nimage regression: the independent variable is an image and the dependent variable is one or more floating point numbers\nkey point model:\n\na key point refers to a specific location represented in an image\n\n\n\nAssemble the Data\n\nBIWI Kinect Head Pose Database\n\nhttps://icu.ee.ethz.ch/research/datsets.html\nover 15k images of 20 people recorded with a Kinect while turning their heads around freely\nDepth and rgb images are provided for each frame\nground in the form of the 3D location of the head and its rotation angles\ncontains 24 directories numbered from 01 to 24 which correspond to the different people photographed\n\neach directory has a corresponding .obj file\neach directory contains .cal files containing the calibration data for the depth and color cameras\neach image has a corresponding _pose.txt file containing the location of center of the head in 3D and the head rotation encoded as 3D rotation matrix\n\n\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\npath\nPath('/home/innom-dt/.fastai/data/biwi_head_pose')\n\npath.ls().sorted()\n(#50) [Path('/home/innom-dt/.fastai/data/biwi_head_pose/01'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01.obj'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/02'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/02.obj'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/03'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/03.obj'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/04'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/04.obj'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/05'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/05.obj')...]\n\n(path/'01').ls().sorted()\n(#1000) [Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/depth.cal'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00003_pose.txt'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00003_rgb.jpg'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00004_pose.txt'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00004_rgb.jpg'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00005_pose.txt'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00005_rgb.jpg'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00006_pose.txt'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00006_rgb.jpg'),Path('/home/innom-dt/.fastai/data/biwi_head_pose/01/frame_00007_pose.txt')...]\n\n# recursivley get all images in the 24 subdirectories\nimg_files = get_image_files(path)\n# get the file names for the corresponding pose.txt files\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\npose_file = img2pose(img_files[0])\npose_file\nPath('/home/innom-dt/.fastai/data/biwi_head_pose/22/frame_00304_pose.txt')\n\n!cat $pose_file\n0.999485 -0.00797222 -0.031067 \n-0.00416483 0.928156 -0.372168 \n0.031802 0.372106 0.927645 \n\n62.3638 96.2159 979.839 \n\nim = PILImage.create(img_files[0])\nim.shape\n(480, 640)\n\nim.to_thumb(160)\n\n\n\n\n\n\n\nnp.genfromtxt\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html\nLoad data from a text file\n\n\nnp.genfromtxt\n&lt;function numpy.genfromtxt(fname, dtype=&lt;class 'float'&gt;, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+,-./:;&lt;=&gt;?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)&gt;\n\n# Contains the calibration values for this folder's rgb camera\n# Skip the last six lines in the file \ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ncal\narray([[517.679,   0.   , 320.   ],\n       [  0.   , 517.679, 240.5  ],\n       [  0.   ,   0.   ,   1.   ]])\n\n# Extract the 2D coordinates for the center of a head\n# Serves as the get_y function for a DataBlock\ndef get_ctr(f):\n    # Skip the last 3 lines in the file\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\nnp.genfromtxt(img2pose(img_files[0]), skip_header=3)\narray([ 62.3638,  96.2159, 979.839 ])\n\nget_ctr(img_files[0])\ntensor([352.9487, 291.3338])\n\n\nPointBlock\n\nDocumentation: https://docs.fast.ai/vision.data.html#PointBlock\nSource Code: https://github.com/fastai/fastai/blob/d84b426e2afe17b3af09b33f49c77bd692625f0d/fastai/vision/data.py#L74\nA TransfromBlock for points in an image\nLets fastai know to perform the same data augmentation steps to the key point values as to the images\n\n\nPointBlock\n&lt;fastai.data.block.TransformBlock at 0x7fb65aaf5490&gt;\n\n# Construct custom DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    # Have the validation set contain images for a single person\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), \n                Normalize.from_stats(*imagenet_stats)]\n)\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\nyb[0]\nTensorPoint([[-0.1246,  0.0960]], device='cuda:0')\n\n\n\nTraining a Model\n# Set range of coordinate values for the model output to [-1,1]\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n\n\n\n\n\ndls.loss_func\nFlattenedLoss of MSELoss()\n\nmin_lr, steep_lr, valley = learn.lr_find(suggest_funcs=(minimum, steep, valley))\n\n\n\n\n\n\nmin_lr\n0.006918309628963471\n\nsteep_lr\n2.0892961401841603e-05\n\nvalley\n0.0010000000474974513\n\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.048689\n\n\n0.026659\n\n\n00:36\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.007270\n\n\n0.002140\n\n\n00:47\n\n\n\n\n1\n\n\n0.002966\n\n\n0.000160\n\n\n00:47\n\n\n\n\n2\n\n\n0.001556\n\n\n0.000042\n\n\n00:47\n\n\n\n\n\n\n\n# Calculate the Root Mean Squared Error\nmath.sqrt(0.000042)\n0.00648074069840786\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-6/index.html#references",
    "href": "posts/fastai-book-notes/chapter-6/index.html#references",
    "title": "Notes on fastai Book Ch. 6",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 5\nNext: Notes on fastai Book Ch. 7"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html",
    "href": "posts/fastai-book-notes/chapter-7/index.html",
    "title": "Notes on fastai Book Ch. 7",
    "section": "",
    "text": "Training a State-of-the-Art Model\nImagenette\nNormalization\nProgressive Resizing\nTest Time Augmentation\nMixup\nLabel Smoothing\nPapers and Math\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#training-a-state-of-the-art-model",
    "href": "posts/fastai-book-notes/chapter-7/index.html#training-a-state-of-the-art-model",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Training a State-of-the-Art Model",
    "text": "Training a State-of-the-Art Model\n\nthe dataset you are given is not necessarily the dataset you want.\naim to have an iteration speed of no more than a couple of minutes\n\nthink about how you can cut down your dataset, or simplify your model to improve your experimentation speed\n\nthe more experiments your can do the better"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#imagenette",
    "href": "posts/fastai-book-notes/chapter-7/index.html#imagenette",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Imagenette",
    "text": "Imagenette\n\nhttps://docs.fast.ai/data.external.html\nA smaller version of the imagenet dataset\nUseful for quick experimentation and iteration\n\n\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.IMAGENETTE)\npath\nPath('/home/innom-dt/.fastai/data/imagenette2')\n\nparent_label\n\nhttps://docs.fast.ai/data.transforms.html#parent_label\nLabel item with the parent folder name.\n\n\nparent_label\n&lt;function fastai.data.transforms.parent_label(o)&gt;\n\ndblock = DataBlock(blocks=(\n    # TransformBlock for images\n    ImageBlock(), \n    # TransformBlock for single-label categorical target\n    CategoryBlock()),\n                   # recursively load image files from path\n                   get_items=get_image_files,\n                   # label images using the parent folder name\n                   get_y=parent_label,\n                   # presize images to 460px\n                   item_tfms=Resize(460),\n                   # Batch resize to 224 and perform data augmentations\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64, num_workers=8)\n\nxresnet50\n&lt;function fastai.vision.models.xresnet.xresnet50(pretrained=False, **kwargs)&gt;\n\n\nCrossEntropyLossFlat\n\nhttps://docs.fast.ai/losses.html#CrossEntropyLossFlat\nSame as nn.CrossEntropyLoss, but flattens input and target.\n\n\nCrossEntropyLossFlat\nfastai.losses.CrossEntropyLossFlat\n\n# Initialize the model without pretrained weights\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.672769\n\n\n3.459394\n\n\n0.301718\n\n\n00:59\n\n\n\n\n1\n\n\n1.224001\n\n\n1.404229\n\n\n0.552651\n\n\n01:00\n\n\n\n\n2\n\n\n0.968035\n\n\n0.996460\n\n\n0.660941\n\n\n01:00\n\n\n\n\n3\n\n\n0.699550\n\n\n0.709341\n\n\n0.771471\n\n\n01:00\n\n\n\n\n4\n\n\n0.578120\n\n\n0.571692\n\n\n0.820388\n\n\n01:00\n\n\n\n\n\n\n\n# Initialize the model without pretrained weights\nmodel = xresnet50(n_out=dls.c)\n# Use mixed precision\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.569645\n\n\n3.962554\n\n\n0.329724\n\n\n00:33\n\n\n\n\n1\n\n\n1.239950\n\n\n2.608771\n\n\n0.355489\n\n\n00:33\n\n\n\n\n2\n\n\n0.964794\n\n\n0.982138\n\n\n0.688200\n\n\n00:34\n\n\n\n\n3\n\n\n0.721289\n\n\n0.681677\n\n\n0.791636\n\n\n00:33\n\n\n\n\n4\n\n\n0.606473\n\n\n0.581621\n\n\n0.824122\n\n\n00:33"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#normalization",
    "href": "posts/fastai-book-notes/chapter-7/index.html#normalization",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Normalization",
    "text": "Normalization\n\nnormalized data: has a mean value of 0 and a standard deviation of 1\nit is easier to train models with normalized data\nnormalization is especially important when using pretrained models\n\nmake sure to use the same normalization stats the pretrained model was trained on\n\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n(TensorImage([0.4498, 0.4448, 0.4141], device='cuda:0'),\n TensorImage([0.2893, 0.2792, 0.3022], device='cuda:0'))\n\nNormalize\n\nhttps://docs.fast.ai/data.transforms.html#Normalize\nNormalize/denormalize a bath of TensorImage\n\n\nNormalize\nfastai.data.transforms.Normalize\n\nNormalize.from_stats\n&lt;bound method Normalize.from_stats of &lt;class 'fastai.data.transforms.Normalize'&gt;&gt;\n\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\ndls = get_dls(64, 224)\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n(TensorImage([-0.2055, -0.0843,  0.0192], device='cuda:0'),\n TensorImage([1.1835, 1.1913, 1.2377], device='cuda:0'))\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.545518\n\n\n3.255928\n\n\n0.342046\n\n\n00:35\n\n\n\n\n1\n\n\n1.234556\n\n\n1.449043\n\n\n0.560866\n\n\n00:35\n\n\n\n\n2\n\n\n0.970857\n\n\n1.310043\n\n\n0.617252\n\n\n00:35\n\n\n\n\n3\n\n\n0.736170\n\n\n0.770678\n\n\n0.758402\n\n\n00:36\n\n\n\n\n4\n\n\n0.619965\n\n\n0.575979\n\n\n0.822629\n\n\n00:36"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#progressive-resizing",
    "href": "posts/fastai-book-notes/chapter-7/index.html#progressive-resizing",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Progressive Resizing",
    "text": "Progressive Resizing\n\nstart training with smaller images and end training with larger images\n\ngradually using larger and larger images as you train\n\nused by a team of fast.ai students to win the DAWNBench competition in 2018\nsmaller images helps training complete much faster\nlarger images helps makes accuracy much higher\nprogressive resizing serves as another form of data augmentation\n\nshould result in better generalization\n\nprogressive resizing might hurt performance when using transfer learning\n\nmost likely to happen if your pretrained model was very similar to your target task and the dataset it was trained on had similar-sized images\n\n\n\ndls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(4, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.627504\n\n\n2.495554\n\n\n0.393951\n\n\n00:21\n\n\n\n\n1\n\n\n1.264693\n\n\n1.233987\n\n\n0.613518\n\n\n00:21\n\n\n\n\n2\n\n\n0.970736\n\n\n0.958903\n\n\n0.707618\n\n\n00:21\n\n\n\n\n3\n\n\n0.740324\n\n\n0.659166\n\n\n0.794996\n\n\n00:21\n\n\n\n\n\n\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.828744\n\n\n1.024683\n\n\n0.669529\n\n\n00:36\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.670041\n\n\n0.716627\n\n\n0.776326\n\n\n00:36\n\n\n\n\n1\n\n\n0.689798\n\n\n0.706051\n\n\n0.768857\n\n\n00:36\n\n\n\n\n2\n\n\n0.589789\n\n\n0.519608\n\n\n0.831217\n\n\n00:35\n\n\n\n\n3\n\n\n0.506784\n\n\n0.436529\n\n\n0.870426\n\n\n00:36\n\n\n\n\n4\n\n\n0.453270\n\n\n0.401451\n\n\n0.877147\n\n\n00:36"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#test-time-augmentation",
    "href": "posts/fastai-book-notes/chapter-7/index.html#test-time-augmentation",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\n\nduring inference or validation, creating multiple versions of each image using augmentation, and then taking the average or maximum of the predictions for each augmented version of the image\ncan result in dramatic improvements in accuracy, depending on the dataset\ndoes not change the time required to train\nwill increase the amount of time required for validation or inference\n\n\nLearner.tta\n\nhttps://docs.fast.ai/learner.html#Learner.tta\nreturns predictions using Test Time Augmentation\n\n\nlearn.tta\n&lt;bound method Learner.tta of &lt;fastai.learner.Learner object at 0x7f75b4be5f40&gt;&gt;\n\npreds,targs = learn.tta()\naccuracy(preds, targs).item()\n0.882001519203186"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#mixup",
    "href": "posts/fastai-book-notes/chapter-7/index.html#mixup",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Mixup",
    "text": "Mixup\n\na powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pretrained model\nintroduced in the 2017 paper mixup: Beyond Empirical Risk Minimization\n\n“While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge\n\nMixup steps\n\nSelect another image from your dataset at random\nPick a weight at random\nTake a weighted average of the selected image with your image, to serve as your independent variable\nTake a weighted average of this image’s labels with your image’s labels, to server as your dependent variable\n\ntarget needs to be one-hot encoded\n\\(\\tilde{x} = \\lambda x_{i} + (1 - \\lambda) x_{j} \\text{, where } x_{i} \\text{ and } x_{j} \\text{ are raw input vectors}\\)\n\\(\\tilde{y} = \\lambda y_{i} + (1 - \\lambda) y_{j} \\text{, where } y_{i} \\text{ and } y_{j} \\text{ are one-hot label encodings}\\)\nmore difficult to train\nless prone to overfitting\nrequires far more epochs to to train to get better accuracy\ncan be applied to types of data other than photos\ncan even be used on activations inside of model\nresolves the issue where it is not typically possible to achieve a perfect loss score\n\nour labels are 1s and 0s, but the outputs of softmax and sigmoid can never equal 1 or 0\nwith Mixup our labels will only be exactly 1 or 0 if two images from the same class are mixed\n\nMixup is “accidentally” making the labels bigger than 0 or smaller than 1\n\ncan be resolved with Label Smoothing\n\n\n\n# Get two images from different classes\nchurch = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\n# Resize images\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\n\n# Scale pixel values to the range [0,1]\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\n# Show the first image\nshow_image(tchurch, ax=axs[0]);\n# Show the second image\nshow_image(tgas, ax=axs[1]);\n# Take the weighted average of the two images\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);\n\n\n\n\n\n\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixUp).to_fp16()\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.332906\n\n\n1.680691\n\n\n0.431292\n\n\n00:21\n\n\n\n\n1\n\n\n1.823880\n\n\n1.699880\n\n\n0.481329\n\n\n00:21\n\n\n\n\n2\n\n\n1.660909\n\n\n1.162998\n\n\n0.650112\n\n\n00:21\n\n\n\n\n3\n\n\n1.520751\n\n\n1.302749\n\n\n0.582524\n\n\n00:21\n\n\n\n\n4\n\n\n1.391567\n\n\n1.256566\n\n\n0.595967\n\n\n00:21\n\n\n\n\n5\n\n\n1.308175\n\n\n1.193670\n\n\n0.638163\n\n\n00:21\n\n\n\n\n6\n\n\n1.224825\n\n\n0.921357\n\n\n0.706871\n\n\n00:21\n\n\n\n\n7\n\n\n1.190292\n\n\n0.846658\n\n\n0.733383\n\n\n00:21\n\n\n\n\n8\n\n\n1.124314\n\n\n0.707856\n\n\n0.780807\n\n\n00:21\n\n\n\n\n9\n\n\n1.085013\n\n\n0.701829\n\n\n0.778193\n\n\n00:21\n\n\n\n\n10\n\n\n1.028223\n\n\n0.509176\n\n\n0.851008\n\n\n00:21\n\n\n\n\n11\n\n\n0.992827\n\n\n0.518169\n\n\n0.845780\n\n\n00:21\n\n\n\n\n12\n\n\n0.945492\n\n\n0.458248\n\n\n0.864078\n\n\n00:21\n\n\n\n\n13\n\n\n0.923450\n\n\n0.418989\n\n\n0.871546\n\n\n00:21\n\n\n\n\n14\n\n\n0.904607\n\n\n0.416422\n\n\n0.876400\n\n\n00:21"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#label-smoothing",
    "href": "posts/fastai-book-notes/chapter-7/index.html#label-smoothing",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Label Smoothing",
    "text": "Label Smoothing\n\nRethinking the Inception Architecture for Computer Vision\nin the theoretical expression of loss, in Classification problems, our targets are one-hot encoded\n\nthe model is trained to return 0 for all categories but one, for which it is trained to return 1\nthis encourages overfitting and gives your a model at inference time that is not going to give meaningful probabilities\nthis can be harmful if your data is not perfectly labeled\n\nlabel smoothing: replace all our 1s with a number that is a bit less than 1, and our 0s with a number that is a bit more then 0\n\nencourages your model to be less confident\nmakes your training more robust, even if there is mislabeled data\nresults in a model that generalizes better at inference\n\nSteps\n\nstart with one-hot encoded labels\nreplace all 0s with \\(\\frac{\\epsilon}{N}\\) where \\(N\\) is the number of classes and \\(\\epsilon\\) is a parameter (usually 0.1)\nreplace all 1s with \\(1 - \\epsilon + \\frac{\\epsilon}{N}\\) to make sure the labels add up to 1\n\n\n\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n2.796061\n\n\n2.399328\n\n\n0.513443\n\n\n00:21\n\n\n\n\n1\n\n\n2.335293\n\n\n2.222970\n\n\n0.584391\n\n\n00:21\n\n\n\n\n2\n\n\n2.125152\n\n\n2.478721\n\n\n0.490291\n\n\n00:21\n\n\n\n\n3\n\n\n1.967522\n\n\n1.977260\n\n\n0.690441\n\n\n00:21\n\n\n\n\n4\n\n\n1.853788\n\n\n1.861635\n\n\n0.715459\n\n\n00:21\n\n\n\n\n5\n\n\n1.747451\n\n\n1.889759\n\n\n0.699776\n\n\n00:21\n\n\n\n\n6\n\n\n1.683000\n\n\n1.710128\n\n\n0.770351\n\n\n00:21\n\n\n\n\n7\n\n\n1.610975\n\n\n1.672254\n\n\n0.780807\n\n\n00:21\n\n\n\n\n8\n\n\n1.534964\n\n\n1.691175\n\n\n0.769231\n\n\n00:21\n\n\n\n\n9\n\n\n1.480721\n\n\n1.490685\n\n\n0.842420\n\n\n00:21\n\n\n\n\n10\n\n\n1.417200\n\n\n1.463211\n\n\n0.852502\n\n\n00:21\n\n\n\n\n11\n\n\n1.360376\n\n\n1.395671\n\n\n0.867812\n\n\n00:21\n\n\n\n\n12\n\n\n1.312882\n\n\n1.360292\n\n\n0.887603\n\n\n00:21\n\n\n\n\n13\n\n\n1.283740\n\n\n1.346170\n\n\n0.890217\n\n\n00:21\n\n\n\n\n14\n\n\n1.264030\n\n\n1.339298\n\n\n0.892830\n\n\n00:21"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#label-smoothing-mixup-and-progressive-resizing",
    "href": "posts/fastai-book-notes/chapter-7/index.html#label-smoothing-mixup-and-progressive-resizing",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Label Smoothing, Mixup and Progressive Resizing",
    "text": "Label Smoothing, Mixup and Progressive Resizing\ndls = get_dls(128, 128)\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy, cbs=MixUp).to_fp16()\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.045166\n\n\n2.561215\n\n\n0.449589\n\n\n00:21\n\n\n\n\n1\n\n\n2.642317\n\n\n2.906508\n\n\n0.405900\n\n\n00:21\n\n\n\n\n2\n\n\n2.473271\n\n\n2.389416\n\n\n0.516804\n\n\n00:21\n\n\n\n\n3\n\n\n2.356234\n\n\n2.263084\n\n\n0.557506\n\n\n00:21\n\n\n\n\n4\n\n\n2.268788\n\n\n2.401770\n\n\n0.544436\n\n\n00:21\n\n\n\n\n5\n\n\n2.181318\n\n\n2.040797\n\n\n0.650485\n\n\n00:21\n\n\n\n\n6\n\n\n2.122742\n\n\n1.711615\n\n\n0.761762\n\n\n00:21\n\n\n\n\n7\n\n\n2.068317\n\n\n1.961520\n\n\n0.688200\n\n\n00:21\n\n\n\n\n8\n\n\n2.022716\n\n\n1.751058\n\n\n0.743839\n\n\n00:21\n\n\n\n\n9\n\n\n1.980203\n\n\n1.635354\n\n\n0.792009\n\n\n00:21\n\n\n\n\n10\n\n\n1.943118\n\n\n1.711313\n\n\n0.758028\n\n\n00:21\n\n\n\n\n11\n\n\n1.889408\n\n\n1.454949\n\n\n0.854742\n\n\n00:21\n\n\n\n\n12\n\n\n1.853412\n\n\n1.433971\n\n\n0.862584\n\n\n00:21\n\n\n\n\n13\n\n\n1.847395\n\n\n1.412596\n\n\n0.867438\n\n\n00:22\n\n\n\n\n14\n\n\n1.817760\n\n\n1.409608\n\n\n0.875280\n\n\n00:23\n\n\n\n\n\n\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(10, 1e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.951753\n\n\n1.672776\n\n\n0.789395\n\n\n00:36\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.872399\n\n\n1.384301\n\n\n0.892457\n\n\n00:36\n\n\n\n\n1\n\n\n1.860005\n\n\n1.441491\n\n\n0.864078\n\n\n00:36\n\n\n\n\n2\n\n\n1.876859\n\n\n1.425859\n\n\n0.867438\n\n\n00:36\n\n\n\n\n3\n\n\n1.851872\n\n\n1.460640\n\n\n0.863331\n\n\n00:36\n\n\n\n\n4\n\n\n1.840423\n\n\n1.413441\n\n\n0.880508\n\n\n00:36\n\n\n\n\n5\n\n\n1.808990\n\n\n1.444332\n\n\n0.863704\n\n\n00:36\n\n\n\n\n6\n\n\n1.777755\n\n\n1.321098\n\n\n0.910754\n\n\n00:36\n\n\n\n\n7\n\n\n1.761589\n\n\n1.312523\n\n\n0.912621\n\n\n00:36\n\n\n\n\n8\n\n\n1.756679\n\n\n1.302988\n\n\n0.919716\n\n\n00:36\n\n\n\n\n9\n\n\n1.745481\n\n\n1.304583\n\n\n0.918969\n\n\n00:36"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#papers-and-math",
    "href": "posts/fastai-book-notes/chapter-7/index.html#papers-and-math",
    "title": "Notes on fastai Book Ch. 7",
    "section": "Papers and Math",
    "text": "Papers and Math\n\nGreek letters used in mathematics, science, and engineering\nGlossary of mathematical symbols\nDetexify\n\ndraw a mathematical symbol and get the latex code"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-7/index.html#references",
    "href": "posts/fastai-book-notes/chapter-7/index.html#references",
    "title": "Notes on fastai Book Ch. 7",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 6\nNext: Notes on fastai Book Ch. 8"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html",
    "href": "posts/fastai-book-notes/chapter-8/index.html",
    "title": "Notes on fastai Book Ch. 8",
    "section": "",
    "text": "Collaborative Filtering\nA First Look at the Data\nLearning the Latent Factors\nCreating the DataLoaders\nCollaborative Filtering from Scratch\nInterpreting Embeddings and Biases\nBootstrapping a Collaborative Filtering Model\nDeep Learning for Collaborative Filtering\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#collaborative-filtering",
    "href": "posts/fastai-book-notes/chapter-8/index.html#collaborative-filtering",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\n\nLook at which items the current user has used or liked, find other users who have used or liked similar items, and then recommend other items that those users have used or like.\n\nidentifies latent factors to compare similarity\n\nused for recommendation systems\nUses\n\nrecommending movies\nfiguring out what to highlight on a user’s homepage\ndeciding what stories to show in a social media feed\nselecting diagnoses for patients\n\n\nProbabilistic Matrix Factorization\nProbabilistic Matrix Factorization Paper\nProbabilistic Matrix Factorization - Matrix Factorization (Part 1)\n\n#hide\n# !pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#a-first-look-at-the-data",
    "href": "posts/fastai-book-notes/chapter-8/index.html#a-first-look-at-the-data",
    "title": "Notes on fastai Book Ch. 8",
    "section": "A First Look at the Data",
    "text": "A First Look at the Data\n\nMovieLens Dataset\n\nhttps://grouplens.org/datasets/movielens/\n25M Dataset\n\n25 million movie ratings\none million tag applications applied to 62,000 movies by 162,000 users\nincludes tag genome data with 15 million relevance scores across 1,129 tags\nreleased 12/2019\n\n100K Dataset\n\n100 thousand movie ratings from 1000 users on 1700 movies\nreleased 4/1998\n\n\n\nURLs.ML_100k\n'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\npath = untar_data(URLs.ML_100k)\npath\nPath('/home/innom-dt/.fastai/data/ml-100k')\n\npd.DataFrame(list(path.ls()))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n/home/innom-dt/.fastai/data/ml-100k/ub.base\n\n\n\n\n1\n\n\n/home/innom-dt/.fastai/data/ml-100k/u5.test\n\n\n\n\n2\n\n\n/home/innom-dt/.fastai/data/ml-100k/u4.base\n\n\n\n\n3\n\n\n/home/innom-dt/.fastai/data/ml-100k/u1.test\n\n\n\n\n4\n\n\n/home/innom-dt/.fastai/data/ml-100k/ua.base\n\n\n\n\n5\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.occupation\n\n\n\n\n6\n\n\n/home/innom-dt/.fastai/data/ml-100k/mku.sh\n\n\n\n\n7\n\n\n/home/innom-dt/.fastai/data/ml-100k/ub.test\n\n\n\n\n8\n\n\n/home/innom-dt/.fastai/data/ml-100k/allbut.pl\n\n\n\n\n9\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.info\n\n\n\n\n10\n\n\n/home/innom-dt/.fastai/data/ml-100k/u5.base\n\n\n\n\n11\n\n\n/home/innom-dt/.fastai/data/ml-100k/u2.test\n\n\n\n\n12\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.genre\n\n\n\n\n13\n\n\n/home/innom-dt/.fastai/data/ml-100k/u2.base\n\n\n\n\n14\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.user\n\n\n\n\n15\n\n\n/home/innom-dt/.fastai/data/ml-100k/README\n\n\n\n\n16\n\n\n/home/innom-dt/.fastai/data/ml-100k/u3.test\n\n\n\n\n17\n\n\n/home/innom-dt/.fastai/data/ml-100k/u1.base\n\n\n\n\n18\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.data\n\n\n\n\n19\n\n\n/home/innom-dt/.fastai/data/ml-100k/u.item\n\n\n\n\n20\n\n\n/home/innom-dt/.fastai/data/ml-100k/ua.test\n\n\n\n\n21\n\n\n/home/innom-dt/.fastai/data/ml-100k/u3.base\n\n\n\n\n22\n\n\n/home/innom-dt/.fastai/data/ml-100k/u4.test\n\n\n\n\n\n\n\n!cat $path/'u.data' | head -5\n196 242 3   881250949\n186 302 3   891717742\n22  377 1   878887116\n244 51  2   880606923\n166 346 1   886397596\ncat: write error: Broken pipe\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n\n\n\n\n\n\nuser\n\n\nmovie\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n0\n\n\n196\n\n\n242\n\n\n3\n\n\n881250949\n\n\n\n\n1\n\n\n186\n\n\n302\n\n\n3\n\n\n891717742\n\n\n\n\n2\n\n\n22\n\n\n377\n\n\n1\n\n\n878887116\n\n\n\n\n3\n\n\n244\n\n\n51\n\n\n2\n\n\n880606923\n\n\n\n\n4\n\n\n166\n\n\n346\n\n\n1\n\n\n886397596\n\n\n\n\n\n\n\nratings[ratings['movie'] == 242][ratings['user'] == 305]\n\n\n\n\n\n\n\n\nuser\n\n\nmovie\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n95720\n\n\n305\n\n\n242\n\n\n5\n\n\n886307828\n\n\n\n\n\n\n\npandas pivot table\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n\n\npd.pivot_table(ratings.head(10), values='rating', index=['user'], columns=['movie'], fill_value=None, sort=False)\n\n\n\n\n\n\nmovie\n\n\n51\n\n\n86\n\n\n242\n\n\n265\n\n\n302\n\n\n346\n\n\n377\n\n\n451\n\n\n465\n\n\n474\n\n\n\n\nuser\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n196\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n186\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n22\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n244\n\n\n2.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n166\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n298\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n4.0\n\n\n\n\n115\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n253\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n5.0\n\n\nNaN\n\n\n\n\n305\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\n\n\n6\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\nNote: The NaN values indicate a given user has not provided a rating for the corresponding movie\n\n\npandas DataFrame.pivot\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html\n\n\nratings.head(10).pivot(values='rating', index=['user'], columns=['movie'])\n\n\n\n\n\n\nmovie\n\n\n51\n\n\n86\n\n\n242\n\n\n265\n\n\n302\n\n\n346\n\n\n377\n\n\n451\n\n\n465\n\n\n474\n\n\n\n\nuser\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n22\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n115\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n166\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n186\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n196\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n244\n\n\n2.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n253\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n5.0\n\n\nNaN\n\n\n\n\n298\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n4.0\n\n\n\n\n305\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\n\n# Create a sample movie entry\n# Index 0: science-fiction\n# Index 1: action\n# Index 2: old movies\n# High values for science fiction and action\n# Low value for old movies\nlast_skywalker = np.array([0.98,0.9,-0.9])\n# Create a sample user\n# High values for science fiction and action\n# Low value for old movies\nuser1 = np.array([0.9,0.8,-0.6])\n\n\nDot Product\n\nthe mathematical operation of multiplying the elements of two vectors together, and then summing up the results\n\n\n# Multiply the movie and user properties to\n# determine how likely the user is to like the movie\n(user1*last_skywalker).sum()\n2.1420000000000003\nNote: The closer the values for the user and movie, the more likely the user is to like the movie\n# The user probably will not like this movie\ncasablanca = np.array([-0.99,-0.3,0.8])\n\n(user1*casablanca).sum()\n-1.611"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#learning-the-latent-factors",
    "href": "posts/fastai-book-notes/chapter-8/index.html#learning-the-latent-factors",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Learning the Latent Factors",
    "text": "Learning the Latent Factors\n\ncan use gradient descent to learn the latent factors for each item and user in a dataset\n\n\nSteps\n\nRandomly initialize some parameters for every user and item in the dataset\n\nthe parameters represent the latent factors\nparameters for each user and item are represented as vectors of numbers\nthe size of the vectors for the users and the items must be dot product compatible\n\nCalculate predictions\n\ntake the dot product of the parameters for each movie with the parameters of each user to predict a rating each user would give each item\n\nCalculate the loss from predictions\n\ncan use any loss function, such as Mean Square Error\n\nUpdate the parameter values for the items and users"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#creating-the-dataloaders",
    "href": "posts/fastai-book-notes/chapter-8/index.html#creating-the-dataloaders",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\n!cat $path/'u.item' | head -5\n1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0\n5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0\ncat: write error: Broken pipe\n\npandas.read_csv\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nread a csv file\nsupports custom delimiters\n\n\nmovies = pd.read_csv(path/'u.item',\n                     # separate columns using '|' instead of ','\n                     delimiter='|', \n                     encoding='latin-1',\n                     # only use the first two columns\n                     usecols=(0,1), \n                     names=('movie','title'), \n                     header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\n\n\ntitle\n\n\n\n\n\n\n0\n\n\n1\n\n\nToy Story (1995)\n\n\n\n\n1\n\n\n2\n\n\nGoldenEye (1995)\n\n\n\n\n2\n\n\n3\n\n\nFour Rooms (1995)\n\n\n\n\n3\n\n\n4\n\n\nGet Shorty (1995)\n\n\n\n\n4\n\n\n5\n\n\nCopycat (1995)\n\n\n\n\n\n\n\n\nDataFrame.merge\n\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\nperform a database-style join\n\n\n# Add the movie titles to the ratings DataFrame\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\n\n\nmovie\n\n\nrating\n\n\ntimestamp\n\n\ntitle\n\n\n\n\n\n\n0\n\n\n196\n\n\n242\n\n\n3\n\n\n881250949\n\n\nKolya (1996)\n\n\n\n\n1\n\n\n63\n\n\n242\n\n\n3\n\n\n875747190\n\n\nKolya (1996)\n\n\n\n\n2\n\n\n226\n\n\n242\n\n\n5\n\n\n883888671\n\n\nKolya (1996)\n\n\n\n\n3\n\n\n154\n\n\n242\n\n\n3\n\n\n879138235\n\n\nKolya (1996)\n\n\n\n\n4\n\n\n306\n\n\n242\n\n\n5\n\n\n876503793\n\n\nKolya (1996)\n\n\n\n\n\n\n\n\nfastai CollabDataLoaders\n\nhttps://docs.fast.ai/collab.html#CollabDataLoaders\nBase DataLoaders for collaborative filtering.\n\n\n\nCollabDataLoaders.from_df\n\nhttps://docs.fast.ai/collab.html#CollabDataLoaders.from_df\nCreate a DataLoaders suitable for collaborative filtering from a pandas DataFrame.\n\n\ndls = CollabDataLoaders.from_df(ratings, \n                                # The column containing the users\n                                user_name='user', \n                                # The column containing the items\n                                item_name='title', \n                                # The column containing the user ratings\n                                rating_name='rating', \n                                bs=64)\ndls.show_batch()\n\n\n\n\n\n\n\n\nuser\n\n\ntitle\n\n\nrating\n\n\n\n\n\n\n0\n\n\n795\n\n\nShining, The (1980)\n\n\n3\n\n\n\n\n1\n\n\n573\n\n\nLeaving Las Vegas (1995)\n\n\n3\n\n\n\n\n2\n\n\n38\n\n\nSnow White and the Seven Dwarfs (1937)\n\n\n5\n\n\n\n\n3\n\n\n378\n\n\nBreakdown (1997)\n\n\n3\n\n\n\n\n4\n\n\n698\n\n\nThird Man, The (1949)\n\n\n2\n\n\n\n\n5\n\n\n452\n\n\nMary Poppins (1964)\n\n\n4\n\n\n\n\n6\n\n\n668\n\n\nIndiana Jones and the Last Crusade (1989)\n\n\n5\n\n\n\n\n7\n\n\n167\n\n\nEscape from L.A. (1996)\n\n\n3\n\n\n\n\n8\n\n\n83\n\n\nFirst Kid (1996)\n\n\n4\n\n\n\n\n9\n\n\n650\n\n\nGlengarry Glen Ross (1992)\n\n\n3\n\n\n\n\n\n\n\ndls.after_iter\n&lt;bound method after_iter of &lt;fastai.tabular.core.TabDataLoader object at 0x7f8a75c0ffd0&gt;&gt;\n\nTabDataLoader\nfastai.tabular.core.TabDataLoader\n\ndls.after_batch\nPipeline: ReadTabBatch\n\nReadTabBatch\nfastai.tabular.core.ReadTabBatch\n\n\nfastai TabDataLoader\n\nhttps://docs.fast.ai/tabular.core.html#TabDataLoader\nA transformed DataLoader for Tabular data\n\n\n\nfastai ReadTabBatch\n\nhttps://docs.fast.ai/tabular.core.html#ReadTabBatch\nTransform TabularPandas values into a Tensor with the ability to decode\n\n\nn_users  = len(dls.classes['user'])\nprint(f\"Number of users: {n_users}\")\nn_movies = len(dls.classes['title'])\nprint(f\"Numer of movies: {n_movies}\")\nn_factors = 5\n\n# Create randomly initialized parameters for users and movies\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nNumber of users: 944\nNumer of movies: 1665\n\none_hot\n&lt;function fastai.torch_core.one_hot(x, c)&gt;\n\n\nfastai one_hot\n\nhttps://docs.fast.ai/torch_core.html#one_hot\nOne-hot encode a value with a specified number of classes.\n\n\n# Create a one-hot encoding for the user at index 3\none_hot_3 = one_hot(3, n_users).float()\nprint(one_hot_3.shape)\nprint(one_hot_3[:10])\ntorch.Size([944])\ntensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n\n# Look up the randomly initialized parameters for the user at index 3\nuser_factors.t() @ one_hot_3\ntensor([-1.2274,  0.0769, -0.1502, -0.7066,  0.3554])\n\nuser_factors[3]\ntensor([-1.2274,  0.0769, -0.1502, -0.7066,  0.3554])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#collaborative-filtering-from-scratch",
    "href": "posts/fastai-book-notes/chapter-8/index.html#collaborative-filtering-from-scratch",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nEmbedding\nfastai.layers.Embedding\n\nfastai Embedding\n\nhttps://docs.fast.ai/layers.html#Embedding\nEmbedding layer with truncated normal initialization\n\n\n\nPyTorch Embedding\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        # Initialize parameters for users and items\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\nx,y = dls.one_batch()\nx.shape\ntorch.Size([64, 2])\n\nmodel = DotProduct(n_users, n_movies, 50)\nprint(model.user_factors)\nprint(model.movie_factors)\nEmbedding(944, 50)\nEmbedding(1665, 50)\n\nprint(list(model.user_factors.parameters())[0].shape)\nlist(model.user_factors.parameters())\ntorch.Size([944, 50])\n\n[Parameter containing:\n tensor([[ 0.0077,  0.0033, -0.0076,  ..., -0.0113,  0.0040,  0.0027],\n         [ 0.0159, -0.0169, -0.0066,  ...,  0.0090,  0.0019,  0.0085],\n         [ 0.0098,  0.0111, -0.0081,  ..., -0.0098,  0.0037,  0.0079],\n         ...,\n         [-0.0009, -0.0022, -0.0017,  ..., -0.0001, -0.0034, -0.0163],\n         [ 0.0065,  0.0161,  0.0046,  ..., -0.0084,  0.0055,  0.0117],\n         [-0.0099,  0.0070, -0.0147,  ...,  0.0002,  0.0051,  0.0035]], requires_grad=True)]\n\nprint(list(model.user_factors.parameters())[0][0].shape)\nlist(model.user_factors.parameters())[0][0]\ntorch.Size([50])\n\ntensor([ 0.0077,  0.0033, -0.0076, -0.0052,  0.0114,  0.0011, -0.0099,  0.0103, -0.0180, -0.0123, -0.0114,  0.0116,  0.0187,  0.0104, -0.0078, -0.0100,  0.0111,  0.0040, -0.0034, -0.0064, -0.0039,\n        -0.0153,  0.0170,  0.0067, -0.0055, -0.0033, -0.0050, -0.0032, -0.0059, -0.0064,  0.0094,  0.0142,  0.0060,  0.0111, -0.0008, -0.0057,  0.0135,  0.0094,  0.0050,  0.0130, -0.0070,  0.0061,\n         0.0043, -0.0046,  0.0059,  0.0027, -0.0030, -0.0113,  0.0040,  0.0027], grad_fn=&lt;SelectBackward0&gt;)\n\nprint(model.user_factors(x[:,0]).shape)\nprint(model.movie_factors(x[:,1]).shape)\ntorch.Size([64, 50])\ntorch.Size([64, 50])\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.327861\n\n\n1.305103\n\n\n00:03\n\n\n\n\n1\n\n\n1.077520\n\n\n1.088959\n\n\n00:03\n\n\n\n\n2\n\n\n0.962058\n\n\n0.962743\n\n\n00:03\n\n\n\n\n3\n\n\n0.818917\n\n\n0.885188\n\n\n00:03\n\n\n\n\n4\n\n\n0.793682\n\n\n0.870693\n\n\n00:03\n\n\n\n\n\n\n\nprint(list(model.user_factors.parameters())[0][0].shape)\nlist(model.user_factors.parameters())[0][0]\ntorch.Size([50])\n\ntensor([ 0.0066,  0.0028, -0.0065, -0.0044,  0.0098,  0.0010, -0.0084,  0.0088, -0.0154, -0.0105, -0.0098,  0.0099,  0.0160,  0.0088, -0.0066, -0.0085,  0.0094,  0.0034, -0.0029, -0.0055, -0.0033,\n        -0.0131,  0.0145,  0.0057, -0.0047, -0.0028, -0.0043, -0.0027, -0.0050, -0.0055,  0.0080,  0.0121,  0.0052,  0.0095, -0.0007, -0.0049,  0.0115,  0.0080,  0.0042,  0.0111, -0.0060,  0.0052,\n         0.0036, -0.0039,  0.0050,  0.0023, -0.0026, -0.0097,  0.0034,  0.0023], device='cuda:0', grad_fn=&lt;SelectBackward0&gt;)\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        # Force predictions to be in the valid range of values\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.018389\n\n\n0.986983\n\n\n00:03\n\n\n\n\n1\n\n\n0.904263\n\n\n0.896296\n\n\n00:03\n\n\n\n\n2\n\n\n0.678135\n\n\n0.870120\n\n\n00:03\n\n\n\n\n3\n\n\n0.486659\n\n\n0.874074\n\n\n00:03\n\n\n\n\n4\n\n\n0.368135\n\n\n0.878279\n\n\n00:03\n\n\n\n\n\n\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        # Add bias values for individual users and items\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.939382\n\n\n0.945063\n\n\n00:03\n\n\n\n\n1\n\n\n0.816200\n\n\n0.851248\n\n\n00:03\n\n\n\n\n2\n\n\n0.612317\n\n\n0.852061\n\n\n00:03\n\n\n\n\n3\n\n\n0.410081\n\n\n0.881404\n\n\n00:03\n\n\n\n\n4\n\n\n0.292610\n\n\n0.889636\n\n\n00:03\n\n\n\n\n\n\nNote: The validation loss stopped improving half way through training, while the train loss continues to improve. This suggests the model is overfitting. * We can’t use data augmentation * An alternative is to use weight decay\n\n\nWeight Decay\n\nAlso called L2 regularization\nconsists of adding the sum of all the weights squared to your loss function\n\na weight decay scalar value is used to control the influence of this addition\n\nencourages the weights to be as small as possible\ncan reduce overfitting by forcing the model to approximate a less complex function\nhinders training, but improves generalization\nfastai weight_decay function\n\n\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n# Add weight decay\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.946304\n\n\n0.936147\n\n\n00:03\n\n\n\n\n1\n\n\n0.854285\n\n\n0.870890\n\n\n00:03\n\n\n\n\n2\n\n\n0.725005\n\n\n0.828756\n\n\n00:03\n\n\n\n\n3\n\n\n0.602717\n\n\n0.819495\n\n\n00:03\n\n\n\n\n4\n\n\n0.495025\n\n\n0.820400\n\n\n00:03\n\n\n\n\n\n\n\n\nCreating Our Own Embedding Module\nclass T(Module):\n    # Tensors are not automatically added as parameters\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n(#0) []\n\nclass T(Module):\n    # Need to wrap Tensors in nn.Parameter()\n    # Create an embedding of size 3\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n(#1) [Parameter containing:\ntensor([[0.7957],\n        [0.3785],\n        [0.9707]], requires_grad=True)]\n\ntype(t.a.weight)\ntorch.nn.parameter.Parameter\n\nPyTorch Tensor.normal_\n\nhttps://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_\nFills tensor with elements sampled from the normal distribution parameterized by the specified mean and std.\n\n\n# Create an Embedding of the specified size\ndef create_params(size):\n    # Initialize values to have a mean of 0 and a standard deviation of 0.01\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.961887\n\n\n0.941220\n\n\n00:04\n\n\n\n\n1\n\n\n0.810713\n\n\n0.871038\n\n\n00:04\n\n\n\n\n2\n\n\n0.738180\n\n\n0.831898\n\n\n00:04\n\n\n\n\n3\n\n\n0.581444\n\n\n0.820112\n\n\n00:04\n\n\n\n\n4\n\n\n0.468566\n\n\n0.820132\n\n\n00:04\n\n\n\n\n\n\nNote: Results should be nearly identical to using the provided Embedding class"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#interpreting-embeddings-and-biases",
    "href": "posts/fastai-book-notes/chapter-8/index.html#interpreting-embeddings-and-biases",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nmovie_bias = learn.model.movie_bias.squeeze()\n# Get the five movies with the lowest bias values\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n['Children of the Corn: The Gathering (1996)',\n 'Cable Guy, The (1996)',\n 'Mortal Kombat: Annihilation (1997)',\n '3 Ninjas: High Noon At Mega Mountain (1998)',\n 'Grease 2 (1982)']\nNote: A low bias value for a movie indicates that even well matched users probably will give them low ratings.\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n['Titanic (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\",\n 'Shawshank Redemption, The (1994)',\n 'As Good As It Gets (1997)']\nNote: A high bias value for a movie indicates the even users who are poorly matched will probably give them high ratings.\n\nPrinciple Component Analysis (PCA)\n\nA technique used to emphasize variation and bring out strong patterins in a dataset\nUsed to make data easy to explore and visualize\nLeverages the fact the data has low intrinsic dimensionality\n\n\nPrinciple Component Analysis Explained Visually\n\n\nComputational Linear Algebra 4: Randomized SVD & Robust PCA\n\n\nfastai Tensor.pca\n\nhttps://docs.fast.ai/torch_core.html#Tensor.pca\nCompute PCA of x with k dimensions.\n\n\nratings.groupby('title')['rating'].count().head()\ntitle\n'Til There Was You (1997)      9\n1-900 (1994)                   5\n101 Dalmatians (1996)        109\n12 Angry Men (1957)          125\n187 (1997)                    41\nName: rating, dtype: int64\n\n# Get the number of ratings for each movie\ng = ratings.groupby('title')['rating'].count()\n# Get the 1000 most rated movies\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\n# Get the index values for the top movies\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\n# Detach the movie_factors embedding from the GPU\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\n# Compute PCA\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\nUsing fastai.collab\n\nfastai collab_learner\n\nhttps://docs.fast.ai/collab.html#collab_learner\nCreate a learner for collaborative filtering\n\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.994621\n\n\n0.937675\n\n\n00:03\n\n\n\n\n1\n\n\n0.824818\n\n\n0.857471\n\n\n00:03\n\n\n\n\n2\n\n\n0.742480\n\n\n0.824739\n\n\n00:03\n\n\n\n\n3\n\n\n0.589424\n\n\n0.814619\n\n\n00:03\n\n\n\n\n4\n\n\n0.514074\n\n\n0.814143\n\n\n00:03\n\n\n\n\n\n\n\nlearn.model\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n['Shawshank Redemption, The (1994)',\n \"Schindler's List (1993)\",\n 'L.A. Confidential (1997)',\n 'Titanic (1997)',\n 'Star Wars (1977)']\n\n\n\nEmbedding Distance\n\nitems with similar embedding values should have similar qualities\nWe can calculate the distance between two 2D coordinates using \\(\\sqrt{x^{2} + y^{2}}\\)\n\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n'Everest (1998)'"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#bootstrapping-a-collaborative-filtering-model",
    "href": "posts/fastai-book-notes/chapter-8/index.html#bootstrapping-a-collaborative-filtering-model",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Bootstrapping a Collaborative Filtering Model",
    "text": "Bootstrapping a Collaborative Filtering Model\n\nThe Bootstrapping Problem\n\nWhat items do you recommend your very first user?\nWhat do you do when a new user signs up?\n\n\nNo magic solution\n\nneed to use common sense\ncould assign new users the mean of all the embedding vectors of your other users\n\nhas the problem that the mean of all the embedding vectors might not be a common combination\nwould probably be better to pick a user to represent average taste\n\ncould use a tabular model based on user metadata to constaruct your initial embedding vector\n\nwhen a user signs up, think about what questions you could ask to hellp you understand their tastes\ncreate a model in which the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions ou ask them, along with their signup metadata\n\nbe wary of a small number of extremely enthusiastic users effectively setting the recommendations for your whole user base\n\ncan trigger positive feedback loops\n\nTry to think about all the ways in which feedback loops may be represented in your system and how you might be able to identify them in your data."
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#deep-learning-for-collaborative-filtering",
    "href": "posts/fastai-book-notes/chapter-8/index.html#deep-learning-for-collaborative-filtering",
    "title": "Notes on fastai Book Ch. 8",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\n\ntake the results of the embedding lookup and concatenate them together\n\ngives us a matrix we can pass through through linear layers and non-linearities\n\nallows us to directly incorporate other data that may be relevant to the recommendation\n\n\nget_emb_sz\n&lt;function fastai.tabular.model.get_emb_sz(to, sz_dict=None)&gt;\n\nfastai get_emb_sz\n\nhttps://docs.fast.ai/tabular.model.html#get_emb_sz\nGet default embedding size from TabularPreprocessor proc or the ones in sz_dict\n\n\nembs = get_emb_sz(dls)\nembs\n[(944, 74), (1665, 102)]\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nmodel = CollabNN(*embs)\nmodel\nCollabNN(\n  (user_factors): Embedding(944, 74)\n  (item_factors): Embedding(1665, 102)\n  (layers): Sequential(\n    (0): Linear(in_features=176, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=1, bias=True)\n  )\n)\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.979066\n\n\n0.948761\n\n\n00:04\n\n\n\n\n1\n\n\n0.881136\n\n\n0.900475\n\n\n00:04\n\n\n\n\n2\n\n\n0.860850\n\n\n0.873675\n\n\n00:04\n\n\n\n\n3\n\n\n0.812708\n\n\n0.859018\n\n\n00:04\n\n\n\n\n4\n\n\n0.757145\n\n\n0.862925\n\n\n00:04\n\n\n\n\n\n\n\n# Use a fastai provided model with the specified number of layers of the specified sizes\n# Add two linear layers of size 100 and 50 respectively\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.989267\n\n\n1.016765\n\n\n00:04\n\n\n\n\n1\n\n\n0.956782\n\n\n0.913140\n\n\n00:04\n\n\n\n\n2\n\n\n0.880594\n\n\n0.879068\n\n\n00:04\n\n\n\n\n3\n\n\n0.822915\n\n\n0.852487\n\n\n00:04\n\n\n\n\n4\n\n\n0.748644\n\n\n0.858280\n\n\n00:04\n\n\n\n\n\n\n\nlearn.model\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=5.5)\n  )\n)\n\nEmbeddingNN\nfastai.collab.EmbeddingNN\n\n\nfastai EmbeddingNN\n\nhttps://docs.fast.ai/collab.html#EmbeddingNN\nCreate a neural network suitable for collaborative filtering\nA subclass of TabularModel\n\nTabularModel\nfastai.tabular.model.TabularModel\n\n\nTabularModel\n\nhttps://docs.fast.ai/tabular.model.html#TabularModel\nBasic model for tabular data"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-8/index.html#references",
    "href": "posts/fastai-book-notes/chapter-8/index.html#references",
    "title": "Notes on fastai Book Ch. 8",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 7\nNext: Notes on fastai Book Ch. 9"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html",
    "href": "posts/fastai-book-notes/chapter-9/index.html",
    "title": "Notes on fastai Book Ch. 09",
    "section": "",
    "text": "Tabular Modeling\nCategorical Embeddings\nBeyond Deep Learning\nThe Dataset\nDecision Trees\nRandom Forests\nModel Interpretation\nExtrapolation and Neural Networks\nEnsembling\nReferences"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#tabular-modeling",
    "href": "posts/fastai-book-notes/chapter-9/index.html#tabular-modeling",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Tabular Modeling",
    "text": "Tabular Modeling\n\ntakes data in the form of a table.\ngoal is to predict the value in one column based on the values in the other columns"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#categorical-embeddings",
    "href": "posts/fastai-book-notes/chapter-9/index.html#categorical-embeddings",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Categorical Embeddings",
    "text": "Categorical Embeddings\n\nContinuous Variables\n\nnumerical data that can be directly fed to a model\ncan add and multiply them directly\n\nCategorical Variables\n\nnonnumerical data that contain a number of discrete levels\nneeds to be converted to numerical data\noperations like addition and multiplication do not have meaning, even when stored as numbers\n\nAn embedding layer is exactly equivalent to placing a linear layer after every one-hot encoded input layer\nEmbeddings transform categorical variables into inputs that are both continuous and meaningful\n\nWe can combine the continuous embedding values with other continuous input data by concatenating the variables\n\n\n\nEarly state of the art tabular deep learning model (2015)\n\nRossman Store Sales Kaggle Competition\n\nCompetition Page\nForecast sales using store, promotion, and competitor data\n\nEntity Embeddings of Categorical Variables (paper)\n\none a gold medal for the competition using deep learning\nmethod involved far less feature engineering based on domain knowledge\n\n\nWide & Deep Learning for Recommender Systems * Explains the recommendation systems used for the Google Play Store * Uses a combination of the dot product and embedding approaches"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#beyond-deep-learning",
    "href": "posts/fastai-book-notes/chapter-9/index.html#beyond-deep-learning",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Beyond Deep Learning",
    "text": "Beyond Deep Learning\n\ndeep learning is not (yet) always the best starting point for analyzing categorical data\nmodern machine learning can be distilled into a couple of widely applicable techniques\n\nEnsembles of decision trees\n\nrandom forests\ngradient boosting machines\nmainly for structured data (e.g. tables)\ntrain faster\noften easier to interpret\ndo not require special GPU hardware for inference at scale\noften require less hyperparameter tuning\nhas a more mature ecosystem for tooling and documentation around them\nsignificantly easier to interpret a model of tabular data\n\nWhich columns in the dataset were the most important for your predictions?\nWhich particular features were most important for some particular observation?\n\n\nMultilayered neural networks trained trained with SGD\n\nshallow and/or deep learning\nmainly for unstructured data\n\naudio\nimages\nnatural language processing\n\nnearly always superior for unstructured data\ntends to result in similar accuracy to ensembles of decision trees for many kinds of structured data\n\n\nUse ensembles of decision trees as your first approach for analyzing a new tabular dataset\nWhen to use deep learning\n\nThere are some high-cardinality categorical variables that are very important\nThere are some columns that contain data that would be best understood with a neural network such as plain text data\n\n\n\nScikit-Learn\n\na popular library for creating machine learning models, using non-deep learning approaches"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#the-dataset",
    "href": "posts/fastai-book-notes/chapter-9/index.html#the-dataset",
    "title": "Notes on fastai Book Ch. 09",
    "section": "The Dataset",
    "text": "The Dataset\n\nBlue Book for Bulldozers\n\npredict the sale price of a piece of equipment at auction, based on its usage, equipment type, and configuration\ndata is sourced from auction result postings and includes information on usage and equipment configurations\nDataset Page\n\nrequires a Kaggle account\n\n\n\n\nKaggle API\n\nGitHub Repository\nDocumentation\nInstall: pip install kaggle\nNeed to get an API Key for your Kaggle account\n\nhttps://www.kaggle.com/me/account\n\n\n\ncreds = '{\"username\":\"\",\"key\":\"\"}'\n\nprint(\"Path.write_text\")\nprint_source(Path.write_text)\n\nprint(\"Path.chmod\")\nprint_source(Path.chmod)\nPath.write_text\n    def write_text(self, data, encoding=None, errors=None):\n        \"\"\"\n        Open the file in text mode, write to it, and close the file.\n        \"\"\"\n        if not isinstance(data, str):\n            raise TypeError('data must be str, not %s' %\n                            data.__class__.__name__)\n        with self.open(mode='w', encoding=encoding, errors=errors) as f:\n            return f.write(data)\n\nPath.chmod\n    def chmod(self, mode):\n        \"\"\"\n        Change the permissions of the path, like os.chmod().\n        \"\"\"\n        self._accessor.chmod(self, mode)\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\n# Save API key to a json file if it does not already exist\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\npath = URLs.path('bluebook')\npath\nPath('/home/innom-dt/.fastai/archive/bluebook')\n\nprint(\"api.competition_download_cli\")\nprint_source(api.competition_download_cli)\napi.competition_download_cli\n    def competition_download_cli(self,\n                                 competition,\n                                 competition_opt=None,\n                                 file_name=None,\n                                 path=None,\n                                 force=False,\n                                 quiet=False):\n        \"\"\" a wrapper to competition_download_files, but first will parse input\n            from API client. Additional parameters are listed here, see\n            competition_download for remaining.\n\n            Parameters\n            =========\n            competition: the name of the competition\n            competition_opt: an alternative competition option provided by cli\n            file_name: the configuration file name\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        competition = competition or competition_opt\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            if file_name is None:\n                self.competition_download_files(competition, path, force,\n                                                quiet)\n            else:\n                self.competition_download_file(competition, file_name, path,\n                                               force, quiet)\n\ndef file_extract(fname, dest=None):\n    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n    if dest is None: dest = Path(fname).parent\n    fname = str(fname)\n    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n    else: raise Exception(f'Unrecognized archive: {fname}')\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli('bluebook-for-bulldozers', path=path)\n    file_extract(path/'bluebook-for-bulldozers.zip')\n\npath.ls(file_type='text')\n(#7) [Path('/home/innom-dt/.fastai/archive/bluebook/ValidSolution.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/Machine_Appendix.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/Valid.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/Test.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/random_forest_benchmark_test.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/median_benchmark.csv'),Path('/home/innom-dt/.fastai/archive/bluebook/TrainAndValid.csv')]\n\n\nLook at the Data\n\nLook at your data directly\n\nunderstand the format\nunderstand how it’s stored\nunderstand what type of values it holds\netc.\n\nSalesID: The unique identifier of the sale\nMachineID: The unique identifier of a machine. A machine can be sold multiple times.\nsaleprice: What the machine sold for at auction (provided only in train.csv)\nsaledate: The data of the sale\n\n\n!cat $path/'TrainAndValid.csv' | head -5\n\ndf = pd.read_csv(path/'TrainAndValid.csv', \n                 # Tell pandas to look at more rows to figure out the data type for each column\n                 low_memory=False)\n\ndf.columns\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\nOrdinal Columns\n\ncolumns containing strings or similar, where those strings have a natural ordering\nneed to tell Pandas the correct ordering for ordinal columns\n\n\ndf['ProductSize'].unique()\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\nsizes\n('Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact')\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\n\n\npandas.core.categorical.Categorical\n\nDocumentation\nRepresents a categorical variable in classic R/ S-plus fashion\nCategoricals can only take on only a limited, and usually fixed, number of possible values\nAll values of the Categorical are either in categories or np.nan\n\n\n\npandas.core.categorical.Categorical.set_categories:\n\nDocumentation\nSets the categories to the specified new_categories\n\n\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n/home/innom-dt/miniconda3/envs/fastbook/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2747: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  res = method(*args, **kwargs)\n\ndep_var = 'SalePrice'\n\n\n\nSelecting a Metric\n\nit is important to note what metric is being used for a project\nselecting the metric is part of the project setup\noften requires more than just selecting a variable that already exists\nthink carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you\nmay need to build the metric from the variables that are available\n\n\nKaggle Competition Metric\n\nRoot Mean Squared Log Error (RMSLE) between the actual prediction prices and predicted auction prices\n\n\n\nnumpy.log\n\nDocumentation\nNatural logarithm, element-wise\n\n\ndf[dep_var] = np.log(df[dep_var])"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#decision-trees",
    "href": "posts/fastai-book-notes/chapter-9/index.html#decision-trees",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nasks a series of binary (yes or no) questions about the data\nthe data is split between a Yes and a No branch\npredictions are made based on the answers to the questions\nthere is a fundamental compromise between how well a decision tree generalizes and how accurate it is on the training set\n\n\nTraining Steps\n\nLoop through each column of the dataset in turn\nFor each column, loop through each possible level of that column in turn\nTry splitting the data into two groups based on whether they are greater than or less than that value\nFind the average sale prices for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group\nAfter looping through all the columns and all the possible levels for each, pick the split point that gen the best predictions using that simple model\nTreat each of the two resulting groups as a separate dataset, and repeat steps 1-5 for each group\nContinue this process recursively, until you reach some stopping criterion\n\n\n\nHandling Dates\n\nsome dates are qualitatively different from others in a way that is often relevant to the systems we are modeling\nwe might want our model to know more than whether a date is more or less recent than another\n\nthe day of the week\nwhether a day is a holiday\nthe month a day is in\netc.\n\nreplace every date column with a set of date metadata\n\n\nfastai add_datepart\n\nDocumentation\nHelper function that adds columns relevant to a date\n\n\nadd_datepart\n&lt;function fastai.tabular.core.add_datepart(df, field_name, prefix=None, drop=True, time=False)&gt;\n\nprint(\"add_datepart\")\nprint_source(add_datepart)\nadd_datepart\ndef add_datepart(df, field_name, prefix=None, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n    make_date(df, field_name)\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    # Pandas removed `dt.week` in v1.1.10\n    week = field.dt.isocalendar().week.astype(field.dt.day.dtype) if hasattr(field.dt, 'isocalendar') else field.dt.week\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) if n != 'Week' else week\n    mask = ~field.isna()\n    df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) // 10 ** 9,np.nan)\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df\n\ndf = add_datepart(df, 'saledate')\n\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\n\nUsing TabularPandas and TabularProc\n\nTabularPandas\n\nDocumentation\nA Tabular object with transforms\nWraps a Pandas DataFrame and provides a few conveniences\nHandles splitting the dataset into training and validation sets\nNeeds to be told which columns are continuous and which columns are categorical\n\n\nTabularPandas\nfastai.tabular.core.TabularPandas\n\nprint_source(TabularPandas)\nclass TabularPandas(Tabular):\n    \"A `Tabular` object with transforms\"\n    def transform(self, cols, f, all_col=True):\n        if not all_col: cols = [c for c in cols if c in self.items.columns]\n        if len(cols) &gt; 0: self[cols] = self[cols].transform(f)\n\n\nTabularProc\n\nDocumentation\nBase class to write a non-lazy tabular processor for dataframes\nreturns the exact same object that is passed to it, after modifying it in place\nruns the transform once, when data is first passed in, rather than lazily as the data is accessed\n\n\nTabularProc\nfastai.tabular.core.TabularProc\n\nprint_source(TabularProc)\nclass TabularProc(InplaceTransform):\n    \"Base class to write a non-lazy tabular processor for dataframes\"\n    def setup(self, items=None, train_setup=False): #TODO: properly deal with train_setup\n        super().setup(getattr(items,'train',items), train_setup=False)\n        # Procs are called as soon as data is available\n        return self(items.items if isinstance(items,Datasets) else items)\n\n    @property\n    def name(self): return f\"{super().name} -- {getattr(self,'__stored_args__',{})}\"\n\n\nCategorify\n\nDocumentation\nreplaces a column with a numeric categorical column\n\nCategorify\nfastai.tabular.core.Categorify\n\nprint_source(Categorify)\nclass Categorify(TabularProc):\n    \"Transform the categorical variables to something similar to `pd.Categorical`\"\n    order = 1\n    def setups(self, to):\n        store_attr(classes={n:CategoryMap(to.iloc[:,n].items, add_na=(n in to.cat_names)) for n in to.cat_names}, but='to')\n\n    def encodes(self, to): to.transform(to.cat_names, partial(_apply_cats, self.classes, 1))\n    def decodes(self, to): to.transform(to.cat_names, partial(_decode_cats, self.classes))\n    def __getitem__(self,k): return self.classes[k]\n\n\nFillMissing\n\nDocumentation\nreplaces values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing\n\n\nFillMissing\nfastai.tabular.core.FillMissing\n\nprint_source(FillMissing)\nclass FillMissing(TabularProc):\n    \"Fill the missing values in continuous columns.\"\n    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n        if fill_vals is None: fill_vals = defaultdict(int)\n        store_attr()\n\n    def setups(self, to):\n        missing = pd.isnull(to.conts).any()\n        store_attr(but='to', na_dict={n:self.fill_strategy(to[n], self.fill_vals[n])\n                            for n in missing[missing].keys()})\n        self.fill_strategy = self.fill_strategy.__name__\n\n    def encodes(self, to):\n        missing = pd.isnull(to.conts)\n        for n in missing.any()[missing.any()].keys():\n            assert n in self.na_dict, f\"nan values in `{n}` but not in setup training set\"\n        for n in self.na_dict.keys():\n            to[n].fillna(self.na_dict[n], inplace=True)\n            if self.add_col:\n                to.loc[:,n+'_na'] = missing[n]\n                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')\n\nprocs = [Categorify, FillMissing]\nNote: Validation sets for time series applications should contain data from a sequence of time after the training set\n# Split the train/validation sets based on sales before and after November 2011 \ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\n\n\nNumPy where\n\nDocumentation\nReturn elements chosen from x or y depending on condition.\n\n\nnp.where\n&lt;function numpy.where&gt;\n\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\n\nfastai cont_cat_split\n\nDocumentation\nreturns column names of cont and cat variables from given DataFrame\n\n\ncont_cat_split\n&lt;function fastai.tabular.core.cont_cat_split(df, max_card=20, dep_var=None)&gt;\n\nprint_source(cont_cat_split)\ndef cont_cat_split(df, max_card=20, dep_var=None):\n    \"Helper function that returns column names of cont and cat variables from given `df`.\"\n    cont_names, cat_names = [], []\n    for label in df:\n        if label in L(dep_var): continue\n        if ((pd.api.types.is_integer_dtype(df[label].dtype) and\n            df[label].unique().shape[0] &gt; max_card) or\n            pd.api.types.is_float_dtype(df[label].dtype)):\n            cont_names.append(label)\n        else: cat_names.append(label)\n    return cont_names, cat_names\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nlen(to.train),len(to.valid)\n(404710, 7988)\n\nto.show(3)\n\n\n\n\n\n\n\n\nUsageBand\n\n\nfiModelDesc\n\n\nfiBaseModel\n\n\nfiSecondaryDesc\n\n\nfiModelSeries\n\n\nfiModelDescriptor\n\n\nProductSize\n\n\nfiProductClassDesc\n\n\nstate\n\n\nProductGroup\n\n\nProductGroupDesc\n\n\nDrive_System\n\n\nEnclosure\n\n\nForks\n\n\nPad_Type\n\n\nRide_Control\n\n\nStick\n\n\nTransmission\n\n\nTurbocharged\n\n\nBlade_Extension\n\n\nBlade_Width\n\n\nEnclosure_Type\n\n\nEngine_Horsepower\n\n\nHydraulics\n\n\nPushblock\n\n\nRipper\n\n\nScarifier\n\n\nTip_Control\n\n\nTire_Size\n\n\nCoupler\n\n\nCoupler_System\n\n\nGrouser_Tracks\n\n\nHydraulics_Flow\n\n\nTrack_Type\n\n\nUndercarriage_Pad_Width\n\n\nStick_Length\n\n\nThumb\n\n\nPattern_Changer\n\n\nGrouser_Type\n\n\nBackhoe_Mounting\n\n\nBlade_Type\n\n\nTravel_Controls\n\n\nDifferential_Type\n\n\nSteering_Controls\n\n\nsaleIs_month_end\n\n\nsaleIs_month_start\n\n\nsaleIs_quarter_end\n\n\nsaleIs_quarter_start\n\n\nsaleIs_year_end\n\n\nsaleIs_year_start\n\n\nauctioneerID_na\n\n\nMachineHoursCurrentMeter_na\n\n\nSalesID\n\n\nMachineID\n\n\nModelID\n\n\ndatasource\n\n\nauctioneerID\n\n\nYearMade\n\n\nMachineHoursCurrentMeter\n\n\nsaleYear\n\n\nsaleMonth\n\n\nsaleWeek\n\n\nsaleDay\n\n\nsaleDayofweek\n\n\nsaleDayofyear\n\n\nsaleElapsed\n\n\nSalePrice\n\n\n\n\n\n\n0\n\n\nLow\n\n\n521D\n\n\n521\n\n\nD\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nWheel Loader - 110.0 to 120.0 Horsepower\n\n\nAlabama\n\n\nWL\n\n\nWheel Loader\n\n\n#na#\n\n\nEROPS w AC\n\n\nNone or Unspecified\n\n\n#na#\n\n\nNone or Unspecified\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n2 Valve\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nNone or Unspecified\n\n\nNone or Unspecified\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nStandard\n\n\nConventional\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n1139246\n\n\n999089\n\n\n3157\n\n\n121\n\n\n3.0\n\n\n2004\n\n\n68.0\n\n\n2006\n\n\n11\n\n\n46\n\n\n16\n\n\n3\n\n\n320\n\n\n1.163635e+09\n\n\n11.097410\n\n\n\n\n1\n\n\nLow\n\n\n950FII\n\n\n950\n\n\nF\n\n\nII\n\n\n#na#\n\n\nMedium\n\n\nWheel Loader - 150.0 to 175.0 Horsepower\n\n\nNorth Carolina\n\n\nWL\n\n\nWheel Loader\n\n\n#na#\n\n\nEROPS w AC\n\n\nNone or Unspecified\n\n\n#na#\n\n\nNone or Unspecified\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n2 Valve\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n23.5\n\n\nNone or Unspecified\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nStandard\n\n\nConventional\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n1139248\n\n\n117657\n\n\n77\n\n\n121\n\n\n3.0\n\n\n1996\n\n\n4640.0\n\n\n2004\n\n\n3\n\n\n13\n\n\n26\n\n\n4\n\n\n86\n\n\n1.080259e+09\n\n\n10.950807\n\n\n\n\n2\n\n\nHigh\n\n\n226\n\n\n226\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\n\n\nNew York\n\n\nSSL\n\n\nSkid Steer Loaders\n\n\n#na#\n\n\nOROPS\n\n\nNone or Unspecified\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nAuxiliary\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nNone or Unspecified\n\n\nNone or Unspecified\n\n\nNone or Unspecified\n\n\nStandard\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\n#na#\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n1139249\n\n\n434808\n\n\n7009\n\n\n121\n\n\n3.0\n\n\n2001\n\n\n2838.0\n\n\n2004\n\n\n2\n\n\n9\n\n\n26\n\n\n3\n\n\n57\n\n\n1.077754e+09\n\n\n9.210340\n\n\n\n\n\n\n\nto.items.head(3)[['state', 'ProductGroup', 'Drive_System', 'Enclosure']]\n\n\n\n\n\n\n\n\nstate\n\n\nProductGroup\n\n\nDrive_System\n\n\nEnclosure\n\n\n\n\n\n\n0\n\n\n1\n\n\n6\n\n\n0\n\n\n3\n\n\n\n\n1\n\n\n33\n\n\n6\n\n\n0\n\n\n3\n\n\n\n\n2\n\n\n32\n\n\n3\n\n\n0\n\n\n6\n\n\n\n\n\n\n\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n\n\n\n\n\n\n\n\nstate\n\n\nProductGroup\n\n\nDrive_System\n\n\nEnclosure\n\n\nSalePrice\n\n\n\n\n\n\n0\n\n\nAlabama\n\n\nWL\n\n\n#na#\n\n\nEROPS w AC\n\n\n11.097410\n\n\n\n\n1\n\n\nNorth Carolina\n\n\nWL\n\n\n#na#\n\n\nEROPS w AC\n\n\n10.950807\n\n\n\n\n2\n\n\nNew York\n\n\nSSL\n\n\n#na#\n\n\nOROPS\n\n\n9.210340\n\n\n\n\n\n\n\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\n\n\nSalePrice\n\n\nMachineID\n\n\nModelID\n\n\n…\n\n\nsaleIs_year_start\n\n\nsaleElapsed\n\n\nauctioneerID_na\n\n\nMachineHoursCurrentMeter_na\n\n\n\n\n\n\n0\n\n\n1139246\n\n\n11.097410\n\n\n999089\n\n\n3157\n\n\n…\n\n\n1\n\n\n1.163635e+09\n\n\n1\n\n\n1\n\n\n\n\n1\n\n\n1139248\n\n\n10.950807\n\n\n117657\n\n\n77\n\n\n…\n\n\n1\n\n\n1.080259e+09\n\n\n1\n\n\n1\n\n\n\n\n2\n\n\n1139249\n\n\n9.210340\n\n\n434808\n\n\n7009\n\n\n…\n\n\n1\n\n\n1.077754e+09\n\n\n1\n\n\n1\n\n\n\n\n\n\n3 rows × 67 columns\n\n\n\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n\n\n\n\n\n\n\n\nstate\n\n\nProductGroup\n\n\nDrive_System\n\n\nEnclosure\n\n\n\n\n\n\n0\n\n\n1\n\n\n6\n\n\n0\n\n\n3\n\n\n\n\n1\n\n\n33\n\n\n6\n\n\n0\n\n\n3\n\n\n\n\n2\n\n\n32\n\n\n3\n\n\n0\n\n\n6\n\n\n\n\n\n\n\nto.classes['ProductSize']\n['#na#', 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact']\n\n\nfastcore save_pickle\n\nDocumentation\nSave a pickle file, to a file name or opened file\n\n\nsave_pickle\n&lt;function fastcore.xtras.save_pickle(fn, o)&gt;\n\nprint_source(save_pickle)\n\ndef save_pickle(fn, o):\n    \"Save a pickle file, to a file name or opened file\"\n    with open_file(fn, 'wb') as f: pickle.dump(o, f)\nsave_pickle(path/'to.pkl',to)\n\n\n\nCreating the Decision Tree\n\nfastcore load_pickle\n\nDocumentation\nLoack a pickle file from a file name or opened file\n\n\nload_pickle\n&lt;function fastcore.xtras.load_pickle(fn)&gt;\n\nprint_source(load_pickle)\ndef load_pickle(fn):\n    \"Load a pickle file from a file name or opened file\"\n    with open_file(fn, 'rb') as f: return pickle.load(f)\n\nto = load_pickle(path/'to.pkl')\n# Define independent and dependent variables\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\nDecisionTreeRegressor\nsklearn.tree._classes.DecisionTreeRegressor\n\n\nscikit-learn DecisionTreeRegressor\n\nDocumentation\nA decision tree regressor\n\n\n# Limit the number of leaf nodes to 4\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\ndraw_tree\n&lt;function fastbook.draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs)&gt;\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\n\n\ndtreeviz\n&lt;function dtreeviz.trees.dtreeviz(tree_model, x_data: (&lt;class 'pandas.core.frame.DataFrame'&gt;, &lt;class 'numpy.ndarray'&gt;) = None, y_data: (&lt;class 'pandas.core.frame.DataFrame'&gt;, &lt;class 'numpy.ndarray'&gt;) = None, feature_names: List[str] = None, target_name: str = None, class_names: (typing.Mapping[numbers.Number, str], typing.List[str]) = None, tree_index: int = None, precision: int = 2, orientation: ('TD', 'LR') = 'TD', instance_orientation: ('TD', 'LR') = 'LR', show_root_edge_labels: bool = True, show_node_labels: bool = False, show_just_path: bool = False, fancy: bool = True, histtype: ('bar', 'barstacked', 'strip') = 'barstacked', highlight_path: List[int] = [], X: numpy.ndarray = None, max_X_features_LR: int = 10, max_X_features_TD: int = 20, depth_range_to_display: tuple = None, label_fontsize: int = 12, ticks_fontsize: int = 8, fontname: str = 'Arial', title: str = None, title_fontsize: int = 14, colors: dict = None, scale=1.0) -&gt; dtreeviz.trees.DTreeViz&gt;\n\n\ndtreeviz : Decision Tree Visualization\n\nA python library for decision tree visualization and model interpretation.\nGitHub Repository\nBlog Post\n\n\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\nNote: There is a clear problem with the year made data. There are bulldozers with YearMade values of 1000. * The value 1000 is likely a placeholder value for missing data. * Makes visualizing the values we are interested more difficult\n\n# Replace YearMade placeholder value with 1950\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n# Don't limit the number of leaf nodes\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\nm_rmse(m, xs, y)\n0.0\n\nm_rmse(m, valid_xs, valid_y)\n0.334935\nNote: Having too many leaf nodes can result in overfitting\nm.get_n_leaves(), len(xs)\n(324560, 404710)\nThere are nearly as many leaves as there are data points\n# Make sure each leaf node contains are least 25 auction records\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n(0.248593, 0.323339)\n\nm.get_n_leaves()\n12397\n\n\n\nCategorical Variables\n\ndecision trees do not need embeddings to work with categorical variables\ncan use one-hot encoding to replace a single categorical variable with multiple one-hot-encoded columns\n\nuse the pandas.get_dummies() method\nthere is not really any evidence that this improves the end result\nSplitting on categorical predictors in random forests"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#random-forests",
    "href": "posts/fastai-book-notes/chapter-9/index.html#random-forests",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Random Forests",
    "text": "Random Forests\n\nperhaps the most widely used and practically important machine learning method\ntrain a a bunch of decision trees, each one on a different random subset of the data, and average the predictions.\nrandom forests are not very sensitive to hyperparameter choices\n\n\nBagging Predictors\n\nPublished by retired Berkeley professor Leo Breiman in 1994\nProcedure\n\nRandomly choose a subset of the rows of your data\nTrain a model using this subset\nSave that model and then return to step 1 a few times\nThis will give you multiple models. To make a prediction, predict using all of the models, and then take the average of each of those predictions.\n\nkey insight: although each of models trained on a subset of data will make more errors, those errors will not be correlated with each other\n\ndifferent models will make different errors\n\nwe can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions.\n\n\n# pip install —pre -f https://sklearn-nightly.scdn8.secure.raxcdn.com scikit-learn —U\n\n\nCreating a Random Forest\n\nn_estimators: number of decision trees\n\ncan have as many as you have time to train\n\nmax_sample: how many rows to sample for training each tree\nmax_features: how many colums to sample at each split point\n\nrandom forests are not very sensitive to this\n\n\n\nRandomForestRegressor\nsklearn.ensemble._forest.RandomForestRegressor\n\nscikit-learn RandomForestRegressor\n\nDocumentation\nA random forest regressor\n\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\nm = rf(xs, y);\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n(0.170966, 0.232699)\n\n# Get the predictions from each decision tree in the forest\npreds = np.stack([t.predict(valid_xs.values) for t in m.estimators_])\n\nr_mse(preds.mean(0), valid_y)\n0.232699\n\n# See the impact of adding more trees on accuracy\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\n\nNote: There is a significant diminishing returns after about 30 trees\n\n\n\nOut-of-Bag Error\n\nmeasure the prediction error of trees on data not included in their data subset\n\n\nr_mse(m.oob_prediction_, y)\n0.210776"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#model-interpretation",
    "href": "posts/fastai-book-notes/chapter-9/index.html#model-interpretation",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are effectively redundant with each other, for purposes of our prediction?\nHow do predictions vary as we vary these columns?\n\n\nTree Variance for Prediction Confidence\n\nuse the standard deviation of predictions across the trees, instead of just the mean\ntells us the relative confidence of predictions\nwe would want to be more cautious of using results for rows where trees give very different results, compared to cases where they are more consistent\n\n\n# Get the predictions for every tree in the forest\npreds = np.stack([t.predict(valid_xs.values) for t in m.estimators_])\n\npreds.shape\n(40, 7988)\n\n# Calculate the standard deviation\npreds_std = preds.std(0)\n\npreds_std[:5]\narray([0.26069358, 0.10409366, 0.09904178, 0.27184634, 0.13110276])\n\n\nFeature Importance\n\nhelps us learn how the random forest makes its predictions #### Steps\n\n\nloop through each tree\nrecursively explore each branch\nat each branch, look to see what feature was used for that split, and how much the model improves as a result of that split\nthe improvement is added to the importance score for that feature\n\n\nsum importance scores across all branches of all trees and normalize them such that they add to 1\n\n\ndef rf_feat_importance(m, df):\n    # Get the feature importance values of each column and place them into a dataframe\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\n\n\nimp\n\n\n\n\n\n\n57\n\n\nYearMade\n\n\n0.173023\n\n\n\n\n6\n\n\nProductSize\n\n\n0.117253\n\n\n\n\n30\n\n\nCoupler_System\n\n\n0.117053\n\n\n\n\n7\n\n\nfiProductClassDesc\n\n\n0.073112\n\n\n\n\n54\n\n\nModelID\n\n\n0.054777\n\n\n\n\n65\n\n\nsaleElapsed\n\n\n0.048835\n\n\n\n\n3\n\n\nfiSecondaryDesc\n\n\n0.046104\n\n\n\n\n31\n\n\nGrouser_Tracks\n\n\n0.041196\n\n\n\n\n12\n\n\nEnclosure\n\n\n0.040495\n\n\n\n\n32\n\n\nHydraulics_Flow\n\n\n0.032725\n\n\n\n\n\n\nNote: The first few most important features have much higher importance scores than the rest\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\n\nRemoving Low-Importance Variables\n\ngenerally, the first step to improving a model is simplifying it\na simpler, more interpretable model is often easier to deploy and maintain\n\n\n# Only keep columns with a feature importance greater than 0.005\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n21\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\nm = rf(xs_imp, y)\n\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n(0.18131, 0.230503)\nNote: Accuracy is about the same, but there are far fewer columns to study\n\nlen(xs.columns), len(xs_imp.columns)\n(66, 21)\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\nRemoving Redundant Features\n\nDetermining Similarity\n\nthe most similar pairs are found by calculating the rank correlation\nrank correlation: all the values are replaced with their rank within the column, and then the correlation is calculated\n\n\ncluster_columns(xs_imp)\n\n\n\n\n\nNote: The pairs of columns that are most similar are the ones that were merged together early in the chart.\n# Train a relatively small random forest and return the OOB score\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nget_oob(xs_imp)\n0.8781576734893485\nNote: The OOB score is a number returns by sklearn that ranges between 1.0 for a perfect model and 0 for a random model\nTry removing potentially redundant variables one at a time.\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n{'saleYear': 0.8768628311464004,\n 'saleElapsed': 0.8722097904682757,\n 'ProductGroupDesc': 0.8770087512874477,\n 'ProductGroup': 0.8778594920344923,\n 'fiModelDesc': 0.8754781084425128,\n 'fiBaseModel': 0.8761168180455399,\n 'Hydraulics_Flow': 0.8774995916903535,\n 'Grouser_Tracks': 0.8775565092698138,\n 'Coupler_System': 0.8770165273393064}\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n0.8750482697068109\nNote: The result is nearly identical to the performance of the model with all the fields.\n# Create DataFrames without the redundant columns\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n# Save the updated DataFrames\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\nxs_final = load_pickle(path/'xs_final.pkl')\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl')\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n(0.183251, 0.232259)\nNote: The accuracy has not really changed after removing the redundant columns\n\n\n\nPartial Dependence\n\nIf a row varied on nothing other than the feature in question, how would it impact the dependent variable?\nExample: How does YearMade impact sales price, all other things being equal?\n\n\nTHE BOOK OF WHY: THE NEW SCIENCE OF CAUSE AND EFFECT\n\nWritten by JUDEA PEARL AND DANA MACKENZIE\n\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\nNote: A large amount of data points have missing values for ProductSize\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\n\nNote: Most of the data is from after 1990\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator\n&lt;bound method PartialDependenceDisplay.from_estimator of &lt;class 'sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay'&gt;&gt;\n\n\nsklearn PartialDependenceDisplay.from_estimator\n\nDocumentation\nPartial Dependence Plot (PDP)\n\n\nfig,ax = plt.subplots(figsize=(12, 4))\nPartialDependenceDisplay.from_estimator(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n\n\n\n\n\nNote: There is a nearly linear (log) relationship between sale price and year made.\nNote: The group with missing values has the lowest price. * Missing values can sometimes be useful predictors, depending on what causes them to be missing * Missing values can also indicate data leakage.\n\n\n\nData Leakage\n\nLeakage in data mining: formulation, detection, and avoidance\ndata leakage: “the introduction of information about the target of a data mining problem which should not be legitimately available to mine from”\ndata leakage is subtle and can take many forms\nmissing values often represent data leakage\n\n\nCase Study\n\nA Kaggle competition designed to predict which researchers would end up receiving research grants\nUsing feature importance on a random forest revealed that\n\nthe model was able to correctly predict who would receive grants 95% of the time\napparently meaningless identifier columns were the most important predictors\nthe day of week and day of year columns were also highly predictive\n\nthe vast majority of grant applications dated on a Sunday were accepted, and many accepted applications were dated on January 1\n\n\na partial dependence plot showed that when the information for the identifier columns was missing, the application was almost always rejected\nIt turned out that the University only filled out much of after an application was already accepted\n\nThe information was not available at the time the application was received and would not be available to a predictive model\n\nAdditionaly, the final processing of applications was often done automatically as a batch at the end of the week, or the end of the year.\n\n\n\nIdentifying Data Leakage\n\nCheck whether the accuracy of the model is too good to be true\nLook for important predictors that don’t make sense in practice\nLook for partial dependence plot results that don’t make sense in practice\n\n\n\n\nTree Interpreter\n\nHelps answer the question “For predicting widht a particular row of data, what were the most important factors, and how did they influence that prediction”\n\n\n\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\nrow = valid_xs_final.iloc[:5]\n\nTreeInterpreter\n\nGitHub Repository\nPackage for interpreting scikit-learn’s decision tree and random forest predictions.\n\n\ntreeinterpreter.predict\n&lt;function treeinterpreter.treeinterpreter.predict(model, X, joint_contribution=False)&gt;\n\nprint_source(treeinterpreter.predict)\ndef predict(model, X, joint_contribution=False):\n    \"\"\" Returns a triple (prediction, bias, feature_contributions), such\n    that prediction ≈ bias + feature_contributions.\n    Parameters\n    ----------\n    model : DecisionTreeRegressor, DecisionTreeClassifier,\n        ExtraTreeRegressor, ExtraTreeClassifier,\n        RandomForestRegressor, RandomForestClassifier,\n        ExtraTreesRegressor, ExtraTreesClassifier\n    Scikit-learn model on which the prediction should be decomposed.\n\n    X : array-like, shape = (n_samples, n_features)\n    Test samples.\n    \n    joint_contribution : boolean\n    Specifies if contributions are given individually from each feature,\n    or jointly over them\n\n    Returns\n    -------\n    decomposed prediction : triple of\n    * prediction, shape = (n_samples) for regression and (n_samples, n_classes)\n        for classification\n    * bias, shape = (n_samples) for regression and (n_samples, n_classes) for\n        classification\n    * contributions, If joint_contribution is False then returns and  array of \n        shape = (n_samples, n_features) for regression or\n        shape = (n_samples, n_features, n_classes) for classification, denoting\n        contribution from each feature.\n        If joint_contribution is True, then shape is array of size n_samples,\n        where each array element is a dict from a tuple of feature indices to\n        to a value denoting the contribution from that feature tuple.\n    \"\"\"\n    # Only single out response variable supported,\n    if model.n_outputs_ &gt; 1:\n        raise ValueError(\"Multilabel classification trees not supported\")\n\n    if (isinstance(model, DecisionTreeClassifier) or\n        isinstance(model, DecisionTreeRegressor)):\n        return _predict_tree(model, X, joint_contribution=joint_contribution)\n    elif (isinstance(model, RandomForestClassifier) or\n          isinstance(model, ExtraTreesClassifier) or\n          isinstance(model, RandomForestRegressor) or\n          isinstance(model, ExtraTreesRegressor)):\n        return _predict_forest(model, X, joint_contribution=joint_contribution)\n    else:\n        raise ValueError(\"Wrong model type. Base learner needs to be a \"\n                         \"DecisionTreeClassifier or DecisionTreeRegressor.\")\n\nprediction: the prediction made by the random forest\nbias: the prediction based on taking the mean of the dependent variable\ncontributions: the total change in prediction due to each of the independent variables\nthe sum of contributions plus bias must equal the prediction for each row\n\n\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\nprediction[0], bias[0], contributions[0].sum()\n(array([9.94708073]), 10.104746057831763, -0.15766532528651994)\n\nwaterfall\n&lt;function waterfall_chart.plot(index, data, Title='', x_lab='', y_lab='', formatting='{:,.1f}', green_color='#29EA38', red_color='#FB3C62', blue_color='#24CAFF', sorted_value=False, threshold=None, other_label='other', net_label='net', rotation_value=30)&gt;\n\n\nwaterfallcharts\n\nGitHub Repository\nQuickly generates standard waterfall charts, takes two ordered lists as inputs.\nWaterfall charts are useful for visualizing marginal value contributions to some system\n\n\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');\n\n\n\n\n\nNote: This kind of information is most useful in production, rather than during model development. * Can provide useful information to users of your data product about the underlying reasoning behind the predictions"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#extrapolation-and-neural-networks",
    "href": "posts/fastai-book-notes/chapter-9/index.html#extrapolation-and-neural-networks",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Extrapolation and Neural Networks",
    "text": "Extrapolation and Neural Networks\n\nrandom forests don’t always generalize well to new data\n\n\nThe Extrapolation Problem\n\nnp.random.seed(42)\n# Generate some data points with a slightly noisy linear relationship\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\n\n\n# Expand single independent variable into a matrix to accommodate sklearn\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n(torch.Size([40]), torch.Size([40, 1]))\n\n# Alternative: Slice the tensor with the special value None\n# introduces an additional unit axis at that location\nx_lin[:,None].shape\ntorch.Size([40, 1])\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\n\nNone: The predictions outside the domain of the training data are all too low. * A tree and a random forest can never predict values outside the range of the training data * A random forest just averages the predictions of a number of trees * A tree just predicts the average value of the rows in a leaf * Need to make sure our validation set does not contain out of domain data\n\n\nFinding Out-of-Domain Data\n\nwe can use a random forest to find out-of-domain data\n\ntry to predict whether a row is in the validation set or the training set\n\n\n\n# Combine validation and training sets\ndf_dom = pd.concat([xs_final, valid_xs_final])\n# Add a dependent variable to track whether a data point is from the validation set\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n# Create and train a random forest\nm = rf(df_dom, is_valid)\n# Get the feature importance for the random forest\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\n\n\nimp\n\n\n\n\n\n\n5\n\n\nsaleElapsed\n\n\n0.858008\n\n\n\n\n10\n\n\nSalesID\n\n\n0.098377\n\n\n\n\n13\n\n\nMachineID\n\n\n0.035284\n\n\n\n\n0\n\n\nYearMade\n\n\n0.002645\n\n\n\n\n4\n\n\nModelID\n\n\n0.001238\n\n\n\n\n7\n\n\nEnclosure\n\n\n0.000891\n\n\n\n\n\n\nNote: Three columns differ significantly between the training and validation sets * saleElapsed: the number of days between the start of the dataset and each row * SalesID: suggests the identifiers for auction sales might increment over time * MachineID: suggests the identifier might increment over time\n\n# Get a baseline of the original random forest model's RMSE\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n# Determine the effect of removing the three columns of note\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\norig 0.231847\nSalesID 0.231492\nsaleElapsed 0.235826\nMachineID 0.231672\nNote: We should be able to remove SalesID and MachineID without losing any accuracy.\n# Remove both SalesID and MachineID\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n0.228826\nNote: Removing the variables slightly improved the model’s accuracy. * Should also make it more resilient over time, and the model should be easier to maintain and understand.\nNote: It can also help to remove old data when it show relationships that are not valid anymore.\nxs['saleYear'].hist();\n\n\n\n\n\n\n# Only use data more recent than 2004\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n(0.177757, 0.229866)\nNote: Accuracy is a little bit better.\n\n\nUsing a Neural Network\n\nfastai’s Tabular Classes\n\na fastai tabular model is a model that takes columns of continuous or categorical data, and predicts a catefory or a continuous value\ncategorical independent variables are passed through an embedding and concatenated, and then any continuous variables are concatenated as well\nthe model created in tabular_learner is an object of class TabularModel\n\n\nprint_source(tabular_learner)\n@delegates(Learner.__init__)\ndef tabular_learner(dls, layers=None, emb_szs=None, config=None, n_out=None, y_range=None, **kwargs):\n    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n    if config is None: config = tabular_config()\n    if layers is None: layers = [200,100]\n    to = dls.train_ds\n    emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n    model = TabularModel(emb_szs, len(dls.cont_names), n_out, layers, y_range=y_range, **config)\n    return TabularLearner(dls, model, **kwargs)\n\nprint_source(TabularModel)\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0.,\n                 y_range=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True),\n                 lin_first=True):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\n\n# Repeat steps to set up a TabularPandas object\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\n# Remove unwanted columns as was done for the random forest\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n# Split continuous and categorical columns, so the categorical columns can be used for embeddings\n# Columns with more than 9000 unique values will be considered continuous\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\nNote: Make sure saleElapsed is set as a continuous variable.\ncont_nn\n['saleElapsed']\n\ndf_nn_final[cat_nn].nunique()\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nModelID               5281\nfiSecondaryDesc        177\nEnclosure                6\nHydraulics_Flow          3\nfiModelDesc           5059\nProductGroup             6\nfiModelDescriptor      140\nHydraulics              12\nDrive_System             4\nTire_Size               17\ndtype: int64\nNote: The fact there are two variables pertaining to the “model” of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information. * This would not necessarily show up when analyzing redundant features, since that relies on the number of variables being sorted in the same order\n\n# Test the impact of removing the 'fiModelDescriptor' column\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n(0.176845, 0.229738)\n\ncat_nn.remove('fiModelDescriptor')\n\nNormalize\nfastai.data.transforms.Normalize\n\nprint_source(Normalize)\n@docs\nclass Normalize(DisplayedTransform):\n    \"Normalize/denorm batch of `TensorImage`\"\n    parameters,order = L('mean', 'std'),99\n    def __init__(self, mean=None, std=None, axes=(0,2,3)): store_attr()\n\n    @classmethod\n    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n\n    def setups(self, dl:DataLoader):\n        if self.mean is None or self.std is None:\n            x,*_ = dl.one_batch()\n            self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n\n    def encodes(self, x:TensorImage): return (x-self.mean) / self.std\n    def decodes(self, x:TensorImage):\n        f = to_cpu if x.device.type=='cpu' else noop\n        return (x*f(self.std) + f(self.mean))\n\n    _docs=dict(encodes=\"Normalize batch\", decodes=\"Denormalize batch\")\n\n# Initialize TabularPandas object with the filtered data\n# Add normalization for neural network\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\n# Can use a large batch size for tabular data\ndls = to_nn.dataloaders(1024)\n\n# Check the min and max values of the dependent variable\ny = to_nn.train.y\ny.min(),y.max()\n(8.465899, 11.863583)\n\n# Limit output values to the range [8,12]\n# Create a neural network with layer size 500 and 250 for the two hidden layers\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.model\nTabularModel(\n  (embeds): ModuleList(\n    (0): Embedding(73, 18)\n    (1): Embedding(7, 5)\n    (2): Embedding(3, 3)\n    (3): Embedding(75, 18)\n    (4): Embedding(5242, 194)\n    (5): Embedding(178, 29)\n    (6): Embedding(7, 5)\n    (7): Embedding(4, 3)\n    (8): Embedding(5060, 190)\n    (9): Embedding(7, 5)\n    (10): Embedding(141, 26)\n    (11): Embedding(13, 7)\n    (12): Embedding(5, 4)\n    (13): Embedding(18, 8)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=516, out_features=500, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=500, out_features=250, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=250, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=8, high=12)\n  )\n)\n\nlearn.lr_find()\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.062503\n\n\n0.059019\n\n\n00:02\n\n\n\n\n1\n\n\n0.053592\n\n\n0.060456\n\n\n00:02\n\n\n\n\n2\n\n\n0.047904\n\n\n0.055088\n\n\n00:02\n\n\n\n\n3\n\n\n0.043731\n\n\n0.051928\n\n\n00:02\n\n\n\n\n4\n\n\n0.040187\n\n\n0.050182\n\n\n00:02\n\n\n\n\n\n\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n0.224014\nNote: Accuracy is better but the neural network took longer to train and is fussier about hyperparameter tuning.\nlearn.save('nn')\nPath('models/nn.pth')"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#ensembling",
    "href": "posts/fastai-book-notes/chapter-9/index.html#ensembling",
    "title": "Notes on fastai Book Ch. 09",
    "section": "Ensembling",
    "text": "Ensembling\n\nWe can average the predictions of models trained using different algorithms\nThe kinds of errors made by the random forest and neural network models are likely quite different\n\nThe average of their predictions are likely better than either one alone\n\n\nNote: The sklearn and PyTorch models create data of different types. * Need to convert the PyTorch data to NumPy format\n\nprint_source(to_np)\ndef to_np(x):\n    \"Convert a tensor to a numpy array.\"\n    return apply(lambda o: o.data.cpu().numpy(), x)\n\nprint_source(apply)\ndef apply(func, x, *args, **kwargs):\n    \"Apply `func` recursively to `x`, passing on args\"\n    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n    res = func(x, *args, **kwargs)\n    return res if x is None else retain_type(res, x)\n\nrf_preds = m.predict(valid_xs_time)\n# Average neural network and random forest predictions\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\nr_mse(ens_preds,valid_y)\n0.221967\nNote The accuracy obtained by averaging the predictions of the random forest and neural network is better than either the random forest or neural network alone\n\nBoosting\n\nAdd models instead of averaging them\n\n\nSteps\n\nTrain a small model that underfits your dataset\nCalculate the predictions in the training set for this model\nSubstract the predictions from the targets\n\nThese are called residuals and represent the error for each point in the training set\n\nGo back to step 1, but use the residuals as the targets for training instead of the original targets\nContinue doing this until you reach a stopping criterion (e.g. max number of trees), or your validation error gets worse\n\n\nThe residuals should get smaller and smaller as more trees are added\nMake predictions by calculating the predictions from each tree and adding them all together\nThere is nothing to prevent overfitting when using boosting\nGradient boosted trees are extremely sensitive to hyperparameter tuning\n\nit is common practice to loop through a range of hyperparameter values to see which works best\n\n\n\n\nscikit-learn HistGradientBoostingRegressor\n\nDocumentation\nHistogram-based Gradient Boosting Regression Tree.\n\n\n\n\nCombining Embeddings with Other Methods\n\nthe embedding learned by a neural network can be used to boost the performance of other machine learning models\nuse the learned embeddings as input features\ncan dramatically improve accuracy over using raw categorical variables\nallows you to get much of the accuracy of a neural network without having to use a neural network at inference time\na set of embeddings can be trained once be used across multiple models"
  },
  {
    "objectID": "posts/fastai-book-notes/chapter-9/index.html#references",
    "href": "posts/fastai-book-notes/chapter-9/index.html#references",
    "title": "Notes on fastai Book Ch. 09",
    "section": "References",
    "text": "References\n\nDeep Learning for Coders with fastai & PyTorch\nThe fastai book GitHub Repository\n\nPrevious: Notes on fastai Book Ch. 8\nNext: Notes on fastai Book Ch. 10"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nInstall Dependencies\nSelect a Model\nModify Transforms\nDefine Learner\nExport the Model\nSummary"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#introduction",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nThe previous fastai-to-unity tutorial series implemented a ResNet-based image classifier in Unity with the Barracuda inference library. The Barracuda library works well with the older ResNet architecture but does not support more recent ones like ConvNeXt and MobileViT at the time of writing.\nThis follow-up series covers using LibTorch, the C++ distribution of PyTorch, to perform inference with these newer model architectures. We’ll modify the original tutorial code and create a dynamic link library (DLL) file to access the LibTorch functionality in Unity.\n\n\nVideo"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#overview",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#overview",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Overview",
    "text": "Overview\nThis post covers the required modifications to the original training code. We’ll finetune models from the Timm library on the same ASL dataset as the original tutorial. The Timm library provides access to a wide range of pretrained computer vision models and integrates with the fastai library. Below is a link to the complete modified training code, along with links for running the notebook on Google Colab and Kaggle.\n\n\n\nGitHub Repository\nColab\nKaggle\n\n\n\n\nJupyter Notebook\nOpen in Colab\nOpen in Kaggle"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#install-dependencies",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#install-dependencies",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Install Dependencies",
    "text": "Install Dependencies\nThe pip package for the Timm library is more stable than the GitHub repository but has fewer model types and pretrained weights. For example, the pip package has pretrained ConvNeXt models but no MobileViT models. However, the latest GitHub version had some issues running the MobileNetV3 models at the time of writing.\nRecent updates to the fastai library resolve some performance issues with PyTorch so let’s update that too. They also provide a new ChannelsLast (beta) callback that further improves performance on modern GPUs.\nUncomment the cell below if running on Google Colab or Kaggle\n# %%capture\n# !pip3 install -U torch torchvision torchaudio\n# !pip3 install -U fastai==2.7.2\n# !pip3 install -U kaggle==1.5.12\n# !pip3 install -U Pillow==9.1.0\n# !pip3 install -U timm==0.5.4 # more stable fewer models\n# !pip3 install -U git+https://github.com/rwightman/pytorch-image-models.git # more models less stable\nNote for Colab: You must restart the runtime in order to use newly installed version of Pillow.\nImport all fastai computer vision functionality\nfrom fastai.vision.all import *\nDisable max rows and columns for pandas\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#select-a-model",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#select-a-model",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Select a Model",
    "text": "Select a Model\nLet’s start by selecting a model from the Timm library to finetune. The available pretrained models depend on the version of the Timm library installed.\nImport the Timm library\nimport timm\ntimm.__version__\n'0.6.2.dev0'\nCheck available pretrained model types\nWe can check which model types have pretrained weights using the timm.list_models() function.\nmodel_types = list(set([model.split('_')[0] for model in timm.list_models(pretrained=True)]))\nmodel_types.sort()\npd.DataFrame(model_types)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nadv\n\n\n\n\n1\n\n\nbat\n\n\n\n\n2\n\n\nbeit\n\n\n\n\n3\n\n\nbotnet26t\n\n\n\n\n4\n\n\ncait\n\n\n\n\n5\n\n\ncoat\n\n\n\n\n6\n\n\nconvit\n\n\n\n\n7\n\n\nconvmixer\n\n\n\n\n8\n\n\nconvnext\n\n\n\n\n9\n\n\ncrossvit\n\n\n\n\n10\n\n\ncspdarknet53\n\n\n\n\n11\n\n\ncspresnet50\n\n\n\n\n12\n\n\ncspresnext50\n\n\n\n\n13\n\n\ndeit\n\n\n\n\n14\n\n\ndensenet121\n\n\n\n\n15\n\n\ndensenet161\n\n\n\n\n16\n\n\ndensenet169\n\n\n\n\n17\n\n\ndensenet201\n\n\n\n\n18\n\n\ndensenetblur121d\n\n\n\n\n19\n\n\ndla102\n\n\n\n\n20\n\n\ndla102x\n\n\n\n\n21\n\n\ndla102x2\n\n\n\n\n22\n\n\ndla169\n\n\n\n\n23\n\n\ndla34\n\n\n\n\n24\n\n\ndla46\n\n\n\n\n25\n\n\ndla46x\n\n\n\n\n26\n\n\ndla60\n\n\n\n\n27\n\n\ndla60x\n\n\n\n\n28\n\n\ndm\n\n\n\n\n29\n\n\ndpn107\n\n\n\n\n30\n\n\ndpn131\n\n\n\n\n31\n\n\ndpn68\n\n\n\n\n32\n\n\ndpn68b\n\n\n\n\n33\n\n\ndpn92\n\n\n\n\n34\n\n\ndpn98\n\n\n\n\n35\n\n\neca\n\n\n\n\n36\n\n\necaresnet101d\n\n\n\n\n37\n\n\necaresnet269d\n\n\n\n\n38\n\n\necaresnet26t\n\n\n\n\n39\n\n\necaresnet50d\n\n\n\n\n40\n\n\necaresnet50t\n\n\n\n\n41\n\n\necaresnetlight\n\n\n\n\n42\n\n\nefficientnet\n\n\n\n\n43\n\n\nefficientnetv2\n\n\n\n\n44\n\n\nens\n\n\n\n\n45\n\n\nese\n\n\n\n\n46\n\n\nfbnetc\n\n\n\n\n47\n\n\nfbnetv3\n\n\n\n\n48\n\n\ngc\n\n\n\n\n49\n\n\ngcresnet33ts\n\n\n\n\n50\n\n\ngcresnet50t\n\n\n\n\n51\n\n\ngcresnext26ts\n\n\n\n\n52\n\n\ngcresnext50ts\n\n\n\n\n53\n\n\ngernet\n\n\n\n\n54\n\n\nghostnet\n\n\n\n\n55\n\n\ngluon\n\n\n\n\n56\n\n\ngmixer\n\n\n\n\n57\n\n\ngmlp\n\n\n\n\n58\n\n\nhalo2botnet50ts\n\n\n\n\n59\n\n\nhalonet26t\n\n\n\n\n60\n\n\nhalonet50ts\n\n\n\n\n61\n\n\nhaloregnetz\n\n\n\n\n62\n\n\nhardcorenas\n\n\n\n\n63\n\n\nhrnet\n\n\n\n\n64\n\n\nig\n\n\n\n\n65\n\n\ninception\n\n\n\n\n66\n\n\njx\n\n\n\n\n67\n\n\nlambda\n\n\n\n\n68\n\n\nlamhalobotnet50ts\n\n\n\n\n69\n\n\nlcnet\n\n\n\n\n70\n\n\nlegacy\n\n\n\n\n71\n\n\nlevit\n\n\n\n\n72\n\n\nmixer\n\n\n\n\n73\n\n\nmixnet\n\n\n\n\n74\n\n\nmnasnet\n\n\n\n\n75\n\n\nmobilenetv2\n\n\n\n\n76\n\n\nmobilenetv3\n\n\n\n\n77\n\n\nmobilevit\n\n\n\n\n78\n\n\nnasnetalarge\n\n\n\n\n79\n\n\nnf\n\n\n\n\n80\n\n\nnfnet\n\n\n\n\n81\n\n\npit\n\n\n\n\n82\n\n\npnasnet5large\n\n\n\n\n83\n\n\npoolformer\n\n\n\n\n84\n\n\nregnetv\n\n\n\n\n85\n\n\nregnetx\n\n\n\n\n86\n\n\nregnety\n\n\n\n\n87\n\n\nregnetz\n\n\n\n\n88\n\n\nrepvgg\n\n\n\n\n89\n\n\nres2net101\n\n\n\n\n90\n\n\nres2net50\n\n\n\n\n91\n\n\nres2next50\n\n\n\n\n92\n\n\nresmlp\n\n\n\n\n93\n\n\nresnest101e\n\n\n\n\n94\n\n\nresnest14d\n\n\n\n\n95\n\n\nresnest200e\n\n\n\n\n96\n\n\nresnest269e\n\n\n\n\n97\n\n\nresnest26d\n\n\n\n\n98\n\n\nresnest50d\n\n\n\n\n99\n\n\nresnet101\n\n\n\n\n100\n\n\nresnet101d\n\n\n\n\n101\n\n\nresnet152\n\n\n\n\n102\n\n\nresnet152d\n\n\n\n\n103\n\n\nresnet18\n\n\n\n\n104\n\n\nresnet18d\n\n\n\n\n105\n\n\nresnet200d\n\n\n\n\n106\n\n\nresnet26\n\n\n\n\n107\n\n\nresnet26d\n\n\n\n\n108\n\n\nresnet26t\n\n\n\n\n109\n\n\nresnet32ts\n\n\n\n\n110\n\n\nresnet33ts\n\n\n\n\n111\n\n\nresnet34\n\n\n\n\n112\n\n\nresnet34d\n\n\n\n\n113\n\n\nresnet50\n\n\n\n\n114\n\n\nresnet50d\n\n\n\n\n115\n\n\nresnet51q\n\n\n\n\n116\n\n\nresnet61q\n\n\n\n\n117\n\n\nresnetblur50\n\n\n\n\n118\n\n\nresnetrs101\n\n\n\n\n119\n\n\nresnetrs152\n\n\n\n\n120\n\n\nresnetrs200\n\n\n\n\n121\n\n\nresnetrs270\n\n\n\n\n122\n\n\nresnetrs350\n\n\n\n\n123\n\n\nresnetrs420\n\n\n\n\n124\n\n\nresnetrs50\n\n\n\n\n125\n\n\nresnetv2\n\n\n\n\n126\n\n\nresnext101\n\n\n\n\n127\n\n\nresnext26ts\n\n\n\n\n128\n\n\nresnext50\n\n\n\n\n129\n\n\nresnext50d\n\n\n\n\n130\n\n\nrexnet\n\n\n\n\n131\n\n\nsebotnet33ts\n\n\n\n\n132\n\n\nsehalonet33ts\n\n\n\n\n133\n\n\nselecsls42b\n\n\n\n\n134\n\n\nselecsls60\n\n\n\n\n135\n\n\nselecsls60b\n\n\n\n\n136\n\n\nsemnasnet\n\n\n\n\n137\n\n\nsequencer2d\n\n\n\n\n138\n\n\nseresnet152d\n\n\n\n\n139\n\n\nseresnet33ts\n\n\n\n\n140\n\n\nseresnet50\n\n\n\n\n141\n\n\nseresnext101\n\n\n\n\n142\n\n\nseresnext101d\n\n\n\n\n143\n\n\nseresnext26d\n\n\n\n\n144\n\n\nseresnext26t\n\n\n\n\n145\n\n\nseresnext26ts\n\n\n\n\n146\n\n\nseresnext50\n\n\n\n\n147\n\n\nseresnextaa101d\n\n\n\n\n148\n\n\nskresnet18\n\n\n\n\n149\n\n\nskresnet34\n\n\n\n\n150\n\n\nskresnext50\n\n\n\n\n151\n\n\nspnasnet\n\n\n\n\n152\n\n\nssl\n\n\n\n\n153\n\n\nswin\n\n\n\n\n154\n\n\nswinv2\n\n\n\n\n155\n\n\nswsl\n\n\n\n\n156\n\n\ntf\n\n\n\n\n157\n\n\ntinynet\n\n\n\n\n158\n\n\ntnt\n\n\n\n\n159\n\n\ntresnet\n\n\n\n\n160\n\n\ntv\n\n\n\n\n161\n\n\ntwins\n\n\n\n\n162\n\n\nvgg11\n\n\n\n\n163\n\n\nvgg13\n\n\n\n\n164\n\n\nvgg16\n\n\n\n\n165\n\n\nvgg19\n\n\n\n\n166\n\n\nvisformer\n\n\n\n\n167\n\n\nvit\n\n\n\n\n168\n\n\nvolo\n\n\n\n\n169\n\n\nwide\n\n\n\n\n170\n\n\nxception\n\n\n\n\n171\n\n\nxception41\n\n\n\n\n172\n\n\nxception41p\n\n\n\n\n173\n\n\nxception65\n\n\n\n\n174\n\n\nxception65p\n\n\n\n\n175\n\n\nxception71\n\n\n\n\n176\n\n\nxcit\n\n\n\n\n\n\nTimm provides many pretrained models, but not all of them are fast enough for real-time applications. We can filter the results by providing a full or partial model name.\nCheck available pretrained ConvNeXt models\npd.DataFrame(timm.list_models('convnext*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nconvnext_base\n\n\n\n\n1\n\n\nconvnext_base_384_in22ft1k\n\n\n\n\n2\n\n\nconvnext_base_in22ft1k\n\n\n\n\n3\n\n\nconvnext_base_in22k\n\n\n\n\n4\n\n\nconvnext_large\n\n\n\n\n5\n\n\nconvnext_large_384_in22ft1k\n\n\n\n\n6\n\n\nconvnext_large_in22ft1k\n\n\n\n\n7\n\n\nconvnext_large_in22k\n\n\n\n\n8\n\n\nconvnext_small\n\n\n\n\n9\n\n\nconvnext_small_384_in22ft1k\n\n\n\n\n10\n\n\nconvnext_small_in22ft1k\n\n\n\n\n11\n\n\nconvnext_small_in22k\n\n\n\n\n12\n\n\nconvnext_tiny\n\n\n\n\n13\n\n\nconvnext_tiny_384_in22ft1k\n\n\n\n\n14\n\n\nconvnext_tiny_hnf\n\n\n\n\n15\n\n\nconvnext_tiny_in22ft1k\n\n\n\n\n16\n\n\nconvnext_tiny_in22k\n\n\n\n\n17\n\n\nconvnext_xlarge_384_in22ft1k\n\n\n\n\n18\n\n\nconvnext_xlarge_in22ft1k\n\n\n\n\n19\n\n\nconvnext_xlarge_in22k\n\n\n\n\n\n\nLet’s go with the convnext_tiny model since we want higher framerates. Each model comes with a set of default configuration parameters. We must keep track of the mean and std values used to normalize the model input. Many pretrained models use the ImageNet normalization stats, but others like MobileViT do not.\nInspect the default configuration for the convnext_tiny model\nfrom timm.models import convnext\nconvnext_model = 'convnext_tiny'\npd.DataFrame.from_dict(convnext.default_cfgs[convnext_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nstem.0\n\n\n\n\nclassifier\n\n\nhead.fc\n\n\n\n\n\n\nCheck available pretrained MobileNetV2 models\npd.DataFrame(timm.list_models('mobilenetv2*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nmobilenetv2_050\n\n\n\n\n1\n\n\nmobilenetv2_100\n\n\n\n\n2\n\n\nmobilenetv2_110d\n\n\n\n\n3\n\n\nmobilenetv2_120d\n\n\n\n\n4\n\n\nmobilenetv2_140\n\n\n\n\n\n\nInspect the default configuration for the mobilenetv2_050 model\nfrom timm.models import efficientnet\nmobilenetv2_model = 'mobilenetv2_050'\npd.DataFrame.from_dict(efficientnet.default_cfgs[mobilenetv2_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_050-3d30d450.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv_stem\n\n\n\n\nclassifier\n\n\nclassifier\n\n\n\n\n\n\nCheck available pretrained MobileNetV3 models\npd.DataFrame(timm.list_models('mobilenetv3*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nmobilenetv3_large_100\n\n\n\n\n1\n\n\nmobilenetv3_large_100_miil\n\n\n\n\n2\n\n\nmobilenetv3_large_100_miil_in21k\n\n\n\n\n3\n\n\nmobilenetv3_rw\n\n\n\n\n4\n\n\nmobilenetv3_small_050\n\n\n\n\n5\n\n\nmobilenetv3_small_075\n\n\n\n\n6\n\n\nmobilenetv3_small_100\n\n\n\n\n\n\nInspect the default configuration for the mobilenetv3_small_050 model\nfrom timm.models import mobilenetv3\nmobilenetv3_model = 'mobilenetv3_small_050'\npd.DataFrame.from_dict(mobilenetv3.default_cfgs[mobilenetv3_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_small_050_lambc-4b7bbe87.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv_stem\n\n\n\n\nclassifier\n\n\nclassifier\n\n\n\n\n\n\nCheck available pretrained MobileViT models * Note: MobileViT models are not available in timm 0.5.4\npd.DataFrame(timm.list_models('mobilevit*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nmobilevit_s\n\n\n\n\n1\n\n\nmobilevit_xs\n\n\n\n\n2\n\n\nmobilevit_xxs\n\n\n\n\n\n\nInspect the default configuration for the mobilevit_xxs model\nfrom timm.models import mobilevit\nmobilevit_model = 'mobilevit_xxs'\npd.DataFrame.from_dict(mobilevit.default_cfgs[mobilevit_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevit_xxs-ad385b40.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 256, 256)\n\n\n\n\npool_size\n\n\n(8, 8)\n\n\n\n\ncrop_pct\n\n\n0.9\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0, 0, 0)\n\n\n\n\nstd\n\n\n(1, 1, 1)\n\n\n\n\nfirst_conv\n\n\nstem.conv\n\n\n\n\nclassifier\n\n\nhead.fc\n\n\n\n\nfixed_input_size\n\n\nFalse\n\n\n\n\n\n\nSelect a model\nmodel_type = convnext\nmodel_name = convnext_model\n# model_type = efficientnet\n# model_name = mobilenetv2_model\n# model_type = mobilenetv3\n# model_name = mobilenetv3_model\n# model_type = mobilevit\n# model_name = mobilevit_model\nAfter picking a model, we’ll store the related normalization stats for future use.\nStore normalization stats\nmean = model_type.default_cfgs[model_name]['mean']\nstd = model_type.default_cfgs[model_name]['std']\nmean, std\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\nDefine target input dimensions\n# size_1_1 = (224, 224)\n# size_3_2 = (224, 336)\n# size_4_3 = (216, 288)\nsize_16_9 = (216, 384)\n# size_16_9_l = (288, 512)\ninput_dims = size_16_9"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#modify-transforms",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#modify-transforms",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Modify Transforms",
    "text": "Modify Transforms\nWe can apply the normalization stats at the end of the batch transforms.\nitem_tfms = [FlipItem(p=1.0), Resize(input_dims, method=ResizeMethod.Pad, pad_mode=PadMode.Border)]\n\nbatch_tfms = [\n    Contrast(max_lighting=0.25),\n    Saturation(max_lighting=0.25),\n    Hue(max_hue=0.05),\n    *aug_transforms(\n        size=input_dims, \n        mult=1.0,\n        do_flip=False,\n        flip_vert=False,\n        max_rotate=0.0,\n        min_zoom=0.5,\n        max_zoom=1.5,\n        max_lighting=0.5,\n        max_warp=0.2, \n        p_affine=0.0,\n        pad_mode=PadMode.Border),\n    Normalize.from_stats(mean=mean, std=std)\n]"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#define-learner",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#define-learner",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Define Learner",
    "text": "Define Learner\nThe training process is identical to the original tutorial, and we only need to pass the name of the Timm model to the vision_learner object.\nlearn = vision_learner(dls, model_name, metrics=metrics).to_fp16()\n# learn = vision_learner(dls, model_name, metrics=metrics, cbs=[ChannelsLast]).to_fp16()"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#export-the-model",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#export-the-model",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Export the Model",
    "text": "Export the Model\nOnce training completes, we need to convert our trained PyTorch model to a TorchScript module for use in LibTorch. We do so using the torch.jit.trace() method.\nGenerate a TorchScript module using the test image\ntraced_script_module = torch.jit.trace(learn.model.cpu(), batched_tensor)\nWe can perform inference with the TorchScript module the same way we would a PyTorch model.\nVerify the TorchScript module’s accuracy\nwith torch.no_grad():\n    torchscript_preds = traced_script_module(batched_tensor)\nlearn.dls.vocab[torch.nn.functional.softmax(torchscript_preds, dim=1).argmax()]\n'J'\nDefine TorchScript file name\nmodule_file_name = f\"{dataset_path.name}-{learn.arch}.pt\"\nmodule_file_name\n'asl-and-some-words-convnext_tiny.pt'\nSome models like MobileViT will require the exact input dimensions in LibTorch as was used in the torch.jit.trace() method. Therefore we’ll convert the PyTorch model again using the training dimensions before saving the TorchScript module to a file.\nGenerate a torchscript module using the target input dimensions and save it to a file\ntorch.randn(1, 3, *input_dims).shape\ntorch.Size([1, 3, 216, 384])\ntraced_script_module = torch.jit.trace(learn.model.cpu(), torch.randn(1, 3, *input_dims))\ntraced_script_module.save(module_file_name)\nWe can export the normalization stats to a JSON file using the same method for the class labels. We’ll load the stats in Unity and pass them to the LibTorch plugin.\nExport model normalization stats\nnormalization_stats = {\"mean\": list(mean), \"std\": list(std)}\nnormalization_stats_file_name = f\"{learn.arch}-normalization_stats.json\"\n\nwith open(normalization_stats_file_name, \"w\") as write_file:\n    json.dump(normalization_stats, write_file)"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#summary",
    "href": "posts/fastai-libtorch-unity-tutorial/part-1/index.html#summary",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1",
    "section": "Summary",
    "text": "Summary\nThis post covered how to modify the training code from the fastai-to-unity tutorialto finetune models from the Timm library and export them as TorchScript modules. Part 2 will cover creating a dynamic link library (DLL) file in Visual Studio to perform inference with these TorchScript modules using LibTorch.\nPrevious: Fastai to Unity Tutorial Pt. 3\nNext: How to Create a LibTorch Plugin for Unity on Windows Pt.2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "",
    "text": "Overview\nDownload Dependencies\nCreate DLL Project\nConfigure the Project\nAdd Include Directories\nLink Libraries\nPost Build Events\nUpdate Precompiled Header File\nUpdate dllmain File\nBuild Solution\nGather Dependencies\nSummary"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#overview",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#overview",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Overview",
    "text": "Overview\nPart 1 covered modifying the training code from the fastai-to-unity tutorial to finetune models from the Timm library and export them as TorchScript modules. This post covers creating a dynamic link library (DLL) file in Visual Studio to perform inference with these TorchScript modules using LibTorch."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#download-dependencies",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#download-dependencies",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Download Dependencies",
    "text": "Download Dependencies\nWe need to download some dependencies before creating our Visual Studio project.\n\nDownload OpenCV\nOur LibTorch DLL requires the OpenCV library to process image data from Unity. The tutorial uses OpenCV 4.6.0, which is available at the link below.\n\nOpenCV 4.6.0 GitHub\n\nSelect the opencv-4.6.0-vc14_vc15.exe option from the Assets list.\n\n\n\n\n\nRun the executable once it finishes downloading. You might get a warning from Windows that the executable is an unrecognized app. We can bypass this by clicking the More info text, then the Run anyway button.\n\n\n\n\n\nThen, click the Run anyway button.\n\n\n\n\n\nThe executable will prompt us to select a location to extract the opencv folder. We’ll need to give Visual Studio this location to access the library’s functionality. I tend to place my C++ dependencies in a dedicated folder for consistency.\n\n\n\n\n\nIf we open the opencv folder, we can see a build folder and a source folder. Everything we need is in the build folder.\n\n\n\n\n\n\n\nDownload LibTorch\nPyTorch provides CPU and CUDA versions of LibTorch for Windows, but we’ll only be working with the CPU version for this post.\nI encountered significant variance in CUDA performance between the Stable, Preview, and LTS releases. Stable CUDA inference speed was slower than CPU inference for the ConvNext and MobileViT models. The LTS release did not seem to support those models at all.\nMore importantly, LibTorch requires a “warmup” phase for CUDA inference where the initial model executions take a few seconds instead of a few milliseconds. Unity did not handle this well and kept crashing. There might be a way around this issue, but I’ll leave that for a future post.\nThe LibTorch releases are available on the PyTorch install page linked below. The tutorial uses the Stable (1.1.0) version.\n\nPyTorch install page\n\n\n\n\n\n\nAs with the OpenCV library, we must pick a location to extract the LibTorch library.\n\n\n\n\n\nNow we can create the Visual Studio DLL project."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#create-dll-project",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#create-dll-project",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Create DLL Project",
    "text": "Create DLL Project\nOpen Visual Studio and select the Create a new project option.\n\n\n\n\n\nType DLL into the text box and select the Dynamic-Link Library (DLL) option. This option automatically configures a few parameters for us compared to starting with a standard console application.\n\n\n\n\n\nChoose a name and location for the project and click the Create button. By default, the DLL file will use the project name."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#configure-the-project",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#configure-the-project",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Configure the Project",
    "text": "Configure the Project\nAt the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#add-include-directories",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#add-include-directories",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nWe need to tell Visual Studio where LibTorch and OpenCV are so we can access their APIs. Right-click the project name in the Solution Explorer panel.\n\n\n\n\n\nSelect the Properties option in the popup menu.\n\n\n\n\n\nNote: We can change the name of the DLL file using the Target Name parameter.\n\n\n\n\n\nIn the Properties Window, open on the C/C++ dropdown. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nAdd the paths for the following folders and click OK.\n\nopencv\\build\\include\nlibtorch-win-shared-with-deps-1.11.0+cpu\\libtorch\\include\nlibtorch-win-shared-with-deps-1.11.0+cpu\\libtorch\\include\\torch\\csrc\\api\\include"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#link-libraries",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#link-libraries",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select Input. Select Additional Dependencies and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the paths to the following files and click OK.\n\nopencv\\build\\x64\\vc15\\lib\\*\nlibtorch-win-shared-with-deps-1.11.0+cpu\\libtorch\\lib\\c10.lib\nlibtorch-win-shared-with-deps-1.11.0+cpu\\libtorch\\lib\\torch.lib\nlibtorch-win-shared-with-deps-1.11.0+cpu\\libtorch\\lib\\torch_cpu.lib"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#post-build-events",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#post-build-events",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Post Build Events",
    "text": "Post Build Events\nOur DLL file will depend on the following DLL files included with the LibTorch and OpenCV libraries.\nOpenCV DLL file\n\n\n\n\n\nLibTorch DLL files\n\n\n\n\n\nWe can add a post-build event in Visual Studio to automatically copy these DLL files to the build folder for the project at compile time. Open the Build Events dropdown in the Properties window and select Post-Build Event. Select Command Line and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the following commands, replacing &lt;parent-folder-path&gt; with the path to the parent folder on your system and click OK.\n\nxcopy &lt;parent-folder-path&gt;\\opencv\\build\\x64\\vc15\\bin\\opencv_world452.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\libtorch\\lib\\*.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nExample: xcopy G:\\Projects\\C++_Projects\\Dependencies\\opencv\\build\\x64\\vc15\\bin\\opencv_world452.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window.\n\n\n\n\n\nWith the dependencies taken care of, we can start modifying the code."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nWe need to make a small change to the pch.h Precompiled Header file to avoid some conflicts with LibTorch. Open the pch.h file by selecting it in the Solution Explorer window.\n\n\n\n\n\nComment or remove the “#include” line for the framework.h header file.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#endif //PCH_H\nAdd required header files\nNext, we’ll add the required header files for LibTorch and OpenCV below //#include \"framework.h\" line.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n// One-stop LibTorch header\n#include &lt;torch/script.h&gt;\n// One-stop OpenCV header\n#include &lt;opencv2/opencv.hpp&gt;\n\n#endif //PCH_H"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#update-dllmain-file",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#update-dllmain-file",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Update dllmain File",
    "text": "Update dllmain File\nBy default, the dllmain.cpp file contains the following code.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\nCreate a macro to mark functions we want to make accessible in Unity\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWrap the code in extern “C” to prevent name-mangling issues with the compiler\nThe rest of our code will go inside here.\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n\n}\nDefine variables\nInside the wrapper, we will declare the persistent variables needed for the DLL.\n// The current torchscript model\ntorch::jit::Module network;\n\n// The mean normalization stats for the current model\nstd::vector&lt;float&gt; mean_stats;\n// The std normalization stats for the current model\nstd::vector&lt;float&gt; std_stats;\nDefine a function to load a TorchScript module\nWe’ll place the code for loading a TorchScript module inside a try-catch block to avoid crashing the application if we pass an incorrect file path. We’ll also update the mean and std vectors here since each model might use different normalization stats.\n// Load a torchscript model from the specified file path\nDLLExport int LoadModel(char* modelPath, float mean[3], float std[3]) {\n\n    try {\n        // Deserialize the ScriptModule from a file using torch::jit::load().\n        network = torch::jit::load(modelPath);\n\n        // Empty the normalization vectors\n        mean_stats.clear();\n        std_stats.clear();\n\n        // Update the normalization vectors\n        for (int i = 0; i &lt; 3; i++) {\n            mean_stats.push_back(mean[i]);\n            std_stats.push_back(std[i]);\n        }           \n    }\n    catch (const c10::Error& e) {\n        // Return a value of -1 if the model fails to load\n        return -1;\n    }\n\n    // Return a value of 0 if the model loads successfully\n    return 0;\n}\nDefine a function to perform inference\nWe will access the pixel data for the input image from Unity with a pointer to a uchar (unsigned 1 byte integer) array and wrap the data in a cv::Mat variable for processing.\nWe must first remove the alpha channel and convert the image to a three-channel matrix of 32-bit floats. We can then initialize an input tensor with the pixel data and apply the usual preprocessing steps.\nOnce again, we’ll use a try-catch block to avoid crashing the application if an error occurs during the forward pass. We can apply the same postprocessing steps as in the training code and return the predicted class index to Unity.\n// Perform inference with the provided texture data\nDLLExport int PerformInference(uchar* inputData, int width, int height) {\n\n    // Store the pixel data for the source input image in an OpenCV Mat\n    cv::Mat texture = cv::Mat(height, width, CV_8UC4, inputData);\n    // Remove the alpha channel\n    cv::cvtColor(texture, texture, cv::COLOR_RGBA2RGB);\n    // Convert RGB image to a three-channel matrix of 32-bit floats\n    texture.convertTo(texture, CV_32FC3);\n\n    // Initialize a tensor using the texture data\n    torch::Tensor input = torch::from_blob(texture.data, { 1, height, width, 3 });\n    // Permute tensor dimensions\n    input = input.permute({ 0, 3, 1, 2 });\n    // Scale and normalize color channel values\n    for (int i=0; i &lt; 3; i++) input[0][i].div_(255.0f).sub_(mean_stats[i]).div_(std_stats[i]);\n\n    // Initialize a vector to store model inputs\n    std::vector&lt;torch::jit::IValue&gt; inputs;\n    // Add input tensor to inputs vector\n    inputs.push_back(input);\n\n    // Initialize predicted class index to an invalid value\n    int class_idx = -1;\n\n    try {\n        // Enable inference mode\n        torch::InferenceMode guard(true);\n        // Perform inference and extract the predicted class index\n        class_idx = torch::softmax(network.forward(inputs).toTensor(), 1).argmax().item&lt;int&gt;();\n    }\n    catch (...) {\n        // Return a value of -2 if an error occurs during the forward pass\n        class_idx = -2;\n    }\n\n    return class_idx;\n}\nThat is all the code needed for the plugin. We can now build the solution to generate the DLL file."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#build-solution",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#build-solution",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Build Solution",
    "text": "Build Solution\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. Visual Studio will generate a new x64 folder in the project directory containing the DLL file and its dependencies."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#gather-dependencies",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#gather-dependencies",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nRight-click the project name in the Solution Explorer panel and select Open Folder in File Explorer from the popup menu.\n\n\n\n\n\nIn the new File Explorer window, go to the parent folder.\n\n\n\n\n\nOpen the x64 → Release subfolder.\n\n\n\n\n\nWe’ll need to copy all the DLL files in this folder to the Unity project."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#summary",
    "href": "posts/fastai-libtorch-unity-tutorial/part-2/index.html#summary",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 2",
    "section": "Summary",
    "text": "Summary\nThis post covered creating a dynamic link library (DLL) file to perform inference with TorchScript modules using LibTorch. Part 3 will cover the required modifications for the original Unity project to use this DLL.\nPrevious: How to Create a LibTorch Plugin for Unity on Windows Pt.1\nNext: How to Create a LibTorch Plugin for Unity on Windows Pt.3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "",
    "text": "Overview\nOpen Unity Project\nAdd New Asset Files\nAllow Unsafe Code\nModify Compute Shader\nCreate ImageClassifierTorch Script\nModify GUI\nAdd ImageClassifierTorch Component\nUpdate On Value Changed Events\nSummary"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#overview",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#overview",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Overview",
    "text": "Overview\nPart 2 covered creating a dynamic link library (DLL) file to perform image classification with TorchScript modules using LibTorch. This post covers the required modifications for the Unity project from the fastai-to-unity tutorial to use this DLL."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#open-unity-project",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#open-unity-project",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Open Unity Project",
    "text": "Open Unity Project\nOpen the Fastai-Unity-Tutorial project in the Unity Editor. The project is available in the GitHub repository linked below for anyone who did not follow the previous tutorial series.\n\nfastai-to-unity-tutorial GitHub repository"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#add-new-asset-files",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#add-new-asset-files",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Add New Asset Files",
    "text": "Add New Asset Files\nFirst, we’ll create a new folder to store the DLL files from part 2. Create a new folder called Plugins, then create a subfolder named x86_64.\n\n\n\n\n\nCopy all the DLL files into the Assets/Plugins/x86_64 folder. We then need to close and reopen the project for Unity to load the plugin files.\n\nPlugins Folder Google Drive\n\n\n\n\n\n\nNext, we’ll create a folder to store the TorchScript modules. TorchScript modules are not supported asset types, so we need to place them in a StreamingAssets folder. Create a new folder named StreamingAssets. We’ll put the files in a new subfolder called TorchScriptModules to keep things organized.\n\n\n\n\n\nAdd any TorchScript files into the Assets/StreamingAssets/TorchScriptModules folder.\n\nTorchScriptModules Folder Google Drive\n\n\n\n\n\n\nLastly, we’ll store the JSON files with the normalization stats in a new assets folder called NormalizationStats."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Allow Unsafe Code",
    "text": "Allow Unsafe Code\nRather than copying the input image from Unity to the LibTorch plugin, we’ll pass a pointer to the pixel data. First, we need to allow unsafe code for the Unity project. Select Edit → Project Settings... from the top menu.\n\n\n\n\n\nOpen the Player → Other Settings dropdown and scroll down to the Allow 'unsafe' Code checkbox. Enable the setting and close the Project Settings window.\n\n\n\n\n\nNow we can start modifying the code."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#modify-compute-shader",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#modify-compute-shader",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Modify Compute Shader",
    "text": "Modify Compute Shader\nThe input image gets flipped upside down when we send it to the plugin. We can pre-flip the image in the ProcessingShader compute shader before sending it to the plugin. We need to know the height of the input image, which we can access with the Texture2D::GetDimensions function.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel NormalizeImageNet\n#pragma kernel FlipXAxis\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Flip the image around the x-axis\n[numthreads(8, 8, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Stores the InputImage width\n    uint width;\n    // Stores the InputImage height\n    uint height;\n    // Get the dimensions of the InputImage\n    InputImage.GetDimensions(width, height);\n\n    // Update the y value for the pixel coordinates\n    int2 coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}\n\n// Apply the ImageNet normalization stats from PyTorch to an image\n[numthreads(8, 8, 1)]\nvoid NormalizeImageNet(uint3 id : SV_DispatchThreadID)\n{\n    // Set the pixel color values for the processed image\n    Result[id.xy] = float4(\n        // Normalize the red color channel values\n        (InputImage[id.xy].r - 0.4850f) / 0.2290f,\n        // Normalize the green color channel values\n        (InputImage[id.xy].g - 0.4560f) / 0.2240f,\n        // Normalize the blue color channel values\n        (InputImage[id.xy].b - 0.4060f) / 0.2250f,\n        // Ignore the alpha/transparency channel\n        InputImage[id.xy].a);\n}"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#create-imageclassifiertorch-script",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#create-imageclassifiertorch-script",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Create ImageClassifierTorch Script",
    "text": "Create ImageClassifierTorch Script\nDuplicate the ImageClassifier script and name the copy ImageClassifierTorch.\n\n\n\n\n\nUpdate class name\nOpen the new script in the code editor and replace the class name with the new file name.\npublic class ImageClassifierTorch : MonoBehaviour\nUpdate required namespaces\nWe no longer need the Barracuda namespace. Instead, we need the System.Runtime.InteropServices namespace to handle interactions with the LibTorch plugin.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Rendering;\nusing System;\nusing UnityEngine.UI;\nusing System.Runtime.InteropServices;\nRemove Barracuda code\nWe need to delete all the public and private Barracuda variables, along with the InitializeWorker and OnDisable methods.\nUpdate data processing variables\nWe can remove the processingMaterial variable from the Data Processing section. We no longer need to download model output from the GPU to the CPU. However, we now need to download the input image to the CPU before sending it to the plugin. We can do this asynchronously to help reduce the GPU-to-CPU performance bottleneck.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 216;\n[Tooltip(\"The compute shader for GPU processing\")]\npublic ComputeShader processingShader;\n[Tooltip(\"Asynchronously download input image from the GPU to the CPU.\")]\npublic bool useAsyncGPUReadback = true;\nUpdate variables for user interface\nWe’ll add a new dropdown so that we can switch between the available TorchScript modules at runtime.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayPredictedClass = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.red;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available torchscript models\")]\npublic Dropdown modelDropdown;\nDefine public variables for the LibTorch plugin\nNext, we’ll create variables to indicate the StreamingAssets subfolder for the TorchScript modules and add the JSON files with the normalization stats.\n[Header(\"Libtorch\")]\n[Tooltip(\"The name of the libtorch models folder\")]\npublic string torchscriptModulesDir = \"TorchScriptModules\";\n[Tooltip(\"A list json files containing the normalization stats for available models\")]\npublic TextAsset[] normalizationStatsList;\nUpdate input variables\nLike in the previous tutorial series, when using asynchronous GPU readback, we need one Texture that stores data on the GPU and one that stores data on the CPU.\n// The test image dimensions\nprivate Vector2Int imageDims;\n// The test image texture\nprivate Texture imageTexture;\n// The current screen object dimensions\nprivate Vector2Int screenDims;\n// The model GPU input texture\nprivate RenderTexture inputTextureGPU;\n// The model CPU input texture\nprivate Texture2D inputTextureCPU;\nDefine private variables for the LibTorch plugin\nWe’ll store the full paths and names for the Torchscript modules in separate lists. We also need to create another little class that indicates the structure of the JSON content for files with normalization stats.\n// File paths for the available torchscript models\nprivate List&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available torchscript models\nprivate List&lt;string&gt; modelNames = new List&lt;string&gt;();\n\n// A class for reading in normalization stats from a JSON file\nclass NormalizationStats { public float[] mean; public float[] std; }\nImport functions from the LibTorch plugin\nWe pass the pointer to the input pixel data as an IntPtr.\n// Name of the DLL file\nconst string dll = \"Libtorch_CPU_Image_Classifier_DLL\";\n\n[DllImport(dll)]\nprivate static extern int LoadModel(string model, float[] mean, float[] std);\n\n[DllImport(dll)]\nprivate static extern int PerformInference(IntPtr inputData, int width, int height);\nDefine method to get the available TorchScript modules\n/// &lt;summary&gt;\n/// Get the file paths for available torchscript models\n/// &lt;/summary&gt;\nprivate void GetTorchModels()\n{\n    // Get the paths for the .pt file for each model\n    foreach (string file in System.IO.Directory.GetFiles($\"{Application.streamingAssetsPath}/{modelsDir}\"))\n    {\n        if (file.EndsWith(\".pt\"))\n        {\n            modelPaths.Add(file);\n            string modelName = file.Split('\\\\')[1].Split('.')[0];\n            modelNames.Add(modelName.Substring(0, modelName.Length));\n        }\n    }\n}\nUpdate method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nprivate void InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach(WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add TorchScript model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n}\nUpdate Start method\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Initialize list of class labels from JSON file\n    classes = JsonUtility.FromJson&lt;ClassLabels&gt;(classLabels.text).classes;\n\n    // Get the file paths for available torchscript models\n    GetTorchModels();\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n\n    // Update the selected torchscript model\n    UpdateTorchScriptModel();\n}\nUpdate method to process images using a compute shader\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The target image RenderTexture&lt;/param&gt;\n/// &lt;param name=\"computeShader\"&gt;The target ComputerShader&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The target ComputeShader function&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, ComputeShader computeShader, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = computeShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = new RenderTexture(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    computeShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release RenderTexture\n    result.Release();\n}\nUpdate method to handle asynchronous GPU readback\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Make sure the Texture2D is not null\n    if (inputTextureCPU)\n    {\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        inputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Textur2D\n        inputTextureCPU.Apply();\n    }\n}\nDefine method to send the input texture data to the plugin\n/// &lt;summary&gt;\n/// Pin memory for the input data and pass a reference to the plugin for inference\n/// &lt;/summary&gt;\n/// &lt;param name=\"texture\"&gt;The input texture&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic unsafe int UploadTexture(Texture2D texture)\n{\n    int classIndex = -1;\n\n    //Pin Memory\n    fixed (byte* p = texture.GetRawTextureData())\n    {\n        // Perform inference and get the predicted class index\n        classIndex = PerformInference((IntPtr)p, texture.width, texture.height);\n    }\n\n    return classIndex;\n}\nModify Update method\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputDims.x} x {inputDims.y}\");\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGBA32, false);\n    }\n\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputTextureGPU.width}x{inputTextureGPU.height}\");\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Flip image before sending to DLL\n    ProcessImageGPU(inputTextureGPU, processingShader, \"FlipXAxis\");\n\n    // Download pixel data from GPU to CPU\n    if (useAsyncGPUReadback)\n    {\n        AsyncGPUReadback.Request(inputTextureGPU, 0, TextureFormat.RGBA32, OnCompleteReadback);\n    }\n    else\n    {\n        RenderTexture.active = inputTextureGPU;\n        inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n        inputTextureCPU.Apply();\n    }\n\n    // Send reference to inputData to DLL\n    classIndex = UploadTexture(inputTextureCPU);\n    if (printDebugMessages) Debug.Log($\"Class Index: {classIndex}\");\n\n    // Check if index is valid\n    bool validIndex = classIndex &gt;= 0 && classIndex &lt; classes.Length;\n    if (printDebugMessages) Debug.Log(validIndex ? $\"Predicted Class: {classes[classIndex]}\" : \"Invalid index\");\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n}\nDefine a method to update the current TorchScript model\n/// &lt;summary&gt;\n/// Update the selected torchscript model\n/// &lt;/summary&gt;\npublic void UpdateTorchScriptModel()\n{\n    string modelName = modelNames[modelDropdown.value];\n    float[] mean = new float[] { };\n    float[] std = new float[] { };\n\n    foreach (TextAsset textAsset in normalizationStatsList)\n    {\n        if (modelName.Contains(textAsset.name.Split(\"-\")[0]))\n        {\n            // Initialize the normalization stats from JSON file\n            mean = JsonUtility.FromJson&lt;NormalizationStats&gt;(textAsset.text).mean;\n            std = JsonUtility.FromJson&lt;NormalizationStats&gt;(textAsset.text).std;\n        }\n    }\n\n    if (mean.Length == 0)\n    {\n        Debug.Log(\"Unable to find normalization stats\");\n        return;\n    }\n    {\n        string mean_str = \"\";\n        foreach (float val in mean) mean_str += $\"{val} \";\n        Debug.Log($\"Mean Stats: {mean_str}\");\n        string std_str = \"\";\n        foreach (float val in std) std_str += $\"{val} \";\n        Debug.Log($\"Std Stats: {std_str}\");\n    }\n\n    // Load the specified torchscript model\n    int result = LoadModel(modelPaths[modelDropdown.value], mean, std);\n    Debug.Log(result == 0 ? \"Model loaded successfully\" : \"error loading the model\");\n}"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#modify-gui",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#modify-gui",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Modify GUI",
    "text": "Modify GUI\nAs mentioned earlier, we’ll add a new dropdown menu to the GUI so we can switch between available TorchScript modules at runtime. Select the WebcamDeviceText and WebcamDropdown objects and press Ctrl-d to duplicate them. Rename the duplicates to TorchScriptModelText and TorchScriptModelDropdown respectively.\n\n\n\n\n\nSelect the TorchScriptModelText object and update the Pos Y value to -145 and the Text value to TorchScript Model: in the Inspector tab.\n\n\n\n\n\nThen, select the TorchScriptModelDropdown object and update the Pos Y value to -165 in the Inspector tab.\n\n\n\n\n\nThe updated GUI should look like the image below."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#add-imageclassifiertorch-component",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#add-imageclassifiertorch-component",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Add ImageClassifierTorch Component",
    "text": "Add ImageClassifierTorch Component\nNow we can add the new ImageClassifierTorch script to the InferenceManager object. Make sure to disable the existing ImageClassifier component, as shown below."
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#update-on-value-changed-events",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#update-on-value-changed-events",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Update On Value Changed Events",
    "text": "Update On Value Changed Events\nWith the ImageClassifierTorch component added, we can update the On Value Changed events for the WebcamToggle, WebcamDropdown, and TorchScriptModelDropdown objects.\nUpdate the WebcamToggle On Value Changed Event\n\n\n\n\n\nUpdate the WebcamDropdown On Value Changed Event\n\n\n\n\n\nUpdate the TorchScriptModelDropdown On Value Changed Event"
  },
  {
    "objectID": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#summary",
    "href": "posts/fastai-libtorch-unity-tutorial/part-3/index.html#summary",
    "title": "How to Create a LibTorch Plugin for Unity on Windows Pt. 3",
    "section": "Summary",
    "text": "Summary\nThis tutorial series covered creating a LibTorch plugin to perform inference with recent model architectures in the Unity game engine. LibTorch also provides the ability to update the model weights within the Unity application, which we might explore in a future tutorial.\nPrevious: How to Create a LibTorch Plugin for Unity on Windows Pt.2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nInstall Dependencies\nSelect a Model\nModify Transforms\nDefine Learner\nExport the Model\nBenchmark OpenVINO Inference\nSummary"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#introduction",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial is a follow-up to the fastai-to-unity tutorial series and covers using OpenVINO, an open-source toolkit for optimizing model inference, instead of Unity’s Barracuda library. OpenVINO enables significantly faster CPU inference than Barracuda and supports more model types. It also supports GPU inference for integrated and discrete Intel GPUs and will be able to leverage the AI hardware acceleration available in Intel’s upcoming ARC GPUs.\nWe’ll modify the original tutorial code and create a dynamic link library (DLL) file to access the OpenVINO functionality in Unity.\n\n\nVideo"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#overview",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#overview",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Overview",
    "text": "Overview\nThis post covers the required modifications to the original training code. We’ll finetune models from the Timm library on the same ASL dataset as the original tutorial, just like in this previous follow-up. Below is a link to the complete modified training code, along with links for running the notebook on Google Colab and Kaggle.\n\n\n\nGitHub Repository\nColab\n        Kaggle        \n\n\n\n\nJupyter Notebook\nOpen in Colab\nOpen in Kaggle"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#install-dependencies",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#install-dependencies",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Install Dependencies",
    "text": "Install Dependencies\nThe pip package for the Timm library is generally more stable than the GitHub repository but may have fewer model types and pretrained weights. However, the latest pip version had some issues running the MobileNetV3 models at the time of writing. Downgrade to version 0.5.4 to use those models.\nRecent updates to the fastai library resolve some performance issues with PyTorch so let’s update that too.\nWe need to install the openvino-dev pip package to convert trained models to OpenVINO’s Intermediate Representation (IR) format.\nUncomment the cell below if running on Google Colab or Kaggle\n# %%capture\n# !pip3 install -U torch torchvision torchaudio\n# !pip3 install -U fastai==2.7.6\n# !pip3 install -U kaggle==1.5.12\n# !pip3 install -U Pillow==9.1.0\n# !pip3 install -U timm==0.6.5 # more stable fewer models\n# # !pip3 install -U git+https://github.com/rwightman/pytorch-image-models.git # more models less stable\n# !pip3 install openvino-dev==2022.1.0 \nNote for Colab: You must restart the runtime in order to use newly installed version of Pillow.\nImport all fastai computer vision functionality\nfrom fastai.vision.all import *\nimport fastai\nfastai.__version__\n'2.7.6'\nDisable max rows and columns for pandas\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#select-a-model",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#select-a-model",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Select a Model",
    "text": "Select a Model\nLet’s start by selecting a model from the Timm library to finetune. The available pretrained models depend on the version of the Timm library installed.\nImport the Timm library\nimport timm\ntimm.__version__\n'0.6.5'\nCheck available pretrained model types\nWe can check which model types have pretrained weights using the timm.list_models() function.\nmodel_types = list(set([model.split('_')[0] for model in timm.list_models(pretrained=True)]))\nmodel_types.sort()\npd.DataFrame(model_types)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nadv\n\n\n\n\n1\n\n\nbat\n\n\n\n\n2\n\n\nbeit\n\n\n\n\n3\n\n\nbotnet26t\n\n\n\n\n4\n\n\ncait\n\n\n\n\n5\n\n\ncoat\n\n\n\n\n6\n\n\nconvit\n\n\n\n\n7\n\n\nconvmixer\n\n\n\n\n8\n\n\nconvnext\n\n\n\n\n9\n\n\ncrossvit\n\n\n\n\n10\n\n\ncs3darknet\n\n\n\n\n11\n\n\ncspdarknet53\n\n\n\n\n12\n\n\ncspresnet50\n\n\n\n\n13\n\n\ncspresnext50\n\n\n\n\n14\n\n\ndarknet53\n\n\n\n\n15\n\n\ndeit\n\n\n\n\n16\n\n\ndeit3\n\n\n\n\n17\n\n\ndensenet121\n\n\n\n\n18\n\n\ndensenet161\n\n\n\n\n19\n\n\ndensenet169\n\n\n\n\n20\n\n\ndensenet201\n\n\n\n\n21\n\n\ndensenetblur121d\n\n\n\n\n22\n\n\ndla102\n\n\n\n\n23\n\n\ndla102x\n\n\n\n\n24\n\n\ndla102x2\n\n\n\n\n25\n\n\ndla169\n\n\n\n\n26\n\n\ndla34\n\n\n\n\n27\n\n\ndla46\n\n\n\n\n28\n\n\ndla46x\n\n\n\n\n29\n\n\ndla60\n\n\n\n\n30\n\n\ndla60x\n\n\n\n\n31\n\n\ndm\n\n\n\n\n32\n\n\ndpn107\n\n\n\n\n33\n\n\ndpn131\n\n\n\n\n34\n\n\ndpn68\n\n\n\n\n35\n\n\ndpn68b\n\n\n\n\n36\n\n\ndpn92\n\n\n\n\n37\n\n\ndpn98\n\n\n\n\n38\n\n\neca\n\n\n\n\n39\n\n\necaresnet101d\n\n\n\n\n40\n\n\necaresnet269d\n\n\n\n\n41\n\n\necaresnet26t\n\n\n\n\n42\n\n\necaresnet50d\n\n\n\n\n43\n\n\necaresnet50t\n\n\n\n\n44\n\n\necaresnetlight\n\n\n\n\n45\n\n\nedgenext\n\n\n\n\n46\n\n\nefficientnet\n\n\n\n\n47\n\n\nefficientnetv2\n\n\n\n\n48\n\n\nens\n\n\n\n\n49\n\n\nese\n\n\n\n\n50\n\n\nfbnetc\n\n\n\n\n51\n\n\nfbnetv3\n\n\n\n\n52\n\n\ngc\n\n\n\n\n53\n\n\ngcresnet33ts\n\n\n\n\n54\n\n\ngcresnet50t\n\n\n\n\n55\n\n\ngcresnext26ts\n\n\n\n\n56\n\n\ngcresnext50ts\n\n\n\n\n57\n\n\ngernet\n\n\n\n\n58\n\n\nghostnet\n\n\n\n\n59\n\n\ngluon\n\n\n\n\n60\n\n\ngmixer\n\n\n\n\n61\n\n\ngmlp\n\n\n\n\n62\n\n\nhalo2botnet50ts\n\n\n\n\n63\n\n\nhalonet26t\n\n\n\n\n64\n\n\nhalonet50ts\n\n\n\n\n65\n\n\nhaloregnetz\n\n\n\n\n66\n\n\nhardcorenas\n\n\n\n\n67\n\n\nhrnet\n\n\n\n\n68\n\n\nig\n\n\n\n\n69\n\n\ninception\n\n\n\n\n70\n\n\njx\n\n\n\n\n71\n\n\nlambda\n\n\n\n\n72\n\n\nlamhalobotnet50ts\n\n\n\n\n73\n\n\nlcnet\n\n\n\n\n74\n\n\nlegacy\n\n\n\n\n75\n\n\nlevit\n\n\n\n\n76\n\n\nmixer\n\n\n\n\n77\n\n\nmixnet\n\n\n\n\n78\n\n\nmnasnet\n\n\n\n\n79\n\n\nmobilenetv2\n\n\n\n\n80\n\n\nmobilenetv3\n\n\n\n\n81\n\n\nmobilevit\n\n\n\n\n82\n\n\nmobilevitv2\n\n\n\n\n83\n\n\nnasnetalarge\n\n\n\n\n84\n\n\nnf\n\n\n\n\n85\n\n\nnfnet\n\n\n\n\n86\n\n\npit\n\n\n\n\n87\n\n\npnasnet5large\n\n\n\n\n88\n\n\npoolformer\n\n\n\n\n89\n\n\nregnetv\n\n\n\n\n90\n\n\nregnetx\n\n\n\n\n91\n\n\nregnety\n\n\n\n\n92\n\n\nregnetz\n\n\n\n\n93\n\n\nrepvgg\n\n\n\n\n94\n\n\nres2net101\n\n\n\n\n95\n\n\nres2net50\n\n\n\n\n96\n\n\nres2next50\n\n\n\n\n97\n\n\nresmlp\n\n\n\n\n98\n\n\nresnest101e\n\n\n\n\n99\n\n\nresnest14d\n\n\n\n\n100\n\n\nresnest200e\n\n\n\n\n101\n\n\nresnest269e\n\n\n\n\n102\n\n\nresnest26d\n\n\n\n\n103\n\n\nresnest50d\n\n\n\n\n104\n\n\nresnet101\n\n\n\n\n105\n\n\nresnet101d\n\n\n\n\n106\n\n\nresnet10t\n\n\n\n\n107\n\n\nresnet14t\n\n\n\n\n108\n\n\nresnet152\n\n\n\n\n109\n\n\nresnet152d\n\n\n\n\n110\n\n\nresnet18\n\n\n\n\n111\n\n\nresnet18d\n\n\n\n\n112\n\n\nresnet200d\n\n\n\n\n113\n\n\nresnet26\n\n\n\n\n114\n\n\nresnet26d\n\n\n\n\n115\n\n\nresnet26t\n\n\n\n\n116\n\n\nresnet32ts\n\n\n\n\n117\n\n\nresnet33ts\n\n\n\n\n118\n\n\nresnet34\n\n\n\n\n119\n\n\nresnet34d\n\n\n\n\n120\n\n\nresnet50\n\n\n\n\n121\n\n\nresnet50d\n\n\n\n\n122\n\n\nresnet51q\n\n\n\n\n123\n\n\nresnet61q\n\n\n\n\n124\n\n\nresnetaa50\n\n\n\n\n125\n\n\nresnetblur50\n\n\n\n\n126\n\n\nresnetrs101\n\n\n\n\n127\n\n\nresnetrs152\n\n\n\n\n128\n\n\nresnetrs200\n\n\n\n\n129\n\n\nresnetrs270\n\n\n\n\n130\n\n\nresnetrs350\n\n\n\n\n131\n\n\nresnetrs420\n\n\n\n\n132\n\n\nresnetrs50\n\n\n\n\n133\n\n\nresnetv2\n\n\n\n\n134\n\n\nresnext101\n\n\n\n\n135\n\n\nresnext26ts\n\n\n\n\n136\n\n\nresnext50\n\n\n\n\n137\n\n\nresnext50d\n\n\n\n\n138\n\n\nrexnet\n\n\n\n\n139\n\n\nsebotnet33ts\n\n\n\n\n140\n\n\nsehalonet33ts\n\n\n\n\n141\n\n\nselecsls42b\n\n\n\n\n142\n\n\nselecsls60\n\n\n\n\n143\n\n\nselecsls60b\n\n\n\n\n144\n\n\nsemnasnet\n\n\n\n\n145\n\n\nsequencer2d\n\n\n\n\n146\n\n\nseresnet152d\n\n\n\n\n147\n\n\nseresnet33ts\n\n\n\n\n148\n\n\nseresnet50\n\n\n\n\n149\n\n\nseresnext101\n\n\n\n\n150\n\n\nseresnext101d\n\n\n\n\n151\n\n\nseresnext26d\n\n\n\n\n152\n\n\nseresnext26t\n\n\n\n\n153\n\n\nseresnext26ts\n\n\n\n\n154\n\n\nseresnext50\n\n\n\n\n155\n\n\nseresnextaa101d\n\n\n\n\n156\n\n\nskresnet18\n\n\n\n\n157\n\n\nskresnet34\n\n\n\n\n158\n\n\nskresnext50\n\n\n\n\n159\n\n\nspnasnet\n\n\n\n\n160\n\n\nssl\n\n\n\n\n161\n\n\nswin\n\n\n\n\n162\n\n\nswinv2\n\n\n\n\n163\n\n\nswsl\n\n\n\n\n164\n\n\ntf\n\n\n\n\n165\n\n\ntinynet\n\n\n\n\n166\n\n\ntnt\n\n\n\n\n167\n\n\ntresnet\n\n\n\n\n168\n\n\ntv\n\n\n\n\n169\n\n\ntwins\n\n\n\n\n170\n\n\nvgg11\n\n\n\n\n171\n\n\nvgg13\n\n\n\n\n172\n\n\nvgg16\n\n\n\n\n173\n\n\nvgg19\n\n\n\n\n174\n\n\nvisformer\n\n\n\n\n175\n\n\nvit\n\n\n\n\n176\n\n\nvolo\n\n\n\n\n177\n\n\nwide\n\n\n\n\n178\n\n\nxception\n\n\n\n\n179\n\n\nxception41\n\n\n\n\n180\n\n\nxception41p\n\n\n\n\n181\n\n\nxception65\n\n\n\n\n182\n\n\nxception65p\n\n\n\n\n183\n\n\nxception71\n\n\n\n\n184\n\n\nxcit\n\n\n\n\n\n\nTimm provides many pretrained models, but not all of them are fast enough for real-time applications. We can filter the results by providing a full or partial model name.\nCheck available pretrained ConvNeXt models\npd.DataFrame(timm.list_models('convnext*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nconvnext_base\n\n\n\n\n1\n\n\nconvnext_base_384_in22ft1k\n\n\n\n\n2\n\n\nconvnext_base_in22ft1k\n\n\n\n\n3\n\n\nconvnext_base_in22k\n\n\n\n\n4\n\n\nconvnext_large\n\n\n\n\n5\n\n\nconvnext_large_384_in22ft1k\n\n\n\n\n6\n\n\nconvnext_large_in22ft1k\n\n\n\n\n7\n\n\nconvnext_large_in22k\n\n\n\n\n8\n\n\nconvnext_small\n\n\n\n\n9\n\n\nconvnext_small_384_in22ft1k\n\n\n\n\n10\n\n\nconvnext_small_in22ft1k\n\n\n\n\n11\n\n\nconvnext_small_in22k\n\n\n\n\n12\n\n\nconvnext_tiny\n\n\n\n\n13\n\n\nconvnext_tiny_384_in22ft1k\n\n\n\n\n14\n\n\nconvnext_tiny_hnf\n\n\n\n\n15\n\n\nconvnext_tiny_in22ft1k\n\n\n\n\n16\n\n\nconvnext_tiny_in22k\n\n\n\n\n17\n\n\nconvnext_xlarge_384_in22ft1k\n\n\n\n\n18\n\n\nconvnext_xlarge_in22ft1k\n\n\n\n\n19\n\n\nconvnext_xlarge_in22k\n\n\n\n\n\n\nLet’s go with the convnext_tiny model since we want higher framerates. Each model comes with a set of default configuration parameters. We must keep track of the mean and std values used to normalize the model input.\nInspect the default configuration for the convnext_tiny model\nfrom timm.models import convnext\nconvnext_model = 'convnext_tiny'\npd.DataFrame.from_dict(convnext.default_cfgs[convnext_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nstem.0\n\n\n\n\nclassifier\n\n\nhead.fc\n\n\n\n\n\n\nCheck available pretrained MobileNetV2 models\npd.DataFrame(timm.list_models('mobilenetv2*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nmobilenetv2_050\n\n\n\n\n1\n\n\nmobilenetv2_100\n\n\n\n\n2\n\n\nmobilenetv2_110d\n\n\n\n\n3\n\n\nmobilenetv2_120d\n\n\n\n\n4\n\n\nmobilenetv2_140\n\n\n\n\n\n\nInspect the default configuration for the mobilenetv2_100 model\nfrom timm.models import efficientnet\nmobilenetv2_model = 'mobilenetv2_100'\npd.DataFrame.from_dict(efficientnet.default_cfgs[mobilenetv2_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv_stem\n\n\n\n\nclassifier\n\n\nclassifier\n\n\n\n\n\n\nCheck available pretrained ResNet models\npd.DataFrame(timm.list_models('resnet*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nresnet10t\n\n\n\n\n1\n\n\nresnet14t\n\n\n\n\n2\n\n\nresnet18\n\n\n\n\n3\n\n\nresnet18d\n\n\n\n\n4\n\n\nresnet26\n\n\n\n\n5\n\n\nresnet26d\n\n\n\n\n6\n\n\nresnet26t\n\n\n\n\n7\n\n\nresnet32ts\n\n\n\n\n8\n\n\nresnet33ts\n\n\n\n\n9\n\n\nresnet34\n\n\n\n\n10\n\n\nresnet34d\n\n\n\n\n11\n\n\nresnet50\n\n\n\n\n12\n\n\nresnet50_gn\n\n\n\n\n13\n\n\nresnet50d\n\n\n\n\n14\n\n\nresnet51q\n\n\n\n\n15\n\n\nresnet61q\n\n\n\n\n16\n\n\nresnet101\n\n\n\n\n17\n\n\nresnet101d\n\n\n\n\n18\n\n\nresnet152\n\n\n\n\n19\n\n\nresnet152d\n\n\n\n\n20\n\n\nresnet200d\n\n\n\n\n21\n\n\nresnetaa50\n\n\n\n\n22\n\n\nresnetblur50\n\n\n\n\n23\n\n\nresnetrs50\n\n\n\n\n24\n\n\nresnetrs101\n\n\n\n\n25\n\n\nresnetrs152\n\n\n\n\n26\n\n\nresnetrs200\n\n\n\n\n27\n\n\nresnetrs270\n\n\n\n\n28\n\n\nresnetrs350\n\n\n\n\n29\n\n\nresnetrs420\n\n\n\n\n30\n\n\nresnetv2_50\n\n\n\n\n31\n\n\nresnetv2_50d_evos\n\n\n\n\n32\n\n\nresnetv2_50d_gn\n\n\n\n\n33\n\n\nresnetv2_50x1_bit_distilled\n\n\n\n\n34\n\n\nresnetv2_50x1_bitm\n\n\n\n\n35\n\n\nresnetv2_50x1_bitm_in21k\n\n\n\n\n36\n\n\nresnetv2_50x3_bitm\n\n\n\n\n37\n\n\nresnetv2_50x3_bitm_in21k\n\n\n\n\n38\n\n\nresnetv2_101\n\n\n\n\n39\n\n\nresnetv2_101x1_bitm\n\n\n\n\n40\n\n\nresnetv2_101x1_bitm_in21k\n\n\n\n\n41\n\n\nresnetv2_101x3_bitm\n\n\n\n\n42\n\n\nresnetv2_101x3_bitm_in21k\n\n\n\n\n43\n\n\nresnetv2_152x2_bit_teacher\n\n\n\n\n44\n\n\nresnetv2_152x2_bit_teacher_384\n\n\n\n\n45\n\n\nresnetv2_152x2_bitm\n\n\n\n\n46\n\n\nresnetv2_152x2_bitm_in21k\n\n\n\n\n47\n\n\nresnetv2_152x4_bitm\n\n\n\n\n48\n\n\nresnetv2_152x4_bitm_in21k\n\n\n\n\n\n\nInspect the default configuration for the resnet10t model\nfrom timm.models import resnet\nresnet_model = 'resnet10t'\npd.DataFrame.from_dict(resnet.default_cfgs[resnet_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet10t_176_c3-f3215ab1.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 176, 176)\n\n\n\n\npool_size\n\n\n(6, 6)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbilinear\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv1.0\n\n\n\n\nclassifier\n\n\nfc\n\n\n\n\ntest_crop_pct\n\n\n0.95\n\n\n\n\ntest_input_size\n\n\n(3, 224, 224)\n\n\n\n\n\n\nSelect a model\n# model_type = convnext\n# model_name = convnext_model\n# model_type = efficientnet\n# model_name = mobilenetv2_model\nmodel_type = resnet\nmodel_name = resnet_model\nStore normalization stats\nmean = model_type.default_cfgs[model_name]['mean']\nstd = model_type.default_cfgs[model_name]['std']\nmean, std\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#modify-transforms",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#modify-transforms",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Modify Transforms",
    "text": "Modify Transforms\nWe can apply the normalization stats at the end of the batch transforms.\nitem_tfms = [FlipItem(p=1.0), Resize(input_dims, method=ResizeMethod.Pad, pad_mode=PadMode.Border)]\n\nbatch_tfms = [\n    Contrast(max_lighting=0.25),\n    Saturation(max_lighting=0.25),\n    Hue(max_hue=0.05),\n    *aug_transforms(\n        size=input_dims, \n        mult=1.0,\n        do_flip=False,\n        flip_vert=False,\n        max_rotate=0.0,\n        min_zoom=0.5,\n        max_zoom=1.5,\n        max_lighting=0.5,\n        max_warp=0.2, \n        p_affine=0.0,\n        pad_mode=PadMode.Border),\n    Normalize.from_stats(mean=mean, std=std)\n]"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#define-learner",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#define-learner",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Define Learner",
    "text": "Define Learner\nThe training process is identical to the original tutorial, and we only need to pass the name of the Timm model to the vision_learner object.\nlearn = vision_learner(dls, model_name, metrics=metrics).to_fp16()"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#export-the-model",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#export-the-model",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Export the Model",
    "text": "Export the Model\nThe OpenVINO model conversion script does not support PyTorch models, so we need to export the trained model to ONNX. We can then convert the ONNX model to OpenVINO’s IR format.\nDefine ONNX file name\nonnx_file_name = f\"{dataset_path.name}-{learn.arch}.onnx\"\nonnx_file_name\n'asl-and-some-words-resnet10t.onnx'\nExport trained model to ONNX\ntorch.onnx.export(learn.model.cpu(),\n                  batched_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=11,\n                  do_constant_folding=False,\n                  input_names = ['input'],\n                  output_names = ['output'],\n                  dynamic_axes={'input': {2 : 'height', 3 : 'width'}}\n                 )\nNow we can define the argument for OpenVINO’s model conversion script.\nImport OpenVINO Dependencies\nfrom IPython.display import Markdown, display\nfrom openvino.runtime import Core\nDefine export directory\noutput_dir = Path('./')\noutput_dir\nPath('.')\nDefine path for OpenVINO IR xml model file\nThe conversion script generates an XML containing information about the model architecture and a BIN file that stores the trained weights. We need both files to perform inference. OpenVINO uses the same name for the BIN file as provided for the XML file.\nir_path = Path(f\"{onnx_file_name.split('.')[0]}.xml\")\nir_path\nPath('asl-and-some-words-resnet10t.xml')\nDefine arguments for model conversion script\nOpenVINO provides the option to include the normalization stats in the IR model. That way, we don’t need to account for different normalization stats when performing inference with multiple models. We can also convert the model to FP16 precision to reduce file size and improve inference speed.\n# Construct the command for Model Optimizer\nmo_command = f\"\"\"mo\n                 --input_model \"{onnx_file_name}\"\n                 --input_shape \"[1,3, {input_dims[0]}, {input_dims[1]}]\"\n                 --mean_values=\"{mean}\"\n                 --scale_values=\"{std}\"\n                 --data_type FP16\n                 --output_dir \"{output_dir}\"\n                 \"\"\"\nmo_command = \" \".join(mo_command.split())\nprint(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\ndisplay(Markdown(f\"`{mo_command}`\"))\nModel Optimizer command to convert the ONNX model to OpenVINO:\nmo --input_model \"asl-and-some-words-resnet10t.onnx\" --input_shape \"[1,3, 216, 384]\" --mean_values=\"(0.485, 0.456, 0.406)\" --scale_values=\"(0.229, 0.224, 0.225)\" --data_type FP16 --output_dir \".\"\nConvert ONNX model to OpenVINO IR\nif not ir_path.exists():\n    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n    mo_result = %sx $mo_command\n    print(\"\\n\".join(mo_result))\nelse:\n    print(f\"IR model {ir_path} already exists.\")\n    Exporting ONNX model to IR... This may take a few minutes.\n    Model Optimizer arguments:\n    Common parameters:\n        - Path to the Input Model:  /media/innom-dt/Samsung_T3/My_Environments/jupyter-notebooks/openvino/asl-and-some-words-resnet10t.onnx\n        - Path for generated IR:    /media/innom-dt/Samsung_T3/My_Environments/jupyter-notebooks/openvino/.\n        - IR output name:   asl-and-some-words-resnet10t\n        - Log level:    ERROR\n        - Batch:    Not specified, inherited from the model\n        - Input layers:     Not specified, inherited from the model\n        - Output layers:    Not specified, inherited from the model\n        - Input shapes:     [1,3, 216, 384]\n        - Source layout:    Not specified\n        - Target layout:    Not specified\n        - Layout:   Not specified\n        - Mean values:  (0.485, 0.456, 0.406)\n        - Scale values:     (0.229, 0.224, 0.225)\n        - Scale factor:     Not specified\n        - Precision of IR:  FP16\n        - Enable fusing:    True\n        - User transformations:     Not specified\n        - Reverse input channels:   False\n        - Enable IR generation for fixed input shape:   False\n        - Use the transformations config file:  None\n    Advanced parameters:\n        - Force the usage of legacy Frontend of Model Optimizer for model conversion into IR:   False\n        - Force the usage of new Frontend of Model Optimizer for model conversion into IR:  False\n    OpenVINO runtime found in:  /home/innom-dt/mambaforge/envs/fastai-openvino/lib/python3.9/site-packages/openvino\n    OpenVINO runtime version:   2022.1.0-7019-cdb9bec7210-releases/2022/1\n    Model Optimizer version:    2022.1.0-7019-cdb9bec7210-releases/2022/1\n    [ WARNING ]  \n    Detected not satisfied dependencies:\n        numpy: installed: 1.23.0, required: &lt; 1.20\n    \n    Please install required versions of components or run pip installation\n    pip install openvino-dev\n    [ SUCCESS ] Generated IR version 11 model.\n    [ SUCCESS ] XML file: /media/innom-dt/Samsung_T3/My_Environments/jupyter-notebooks/openvino/asl-and-some-words-resnet10t.xml\n    [ SUCCESS ] BIN file: /media/innom-dt/Samsung_T3/My_Environments/jupyter-notebooks/openvino/asl-and-some-words-resnet10t.bin\n    [ SUCCESS ] Total execution time: 0.43 seconds. \n    [ SUCCESS ] Memory consumed: 123 MB. \n    It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n    [ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n    Find more information about API v2.0 and IR v11 at https://docs.openvino.ai"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#benchmark-openvino-inference",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#benchmark-openvino-inference",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Benchmark OpenVINO Inference",
    "text": "Benchmark OpenVINO Inference\nNow we can compare inference speed between OpenVINO and PyTorch. OpenVINO supports inference with ONNX models in addition to its IR format.\nGet available OpenVINO compute devices\nOpenVINO does not support GPU inference with non-Intel GPUs.\ndevices = ie.available_devices\nfor device in devices:\n    device_name = ie.get_property(device_name=device, name=\"FULL_DEVICE_NAME\")\n    print(f\"{device}: {device_name}\")\nCPU: 11th Gen Intel(R) Core(TM) i7-11700K @ 3.60GHz\nCreate normalized input for ONNX model\nnormalized_input_image = batched_tensor.cpu().detach().numpy()\nnormalized_input_image.shape\n(1, 3, 224, 224)\nTest ONNX model using OpenVINO\n# Load network to Inference Engine\nie = Core()\nmodel_onnx = ie.read_model(model=onnx_file_name)\ncompiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n\ninput_layer_onnx = next(iter(compiled_model_onnx.inputs))\noutput_layer_onnx = next(iter(compiled_model_onnx.outputs))\n\n# Run inference on the input image\nres_onnx = compiled_model_onnx(inputs=[normalized_input_image])[output_layer_onnx]\nlearn.dls.vocab[np.argmax(res_onnx)]\n'J'\nBenchmark ONNX model CPU inference speed\n%%timeit\ncompiled_model_onnx(inputs=[normalized_input_image])[output_layer_onnx]\n3.62 ms ± 61.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nPrepare input image for OpenVINO IR model\ninput_image = scaled_tensor.unsqueeze(dim=0)\ninput_image.shape\ntorch.Size([1, 3, 224, 224])\nTest OpenVINO IR model\n# Load the network in Inference Engine\nie = Core()\nmodel_ir = ie.read_model(model=ir_path)\nmodel_ir.reshape(input_image.shape)\ncompiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n\n# Get input and output layers\ninput_layer_ir = next(iter(compiled_model_ir.inputs))\noutput_layer_ir = next(iter(compiled_model_ir.outputs))\n\n# Run inference on the input image\nres_ir = compiled_model_ir([input_image])[output_layer_ir]\nlearn.dls.vocab[np.argmax(res_ir)]\n'J'\nBenchmark OpenVINO IR model CPU inference speed\n%%timeit\ncompiled_model_ir([input_image])[output_layer_ir]\n3.39 ms ± 84.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nNote: The IR model is slightly faster than the ONNX model and half the file size.\nBenchmark PyTorch model GPU inference speed\n%%timeit\nwith torch.no_grad(): preds = learn.model.cuda()(batched_tensor.cuda())\n1.81 ms ± 5.52 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPyTorch inference with a Titan RTX is still faster than OpenVINO inference with an i7-11700K for a ResNet10 model. However, OpenVINO CPU inference is often faster when using models optimized for mobile devices, like MobileNet.\nBenchmark PyTorch model CPU inference speed\n%%timeit\nwith torch.no_grad(): preds = learn.model.cpu()(batched_tensor.cpu())\n8.94 ms ± 52.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nOpenVINO is easily faster than PyTorch for CPU inference."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-1/index.html#summary",
    "href": "posts/fastai-openvino-unity-tutorial/part-1/index.html#summary",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1",
    "section": "Summary",
    "text": "Summary\nThis post covered how to modify the training code from the fastai-to-unity tutorialto finetune models from the Timm library and export them as OpenVINO IR models. Part 2 will cover creating a dynamic link library (DLL) file in Visual Studio to perform inference with these models using OpenVINO.\nPrevious: Fastai to Unity Tutorial Pt. 3\nNext: How to Create an OpenVINO Plugin for Unity on Windows Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "",
    "text": "Overview\nInstall OpenVINO\nCreate DLL Project\nConfigure the Project\nAdd Include Directories\nLink Libraries\nPost Build Events\nUpdate Precompiled Header File\nUpdate dllmain File\nBuild Solution\nGather Dependencies\nSummary"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#overview",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#overview",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Overview",
    "text": "Overview\nPart 1 covered modifying the training code from the fastai-to-unity tutorial to finetune models from the Timm library and exporting them as OpenVINO IR models. This post covers creating a dynamic link library (DLL) file in Visual Studio to perform inference with these IR models using OpenVINO."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#install-openvino",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#install-openvino",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Install OpenVINO",
    "text": "Install OpenVINO\nWe need to download the OpenVINO Toolkit before creating our Visual Studio project. Go to the OpenVINO download page linked below.\n\nOpenVINO Download page\n\nDownload OpenVINO Toolkit\nSelect the options outlined in the image below and click the Download button.\n\n\n\n\n\nDouble-click the file once it finishes downloading and click the Extract button in the popup window.\n\n\n\n\n\nThe installer will then verify the computer meets the system requirements. The toolkit includes the Python scripts for converting models, which require Python 3.6, 3.7, 3.8, or 3.9 to run. We will only use the files for C++ development in this post.\n\n\n\n\n\nWe can stick with the default Recommended Installation option.\n\n\n\n\n\nThe installer will then ask whether Intel can collect some information before starting the installation process.\n\n\n\n\n\n\n\n\n\n\nClick Finish once the installation process completes.\n\n\n\n\n\nInspect OpenVINO Folder\nIf we look at the installation folder for the toolkit, we can see it also includes a version of OpenCV. We’ll use OpenCV to prepare image data from Unity before feeding it to the model.\n\n\n\n\n\nI like to copy the OpenVINO folder to a separate directory with other dependencies for my C++ projects.\n\n\n\n\n\nNow we can create our Visual Studio DLL project."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#create-dll-project",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#create-dll-project",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Create DLL Project",
    "text": "Create DLL Project\nOpen Visual Studio and select the Create a new project option.\n\n\n\n\n\nType DLL into the text box and select the Dynamic-Link Library (DLL) option. This option automatically configures a few parameters for us compared to starting with a standard console application.\n\n\n\n\n\nChoose a name and location for the project and click the Create button. By default, the DLL file will use the project name."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#configure-the-project",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#configure-the-project",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Configure the Project",
    "text": "Configure the Project\nAt the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#add-include-directories",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#add-include-directories",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nWe need to tell Visual Studio where OpenVINO and OpenCV are so we can access their APIs. Right-click the project name in the Solution Explorer panel.\n\n\n\n\n\nSelect the Properties option in the popup menu.\n\n\n\n\n\nIn the Properties Window, open on the C/C++ dropdown. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nAdd the paths for the following folders, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\include\\ie\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\include\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\include\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\include"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#link-libraries",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#link-libraries",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select Input. Select Additional Dependencies and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the paths to the following files, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\lib\\*\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\lib\\intel64\\Release\\*\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\lib\\*.lib"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#post-build-events",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#post-build-events",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Post Build Events",
    "text": "Post Build Events\nOur DLL file will depend on the following DLL files included with the OpenVINO and OpenCV libraries.\nOpenCV DLL files\n\n\n\n\n\nOpenVINO DLL files\n\n\n\n\n\n\n\n\n\n\nWe can add a post-build event in Visual Studio to automatically copy these DLL files to the build folder for the project at compile time. Open the Build Events dropdown in the Properties window and select Post-Build Event. Select Command Line and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the following commands, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_core453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_imgproc453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_imgcodecs453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\bin\\intel64\\Release\\* $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\bin\\tbb.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window.\n\n\n\n\n\nWith the dependencies taken care of, we can start modifying the code."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nWe’ll first update the pch.h Precompiled Header file with the required header files. We can open the pch.h file by selecting it in the Solution Explorer window.\n\n\n\n\n\nComment or remove the “#include” line for the framework.h header file.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#endif //PCH_H\nAdd required header files\nNext, we’ll add the required header files for OpenVINO and OpenCV below //#include \"framework.h\" line.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#include \"openvino/openvino.hpp\"\n#include &lt;opencv2/opencv.hpp&gt;\n\n#endif //PCH_H"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#update-dllmain-file",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#update-dllmain-file",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Update dllmain File",
    "text": "Update dllmain File\nBy default, the dllmain.cpp file contains the following code.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\nCreate a macro to mark functions we want to make accessible in Unity\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWrap the code in extern “C” to prevent name-mangling issues with the compiler\nThe rest of our code will go inside here.\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n\n}\nDefine variables\nInside the wrapper, we will declare the persistent variables needed for the DLL.\n\nov::Core: represents an OpenVINO runtime Core entity\nov::Model: A user-defined model\nov::CompiledModel: represents a compiled model\nov::InferRequest: an infer request that can be run in asynchronous or synchronous manners\nov::Tensor: API holding host memory\n\n\n// Inference engine instance\nov::Core core;\n// The user define model representation\nstd::shared_ptr&lt;ov::Model&gt; model;\n// A device-specific compiled model\nov::CompiledModel compiled_model;\n\n// List of available compute devices\nstd::vector&lt;std::string&gt; available_devices;\n// An inference request for a compiled model\nov::InferRequest infer_request;\n// Stores the model input data\nov::Tensor input_tensor;\n// A pointer for accessing the input tensor data\nfloat* input_data;\n\n// The number of image classes the current model can detect\nint num_classes = 0;\n// The current input image width\nint input_w;\n// The current input image height\nint input_h;\n// The total number pixels in the input image\nint nPixels;\n// The number of color channels in the input image\nint num_channels = 3;\nDefine a function to get the number of compute devices\nThe first function we’ll define will create a list of available device names and return the number of devices accessible by OpenVINO. We’ll use this information to select which device to use to perform inference from the Unity application. There might be an option named GNA (Gaussian & Neural Accelerator). GNA is a highly specialized neural coprocessor for tasks like noise cancellation. We’ll exclude it from the list of devices presented to the end user.\n\nov::Core::get_available_devices(): Returns devices available for inference\n\n\n/// &lt;summary&gt;\n/// Get the number of available compute devices\n/// &lt;/summary&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nDLLExport int GetDeviceCount() {\n\n    // Reset list of available compute devices\n    available_devices.clear();\n\n    // Populate list of available compute devices\n    for (std::string device : core.get_available_devices()) {\n        // Skip GNA device\n        if (device.find(\"GNA\") == std::string::npos) {\n            available_devices.push_back(device);\n        }\n    }\n    // Return the number of available compute devices\n    return available_devices.size();\n}\nDefine a function to get the name of a compute device\nNext, we’ll define a function to return the name of a device at a specified index for the list of available devices.\n/// &lt;summary&gt;\n/// Get the name of the compute device name at the specified index\n/// &lt;/summary&gt;\n/// &lt;param name=\"index\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nDLLExport std::string* GetDeviceName(int index) {\n    return &available_devices[index];\n}\nDefine a function to load an OpenVINO model\nOpenVINO needs to compile models for the target device. This process can take several seconds when using GPU inference. We can create a cache directory, so we only need to compile models for a specific resolution-device pair once.\nWe’ll place the code for loading an OpenVINO model inside a try-catch block to avoid crashing the application if we pass an incorrect file path.\nIf the model loads successfully, we’ll attempt to reshape the model input to the desired input dimensions. Note that models like MobileViT will need to use the input dimensions from training.\nAfter reshaping the model input, we can compile the model for the target device.\nWe can get pointers to the model input tensor and create an inference request using the compiled model.\n\nov::Core::set_property(): Sets properties for a device\nov::Core::read_model(): Reads models from IR/ONNX/PDPD formats\nov::Model::reshape(): Updates input shapes and propagates them down to the outputs of the model through all intermediate layers\nov::Core::compile_model(): Creates a compiled model from a source model object\nov::CompiledModel::create_infer_request(): Creates an inference request object used to infer the compiled model\nov::InferRequest::get_input_tensor(): Gets an input tensor for inference\n\n\n/// &lt;summary&gt;\n/// Load a model from the specified file path\n/// &lt;/summary&gt;\n/// &lt;param name=\"modelPath\"&gt;The path to the OpenVINO IR model file&lt;/param&gt;\n/// &lt;param name=\"index\"&gt;The compute device index&lt;/param&gt;\n/// &lt;param name=\"inputDims\"&gt;The source image resolution&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nDLLExport int LoadModel(char* modelPath, int index, int inputDims[2]) {\n\n    // Initialize return value\n    int return_val = 0;\n    // Specify the cache directory for compiled gpu models\n    core.set_property(\"GPU\", ov::cache_dir(\"cache\"));\n\n    // Try loading the specified model\n    try { model = core.read_model(modelPath); } \n    // Return 1 if the model fails to load\n    catch (...) { return 1; }\n\n    // Try updating the model input dimensions\n    try { model-&gt;reshape({ 1, 3, inputDims[1], inputDims[0] }); }\n    // Return a value of 2 if we can't update the model input dimensions\n    catch (...) { return_val = 2; }\n\n    // Compile the loaded model for the target compute device\n    auto compiled_model = core.compile_model(model, \"MULTI\",\n         ov::device::priorities(available_devices[index]),\n         ov::hint::performance_mode(ov::hint::PerformanceMode::LATENCY),\n         ov::hint::inference_precision(ov::element::f32));\n\n    // Get the number of classes the current model can detect\n    ov::Output&lt;const ov::Node&gt; output = compiled_model.output();\n    num_classes = output.get_shape()[1];\n    // Create an inference request to use the compiled model\n    infer_request = compiled_model.create_infer_request();\n\n    // Get input tensor by index\n    input_tensor = infer_request.get_input_tensor(0);\n\n    // Get model input dimensions\n    input_w = input_tensor.get_shape()[3];\n    input_h = input_tensor.get_shape()[2];\n    nPixels = input_w * input_h;\n\n    // Get a pointer to the input tensor\n    input_data = input_tensor.data&lt;float&gt;();\n\n    // Return a value of 0 if the model loads successfully\n    return return_val;\n}\nDefine a function to perform inference\nWe will access the pixel data for the input image from Unity with a pointer to a uchar (unsigned 1-byte integer) array and wrap the data in a cv::Mat variable for processing.\nWe don’t need to normalize the input image since the IR model does it internally.\nOnce again, we’ll use a try-catch block to avoid crashing the application if an error occurs during the forward pass. We can use the std::max_element() and std::distance() functions to find the class index with the highest confidence score.\n\ncv::Mat: n-dimensional dense array class\n\ncv::cvtColor(): Converts an image from one color space to another\nov::InferRequest::infer(): Infers specified input in synchronous mode\nov::InferRequest::get_output_tensor(): Gets an output tensor for inference\nstd::distance(): Calculates the number of elements between first and last\nstd::max_element(): Returns an iterator pointing to the element with the largest value in the range [first,last)\n\n\n/// &lt;summary&gt;\n/// Perform inference with the provided texture data\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputData\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nDLLExport int PerformInference(uchar* inputData) {\n\n    // Initialize predicted class index to an invalid value\n    int class_idx = -1;\n\n    try {\n\n        // Store the pixel data for the source input image in an OpenCV Mat\n        cv::Mat texture = cv::Mat(input_h, input_w, CV_8UC4, inputData);\n        // Remove the alpha channel\n        cv::cvtColor(texture, texture, cv::COLOR_RGBA2RGB);\n\n        // Iterate over each pixel in image\n        for (int p = 0; p &lt; nPixels; p++) {\n            // Iterate over each color channel for each pixel in image\n            for (int ch = 0; ch &lt; num_channels; ++ch) {\n                input_data[ch * nPixels + p] = texture.data[p * num_channels + ch] / 255.0f;\n            }\n        }\n\n        // Perform inference\n        infer_request.infer();\n\n        // model has only one output\n        ov::Tensor output_tensor = infer_request.get_output_tensor();\n        // IR v10 works with converted precisions (i64 -&gt; i32)\n        auto out_data = output_tensor.data&lt;float&gt;();\n\n        // Get the predicted class index with the highest confidence score\n        class_idx = std::distance(out_data, std::max_element(out_data, out_data + num_classes));\n    }\n    catch (...) {\n        // Return a value of -2 if an error occurs during the forward pass\n        class_idx = -2;\n    }\n\n    return class_idx;\n}\nThat is all the code needed for the plugin. We can now build the solution to generate the DLL file."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#build-solution",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#build-solution",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Build Solution",
    "text": "Build Solution\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. Visual Studio will generate a new x64 folder in the project directory containing the DLL file and its dependencies."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#gather-dependencies",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#gather-dependencies",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nRight-click the project name in the Solution Explorer panel and select Open Folder in File Explorer from the popup menu.\n\n\n\n\n\nIn the new File Explorer window, go to the parent folder.\n\n\n\n\n\nOpen the x64 → Release subfolder.\n\n\n\n\n\nWe’ll need to copy all the DLL files in this folder and the plugins.xml file to the Unity project."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-2/index.html#summary",
    "href": "posts/fastai-openvino-unity-tutorial/part-2/index.html#summary",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 2",
    "section": "Summary",
    "text": "Summary\nThis post covered creating a dynamic link library (DLL) file to perform inference using OpenVINO. Part 3 will cover the required modifications for the original Unity project to use this DLL.\nPrevious: How to Create an OpenVINO Plugin for Unity on Windows Pt. 1\nNext: How to Create an OpenVINO Plugin for Unity on Windows Pt. 3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "",
    "text": "Overview\nOpen Unity Project\nAdd New Asset Files\nAllow Unsafe Code\nModify Compute Shader\nCreate ImageClassifierOpenVINO Script\nModify GUI\nAdd ImageClassifierOpenVINO Component\nUpdate On Value Changed Events\nSummary"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#overview",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#overview",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Overview",
    "text": "Overview\nPart 2 covered creating a dynamic link library (DLL) file to perform image classification using OpenVINO. This post covers the required modifications for the Unity project from the fastai-to-unity tutorial to use this DLL."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#open-unity-project",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#open-unity-project",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Open Unity Project",
    "text": "Open Unity Project\nOpen the Fastai-Unity-Tutorial project in the Unity Editor. The project is available in the GitHub repository linked below for anyone who did not follow the original tutorial series.\n\nfastai-to-unity-tutorial GitHub repository"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#add-new-asset-files",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#add-new-asset-files",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Add New Asset Files",
    "text": "Add New Asset Files\nWe’ll store the DLL files from part 2 in a new folder called Plugins. The DLL targets 64-bit x86 architectures, so we need to place the DLL files in a subfolder named x86_64.\n\nPlugins Folder Google Drive\n\n\n\n\n\n\nCopy all the DLL files and the plugins.xml file into the Assets/Plugins/x86_64 folder.\n\n\n\n\n\nWe then need to close and reopen the project for Unity to load the plugin files.\nNext, we’ll create a folder to store the OpenVINO IR models. We need to place the XML and BIN files for the IR models in a StreamingAssets folder to include them in project builds. Create a new folder named StreamingAssets. We’ll put the files in a new subfolder called OpenVINOModels to keep things organized.\n\nOpenVINOModels Folder Google Drive\n\n\n\n\n\n\nThe plugins.xml file included with the DLL files contains locations for the DLL files needed for using different types of devices.\nplugins.xml content:\n&lt;ie&gt;\n    &lt;plugins&gt;\n        &lt;plugin name=\"AUTO\" location=\"openvino_auto_plugin.dll\"&gt;\n            &lt;properties&gt;\n                &lt;property key=\"MULTI_WORK_MODE_AS_AUTO\" value=\"YES\"/&gt;\n            &lt;/properties&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"BATCH\" location=\"openvino_auto_batch_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"CPU\" location=\"openvino_intel_cpu_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"GNA\" location=\"openvino_intel_gna_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"GPU\" location=\"openvino_intel_gpu_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"HETERO\" location=\"openvino_hetero_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"MULTI\" location=\"openvino_auto_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"MYRIAD\" location=\"openvino_intel_myriad_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"HDDL\" location=\"openvino_intel_hddl_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"VPUX\" location=\"openvino_intel_vpux_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/ie&gt;\nIt needs to be in the same folder as the DLL files for the plugin to work. However, Unity does not include XML files in the Plugins folder when building the project. We need to store a copy of the plugins.xml file in the StreamingAssets folder and then copy it back to the Plugins/x86_64 folder when first running the built project. We can handle both steps automatically in code."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Allow Unsafe Code",
    "text": "Allow Unsafe Code\nRather than copying the input image from Unity to the OpenVINO plugin, we’ll pass a pointer to the pixel data. First, we need to allow unsafe code for the Unity project. Select Edit → Project Settings... from the top menu.\n\n\n\n\n\nOpen the Player → Other Settings dropdown and scroll down to the Allow 'unsafe' Code checkbox. Enable the setting and close the Project Settings window.\n\n\n\n\n\nNow we can start modifying the code."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#modify-compute-shader",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#modify-compute-shader",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Modify Compute Shader",
    "text": "Modify Compute Shader\nThe input image gets flipped upside down when we send it to the plugin. We can pre-flip the image in the ProcessingShader compute shader before sending it to the plugin. We need to know the height of the input image, which we can access with the Texture2D::GetDimensions function.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel NormalizeImageNet\n#pragma kernel FlipXAxis\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Flip the image around the x-axis\n[numthreads(8, 8, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Stores the InputImage width\n    uint width;\n    // Stores the InputImage height\n    uint height;\n    // Get the dimensions of the InputImage\n    InputImage.GetDimensions(width, height);\n\n    // Update the y value for the pixel coordinates\n    int2 coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}\n\n// Apply the ImageNet normalization stats from PyTorch to an image\n[numthreads(8, 8, 1)]\nvoid NormalizeImageNet(uint3 id : SV_DispatchThreadID)\n{\n    // Set the pixel color values for the processed image\n    Result[id.xy] = float4(\n        // Normalize the red color channel values\n        (InputImage[id.xy].r - 0.4850f) / 0.2290f,\n        // Normalize the green color channel values\n        (InputImage[id.xy].g - 0.4560f) / 0.2240f,\n        // Normalize the blue color channel values\n        (InputImage[id.xy].b - 0.4060f) / 0.2250f,\n        // Ignore the alpha/transparency channel\n        InputImage[id.xy].a);\n}"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#create-imageclassifieropenvino-script",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#create-imageclassifieropenvino-script",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Create ImageClassifierOpenVINO Script",
    "text": "Create ImageClassifierOpenVINO Script\nDuplicate the ImageClassifier script and name the copy ImageClassifierOpenVINO.\n\n\n\n\n\nUpdate class name\nOpen the new script in the code editor and replace the class name with the new file name.\npublic class ImageClassifierOpenVINO : MonoBehaviour\nUpdate required namespaces\nWe no longer need the Barracuda namespace. Instead, we need the System.Runtime.InteropServices namespace to handle interactions with the OpenVINO plugin.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Rendering;\nusing System;\nusing UnityEngine.UI;\nusing System.Runtime.InteropServices;\nRemove Barracuda code\nWe need to delete all the public and private Barracuda variables, along with the InitializeWorker and OnDisable methods.\nAdd code to copy plugins.xml file to StreamingAssets folder\nUnity provides an InitializeOnLoad attribute to run code in the Unity Editor without requiring action from the user. This attribute requires the UnityEditor namespace. We can only use this while in the Editor, so we need to wrap the code in Conditional compilation preprocessor directives.\n#if UNITY_EDITOR\nusing UnityEditor;\n\n[InitializeOnLoad]\npublic class Startup\n{\n    static Startup()\n    {\n        // Get all files named \"plugins.xml\"\n        string[] files = Directory.GetFiles(\"./Assets/\", \"plugins.xml\", SearchOption.AllDirectories);\n        // Iterate through each found file\n        foreach (string file in files)\n        {\n            // Check if the file is in the \"x86_64\" folder\n            if (file.Contains(\"x86_64\"))\n            {\n                // Define file path for StreamingAssets folder\n                string targetPath = $\"{Application.streamingAssetsPath}/plugins.xml\";\n                // Print the source file path\n                Debug.Log(file);\n                // Only copy the file to the StreamingAssets folder if it is not already present\n                if (!File.Exists(targetPath)) File.Copy(file, targetPath);\n            }\n        }\n    }\n}\n#endif\nWe use the UNITY_EDITOR scripting symbol to check whether we are in the Unity Editor. We are in the Editor, so it returns true, and the code executes.\n\n\n\n\n\nIf we check if we are not in the Unity Editor, it returns false, and the code block does not execute.\n\n\n\n\n\nWe can verify the code works by saving the script and going to the StreamingAssets folder in the Editor. The plugins.xml file should be present.\n\n\n\n\n\nUpdate data processing variables\nNext, we can remove the processingMaterial variable from the Data Processing section. We no longer need to download model output from the GPU to the CPU. However, we now need to download the input image to the CPU before sending it to the plugin. We can do this asynchronously to help reduce the GPU-to-CPU performance bottleneck.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 216;\n[Tooltip(\"The compute shader for GPU processing\")]\npublic ComputeShader processingShader;\n[Tooltip(\"Asynchronously download input image from the GPU to the CPU.\")]\npublic bool useAsyncGPUReadback = true;\nUpdate variables for user interface\nWe’ll add new dropdown menus so that we can switch between the available OpenVINO models and devices at runtime.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayPredictedClass = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.red;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available OpenVINO models\")]\npublic Dropdown modelDropdown;\n[Tooltip(\"The dropdown menu that lists available OpenVINO devices\")]\npublic Dropdown deviceDropdown;\nDefine public variables for the OpenVINO plugin\n[Header(\"OpenVINO\")]\n[Tooltip(\"The name of the openvino models folder\")]\npublic string openvinoModelsDir = \"OpenVINOModels\";\nUpdate input variables\nLike in the previous tutorial series, when using asynchronous GPU readback, we need one Texture that stores data on the GPU and one that stores data on the CPU.\n// The test image dimensions\nprivate Vector2Int imageDims;\n// The test image texture\nprivate Texture imageTexture;\n// The current screen object dimensions\nprivate Vector2Int screenDims;\n// The model GPU input texture\nprivate RenderTexture inputTextureGPU;\n// The model CPU input texture\nprivate Texture2D inputTextureCPU;\nDefine private variables for the OpenVINO plugin\n// File paths for the available OpenVINO models\nprivate List&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available OpenVINO models\nprivate List&lt;string&gt; modelNames = new List&lt;string&gt;();\n// Names of the available OpenVINO devices\nprivate List&lt;string&gt; openvinoDevices = new List&lt;string&gt;();\nImport functions from the OpenVINO plugin\nWe pass the pointer to the input pixel data as an IntPtr.\n// Name of the DLL file\nconst string dll = \"OpenVINO_Image_Classifier_DLL\";\n\n[DllImport(dll)]\nprivate static extern int GetDeviceCount();\n\n[DllImport(dll)]\nprivate static extern IntPtr GetDeviceName(int index);\n\n[DllImport(dll)]\nprivate static extern int LoadModel(string model, int index, int[] inputDims);\n\n[DllImport(dll)]\nprivate static extern int PerformInference(IntPtr inputData);\nDefine method to get the available OpenVINO models\n/// &lt;summary&gt;\n/// Get the file paths for available OpenVION models\n/// &lt;/summary&gt;\nprivate void GetOpenVINOModels()\n{\n    // Get the paths for the XML file for each model\n    foreach (string file in System.IO.Directory.GetFiles($\"{Application.streamingAssetsPath}/{openvinoModelsDir}\"))\n    {\n        if (file.EndsWith(\".xml\"))\n        {\n            modelPaths.Add(file);\n            string modelName = file.Split('\\\\')[1].Split('.')[0];\n            modelNames.Add(modelName.Substring(0, modelName.Length));\n        }\n    }\n}\nDefine method to get the names of available OpenVINO devices\n/// &lt;summary&gt;\n/// Get the names of the available OpenVINO devices\n/// &lt;/summary&gt;\nprivate void GetOpenVINODevices()\n{\n    // Get the number of available OpenVINO devices\n    int deviceCount = GetDeviceCount();\n\n    for (int i = 0; i &lt; deviceCount; i++)\n    {\n        openvinoDevices.Add(Marshal.PtrToStringAnsi(GetDeviceName(i)));\n    }\n}\nUpdate method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nprivate void InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach(WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add OpenVINO model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n\n    // Remove default dropdown options\n    deviceDropdown.ClearOptions();\n    // Add OpenVINO device names to menu\n    deviceDropdown.AddOptions(openvinoDevices);\n    // Select the first option in the dropdown\n    deviceDropdown.SetValueWithoutNotify(0);\n}\nDefine Awake Method\nWe’ll implement the code to copy the plugins.xml file from the StreamingAssets folder to the Plugins/x86_64 folder in the build folder in the Awake() method. The code should be inactive since we are in the Editor.\n// Awake is called when the script instance is being loaded\nprivate void Awake()\n{\n    #if !UNITY_EDITOR\n        // Define the path for the plugins.xml file in the StreamingAssets folder\n        string sourcePath = $\"{Application.streamingAssetsPath}/plugins.xml\";\n    // Define the destination path for the plugins.xml file\n    string targetPath = $\"{Application.dataPath}/Plugins/x86_64/plugins.xml\";\n    // Only copy the file if it is not already present at the destination\n    if (!File.Exists(targetPath)) File.Copy(sourcePath, targetPath);\n    #endif\n}\nUpdate Start method\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Initialize list of class labels from JSON file\n    classes = JsonUtility.FromJson&lt;ClassLabels&gt;(classLabels.text).classes;\n\n    // Get the file paths for available OpenVINO models\n    GetOpenVINOModels();\n    // Get the names of available OpenVINO devices\n    GetOpenVINODevices();\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n}\nUpdate method to process images using a compute shader\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The target image RenderTexture&lt;/param&gt;\n/// &lt;param name=\"computeShader\"&gt;The target ComputerShader&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The target ComputeShader function&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, ComputeShader computeShader, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = computeShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = new RenderTexture(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    computeShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release RenderTexture\n    result.Release();\n}\nUpdate method to handle asynchronous GPU readback\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Make sure the Texture2D is not null\n    if (inputTextureCPU)\n    {\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        inputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Textur2D\n        inputTextureCPU.Apply();\n    }\n}\nDefine method to send the input texture data to the plugin\n/// &lt;summary&gt;\n/// Pin memory for the input data and pass a reference to the plugin for inference\n/// &lt;/summary&gt;\n/// &lt;param name=\"texture\"&gt;The input texture&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic unsafe int UploadTexture(Texture2D texture)\n{\n    int classIndex = -1;\n\n    //Pin Memory\n    fixed (byte* p = texture.GetRawTextureData())\n    {\n        // Perform inference and get the predicted class index\n        classIndex = PerformInference((IntPtr)p);\n    }\n\n    return classIndex;\n}\nModify Update method\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputDims.x} x {inputDims.y}\");\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGBA32, false);\n        // Update the selected OpenVINO model\n        UpdateOpenVINOModel();\n    }\n\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputTextureGPU.width}x{inputTextureGPU.height}\");\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Flip image before sending to DLL\n    ProcessImageGPU(inputTextureGPU, processingShader, \"FlipXAxis\");\n\n    // Download pixel data from GPU to CPU\n    if (useAsyncGPUReadback)\n    {\n        AsyncGPUReadback.Request(inputTextureGPU, 0, TextureFormat.RGBA32, OnCompleteReadback);\n    }\n    else\n    {\n        RenderTexture.active = inputTextureGPU;\n        inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n        inputTextureCPU.Apply();\n    }\n\n    // Send reference to inputData to DLL\n    classIndex = UploadTexture(inputTextureCPU);\n    if (printDebugMessages) Debug.Log($\"Class Index: {classIndex}\");\n\n    // Check if index is valid\n    bool validIndex = classIndex &gt;= 0 && classIndex &lt; classes.Length;\n    if (printDebugMessages) Debug.Log(validIndex ? $\"Predicted Class: {classes[classIndex]}\" : \"Invalid index\");\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n}\nDefine a method to update the current OpenVINO model and device\n/// &lt;summary&gt;\n/// Update the selected OpenVINO model\n/// &lt;/summary&gt;\npublic void UpdateOpenVINOModel()\n{\n    int[] inputDims = new int[] {\n        inputTextureCPU.width,\n        inputTextureCPU.height\n    };\n\n    Debug.Log($\"Selected Device: {openvinoDevices[deviceDropdown.value]}\");\n\n    // Load the specified OpenVINO model\n    int return_val = LoadModel(modelPaths[modelDropdown.value], deviceDropdown.value, inputDims);\n\n    string[] return_messages = {\n        \"Model loaded and reshaped successfully\", \n        \"Failed to load model\",\n        \"Failed to reshape model input\",\n    };\n\n    Debug.Log(return_messages[return_val]);\n}\nThat covers the required code changes."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#modify-gui",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#modify-gui",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Modify GUI",
    "text": "Modify GUI\nAs mentioned earlier, we’ll add new dropdown menus to the GUI so we can switch between available OpenVINO models and devices at runtime.\nCreate new GUI objects\nSelect the WebcamDeviceText and WebcamDropdown objects and press Ctrl-d to duplicate them. Rename the duplicates to OpenVINOModelText and OpenVINOModelDropdown, respectively. Then select OpenVINOModelText and OpenVINOModelDropdown and press Ctrl-d. Rename the copies to OpenVINODevicelText and OpenVINODevicelDropdown, respectively.\n\n\n\n\n\nUpdate GUI Settings Panel Height\nSelect the SettingsPanel object and update the Height value to 255.\n\n\n\n\n\nUpdate GUI object positions\nSelect the OpenVINOModelText object and update the Pos Y value to -145 and the Text value to OpenVINO Model: in the Inspector tab.\n\n\n\n\n\nThen, select the OpenVINOModelDropdown object and update the Pos Y value to -165 in the Inspector tab.\n\n\n\n\n\nNext, select the OpenVINODevicelText object and update the Pos Y value to -200 and the Text value to OpenVINO Device: in the Inspector tab.\n\n\n\n\n\nLast but not least, select the OpenVINODevicelDropdown object and update the Pos Y value to -220 in the Inspector tab.\n\n\n\n\n\nThe updated GUI should look like the image below."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#add-imageclassifieropenvino-component",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#add-imageclassifieropenvino-component",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Add ImageClassifierOpenVINO Component",
    "text": "Add ImageClassifierOpenVINO Component\nNow we can add the new ImageClassifierOpenVINO script to the InferenceManager object. Make sure to disable the existing ImageClassifier component, as shown below."
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#update-on-value-changed-events",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#update-on-value-changed-events",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Update On Value Changed Events",
    "text": "Update On Value Changed Events\nWith the ImageClassifierOpenVINO component added, we can update the On Value Changed events for the WebcamToggle, WebcamDropdown, OpenVINOModelDropdown, and OpenVINODevicelDropdown objects.\nUpdate the WebcamToggle On Value Changed Event\n\n\n\n\n\nUpdate the WebcamDropdown On Value Changed Event\n\n\n\n\n\nUpdate the OpenVINOModelDropdown On Value Changed Event\n\n\n\n\n\nUpdate the OpenVINODeviceDropdown On Value Changed Event"
  },
  {
    "objectID": "posts/fastai-openvino-unity-tutorial/part-3/index.html#summary",
    "href": "posts/fastai-openvino-unity-tutorial/part-3/index.html#summary",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 3",
    "section": "Summary",
    "text": "Summary\nThis tutorial series covered creating an OpenVINO plugin to improve inference speed in the Unity game engine.\nPrevious: How to Create an OpenVINO Plugin for Unity on Windows Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nInstall Dependencies\nConfigure Kaggle API\nDownload Dataset\nInspect Dataset\nDefine Dataloaders\nDefine Learner\nInspect Trained Model\nImplement Processing Steps\nExport the Model\nSummary"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#introduction",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nIn this tutorial series, we will walk through training an image classifier using the fastai library and implementing it in a Unity game engine project using the Barracuda inference library. Check out this post for more information about Barracuda. We will then build the Unity project to run in a web browser and host it using GitHub Pages.\nThe tutorial uses this American Sign Language (ASL) dataset from Kaggle but feel free to follow along with a different dataset. The dataset contains sample images for digits 1-9, letters A-Z, and some common words. One could use a model trained on this dataset to map hand gestures to user input or make an ASL education game.\nIn-Browser Demo: ASL Classifier"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#overview",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#overview",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Overview",
    "text": "Overview\nPart 1 covers how to finetune a ResNet model for image classification using the fastai library and export it to ONNX format. The training code is available in the Jupyter notebook linked below, and links for running the notebook on Google Colab and Kaggle are below as well.\n\n\n\nJupyter Notebook\nColab\nKaggle\n\n\n\n\nGitHub Repository\nOpen In Colab\nOpen in Kaggle"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#install-dependencies",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#install-dependencies",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Install Dependencies",
    "text": "Install Dependencies\nThe training code requires PyTorch for the fastai library, the fastai library itself for training, and the Kaggle API Python package for downloading the dataset. Google Colab uses an older version of Pillow, so update that package when training there.\nUncomment the cell below if running on Google Colab or Kaggle\n# %%capture\n# !pip3 install -U torch torchvision torchaudio\n# !pip3 install -U fastai\n# !pip3 install -U kaggle\n# !pip3 install -U Pillow\nNote for Colab: You must restart the runtime in order to use newly installed version of Pillow.\nImport all fastai computer vision functionality\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#configure-kaggle-api",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#configure-kaggle-api",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Configure Kaggle API",
    "text": "Configure Kaggle API\nThe Kaggle API tool requires an API Key for a Kaggle account. Sign in or create a Kaggle account using the link below, then click the Create New API Token button.\n\nKaggle Account Settings: https://www.kaggle.com/me/account\n\n\n\n\n\n\nKaggle will generate and download a kaggle.json file containing your username and new API token. Paste the values for each in the code cell below.\nEnter Kaggle username and API token\ncreds = '{\"username\":\"\",\"key\":\"\"}'\nSave Kaggle credentials if none are present * Source: https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\n# Save API key to a json file if it does not already exist\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\nImport Kaggle API\nfrom kaggle import api\n(Optional) Define method to display default function arguments\nThe code cell below defines a method to display the default arguments for a specified function. It’s not required, but I find it convenient for creating quick references in notebooks.\nimport inspect\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\ndef inspect_default_args(target, annotations: bool=False):\n    # Get the argument names\n    args = inspect.getfullargspec(target).args\n    # Get the default values\n    defaults = inspect.getfullargspec(target).defaults\n\n    index = [\"Default Value\"]\n\n    # Pad defaults\n    defaults = [None]*(len(args)-len(defaults)) + list(defaults)\n    if annotations:\n        index.append(\"Annotation\")\n        annotations = inspect.getfullargspec(target).annotations.values()\n        # Pad annotations\n        annotations = [None]*(len(args)-len(annotations)) + list(annotations)\n        default_args = {arg:[df, annot] for arg,df,annot in zip(args, defaults, annotations)}\n    else:\n        default_args = {arg:[default] for arg,default in zip(args, defaults)}\n        \n    return pd.DataFrame(default_args, index=index).T"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#download-dataset",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#download-dataset",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Download Dataset",
    "text": "Download Dataset\nNow that we have our Kaggle credentials set, we need to define the dataset and where to store it.\nDefine path to dataset\nWe’ll use the default archive and data folders for the fastai library to store the compressed and uncompressed datasets.\nkaggle_dataset = 'belalelwikel/asl-and-some-words'\narchive_dir = URLs.path()\ndataset_dir = archive_dir/'../data'\ndataset_name = 'asl-and-some-words'\narchive_path = Path(f'{archive_dir}/{dataset_name}.zip')\ndataset_path = Path(f'{dataset_dir}/{dataset_name}')\nDefine method to extract the dataset from an archive file\ndef file_extract(fname, dest=None):\n    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n    if dest is None: dest = Path(fname).parent\n    fname = str(fname)\n    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n    else: raise Exception(f'Unrecognized archive: {fname}')\nDownload the dataset if it is not present\nThe archive file is over 2GB, so we don’t want to download it more than necessary.\nif not archive_path.exists():\n    api.dataset_download_cli(kaggle_dataset, path=archive_dir)\n    file_extract(fname=archive_path, dest=dataset_path)"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#inspect-dataset",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#inspect-dataset",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Inspect Dataset",
    "text": "Inspect Dataset\nWe can start inspecting the dataset once it finishes downloading.\nInspect the dataset path\nThe training data is in a subfolder named ASL, and there are over 200,000 samples.\ndataset_path.ls()\n    (#1) [Path('/home/innom-dt/.fastai/archive/../data/asl-and-some-words/ASL')]\nGet image file paths\nfiles = get_image_files(dataset_path/\"ASL\")\nlen(files)\n    203000\nInspect files\nThe dataset indicates the object class in both the folder and file names.\nfiles[0], files[-1]\n    (Path('/home/innom-dt/.fastai/archive/../data/asl-and-some-words/ASL/J/J1491.jpg'),\n     Path('/home/innom-dt/.fastai/archive/../data/asl-and-some-words/ASL/E/E1063.jpg'))\nInspect class folder names\nThere are 51 class folders, and the dataset does not predefine a training-validation split.\nfolder_names = [path.name for path in Path(dataset_path/'ASL').ls()]\nfolder_names.sort()\nprint(f\"Num classes: {len(folder_names)}\")\npd.DataFrame(folder_names)\n    Num classes: 51\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n1\n\n\n3\n\n\n\n\n2\n\n\n4\n\n\n\n\n3\n\n\n5\n\n\n\n\n4\n\n\n7\n\n\n\n\n5\n\n\n8\n\n\n\n\n6\n\n\n9\n\n\n\n\n7\n\n\nA\n\n\n\n\n8\n\n\nB\n\n\n\n\n9\n\n\nBaby\n\n\n\n\n10\n\n\nBrother\n\n\n\n\n11\n\n\nC\n\n\n\n\n12\n\n\nD\n\n\n\n\n13\n\n\nDont_like\n\n\n\n\n14\n\n\nE\n\n\n\n\n15\n\n\nF\n\n\n\n\n16\n\n\nFriend\n\n\n\n\n17\n\n\nG\n\n\n\n\n18\n\n\nH\n\n\n\n\n19\n\n\nHelp\n\n\n\n\n20\n\n\nHouse\n\n\n\n\n21\n\n\nI\n\n\n\n\n22\n\n\nJ\n\n\n\n\n23\n\n\nK\n\n\n\n\n24\n\n\nL\n\n\n\n\n25\n\n\nLike\n\n\n\n\n26\n\n\nLove\n\n\n\n\n27\n\n\nM\n\n\n\n\n28\n\n\nMake\n\n\n\n\n29\n\n\nMore\n\n\n\n\n30\n\n\nN\n\n\n\n\n31\n\n\nName\n\n\n\n\n32\n\n\nNo\n\n\n\n\n33\n\n\nO_OR_0\n\n\n\n\n34\n\n\nP\n\n\n\n\n35\n\n\nPay\n\n\n\n\n36\n\n\nPlay\n\n\n\n\n37\n\n\nQ\n\n\n\n\n38\n\n\nR\n\n\n\n\n39\n\n\nS\n\n\n\n\n40\n\n\nStop\n\n\n\n\n41\n\n\nT\n\n\n\n\n42\n\n\nU\n\n\n\n\n43\n\n\nV_OR_2\n\n\n\n\n44\n\n\nW_OR_6\n\n\n\n\n45\n\n\nWith\n\n\n\n\n46\n\n\nX\n\n\n\n\n47\n\n\nY\n\n\n\n\n48\n\n\nYes\n\n\n\n\n49\n\n\nZ\n\n\n\n\n50\n\n\nnothing\n\n\n\n\n\n\nInspect one of the training images\nThe sample images all have a resolution of 200x200.\nimport PIL\nimg = PIL.Image.open(files[0])\nprint(f\"Image Dims: {img.shape}\")\nimg\n    Image Dims: (200, 200)"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#define-dataloaders",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#define-dataloaders",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Define Dataloaders",
    "text": "Define Dataloaders\nNext, we need to define the Transforms for the DataLoaders object.\nDefine target input dimensions\nThe Unity project will take input from a webcam, and most webcams don’t have a square aspect ratio like the training samples. We will need to account for this to get more accurate predictions.\nWe can train with a square aspect ratio and crop the webcam input in Unity, but that might make users feel cramped when using the application.\nAlternatively, we can expand the training images to a more typical aspect ratio like 4:3 or 16:9. This approach will allow us to use the entire webcam input, so we’ll go with this one.\nI have a separate tutorial for cropping images on the GPU in Unity for anyone that wants to try the other approach.\nBelow are some sample input dimensions in different aspect ratios.\n# size_1_1 = (224, 224)\n# size_3_2 = (224, 336)\n# size_4_3 = (216, 288)\nsize_16_9 = (216, 384)\n# size_16_9_l = (288, 512)\nDefine Transforms\nSomething else to consider is that the webcam input in Unity mirrors the actual image. Mirrored input would likely not be an issue for something like a pet classifier, but hand orientation matters for ASL. We either need to flip the input image each time in Unity, or we can train the model with pre-flipped images. It is easier to mirror the training images, so we’ll use the FlipItem transform with a probability of 1.0 to flip every training sample.\nI have a separate tutorial covering how to flip images on the GPU in Unity for anyone that wants to try that approach.\nSince we are resizing to a different aspect ratio, we need to choose a padding method. The default reflection padding might add more fingers, changing an image’s meaning. The zeros padding option might work, but most user backgrounds will not be pure black. Therefore, we’ll go with border padding.\nWe can add some batch transforms like tweaking the contrast, saturation, hue, zoom, brightness, and warping to help crappify the images. However, we need to disable the do_flip and max_rotate options in aug_transforms.\ninspect_default_args(aug_transforms)\n\n\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\nmult\n\n\n1.0\n\n\n\n\ndo_flip\n\n\nTrue\n\n\n\n\nflip_vert\n\n\nFalse\n\n\n\n\nmax_rotate\n\n\n10.0\n\n\n\n\nmin_zoom\n\n\n1.0\n\n\n\n\nmax_zoom\n\n\n1.1\n\n\n\n\nmax_lighting\n\n\n0.2\n\n\n\n\nmax_warp\n\n\n0.2\n\n\n\n\np_affine\n\n\n0.75\n\n\n\n\np_lighting\n\n\n0.75\n\n\n\n\nxtra_tfms\n\n\nNone\n\n\n\n\nsize\n\n\nNone\n\n\n\n\nmode\n\n\nbilinear\n\n\n\n\npad_mode\n\n\nreflection\n\n\n\n\nalign_corners\n\n\nTrue\n\n\n\n\nbatch\n\n\nFalse\n\n\n\n\nmin_scale\n\n\n1.0\n\n\n\n\n\n\n\nitem_tfms = [FlipItem(p=1.0), Resize(size_16_9, method=ResizeMethod.Pad, pad_mode=PadMode.Border)]\n\nbatch_tfms = [\n    Contrast(max_lighting=0.25),\n    Saturation(max_lighting=0.25),\n    Hue(max_hue=0.05),\n    *aug_transforms(\n        size=size_16_9, \n        mult=1.0,\n        do_flip=False,\n        flip_vert=False,\n        max_rotate=0.0,\n        min_zoom=0.5,\n        max_zoom=1.5,\n        max_lighting=0.5,\n        max_warp=0.2, \n        p_affine=0.0,\n        pad_mode=PadMode.Border)\n]\nDefine batch size\nbs = 128\nDefine DataLoaders object\nWe can use the from_folder method to instantiate the DataLoaders object.\ninspect_default_args(ImageDataLoaders.from_folder)\n\n\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\ncls\n\n\nNone\n\n\n\n\npath\n\n\nNone\n\n\n\n\ntrain\n\n\ntrain\n\n\n\n\nvalid\n\n\nvalid\n\n\n\n\nvalid_pct\n\n\nNone\n\n\n\n\nseed\n\n\nNone\n\n\n\n\nvocab\n\n\nNone\n\n\n\n\nitem_tfms\n\n\nNone\n\n\n\n\nbatch_tfms\n\n\nNone\n\n\n\n\nbs\n\n\n64\n\n\n\n\nval_bs\n\n\nNone\n\n\n\n\nshuffle\n\n\nTrue\n\n\n\n\ndevice\n\n\nNone\n\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    path=dataset_path/'ASL', \n    valid_pct=0.2, \n    bs=bs, \n    item_tfms=item_tfms, \n    batch_tfms=batch_tfms\n)\nVerify DataLoaders object\nLet’s verify the DataLoaders object works as expected before training a model.\ndls.train.show_batch()\n\n\n\n\n\nWe can see that the DataLoaders object applies the transforms to the training split, including mirroring the image. However, it does not appear to mirror images from the validation split.\ndls.valid.show_batch()\n\n\n\n\n\nWe can get around this by using a solution provided on the fastai forums to apply the training split transforms to the validation split. It is not strictly necessary to mirror the validation split, but the accuracy metrics would be confusing during training without it.\nApply training split transforms to validation split\nwith dls.valid.dataset.set_split_idx(0): dls[1].show_batch()"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#define-learner",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#define-learner",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Define Learner",
    "text": "Define Learner\nNow we need to define the Learner object for training the model.\nInspect Learner parameters\ninspect_default_args(vision_learner)\n\n\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\ndls\n\n\nNone\n\n\n\n\narch\n\n\nNone\n\n\n\n\nnormalize\n\n\nTrue\n\n\n\n\nn_out\n\n\nNone\n\n\n\n\npretrained\n\n\nTrue\n\n\n\n\nloss_func\n\n\nNone\n\n\n\n\nopt_func\n\n\n&lt;function Adam at 0x7fa5e274a560&gt;\n\n\n\n\nlr\n\n\n0.001\n\n\n\n\nsplitter\n\n\nNone\n\n\n\n\ncbs\n\n\nNone\n\n\n\n\nmetrics\n\n\nNone\n\n\n\n\npath\n\n\nNone\n\n\n\n\nmodel_dir\n\n\nmodels\n\n\n\n\nwd\n\n\nNone\n\n\n\n\nwd_bn_bias\n\n\nFalse\n\n\n\n\ntrain_bn\n\n\nTrue\n\n\n\n\nmoms\n\n\n(0.95, 0.85, 0.95)\n\n\n\n\ncut\n\n\nNone\n\n\n\n\nn_in\n\n\n3\n\n\n\n\ninit\n\n\n&lt;function kaiming_normal_ at 0x7fa60b397be0&gt;\n\n\n\n\ncustom_head\n\n\nNone\n\n\n\n\nconcat_pool\n\n\nTrue\n\n\n\n\nlin_ftrs\n\n\nNone\n\n\n\n\nps\n\n\n0.5\n\n\n\n\npool\n\n\nTrue\n\n\n\n\nfirst_bn\n\n\nTrue\n\n\n\n\nbn_final\n\n\nFalse\n\n\n\n\nlin_first\n\n\nFalse\n\n\n\n\ny_range\n\n\nNone\n\n\n\n\n\n\nDefine model\nI recommend sticking with a ResNet18 or ResNet34 model, as the larger models can significantly lower frame rates.\nmodel = resnet18\nDefine metrics\nmetrics = [error_rate, accuracy]\nDefine Learner object\nlearn = vision_learner(dls, model, metrics=metrics).to_fp16()\nFind learning rate\ninspect_default_args(learn.lr_find)\n\n\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\nself\n\n\nNone\n\n\n\n\nstart_lr\n\n\n0.0\n\n\n\n\nend_lr\n\n\n10\n\n\n\n\nnum_it\n\n\n100\n\n\n\n\nstop_div\n\n\nTrue\n\n\n\n\nshow_plot\n\n\nTrue\n\n\n\n\nsuggest_funcs\n\n\n&lt;function valley at 0x7fa5e24996c0&gt;\n\n\n\n\n\n\nDefine suggestion methods\nsuggest_funcs = [valley, minimum, steep]\nwith dls.valid.dataset.set_split_idx(0): learn.lr_find(suggest_funcs=suggest_funcs)\n\n\n\n\n\nDefine learning rate\nlr = 2e-3\nlr\n    0.002\nDefine number of epochs\nepochs = 3\nFine tune model\nAfter picking a learning rate, we can train the model for a few epochs. Training can take a while on Google Colab and Kaggle.\ninspect_default_args(learn.fine_tune)\n\n\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\nself\n\n\nNone\n\n\n\n\nepochs\n\n\nNone\n\n\n\n\nbase_lr\n\n\n0.002\n\n\n\n\nfreeze_epochs\n\n\n1\n\n\n\n\nlr_mult\n\n\n100\n\n\n\n\npct_start\n\n\n0.3\n\n\n\n\ndiv\n\n\n5.0\n\n\n\n\nlr_max\n\n\nNone\n\n\n\n\ndiv_final\n\n\n100000.0\n\n\n\n\nwd\n\n\nNone\n\n\n\n\nmoms\n\n\nNone\n\n\n\n\ncbs\n\n\nNone\n\n\n\n\nreset_opt\n\n\nFalse\n\n\n\n\n\n\n\nwith dls.valid.dataset.set_split_idx(0): learn.fine_tune(epochs, base_lr=lr)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.365705\n\n\n0.175888\n\n\n0.056305\n\n\n0.943695\n\n\n04:52\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.038334\n\n\n0.021014\n\n\n0.008103\n\n\n0.991897\n\n\n04:56\n\n\n\n\n1\n\n\n0.012614\n\n\n0.011383\n\n\n0.004236\n\n\n0.995764\n\n\n04:59\n\n\n\n\n2\n\n\n0.006508\n\n\n0.006591\n\n\n0.003325\n\n\n0.996675\n\n\n04:55"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#inspect-trained-model",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#inspect-trained-model",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Inspect Trained Model",
    "text": "Inspect Trained Model\nOnce the model finishes training, we can test it on a sample image and see where it struggles.\nSelect a test image\nimport PIL\ntest_file = files[0]\ntest_file.name\n    'J1491.jpg'\ntest_img = PIL.Image.open(test_file)\ntest_img\n\n\n\n\n\nMake a prediction on a single image using a fastai.vision.core.PILImage\nRemember that we need to flip the test image before feeding it to the model.\nlearn.predict(PILImage(test_img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)))\n    ('J',\n     TensorBase(22),\n     TensorBase([9.6170e-14, 7.7060e-13, 2.5787e-13, 1.1222e-13, 1.5709e-10, 3.6805e-11,\n             1.7642e-11, 2.3571e-13, 3.5861e-15, 9.8273e-13, 4.1524e-14, 1.3218e-12,\n             7.3592e-14, 3.8404e-14, 4.9230e-12, 8.4399e-12, 2.0167e-11, 3.2757e-13,\n             4.0114e-10, 2.3624e-11, 8.3717e-14, 1.9143e-07, 1.0000e+00, 9.7685e-14,\n             9.4480e-15, 3.3952e-15, 9.4246e-12, 2.3079e-12, 1.6612e-15, 6.6745e-14,\n             3.9778e-14, 2.2675e-11, 1.7859e-14, 1.7659e-11, 5.1701e-11, 8.4209e-14,\n             4.6891e-11, 1.3487e-11, 1.0827e-11, 1.0881e-10, 2.6260e-09, 4.2682e-13,\n             3.1842e-13, 7.4326e-13, 4.8983e-13, 2.0801e-13, 9.1052e-14, 1.0467e-08,\n             2.3752e-14, 1.0124e-09, 6.7431e-11]))\nMake predictions for a group of images\nwith dls.valid.dataset.set_split_idx(0): learn.show_results()\n\n\n\n\n\nDefine an Interpretation object\nwith dls.valid.dataset.set_split_idx(0): interp = Interpretation.from_learner(learn)\nPlot top losses\nwith dls.valid.dataset.set_split_idx(0): interp.plot_top_losses(k=9, figsize=(15,10))"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#implement-processing-steps",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#implement-processing-steps",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Implement Processing Steps",
    "text": "Implement Processing Steps\nWhen we are satisfied with the model, we can start preparing for implementing it in Unity. We will need to apply the same preprocessing and post-processing in Unity that fastai applies automatically. We will verify we understand the processing steps by implementing them in Python first.\nInspect the after_item pipeline\nWe don’t need to worry about flipping or padding the image in Unity with the current training approach.\nlearn.dls.after_item\n    Pipeline: FlipItem -- {'p': 1.0} -&gt; Resize -- {'size': (384, 216), 'method': 'pad', 'pad_mode': 'border', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, 0), 'p': 1.0} -&gt; ToTensor\nInspect the after_batch pipeline\nThe after_batch pipeline first scales the image color channel values from \\([0,255]\\) to \\([0,1]\\). Unity already uses the range \\([0,1]\\), so we don’t need to implement this step. We also don’t need to implement any of the image augmentations. However, we do need to normalize the image using the ImageNet stats.\nlearn.dls.after_batch\n    Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Warp -- {'magnitude': 0.2, 'p': 1.0, 'draw_x': None, 'draw_y': None, 'size': (216, 384), 'mode': 'bilinear', 'pad_mode': 'border', 'batch': False, 'align_corners': True, 'mode_mask': 'nearest'} -&gt; Contrast -- {'max_lighting': 0.25, 'p': 1.0, 'draw': None, 'batch': False} -&gt; Saturation -- {'max_lighting': 0.25, 'p': 1.0, 'draw': None, 'batch': False} -&gt; Hue -- {'p': 1.0} -&gt; Brightness -- {'max_lighting': 0.5, 'p': 1.0, 'draw': None, 'batch': False} -&gt; Normalize -- {'mean': tensor([[[[0.4850]],\n    \n             [[0.4560]],\n    \n             [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n    \n             [[0.2240]],\n    \n             [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\nReset test image\ntest_img = PIL.Image.open(test_file)\ntest_img\n\n\n\n\n\ntest_img = test_img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\ntest_img\n\n\n\n\n\ntest_img.size\n    (200, 200)\n\nmin(test_img.size)\n    200\n\nmin_dim = test_img.size.index(min(test_img.size))\nmax_dim = 1 - min_dim\ntarget_dim = 224\nSet input dims\ninp_dims = [0,0]\ninp_dims[min_dim] = target_dim\ninp_dims[max_dim] = int(test_img.size[max_dim] / (test_img.size[min_dim]/target_dim))\ninp_dims\n    [224, 224]\n\nresized_img = test_img.resize(inp_dims)\nresized_img\n\n\n\n\n\nConvert image to tensor\nimg_tensor = tensor(resized_img).permute(2, 0, 1)\nimg_tensor.shape, img_tensor\n    (torch.Size([3, 224, 224]),\n     tensor([[[  0,   0,   0,  ...,   1,   0,   0],\n              [  0,   4,   2,  ...,   9,   2,   0],\n              [  5,  82,  99,  ...,  74,   8,   0],\n              ...,\n              [  3, 127, 154,  ..., 141,   0,   3],\n              [  3, 102, 125,  ..., 120,   0,   0],\n              [  0,   0,   4,  ...,   0,   1,   0]],\n     \n             [[  4,   1,   2,  ...,   0,   2,   5],\n              [  2,   1,   0,  ...,   0,   0,   5],\n              [  0,  75,  91,  ...,  63,   1,   1],\n              ...,\n              [  3, 126, 150,  ..., 151,   0,   0],\n              [  7, 105, 122,  ..., 127,   1,   0],\n              [  8,   5,   3,  ...,   4,   6,   2]],\n     \n             [[253, 254, 255,  ..., 253, 255, 254],\n              [244, 220, 199,  ..., 209, 237, 255],\n              [212, 222, 180,  ..., 188, 211, 251],\n              ...,\n              [196, 225, 171,  ..., 238, 204, 255],\n              [207, 247, 222,  ..., 242, 218, 255],\n              [223, 203, 193,  ..., 219, 247, 254]]], dtype=torch.uint8))\nScale tensor values\nscaled_tensor = img_tensor.float().div_(255)\nPrepare imagenet mean values\nmean_tensor = tensor(imagenet_stats[0]).view(1,1,-1).permute(2, 0, 1)\nmean_tensor.shape, mean_tensor\n    (torch.Size([3, 1, 1]),\n     tensor([[[0.4850]],\n     \n             [[0.4560]],\n     \n             [[0.4060]]]))\nPrepare imagenet std values\nstd_tensor = tensor(imagenet_stats[1]).view(1,1,-1).permute(2, 0, 1)\nstd_tensor.shape, std_tensor\n    (torch.Size([3, 1, 1]),\n     tensor([[[0.2290]],\n     \n             [[0.2240]],\n     \n             [[0.2250]]]))\nNormalize and batch image tensor\nnormalized_tensor = (scaled_tensor - mean_tensor) / std_tensor\nbatched_tensor = normalized_tensor.unsqueeze(dim=0)\nbatched_tensor.shape, batched_tensor\n    (torch.Size([1, 3, 224, 224]),\n     tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1179, -2.1179],\n               [-2.1179, -2.0494, -2.0837,  ..., -1.9638, -2.0837, -2.1179],\n               [-2.0323, -0.7137, -0.4226,  ..., -0.8507, -1.9809, -2.1179],\n               ...,\n               [-2.0665,  0.0569,  0.5193,  ...,  0.2967, -2.1179, -2.0665],\n               [-2.0665, -0.3712,  0.0227,  ..., -0.0629, -2.1179, -2.1179],\n               [-2.1179, -2.1179, -2.0494,  ..., -2.1179, -2.1008, -2.1179]],\n     \n              [[-1.9657, -2.0182, -2.0007,  ..., -2.0357, -2.0007, -1.9482],\n               [-2.0007, -2.0182, -2.0357,  ..., -2.0357, -2.0357, -1.9482],\n               [-2.0357, -0.7227, -0.4426,  ..., -0.9328, -2.0182, -2.0182],\n               ...,\n               [-1.9832,  0.1702,  0.5903,  ...,  0.6078, -2.0357, -2.0357],\n               [-1.9132, -0.1975,  0.1001,  ...,  0.1877, -2.0182, -2.0357],\n               [-1.8957, -1.9482, -1.9832,  ..., -1.9657, -1.9307, -2.0007]],\n     \n              [[ 2.6051,  2.6226,  2.6400,  ...,  2.6051,  2.6400,  2.6226],\n               [ 2.4483,  2.0300,  1.6640,  ...,  1.8383,  2.3263,  2.6400],\n               [ 1.8905,  2.0648,  1.3328,  ...,  1.4722,  1.8731,  2.5703],\n               ...,\n               [ 1.6117,  2.1171,  1.1759,  ...,  2.3437,  1.7511,  2.6400],\n               [ 1.8034,  2.5006,  2.0648,  ...,  2.4134,  1.9951,  2.6400],\n               [ 2.0823,  1.7337,  1.5594,  ...,  2.0125,  2.5006,  2.6226]]]]))\nPass tensor to model\nwith torch.no_grad():\n    preds = learn.model(batched_tensor.cuda())\npreds\n    TensorBase([[-4.9931e+00, -1.9711e+00, -3.3677e+00, -3.0452e+00,  3.9567e+00,\n              3.9293e+00,  3.1657e+00, -5.3549e+00, -7.9026e+00, -1.5491e+00,\n             -2.4086e+00, -2.6251e+00, -4.0321e+00, -7.3666e+00, -1.0557e+00,\n             -3.2344e-01,  4.7887e+00, -4.8819e+00,  6.5188e+00,  1.1152e+00,\n             -5.9519e-01,  1.1730e+01,  3.0779e+01, -4.4505e+00, -1.0000e+01,\n             -9.1124e+00, -3.7176e-01, -4.2437e+00, -8.6924e+00, -1.5119e+00,\n             -8.4118e+00,  9.1559e-01, -7.6669e+00,  1.7187e+00,  2.0639e+00,\n             -4.0788e+00,  9.0079e+00, -2.8547e-02,  1.1223e+00, -3.2541e-02,\n              8.9209e+00, -4.2307e+00, -3.6343e+00, -9.8461e-01, -4.2557e+00,\n             -2.2238e+00, -5.9167e+00,  7.0386e+00, -7.7322e+00,  4.3321e+00,\n             -3.1247e-01]], device='cuda:0')\nProcess model output\ntorch.nn.functional.softmax(preds, dim=1)\n    TensorBase([[2.9133e-16, 5.9815e-15, 1.4800e-15, 2.0433e-15, 2.2450e-12, 2.1844e-12,\n             1.0179e-12, 2.0287e-16, 1.5878e-17, 9.1219e-15, 3.8617e-15, 3.1101e-15,\n             7.6160e-16, 2.7138e-17, 1.4940e-14, 3.1072e-14, 5.1585e-12, 3.2557e-16,\n             2.9103e-11, 1.3097e-13, 2.3678e-14, 5.3343e-09, 1.0000e+00, 5.0120e-16,\n             1.9486e-18, 4.7354e-18, 2.9607e-14, 6.1632e-16, 7.2077e-18, 9.4674e-15,\n             9.5424e-18, 1.0727e-13, 2.0099e-17, 2.3949e-13, 3.3822e-13, 7.2685e-16,\n             3.5069e-10, 4.1729e-14, 1.3190e-13, 4.1563e-14, 3.2148e-10, 6.2438e-16,\n             1.1337e-15, 1.6041e-14, 6.0902e-16, 4.6457e-15, 1.1568e-16, 4.8942e-11,\n             1.8828e-17, 3.2679e-12, 3.1415e-14]], device='cuda:0')\npreds.argmax()\n    TensorBase(22, device='cuda:0')\ntorch.nn.functional.softmax(preds, dim=1)[0][preds.argmax()]\n    TensorBase(1., device='cuda:0')\nGet the class labels\nlearn.dls.vocab\n    ['1', '3', '4', '5', '7', '8', '9', 'A', 'B', 'Baby', 'Brother', 'C', 'D', 'Dont_like', 'E', 'F', 'Friend', 'G', 'H', 'Help', 'House', 'I', 'J', 'K', 'L', 'Like', 'Love', 'M', 'Make', 'More', 'N', 'Name', 'No', 'O_OR_0', 'P', 'Pay', 'Play', 'Q', 'R', 'S', 'Stop', 'T', 'U', 'V_OR_2', 'W_OR_6', 'With', 'X', 'Y', 'Yes', 'Z', 'nothing']\nGet the predicted class label\nlearn.dls.vocab[torch.nn.functional.softmax(preds, dim=1).argmax()]\n    'J'"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#export-the-model",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#export-the-model",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Export the Model",
    "text": "Export the Model\nThe last step is to export the trained model to ONNX format.\nDefine ONNX file name\nonnx_file_name = f\"{dataset_path.name}-{learn.arch.__name__}.onnx\"\nonnx_file_name\n    'asl-and-some-words-resnet18.onnx'\nExport trained model to ONNX\nWe’ll use an older opset_version to ensure the model is compatible with the Barracuda library. We will also unlock the input dimensions for the model to give ourselves more flexibility in Unity. Although, we’ll want to stick close to the training resolution for the best accuracy.\ntorch.onnx.export(learn.model.cpu(),\n                  batched_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=9,\n                  do_constant_folding=True,\n                  input_names = ['input'],\n                  output_names = ['output'],\n                  dynamic_axes={'input': {2 : 'height', 3 : 'width'}}\n                 )\nExport class labels\nWe can export the list of class labels to a JSON file and import it into the Unity project. That way, we don’t have to hardcode them, and we can easily swap in models trained on different datasets.\nimport json\n\nclass_labels = {\"classes\": list(learn.dls.vocab)}\nclass_labels_file_name = f\"{dataset_path.name}-classes.json\"\n\nwith open(class_labels_file_name, \"w\") as write_file:\n    json.dump(class_labels, write_file)"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-1/index.html#summary",
    "href": "posts/fastai-to-unity-tutorial/part-1/index.html#summary",
    "title": "Fastai to Unity Beginner Tutorial Pt. 1",
    "section": "Summary",
    "text": "Summary\nIn this post, we walked through how to finetune a ResNet model for image classification using the fastai library and export it to ONNX format. Part 2 will cover implementing the trained model in a Unity project using the Barracuda library.\nPrevious: Getting Started With Deep Learning in Unity\nNext: Fastai to Unity Tutorial Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "",
    "text": "Overview\nSet Up Unity Hub\nInstall Unity Editor\nCreate New Project\nInstall Barracuda Package\nImport Assets\nCreate Image Classifier Script\nCreate Processing Shaders\nSet up Unity Scene\nTest in Editor\nSummary"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#overview",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#overview",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Overview",
    "text": "Overview\nPart 1 covered training an image classification model using the fastai library and exporting it to ONNX. In this post, we’ll cover implementing a trained image classification model in a Unity project using the Barracuda library."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#set-up-unity-hub",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#set-up-unity-hub",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Set Up Unity Hub",
    "text": "Set Up Unity Hub\nBefore creating a project, we need to install Unity Hub, create a UnityID account, and activate a (free) Unity license. The beginner Unity tutorial linked below covers all these steps and how to create a simple flappy bird-style game.\n\nMake Your First Video Game - Ultimate Beginner Unity3D Tutorial\n\nThe link opens to the part covering how to install Unity for the first time, but I recommend watching the entire tutorial for those new to Unity."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#install-unity-editor",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#install-unity-editor",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Install Unity Editor",
    "text": "Install Unity Editor\nOnce we have Unity Hub installed and an activated license, we need to install a version of the Unity Editor. We will use the latest 2022.1+ release as early versions have some issues building WebGL projects with Barracuda. The tutorial uses 2022.1.3.f1, which you can install directly from the link below.\n\nUnity download archive: Unity 2022.1.3\n\nOpen Unity Hub and select the Installs section in the side panel. Then click the Install Editor button in the upper right-hand corner.\n\n\n\n\n\nClick the Install button next to the latest 2022.1 version under Other Versions.\n\n\n\n\n\nScroll down the Add modules selection menu and click the check box next to WebGL Build Support. Feel free to add any additional target platforms here as well. Click the Install button after selecting all desired modules.\n\n\n\n\n\nUnity Hub will begin downloading and installing the selected editor version. If the install fails the first time, click the retry button, and it should complete successfully."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#create-new-project",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#create-new-project",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Create New Project",
    "text": "Create New Project\nGo back to the Projects section after the editor finishes installing and click New Project.\n\n\n\n\n\nSelect the target editor version from the Editor Version dropdown menu.\n\n\n\n\n\nSelect the 2D Core template.\n\n\n\n\n\nPick a name for the project and a location for the project folder.\n\n\n\n\n\nFinally, click Create Project in the lower right-hand corner."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#install-barracuda-package",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#install-barracuda-package",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nInside the editor window, we’ll first install the Barracuda package. Select Window → Package Manager from the top menu.\n\n\n\n\n\nIn the Package Manager window, click the little + sign in the upper left-hand corner and select Add package from git URL... from the dropdown menu.\n\n\n\n\n\nEnter com.unity.barracuda into the text box and click Add.\n\n\n\n\n\nWait for the Barracuda package to install and close the Package Manager window."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#import-assets",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#import-assets",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Import Assets",
    "text": "Import Assets\nNext, we’ll import any ONNX, JSON, and test image files into the Assets folder. Right-click a space in the Assets section and select Create → Folder from the popup menu.\n\n\n\n\n\nName the new folder Models.\n\n\n\n\n\nDrag and drop any ONNX files and JSON class label files from the operating system’s file explorer into the Models folder. Sample files are available in the Google Drive link below.\n\nGoogle Drive: Model Assets\n\n\n\n\n\n\nWe can click on an ONNX file to examine it in the Inspector tab on the right-hand side of the editor window.\n\n\n\n\n\nWhen Netron is available, we can double-click on the ONNX file to open it in Netron.\n\n\n\n\n\n\n&lt;img src=\"./images/asl-and-some-words-resnet18.png\" alt=\"asl-and-some-words-resnet18\"&gt;\n\nNext, create an Images folder and drop any test images into it.\n\n\n\n\n\nMaybe stick with symmetrical hand signs (e.g., Play) since the model expects mirrored input images.\nGoogle Drive: Image Assets\n\n\n\n\n\nUnity automatically imports images as a Sprite (2D and UI) texture type. We don’t need to change it for our purposes."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#create-image-classifier-script",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#create-image-classifier-script",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Create Image Classifier Script",
    "text": "Create Image Classifier Script\nNow we can start coding. We’ll store C# scripts in a new Scripts folder. Right-click a space inside it and select Create → C# Script.\n\n\n\n\n\nWe’ll name the script ImageClassifier.\n\n\n\n\n\nDefault script code\nBy default, C# scripts contain the following code.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class ImageClassifier : MonoBehaviour\n{\n    // Start is called before the first frame update\n    void Start()\n    {\n        \n    }\n\n    // Update is called once per frame\n    void Update()\n    {\n        \n    }\n}\nAdd required namespaces\n\nUnity.Barracuda: Provides access to the Barracuda API.\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\n\n\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Barracuda;\nusing UnityEngine.Rendering;\nusing System;\nusing UnityEngine.UI;\n\nDefine public variables\nWe’ll add the required public variables above the Start method. We will be able to access these variables in the Inspector tab. We can add Header attributes to organize the public variables in the Inspector tab and use Tooltip attributes to provide information about variables.\nDefine scene object variables\nFirst, we need a variable to access the screen object that displays either a test image or webcam input.\n[Header(\"Scene Objects\")]\n[Tooltip(\"The Screen object for the scene\")]\npublic Transform screen;\nDefine data processing variables\nNext, we’ll define the variables for processing model input. We can set the default target input resolution to 216 and use it to scale the source resolution while maintaining the original aspect ratio.\nThe only preprocessing step we need to implement is normalizing images using the ImageNet stats. The fastest way to do this is with shaders. Shaders are programs that run on the GPU. We will implement ImageNet normalization in a Compute Shader and a Fragment shader to account for platform support. We attach a Fragment shader to a Material, so that is what we’ll pass into the script.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 216;\n[Tooltip(\"The compute shader for GPU processing\")]\npublic ComputeShader processingShader;\n[Tooltip(\"The material with the fragment shader for GPU processing\")]\npublic Material processingMaterial;\nDefine Barracuda variables\nWe’ll add the required private variables right below the public variables. We pass in ONNX files as an NNModel object.\nWe’ll be adding a SoftMax and an Argmax layer to the end of the model, so we need to define names for those. We need to indicate the output layer we want to modify with the new layers. In our case, there is only one output layer.\nBy default, Barracuda uses a channels-last order for Tensors. However, switching to channels first can significantly improve performance on some GPUs.\nWe can choose from several different inference backends (Although we’ll only want to use two in practice). The Auto option will automatically pick the best backend for the target platform.\n[Header(\"Barracuda\")]\n[Tooltip(\"The Barracuda/ONNX asset file\")]\npublic NNModel modelAsset;\n[Tooltip(\"The name for the custom softmax output layer\")]\npublic string softmaxLayer = \"softmaxLayer\";\n[Tooltip(\"The name for the custom softmax output layer\")]\npublic string argmaxLayer = \"argmaxLayer\";\n[Tooltip(\"The target output layer index\")]\npublic int outputLayerIndex = 0;\n[Tooltip(\"EXPERIMENTAL: Indicate whether to order tensor data channels first\")]\npublic bool useNCHW = true;\n[Tooltip(\"The model execution backend\")]\npublic WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\nDefine output processing variables\nAs mentioned in a previous post, reading model output from the GPU to the CPU can cause a significant performance bottleneck. Therefore, we will add the option to read the model output asynchronously at the cost of a few frames of latency. Unfortunately, this feature does not work with the inference backend used for WebGL builds.\nWe pass in the JSON file containing the class labels as a TextAsset.\n[Header(\"Output Processing\")]\n[Tooltip(\"Asynchronously download model output from the GPU to the CPU.\")]\npublic bool useAsyncGPUReadback = true;\n[Tooltip(\"A json file containing the class labels\")]\npublic TextAsset classLabels;\nDefine variables for debugging\nNext, we’ll add a Boolean variable to toggle printing debug messages to the console. These messages get printed to the console in the browser as well.\n[Header(\"Debugging\")]\n[Tooltip(\"Print debugging messages to the console\")]\npublic bool printDebugMessages = true;\nDefine webcam variables\nWe need to specify a desired resolution and framerate when using a webcam as input.\n[Header(\"Webcam\")]\n[Tooltip(\"Use a webcam as input\")]\npublic bool useWebcam = false;\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"The requested webcam framerate\")]\n[Range(0, 60)]\npublic int webcamFPS = 60;\nDefine variables for user interface\nWe’ll make a simple GUI that displays the predicted class, the current framerate, and controls for selecting webcam devices.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayPredictedClass = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.red;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n\n\nDefine private variables\nWe’ll add the required private variables right below the public variables.\nDefine private webcam variables\nWe’ll keep a list of available webcam devices so users can switch between them. Unity renders webcam input to a WebcamTexture.\n// List of available webcam devices\nprivate WebCamDevice[] webcamDevices;\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n// The name of the current webcam  device\nprivate string currentWebcam;\nDefine input variables\nWe’ll update the dimensions and content of the screen object based on the test image or webcam.\n// The test image dimensions\nprivate Vector2Int imageDims;\n// The test image texture\nprivate Texture imageTexture;\n// The current screen object dimensions\nprivate Vector2Int screenDims;\n// The model input texture\nprivate RenderTexture inputTexture;\nDefine Barracuda variables\nWe execute models in Barracuda using an IWorkerinterface and store data in Tensor objects.\n// The main interface to execute models\nprivate IWorker engine;\n// Stores the input data for the model\nprivate Tensor input;\nDefine variables for handling asynchronous GPU readback\nWhen using asynchronous GPU readback, we need one Texture that stores data on the GPU and one that stores data on the CPU.\n// Stores the raw model output on the GPU when using useAsyncGPUReadback\nprivate RenderTexture outputTextureGPU;\n// Stores the raw model output on the CPU when using useAsyncGPUReadback\nprivate Texture2D outputTextureCPU;\nDefine variables for tracking class labels\nWe need to create a little class that indicates the structure of the JSON content. Our JSON file only contains a single array of strings. We can store this array in a dedicated variable.\n// A class for reading in class labels from a JSON file\nclass ClassLabels { public string[] classes; }\n// The ordered list of class names\nprivate string[] classes;\n// Stores the predicted class index\nprivate int classIndex;\nDefine variables for tracking the framerate\nLastly, we need to define a couple of variables for the custom fps counter.\n// The current frame rate value\nprivate int fps = 0;\n// Controls when the frame rate value updates\nprivate float fpsTimer = 0f;\n\n\nDefine Initialization Methods\nWe first need to define some methods to initialize webcams, the screen object, any GUI dropdown menus, and the in-game camera.\nDefine method to initialize a webcam device\n/// &lt;summary&gt;\n/// Initialize the selected webcam device\n/// &lt;/summary&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the selected webcam device&lt;/param&gt;\nprivate void InitializeWebcam(string deviceName)\n{\n    // Stop any webcams already playing\n    if (webcamTexture && webcamTexture.isPlaying) webcamTexture.Stop();\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(deviceName, webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the webcam\n    webcamTexture.Play();\n    // Check if webcam is playing\n    useWebcam = webcamTexture.isPlaying;\n    // Update toggle value\n    useWebcamToggle.SetIsOnWithoutNotify(useWebcam);\n\n    Debug.Log(useWebcam ? \"Webcam is playing\" : \"Webcam not playing, option disabled\");\n}\nDefine method to initialize the in-scene screen object\n/// &lt;summary&gt;\n/// Resize and position an in-scene screen object\n/// &lt;/summary&gt;\nprivate void InitializeScreen()\n{\n    // Set the texture for the screen object\n    screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture = useWebcam ? webcamTexture : imageTexture;\n    // Set the screen dimensions\n    screenDims = useWebcam ? new Vector2Int(webcamTexture.width, webcamTexture.height) : imageDims;\n\n    // Flip the screen around the Y-Axis when using webcam\n    float yRotation = useWebcam ? 180f : 0f;\n    // Invert the scale value for the Z-Axis when using webcam\n    float zScale = useWebcam ? -1f : 1f;\n\n    // Set screen rotation\n    screen.rotation = Quaternion.Euler(0, yRotation, 0);\n    // Adjust the screen dimensions\n    screen.localScale = new Vector3(screenDims.x, screenDims.y, zScale);\n\n    // Adjust the screen position\n    screen.position = new Vector3(screenDims.x / 2, screenDims.y / 2, 1);\n}\nDefine method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nprivate void InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach (WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n}\nDefine method to initialize the in-scene camera object\n/// &lt;summary&gt;\n/// Resize and position the main camera based on an in-scene screen object\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of an in-scene screen object&lt;/param&gt;\nprivate void InitializeCamera(Vector2Int screenDims, string cameraName = \"Main Camera\")\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject camera = GameObject.Find(cameraName);\n    // Adjust the camera position to account for updates to the screenDims\n    camera.transform.position = new Vector3(screenDims.x / 2, screenDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    camera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the screenDims\n    camera.GetComponent&lt;Camera&gt;().orthographicSize = screenDims.y / 2;\n}\nDefine method to initialize a Barracuda inference interface\n/// &lt;summary&gt;\n/// Initialize an interface to execute the specified model using the specified backend\n/// &lt;/summary&gt;\n/// &lt;param name=\"model\"&gt;The target model representation&lt;/param&gt;\n/// &lt;param name=\"workerType\"&gt;The target compute backend&lt;/param&gt;\n/// &lt;param name=\"useNCHW\"&gt;EXPERIMENTAL: The channel order for the compute backend&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate IWorker InitializeWorker(Model model, WorkerFactory.Type workerType, bool useNCHW = true)\n{\n    // Validate the selected worker type\n    workerType = WorkerFactory.ValidateType(workerType);\n\n    // Set the channel order of the compute backend to channel-first\n    if (useNCHW) ComputeInfo.channelsOrder = ComputeInfo.ChannelsOrder.NCHW;\n\n    // Create a worker to execute the model using the selected backend\n    return WorkerFactory.CreateWorker(workerType, model);\n}\n\n\nDefine Start method\nThe Start method is called once before the first frame update, so we’ll perform any required setup steps here.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Get an object oriented representation of the model\n    Model m_RunTimeModel = ModelLoader.Load(modelAsset);\n    // Get the name of the target output layer\n    string outputLayer = m_RunTimeModel.outputs[outputLayerIndex];\n\n    // Create a model builder to modify the m_RunTimeModel\n    ModelBuilder modelBuilder = new ModelBuilder(m_RunTimeModel);\n\n    // Add a new Softmax layer\n    modelBuilder.Softmax(softmaxLayer, outputLayer);\n    // Add a new Argmax layer\n    modelBuilder.Reduce(Layer.Type.ArgMax, argmaxLayer, softmaxLayer);\n    // Initialize the interface for executing the model\n    engine = InitializeWorker(modelBuilder.model, workerType, useNCHW);\n\n    // Initialize the GPU output texture\n    outputTextureGPU = RenderTexture.GetTemporary(1, 1, 24, RenderTextureFormat.ARGBHalf);\n    // Initialize the CPU output texture\n    outputTextureCPU = new Texture2D(1, 1, TextureFormat.RGBAHalf, false);\n\n    // Initialize list of class labels from JSON file\n    classes = JsonUtility.FromJson&lt;ClassLabels&gt;(classLabels.text).classes;\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n}\n\n\nDefine Processing Methods\nNext, we need to define methods to process images using the Compute Shader, calculate the input resolution, handle asynchronous GPU readback, and process raw model output.\nDefine method to process images using a compute shader\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The target image RenderTexture&lt;/param&gt;\n/// &lt;param name=\"computeShader\"&gt;The target ComputerShader&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The target ComputeShader function&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, ComputeShader computeShader, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = computeShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    computeShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release the temporary RenderTexture\n    RenderTexture.ReleaseTemporary(result);\n}\nDefine method to calculate input resolution\n/// &lt;summary&gt;\n/// Scale the source image resolution to the target input dimensions\n/// while maintaing the source aspect ratio.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetDims\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate Vector2Int CalculateInputDims(Vector2Int imageDims, int targetDim)\n{\n    // Clamp the minimum dimension value to 64px\n    targetDim = Mathf.Max(targetDim, 64);\n\n    Vector2Int inputDims = new Vector2Int();\n\n    // Calculate the input dimensions using the target minimum dimension\n    if (imageDims.x &gt;= imageDims.y)\n    {\n        inputDims[0] = (int)(imageDims.x / ((float)imageDims.y / (float)targetDim));\n        inputDims[1] = targetDim;\n    }\n    else\n    {\n        inputDims[0] = targetDim;\n        inputDims[1] = (int)(imageDims.y / ((float)imageDims.x / (float)targetDim));\n    }\n\n    return inputDims;\n}\nDefine method to handle asynchronous GPU readback\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Make sure the Texture2D is not null\n    if (outputTextureCPU)\n    {\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        outputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Textur2D\n        outputTextureCPU.Apply();\n    }\n}\nDefine method to process raw model output\n/// &lt;summary&gt;\n/// Process the raw model output to get the predicted class index\n/// &lt;/summary&gt;\n/// &lt;param name=\"engine\"&gt;The interface for executing the model&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate int ProcessOutput(IWorker engine)\n{\n    int classIndex = -1;\n\n    // Get raw model output\n    Tensor output = engine.PeekOutput(argmaxLayer);\n\n    if (useAsyncGPUReadback)\n    {\n        // Copy model output to a RenderTexture\n        output.ToRenderTexture(outputTextureGPU);\n        // Asynchronously download model output from the GPU to the CPU\n        AsyncGPUReadback.Request(outputTextureGPU, 0, TextureFormat.RGBAHalf, OnCompleteReadback);\n        // Get the predicted class index\n        classIndex = (int)outputTextureCPU.GetPixel(0, 0).r;\n\n        // Check if index is valid\n        if (classIndex &lt; 0 || classIndex &gt;= classes.Length) Debug.Log(\"Output texture not initialized\");\n    }\n    else\n    {\n        // Get the predicted class index\n        classIndex = (int)output[0];\n    }\n\n    if (printDebugMessages) Debug.Log($\"Class Index: {classIndex}\");\n\n    // Dispose Tensor and associated memories.\n    output.Dispose();\n\n    return classIndex;\n}\n\n\nDefine Update method\nWe’ll place anything we want to run every frame in the Update method.\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputDims.x} x {inputDims.y}\");\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTexture = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGBHalf);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputTexture.width}x{inputTexture.height}\");\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTexture);\n\n    // Disable asynchronous GPU readback when not using a Compute Shader backend\n    useAsyncGPUReadback = engine.Summary().Contains(\"Unity.Barracuda.ComputeVarsWithSharedModel\") ? useAsyncGPUReadback : false;\n\n    if (SystemInfo.supportsComputeShaders)\n    {\n        // Normalize the input pixel data\n        ProcessImageGPU(inputTexture, processingShader, \"NormalizeImageNet\");\n\n        // Initialize a Tensor using the inputTexture\n        input = new Tensor(inputTexture, channels: 3);\n    }\n    else\n    {\n        // Define a temporary HDR RenderTexture\n        RenderTexture result = RenderTexture.GetTemporary(inputTexture.width,\n                                                          inputTexture.height, 24, RenderTextureFormat.ARGBHalf);\n        RenderTexture.active = result;\n\n        // Apply preprocessing steps\n        Graphics.Blit(inputTexture, result, processingMaterial);\n\n        // Initialize a Tensor using the inputTexture\n        input = new Tensor(result, channels: 3);\n        RenderTexture.ReleaseTemporary(result);\n    }\n\n    // Execute the model with the input Tensor\n    engine.Execute(input);\n    // Dispose Tensor and associated memories.\n    input.Dispose();\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTexture);\n    // Get the predicted class index\n    classIndex = ProcessOutput(engine);\n    // Check if index is valid\n    bool validIndex = classIndex &gt;= 0 && classIndex &lt; classes.Length;\n    if (printDebugMessages) Debug.Log(validIndex ? $\"Predicted Class: {classes[classIndex]}\" : \"Invalid index\");\n\n    // Unload assets when running in a web browser\n    if (Application.platform == RuntimePlatform.WebGLPlayer) Resources.UnloadUnusedAssets();\n}\n\n\nDefine GUI Methods\nWe need some methods to handle user interactions with the GUI and display the predicted class and current framerate.\nDefine method to update webcam usage from GUI\n/// &lt;summary&gt;\n/// This method is called when the value for the webcam toggle changes\n/// &lt;/summary&gt;\n/// &lt;param name=\"useWebcam\"&gt;&lt;/param&gt;\npublic void UpdateWebcamToggle(bool useWebcam)\n{\n    this.useWebcam = useWebcam;\n}\nDefine method to update webcam device from GUI\n/// &lt;summary&gt;\n/// The method is called when the selected value for the webcam dropdown changes\n/// &lt;/summary&gt;\npublic void UpdateWebcamDevice()\n{\n    currentWebcam = webcamDevices[webcamDropdown.value].name;\n    Debug.Log($\"Selected Webcam: {currentWebcam}\");\n    // Initialize webcam if it is not already playing\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n}\nDefine OnGUI method\nWe’ll display the predicted class and current frame rate in the OnGUI method.\n// OnGUI is called for rendering and handling GUI events.\npublic void OnGUI()\n{\n    // Define styling information for GUI elements\n    GUIStyle style = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * (1f / (100f - fontScale)))\n    };\n    style.normal.textColor = textColor;\n\n    // Define screen spaces for GUI elements\n    Rect slot1 = new Rect(10, 10, 500, 500);\n    Rect slot2 = new Rect(10, style.fontSize * 1.5f, 500, 500);\n\n    // Verify predicted class index is valid\n    bool validIndex = classIndex &gt;= 0 && classIndex &lt; classes.Length;\n    string content = $\"Predicted Class: {(validIndex ? classes[classIndex] : \"Invalid index\")}\";\n    if (displayPredictedClass) GUI.Label(slot1, new GUIContent(content), style);\n\n    // Update framerate value\n    if (Time.unscaledTime &gt; fpsTimer)\n    {\n        fps = (int)(1f / Time.unscaledDeltaTime);\n        fpsTimer = Time.unscaledTime + fpsRefreshRate;\n    }\n\n    // Adjust screen position when not showing predicted class\n    Rect fpsRect = displayPredictedClass ? slot2 : slot1;\n    if (displayFPS) GUI.Label(fpsRect, new GUIContent($\"FPS: {fps}\"), style);\n}\n\n\nDefine OnDisable Method\nWe’ll perform any clean-up steps in the OnDisablemethod.\n// OnDisable is called when the MonoBehavior becomes disabled\nprivate void OnDisable()\n{\n    // Release the resources allocated for the outputTextureGPU\n    RenderTexture.ReleaseTemporary(outputTextureGPU);\n\n    // Release the resources allocated for the inference engine\n    engine.Dispose();\n}"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#create-processing-shaders",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#create-processing-shaders",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Create Processing Shaders",
    "text": "Create Processing Shaders\nNow we need to create the Shaders for normalizing input images. We’ll store the shaders in a new Shaders folder.\n\nCreate Compute Shader\nRight-click a space in the Shaders folder and select Create → Shader → Compute Shader.\n\n\n\n\n\nName the Compute Shader ProcessingShader and open it in the code editor.\n\n\n\n\n\nDefault Compute Shader Code\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\nWe need to add a new Texture2D variable to store the pixel data for the input image. We’ll remove the default method and create a new one called NormalizeImageNet. We need to replace the default method name in the #pragma kernel line at the top.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel NormalizeImageNet\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Apply the ImageNet normalization stats from PyTorch to an image\n[numthreads(8, 8, 1)]\nvoid NormalizeImageNet(uint3 id : SV_DispatchThreadID)\n{\n    // Set the pixel color values for the processed image\n    Result[id.xy] = float4(\n        // Normalize the red color channel values\n        (InputImage[id.xy].r - 0.4850f) / 0.2290f,\n        // Normalize the green color channel values\n        (InputImage[id.xy].g - 0.4560f) / 0.2240f,\n        // Normalize the blue color channel values\n        (InputImage[id.xy].b - 0.4060f) / 0.2250f,\n        // Ignore the alpha/transparency channel\n        InputImage[id.xy].a);\n}\n\n\nCreate Image Effect Shader\nRight-click a space in the Shaders folder and select Create → Shader → Image Effect Shader.\n\n\n\n\n\nName the new shader NormalizeImageNet and open it in the code editor.\n\n\n\n\n\nDefault Image Effect Shader Code\nShader \"Hidden/NormalizeImageNet\"\n{\n    Properties\n    {\n        _MainTex (\"Texture\", 2D) = \"white\" {}\n    }\n    SubShader\n    {\n        // No culling or depth\n        Cull Off ZWrite Off ZTest Always\n\n        Pass\n        {\n            CGPROGRAM\n            #pragma vertex vert\n            #pragma fragment frag\n\n            #include \"UnityCG.cginc\"\n\n            struct appdata\n            {\n                float4 vertex : POSITION;\n                float2 uv : TEXCOORD0;\n            };\n\n            struct v2f\n            {\n                float2 uv : TEXCOORD0;\n                float4 vertex : SV_POSITION;\n            };\n\n            v2f vert (appdata v)\n            {\n                v2f o;\n                o.vertex = UnityObjectToClipPos(v.vertex);\n                o.uv = v.uv;\n                return o;\n            }\n\n            sampler2D _MainTex;\n\n            fixed4 frag (v2f i) : SV_Target\n            {\n                fixed4 col = tex2D(_MainTex, i.uv);\n                // just invert the colors\n                col.rgb = 1 - col.rgb;\n                return col;\n            }\n            ENDCG\n        }\n    }\n}\nThe string at the top of the file indicates the path to access the shader in the Unity Editor. We can replace the Hidden folder name with Processing Shaders to keep things more organized. We’ll replace the fixed4 frag method with the normalization steps.\nShader \"Processing Shaders/NormalizeImageNet\"\n{\n    Properties\n    {\n        _MainTex(\"Texture\", 2D) = \"white\" {}\n    }\n    SubShader\n    {\n        // No culling or depth\n        Cull Off ZWrite Off ZTest Always\n\n        Pass\n        {\n            CGPROGRAM\n            #pragma vertex vert\n            #pragma fragment frag\n\n            #include \"UnityCG.cginc\"\n\n            struct appdata\n            {\n                float4 vertex : POSITION;\n                float2 uv : TEXCOORD0;\n            };\n\n            struct v2f\n            {\n                float2 uv : TEXCOORD0;\n                float4 vertex : SV_POSITION;\n            };\n\n            v2f vert(appdata v)\n            {\n                v2f o;\n                o.vertex = UnityObjectToClipPos(v.vertex);\n                o.uv = v.uv;\n                return o;\n            }\n\n            sampler2D _MainTex;\n\n            // Set the pixel color values for the processed image\n            float4 frag(v2f i) : SV_Target\n            {\n                float4 col = tex2D(_MainTex, i.uv);\n                // Normalize the red color channel values\n                col.r = (col.r - 0.4850) / 0.2290;\n                // Normalize the green color channel values\n                col.g = (col.g - 0.4560) / 0.2240;\n                // Normalize the blue color channel values\n                col.b = (col.b - 0.4060) / 0.2250;\n                return col;\n            }\n            ENDCG\n        }\n    }\n}\nCreate Normalize ImageNet Material\nNext, we need to create a new material to use the NormalizeImageNet shader. Right-click a space in the Shaders folder and select Create → Material.\n\n\n\n\n\nWe can name it NormalizeImageNet as well.\n\n\n\n\n\nWith the new Material selected, open the Shader dropdown menu at the top of the Inspector tab. Type in the Material’s name and press enter."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Set up Unity Scene",
    "text": "Set up Unity Scene\nWe can, at last, start setting up our Unity scene. We need a screen to display the webcam feed, an empty object to attach the ImageClassifier script, a dropdown menu for selecting webcam devices, and a toggle to switch between the test image and a webcam feed.\nCreate Screen object\nRight-click a space in the Hierarchy tab and select 3D Object → Quad. We can name the new object Screen.\n\n\n\n\n\nNext, drag and drop a test image from the Assets → Images folder onto the Screen object in the Scene view. Note that the Screen looks a bit dim. We need to change the shader for the Screen’s Material so that it does not require an external light source.\n\n\n\n\n\nSelect the Screen in the Hierarchy tab and open the Shader dropdown menu in the Inspector tab. Type Unlit/Texture into the search box and press enter.\n\n\n\n\n\nCreate Inference Manager object\nRight-click a space in the Hierarchy tab and select Create Empty. Name the empty object InferenceManager.\n\n\n\n\n\nWith the InferenceManager object selected, drag the ImageClassifier script into the Inspector tab.\n\n\n\n\n\nNow we can assign the Screen, compute shader, Material, ONNX file, and class labels file in the Inspector tab by dragging them into their respective fields.\n\n\n\n\n\nAdd GUI prefab\nWe still need to create the GUI toggle and dropdown menu. To save time, I made a Prefab that we can drop into the Scene.\n\nGoogle Drive: Canvas Prefab\n\nUnity provides a free UI Samplepackage for anyone that wants to try creating a custom UI, and there are plenty of options on the Asset Store.\nDrag and drop the Canvas prefab into a new folder called Prefabs.\n\n\n\n\n\nFrom there, drag the prefab into the Hierarchy tab. We can see the GUI by switching to the Game view.\n\n\n\n\n\nConfigure Webcam Toggle On Value Changed function\nNext, we need to pair the WebcamToggle with the UpdateWebcamToggle function in the ImageClassifier script. Expand the Canvas object and select the WebcamToggle.\n\n\n\n\n\nClick and drag the InferenceManager into the On Value Changed field.\n\n\n\n\n\nOpen the No Function dropdown menu and select ImageClassifier → UpdateWebcamToggle.\n\n\n\n\n\nConfigure Webcam Dropdown On Value Changed function\nWe can follow the same steps to pair the WebcamDropdown with the UpdateWebcamDevice function in the ImageClassifier script.\n\n\n\n\n\nThis time select ImageClassifier → UpdateWebcamDevice.\n\n\n\n\n\nAssign GUI objects to Inference Manager\nWe can now assign the WebcamToggle and WebcamDropdown objects to their respective fields for the ImageClassifier script.\n\n\n\n\n\nAdd Event System\nBefore we can use the GUI, we need to add an Event System. Right-click a space in the Hierarchy tab and select UI → Event System."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#test-in-editor",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#test-in-editor",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Test in Editor",
    "text": "Test in Editor\nWe can finally test the project in the editor. Click the play button in the top-middle of the Editor window.\n\n\n\n\n\nThe predicted class should be Play, the dropdown menu should update with available webcam devices, and the Screen object should fill the preview window while maintaining the source aspect ratio. GPU utilization should hover near 100% when using asynchronous GPU readback."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-2/index.html#summary",
    "href": "posts/fastai-to-unity-tutorial/part-2/index.html#summary",
    "title": "Fastai to Unity Beginner Tutorial Pt. 2",
    "section": "Summary",
    "text": "Summary\nThis post covered implementing an image classification model in a Unity project using the Barracuda library. Part 3 will cover building the Unity project to run in a web browser and hosting it using GitHub Pages.\nPrevious: Fastai to Unity Tutorial Pt. 1\nNext: Fastai to Unity Tutorial Pt. 3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "",
    "text": "Overview\nCreate GitHub Pages Repository\nBuild WebGL Application\nTest Live Demo\nSummary"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html#overview",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html#overview",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "Overview",
    "text": "Overview\nPart 1 covered training an image classification model using the fastai library and exporting it to ONNX. Part 2 covered implementing a trained image classification model in a Unity project using the Barracuda library. In this post, we’ll build a Unity project as a shareable web demo and host it for free using GitHub Pages. The image classifier will execute locally in the user’s web browser.\nWebGL builds use Barracuda’s Pixel Shader backend, which is not nearly as performant as the Compute Shader backend. Therefore, I recommend using WebGL for sharing prototypes and target operating systems for final projects.\nPixel Shader Backend: ResNet18\n\n\n\n\n\nPixel Shader Backend: ResNet34\n\n\n\n\n\nCompute Shader Backend: ResNet18\n\n\n\n\n\nCompute Shader Backend: ResNet34\n\n\n\n\n\nCompute Shader Backend with asynchronous GPU readback: ResNet18\n\n\n\n\n\nCompute Shader Backend with asynchronous GPU readback: ResNet34"
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html#create-github-pages-repository",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html#create-github-pages-repository",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "Create GitHub Pages Repository",
    "text": "Create GitHub Pages Repository\nWe first need to create a new GitHub repository to store the WebGL build. We can do this on GitHub or locally using Git, GitHub Desktop, or another tool.\n\n\n\n\n\nOpen the Settings tab for the new repository on GitHub.\n\n\n\n\n\nOpen the Pages section and select the main branch as the source for GitHub Pages.\n\n\n\n\n\nClick the Save button to start the automated build process.\n\n\n\n\n\nGitHub will provide a URL for accessing the web demo once it finishes building.\n\n\n\n\n\nWe can check the GitHub Pages build progress under the Actions tab for the repository.\n\n\n\n\n\nThe web page will be accessible once the “pages build and deployment” workflow completes. Although, we don’t have any web pages at the moment."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html#build-webgl-application",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html#build-webgl-application",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "Build WebGL Application",
    "text": "Build WebGL Application\nIn the Unity project, select File → Build Settings... in the top menu bar to open the Build Settings window.\n\n\n\n\n\nSelect WebGL from the list of platforms and click Switch Platform.\n\n\n\n\n\nUnity enables compression by default for WebGL builds, which GitHub Pages does not support. We can disable compression in the Player Settings. Click the Player Settings button in the bottom-left corner of the Build Settings window.\n\n\n\n\n\nSelect Disabled from the Compression Format dropdown menu and close the Project Settings window.\n\n\n\n\n\nWe can test the WebGL build locally by clicking Build and Run in the Build Settings window.\n\n\n\n\n\nUnity will prompt us to select a folder to store the build files.\n\n\n\n\n\nNavigate to the local folder for the GitHub Pages repository and click Select Folder to start the build process.\n\n\n\n\n\nOnce the build completes, Unity will launch the demo in the default web browser. Unity caps the framerate to the platform’s default target framerate by default. On my Windows 10 desktop, that is 60fps.\n\n\n\n\n\nIf we examine the repository folder, we can see a new Build folder, a StreamingAssets folder, a TemplateData folder, and an index.html file.\n\n\n\n\n\nWe can push the local changes to GitHub, which will automatically trigger the “pages build and deployment” workflow."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html#test-live-demo",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html#test-live-demo",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "Test Live Demo",
    "text": "Test Live Demo\nWe can test the web demo at the URL provided by GitHub once the build workflow completes."
  },
  {
    "objectID": "posts/fastai-to-unity-tutorial/part-3/index.html#summary",
    "href": "posts/fastai-to-unity-tutorial/part-3/index.html#summary",
    "title": "Fastai to Unity Beginner Tutorial Pt. 3",
    "section": "Summary",
    "text": "Summary\nThis post covered how to build a Unity project as a shareable web demo and host it using GitHub Pages.\nPrevious: Fastai to Unity Tutorial Pt. 2\nFollow Up: How to Create a LibTorch Plugin for Unity on Windows Pt.1\nFollow Up: How to Create an OpenVINO Plugin for Unity on Windows Pt. 1\nIntermediate Tutorial: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html",
    "href": "posts/flip-image-compute-shader-tutorial/index.html",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "",
    "text": "Introduction\nCreate a 2D Unity Project\nCreate Compute Shader\nCreate Flip Script\nCreate Screen GameObject\nCreate ImageFlipper\nTest it Out\nConclusion"
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#introduction",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#introduction",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we’ll cover how to use a compute shader to flip an image across the x-axis, y-axis, and diagonal axis. We will also demonstrate how these operations can be combined to rotate an image."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#create-a-2d-unity-project",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#create-a-2d-unity-project",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Create a 2D Unity Project",
    "text": "Create a 2D Unity Project\nOpen the Unity Hub and create a new 2D project. I’m using Unity 2019.4.20f1, but you should be fine using other versions."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#create-compute-shader",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#create-compute-shader",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Create Compute Shader",
    "text": "Create Compute Shader\nIn Unity, right-click an empty space in the Assets folder and open the Create submenu. Select ComputeShader from the Shader submenu and name it FlipShader.\n\n\n\n\n\nOpen the new compute shader in your code editor. By default, compute shaders contain the following code.\n\n\n\n\n\nWe’ll delete the CSMain function and create a new one for each of our three flip operations.\n\nDefine Variables\nBefore we create our functions, we need to define some extra variables.\n\nTexture2D&lt;float4&gt; InputImage: stores the original image\nint height: the height of the input image\nint width: the width of the input image\nint2 coords: stores the new (x,y) coordinates for individual pixel values\n\n\n\n\n\n\n\n\nDefine Flip Functions\nThe individual flip operations quite simple. They determine the coordinates of the pixel that will replace the values for a given pixel in the image. The RGB pixel values at the calculated coordinates will be stored at the current coordinates in the Result variable.\n\nFlip x-axis: subtract the y value for the current pixel’s (x,y) coordinates from the height of the image\nFlip y-axis: subtract the x value for the current pixel’s (x,y) coordinates from the width of the image\nFlip diagonal: swap the x and y values for the current pixel’s (x,y) coordinates\n\nThese operations are performed on each pixel in parallel on the GPU. We’ll use the default numthreads(8, 8, 1) for each function."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#create-flip-script",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#create-flip-script",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Create Flip Script",
    "text": "Create Flip Script\nBack in Unity, right-click an empty space in the Assets folder and select C# Script in the Create submenu. Name the new script, Flip and open it in your code editor.\n\n\n\n\n\n\nDefine Variables\nWe’ll define the following variables at the top of the script.\n\npublic ComputeShader computeShader: The compute shader that contains the flip operations\npublic GameObject screen: The screen to which the test image is attached\npublic bool flipXAxis: Toggle whether to flip the image across the x-axis\npublic bool flipYAxis: Toggle whether to flip the image across the y-axis\npublic bool flipDiag: Toggle whether to flip the image across the diagonal axis\nprivate GameObject mainCamera: Stores a reference to the Main Camera object\nprivate RenderTexture image: A copy of the original test image\n\n\n\n\n\n\n\n\nDefine Start() Method\nIn the Start() method, we’ll store a copy the original test image in the image RenderTexture. We can do so by getting a reference to the Texture attached to the screen and using the Graphics.Blit() method. We’ll also get a reference to the camera so that we can adjust the view to fit the current image.\n\n\n\n\n\n\n\nDefine FlipImage() Method\nNext, we’ll define a new method called FlipImage to handle executing the compute shader. This method will take in the image to be flipped, an empty RenderTexture to store the flipped image, and the name of the function to execute on the compute shader.\nTo execute the compute shader, we need to first get the kernel index for the specified function and initialize the variables we defined in the compute shader. Once we execute the compute shader using the computeShader.Dispatch() method, we can copy the result to the empty RenderTexture we passed in. We could copy the result directly to the RenderTexture containing the original image. However, this would cause an error when flipping non-square images across the diagonal axis. This is because a RenderTexture can not dynamically change dimensions.\n\n\n\n\n\n\n\nDefine Update() Method\nFirst, we need to make another copy of the original image so that we can edit it. We’ll store this copy in a temporary RenderTexture called rTex that will get released at the end of the method.\nThe steps are basically the same for performing each of the three flip operations. We first allocate a temporary RenderTexture called tempTex to store the flipped image. We then call the FlipImage method with the appropriate function name. Next, we copy the flipped image to rTex. Finally, we release the resources allocated for tempTex. The steps for flipping the image across the diagonal axis is slightly different as we can’t directly copy a flipped image with different dimensions back to rTex. Instead, we have to directly assign the currently active RenderTexture to rTex.\nAfter we copy tempTex back to rTex we’ll update the Texture for the screen with the flipped image and adjust the shape of the screen to fit the new dimensions."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#create-screen-gameobject",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#create-screen-gameobject",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Create Screen GameObject",
    "text": "Create Screen GameObject\nBack in Unity, right-click an empty space in the Hierarchy tab and select Quad from the 3D Object submenu. Name the new object Screen. The size will be updated automatically by the Flip.cs script."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#create-imageflipper",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#create-imageflipper",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Create ImageFlipper",
    "text": "Create ImageFlipper\nRight-click an empty space in the Hierarchy tab and select Create Empty from the pop-up menu. Name the empty object ImageFlipper\n\n\n\n\n\nWith the ImageFlipper selected, drag and drop the Flip.cs script into the Inspector tab.\n\n\n\n\n\nDrag and drop the Screen object from the Hierarchy tab as well as the FlipShader from the Assets folder onto their respective spots in the Inspector tab."
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#test-it-out",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#test-it-out",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Test it Out",
    "text": "Test it Out\nWe’ll need a test image to try out the ImageFlipper. You can use your own or download the one I used for this tutorial.\n\nTest Image\n\nDrag and drop the test image into the Assets folder. Then drag it onto the Screen in the Scene.\n\n\n\n\n\nNext, we need to set our Screen to use an Unlit shader. Otherwise it will be a bit dim. With the Screen object selected, open the Shader drop-down menu in the Inspector tab and select Unlit.\n\n\n\n\n\nSelect Texture from the Unlit submenu.\n\n\n\n\n\nNow we can click the Play button and toggle the different flip checkboxes to confirm our script is working properly. If you check the performance stats, you should see that there is a negligible performance hit from flipping the image even when performing all three operations at once.\n\nDefault Image\n\n\n\n\n\n\n\nFlip X-Axis\n\n\n\n\n\n\n\nFlip Y-Axis\n\n\n\n\n\n\n\nFlip Diagonal Axis\n\n\n\n\n\n\n\nFlip X-Axis and Y-Axis\n\n\n\n\n\n\n\nFlip X-Axis and Diagonal Axis\n\n\n\n\n\n\n\nFlip Y-Axis and Diagonal Axis\n\n\n\n\n\n\n\nFlip X-Axis, Y-Axis and Diagonal Axis"
  },
  {
    "objectID": "posts/flip-image-compute-shader-tutorial/index.html#conclusion",
    "href": "posts/flip-image-compute-shader-tutorial/index.html#conclusion",
    "title": "How to Flip an Image With a Compute Shader",
    "section": "Conclusion",
    "text": "Conclusion\nThat is one approach to efficiently flip images on the GPU in Unity. As demonstrated above, the operations can be combined in different ways to rotate the image as well.\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/git-branches-notes/index.html",
    "href": "posts/git-branches-notes/index.html",
    "title": "Notes on Git Branches",
    "section": "",
    "text": "Overview\nThe HEAD Branch\nLocal and Remote Branches\nLocal and Remote Branches\nCreating New Branches\nList branches\nSwitching Branches\nRenaming Branches\nPublish Branch\nTracking Branches\nPulling and Pushing Branches\nDeleting Branches\nMerging Branches\nRebasing Branches\nComparing Branches"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#overview",
    "href": "posts/git-branches-notes/index.html#overview",
    "title": "Notes on Git Branches",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Tobias Gunther’s video covering git branches."
  },
  {
    "objectID": "posts/git-branches-notes/index.html#the-head-branch",
    "href": "posts/git-branches-notes/index.html#the-head-branch",
    "title": "Notes on Git Branches",
    "section": "The HEAD Branch",
    "text": "The HEAD Branch\n\nThe currently “active” or “checked out” branch\nonly one can be active at a time"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#local-and-remote-branches",
    "href": "posts/git-branches-notes/index.html#local-and-remote-branches",
    "title": "Notes on Git Branches",
    "section": "Local and Remote Branches",
    "text": "Local and Remote Branches\n\n99% of the time, “working” with branches means your local branches\nremote branches are more for synchronizing\n\nGitHub\nGit Lab\nBitBucket\nAzure DevOps"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#creating-new-branches",
    "href": "posts/git-branches-notes/index.html#creating-new-branches",
    "title": "Notes on Git Branches",
    "section": "Creating New Branches",
    "text": "Creating New Branches\n\nYou can only create new branches in your local repository\nCreate branches in a remote repository by publishing the branch in the local repository\nBased on your current HEAD branch\n\ngit branch &lt;new-branch-name&gt;\n\nBased on a different revision\n\ngit branch &lt;new-branch-name&gt; &lt;revision-hash&gt;"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#list-branches",
    "href": "posts/git-branches-notes/index.html#list-branches",
    "title": "Notes on Git Branches",
    "section": "List Branches",
    "text": "List Branches\n\ngit branch"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#switching-branches",
    "href": "posts/git-branches-notes/index.html#switching-branches",
    "title": "Notes on Git Branches",
    "section": "Switching Branches",
    "text": "Switching Branches\n\nCurrent branch defines where new commits will be created\nOlder\n\ngit checkout &lt;branch-name&gt;\nLots of different uses\n\nNewer\n\ngit switch &lt;branch-name&gt;\nSpecifically for switching branches"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#renaming-branches",
    "href": "posts/git-branches-notes/index.html#renaming-branches",
    "title": "Notes on Git Branches",
    "section": "Renaming Branches",
    "text": "Renaming Branches\n\nRename local head branch\n\ngit branch -m &lt;new-name&gt;\n\nRename different branch\n\ngit branch -m &lt;target-branch-name&gt; &lt;new-branch-name&gt;\n\nRename remote branch\n\nDelete target branch\n\ngit push origin --delete &lt;old-name&gt;\n\nPublish new branch with desired name\n\ngit push -u origin &lt;new-name&gt;"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#publish-branch",
    "href": "posts/git-branches-notes/index.html#publish-branch",
    "title": "Notes on Git Branches",
    "section": "Publish Branch",
    "text": "Publish Branch\n\nUpload a local branch for the first time\n\ngit push -u origin &lt;local-branch&gt;\n\n-u flag: Tells git to establish a tracking connection\n\nmakes pushing and pulling easier"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#tracking-branches",
    "href": "posts/git-branches-notes/index.html#tracking-branches",
    "title": "Notes on Git Branches",
    "section": "Tracking Branches",
    "text": "Tracking Branches\n\nConnecting branches with each other\nBy default, local and remote branches have nothing to do with each other\nGet remote branch to local branch\n\ngit branch --track &lt;local-branch-name&gt; &lt;target-remote-branch&gt;\n\nOr:\n\ngit checkout --track &lt;target-remote-branch&gt;\nUses remote branch name as local branch name"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#pulling-and-pushing-branches",
    "href": "posts/git-branches-notes/index.html#pulling-and-pushing-branches",
    "title": "Notes on Git Branches",
    "section": "Pulling and Pushing Branches",
    "text": "Pulling and Pushing Branches\n\nSynchronizing local and remote branches\nMuch easier when tracking is already enabled\n\ngit pull\ngit push\n\nGit tells you if your local branch and tracked remote branch diverge\n\ngit branch -v"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#deleting-branches",
    "href": "posts/git-branches-notes/index.html#deleting-branches",
    "title": "Notes on Git Branches",
    "section": "Deleting Branches",
    "text": "Deleting Branches\n\nCannot delete current head branch\n\nSwitch to other branch first\n\nDeleting a branch in your local repository\n\ngit branch -d &lt;branch-name&gt;\nMight cause errors if you delete a branch with commits that do not exist elsewhere\n\n-f flag: force deletion\nBe careful with this option\n\n\nDeleting a remote branch\n\ngit push origin --delete &lt;remote-branch-name&gt;\n\nWhen deleting a branch, keep in mind whether you need to delete its remote/local counterpart as well"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#merging-branches",
    "href": "posts/git-branches-notes/index.html#merging-branches",
    "title": "Notes on Git Branches",
    "section": "Merging Branches",
    "text": "Merging Branches\n\nIntegrating changes from another branch into your current local HEAD branch\nMerging often produces a merge commit\nSwitch to the branch that should receive changes\n\ngit switch &lt;branch-to-change&gt;\n\nMerge the branch with desired changes into current branch\n\ngit merge &lt;branch-with-changes&gt;"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#rebasing-branches",
    "href": "posts/git-branches-notes/index.html#rebasing-branches",
    "title": "Notes on Git Branches",
    "section": "Rebasing Branches",
    "text": "Rebasing Branches\n\nAn alternative way to integrate changes from another branch into your current local HEAD branch\n\nNot really better or worse than merge, just different\nThere is no separate merge commit\nIt appears as if development history happened in a straight line\n\nSwitch to the branch that should receive changes\n\ngit switch &lt;branch-to-change&gt;\n\nRebase the branch with desired changes into current branch\n\ngit rebase &lt;branch with changes&gt;"
  },
  {
    "objectID": "posts/git-branches-notes/index.html#comparing-branches",
    "href": "posts/git-branches-notes/index.html#comparing-branches",
    "title": "Notes on Git Branches",
    "section": "Comparing Branches",
    "text": "Comparing Branches\n\nChecking which commits are in branch-B, but not in branch-A\nBetween two Local branches\n\ngit log &lt;branch-A&gt;..&lt;branch-B&gt;\n\nBetween a Local and Remote branch\n\ngit log &lt;remote-branch&gt;..&lt;local-branch&gt;\n\n\nReferences:\n\nGit Branches Tutorial"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html",
    "href": "posts/google-colab-getting-started-tutorial/index.html",
    "title": "Getting Started with Google Colab",
    "section": "",
    "text": "Introduction\nAccess Google Colab\nThe Notebook Selection Window\nUnderstanding the Notebook Interface\nWorking with Data\nUsing Hardware Acceleration\nCreate a New Notebook\nSave Your Notebook\nConclusion"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#introduction",
    "href": "posts/google-colab-getting-started-tutorial/index.html#introduction",
    "title": "Getting Started with Google Colab",
    "section": "Introduction",
    "text": "Introduction\nIn this tutorial, I’ll introduce you to Google Colab, its features, and how to use it to run your code. Google Colab provides a free, cloud-based Jupyter Notebook environment that allows you to write, run, and share Python code in your browser without any setup or installation. A Jupyter Notebook is an interactive web-based tool for creating and sharing documents that contain live code, visualizations, and narrative text, often used in data analysis, visualization, and education."
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#access-google-colab",
    "href": "posts/google-colab-getting-started-tutorial/index.html#access-google-colab",
    "title": "Getting Started with Google Colab",
    "section": "Access Google Colab",
    "text": "Access Google Colab\nTo access Google Colab, follow these steps:\n\nGo to colab.research.google.com.\nSign in with your Google account. If you don’t have a Google account, create one here.\n\n\n\n\ngoogle-colab-welcome-notebook-signed-out"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#the-notebook-selection-window",
    "href": "posts/google-colab-getting-started-tutorial/index.html#the-notebook-selection-window",
    "title": "Getting Started with Google Colab",
    "section": "The Notebook Selection Window",
    "text": "The Notebook Selection Window\nAfter signing in, the Notebook Selection window will pop up. This window displays a list of your recent notebooks, allowing you to access and open them. If this is your first time using Google Colab, you will only see the “Welcome to Colaboratory” notebook listed. The Notebook Selection window also allows you to import Jupyter Notebooks from Google Drive and GitHub or upload a notebook from your computer.\n\n\n\ngoogle-colab-welcome-page\n\n\nThe “Welcome to Colaboratory” notebook is already open behind the Notebook Selection window, so we’ll work with that one. Click the cancel button in the bottom right corner of the popup window to view the welcome notebook.\n\n\n\ngoogle-colab-welcome-page-exit-popup"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#understanding-the-notebook-interface",
    "href": "posts/google-colab-getting-started-tutorial/index.html#understanding-the-notebook-interface",
    "title": "Getting Started with Google Colab",
    "section": "Understanding the Notebook Interface",
    "text": "Understanding the Notebook Interface\nA notebook consists of a list of cells. Google Colab notebooks have two main types of cells: code cells and text cells. Code cells allow you to write and run Python code, while text cells let you add formatted text, images, and equations using Markdown. The first few cells in the welcome notebook are text cells.\n\n\n\ngoogle-colab-welcome-notebook-top\n\n\n\nText Cells\nYou can view and edit the Markdown source of a text cell by double-clicking it. In editor mode, Google Colab will show the Markdown source and the rendered version side-by-side.\n\n\n\ngoogle-colab-welcome-notebook-inspect-markdown-source\n\n\nWe can edit the Markdown source, and the rendered version will update in real time.\n\n\n\ngoogle-colab-welcome-notebook-edit-markdown-source\n\n\nYou can exit the editor mode by pressing Shift+Enter, clicking the Close Markdown Editor icon in the top-right corner of the text cell, or clicking another cell.\n\n\n\ngoogle-colab-welcome-notebook-exit-markdown-editor\n\n\nTo create a new text cell, click the + Text button in the toolbar.\n\n\n\ngoogle-colab-welcome-notebook-add-text-cell\n\n\nGoogle Colab will add the new Markdown cell below the currently selected cell.\n\n\n\ngoogle-colab-welcome-notebook-new-text-cell\n\n\n\n\nCode Cells\nTo create a new code cell, click the + Code button in the toolbar at the top of the notebook or press Ctrl+M B.\n\n\n\ngoogle-colab-welcome-notebook-new-code-cell\n\n\nGoogle Colab will add the new code cell below the currently selected cell.\n\n\n\ngoogle-colab-welcome-notebook-new-code-cell\n\n\nYou can write Python code in the code cell and execute it by pressing Shift + Enter or clicking the Play button on the left side of the cell. Any output from the code will appear directly below the code cell.\n\n\n\ngoogle-colab-welcome-notebook-run-code-cell\n\n\nWe can also use code cells to access the command line by adding an exclamation point at the start of the cell. We can use this ability to install Python packages via the pip package installer.\n\n\n\ngoogle-colab-welcome-notebook-access-command-line"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#working-with-data",
    "href": "posts/google-colab-getting-started-tutorial/index.html#working-with-data",
    "title": "Getting Started with Google Colab",
    "section": "Working with Data",
    "text": "Working with Data\nGoogle Colab allows you to upload and download files to and from your computer and connect notebooks to your Google Drive.\n\nUploading Files\nYou can upload files from your local machine to use in your Google Colab notebook by following these steps:\n\nClick the Files icon in the left sidebar to open the file browser.\n\nClick the Upload button.\n\n\n\n\ngoogle-colab-upload-button\n\n\n\nGo to the file location on your local machine, select it, and click Open to upload it to your Google Colab workspace.\n\nColab will display a warning that the runtime’s files get deleted when it terminates. Click OK in the bottom-right corner of the popup window.\n\nThe uploaded file will appear in the file browser, and you can access it in your notebook.\n\nWe can view the file by double-clicking it in the file browser or loading it in the notebook via Python.\n\n\n\n\nDownloading Files\nTo download a file from your Google Colab workspace to your local machine, follow these steps:\n\nLocate the file in the file browser.\nRight-click the file and select Download.\n\n\n\n\ngoogle-colab-download-file\n\n\nThe file will download to your local machine.\n\n\nConnecting to Google Drive\nGoogle Colab notebooks can connect to Google Drive to access, store, and manage your files. To do this, follow these steps:\n\nClick the Mount Drive button in the file browser.\n\nGoogle Colab will create a new code cell containing the following code:\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nRun the code cell by pressing Shift + Enter or clicking the Play button on the left side of the cell. A popup window will appear, prompting you to authorize access to your Google Drive.\n\nClick the Connect to Google Drive button to open the authorization page.\n\nSign in with your Google account, and click Allow to grant access.\n\nReturn to your Google Colab notebook. The code cell should have printed a message indicating your Google Drive is now mounted.\n\nClick the Refresh button in the file browser to update the contents.\n\n\n\n\ngoogle-colab-refresh-file-browser\n\n\nYour Google Drive should now be mounted and accessible from the file browser.\n\n\n\ngoogle-colab-verify-driver-accessible-in-file-browser\n\n\nYou can read, write, and manage your Google Drive files directly from your Google Colab notebook. To access the files, use the path /content/drive/MyDrive/ followed by the file or folder name."
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#using-hardware-acceleration",
    "href": "posts/google-colab-getting-started-tutorial/index.html#using-hardware-acceleration",
    "title": "Getting Started with Google Colab",
    "section": "Using Hardware Acceleration",
    "text": "Using Hardware Acceleration\nGoogle Colab offers free access to GPUs and TPUs to accelerate your code. To enable GPU or TPU acceleration:\n\nClick the “Runtime” menu at the top of the notebook.\n\nSelect “Change runtime type.”\n\nChoose “GPU” from the “Hardware accelerator” drop-down menu.\n\nClick “Save.”\n\nChanging the hardware accelerator requires loading a new runtime. Loading a new runtime will delete any files we added and disconnect Google Drive. Google Colab will show a popup window asking you to confirm you want to delete the current runtime. Click “OK” to confirm.\n\nVerify the notebook has GPU access by running the following code in a code cell:\n!nvidia-smi\n\n\nYour notebook will now use the selected hardware accelerator. Note that free GPU and TPU usage is time-limited. You can run notebooks on the free tier for at most 12 hours at a time (usually less). If you exceed the time allotment, you must wait until it resets (typically about 12 hours). Therefore, only enable hardware acceleration when needed and disable it when you don’t. To disable hardware acceleration, select None from the Hardware Accelerator drop-down menu.\n\n\n\ngoogle-colab-disable-hardware-acceleration"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#create-a-new-notebook",
    "href": "posts/google-colab-getting-started-tutorial/index.html#create-a-new-notebook",
    "title": "Getting Started with Google Colab",
    "section": "Create a New Notebook",
    "text": "Create a New Notebook\nTo create a new notebook:\n\nOpen the File menu in the top-left corner and select New notebook.\n\n\n\n\ngoogle-colab-create-new-notebook\n\n\nA new notebook will open in a separate tab. The runtime for the previous notebook is still active.\n\n\n\ngoogle-colab-new-notebook\n\n\nYou can rename the notebook by clicking the notebook name at the top of the page. For now, we can name it “My First Notebook.”\n\n\n\ngoogle-colab-rename-notebook"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#save-your-notebook",
    "href": "posts/google-colab-getting-started-tutorial/index.html#save-your-notebook",
    "title": "Getting Started with Google Colab",
    "section": "Save Your Notebook",
    "text": "Save Your Notebook\nGoogle Colab automatically saves your notebooks to a “Colab Notebooks” folder in Google Drive.\n\n\n\ngoogle-drive-colab-notebooks-folder\n\n\nNote the “Welcome to Colaboratory” notebook is not in the folder. Since we did not create that notebook, we must save our copy manually. Switch to that notebook’s tab and click the “Copy to Drive” button.\n\n\n\ngoogle-colab-save-copy-to-drive\n\n\nGoogle Colab will open our new copy of the notebook in a separate tab.\n\n\n\ngoogle-colab-copy-of-welcome-notebook\n\n\nIf we check the “Colab Notebooks” folder in Google Drive, we should now see our copy of the welcome notebook.\n\n\n\ngoogle-drive-verify-copy-of-welcome-notebook"
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#sharing-notebooks",
    "href": "posts/google-colab-getting-started-tutorial/index.html#sharing-notebooks",
    "title": "Getting Started with Google Colab",
    "section": "Sharing Notebooks",
    "text": "Sharing Notebooks\nYou can share your Google Colab notebook with others, similar to other Google Drive documents. To share your notebook:\n\nClick the “Share” button in the top-right corner of the notebook.\n\nIn the sharing settings dialog box, enter the email address of the person you want to share the notebook with.\n\nSelect their permission level: “Viewer,” “Commenter,” or “Editor.”\n\nAlternatively, you can create a shareable link with specific access settings (view, comment, or edit). You can share this link with others, and they can access the notebook according to the chosen permission level.\n\nOpen the drop-down menu under General Access and select the “Anyone with a link” option.\n\nSet the permission level: “Viewer,” “Commenter,” or “Editor.”\n\nClick “Copy link” to copy the shareable link."
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#version-control-with-github",
    "href": "posts/google-colab-getting-started-tutorial/index.html#version-control-with-github",
    "title": "Getting Started with Google Colab",
    "section": "Version Control with GitHub",
    "text": "Version Control with GitHub\nGoogle Colab can save and load notebooks from GitHub repositories, enabling seamless collaboration and tracking of changes in your code.\n\nSaving a notebook to a GitHub repository:\n\nOpen the File menu in the top-left corner and select Save a copy in GitHub.\n\nIf you haven’t connected your GitHub account yet, follow the prompts to authorize Google Colab to access your repositories.\n\nChoose a repository, branch, and file path for your notebook. You can also update the commit message.\n\nClick OK to save the notebook to the specified GitHub repository.\n\n\n\n\nLoading a notebook from a GitHub repository:\n\nGo to the Google Colab website.\n{fig-align=“center”}\nClick the GitHub tab in the Notebook Selection window.\n\nEnter the URL for the GitHub repository containing the notebook you want to open. You can also search for one by entering a username or organization and repository name.\n\nSelect the notebook you want to open, and it will open in a new tab."
  },
  {
    "objectID": "posts/google-colab-getting-started-tutorial/index.html#conclusion",
    "href": "posts/google-colab-getting-started-tutorial/index.html#conclusion",
    "title": "Getting Started with Google Colab",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve now learned the fundamentals of Google Colab. This tutorial covered creating and editing cells, working with data, hardware acceleration, and saving and sharing notebooks via Google Drive and GitHub. Keep exploring Google Colab to uncover more features that can enhance your projects."
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html",
    "href": "posts/how-to-speak-notes/index.html",
    "title": "Notes on How To Speak",
    "section": "",
    "text": "Overview\nIntroduction\nHow To Start\nFour Sample Heuristics\nBuild Up Your Personal Repertoire and Style\nSpecial Cases\nHow to Stop"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#overview",
    "href": "posts/how-to-speak-notes/index.html#overview",
    "title": "Notes on How To Speak",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Patrick Winston’s course on how to speak."
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#introduction",
    "href": "posts/how-to-speak-notes/index.html#introduction",
    "title": "Notes on How To Speak",
    "section": "Introduction",
    "text": "Introduction\n\nThe Uniform Code of Military Justice specifies court martial for any officer who sends a soldier into battle without a weapon\nStudents should not go out into life without the ability to communicate.\nYour success in life will largely be determined by your ability to speak, your ability to write, and the quality of your ideas\n\nYour ability to speak is the most important\n\nQuality of communication is a function of your knowledge, how much you practice with that knowledge, and your inherent talent\n\nKnowledge is the most important factor\nInherent talent is the least important factor\n\nYou can get a lot better than people who have inherent talents when you have the right amount of knowledge"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#how-to-start",
    "href": "posts/how-to-speak-notes/index.html#how-to-start",
    "title": "Notes on How To Speak",
    "section": "How To Start",
    "text": "How To Start\n\nDo not start with a joke\n\nPeople are not ready for a joke at the beginning\n\nStart with empowerment promise\n\nThe reason for being here\nExample: Tell people what they are going to know by the end of the conversation that they did not know at the start"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#four-sample-heuristics",
    "href": "posts/how-to-speak-notes/index.html#four-sample-heuristics",
    "title": "Notes on How To Speak",
    "section": "Four Sample Heuristics",
    "text": "Four Sample Heuristics\n\nCycle on the subject\n\nGo around it again and again\nTell them what you are going to tell them, tell them, tell them again, and again (3 times total)\nHelps increase the probability that your audience will absorb what you are trying to communicate\n\nBuild a fence around your idea so that it is not confused with someone else’s idea\n\nExplain how your idea is different\n\nVerbal Punctuation\n\nProvide landmarks where you announce to people who lose focus that it is a good time to start paying attention again\nEnumerate through what you have covered\nProvide numbers\nGive a sense that there is a seam in the talk and they can get back on\n\nAsk a Question\n\nYou can wait for about seven seconds for an answer\nThe question has to be carefully chosen\n\nCan’t be too obvious\nCan’t be too hard"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#build-up-your-personal-repertoire-and-style",
    "href": "posts/how-to-speak-notes/index.html#build-up-your-personal-repertoire-and-style",
    "title": "Notes on How To Speak",
    "section": "Build Up Your Personal Repertoire and Style",
    "text": "Build Up Your Personal Repertoire and Style\nThe Tools\n\nTime and Place\n\nBest time to have a lecture is often 11AM\n\nMost people are awake and have not gone back to sleep\nIt’s not right after a meal\nPeople are not fatigued\n\nThe place needs to be well lit\n\nLow light tends to signal that it is time to go to sleep\n\nThe place should be “cased”\n\nGo there before the talk and see what it is like\nMake sure there are no surprises\nImagine the seats are filled with disinterested farm animals\n\nThe place should be reasonably populated\n\nGet a an appropriately sized place for the expected audience size\nMore than half full\n\n\nBoards, Props, and Slides\n\nChalk is a good tool when your purpose is informing\n\nYou can exploit the fact that you can use graphics in your presentation\nSpeed with which you write on the board is approximately the speed at which people can absorb ideas\nGives you something that you can do with your hands as either something to write on or point at.\n\nSlides are good when your purpose is exposition\n\nExample: job talks and conference talks\nUse fewer slides and fewer words\nDon’t read the words on your slides\nBe in view when the audience is looking at the slides\n\nDon’t force your audience to constantly shift their view from the slide to the speaker\n\nSlides should be condiments to what you are saying\nKeep images simple\nEliminate Clutter\n\nremove logos\nremove titles: tell them the title\nremove the bullet points from lists\n\nReducing what what is on the slide allows the audience to pay more attention to what you say\nUse a sample slide to determine the minimum font size that is easily ledgible\n\nProbably font size of 40 - 50\n\nDon’t use laser pointers or pointing sticks\n\nWhen you are using these, you are not making eye contact with the audience\n\nUse onscreen arrows to point to things on a slide\nPrint your presentation out and lay it out on a table\n\nMakes it easy to see if there is too much in it\n\nWhen you need to have text, give your audience time to read it\nYou can have at most one visually complex slide in a presentation\n\nExample: to make a point of how incomprehensibly complex something is\n\n\nProps\n\ngive the audience an idea of where the talk is going\nhelps to view a problem in a different way\nhelps emphasize a point\nhelps make the talk memorable\n\nChalk and props can help with empathetic mirroring\n\nYour might feel like they are writing on the chalkboard\nCan’t do that with slides"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#special-cases",
    "href": "posts/how-to-speak-notes/index.html#special-cases",
    "title": "Notes on How To Speak",
    "section": "Special Cases",
    "text": "Special Cases\n\nInforming\n\nStart with a promise\n\nExpress how cool the topic is\n\nInspiration\n\nTell beginners they can do it\nHelp the experienced see a problem in a new way\nExhibit passion about what you are doing\n\nTeaching people how to think\n\nProvide audiences with the stories they need to know\nProvide audiences with the questions they need to ask about those stories\nProvide mechanisms for analyzing those stories\nProvide ways of putting stories together\nProvide ways of evaluating how reliable a story is\n\n\nPersuading\n\nOral Exams\n\nThe most common reason for people failing an oral exam is a failure to situate and a failure to practice\nsituate\n\nIt is important to talk about your research in context\n\nExample: This is a problem that is being pursued all over the world\nExample: There has not been any progress on this before me in the past 30 years\nEveryone is looking for a solution because it will have impact on so many other things\n\n\npractice\n\npractice does not mean sharing your slides with people you share an office with\n\nIf people know what you are doing, they will hallucinate that there is explanatory material in your presentation that is not there\n\na faculty supervisor is not a good person to help you debug a talk\nyou need to practice presenting with friends who do no know what you are doing\n\nStart your practice session by saying if they can’t make you cry, you won’t value them as a friend anymore\n\nIt is better to have an examining committee that is much older\n\n\nJob Talks\n\nShow them that you have some kind of vision\n\na vision is in part a problem that somebody cares about and something new in your approach\n\nShow them that you have done something\n\nList the steps that need to be taken in order to achieve the solution to a problem (preferably the problem is the one expressed in the vision)\nEnumerate your contributions\n\nYou have five minutes to express your vision and tell them what you have done\n\nGetting Famous (How to ensure your work is recognized)\n\nWhy would you want to be famous?\n\nYou get used to being famous, you never get used to being ignored\nYour ideas are like your children. You don’t want them to go into the world in rags\n\nHow to get remembered\n\nHave a symbol associated with your work\nHave a (simple) slogan that provides a handle on the work\nHave a surprising attribute in your work\nHave a salient idea (an idea that sticks out)\nHave a story that tells how you did it, how it works, and why it’s important"
  },
  {
    "objectID": "posts/how-to-speak-notes/index.html#how-to-stop",
    "href": "posts/how-to-speak-notes/index.html#how-to-stop",
    "title": "Notes on How To Speak",
    "section": "How to Stop",
    "text": "How to Stop\n\nWhat is the final slide?\n\nRecognize collaborators on the first slide, not the last\nDo not end with a slide saying “Questions?”\n\nThis is a wast of real estate as the last slide can be up there for 20 minutes\n\nDo not end with a URL address\nDo not end with a slide saying “THE END” or “Thank You”\nThe last slide is an opportunity to leave people with who you are\nThe last slide should end with your contributions\n\nWhat you talked about\nWhat you demonstrated\nWhat the audience got out of it\n\n\nWhat are the final words?\n\nCan finish with a joke\n\nThe audience is ready for it by then\n\nDon’t end with “Thank You”\n\nIt’s a weak move\n\nCould say “It’s been great to be here and look forward to coming back”\n\n\nReferences:\n\nMIT OpenCourseWare Video: How To Speak by Patrick Winston"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "",
    "text": "Overview\nHow to Make This Course Work\nEmail Two Weeks Before the Semester\nGame Design Principles and Prompts\nGiving Feedback\nAccountability\nIndustry Engagement"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#overview",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#overview",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Jason Wiser’s talk how he successfully teaches class that covers 5 semesters of game design in a single semester.\nCourse website\nGame Design - Tufts"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#how-to-make-this-course-work",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#how-to-make-this-course-work",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "How to Make This Course Work",
    "text": "How to Make This Course Work\n\nTeaching Assistants offering coding labs\nFlexible Classroom\nAll lessons built on Team Production:\n\nMotivating, inspiring, and no one feels like the burden is all on them (as long as the team are well supported).\n\nRobust Course Website\n\nA website with all schedules, materials and lecture notes and tutorials covered in class, and further opportunities for further learning\nAllows students to see the full scope of the course\n\nStacking Theory\n\nThe class does not cover content in a purely linear fashion\nStudents are taught one critical concept a week and exposed to other concepts before they are needed.\n\nAllows them to get curious\n\nClass meets once a week for three hours\n\nDiscuss and practice the one critical topic for that week’s homework\nIntroduce two or three other ideas will be important later\n\nExample: A month before anyone will need to make anything in 3D for homework, they start learning basic modelling and texture\n\n\n\nSummary\n\nClass meets 1/week, 3hours/class divided into 3-5 lesson-units\nWeeks 1-6: Students work in 2-week teams to make tabletop games, then switch. Lessons in Design, Teamwork, Engine, and Art tools.\nWeek 7: Students Pitch Final Game Concepts, and form Final Teams\nWeeks 8-14: Students Develop Final Games. Lessons in Pipeline Dev, Project Management, Level Design, UI, Audio, Tutorials, Marketing\nWeek 15: Final Presentations Party"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#email-two-weeks-before-the-semester",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#email-two-weeks-before-the-semester",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Email Two Weeks Before the Semester",
    "text": "Email Two Weeks Before the Semester\n\nWelcome to the course\nWhat games do you love?\nWhat games do you want to make?\nDownload Unity and Maya\nGet familiar with tabletop games"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#game-design-principles-and-prompts",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#game-design-principles-and-prompts",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Game Design Principles and Prompts",
    "text": "Game Design Principles and Prompts\n\nMechanics vs Story\n\nfocus on mechanics\nform team and design a table top game inspired by weird boards and toys\n\nPlaytesting and Radical Revision\n\nfind the fun parts of the game\ndiscuss radical revision to refocus around that fun\n\nDisruption of Existing Games\n\nform a second two-week team\ndisrupt an existing game into something new\nUnity 3D 1 hour\n\nAlternate Reality Games to solve misery\n\nRead Reality is Broken\nBrainstorm and present an alternate reality game to solve a misery\nUnity 2D 1 hour\nHomework: Unity Roll-a-ball\n\nWorkplace Routine Games\n\nForm third team to make a final tabletop project based on the routines of a workplace\nIntro Maya to Unity pipeline 1 hour\n\n\nIntro VR, Unity Builds\nMake the workplace game digital (zero expectation of success)\n\nFinal Games Pitch\n\nBrainstorming and silent reflection\nEveryone chooses a colored sticky-note for their production role, adds their name\n45 seconds to pitch idea\nEveryone votes on the game they want to make\nTeacher rebalances roles\n\nLast 8 Weeks: Final Digital Games\n\n2 weeks of predevelopment\n\npaper prototypes, design docs, tools research, and pipeline development\n\n3 weeks to get a full functioning prototype\n3 weeks to get a revised prototype built-out to multiple levels and more polished art and audio\nPhotoshop for 2D assets\nAnimation and VFX in Unity\nIntro to Audio Design\nIntro to UI design\n3 weeks on Level Design\n2 weeks on Marketing\nWeekly in-and-out-of-class playtesting"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#giving-feedback",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#giving-feedback",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Giving Feedback",
    "text": "Giving Feedback\n\nClarity: Do players know what to do?\n\nA game that is intentionally obscure is taken and discussed on its intentions and merits\n\nInnovation: What new gameplay to stimulate interest?\nImmersion: Is the “story” compelling (implied in setting, art, music)?\nFlow: Does the player feel constantly productive?\n\nA game that is designed to inhibit flow is taken and discussed on its intentions and merits\n\nFiero: Are there multiple big victory moments for players?\nGrade is as much on collaboration, experimentation, risk taking, and design, as it is about the games being successful as games"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#accountability",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#accountability",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Accountability",
    "text": "Accountability\n\nWeekly playtesting and responses to rubric.\n\nverbal and typed feedback\n\nPosting weekly task divisions\n\nshow an equal share of production per member\n\nPosting weekly personal progress report\n\none paragraph with screenshots\nWhat they agreed to take on that week\nWhat they actually completed\nWho helped them\nWho they helped\nLinks to tutorials they used\n\nPeer evaluation form 3 times in the semester\n\nDiscuss teammate contributions to team productivity and moral\nDivide a pool of point unevenly between their teammates\nOnly the last one, handed in on the last day of class, impacts their teammates’ grades (20%)"
  },
  {
    "objectID": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#industry-engagement",
    "href": "posts/how-to-teach-5-semesters-of-game-design-in-1-class-notes/index.html#industry-engagement",
    "title": "Notes on How to Teach 5 Semesters of Game Design in 1 Class",
    "section": "Industry Engagement",
    "text": "Industry Engagement\n\nOff-campus Networking Event attendance and write-up\nWeekly videos by game industry designers\n\nGets other voices into the room besides the teacher\n\n\nOther Notes:\n\nTry not to talk for more than 10 minutes before having the students do something\nTalk in the beginning of the semester about what each student is in the class for\n\nReferences:\n\nHow to Teach 5 Semesters of Game Design in 1 Class"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "",
    "text": "What is Reinforcement Learning?\nThe Reinforcement Learning Framework\nExploration-Exploitation Tradeoff\nThe Policy\nDeep Reinforcement Learning\nLab\nReferences"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#what-is-reinforcement-learning",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#what-is-reinforcement-learning",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "What is Reinforcement Learning?",
    "text": "What is Reinforcement Learning?\n\nReinforcement learning (RL) is a framework for solving control tasks where agents learn from the environment by interacting with it through trial and error and receiving rewards as unique feedback."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#the-reinforcement-learning-framework",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#the-reinforcement-learning-framework",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "The Reinforcement Learning Framework",
    "text": "The Reinforcement Learning Framework\n\nThe RL Process\n\nThe RL process is a loop that outputs a sequence of state \\(S_{0}\\), action \\(A_{0}\\), reward \\(R_{1}\\), and next state \\(S_{1}\\).\n\n\n\nThe Reward Hypothesis\n\nThe reward and next state result from taking the current action in the current state.\nThe goal is to maximize the expected cumulative reward, called the expected return.\n\n\n\nMarkov Property\n\nThe Markov property implies that agents only need the current state to decide what action to take and not the history of all the states and actions.\n\n\n\nObservation/States Space\n\nObservations/States are the information agents get from the environment.\nThe state is a complete description of the agent’s environment (e.g., a chessboard).\nAn observation is a partial description of the state (e.g., the current frame of a video game).\n\n\n\nAction Space\n\nThe action space is the set of all possible actions in an environment.\nActions can be discrete (e.g., up, down, left, right) or continuous (e.g., steering angle).\nDifferent RL algorithms are suited for discrete and continuous actions.\n\n\n\nRewards and discounting\n\nThe reward is the only feedback the agent receives for its actions.\nRewards that happen earlier in a session (e.g., at the beginning of the game) are more probable since they are more predictable than the long-term reward.\nWe can discount longer-term reward values that are less predictable.\nWe define a discount rate called gamma with a value between 0 and 1. The discount rate is typically 0.99 or 0.95.\nThe larger the gamma, the smaller the discount, meaning agents care more about long-term rewards.\nWe discount each reward by gamma to the exponent of the time step, so they are less predictable the further into the future.\nWe can write the cumulative reward at each time step \\(t\\) as:\n\n\n\n\\[R(\\tau) = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + \\ldots\\]\n\n\n\\[R(\\tau) = \\sum^{\\infty}_{k=0}{r_{t} + k + 1}\\]\n\nDiscounted cumulative expected reward:\n\n\n\n\\[R(\\tau) = r_{t+1} + \\gamma r_{t+2} + \\gamma^{2}r_{t+3} + \\gamma^{3}r_{t+4} + \\ldots\\]\n\n\n\\[R(\\tau) = \\sum^{\\infty}_{k=0}{\\gamma^k{} r_{t} + k + 1}\\]\n\n\nType of tasks\n\nA task is an instance of a Reinforcement Learning problem and is either episodic or continuous.\n\n\nEpisodic Tasks\n\nEpisodic tasks have starting points and ending points.\nWe can represent episodes as a list of states, actions, rewards, and new states.\n\n\n\nContinuous Tasks\n\nContinuous tasks have no terminal state, and the agent must learn to choose the best actions and simultaneously interact with the environment."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#exploration-exploitation-tradeoff",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#exploration-exploitation-tradeoff",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "Exploration-Exploitation Tradeoff",
    "text": "Exploration-Exploitation Tradeoff\n\nWe must balance gaining more information about the environment and exploiting known information to maximize reward (e.g., going with the usual restaurant or trying a new one)."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#the-policy",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#the-policy",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "The Policy",
    "text": "The Policy\n\nThe policy is the function that tells the agent what action to take given the current state.\nThe goal is to find the optimal policy \\(\\pi\\) which maximizes the expected return.\n\\(a = \\pi(s)\\)\n\\(\\pi\\left( a \\vert s \\right) = P \\left[ A \\vert s \\right]\\)\n\\(\\text{policy} \\left( \\text{actions} \\ \\vert \\ \\text{state} \\right) = \\text{probability distribution over the set of actions given the current state}\\)\n\n\nPolicy-based Methods\n\nPolicy-based methods involve learning a policy function directly by teaching the agent which action to take in a given state.\nA deterministic policy will always return the same action in a given state.\nA stochastic policy outputs a probability distribution over actions.\n\n\n\nValue-based methods\n\nValue-based methods teach the agent to learn which future state is more valuable.\nValue-based methods involve training a value function that maps a state to the expected value of being in that state.\nThe value of a state is the expected discounted return the agent can get if it starts in that state and then acts according to the policy."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#deep-reinforcement-learning",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#deep-reinforcement-learning",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "Deep Reinforcement Learning",
    "text": "Deep Reinforcement Learning\n\nDeep reinforcement learning introduces deep neural networks to solve RL problems."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#lab",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#lab",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "Lab",
    "text": "Lab\n\nObjective: Train a lander agent to land correctly, share it to the community, and experiment with different configurations.\nSyllabus\nDiscord server\n#study-group-unit1 discord channel\nEnvironment: LunarLander-v2\nRL-Library: Stable-Baselines3\n\n\nPrerequisites\n\nUnit 1 README\nAn Introduction to Deep Reinforcement Learning\n\n\n\nObjectives\n\nBe able to use Gym, the environment library.\nBe able to use Stable-Baselines3, the deep reinforcement learning library.\nBe able to push your trained agent to the Hub with a nice video replay and an evaluation score.\n\n\n\nSet the GPU (Google Colab)\n\nRuntime &gt; Change Runtime type\nHardware Accelerator &gt; GPU\n\n\n\nInstall dependencies\nInstall virtual screen libraries for rendering the environment\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n\nCreate and run a virual screen\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n    &lt;pyvirtualdisplay.display.Display at 0x7f2df34855d0&gt;\n\n\nGym[box2d]\n\nGym is a toolkit that contains test environments for developing and comparing reinforcement learning algorithms.\nBox2D environments all involve toy games based around physics control, using box2d-based physics and PyGame-based rendering.\nGitHub Repository\nGym Documentation\n\n\n\nStable Baselines\n\nThe Stable Baselines3 library is a set of reliable implementations of reinforcement learning algorithms in PyTorch.\nGitHub Repository\nDocumentation\n\n\n\nHugging Face x Stable-baselines\n\nLoad and upload Stable-baseline3 models from the Hugging Face Hub.\nGitHub Repository\n\n\n%%capture\n!pip install gym[box2d]\n!pip install stable-baselines3[extra]\n!pip install huggingface_sb3\n!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)\n\n\n\n\nImport the packages\nThe Hugging Face Hub Hugging Face works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easilly collaborate with others.\nHugging Face Hub Deep reinforcement Learning models\n\nload_from_hub\n\nDownload a model from Hugging Face Hub.\nSource Code\n\n\n\npackage_to_hub\n\nEvaluate a model, generate a demo video, and upload the model to Hugging Face Hub.\nSource Code\n\n\n\npush_to_hub\n\nUpload a model to Hugging Face Hub.\nSource Code\n\n\n\nnotebook_login\n\nDisplay a widget to login to the HF website and store the token.\nSource Code\n\n\n\nPPO\n\nThe Proximal Policy Optimization algorithm\nDocumentation\n\n\n\nevaluate_policy\n\nRun a policy and return the average reward.\nDocumentation\n\n\n\nmake_vec_env\n\nCreate a wrapped, monitored vectorized environment (VecEnv).\nDocumentation\n\n\nimport gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub\nfrom huggingface_hub import notebook_login\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.env_util import make_vec_env\n\n\n\n\nUnderstand the Gym API\n\nCreate our environment using gym.make()\nReset the environment to its initial state with observation = env.reset()\nGet an action using our model\nPerform the action using env.step(action), which returns:\n\n\nobsevation: The new state (st+1)\nreward: The reward we get after executing the action\ndone: Indicates if the episode terminated\ninfo: A dictionary that provides additional environment-specific information.\n\n\nReset the environment to its initial state with observation = env.reset() at the end of each episode\n\n\n\nCreate the LunarLander environment and understand how it works\n\nLunar Lander Environment\n\nThis environment is a classic rocket trajectory optimization problem.\nThe agent needs to learn to adapt its speed and position(horizontal, vertical, and angular) to land correctly.\nDocumentation\n\n\n\n\nAction Space\nDiscrete(4)\n\n\nObservation Space\n(8,)\n\n\nObservation High\n[inf inf inf inf inf inf inf inf]\n\n\nObservation Low\n[-inf -inf -inf -inf -inf -inf -inf -inf]\n\n\nImport\ngym.make(\"LunarLander-v2\")\n\n\n\nCreate a Lunar Lander environment\nenv = gym.make(\"LunarLander-v2\")\n\nReset the environment\nobservation = env.reset()\n\nTake some random actions in the environment\nfor _ in range(20):\n  # Take a random action\n  action = env.action_space.sample()\n  print(\"Action taken:\", action)\n\n  # Do this action in the environment and get\n  # next_state, reward, done and info\n  observation, reward, done, info = env.step(action)\n  \n  # If the game is done (in our case we land, crashed or timeout)\n  if done:\n      # Reset the environment\n      print(\"Environment is reset\")\n      observation = env.reset()\n    Action taken: 0\n    Action taken: 1\n    Action taken: 0\n    Action taken: 3\n    Action taken: 0\n    Action taken: 3\n    Action taken: 1\n    Action taken: 1\n    Action taken: 0\n    Action taken: 1\n    Action taken: 0\n    Action taken: 1\n    Action taken: 0\n    Action taken: 2\n    Action taken: 1\n    Action taken: 2\n    Action taken: 3\n    Action taken: 3\n    Action taken: 3\n    Action taken: 3\n\nInspect the observation space\n# We create a new environment\nenv = gym.make(\"LunarLander-v2\")\n# Reset the environment\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n    _____OBSERVATION SPACE_____ \n    \n    Observation Space Shape (8,)\n    Sample observation [ 1.9953048  -0.9302978   0.26271465 -1.406391    0.42527643 -0.07207114\n      2.1984298   0.4171027 ]\nNote: * The observation is a vector of size 8, where each value is a different piece of information about the lander. 1. Horizontal pad coordinate (x) 2. Vertical pad coordinate (y) 3. Horizontal speed (x) 4. Vertical speed (y) 5. Angle 6. Angular speed 7. If the left leg has contact point touched the land 8. If the right leg has contact point touched the land\n\nInspect the action space\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n\n     _____ACTION SPACE_____ \n    \n    Action Space Shape 4\n    Action Space Sample 1\nNote: * The action space is discrete, with four available actions. 1. Do nothing. 2. Fire left orientation engine. 3. Fire the main engine. 4. Fire right orientation engine.\n\nReward function details:\n\nMoving from the top of the screen to the landing pad and zero speed is about 100~140 points.\nFiring main engine is -0.3 each frame\nEach leg ground contact is +10 points\nEpisode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)\nThe game is solved if your agent does 200 points.\n\n\n\n\nVectorized Environment\n\nWe can stack multiple independent environments into a single vector to get more diverse experiences during the training.\n\nStack 16 independent environments\nenv = make_vec_env('LunarLander-v2', n_envs=16)\n\n\n\n\nCreate the Model\n\nPPO (aka Proximal Policy Optimization) is a combination of:\n\nValue-based reinforcement learning method: learning an action-value function that will tell us what’s the most valuable action to take given a state and action.\nPolicy-based reinforcement learning method: learning a policy that will gives us a probability distribution over actions.\n\n\n\nStable-Baselines3 setup steps:\n\nYou create your environment (in our case it was done above)\nYou define the model you want to use and instantiate this model model = PPO(\"MlpPolicy\")\nYou train the agent with model.learn and define the number of training timesteps\n\nSample Code:\n# Create environment\nenv = gym.make('LunarLander-v2')\n\n# Instantiate the agent\nmodel = PPO('MlpPolicy', env, verbose=1)\n# Train the agent\nmodel.learn(total_timesteps=int(2e5))\n\nimport inspect\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nInspect default PPO arguments\nargs = inspect.getfullargspec(PPO).args\ndefaults = inspect.getfullargspec(PPO).defaults\ndefaults = [None]*(len(args)-len(defaults)) + list(defaults)\nannotations = inspect.getfullargspec(PPO).annotations.values()\nannotations = [None]*(len(args)-len(annotations)) + list(annotations)\nppo_default_args = {arg:[default, annotation] for arg,default,annotation in zip(args, defaults, annotations)}\npd.DataFrame(ppo_default_args, index=[\"Default Value\", \"Annotation\"]).T\n\n\n\n\n\n\n\n\nDefault Value\n\n\nAnnotation\n\n\n\n\n\n\nself\n\n\nNone\n\n\nNone\n\n\n\n\npolicy\n\n\nNone\n\n\ntyping.Union[str, typing.Type[stable_baselines3.common.policies.ActorCriticPolicy]]\n\n\n\n\nenv\n\n\nNone\n\n\ntyping.Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str]\n\n\n\n\nlearning_rate\n\n\n0.0003\n\n\ntyping.Union[float, typing.Callable[[float], float]]\n\n\n\n\nn_steps\n\n\n2048\n\n\n&lt;class ‘int’&gt;\n\n\n\n\nbatch_size\n\n\n64\n\n\n&lt;class ‘int’&gt;\n\n\n\n\nn_epochs\n\n\n10\n\n\n&lt;class ‘int’&gt;\n\n\n\n\ngamma\n\n\n0.99\n\n\n&lt;class ‘float’&gt;\n\n\n\n\ngae_lambda\n\n\n0.95\n\n\n&lt;class ‘float’&gt;\n\n\n\n\nclip_range\n\n\n0.2\n\n\ntyping.Union[float, typing.Callable[[float], float]]\n\n\n\n\nclip_range_vf\n\n\nNone\n\n\ntyping.Union[NoneType, float, typing.Callable[[float], float]]\n\n\n\n\nnormalize_advantage\n\n\nTrue\n\n\n&lt;class ‘bool’&gt;\n\n\n\n\nent_coef\n\n\n0.0\n\n\n&lt;class ‘float’&gt;\n\n\n\n\nvf_coef\n\n\n0.5\n\n\n&lt;class ‘float’&gt;\n\n\n\n\nmax_grad_norm\n\n\n0.5\n\n\n&lt;class ‘float’&gt;\n\n\n\n\nuse_sde\n\n\nFalse\n\n\n&lt;class ‘bool’&gt;\n\n\n\n\nsde_sample_freq\n\n\n-1\n\n\n&lt;class ‘int’&gt;\n\n\n\n\ntarget_kl\n\n\nNone\n\n\ntyping.Optional[float]\n\n\n\n\ntensorboard_log\n\n\nNone\n\n\ntyping.Optional[str]\n\n\n\n\ncreate_eval_env\n\n\nFalse\n\n\n&lt;class ‘bool’&gt;\n\n\n\n\npolicy_kwargs\n\n\nNone\n\n\ntyping.Optional[typing.Dict[str, typing.Any]]\n\n\n\n\nverbose\n\n\n0\n\n\n&lt;class ‘int’&gt;\n\n\n\n\nseed\n\n\nNone\n\n\ntyping.Optional[int]\n\n\n\n\ndevice\n\n\nauto\n\n\ntyping.Union[torch.device, str]\n\n\n\n\n_init_setup_model\n\n\nTrue\n\n\n&lt;class ‘bool’&gt;\n\n\n\n\n\n\n\nDefine a PPO MlpPolicy architecture\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n    Using cuda device\nNote: * We use a Multilayer Perceptron because the observations are vectors instead of images.\n\nRecommended Values:\n\n\n\n\nArgument\nValue\n\n\n\n\nn_steps\n1024\n\n\nbatch_size\n64\n\n\nn_epochs\n4\n\n\ngamma\n0.999\n\n\ngae_lambda\n0.98\n\n\nent_coef\n0.01\n\n\nverbose\n1\n\n\n\n\n\n\n\nTrain the PPO agent\nTrain the model\nmodel.learn(total_timesteps=int(2000000))\n    ---------------------------------\n    | rollout/           |          |\n    |    ep_len_mean     | 94.8     |\n    |    ep_rew_mean     | -199     |\n    | time/              |          |\n    |    fps             | 2891     |\n    |    iterations      | 1        |\n    |    time_elapsed    | 11       |\n    |    total_timesteps | 32768    |\n    ---------------------------------\n...\n    ------------------------------------------\n    | rollout/                |              |\n    |    ep_len_mean          | 187          |\n    |    ep_rew_mean          | 281          |\n    | time/                   |              |\n    |    fps                  | 593          |\n    |    iterations           | 62           |\n    |    time_elapsed         | 3421         |\n    |    total_timesteps      | 2031616      |\n    | train/                  |              |\n    |    approx_kl            | 0.0047587324 |\n    |    clip_fraction        | 0.0585       |\n    |    clip_range           | 0.2          |\n    |    entropy_loss         | -0.469       |\n    |    explained_variance   | 0.986        |\n    |    learning_rate        | 0.0003       |\n    |    loss                 | 3.62         |\n    |    n_updates            | 610          |\n    |    policy_gradient_loss | -0.0007      |\n    |    value_loss           | 11.5         |\n    ------------------------------------------\n\n    &lt;stable_baselines3.ppo.ppo.PPO at 0x7fcc807b8410&gt;\n\n\n\nEvaluate the agent\n\nWe can evaluate the model’s performance using the evaluate_policy() method.\nExample\n\nCreate a new environment for evaluation\neval_env = gym.make('LunarLander-v2')\n\nEvaluate the model with 10 evaluation episodes and deterministic=True\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n\nPrint the results\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n    mean_reward=78.15 +/- 94.84891574522395\n\n\n\nPublish our trained model on the Hub\n\nWe can use the package_to_hub() method to evaluate the model, record a replay, generate a model card, and push the model to the Hub in a single line of code.\nLeaderboard\nThe package_to_hub() method returns a link to a Hub model repository such as https://huggingface.co/osanseviero/test_sb3.\nModel repository features:\n\nA video preview of your agent at the right.\nClick “Files and versions” to see all the files in the repository.\nClick “Use in stable-baselines3” to get a code snippet that shows how to load the model.\nA model card (README.md file) which gives a description of the model\n\nHugging Face Hub uses git-based repositories so we can update the model with new versions.\n\nConnect to Hugging Face Hub: 1. Create Hugging Face account https://huggingface.co/join 2. Create a new authentication token (https://huggingface.co/settings/tokens) with write role 3. Run the notebook_login() method.\nLog into Hugging Face account\nnotebook_login()\n!git config --global credential.helper store\n    Login successful\n    Your token has been saved to /root/.huggingface/token\n\npackage_to_hub function arguments: - model: our trained model. - model_name: the name of the trained model that we defined in model_save - model_architecture: the model architecture we used (e.g., PPO) - env_id: the name of the environment, in our case LunarLander-v2 - eval_env: the evaluation environment defined in eval_env - repo_id: the name of the Hugging Face Hub Repository that will be created/updated (repo_id = {username}/{repo_name}) * Example format: {username}/{model_architecture}-{env_id} - commit_message: message of the commit\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom huggingface_sb3 import package_to_hub\n\nPush the model to the Hugging Face Hub\n# Define the name of the environment\nenv_id = \"LunarLander-v2\"\n\n# Create the evaluation env\neval_env = DummyVecEnv([lambda: gym.make(env_id)])\n\n# Define the model architecture we used\nmodel_architecture = \"ppo\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\nrepo_id = f\"cj-mills/{model_architecture}-{env_id}\"\n\nmodel_name = f\"{model_architecture}-{env_id}\"\n\n## Define the commit message\ncommit_message = f\"Upload {model_name} model with longer training session\"\n\n# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\npackage_to_hub(model=model, # Our trained model\n               model_name=model_name, # The name of our trained model \n               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n               env_id=env_id, # Name of the environment\n               eval_env=eval_env, # Evaluation Environment\n               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n               commit_message=commit_message)\n    'https://huggingface.co/cj-mills/ppo-LunarLander-v2'\n\n\n\nSome additional challenges\n\nTrain for more steps.\nTry different hyperparameters of PPO.\nCheck the Stable-Baselines3 documentation and try another model such as DQN.\nTry using the CartPole-v1, MountainCar-v0 or CarRacing-v0 environments."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#references",
    "href": "posts/hugging-face-deep-rl-course-notes/part-1/index.html#references",
    "title": "Notes on The Hugging Face Deep RL Class Pt.1",
    "section": "References",
    "text": "References\n\nThe Hugging Face Deep Reinforcement Learning Class\nAn Introduction to Deep Reinforcement Learning"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "",
    "text": "Types of Value-Based Methods\nThe Bellman Equation\nMonte Carlo vs Temporal Difference Learning\nIntroducing Q-Learning\nLab\nReferences"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#types-of-value-based-methods",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#types-of-value-based-methods",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "Types of Value-Based Methods",
    "text": "Types of Value-Based Methods\n\nThe value of a state is the expected discounted return from starting in that state and following the policy.\nValue-based methods involve learning a value function that maps a state to the expected value of being in that state.\nFinding an optimal value function leads to having an optimal policy.\n\\(\\pi^{*}(s) = argmax_{a} Q^{*}(s,a)\\)\nValue-based methods require us to define how the agent acts (i.e., the policy) based on the predicted value map.\nGreedy policies always take the action that leads to the biggest reward.\nEpsilon-Greedy policies switch between exploring random actions and taking actions with the highest known reward.\n\nThe probability of exploring random actions is high at the beginning of training and decreases as training progresses.\n\n\n\nThe State-Value function\n\nThe state-value function, for each state \\(S_{t}\\), outputs the expected return \\(E_{\\pi}\\left[ G_{t} \\right]\\) if the agent starts in that state \\(S_{t}\\) and then follows the policy \\(\\pi\\) forever.\n\\[V_{\\pi}(s) = E_{\\pi}\\left[ G _{t} \\vert S_{t} = s \\right]\\]\n\n\n\nThe Action-Value function\n\nThe action-value function outputs the expected return $E_{}$ for each state-action pair \\(\\left( S_{t}, A_{t} \\right)\\) if the agent takes a given action \\(A_{t}\\) when starting in a given state \\(S_{t}\\) and then follows the policy \\(\\pi\\) forever.\n\\[Q_{\\pi} (s,a) = E_{\\pi} \\left[ G_{t} \\vert S_{t} = s, A_{t} = a \\right]\\]"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#the-bellman-equation",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#the-bellman-equation",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "The Bellman Equation",
    "text": "The Bellman Equation\n\nThe Bellman equation simplifies our value estimation.\nThe Bellman equation is a recursive equation that allows us to consider the value of any state \\(S_{t}\\) as the immediate reward \\(R_{t+1}\\) plus the discounted value of the state that follows \\(gamma \\cdot V(S_{t+1})\\).\n\\(V_{\\pi}(s) = E_{\\pi} \\left[ R_{t+1} + \\gamma \\cdot V_{\\pi}(S_{t+1}) \\vert S_{t} = s \\right]\\)"
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#monte-carlo-vs-temporal-difference-learning",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#monte-carlo-vs-temporal-difference-learning",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "Monte Carlo vs Temporal Difference Learning",
    "text": "Monte Carlo vs Temporal Difference Learning\n\nMonte Carlo uses an entire episode of experience before learning.\nTemporal difference learning learns after each step.\n\n\nMonte Carlo: learning at the end of the episode\n\nMonte Carlo waits until the end of the episode, calculates the total rewards \\(G_{t}\\), and uses it as a target for updating the value function \\(V(S_{t})\\) using a learning rate \\(\\alpha\\).\n\\(V(S_{t}) \\leftarrow V(S_{t}) + \\alpha \\left[G_{t} - V(S_{t}) \\right]\\)\nAt the end of each episode, we have a list of States, Actions, Rewards, and new States.\nThe agent improves by running more and more episodes.\nMonte Carlo uses the actual accurate discounted return of an episode.\n\n\n\nTemporal Difference Learning: learning at each step\n\nTemporal difference waits for one interaction \\(S_{t+1}\\), forms a TD target \\(R_{t+1} + \\gamma \\cdot V(S_{t+1})\\), and updates the value function \\(V(S_{t})\\) using the immediate reward plus \\(R_{t+1}\\) the discounted value of the following state \\(gamma \\cdot V(S_{t+1})\\) scaled by a learning rate \\(\\alpha\\).\n\\(V(S_{t}) \\leftarrow V(S_{t}) + \\alpha \\left[R_{t+1} + \\gamma \\cdot V(S_{t+1}) - V(S_{t}) \\right]\\)\nTD Learning that waits for one step is TD(0) or one-step TD.\nThe agent improves by running more and more steps.\nTD Learning uses an estimated return called TD target."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#introducing-q-learning",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#introducing-q-learning",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "Introducing Q-Learning",
    "text": "Introducing Q-Learning\n\nWhat is Q-Learning?\n\nQ-Learning (a.k.a. Sarsamax) is an off-policy value-based method that uses a TD approach to train its action-value function called the Q-Function.\nOff-policy refers to using a different policy for acting and updating.\n\nWe use a greedy policy for updating the action-value function and an epsilon-greedy function for choosing actions.\n\nThe “Q” refers to the quality of a given action in a given state.\nThe Q-Function maintains a Q-table that tracks the value of each possible state-action pair.\nEach cell in the Q-table stores the value from taking a given action in a given state.\nWe initialize the values for each state-action pair in the Q-table to 0.\n\n\n\nThe Q-Learning algorithm\n\nQ Learning waits for one interaction, forms a TD target \\(R_{t+1} + \\gamma max_{a} Q(S_{t+1} , a)\\), and updates the Q-value \\(Q(S_{t} , A_{t} )\\) for the state-action pair $(S_{t} , A_{t} ) $ in the Q-table using the immediate reward \\(R_{t+1}\\) plus the discounted optimal (i.e., greedy) Q-Value of the following state \\(\\gamma max_{a} Q(S_{t+1} , a)\\) scaled by a learning rate \\(\\alpha\\).\nThe Q-Values in the Q-table become more accurate with more steps.\nInput: policy \\(\\pi\\), positive integer \\(num\\_episodes\\), small positive fraction \\(\\alpha\\), \\(GLIE\\) \\(\\{\\epsilon_{i}\\}\\)\nOutput: value function \\(Q (\\approx q_{\\pi})\\) if num_episodes is large enough\nSteps:\n\nInitialize \\(Q\\) arbitrarily \\((\\)e.g. \\(Q(s,a) = 0\\) for all \\(s \\ \\epsilon S A(s)\\), and \\(Q(terminal-state, \\cdot) = 0 )\\)\nfor \\(i \\leftarrow 1\\) to num_episodes\n\n\\(\\epsilon \\leftarrow \\epsilon_{i}\\)\nObserve \\(S_{0}\\)\n\\(t \\leftarrow 0\\)\nrepeat until \\(S_{t}\\) is terminal\n\nChoose action \\(A_{t}\\) using policy derived from \\(Q(e.g., \\epsilon\\)-greedy\\()\\)\nTake action \\(A_{t}\\) and observe \\(R_{t+1},S_{t+1}\\)\n\\(Q(S_{t},A_{t}) \\leftarrow Q(S_{t},A_{t}) + \\alpha (R_{t+1} + \\gamma \\cdot max_{a}Q(S_{t+1}, a) - Q(S_{t}, A_{t}))\\)\n\\(t \\leftarrow t + 1\\)\n\n\nreturn \\(Q\\)\n\n\n\n\nOff-policy vs On-policy\n\nOff-policy refers to using a different policy for acting and updating.\nOn-policy refers to using the same policy for acting and updating."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#lab",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#lab",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "Lab",
    "text": "Lab\n\nObjective: Code a Reinforcement Learning agent from scratch to play FrozenLake and Taxi using Q-Learning, share it to the community, and experiment with different configurations.\nEnvironments:\n\nFrozenLake-v1: The agent needs to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoiding holes (H).\nTaxi-v3: The agent needs to learn to navigate a city to transport its passengers from point A to point B.\n\nSyllabus\nDiscord server\n#study-group-unit2 discord channel\n\n\nPrerequisites\n\nUnit 2 README\nAn Introduction to Q-Learning Part 1\nAn Introduction to Q-Learning Part 2\n\n\n\nObjectives\n\nBe able to use Gym, the environment library.\nBe able to code a Q-Learning agent from scratch.\nBe able to push your trained agent and the code to the Hub with a video replay and an evaluation score.\n\nCreate and run a virual screen\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n&lt;pyvirtualdisplay.display.Display at 0x7fea103d3b20&gt;\nImport the packages - random: To generate random numbers (that will be useful for Epsilon-Greedy Policy). - imageio: To generate a replay video\nimport numpy as np\nimport gym\nimport random\nimport imageio\nimport os\n\nimport pickle5 as pickle\n/home/innom-dt/mambaforge/envs/hf-drl-class-unit2/lib/python3.9/site-packages/gym/envs/registration.py:398: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n  logger.warn(\n\n\nCreate and understand FrozenLake environment â\n\nDocumentation\nThe Q-Learning agent needs to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).\nWe can have two sizes of environment:\n\nmap_name=\"4x4\": a 4x4 grid version\nmap_name=\"8x8\": a 8x8 grid version\n\nThe environment has two modes:\n\nis_slippery=False: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake.\nis_slippery=True: The agent may not always move in the intended direction due to the slippery nature of the frozen lake (stochastic).\n\n\nCreate a FrozenLake-v1 environment with a 4x4 non-slippery map\nenv = gym.make(\"FrozenLake-v1\", map_name=f\"4x4\", is_slippery=False)\n(Optional) Define a custom grid: * “S”: start position * “F”: frozen tile * “H”: hole tile * “G”: gift tile\n# Custom 4x4 grid\ndesc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\nNote: This custom grid arrangement would like like the map below.\n\n\nInspect the environment\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n_____OBSERVATION SPACE_____ \n\nObservation Space Discrete(16)\nSample observation 9\nNote: The observation is a value representing the agentâs current position as \\(current\\_row \\cdot nrows + current\\_col\\), where both the row and col start at 0.\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n _____ACTION SPACE_____ \n\nAction Space Shape 4\nAction Space Sample 3\nAction Space: * 0: GO LEFT * 1: GO DOWN * 2: GO RIGHT * 3: GO UP\nReward Function: * Reach goal: +1 * Reach hole: 0 * Reach frozen: 0\n\n\nCreate and Initialize the Q-table\nstate_space = env.observation_space.n\naction_space = env.action_space.n\nprint(f\"There are {state_space} possible states and {action_space} possible actions\")\nThere are 16 possible states and 4 possible actions\nDefine a function to initialize a Q-table\ndef initialize_q_table(state_space, action_space):\n    return np.zeros((state_space, action_space))\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\ndef display_qtable(qtable, actions, num_rows, num_cols):\n    indices = np.array(np.meshgrid(*np.indices((num_rows, num_cols), sparse=True))).T.reshape(-1, 2)\n    map_coords = [f\"({r},{c})\" for r,c in indices]\n    return pd.DataFrame(qtable, index=map_coords, columns=actions)\nQtable_frozenlake = initialize_q_table(state_space, action_space)\naction_names = ['Left', 'Down', 'Right', 'Up']\ndisplay_qtable(Qtable_frozenlake, action_names, 4, 4)\n\n\n\n\n\n\n\n\nLeft\n\n\nDown\n\n\nRight\n\n\nUp\n\n\n\n\n\n\n(0,0)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(0,1)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(0,2)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(0,3)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(1,0)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(1,1)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(1,2)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(1,3)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(2,0)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(2,1)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(2,2)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(2,3)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(3,0)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(3,1)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(3,2)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n(3,3)\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n\n\nDefine the greedy policy\ndef greedy_policy(Qtable, state):\n    # Exploitation: take the action with the highest state, action value\n    return np.argmax(Qtable[state])\n\n\nDefine the epsilon-greedy policy\ndef epsilon_greedy_policy(Qtable, state):\n    # Generate a random number in the interval [0, 1)\n    random_num = random.random()\n    # if random_num &gt; greater than epsilon --&gt; exploitation, else --&gt; exploration\n    return greedy_policy(Qtable, state) if random_num &gt; epsilon else env.action_space.sample()\n\n\nDefine the hyperparameters\n\nWe can use a progressive decay of the epsilon to make sure our agent explores enough of the state space to learn a good value approximation.\nDecreasing the epsilon too quickly might cause the agent to get stuck by not exploring enough of the state space.\n\n# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"     # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability \ndecay_rate = 0.005            # Exponential decay rate for exploration prob\n\n\nCreate the training loop method\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n    for episode in range(n_training_episodes):\n        # Reduce epsilon (because we need less and less exploration)\n        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n        # Reset the environment\n        state = env.reset()\n        step = 0\n        done = False\n        \n        # repeat\n        for step in range(max_steps):\n            # Choose the action At using epsilon greedy policy\n            action = epsilon_greedy_policy(Qtable, state)\n            \n            # Take action At and observe Rt+1 and St+1\n            # Take the action (a) and observe the outcome state(s') and reward (r)\n            new_state, reward, done, info = env.step(action)\n            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n            td_target = reward + gamma * np.max(Qtable[new_state])\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (td_target - Qtable[state][action])\n            \n            # If done, finish the episode\n            if done:\n                break\n            \n            # Our state is the new state\n            state = new_state\n    return Qtable\n\n\nTrain the Q-Learning agent\nQtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n\n\nInspect the updated Q-Learning table\ndisplay_qtable(Qtable_frozenlake, action_names, 4, 4)\n\n\n\n\n\n\n\n\nLeft\n\n\nDown\n\n\nRight\n\n\nUp\n\n\n\n\n\n\n(0,0)\n\n\n0.735092\n\n\n0.773781\n\n\n0.773781\n\n\n0.735092\n\n\n\n\n(0,1)\n\n\n0.735092\n\n\n0.000000\n\n\n0.814506\n\n\n0.773781\n\n\n\n\n(0,2)\n\n\n0.773781\n\n\n0.857375\n\n\n0.773781\n\n\n0.814506\n\n\n\n\n(0,3)\n\n\n0.814506\n\n\n0.000000\n\n\n0.773781\n\n\n0.773781\n\n\n\n\n(1,0)\n\n\n0.773781\n\n\n0.814506\n\n\n0.000000\n\n\n0.735092\n\n\n\n\n(1,1)\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\n(1,2)\n\n\n0.000000\n\n\n0.902500\n\n\n0.000000\n\n\n0.814506\n\n\n\n\n(1,3)\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\n(2,0)\n\n\n0.814506\n\n\n0.000000\n\n\n0.857375\n\n\n0.773781\n\n\n\n\n(2,1)\n\n\n0.814506\n\n\n0.902500\n\n\n0.902500\n\n\n0.000000\n\n\n\n\n(2,2)\n\n\n0.857375\n\n\n0.950000\n\n\n0.000000\n\n\n0.857375\n\n\n\n\n(2,3)\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\n(3,0)\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\n(3,1)\n\n\n0.000000\n\n\n0.902500\n\n\n0.950000\n\n\n0.857375\n\n\n\n\n(3,2)\n\n\n0.902500\n\n\n0.950000\n\n\n1.000000\n\n\n0.902500\n\n\n\n\n(3,3)\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\n\n\n\n\nDefine the evaluation method\ndef evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param Q: The Q-table\n    :param seed: The evaluation seed array (for taxi-v3)\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        if seed:\n            state = env.reset(seed=seed[episode])\n        else:\n            state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n    \n        for step in range(max_steps):\n            # Take the action (index) that have the maximum expected future reward given that state\n            action = np.argmax(Q[state][:])\n            new_state, reward, done, info = env.step(action)\n            total_rewards_ep += reward\n        \n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n    \n    return mean_reward, std_reward\n\n\nEvaluate theQ-Learning agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\nMean_reward=1.00 +/- 0.00\nNote: * The mean reward should be 1.0 * Try using the slippery version of the map.\n\n\nPublish our trained model on the Hub\n%%capture\nfrom huggingface_hub import HfApi, HfFolder, Repository\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\ndef record_video(env, Qtable, out_directory, fps=1):\n    images = []\n    done = False\n    state = env.reset(seed=random.randint(0,500))\n    img = env.render(mode='rgb_array')\n    images.append(img)\n    while not done:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(Qtable[state][:])\n        state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n        img = env.render(mode='rgb_array')\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\nLeaderboard\nLog into Hugging Face account\nfrom huggingface_hub import notebook_login\nnotebook_login()\nLogin successful\nYour token has been saved to /home/innom-dt/.huggingface/token\nCreate a model dictionnary that contains the hyperparameters and the Q_table\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"epsilon\": epsilon,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_frozenlake\n}\nmodel\n{'env_id': 'FrozenLake-v1',\n 'max_steps': 99,\n 'n_training_episodes': 10000,\n 'n_eval_episodes': 100,\n 'eval_seed': [],\n 'learning_rate': 0.7,\n 'gamma': 0.95,\n 'epsilon': 1.0,\n 'max_epsilon': 1.0,\n 'min_epsilon': 0.05,\n 'decay_rate': 0.005,\n 'qtable': array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n        [0.73509189, 0.        , 0.81450625, 0.77378094],\n        [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n        [0.81450625, 0.        , 0.77378094, 0.77378094],\n        [0.77378094, 0.81450625, 0.        , 0.73509189],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.        , 0.81450625],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.81450625, 0.        , 0.857375  , 0.77378094],\n        [0.81450625, 0.9025    , 0.9025    , 0.        ],\n        [0.857375  , 0.95      , 0.        , 0.857375  ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.95      , 0.857375  ],\n        [0.9025    , 0.95      , 1.        , 0.9025    ],\n        [0.        , 0.        , 0.        , 0.        ]])}\nPublish the trained model on the Hub\nusername = \"cj-mills\"\nrepo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)\n{'env_id': 'FrozenLake-v1', 'max_steps': 99, 'n_training_episodes': 10000, 'n_eval_episodes': 100, 'eval_seed': [], 'learning_rate': 0.7, 'gamma': 0.95, 'epsilon': 1.0, 'max_epsilon': 1.0, 'min_epsilon': 0.05, 'decay_rate': 0.005, 'qtable': array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n       [0.73509189, 0.        , 0.81450625, 0.77378094],\n       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n       [0.81450625, 0.        , 0.77378094, 0.77378094],\n       [0.77378094, 0.81450625, 0.        , 0.73509189],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.        , 0.81450625],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.81450625, 0.        , 0.857375  , 0.77378094],\n       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n       [0.857375  , 0.95      , 0.        , 0.857375  ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.95      , 0.857375  ],\n       [0.9025    , 0.95      , 1.        , 0.9025    ],\n       [0.        , 0.        , 0.        , 0.        ]]), 'map_name': '4x4', 'slippery': False}\nPushing repo q-FrozenLake-v1-4x4-noSlippery to the Hugging Face Hub\nYour model is pushed to the hub. You can view your model here: https://huggingface.co/cj-mills/q-FrozenLake-v1-4x4-noSlippery\n\n\nCreate and understand Taxi-v3\n\nDocumentation\nThere are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\nThe taxi starts off at a random square and the passenger is at a random location.\nThe taxi drives to the passengerâs location, picks up the passenger, drives to the passengerâs destination (another one of the four specified locations), and then drops off the passenger.\nThe episode ends once the taxi drops off the passenger.\n\n\n\n\n\n\nenv = gym.make(\"Taxi-v3\")\nNote: There are 25 taxi positions, five possible passenger locations (including when the passenger is in the taxi), and four destination locations, meaning 500 discrete states.\nstate_space = env.observation_space.n\naction_space = env.action_space.n\nprint(f\"There are {state_space} possible states and {action_space} possible actions\")\nThere are 500 possible states and 6 possible actions\nAction space: * 0: move south * 1: move north * 2: move east * 3: move west * 4: pickup passenger * 5: drop off passenger\nReward function: * -1 per step unless other reward is triggered. * +20 delivering passenger. * -10 executing pickup and drop-off actions illegally.\n# Create our Q table with state_size rows and action_size columns (500x6)\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(\"Q-table shape: \", Qtable_taxi .shape)\nindices = np.array(np.meshgrid(*np.indices((25, 5, 4), sparse=True))).T.reshape(-1, 3)\nmap_coords = [f\"TaxiPos: {tp}, PassLoc: {pl}, DestLoc: {dl}\" for tp,pl,dl in indices]\naction_names = ['move south', 'move north', 'move east', 'move west', 'pickup passenger', 'drop off passenger']\npd.DataFrame(Qtable_taxi, index=map_coords, columns=action_names)\nQ-table shape:  (500, 6)\n\n\n\n\n\n\n\n\nmove south\n\n\nmove north\n\n\nmove east\n\n\nmove west\n\n\npickup passenger\n\n\ndrop off passenger\n\n\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n\n\nDefine the hyperparameters\n# Training parameters\nn_training_episodes = 25000   # Total training episodes\nlearning_rate = 0.7           # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05           # Minimum exploration probability \ndecay_rate = 0.005            # Exponential decay rate for exploration prob\nDO NOT MODIFY EVAL_SEED\n# DO NOT MODIFY EVAL_SEED\neval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n                                                          # Each seed has a specific starting state\n\n\nTrain a Q-Learning agent\nQtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nindices = np.array(np.meshgrid(*np.indices((25, 5, 4), sparse=True))).T.reshape(-1, 3)\nmap_coords = [f\"TaxiPos: {tp}, PassLoc: {pl}, DestLoc: {dl}\" for tp,pl,dl in indices]\naction_names = ['move south', 'move north', 'move east', 'move west', 'pickup passenger', 'drop off passenger']\npd.DataFrame(Qtable_taxi, index=map_coords, columns=action_names)\n\n\n\n\n\n\n\n\nmove south\n\n\nmove north\n\n\nmove east\n\n\nmove west\n\n\npickup passenger\n\n\ndrop off passenger\n\n\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 0\n\n\n2.752004\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 0\n\n\n7.933492\n\n\n9.403676\n\n\n7.933492\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 0\n\n\n3.949478\n\n\n5.209976\n\n\n3.949478\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 0\n\n\n-3.275187\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 0\n\n\n-3.275187\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 0\n\n\n5.209976\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 0\n\n\n16.100000\n\n\n18.000000\n\n\n16.100000\n\n\n18.000000\n\n\n9.000000\n\n\n20.000000\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 0\n\n\n6.536817\n\n\n5.209976\n\n\n6.536817\n\n\n5.209976\n\n\n-3.790024\n\n\n3.949478\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 0\n\n\n12.580250\n\n\n10.951237\n\n\n9.403676\n\n\n10.951237\n\n\n1.951237\n\n\n9.403676\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 0\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n6.536817\n\n\n-2.463183\n\n\n5.209976\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 0\n\n\n1.614404\n\n\n2.752004\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 0\n\n\n6.536817\n\n\n7.933492\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 0\n\n\n2.752004\n\n\n3.949478\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-4.111427\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-4.111427\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 0\n\n\n3.949478\n\n\n2.752004\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 0\n\n\n14.295000\n\n\n16.100000\n\n\n16.100000\n\n\n18.000000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 0\n\n\n7.933492\n\n\n6.536817\n\n\n6.536817\n\n\n5.209976\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 0\n\n\n10.951237\n\n\n9.403676\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 0\n\n\n9.403676\n\n\n7.933492\n\n\n7.933492\n\n\n6.536817\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 0\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 0\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 0\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 0\n\n\n3.949478\n\n\n5.209976\n\n\n6.536817\n\n\n5.209976\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 0\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 0\n\n\n-0.493001\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 0\n\n\n2.752004\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 0\n\n\n9.403676\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 0\n\n\n12.580250\n\n\n14.295000\n\n\n16.100000\n\n\n14.295000\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 0\n\n\n9.403676\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 0\n\n\n10.951237\n\n\n9.403676\n\n\n10.951237\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-4.111427\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 0\n\n\n1.614404\n\n\n2.752004\n\n\n3.949478\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 0\n\n\n1.614404\n\n\n2.752004\n\n\n3.949478\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 0\n\n\n5.209976\n\n\n6.536817\n\n\n7.933492\n\n\n5.209976\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-4.111427\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 0\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 0\n\n\n3.949478\n\n\n2.752004\n\n\n1.614404\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 0\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 0\n\n\n7.933492\n\n\n6.536817\n\n\n5.209976\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 0\n\n\n14.295000\n\n\n16.100000\n\n\n18.000000\n\n\n14.295000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 0\n\n\n7.933492\n\n\n6.536817\n\n\n5.209976\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 0\n\n\n12.580250\n\n\n10.951237\n\n\n9.403676\n\n\n9.403676\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 0\n\n\n-3.275187\n\n\n-4.111427\n\n\n-4.111427\n\n\n-3.275187\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 0\n\n\n2.752004\n\n\n3.949478\n\n\n3.949478\n\n\n2.752004\n\n\n5.209976\n\n\n-5.050522\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 0\n\n\n2.752004\n\n\n3.949478\n\n\n3.949478\n\n\n2.752004\n\n\n5.209976\n\n\n-5.050522\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 0\n\n\n6.536817\n\n\n7.933492\n\n\n7.933492\n\n\n6.536817\n\n\n9.403676\n\n\n-1.066508\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 0\n\n\n-3.275187\n\n\n-4.111427\n\n\n-4.111427\n\n\n-3.275187\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 0\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 0\n\n\n2.752004\n\n\n1.614404\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 0\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 0\n\n\n6.536817\n\n\n5.209976\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n3.949478\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 0\n\n\n16.100000\n\n\n18.000000\n\n\n18.000000\n\n\n16.100000\n\n\n9.000000\n\n\n20.000000\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 0\n\n\n6.536817\n\n\n5.209976\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n3.949478\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 0\n\n\n10.951237\n\n\n9.403676\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n7.933492\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 0\n\n\n1.614404\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 0\n\n\n6.536817\n\n\n9.403676\n\n\n6.536817\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 0\n\n\n2.752004\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 0\n\n\n-2.394933\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 0\n\n\n-2.394933\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 0\n\n\n0.533683\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 0\n\n\n6.536817\n\n\n3.949478\n\n\n3.949478\n\n\n5.209976\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 0\n\n\n1.614404\n\n\n-0.493001\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 0\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 0\n\n\n-0.493001\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 0\n\n\n1.614404\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 0\n\n\n-0.493001\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 0\n\n\n14.295000\n\n\n18.000000\n\n\n14.295000\n\n\n16.100000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 0\n\n\n7.933492\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 0\n\n\n14.295000\n\n\n10.951237\n\n\n10.951237\n\n\n12.580250\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 0\n\n\n9.403676\n\n\n6.536817\n\n\n9.403676\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 0\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 0\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 0\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 0\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 0\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-3.275187\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 1\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-3.275187\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 1\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 1\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 1\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 1\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 1\n\n\n12.580250\n\n\n16.100000\n\n\n14.295000\n\n\n16.100000\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 1\n\n\n9.403676\n\n\n6.536817\n\n\n7.933492\n\n\n6.536817\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 1\n\n\n12.580250\n\n\n9.403676\n\n\n10.951237\n\n\n12.580250\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 1\n\n\n10.951237\n\n\n7.933492\n\n\n9.403676\n\n\n7.933492\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 1\n\n\n-0.493001\n\n\n-2.394933\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 1\n\n\n3.949478\n\n\n1.614404\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 1\n\n\n0.533683\n\n\n-1.468351\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 1\n\n\n-0.493001\n\n\n1.614404\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 1\n\n\n-0.493001\n\n\n1.614404\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 1\n\n\n2.752004\n\n\n5.209976\n\n\n5.209976\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 1\n\n\n3.949478\n\n\n1.614404\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 1\n\n\n-0.493001\n\n\n-2.394933\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 1\n\n\n0.533683\n\n\n-1.468351\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 1\n\n\n3.949478\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 1\n\n\n10.951237\n\n\n7.933492\n\n\n7.933492\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 1\n\n\n10.951237\n\n\n14.295000\n\n\n14.295000\n\n\n12.580250\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 1\n\n\n10.951237\n\n\n7.933492\n\n\n7.933492\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 1\n\n\n12.580250\n\n\n9.403676\n\n\n12.580250\n\n\n10.951237\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 1\n\n\n-1.468351\n\n\n-3.275187\n\n\n-3.275187\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 1\n\n\n-0.493001\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 1\n\n\n0.533683\n\n\n2.752004\n\n\n2.752004\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 1\n\n\n0.533683\n\n\n2.752004\n\n\n2.752004\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 1\n\n\n3.949478\n\n\n6.536817\n\n\n6.536817\n\n\n3.949478\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 1\n\n\n-1.468351\n\n\n-3.275187\n\n\n-3.275187\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 1\n\n\n-0.493001\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 1\n\n\n5.209976\n\n\n2.752004\n\n\n2.752004\n\n\n2.752004\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 1\n\n\n9.403676\n\n\n6.536817\n\n\n6.536817\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 1\n\n\n12.580250\n\n\n16.100000\n\n\n16.100000\n\n\n12.580250\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 1\n\n\n9.403676\n\n\n6.536817\n\n\n6.536817\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 1\n\n\n14.295000\n\n\n10.951237\n\n\n10.951237\n\n\n10.951237\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 1\n\n\n-2.394933\n\n\n-4.111427\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 1\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 1\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 1\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 1\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n5.209976\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 1\n\n\n-2.394933\n\n\n-4.111427\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 1\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 1\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 1\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 1\n\n\n7.933492\n\n\n5.209976\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 1\n\n\n14.295000\n\n\n18.000000\n\n\n16.100000\n\n\n14.295000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 1\n\n\n7.933492\n\n\n5.209976\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 1\n\n\n12.580250\n\n\n9.403676\n\n\n10.951237\n\n\n12.580250\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 1\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 1\n\n\n5.209976\n\n\n7.933492\n\n\n5.209976\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 1\n\n\n1.614404\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 1\n\n\n-3.275187\n\n\n-3.275187\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 1\n\n\n-3.275187\n\n\n-3.275187\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 1\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 1\n\n\n7.933492\n\n\n5.209976\n\n\n5.209976\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 1\n\n\n2.752004\n\n\n0.533683\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 1\n\n\n3.949478\n\n\n1.614404\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 1\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 1\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 1\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 1\n\n\n12.580250\n\n\n16.100000\n\n\n12.580250\n\n\n14.295000\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 1\n\n\n6.536817\n\n\n6.536817\n\n\n9.403676\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 1\n\n\n16.100000\n\n\n12.580250\n\n\n12.580250\n\n\n14.295000\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 1\n\n\n7.933492\n\n\n7.933492\n\n\n10.951237\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 1\n\n\n-0.493001\n\n\n1.614404\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 1\n\n\n3.949478\n\n\n6.536817\n\n\n3.949478\n\n\n6.536817\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 1\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 1\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 1\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 1\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 1\n\n\n3.949478\n\n\n3.949478\n\n\n3.949478\n\n\n6.536817\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 1\n\n\n-0.493001\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 1\n\n\n0.533683\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 1\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 1\n\n\n1.614404\n\n\n1.614404\n\n\n3.949478\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 1\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 1\n\n\n10.951237\n\n\n14.295000\n\n\n10.951237\n\n\n14.295000\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 1\n\n\n7.933492\n\n\n7.933492\n\n\n10.951237\n\n\n7.933492\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 1\n\n\n10.951237\n\n\n10.951237\n\n\n10.951237\n\n\n14.295000\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 1\n\n\n9.403676\n\n\n9.403676\n\n\n12.580250\n\n\n9.403676\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 1\n\n\n-1.468351\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 1\n\n\n2.752004\n\n\n2.752004\n\n\n2.752004\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 1\n\n\n-0.493001\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 1\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 1\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 1\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 1\n\n\n1.614404\n\n\n3.949478\n\n\n3.949478\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 1\n\n\n2.752004\n\n\n2.752004\n\n\n2.752004\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 1\n\n\n-1.468351\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 2\n\n\n-0.493001\n\n\n-0.493001\n\n\n-0.493001\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 2\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 2\n\n\n2.752004\n\n\n2.752004\n\n\n5.209976\n\n\n2.752004\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 2\n\n\n0.533683\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 2\n\n\n9.403676\n\n\n9.403676\n\n\n9.403676\n\n\n12.580250\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 2\n\n\n9.403676\n\n\n12.580250\n\n\n12.580250\n\n\n9.403676\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 2\n\n\n9.403676\n\n\n9.403676\n\n\n9.403676\n\n\n12.580250\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 2\n\n\n10.951237\n\n\n10.951237\n\n\n14.295000\n\n\n10.951237\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 2\n\n\n-2.394933\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 2\n\n\n1.614404\n\n\n1.614404\n\n\n1.614404\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 2\n\n\n-1.468351\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n1.614404\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n1.614404\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 2\n\n\n2.752004\n\n\n5.209976\n\n\n5.209976\n\n\n2.752004\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 2\n\n\n1.614404\n\n\n1.614404\n\n\n1.614404\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 2\n\n\n-2.394933\n\n\n-2.394933\n\n\n-2.394933\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 2\n\n\n-1.468351\n\n\n-1.468351\n\n\n-1.468351\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 2\n\n\n3.949478\n\n\n1.614404\n\n\n1.614404\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 2\n\n\n6.536817\n\n\n3.949478\n\n\n3.949478\n\n\n3.949478\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 2\n\n\n3.949478\n\n\n1.614404\n\n\n1.614404\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 2\n\n\n7.933492\n\n\n7.933492\n\n\n7.933492\n\n\n10.951237\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 2\n\n\n10.951237\n\n\n14.295000\n\n\n14.295000\n\n\n10.951237\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 2\n\n\n7.933492\n\n\n7.933492\n\n\n7.933492\n\n\n10.951237\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 2\n\n\n16.100000\n\n\n12.580250\n\n\n12.580250\n\n\n12.580250\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 2\n\n\n-3.275187\n\n\n-3.275187\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 2\n\n\n0.533683\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 2\n\n\n-2.394933\n\n\n-2.394933\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 2\n\n\n3.949478\n\n\n6.536817\n\n\n5.209976\n\n\n3.949478\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 2\n\n\n0.533683\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 2\n\n\n-3.275187\n\n\n-3.275187\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 2\n\n\n-2.394933\n\n\n-2.394933\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 2\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 2\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 2\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 2\n\n\n6.536817\n\n\n6.536817\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 2\n\n\n12.580250\n\n\n16.100000\n\n\n14.295000\n\n\n12.580250\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 2\n\n\n6.536817\n\n\n6.536817\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 2\n\n\n14.295000\n\n\n10.951237\n\n\n12.580250\n\n\n14.295000\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 2\n\n\n3.949478\n\n\n6.536817\n\n\n5.209976\n\n\n5.209976\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 2\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 2\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 2\n\n\n9.403676\n\n\n6.536817\n\n\n7.933492\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 2\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 2\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 2\n\n\n10.951237\n\n\n14.295000\n\n\n12.580250\n\n\n12.580250\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 2\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 2\n\n\n18.000000\n\n\n14.295000\n\n\n16.100000\n\n\n16.100000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 2\n\n\n6.536817\n\n\n9.403676\n\n\n7.933492\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 2\n\n\n2.752004\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 2\n\n\n-3.275187\n\n\n-1.468351\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 2\n\n\n-3.275187\n\n\n-1.468351\n\n\n-1.468351\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 2\n\n\n2.752004\n\n\n5.209976\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 2\n\n\n9.403676\n\n\n12.580250\n\n\n9.403676\n\n\n10.951237\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 2\n\n\n6.536817\n\n\n9.403676\n\n\n9.403676\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 2\n\n\n9.403676\n\n\n12.580250\n\n\n9.403676\n\n\n10.951237\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 2\n\n\n7.933492\n\n\n10.951237\n\n\n10.951237\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 2\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-2.394933\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 2\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 2\n\n\n1.614404\n\n\n3.949478\n\n\n2.752004\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 2\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 2\n\n\n7.933492\n\n\n10.951237\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 2\n\n\n7.933492\n\n\n10.951237\n\n\n9.403676\n\n\n7.933492\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 2\n\n\n7.933492\n\n\n10.951237\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 2\n\n\n9.403676\n\n\n12.580250\n\n\n10.951237\n\n\n9.403676\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 2\n\n\n-3.275187\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 2\n\n\n-1.468351\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 2\n\n\n1.614404\n\n\n3.949478\n\n\n3.949478\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 2\n\n\n0.533683\n\n\n2.752004\n\n\n0.533683\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 2\n\n\n-3.275187\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 2\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 2\n\n\n-2.394933\n\n\n-0.493001\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 2\n\n\n5.209976\n\n\n2.752004\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 2\n\n\n7.933492\n\n\n5.209976\n\n\n5.209976\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 2\n\n\n5.209976\n\n\n2.752004\n\n\n2.752004\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 0, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 0, PassLoc: 1, DestLoc: 3\n\n\n6.536817\n\n\n9.403676\n\n\n6.536817\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 0, PassLoc: 2, DestLoc: 3\n\n\n9.403676\n\n\n12.580250\n\n\n12.580250\n\n\n10.951237\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 0, PassLoc: 3, DestLoc: 3\n\n\n6.536817\n\n\n9.403676\n\n\n6.536817\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 0, PassLoc: 4, DestLoc: 3\n\n\n18.000000\n\n\n14.295000\n\n\n14.295000\n\n\n16.100000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\nTaxiPos: 1, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 1, PassLoc: 1, DestLoc: 3\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 1, PassLoc: 2, DestLoc: 3\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 1, PassLoc: 3, DestLoc: 3\n\n\n-3.275187\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 1, PassLoc: 4, DestLoc: 3\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 2, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 2, PassLoc: 1, DestLoc: 3\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 2, PassLoc: 2, DestLoc: 3\n\n\n2.752004\n\n\n5.209976\n\n\n3.949478\n\n\n2.752004\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 2, PassLoc: 3, DestLoc: 3\n\n\n-0.493001\n\n\n1.614404\n\n\n0.533683\n\n\n1.614404\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 2, PassLoc: 4, DestLoc: 3\n\n\n-4.111427\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 3, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 3, PassLoc: 1, DestLoc: 3\n\n\n-3.275187\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 3, PassLoc: 2, DestLoc: 3\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 3, PassLoc: 3, DestLoc: 3\n\n\n6.536817\n\n\n3.949478\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 3, PassLoc: 4, DestLoc: 3\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n3.949478\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 4, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 4, PassLoc: 1, DestLoc: 3\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 4, PassLoc: 2, DestLoc: 3\n\n\n10.951237\n\n\n14.295000\n\n\n12.580250\n\n\n10.951237\n\n\n3.580250\n\n\n3.580250\n\n\n\n\nTaxiPos: 4, PassLoc: 3, DestLoc: 3\n\n\n5.209976\n\n\n7.933492\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 4, PassLoc: 4, DestLoc: 3\n\n\n16.100000\n\n\n12.580250\n\n\n14.295000\n\n\n16.100000\n\n\n5.295000\n\n\n5.295000\n\n\n\n\nTaxiPos: 5, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 5, PassLoc: 1, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 5, PassLoc: 2, DestLoc: 3\n\n\n3.949478\n\n\n5.209976\n\n\n3.949478\n\n\n3.949478\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 5, PassLoc: 3, DestLoc: 3\n\n\n0.533683\n\n\n1.614404\n\n\n0.533683\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 5, PassLoc: 4, DestLoc: 3\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-4.111427\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 6, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 6, PassLoc: 1, DestLoc: 3\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-4.111427\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 6, PassLoc: 2, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 6, PassLoc: 3, DestLoc: 3\n\n\n9.403676\n\n\n7.933492\n\n\n9.403676\n\n\n9.403676\n\n\n10.951237\n\n\n0.403676\n\n\n\n\nTaxiPos: 6, PassLoc: 4, DestLoc: 3\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n\n\nTaxiPos: 7, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 7, PassLoc: 1, DestLoc: 3\n\n\n5.209976\n\n\n3.949478\n\n\n5.209976\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n\n\nTaxiPos: 7, PassLoc: 2, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 7, PassLoc: 3, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 7, PassLoc: 4, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 8, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 8, PassLoc: 1, DestLoc: 3\n\n\n10.951237\n\n\n12.580250\n\n\n10.951237\n\n\n10.951237\n\n\n1.951237\n\n\n9.403676\n\n\n\n\nTaxiPos: 8, PassLoc: 2, DestLoc: 3\n\n\n5.209976\n\n\n6.536817\n\n\n5.209976\n\n\n5.209976\n\n\n-3.790024\n\n\n3.949478\n\n\n\n\nTaxiPos: 8, PassLoc: 3, DestLoc: 3\n\n\n18.000000\n\n\n16.100000\n\n\n18.000000\n\n\n18.000000\n\n\n9.000000\n\n\n20.000000\n\n\n\n\nTaxiPos: 8, PassLoc: 4, DestLoc: 3\n\n\n6.536817\n\n\n7.933492\n\n\n6.536817\n\n\n6.536817\n\n\n-2.463183\n\n\n5.209976\n\n\n\n\nTaxiPos: 9, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 9, PassLoc: 1, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 9, PassLoc: 2, DestLoc: 3\n\n\n2.752004\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 9, PassLoc: 3, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 9, PassLoc: 4, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-2.394933\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 10, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 10, PassLoc: 1, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-2.394933\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 10, PassLoc: 2, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n0.533683\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 10, PassLoc: 3, DestLoc: 3\n\n\n2.752004\n\n\n3.949478\n\n\n1.614404\n\n\n2.752004\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 10, PassLoc: 4, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-2.394933\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 11, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 11, PassLoc: 1, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-1.468351\n\n\n-0.493001\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 11, PassLoc: 2, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 11, PassLoc: 3, DestLoc: 3\n\n\n0.533683\n\n\n1.614404\n\n\n1.614404\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 11, PassLoc: 4, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 12, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 12, PassLoc: 1, DestLoc: 3\n\n\n9.403676\n\n\n10.951237\n\n\n7.933492\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 12, PassLoc: 2, DestLoc: 3\n\n\n6.536817\n\n\n7.933492\n\n\n7.933492\n\n\n6.536817\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 12, PassLoc: 3, DestLoc: 3\n\n\n9.403676\n\n\n10.951237\n\n\n7.933492\n\n\n9.403676\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 12, PassLoc: 4, DestLoc: 3\n\n\n7.933492\n\n\n9.403676\n\n\n9.403676\n\n\n7.933492\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 13, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 13, PassLoc: 1, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 13, PassLoc: 2, DestLoc: 3\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 13, PassLoc: 3, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 13, PassLoc: 4, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 14, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 14, PassLoc: 1, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-3.275187\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 14, PassLoc: 2, DestLoc: 3\n\n\n0.533683\n\n\n1.614404\n\n\n0.533683\n\n\n-0.493001\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 14, PassLoc: 3, DestLoc: 3\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n2.752004\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 14, PassLoc: 4, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-2.394933\n\n\n-1.468351\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 15, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 15, PassLoc: 1, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-1.468351\n\n\n-0.493001\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 15, PassLoc: 2, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 15, PassLoc: 3, DestLoc: 3\n\n\n1.614404\n\n\n2.752004\n\n\n1.614404\n\n\n0.533683\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 15, PassLoc: 4, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 16, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 16, PassLoc: 1, DestLoc: 3\n\n\n7.933492\n\n\n9.403676\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 16, PassLoc: 2, DestLoc: 3\n\n\n7.933492\n\n\n9.403676\n\n\n7.933492\n\n\n6.536817\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 16, PassLoc: 3, DestLoc: 3\n\n\n7.933492\n\n\n9.403676\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n-1.066508\n\n\n\n\nTaxiPos: 16, PassLoc: 4, DestLoc: 3\n\n\n9.403676\n\n\n10.951237\n\n\n9.403676\n\n\n7.933492\n\n\n0.403676\n\n\n0.403676\n\n\n\n\nTaxiPos: 17, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 17, PassLoc: 1, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-4.111427\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 17, PassLoc: 2, DestLoc: 3\n\n\n0.533683\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 17, PassLoc: 3, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 17, PassLoc: 4, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 18, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 18, PassLoc: 1, DestLoc: 3\n\n\n-1.468351\n\n\n-0.493001\n\n\n-0.493001\n\n\n-1.468351\n\n\n-10.468351\n\n\n-10.468351\n\n\n\n\nTaxiPos: 18, PassLoc: 2, DestLoc: 3\n\n\n1.614404\n\n\n2.752004\n\n\n2.752004\n\n\n1.614404\n\n\n-7.385596\n\n\n-7.385596\n\n\n\n\nTaxiPos: 18, PassLoc: 3, DestLoc: 3\n\n\n0.533683\n\n\n1.614404\n\n\n-0.493001\n\n\n0.533683\n\n\n-8.466317\n\n\n-8.466317\n\n\n\n\nTaxiPos: 18, PassLoc: 4, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-4.111427\n\n\n-3.275187\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 19, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 19, PassLoc: 1, DestLoc: 3\n\n\n-2.394933\n\n\n-1.468351\n\n\n-3.275187\n\n\n-2.394933\n\n\n-11.394933\n\n\n-11.394933\n\n\n\n\nTaxiPos: 19, PassLoc: 2, DestLoc: 3\n\n\n5.209976\n\n\n3.949478\n\n\n3.949478\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n\n\nTaxiPos: 19, PassLoc: 3, DestLoc: 3\n\n\n7.933492\n\n\n6.536817\n\n\n6.536817\n\n\n7.933492\n\n\n9.403676\n\n\n-1.066508\n\n\n\n\nTaxiPos: 19, PassLoc: 4, DestLoc: 3\n\n\n5.209976\n\n\n3.949478\n\n\n3.949478\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n\n\nTaxiPos: 20, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 20, PassLoc: 1, DestLoc: 3\n\n\n6.536817\n\n\n7.933492\n\n\n5.209976\n\n\n6.536817\n\n\n-2.463183\n\n\n5.209976\n\n\n\n\nTaxiPos: 20, PassLoc: 2, DestLoc: 3\n\n\n9.403676\n\n\n10.951237\n\n\n10.951237\n\n\n9.403676\n\n\n0.403676\n\n\n7.933492\n\n\n\n\nTaxiPos: 20, PassLoc: 3, DestLoc: 3\n\n\n6.536817\n\n\n7.933492\n\n\n5.209976\n\n\n6.536817\n\n\n-2.463183\n\n\n5.209976\n\n\n\n\nTaxiPos: 20, PassLoc: 4, DestLoc: 3\n\n\n18.000000\n\n\n16.100000\n\n\n16.100000\n\n\n18.000000\n\n\n9.000000\n\n\n20.000000\n\n\n\n\nTaxiPos: 21, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 21, PassLoc: 1, DestLoc: 3\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-3.275187\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 21, PassLoc: 2, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 21, PassLoc: 3, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 21, PassLoc: 4, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 22, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 22, PassLoc: 1, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n-1.468351\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 22, PassLoc: 2, DestLoc: 3\n\n\n2.752004\n\n\n3.949478\n\n\n2.752004\n\n\n1.614404\n\n\n-6.247996\n\n\n-6.247996\n\n\n\n\nTaxiPos: 22, PassLoc: 3, DestLoc: 3\n\n\n-0.493001\n\n\n0.533683\n\n\n-0.493001\n\n\n0.533683\n\n\n-9.493001\n\n\n-9.493001\n\n\n\n\nTaxiPos: 22, PassLoc: 4, DestLoc: 3\n\n\n-4.111427\n\n\n-3.275187\n\n\n-4.111427\n\n\n-3.275187\n\n\n-13.111427\n\n\n-13.111427\n\n\n\n\nTaxiPos: 23, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 23, PassLoc: 1, DestLoc: 3\n\n\n-3.275187\n\n\n-2.394933\n\n\n-3.275187\n\n\n-2.394933\n\n\n-12.275187\n\n\n-12.275187\n\n\n\n\nTaxiPos: 23, PassLoc: 2, DestLoc: 3\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 23, PassLoc: 3, DestLoc: 3\n\n\n6.536817\n\n\n5.209976\n\n\n6.536817\n\n\n7.933492\n\n\n-2.463183\n\n\n-2.463183\n\n\n\n\nTaxiPos: 23, PassLoc: 4, DestLoc: 3\n\n\n3.949478\n\n\n2.752004\n\n\n3.949478\n\n\n5.209976\n\n\n-5.050522\n\n\n-5.050522\n\n\n\n\nTaxiPos: 24, PassLoc: 0, DestLoc: 3\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n0.000000\n\n\n\n\nTaxiPos: 24, PassLoc: 1, DestLoc: 3\n\n\n5.209976\n\n\n6.536817\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 24, PassLoc: 2, DestLoc: 3\n\n\n10.951237\n\n\n12.580250\n\n\n10.951237\n\n\n9.403676\n\n\n1.951237\n\n\n1.951237\n\n\n\n\nTaxiPos: 24, PassLoc: 3, DestLoc: 3\n\n\n5.209976\n\n\n6.536817\n\n\n5.209976\n\n\n6.536817\n\n\n-3.790024\n\n\n-3.790024\n\n\n\n\nTaxiPos: 24, PassLoc: 4, DestLoc: 3\n\n\n16.100000\n\n\n14.295000\n\n\n16.100000\n\n\n18.000000\n\n\n7.100000\n\n\n7.100000\n\n\n\n\n\n\n\n\nCreate a model dictionary\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"epsilon\": epsilon,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_taxi\n}\nPublish the trained model on the Hub\nusername = \"cj-mills\"\nrepo_name = \"q-Taxi-v3\"\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)\nIMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (550, 350) to (560, 352) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n{'env_id': 'Taxi-v3', 'max_steps': 99, 'n_training_episodes': 25000, 'n_eval_episodes': 100, 'eval_seed': [16, 54, 165, 177, 191, 191, 120, 80, 149, 178, 48, 38, 6, 125, 174, 73, 50, 172, 100, 148, 146, 6, 25, 40, 68, 148, 49, 167, 9, 97, 164, 176, 61, 7, 54, 55, 161, 131, 184, 51, 170, 12, 120, 113, 95, 126, 51, 98, 36, 135, 54, 82, 45, 95, 89, 59, 95, 124, 9, 113, 58, 85, 51, 134, 121, 169, 105, 21, 30, 11, 50, 65, 12, 43, 82, 145, 152, 97, 106, 55, 31, 85, 38, 112, 102, 168, 123, 97, 21, 83, 158, 26, 80, 63, 5, 81, 32, 11, 28, 148], 'learning_rate': 0.7, 'gamma': 0.95, 'epsilon': 1.0, 'max_epsilon': 1.0, 'min_epsilon': 0.05, 'decay_rate': 0.005, 'qtable': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 2.75200369,  3.94947757,  2.75200369,  3.94947757,  5.20997639,\n        -5.05052243],\n       [ 7.93349184,  9.40367562,  7.93349184,  9.40367562, 10.9512375 ,\n         0.40367562],\n       ...,\n       [10.9512375 , 12.58025   , 10.9512375 ,  9.40367562,  1.9512375 ,\n         1.9512375 ],\n       [ 5.20997639,  6.53681725,  5.20997639,  6.53681725, -3.79002361,\n        -3.79002361],\n       [16.1       , 14.295     , 16.1       , 18.        ,  7.1       ,\n         7.1       ]])}\nPushing repo q-Taxi-v3 to the Hugging Face Hub\n[swscaler @ 0x5936a80] Warning: data is not aligned! This can lead to a speed loss\nUpload file replay.mp4:  27%|##7       | 32.0k/118k [00:00&lt;?, ?B/s]\nYour model is pushed to the hub. You can view your model here: https://huggingface.co/cj-mills/q-Taxi-v3\nLeaderboard\n\n\nLoad from Hub\n\nGo to https://huggingface.co/models?other=q-learning to see the list of all the q-learning saved models.\nSelect one and copy its repo_id.\nUse load_from_hub with the repo_id and the filename.\n\n\nDo not modify this code\nfrom urllib.error import HTTPError\n\nfrom huggingface_hub import hf_hub_download\n\n\ndef load_from_hub(repo_id: str, filename: str) -&gt; str:\n    \"\"\"\n    Download a model from Hugging Face Hub.\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param filename: name of the model zip file from the repository\n    \"\"\"\n    try:\n        from huggingface_hub import cached_download, hf_hub_url\n    except ImportError:\n        raise ImportError(\n            \"You need to install huggingface_hub to use `load_from_hub`. \"\n            \"See https://pypi.org/project/huggingface-hub/ for installation.\"\n        )\n\n    # Get the model from the Hub, download and cache the model on your local disk\n    pickle_model = hf_hub_download(\n        repo_id=repo_id,\n        filename=filename\n    )\n\n    with open(pickle_model, 'rb') as f:\n        downloaded_model_file = pickle.load(f)\n    \n    return downloaded_model_file\nmodel = load_from_hub(repo_id=\"cj-mills/q-Taxi-v3\", filename=\"q-learning.pkl\")\n\nprint(model)\nenv = gym.make(model[\"env_id\"])\n\nevaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n{'env_id': 'Taxi-v3', 'max_steps': 99, 'n_training_episodes': 25000, 'n_eval_episodes': 100, 'eval_seed': [16, 54, 165, 177, 191, 191, 120, 80, 149, 178, 48, 38, 6, 125, 174, 73, 50, 172, 100, 148, 146, 6, 25, 40, 68, 148, 49, 167, 9, 97, 164, 176, 61, 7, 54, 55, 161, 131, 184, 51, 170, 12, 120, 113, 95, 126, 51, 98, 36, 135, 54, 82, 45, 95, 89, 59, 95, 124, 9, 113, 58, 85, 51, 134, 121, 169, 105, 21, 30, 11, 50, 65, 12, 43, 82, 145, 152, 97, 106, 55, 31, 85, 38, 112, 102, 168, 123, 97, 21, 83, 158, 26, 80, 63, 5, 81, 32, 11, 28, 148], 'learning_rate': 0.7, 'gamma': 0.95, 'epsilon': 1.0, 'max_epsilon': 1.0, 'min_epsilon': 0.05, 'decay_rate': 0.005, 'qtable': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 2.75200369,  3.94947757,  2.75200369,  3.94947757,  5.20997639,\n        -5.05052243],\n       [ 7.93349184,  9.40367562,  7.93349184,  9.40367562, 10.9512375 ,\n         0.40367562],\n       ...,\n       [10.9512375 , 12.58025   , 10.9512375 ,  9.40367562,  1.9512375 ,\n         1.9512375 ],\n       [ 5.20997639,  6.53681725,  5.20997639,  6.53681725, -3.79002361,\n        -3.79002361],\n       [16.1       , 14.295     , 16.1       , 18.        ,  7.1       ,\n         7.1       ]])}\n(7.56, 2.706732347314747)\n\n\n\nSome additional challenges\n\nTrain for more steps.\nTry different hyperparameters by looking at what your classmates have done.\nTry using FrozenLake-v1 slippery version and other environments."
  },
  {
    "objectID": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#references",
    "href": "posts/hugging-face-deep-rl-course-notes/part-2/index.html#references",
    "title": "Notes on The Hugging Face Deep RL Class Pt.2",
    "section": "References",
    "text": "References\n\nThe Hugging Face Deep Reinforcement Learning Class\nAn Introduction to Deep Reinforcement Learning"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "",
    "text": "Overview\nSetup Conda Environment\nImport Dependencies\nDownload the Dataset\nInspect the Dataset\nCreate Dataset Parser\nDefine DataLoader Objects\nFinetune the Model\nSave Model Checkpoint\nPerform Inference with Checkpoint\nInspect Raw Model Output\nExport Model to ONNX\nVerify ONNX Inference\nDefine Post-processing Steps\nSummary"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#tutorial-links",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#tutorial-links",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a Mask R-CNN model on a custom dataset using the IceVision library and perform inference with ONNX Runtime.\nGitHub Repository"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#overview",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#overview",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Overview",
    "text": "Overview\nThis tutorial shows how to train a Mask R-CNN model on a custom dataset using the IceVision library. It also demonstrates how to export the trained model to the ONNX format and perform inference using ONNX Runtime.\nWe will use a pre-existing dataset of annotated student ID card images for training. I plan to show how to create a custom dataset from scratch in a future post, including how to annotate the images and prepare them for training.\n\nDataset Source: pytorch-for-information-extraction\n\n\n\n\n\n\nYou can find links to view the training code and run it on Google Colab and Kaggle below.\n\n\n\nJupyter Notebook\nColab\n        Kaggle        \n\n\n\n\nGitHub Repository\nOpen In Colab\nKaggle"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#setup-conda-environment",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#setup-conda-environment",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Setup Conda Environment",
    "text": "Setup Conda Environment\nI recommend using a dedicated virtual environment when working with the IceVision library, as it has specific dependency requirements that can conflict with other libraries or versions. The easiest way to create a virtual environment for IceVision is using Conda. Below are the steps to create a Conda environment and activate it. Be sure to follow these steps in the provided order to ensure the environment works for IceVision.\n\nImportant: IceVision currently only supports Linux/macOS. Try using WSL (Windows Subsystem for Linux) if training locally on Windows.\n\nInstall CUDA Toolkit\nIf you plan to run the training code on your local machine, you might need to install the CUDA Toolkit. CUDA requires an Nvidia GPU, and version 11.1.0 of the toolkit is available at the link below. Google Colab and Kaggle Notebooks already have CUDA installed.\n\nCUDA Toolkit 11.1.0\nCUDA Toolkit Archive\n\nConda environment setup steps\n# create a new conda environment\nconda create --name icevision python==3.8\n# activate the environment\nconda activate icevision\n# install PyTorch and torchvision\npip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n# install mmcv-full\npip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n# install mmdet\npip install mmdet==2.17.0\n# install icevision\npip install icevision==0.11.0\n# install icedata\npip install icedata==0.5.1\n# install setuptools\npip install setuptools==59.5.0\n# install jupyter\npip install jupyter\n# install onnxruntime\npip install onnxruntime\nThe icevision package provides the necessary functionality for data curation, data transforms, and training loops that we will use to train the model. Additionally, the icedata package provides the functionality we need to create a custom parser for reading the dataset.\nColab and Kaggle Setup Requirements\nWhen running the training code on Google Colab and Kaggle Notebooks, it is necessary to uninstall specific packages to avoid conflicts with IceVision and its dependencies. The platform-specific setup steps are at the top of the notebooks linked above. Follow these instructions before running the code to ensure it runs smoothly on these platforms."
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#import-dependencies",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#import-dependencies",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nWe will start by importing the IceVision library and configuring Pandas. When you import the IceVision library for the first time, it will automatically download some additional resources that it needs to function correctly.\nImport IceVision library\n# Import all the necessary modules from the icevision package\nfrom icevision.all import *\nImport and configure Pandas\n# Import the pandas package\nimport pandas as pd\n\n# Set the max column width to None\npd.set_option('max_colwidth', None)\n\n# Set the max number of rows and columns to None\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#download-the-dataset",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#download-the-dataset",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Download the Dataset",
    "text": "Download the Dataset\nThe sample dataset we will use for training is available on GitHub, so all you need to do is clone the repository to access it.\nClone dataset repository\n# Clone the dataset repository from GitHub\n!git clone https://github.com/MbassiJaphet/pytorch-for-information-extraction.git"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#inspect-the-dataset",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#inspect-the-dataset",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Inspect the Dataset",
    "text": "Inspect the Dataset\nAfter the dataset finishes downloading, you can inspect its contents by navigating to the code/datasets/detection/student-id/ subfolder, where you will find the image and annotation files. In this step, we will get the file paths for the images and annotations and inspect one of the training images. That will give us a better understanding of the dataset and its structure.\nDefine path to dataset\n# Set the path to the dataset directory\ndata_dir = Path('./pytorch-for-information-extraction/code/datasets/detection/student-id')\n# Set the dataset name\ndataset_name = data_dir.name\nEach image in the dataset has a corresponding JSON file that contains its annotation data.\nInspect dataset contents\n# Get a list of the files in the dataset directory and display them using a DataFrame\npd.DataFrame(list(data_dir.ls())).head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10001.jpg\n\n\n\n\n1\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10001.json\n\n\n\n\n2\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10002.jpg\n\n\n\n\n3\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10002.json\n\n\n\n\n4\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10003.jpg\n\n\n\n\n\n\nGet image file paths\n# Get the file paths for all the images in the dataset\nfiles = get_image_files(data_dir)\n# Count the number of files\nlen(files)\n150\nInspect one of the training images\n# Import the PIL package\nimport PIL\n\n# Open the first image in the dataset\nimg = PIL.Image.open(files[0]).convert('RGB')\n\n# Print the dimensions of the image\nprint(f\"Image Dims: {img.size}\")\n\n# Display the image\nimg\n    Image Dims: (480, 640)\n\n\n\n\n\nTo make it easier to work with the dataset, we will create a dictionary that maps image names to file paths. The dictionary will allow us to retrieve the file path for a given image efficiently.\nCreate a dictionary that maps image names to file paths\n# Create a dictionary that maps image names to file paths\nimg_dict = {file.name.split('.')[0] : file for file in files}\n\n# Display the first item in the dictionary as a DataFrame\npd.DataFrame(list(img_dict.items())[0]).transpose()\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n10001\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10001.jpg\n\n\n\n\n\n\nGet list of annotation file paths\n# Import the os and glob modules\nimport os\nfrom glob import glob\n# Get a list of the annotation file paths in the dataset directory\nannotation_paths = glob(os.path.join(data_dir, \"*.json\"))\n\n# Display the list of annotation file paths as a DataFrame\npd.DataFrame(annotation_paths).head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10001.json\n\n\n\n\n1\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10002.json\n\n\n\n\n2\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10003.json\n\n\n\n\n3\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10004.json\n\n\n\n\n4\n\n\npytorch-for-information-extraction/code/datasets/detection/student-id/10005.json\n\n\n\n\n\n\nAfter getting the list of annotation file paths, we will create an annotation DataFrame that contains all of the annotation data for the dataset. This DataFrame will allow us to manipulate and query the annotations more easily.\nCreate annotation dataframe\n# Read the JSON files using Pandas and concatenate the resulting dataframes\n# into a single dataframe\ncls_dataframes = (pd.read_json(f, orient='index').transpose() for f in annotation_paths)\nannotation_df = pd.concat(cls_dataframes, ignore_index=False)\n\n# Assign the image file name as the index for each row\nannotation_df['index'] = annotation_df.apply(lambda row: row['imagePath'].split('.')[0], axis=1)\nannotation_df = annotation_df.set_index('index')\n\n# Keep only the rows that have corresponding image files\nannotation_df = annotation_df.loc[list(img_dict.keys())]\n\n# View the first few rows of the dataframe\nannotation_df.head()\n\n\n\n\n\n\n\n\nversion\n\n\nflags\n\n\nshapes\n\n\nlineColor\n\n\nfillColor\n\n\nimagePath\n\n\nimageData\n\n\nimageHeight\n\n\nimageWidth\n\n\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10001\n\n\n3.21.1\n\n\n{}\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[19.190476190476204, 244.76190476190476], [23.0, 233.33333333333331], [385.8571428571428, 132.38095238095238], [400.1428571428571, 135.23809523809524], [468.71428571428567, 353.3333333333333], [466.80952380952374, 362.85714285714283], [97.28571428571428, 478.0952380952381], [81.09523809523807, 474.2857142857143]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n[0, 255, 0, 128]\n\n\n[255, 0, 0, 128]\n\n\n10001.jpg\n\n\n\n\n640\n\n\n480\n\n\n\n\n10002\n\n\n3.21.1\n\n\n{}\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[21.095238095238102, 183.33333333333334], [231.41269841269843, 88.09523809523809], [450.46031746031747, 347.6190476190476], [475.06349206349205, 376.1904761904762], [478.2380952380952, 388.8888888888889], [301.25396825396825, 532.5396825396825], [271.0952380952381, 556.3492063492064], [255.22222222222223, 541.2698412698413], [242.52380952380952, 534.9206349206349], [25.85714285714286, 199.20634920634922]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n[0, 255, 0, 128]\n\n\n[255, 0, 0, 128]\n\n\n10002.jpg\n\n\n\n\n640\n\n\n480\n\n\n\n\n10003\n\n\n3.21.1\n\n\n{}\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[138.23809523809524, 71.42857142857143], [407.7619047619047, 31.428571428571427], [418.2380952380952, 39.047619047619044], [422.04761904761904, 539.047619047619], [407.7619047619047, 552.3809523809524], [112.52380952380952, 519.047619047619], [98.23809523809524, 505.71428571428567]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n[0, 255, 0, 128]\n\n\n[255, 0, 0, 128]\n\n\n10003.jpg\n\n\n\n\n640\n\n\n480\n\n\n\n\n10004\n\n\n3.21.1\n\n\n{}\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[119.20529801324503, 218.54304635761588], [440.3973509933775, 184.7682119205298], [445.0331125827814, 190.72847682119206], [391.3907284768212, 366.2251655629139], [384.7682119205298, 372.18543046357615], [250.33112582781456, 401.3245033112583], [82.11920529801324, 446.3576158940397], [76.82119205298014, 441.72185430463577], [49.66887417218544, 239.73509933774835], [107.28476821192052, 228.47682119205297]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n[0, 255, 0, 128]\n\n\n[255, 0, 0, 128]\n\n\n10004.jpg\n\n\n\n\n640\n\n\n480\n\n\n\n\n10005\n\n\n3.21.1\n\n\n{}\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[41.18840579710144, 218.8405797101449], [41.18840579710144, 209.42028985507244], [52.78260869565216, 201.44927536231882], [224.52173913043475, 142.75362318840578], [359.30434782608694, 89.85507246376811], [367.99999999999994, 92.02898550724638], [462.2028985507246, 275.3623188405797], [369.4492753623188, 348.5507246376811], [199.88405797101444, 472.463768115942], [191.91304347826082, 471.01449275362313]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n[0, 255, 0, 128]\n\n\n[255, 0, 0, 128]\n\n\n10005.jpg\n\n\n\n\n640\n\n\n480\n\n\n\n\n\n\n\nWe can retrieve the annotation data for a specific image file using its name.\nInspect annotation data for sample image\n# Set the file ID for the image we want to inspect\nfile_id = files[56].name.split('.')[0]\n\n# Print the file ID\nfile_id\n'10057'\nThe shapes entry contains the point coordinates to draw the image masks. We will also use this information to construct the associated bounding boxes. This particular entry has point coordinates for two image masks.\n# Get the annotation data for the specified image file\nannotation_df.loc[file_id].to_frame()\n\n\n\n\n\n\n\n\n10057\n\n\n\n\n\n\nversion\n\n\n3.21.1\n\n\n\n\nflags\n\n\n{}\n\n\n\n\nshapes\n\n\n[{‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[4.703296703296701, 186.8131868131868], [172.28571428571428, 91.20879120879121], [177.23076923076923, 89.56043956043956], [183.82417582417582, 92.85714285714285], [260.19780219780216, 161.53846153846152], [248.65934065934067, 173.07692307692307], [99.75824175824175, 273.6263736263736], [88.2197802197802, 280.7692307692308], [83.27472527472527, 280.7692307692308], [35.472527472527474, 225.82417582417582]], ‘shape_type’: ‘polygon’, ‘flags’: {}}, {‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[245.36263736263737, 134.06593406593407], [346.46153846153845, 100.0], [352.5054945054945, 101.64835164835165], [465.1428571428571, 248.9010989010989], [461.8461538461538, 252.1978021978022], [356.35164835164835, 300.5494505494505], [350.3076923076923, 297.8021978021978]], ‘shape_type’: ‘polygon’, ‘flags’: {}}, {‘label’: ‘student_id’, ‘line_color’: None, ‘fill_color’: None, ‘points’: [[33.27472527472527, 489.010989010989], [159.64835164835165, 281.3186813186813], [166.7912087912088, 271.97802197802196], [172.28571428571428, 270.3296703296703], [297.010989010989, 330.2197802197802], [300.3076923076923, 335.16483516483515], [299.2087912087912, 340.65934065934067], [223.38461538461536, 506.5934065934066], [192.6153846153846, 571.4285714285714], [184.9230769230769, 574.1758241758241], [172.28571428571428, 569.2307692307692], [47.56043956043956, 501.0989010989011], [36.021978021978015, 496.15384615384613]], ‘shape_type’: ‘polygon’, ‘flags’: {}}]\n\n\n\n\nlineColor\n\n\n[0, 255, 0, 128]\n\n\n\n\nfillColor\n\n\n[255, 0, 0, 128]\n\n\n\n\nimagePath\n\n\n10057.jpg\n\n\n\n\nimageData\n\n\n\n\n\n\nimageHeight\n\n\n640\n\n\n\n\nimageWidth\n\n\n480\n\n\n\n\n\n\n\nWe need a font file to annotate the images with class labels. We can download one from Google Fonts.\nDownload font file\n# Set the font file name\nfont_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n\n# If the font file doesn't exist, download it\nif not os.path.exists(font_file): \n    !wget https://fonts.gstatic.com/s/roboto/v30/$font_file\nAnnotate sample image\n# Import the ImageDraw class from the PIL package\nfrom PIL import ImageDraw\n# Open the image file\nimg = PIL.Image.open(img_dict[file_id]).convert('RGB')\n\n# Get the dimensions of the image\nwidth, height = img.size\n\n# Create a copy of the image to annotate\nannotated_img = img.copy()\n\n# Create a blank image to store the mask\nmask_img = PIL.Image.new('L', img.size, 0)\n\n# Create a drawing object for the annotated image\ndraw = ImageDraw.Draw(annotated_img)\n\n# Set the font size for the object labels\nfnt_size = 25\n\n# Get the annotation data for the specified image\nannotation = annotation_df.loc[file_id]\n\n# Iterate through annotations for sample image\nfor i in range(len(annotation['shapes'])):\n    \n    # Extract mask polygon coords\n    points = np.array(annotation['shapes'][i]['points'])\n    \n    # Extract bounding box coords\n    x_min, y_min = points.min(axis=0)\n    x_max, y_max = points.max(axis=0)\n    \n    # Draw bounding box on sample image\n    shape = (x_min, y_min, x_max, y_max)\n    draw.rectangle(shape, outline='red')\n\n    # Draw segmentation mask on sample image\n    xy = [(p[0],p[1]) for p in points]\n    ImageDraw.Draw(annotated_img, 'RGBA').polygon(xy, fill=(255, 0, 0, 125), outline =\"red\")\n    \n    # Draw segmentation mask on blank image\n    ImageDraw.Draw(mask_img, 'L').polygon(xy, fill=(255))\n    \n    # Draw object label on sample image\n    fnt = PIL.ImageFont.truetype(font_file, fnt_size)\n    label = annotation['shapes'][i]['label']\n    draw.multiline_text((x_min, y_min-fnt_size-5), f\"{label}\", font=fnt, fill='red')\n\n# Print the dimensions of the annotated image\nprint(annotated_img.size)\n\n# Show the annotated image\nannotated_img\n(480, 640)\n\n\n\n\n\nDisplay segmentation mask\nmask_img\n\n\n\n\n\nWe need to provide IceVision with a class map that maps index values to unique class names.\nCreate a class map\n# Explode the 'shapes' column in the annotation_df dataframe\n# Convert the resulting series to a dataframe and rename the 'shapes' column to 'shapes'\n# Apply the pandas Series function to the 'shapes' column of the dataframe\nshapes_df = annotation_df['shapes'].explode().to_frame().shapes.apply(pd.Series)\n# View the first few rows of the resulting dataframe\nshapes_df.head()\n\n\n\n\n\n\n\n\nlabel\n\n\nline_color\n\n\nfill_color\n\n\npoints\n\n\nshape_type\n\n\nflags\n\n\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10001\n\n\nstudent_id\n\n\nNone\n\n\nNone\n\n\n[[19.190476190476204, 244.76190476190476], [23.0, 233.33333333333331], [385.8571428571428, 132.38095238095238], [400.1428571428571, 135.23809523809524], [468.71428571428567, 353.3333333333333], [466.80952380952374, 362.85714285714283], [97.28571428571428, 478.0952380952381], [81.09523809523807, 474.2857142857143]]\n\n\npolygon\n\n\n{}\n\n\n\n\n10002\n\n\nstudent_id\n\n\nNone\n\n\nNone\n\n\n[[21.095238095238102, 183.33333333333334], [231.41269841269843, 88.09523809523809], [450.46031746031747, 347.6190476190476], [475.06349206349205, 376.1904761904762], [478.2380952380952, 388.8888888888889], [301.25396825396825, 532.5396825396825], [271.0952380952381, 556.3492063492064], [255.22222222222223, 541.2698412698413], [242.52380952380952, 534.9206349206349], [25.85714285714286, 199.20634920634922]]\n\n\npolygon\n\n\n{}\n\n\n\n\n10003\n\n\nstudent_id\n\n\nNone\n\n\nNone\n\n\n[[138.23809523809524, 71.42857142857143], [407.7619047619047, 31.428571428571427], [418.2380952380952, 39.047619047619044], [422.04761904761904, 539.047619047619], [407.7619047619047, 552.3809523809524], [112.52380952380952, 519.047619047619], [98.23809523809524, 505.71428571428567]]\n\n\npolygon\n\n\n{}\n\n\n\n\n10004\n\n\nstudent_id\n\n\nNone\n\n\nNone\n\n\n[[119.20529801324503, 218.54304635761588], [440.3973509933775, 184.7682119205298], [445.0331125827814, 190.72847682119206], [391.3907284768212, 366.2251655629139], [384.7682119205298, 372.18543046357615], [250.33112582781456, 401.3245033112583], [82.11920529801324, 446.3576158940397], [76.82119205298014, 441.72185430463577], [49.66887417218544, 239.73509933774835], [107.28476821192052, 228.47682119205297]]\n\n\npolygon\n\n\n{}\n\n\n\n\n10005\n\n\nstudent_id\n\n\nNone\n\n\nNone\n\n\n[[41.18840579710144, 218.8405797101449], [41.18840579710144, 209.42028985507244], [52.78260869565216, 201.44927536231882], [224.52173913043475, 142.75362318840578], [359.30434782608694, 89.85507246376811], [367.99999999999994, 92.02898550724638], [462.2028985507246, 275.3623188405797], [369.4492753623188, 348.5507246376811], [199.88405797101444, 472.463768115942], [191.91304347826082, 471.01449275362313]]\n\n\npolygon\n\n\n{}\n\n\n\n\n\n\n# Create a list of unique labels from the 'label' column of the shapes_df dataframe\nlabels = shapes_df['label'].unique().tolist()\n# Print the list of labels\nlabels\n['student_id']\n# Create a ClassMap object with the list of labels\nclass_map = ClassMap(labels)\n# Print the ClassMap object\nclass_map\n&lt;ClassMap: {'background': 0, 'student_id': 1}&gt;"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#create-dataset-parser",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#create-dataset-parser",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Create Dataset Parser",
    "text": "Create Dataset Parser\nTo create a custom dataset parser for instance segmentation, we can use the template for an instance segmentation record and the template for an instance segmentation parser.\nView template for an instance segmentation record\n# Create an InstanceSegmentationRecord object\ntemplate_record = InstanceSegmentationRecord()\n# Print the InstanceSegmentationRecord object\ntemplate_record\nBaseRecord\n\ncommon: \n    - Image size None\n    - Filepath: None\n    - Img: None\n    - Record ID: None\ndetection: \n    - Class Map: None\n    - Labels: []\n    - BBoxes: []\n    - masks: []\n    - mask_array: None\nView template for an instance segmentation parser\n# Generate a template record for an instance segmentation dataset using the InstanceSegmentationRecord object\nParser.generate_template(template_record)\nclass MyParser(Parser):\n    def __init__(self, template_record):\n        super().__init__(template_record=template_record)\n    def __iter__(self) -&gt; Any:\n    def __len__(self) -&gt; int:\n    def record_id(self, o: Any) -&gt; Hashable:\n    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n        record.set_img_size(&lt;ImgSize&gt;)\n        record.set_filepath(&lt;Union[str, Path]&gt;)\n        record.detection.set_class_map(&lt;ClassMap&gt;)\n        record.detection.add_labels(&lt;Sequence[Hashable]&gt;)\n        record.detection.add_bboxes(&lt;Sequence[BBox]&gt;)\n        record.detection.add_masks(&lt;Sequence[Mask]&gt;)\nDefine custom parser class\nclass StudentIDParser(Parser):\n    # Initialize a StudentIDParser object\n    def __init__(self, template_record, annotations_df, img_dict, class_map):\n        # Call the __init__ method of the parent class\n        super().__init__(template_record=template_record)\n        \n        # Set the instance variables to the values of the corresponding arguments\n        self.img_dict = img_dict\n        self.df = annotations_df\n        self.class_map = class_map\n\n    # Return an iterator over the rows of the DataFrame\n    def __iter__(self):\n        for o in self.df.itertuples(): yield o\n\n    # Return the number of rows in the DataFrame\n    def __len__(self):\n        return len(self.df)\n\n    # Return the index of the current row of the DataFrame\n    def record_id(self, o: Any) -&gt; Hashable:\n        return o.Index\n\n    # Return the image width and height\n    def image_width_height(self, o) -&gt; Tuple[int, int]:\n        return self._size[:2]\n\n    # Parse the data from the DataFrame and populate the record object\n    def parse_fields(self, o, record, is_new):\n        \n        # Get the file path of the image from the img_dict dictionary\n        filepath = self.img_dict[o.Index]\n        \n        # Open the image and get its width and height\n        width, height = PIL.Image.open(filepath).convert('RGB').size\n        \n        # Set the image size and file path of the record object\n        record.set_img_size([width, height])\n        record.set_filepath(Path(filepath))\n        \n        # Set the class map of the record object's detection attribute\n        record.detection.set_class_map(self.class_map)\n                \n        # Initialize empty lists for labels, bounding boxes, and masks\n        labels = []\n        bbox_list = []\n        mask_list = []\n        \n        # Iterate over the shapes in the current row of the DataFrame\n        for i in range(len(o.shapes)):\n            # Get the points of the shape\n            points = np.array(o.shapes[i]['points'])\n\n            # Calculate the minimum and maximum x- and y-coordinates of the points\n            x_min, y_min = points.min(axis=0)\n            x_max, y_max = points.max(axis=0)\n            \n            # Add the label to the labels list\n            labels.append(o.shapes[i]['label'])\n            \n            # Create a bounding box object from the coordinates and add it to the bbox_list\n            bbox_list.append(BBox.from_xyxy(x_min, y_min, x_max, y_max))\n            \n            # Create a mask image and draw the shape on it\n            mask_img = PIL.Image.new('L', (width, height), 0)\n            xy = [(p[0],p[1]) for p in points]\n            ImageDraw.Draw(mask_img, 'L').polygon(xy, fill=(1))\n            # Convert the mask image to a numpy array and add it to the mask_list\n            mask_array = np.array(mask_img).clip(0,1)\n            mask_list.append(MaskArray(mask_array))\n        \n        # Add the labels, bounding boxes, and masks to the record object\n        record.detection.add_labels(labels)\n        record.detection.add_bboxes(bbox_list)\n        record.detection.add_masks(mask_list)\nWe can then create a parser object using the custom parser class.\nCreate a custom parser object\n# Create a StudentIDParser object\nparser = StudentIDParser(template_record, annotation_df, img_dict, class_map)\n\n# Return the length of the parser object, which is the number of rows in the DataFrame\nlen(parser)\n150\nWe use the parser object to parse annotations and create records.\nParse annotations to create records\n# Create a RandomSplitter object\ndata_splitter = RandomSplitter([0.8, 0.2])\n\n# Use the RandomSplitter to split the data into training and validation sets\ntrain_records, valid_records = parser.parse(data_splitter)\n# train_records, valid_records = parser.parse(data_splitter, cache_filepath=f'{dataset_name}-cache.pkl')\nLet’s save the class labels to use later during inference.\nExport class labels\n# Import the json module\nimport json\n\n# Create a dictionary containing the class labels\nclass_labels = {\"classes\": parser.class_map.get_classes()}\n\n# Generate a file name for the class labels file\nclass_labels_file_name = f\"{data_dir.name}-classes.json\"\n\n# Open the class labels file for writing and write the class labels to it\nwith open(class_labels_file_name, \"w\") as write_file:\n    json.dump(class_labels, write_file)\n    \n# Return the class labels and file name\nclass_labels, class_labels_file_name\n({'classes': ['background', 'student_id']}, 'student-id-classes.json')\nFinally, we can inspect the training records to ensure the parser works correctly.\nInspect training records\n# Get the first element of the train_records object\ntrain_records[0]\nBaseRecord\n\ncommon: \n    - Image size [640, 480]\n    - Filepath: pytorch-for-information-extraction/code/datasets/detection/student-id/10115.jpg\n    - Img: None\n    - Record ID: 10115\ndetection: \n    - Class Map: &lt;ClassMap: {'background': 0, 'student_id': 1}&gt;\n    - Labels: [1, 1]\n    - BBoxes: [&lt;BBox (xmin:281.57142857142856, ymin:1.428571428571428, xmax:504.42857142857133, ymax:326.4285714285714)&gt;, &lt;BBox (xmin:185.1428571428571, ymin:124.28571428571428, xmax:382.99999999999994, ymax:448.57142857142856)&gt;]\n    - masks: [&lt;icevision.core.mask.MaskArray object at 0x7f521f95df10&gt;, &lt;icevision.core.mask.MaskArray object at 0x7f521f95da90&gt;]\n    - mask_array: None\n# Annotate the first sample of the train_records object\nshow_record(train_records[0], figsize = (10,10), display_label=True )\n\n\n\n\n\n# Annotate the second to fourth samples of the train_records list\nshow_records(train_records[1:4], ncols=3,display_label=True)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#define-dataloader-objects",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#define-dataloader-objects",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Define DataLoader Objects",
    "text": "Define DataLoader Objects\nTo define DataLoader objects for our task, we must first set the desired input resolution for the model.\nDefine input resolution\n# Set the image size to 512\nimage_size = 512\n\n# Set the presize to 1024\npresize = 1024\nNext, we create a list of transformations to apply to the input images, such as resizing, padding, and normalization. IceVision also applies augmentations, such as horizontal flipping, to improve model performance.\nDefine Transforms\n# Define the data transforms for the training and validation sets\ntrain_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=presize), tfms.A.Normalize()])\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])\nWe then get the mean and standard deviation of the dataset used to train the original model to normalize the input images.\nGet normalization stats\n# Get the mean of the Normalize() transformation\nmean = tfms.A.Normalize().mean\n\n# Get the standard deviation of the Normalize() transformation\nstd = tfms.A.Normalize().std\n\n# Print the mean and standard deviation\nmean, std\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\nNext, we create dataset objects for the training and validation datasets using the defined transforms and normalization stats.\nDefine Datasets\n# Create a Dataset object for the training set\ntrain_ds = Dataset(train_records, train_tfms)\n\n# Create a Dataset object for the validation set\nvalid_ds = Dataset(valid_records, valid_tfms)\n\n# Return the Dataset objects\ntrain_ds, valid_ds\n(&lt;Dataset with 120 items&gt;, &lt;Dataset with 30 items&gt;)\nWe can apply the image augmentations to a sample training image to demonstrate the effects of data augmentation.\nApply augmentations to a training sample\n# Get three samples from the training set\nsamples = [train_ds[0] for _ in range(3)]\n\n# Show the samples\nshow_samples(samples, ncols=3)\n\n\n\n\n\nOnce the datasets are defined, we can specify Mask R-CNN as the model type for training and inference.\nDefine model type\n# Set the model type to Mask R-CNN\nmodel_type = models.torchvision.mask_rcnn\nDefine backbone\n# Create a ResNet50-FPN backbone for the model\nbackbone = model_type.backbones.resnet50_fpn()\nDefine batch size\n# Set the batch size\nbs = 4\n\nNote: Adjust the batch size based on the available GPU memory.\n\nFinally, we create DataLoader objects for the training and validation datasets using the defined batch size. We use these objects to load batches of data for training and evaluation.\nDefine DataLoaders\n# Create a DataLoader for the training set\ntrain_dl = model_type.train_dl(train_ds, batch_size=bs, num_workers=2, shuffle=True)\n\n# Create a DataLoader for the validation set\nvalid_dl = model_type.valid_dl(valid_ds, batch_size=bs, num_workers=2, shuffle=False)\n\nNote: Be careful when increasing the number of workers. There is a bug that significantly increases system memory usage with more workers.\n\n# Get the first mini-batch from the validation set\nvalid_batch = first(valid_dl)\n# Unpack the data from the first mini-batch of the validation set\n(valid_images, valid_labels), valid_records = valid_batch\n# Show a mini-batch of data from the validation set\nmodel_type.show_batch(first(valid_dl), ncols=4)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#finetune-the-model",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#finetune-the-model",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Finetune the Model",
    "text": "Finetune the Model\nTo finetune the Mask-RCNN model, we must first instantiate the model and define metrics to track during training.\nInstantiate the model\n# Create a Mask R-CNN model\nmodel = model_type.model(backbone=backbone, num_classes=parser.class_map.num_classes)\nDefine metrics\n# Define a list of metrics to evaluate the model\nmetrics = [COCOMetric(metric_type=COCOMetricType.mask)]\nWe can then create a Learner object to find the learning rate and handle the training loop.\nDefine Learner object\n# Create a fastai learner object to train and evaluate the Mask R-CNN model\nlearn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)\nFind learning rate\n# Use the learning rate finder to find a good learning rate for the Mask R-CNN model\nsuggested_lrs = learn.lr_find()\n\n\n\n\n\nDefine learning rate\n# Use the optimal learning rate identified by the learning rate finder\nlr = suggested_lrs.valley*3\nlr\n    0.00039547700725961477\nDefine number of epochs\n# Set the number of epochs to train the Mask R-CNN model\nepochs = 60\nAfter defining the training parameters, we can finetune the model by training it on the training dataset and evaluating it on the validation dataset.\nFinetune model\n# Train the Mask R-CNN model\nlearn.fine_tune(epochs, lr, freeze_epochs=1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nCOCOMetric\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.526101\n\n\n1.136230\n\n\n0.000000\n\n\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nCOCOMetric\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.113099\n\n\n0.948657\n\n\n0.000000\n\n\n00:10\n\n\n\n\n1\n\n\n0.879729\n\n\n0.561887\n\n\n0.000000\n\n\n00:08\n\n\n\n\n2\n\n\n0.661906\n\n\n0.404916\n\n\n0.000000\n\n\n00:08\n\n\n\n\n3\n\n\n0.528782\n\n\n0.345191\n\n\n0.000000\n\n\n00:08\n\n\n\n\n4\n\n\n0.464688\n\n\n0.352018\n\n\n0.000000\n\n\n00:08\n\n\n\n\n5\n\n\n0.418978\n\n\n0.320372\n\n\n0.000000\n\n\n00:08\n\n\n\n\n6\n\n\n0.376626\n\n\n0.295002\n\n\n0.000000\n\n\n00:08\n\n\n\n\n7\n\n\n0.340059\n\n\n0.309611\n\n\n0.000000\n\n\n00:08\n\n\n\n\n8\n\n\n0.316866\n\n\n0.267806\n\n\n0.000000\n\n\n00:08\n\n\n\n\n9\n\n\n0.295634\n\n\n0.275406\n\n\n0.000000\n\n\n00:08\n\n\n\n\n10\n\n\n0.282974\n\n\n0.263774\n\n\n0.000000\n\n\n00:08\n\n\n\n\n11\n\n\n0.275979\n\n\n0.254877\n\n\n0.000000\n\n\n00:08\n\n\n\n\n12\n\n\n0.266656\n\n\n0.237095\n\n\n0.000000\n\n\n00:08\n\n\n\n\n13\n\n\n0.260437\n\n\n0.239515\n\n\n0.000000\n\n\n00:08\n\n\n\n\n14\n\n\n0.245464\n\n\n0.222481\n\n\n0.000000\n\n\n00:08\n\n\n\n\n15\n\n\n0.237604\n\n\n0.221115\n\n\n0.000000\n\n\n00:08\n\n\n\n\n16\n\n\n0.233022\n\n\n0.245855\n\n\n0.000000\n\n\n00:08\n\n\n\n\n17\n\n\n0.224769\n\n\n0.277205\n\n\n0.000000\n\n\n00:08\n\n\n\n\n18\n\n\n0.214686\n\n\n0.242116\n\n\n0.000000\n\n\n00:08\n\n\n\n\n19\n\n\n0.207307\n\n\n0.229408\n\n\n0.000000\n\n\n00:08\n\n\n\n\n20\n\n\n0.202318\n\n\n0.217142\n\n\n0.000000\n\n\n00:08\n\n\n\n\n21\n\n\n0.190885\n\n\n0.209915\n\n\n0.000000\n\n\n00:08\n\n\n\n\n22\n\n\n0.190565\n\n\n0.202563\n\n\n0.000000\n\n\n00:08\n\n\n\n\n23\n\n\n0.189903\n\n\n0.199876\n\n\n0.000000\n\n\n00:09\n\n\n\n\n24\n\n\n0.186624\n\n\n0.213171\n\n\n0.000000\n\n\n00:08\n\n\n\n\n25\n\n\n0.178527\n\n\n0.207296\n\n\n0.000000\n\n\n00:08\n\n\n\n\n26\n\n\n0.179156\n\n\n0.246541\n\n\n0.000000\n\n\n00:08\n\n\n\n\n27\n\n\n0.174455\n\n\n0.186960\n\n\n0.000000\n\n\n00:08\n\n\n\n\n28\n\n\n0.162381\n\n\n0.183744\n\n\n0.000000\n\n\n00:08\n\n\n\n\n29\n\n\n0.157665\n\n\n0.181943\n\n\n0.000000\n\n\n00:08\n\n\n\n\n30\n\n\n0.152987\n\n\n0.181777\n\n\n0.000000\n\n\n00:08\n\n\n\n\n31\n\n\n0.149362\n\n\n0.163883\n\n\n0.000000\n\n\n00:08\n\n\n\n\n32\n\n\n0.143986\n\n\n0.185762\n\n\n0.000000\n\n\n00:08\n\n\n\n\n33\n\n\n0.147537\n\n\n0.170019\n\n\n0.000000\n\n\n00:08\n\n\n\n\n34\n\n\n0.139731\n\n\n0.165259\n\n\n0.000000\n\n\n00:08\n\n\n\n\n35\n\n\n0.136252\n\n\n0.166419\n\n\n0.000000\n\n\n00:08\n\n\n\n\n36\n\n\n0.134750\n\n\n0.165301\n\n\n0.000000\n\n\n00:08\n\n\n\n\n37\n\n\n0.131968\n\n\n0.157560\n\n\n0.000000\n\n\n00:08\n\n\n\n\n38\n\n\n0.129044\n\n\n0.162093\n\n\n0.000000\n\n\n00:08\n\n\n\n\n39\n\n\n0.121755\n\n\n0.159642\n\n\n0.000000\n\n\n00:08\n\n\n\n\n40\n\n\n0.118404\n\n\n0.154801\n\n\n0.000000\n\n\n00:08\n\n\n\n\n41\n\n\n0.118744\n\n\n0.157296\n\n\n0.000000\n\n\n00:08\n\n\n\n\n42\n\n\n0.113328\n\n\n0.156315\n\n\n0.000000\n\n\n00:08\n\n\n\n\n43\n\n\n0.114840\n\n\n0.153378\n\n\n0.000000\n\n\n00:08\n\n\n\n\n44\n\n\n0.113884\n\n\n0.146661\n\n\n0.000000\n\n\n00:08\n\n\n\n\n45\n\n\n0.112758\n\n\n0.162629\n\n\n0.000000\n\n\n00:08\n\n\n\n\n46\n\n\n0.110688\n\n\n0.157740\n\n\n0.000000\n\n\n00:08\n\n\n\n\n47\n\n\n0.107804\n\n\n0.156558\n\n\n0.000000\n\n\n00:08\n\n\n\n\n48\n\n\n0.104166\n\n\n0.155466\n\n\n0.000000\n\n\n00:08\n\n\n\n\n49\n\n\n0.102735\n\n\n0.159387\n\n\n0.000000\n\n\n00:08\n\n\n\n\n50\n\n\n0.100563\n\n\n0.157396\n\n\n0.000000\n\n\n00:08\n\n\n\n\n51\n\n\n0.101158\n\n\n0.151178\n\n\n0.000000\n\n\n00:08\n\n\n\n\n52\n\n\n0.100735\n\n\n0.148203\n\n\n0.000000\n\n\n00:08\n\n\n\n\n53\n\n\n0.098154\n\n\n0.165008\n\n\n0.000000\n\n\n00:07\n\n\n\n\n54\n\n\n0.098391\n\n\n0.163613\n\n\n0.000000\n\n\n00:07\n\n\n\n\n55\n\n\n0.100913\n\n\n0.161389\n\n\n0.000000\n\n\n00:07\n\n\n\n\n56\n\n\n0.098609\n\n\n0.155590\n\n\n0.000000\n\n\n00:07\n\n\n\n\n57\n\n\n0.098285\n\n\n0.154783\n\n\n0.000000\n\n\n00:07\n\n\n\n\n58\n\n\n0.097906\n\n\n0.156959\n\n\n0.000000\n\n\n00:07\n\n\n\n\n59\n\n\n0.097475\n\n\n0.160197\n\n\n0.000000\n\n\n00:07\n\n\n\n\n\n\n\nFinally, we can display the results of the finetuned model on the validation set.\nShow results on validation set\n# Show the results of the predictions on the validation set\nmodel_type.show_results(model, valid_ds, detection_threshold=.5)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#save-model-checkpoint",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#save-model-checkpoint",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Save Model Checkpoint",
    "text": "Save Model Checkpoint\nWe can save the trained PyTorch model and use it to finetune the model further or perform inference using IceVision in the future.\nDefine model checkpoint file path\n# Define a path to save the trained model\ncheckpoint_path = f\"{data_dir.name}-{type(model).__name__}.pth\"\ncheckpoint_path\n'student-id-MaskRCNN.pth'\nSave model checkpoint\n# Save the trained model to the specified path\ntorch.save(model.state_dict(), checkpoint_path)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#perform-inference-with-checkpoint",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#perform-inference-with-checkpoint",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Perform Inference with Checkpoint",
    "text": "Perform Inference with Checkpoint\nWe must first load the class labels and the model checkpoint to use the saved model for inference.\nLoad class labels\n# Open the class labels file\nwith open(class_labels_file_name, \"r\") as read_file:\n    # Load the class labels from the file\n    classes = json.loads(read_file.read())\n\n# Print the list of class labels\nprint(classes['classes'])\n['background', 'student_id']\nLoad model checkpoint\n# Load the model from the checkpoint file\ncheckpoint_and_model = models.model_from_checkpoint(\n    # Path to the checkpoint file\n    checkpoint_path, \n    # Name of the model class\n    model_name='torchvision.mask_rcnn', \n    # Name of the backbone to use for the model\n    backbone_name='resnet50_fpn',\n    # Image size for the model\n    img_size=512, \n    # List of class labels for the model\n    classes=classes['classes'],\n)\nVerify class map\n# View the class map for the model\ncheckpoint_and_model[\"class_map\"]\n    &lt;ClassMap: {'background': 0, 'student_id': 1}&gt;\nGet model and device\n# Retrieve the trained model from the dictionary\nmodel = checkpoint_and_model[\"model\"]\n# Retrieve the inference device from the dictionary\ndevice=next(model.parameters()).device\ndevice\ndevice(type='cpu')\nThen, we define the inference preprocessing steps and select a test image.\nDefine inference preprocessing steps\n# Set the image size from the checkpoint dictionary\nimg_size = checkpoint_and_model[\"img_size\"]\n\n# Create the validation transforms: resize and pad to match img_size, normalize\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(img_size), tfms.A.Normalize()])\nSelect a test image\n# Open the 10th file in the files list\ntest_img = open_img(files[9])\n# Display the test image\ntest_img\n\n\n\n\n\nWe create an inference DataLoader and use it to perform inference.\nDefine inference dataloader\n# Create the inference transforms: normalize\ninfer_tfms = tfms.A.Adapter([tfms.A.Normalize()])\n\n# Create an inference dataset from the test image\ninfer_ds = Dataset.from_images([test_img], infer_tfms)\n\n# Create a data loader for the inference dataset\ninfer_dl = model_type.infer_dl(infer_ds, batch_size=1, shuffle=False)\nPerform inference\n# Make predictions on the test image using the model\npreds = model_type.predict_from_dl(model, infer_dl, keep_images=True)\nOnce the inference is complete, we can inspect the source image information and the model prediction.\nInspect source image\n# Access the ground truth information for the first prediction\npreds[0].ground_truth\n    BaseRecord\n    \n    common: \n        - Record ID: 0\n        - Img: 480x640x3 &lt;np.ndarray&gt; Image\n        - Image size ImgSize(width=480, height=640)\n    detection: \n        - Class Map: None\nInspect model prediction\n# Access the predicted information for the first prediction\npreds[0].pred\n    BaseRecord\n    \n    common: \n        - Record ID: 0\n        - Img: 480x640x3 &lt;np.ndarray&gt; Image\n        - Image size ImgSize(width=480, height=640)\n    detection: \n        - Scores: [0.9997528 0.9996158]\n        - BBoxes: [&lt;BBox (xmin:34.445037841796875, ymin:300.6217041015625, xmax:468.1103210449219, ymax:625.0744018554688)&gt;, &lt;BBox (xmin:13.5184326171875, ymin:21.899276733398438, xmax:405.33990478515625, ymax:296.6339111328125)&gt;]\n        - masks: []\n        - mask_array: &lt;icevision.core.mask.MaskArray object at 0x7f5115cc7280&gt;\n        - Class Map: None\n        - Labels: [1, 1]\nFinally, we annotate the image with the model prediction.\nAnnotate image with model prediction\n# Display the predictions for the test image\nshow_preds(preds=preds)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#inspect-raw-model-output",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#inspect-raw-model-output",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Inspect Raw Model Output",
    "text": "Inspect Raw Model Output\nNext, we’ll inspect the raw model output to determine the required post-processing steps when using ONNX Runtime. We first define a method to convert a PIL Image to a normalized Pytorch Tensor.\nDefine method to convert a PIL Image to a Pytorch Tensor\ndef img_to_tensor(img:PIL.Image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    \"\"\"\n    Converts a PIL image to a PyTorch tensor.\n    \n    Args:\n        img: The input PIL image.\n        mean: The mean values for normalization.\n        std: The standard deviation values for normalization.\n    \n    Returns:\n        The normalized tensor.\n    \"\"\"\n    # Convert image to tensor\n    img_tensor = torch.Tensor(np.array(img)).permute(2, 0, 1)\n    # Scale pixels values from [0,255] to [0,1]\n    scaled_tensor = img_tensor.float().div_(255)\n    # Prepare normalization tensors\n    mean_tensor = tensor(mean).view(1,1,-1).permute(2, 0, 1)\n    std_tensor = tensor(std).view(1,1,-1).permute(2, 0, 1)\n    # Normalize tensor    \n    normalized_tensor = (scaled_tensor - mean_tensor) / std_tensor\n    # Batch tensor\n    return normalized_tensor.unsqueeze(dim=0)\nConvert image to a normalized tensor\n# Convert the test image to a tensor\ninput_tensor = img_to_tensor(test_img)\n\n# Display the shape of the tensor\ninput_tensor.shape\ntorch.Size([1, 3, 640, 480])\nThen, we’ll inspect the raw model output and benchmark PyTorch CPU inference to compare with ONNX Runtime.\nInspect raw model output\n# Make predictions on the test image using the model\nwith torch.no_grad():\n    preds = model(input_tensor)\n\n# display the predictions\npreds\n    [{'boxes': tensor([[ 34.4450, 300.6217, 468.1104, 625.0743],\n              [ 13.5184,  21.8993, 405.3399, 296.6339]]),\n      'labels': tensor([1, 1]),\n      'scores': tensor([0.9998, 0.9996]),\n      'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n                [0., 0., 0.,  ..., 0., 0., 0.],\n                [0., 0., 0.,  ..., 0., 0., 0.],\n                ...,\n                [0., 0., 0.,  ..., 0., 0., 0.],\n                [0., 0., 0.,  ..., 0., 0., 0.],\n                [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n​      \n​              [[[0., 0., 0.,  ..., 0., 0., 0.],\n​                [0., 0., 0.,  ..., 0., 0., 0.],\n​                [0., 0., 0.,  ..., 0., 0., 0.],\n​                ...,\n​                [0., 0., 0.,  ..., 0., 0., 0.],\n​                [0., 0., 0.,  ..., 0., 0., 0.],\n​                [0., 0., 0.,  ..., 0., 0., 0.]]]])}]\nThe model output stores predictions for bounding boxes, labels, confidence scores, and image masks in separate tensors.\nBenckmark PyTorch CPU inference\n%%timeit\n# Measure the time it takes to make predictions on the input tensor\nwith torch.no_grad(): model(input_tensor)\n432 ms ± 845 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#export-model-to-onnx",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#export-model-to-onnx",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Export Model to ONNX",
    "text": "Export Model to ONNX\nTo export the trained model to ONNX, we first need to define an ONNX file name. Then, we can use PyTorch’s built-in conversion method to export the trained model to ONNX. We can use the exported model with other applications or frameworks that support ONNX models.\nDefine ONNX file name\n# Create a filename for the ONNX model\nonnx_file_name = f\"{dataset_name}-{type(model).__name__}.onnx\"\n\n# Display the filename\nonnx_file_name\n'student-id-MaskRCNN.onnx'\nExport trained model to ONNX\n# Export the PyTorch model to ONNX format\ntorch.onnx.export(model,\n                  input_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=12,\n                  do_constant_folding=True,\n                  input_names = ['input'],\n                  output_names = ['boxes', 'labels', 'scores', 'masks'],\n                  dynamic_axes={'input': {2 : 'height', 3 : 'width'}}\n                 )"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#verify-onnx-inference",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#verify-onnx-inference",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Verify ONNX Inference",
    "text": "Verify ONNX Inference\nTo verify ONNX inference, we first need to import ONNX Runtime.\nImport ONNX Runtime\n# Import the onnxruntime library\nimport onnxruntime as ort\nThen, we can get the available ONNX Runtime execution providers and select one to use. We did not install any additional execution providers, so only the default CPU provider is available.\nGet available ONNX Runtime execution providers\n# Get the list of available providers for onnxruntime\navailable_providers = ort.get_available_providers()\n\n# Display the available providers\navailable_providers\n['CPUExecutionProvider']\nSelect execution provider\n# Use the first available provider for onnxruntime\nproviders = [available_providers[0]]\n\n# Display the selected provider\nproviders\n['CPUExecutionProvider']\nWe initialize an inference session with the selected execution provider and perform inference using the ONNX model.\nInitialize inference session with selected execution provider\n# Create an inference session for the ONNX model\nort_sess = ort.InferenceSession(onnx_file_name, providers=providers)\nPerform inference using ONNX model\n# Make predictions on the input tensor using the ONNX model\noutputs = ort_sess.run(output_names=None, input_feed={\"input\":input_tensor.numpy()})\n\n# Display the predictions\noutputs\n    [array([[ 34.445053, 300.62167 , 468.11035 , 625.07434 ],\n            [ 13.518433,  21.899261, 405.33997 , 296.6339  ]], dtype=float32),\n     array([1, 1], dtype=int64),\n     array([0.9997528, 0.9996158], dtype=float32),\n     array([[[[0., 0., 0., ..., 0., 0., 0.],\n              [0., 0., 0., ..., 0., 0., 0.],\n              [0., 0., 0., ..., 0., 0., 0.],\n              ...,\n              [0., 0., 0., ..., 0., 0., 0.],\n              [0., 0., 0., ..., 0., 0., 0.],\n              [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n​     \n​            [[[0., 0., 0., ..., 0., 0., 0.],\n​              [0., 0., 0., ..., 0., 0., 0.],\n​              [0., 0., 0., ..., 0., 0., 0.],\n​              ...,\n​              [0., 0., 0., ..., 0., 0., 0.],\n​              [0., 0., 0., ..., 0., 0., 0.],\n​              [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)]\nFinally, we benchmark the ONNX Runtime CPU inference speed to measure its performance.\nBenchmark ONNX Runtime CPU inference speed\n%%timeit\n# Measure the time it takes to make predictions on the input tensor\nort_sess.run(None, {'input': input_tensor.numpy()})\n320 ms ± 1.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#define-post-processing-steps",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#define-post-processing-steps",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Define Post-processing Steps",
    "text": "Define Post-processing Steps\nThe Mask R-CNN model includes the object detection post-processing steps internally. However, there are a few extra steps to annotate the input image with the model predictions.\nThe predicted image masks have values in the range [0,1], representing shades of gray from black to white. To use them to annotate the input image, we need to binarize the masks by applying a threshold value to convert them to either 0 or 1.\nWe can convert the binarized masks to RGBA images, which allows us to paste them on top of the input image to show the predicted object locations and segmentations. That provides a visual representation of the model’s predictions.\nDefine annotation values\n# Set the mask threshold\nmask_threshold = 0.5\n\n# Set the mask color\nmask_rgba = [255, 0, 0, 100]\nAnnotate sample image\n# Create a copy of the test image\nannotated_img = test_img.copy()\n\n# Convert the image to RGBA format\nannotated_img.convert('RGBA')\n\n# Create a drawing context for the image\ndraw = ImageDraw.Draw(annotated_img)\n\n# Set the font size for the labels\nfnt_size = 25\n\n# Iterate through annotations for sample image\nfor i in range(len(outputs[0])):\n    \n    # Extract mask array\n    mask_array = outputs[-1][i][0]\n    # Binarize mask values\n    mask_array[mask_array &gt; mask_threshold] = 1.0\n    mask_array[mask_array &lt;= mask_threshold] = 0.0\n    # Scale mask values\n    mask_array *= 255\n    # Convert mask from 1-channel to 4-channel\n    mask_array = np.tile(mask_array[:, :, None], [1, 1, 4])\n    # Update mask color\n    mask_array[:,:,0][mask_array[:,:,0] == 255] = mask_rgba[0]\n    mask_array[:,:,1][mask_array[:,:,1] == 255] = mask_rgba[1]\n    mask_array[:,:,2][mask_array[:,:,2] == 255] = mask_rgba[2]\n    # Update mask transparency\n    mask_array[:,:,3][mask_array[:,:,0] == 255] = mask_rgba[3]\n    mask_array[:,:,3][mask_array[:,:,1] == 255] = mask_rgba[3]\n    mask_array[:,:,3][mask_array[:,:,2] == 255] = mask_rgba[3]\n    mask_array[:,:,3][mask_array[:,:,0] != 255] = 0\n    # Convert mask array to PIL image\n    mask_img = PIL.Image.fromarray(mask_array.astype(np.uint8)).convert('RGBA')\n    \n    # Draw segmentation mask on sample image\n    annotated_img.paste(mask_img, (0,0), mask=mask_img)\n    \n    # Draw bounding box on sample image\n    shape = list(outputs[0][i])\n    draw.rectangle(shape, outline='red')\n    \n    # Draw object label on sample image\n    fnt = PIL.ImageFont.truetype(font_file, fnt_size)\n    label = class_map.get_classes()[outputs[1][i]]\n    draw.multiline_text((shape[0], shape[1]-fnt_size-5), f\"{label}\", font=fnt, fill='red')\n    \n# Print the size of the annotated image\nprint(annotated_img.size)\n\n# Display the annotated image\nannotated_img\n(480, 640)"
  },
  {
    "objectID": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#summary",
    "href": "posts/icevision-mask-rcnn-tutorial/part-1/index.html#summary",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "Summary",
    "text": "Summary\nThis tutorial demonstrated how to train a Mask R-CNN model on a custom dataset using the IceVision library. We showed how to create a custom dataset parser, train the model, and evaluate its performance. Additionally, we explained how to export the trained model to the ONNX format and perform inference using ONNX Runtime. You can follow the steps outlined in this tutorial to train a Mask R-CNN model on your custom dataset and use it for multiple applications.\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nSetup Conda Environment\nImport Dependencies\nConfigure Kaggle API\nDownload the Dataset\nInspect the Dataset\nCreate Dataset Parser\nDefine DataLoader Objects\nFinetune the Model\nPrepare Model for Export\nExport the Model\nVerify OpenVINO Inference\nDefine Post-processing Steps\nGenerate Colormap\nSummary"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#tutorial-links",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#tutorial-links",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a YOLOX model using IceVision and export it to OpenVINO.\nPart 2: Create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\nPart 3: Perform object detection in a Unity project with OpenVINO.\nGitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#introduction",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nIn this three-part tutorial series, we will explore how to use IceVision and OpenVINO to perform end-to-end object detection in Unity. In part 1, we will train a YOLOX model using IceVision and export it to OpenVINO. In part 2, we will create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO. Finally, in part 3, we will integrate the trained model into a Unity project to perform real-time object detection. By the end of this series, you will have a working object detection system that you can use in your Unity projects.\nUnity Demo\n\n\nVideo\n\n\nThe tutorial uses a downscaled subsample of the HaGRID (HAnd Gesture Recognition Image Dataset), which contains annotated sample images for 18 distinct hand gestures and an additional no_gesture class to account for idle hands.\n\n\n\nReference Images\n\n\n\n\n\n\n\n\nClass\n\n\nImage\n\n\n\n\n\n\ncall\n\n\n\n\n\n\n\ndislike\n\n\n\n\n\n\n\nfist\n\n\n\n\n\n\n\nfour\n\n\n\n\n\n\n\nlike\n\n\n\n\n\n\n\nmute\n\n\n\n\n\n\n\nok\n\n\n\n\n\n\n\none\n\n\n\n\n\n\n\npalm\n\n\n\n\n\n\n\npeace\n\n\n\n\n\n\n\npeace_inverted\n\n\n\n\n\n\n\nrock\n\n\n\n\n\n\n\nstop\n\n\n\n\n\n\n\nstop_inverted\n\n\n\n\n\n\n\nthree\n\n\n\n\n\n\n\nthree2\n\n\n\n\n\n\n\ntwo_up\n\n\n\n\n\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\n\n\n\n\nOne could use a model trained on this dataset to allow users to control a Unity application using hand gestures."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#overview",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#overview",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Overview",
    "text": "Overview\nIn part 1 of this tutorial series, we will learn how to train a YOLOX Tiny model using IceVision and export it to OpenVINO’s Intermediate Representation (IR) format. We will start by setting up a Conda environment and importing the necessary dependencies. Then, we will configure the Kaggle API to download the dataset we will use to train our model. After inspecting the dataset, we will create a parser to process the training samples and define DataLoader objects. Then, we will fine-tune the model and export it. Finally, we will perform inference with the exported model and define post-processing steps for the model output. We will then generate a colormap to visualize model predictions. By the end of this post, you will have a trained YOLOX model that you can deploy in your applications.\nYou can find links to view the training code and run it on Google Colab and Kaggle below.\n\n\n\nJupyter Notebook\nColab\nKaggle\n\n\n\n\nGitHub Repository\nOpen In Colab\nOpen in Kaggle\n\n\n\n\nNote: The free GPU tier for Google Colab takes approximately 11 minutes per epoch, while the free GPU tier for Kaggle Notebooks takes around 15 minutes per epoch."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#setup-conda-environment",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#setup-conda-environment",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Setup Conda Environment",
    "text": "Setup Conda Environment\nThe IceVision library builds upon specific versions of libraries like fastai and mmdetection, and the cumulative dependency requirements mean it is best to use a dedicated virtual environment. Below are the steps to create a virtual environment using Conda. Be sure to execute each command in the provided order.\n\nImportant: IceVision currently only supports Linux/macOS. Try using WSL (Windows Subsystem for Linux) if training locally on Windows.\n\nInstall CUDA Toolkit\nYou might need to install the CUDA Toolkit on your system if you plan to run the training code locally. CUDA requires an Nvidia GPU. Version 11.1.0 of the toolkit is available at the link below. Both Google Colab and Kaggle Notebooks already have CUDA installed.\n\nCUDA Toolkit 11.1.0\nCUDA Toolkit Archive\n\nConda environment setup steps\n# create a new conda environment\nconda create --name icevision python==3.8\n# activate the environment\nconda activate icevision\n# install PyTorch and torchvision\npip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n# install mmcv-full\npip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n# install mmdet\npip install mmdet==2.17.0\n# install icevision\npip install icevision==0.11.0\n# install icedata\npip install icedata==0.5.1\n# install setuptools\npip install setuptools==59.5.0\n# install OpenVINO developer tools\npip install openvino-dev\n# install package for generating visually distinct colours\npip install distinctipy\n# install jupyter\npip install jupyter\n# install onnxruntime\npip install onnxruntime\n# install onnx-simplifier\npip install onnx-simplifier\n# install the kaggle api\npip install kaggle\nThe mmdet package contains the pretrained YOLOX Tiny model we will finetune with IceVision. The package depends on the mmcv-full library, which is picky about the CUDA version used by PyTorch. We need to install the PyTorch version with the exact CUDA version expected by mmcv-full.\nThe icevision package provides the functionality for data curation, data transforms, and training loops we’ll use to train the model. The icedata package provides the functionality we’ll use to create a custom parser to read the dataset.\nThe openvino-dev pip package contains the model-conversion script to convert trained models from ONNX to OpenVINO’s IR format.\nWe’ll use the distinctipy pip package to generate a visually distinct colormap for drawing bounding boxes on images.\nThe ONNX models generated by PyTorch are not always the most concise. We can use the onnx-simplifier package to tidy up the exported model. This step is entirely optional.\nOriginal ONNX model (Netron)\n\n\n\n\n\nSimplified ONNX model (Netron)\n\n\n\n\n\nColab and Kaggle Setup Requirements\nWhen running the training code on Google Colab and Kaggle Notebooks, we need to uninstall several packages before installing IceVision and its dependencies to avoid conflicts. The platform-specific setup steps are at the top of the notebooks linked above."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#import-dependencies",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#import-dependencies",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nWe will start by importing the IceVision library and configuring Pandas. When you import the IceVision library for the first time, it will automatically download some additional resources that it needs to function correctly.\nImport IceVision library\n# Import all the necessary modules from the icevision package\nfrom icevision.all import *\nImport and configure Pandas\n# Import the pandas package\nimport pandas as pd\n\n# Set the max column width to None\npd.set_option('max_colwidth', None)\n\n# Set the max number of rows and columns to None\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#configure-kaggle-api",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#configure-kaggle-api",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Configure Kaggle API",
    "text": "Configure Kaggle API\nThe Kaggle API tool requires an API Key for a Kaggle account. Sign in or create a Kaggle account using the link below, then click the Create New API Token button.\n\nKaggle Account Settings: https://www.kaggle.com/me/account\n\n\n\n\n\n\nEnter Kaggle username and API token\ncreds = '{\"username\":\"\",\"key\":\"\"}'\nSave Kaggle credentials if none are present\n\nSource: https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\n\n\n# Set the path to the kaggle.json file\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\n\n# Check if the file already exists\nif not cred_path.exists():\n    # Create the directory if it does not exist\n    cred_path.parent.mkdir(exist_ok=True)\n    # Save the API key to the file\n    cred_path.write_text(creds)\n    # Set the file permissions to be readable and writable by the current user\n    cred_path.chmod(0o600)\nImport Kaggle API\n# Import the API module from the kaggle package\nfrom kaggle import api"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#download-the-dataset",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#download-the-dataset",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Download the Dataset",
    "text": "Download the Dataset\nNow that we have our Kaggle credentials set, we need to define the dataset and where to store it. I made two versions of the dataset available on Kaggle. One contains approximately thirty thousand training samples, and the other has over one hundred and twenty thousand.\n\nHaGRID Sample 30k 384p\nHaGRID Sample 120k 384p\n\nWe’ll use the default archive and data folders for the fastai library (installed with IceVision) to store the compressed and uncompressed datasets.\nDefine path to dataset\n# Import the URLs object from the fastai.data.external module\nfrom fastai.data.external import URLs\n# Set the name of the dataset\ndataset_name = 'hagrid-sample-30k-384p'\n# dataset_name = 'hagrid-sample-120k-384p'\n\n# Construct the Kaggle dataset name by combining the username and dataset name\nkaggle_dataset = f'innominate817/{dataset_name}'\n\n# Get the path to the directory where datasets are stored\narchive_dir = URLs.path()\n\n# Create the path to the data directory\ndataset_dir = archive_dir/'../data'\n\n# Create the path to the zip file that contains the dataset\narchive_path = Path(f'{archive_dir}/{dataset_name}.zip')\n\n# Create the path to the directory where the dataset will be extracted\ndataset_path = Path(f'{dataset_dir}/{dataset_name}')\nDefine method to extract the dataset from an archive file\ndef file_extract(fname, dest=None):\n    \"\"\"\n    Extract the specified file to the destination directory using `tarfile` or `zipfile`.\n    \n    Args:\n        fname (str): The path to the file to be extracted.\n        dest (str): The path to the directory where the file will be extracted. If not specified, the file will be extracted to the same directory as the source file.\n        \n    Returns:\n        None\n        \n    Raises:\n        Exception: If the file has an unrecognized file extension.\n    \"\"\"\n    # Set the destination directory to the parent directory of the file if not specified\n    if dest is None: dest = Path(fname).parent\n    \n    # Convert the file path to a string\n    fname = str(fname)\n    \n    # Check the file extension and extract the file using the appropriate module\n    if fname.endswith('gz'):\n        tarfile.open(fname, 'r:gz').extractall(dest)\n    elif fname.endswith('zip'):\n        zipfile.ZipFile(fname).extractall(dest)\n    else:\n        raise Exception(f'Unrecognized archive: {fname}')\nThe archive file for the 30K dataset is 4GB, so we don’t want to download it more than necessary.\nDownload the dataset if it is not present\n# Check if the dataset zip file already exists\nif not archive_path.exists():\n    # Download the dataset from Kaggle\n    api.dataset_download_cli(kaggle_dataset, path=archive_dir)\n    \n    # Extract the dataset zip file to the data directory\n    file_extract(fname=archive_path, dest=dataset_dir)"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#inspect-the-dataset",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#inspect-the-dataset",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Inspect the Dataset",
    "text": "Inspect the Dataset\nWe can start inspecting the dataset once it finishes downloading. In this step, we will get the file paths for the images and annotations and examine one of the training images. That will give us a better understanding of the dataset and its structure.\nDefine paths to image and annotation folders\n# Create a list of the items in the 'dataset_path' directory\ndir_content = list(dataset_path.ls())\n\n# Get the path of the 'ann_train_val' directory\nannotation_dir = dataset_path/'ann_train_val'\n\n# Remove the 'ann_train_val' directory from the list of items\ndir_content.remove(annotation_dir)\n\n# Get the path of the remaining directory, which is assumed to be the image directory\nimg_dir = dir_content[0]\n\n# Print the paths of the annotation and image directories\nannotation_dir, img_dir\n(Path('/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val'),\n Path('/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k'))\nThe bounding box annotations for each image are stored in JSON files organized by object class. The files contain annotations for all 552,992 images from the full HaGRID dataset.\nInspect the annotation folder\n# Get a list of files in the 'annotation_dir' directory\nfile_list = list(annotation_dir.ls())\n\n# Display the names of the files using a Pandas DataFrame\npd.DataFrame([file.name for file in file_list])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\ncall.json\n\n\n\n\n1\n\n\ndislike.json\n\n\n\n\n2\n\n\nfist.json\n\n\n\n\n3\n\n\nfour.json\n\n\n\n\n4\n\n\nlike.json\n\n\n\n\n5\n\n\nmute.json\n\n\n\n\n6\n\n\nok.json\n\n\n\n\n7\n\n\none.json\n\n\n\n\n8\n\n\npalm.json\n\n\n\n\n9\n\n\npeace.json\n\n\n\n\n10\n\n\npeace_inverted.json\n\n\n\n\n11\n\n\nrock.json\n\n\n\n\n12\n\n\nstop.json\n\n\n\n\n13\n\n\nstop_inverted.json\n\n\n\n\n14\n\n\nthree.json\n\n\n\n\n15\n\n\nthree2.json\n\n\n\n\n16\n\n\ntwo_up.json\n\n\n\n\n17\n\n\ntwo_up_inverted.json\n\n\n\n\n\n\n\nThe sample images are stored in folders separated by object class.\nInspect the image folder\n# Get a list of folders in the 'img_dir' directory\nfolder_list = list(img_dir.ls())\n\n# Display the names of the folders using a Pandas DataFrame\npd.DataFrame([folder.name for folder in folder_list])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\ntrain_val_call\n\n\n\n\n1\n\n\ntrain_val_dislike\n\n\n\n\n2\n\n\ntrain_val_fist\n\n\n\n\n3\n\n\ntrain_val_four\n\n\n\n\n4\n\n\ntrain_val_like\n\n\n\n\n5\n\n\ntrain_val_mute\n\n\n\n\n6\n\n\ntrain_val_ok\n\n\n\n\n7\n\n\ntrain_val_one\n\n\n\n\n8\n\n\ntrain_val_palm\n\n\n\n\n9\n\n\ntrain_val_peace\n\n\n\n\n10\n\n\ntrain_val_peace_inverted\n\n\n\n\n11\n\n\ntrain_val_rock\n\n\n\n\n12\n\n\ntrain_val_stop\n\n\n\n\n13\n\n\ntrain_val_stop_inverted\n\n\n\n\n14\n\n\ntrain_val_three\n\n\n\n\n15\n\n\ntrain_val_three2\n\n\n\n\n16\n\n\ntrain_val_two_up\n\n\n\n\n17\n\n\ntrain_val_two_up_inverted\n\n\n\n\n\n\n\nGet image file paths\n# Get a list of image files in the 'img_dir' directory\nfiles = get_image_files(img_dir)\n\n# Print the number of image files in the list\nlen(files)\n31833\nInspect files\n# Get the first and last file in the 'files' list\nfile1, file2 = files[0], files[-1]\n\n# Display the first and last files using a Pandas DataFrame\npd.DataFrame([file1, file2])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00005c9c-3548-4a8f-9d0b-2dd4aff37fc9.jpg\n\n\n\n\n1\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_two_up_inverted/fff4d2f6-9890-4225-8d9c-73a02ba8f9ac.jpg\n\n\n\n\n\n\nThe sample images are all downscaled to 384p.\nInspect one of the training images\n# Import the PIL library\nimport PIL\n\n# Open the first file in the 'files' list as a RGB image\nimg = PIL.Image.open(files[0]).convert('RGB')\n\n# Print the dimensions of the image\nprint(f\"Image Dims: {img.shape}\")\n\n# Show the image\nimg\nImage Dims: (512, 384)\n\n\n\n\n\nTo make it easier to work with the dataset, we will create a dictionary that maps image names to file paths. The dictionary will allow us to retrieve the file path for a given image efficiently.\nCreate a dictionary that maps image names to file paths\n# Create a dictionary where the keys are the filenames without the file extensions of the files in the 'files' list,\n# and the values are the file paths\nimg_dict = {file.stem : file for file in files}\n\n# Display the first five entries from the dictionary using a Pandas DataFrame\npd.DataFrame.from_dict(img_dict, orient='index').head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n00005c9c-3548-4a8f-9d0b-2dd4aff37fc9\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00005c9c-3548-4a8f-9d0b-2dd4aff37fc9.jpg\n\n\n\n\n0020a3db-82d8-47aa-8642-2715d4744db5\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/0020a3db-82d8-47aa-8642-2715d4744db5.jpg\n\n\n\n\n004ac93f-0f7c-49a4-aadc-737e0ad4273c\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/004ac93f-0f7c-49a4-aadc-737e0ad4273c.jpg\n\n\n\n\n006cac69-d3f0-47f9-aac9-38702d038ef1\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/006cac69-d3f0-47f9-aac9-38702d038ef1.jpg\n\n\n\n\n00973fac-440e-4a56-b60c-2a06d5fb155d\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00973fac-440e-4a56-b60c-2a06d5fb155d.jpg\n\n\n\n\n\n\n\nGet list of annotation file paths\n# Import the 'os' and 'glob' modules\nimport os\nfrom glob import glob\n# Get a list of paths to JSON files in the 'annotation_dir' directory\nannotation_paths = glob(os.path.join(annotation_dir, \"*.json\"))\n\n# Display the JSON file paths using a Pandas DataFrame\npd.DataFrame(annotation_paths)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/call.json\n\n\n\n\n1\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/palm.json\n\n\n\n\n2\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/rock.json\n\n\n\n\n3\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/stop_inverted.json\n\n\n\n\n4\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/two_up.json\n\n\n\n\n5\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/four.json\n\n\n\n\n6\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/three.json\n\n\n\n\n7\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/stop.json\n\n\n\n\n8\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/one.json\n\n\n\n\n9\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/three2.json\n\n\n\n\n10\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/peace_inverted.json\n\n\n\n\n11\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/ok.json\n\n\n\n\n12\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/like.json\n\n\n\n\n13\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/fist.json\n\n\n\n\n14\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/mute.json\n\n\n\n\n15\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/peace.json\n\n\n\n\n16\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/two_up_inverted.json\n\n\n\n\n17\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/ann_train_val/dislike.json\n\n\n\n\n\n\n\nAfter getting the list of annotation file paths, we will create an annotation DataFrame that contains all of the annotation data for the dataset. This DataFrame will allow us to manipulate and query the annotations more easily. We’ll then filter out annotations for images not present in the current subsample.\nCreate annotations dataframe\n# Create a generator that yields Pandas DataFrames containing the data from each JSON file\ncls_dataframes = (pd.read_json(f).transpose() for f in annotation_paths)\n\n# Concatenate the DataFrames into a single DataFrame\nannotation_df = pd.concat(cls_dataframes, ignore_index=False)\n\n# Keep only the rows that correspond to the filenames in the 'img_dict' dictionary\nannotation_df = annotation_df.loc[list(img_dict.keys())]\n\n# Print the first 5 rows of the DataFrame\nannotation_df.head()\n\n\n\n\n\n\n\n\nbboxes\n\n\nlabels\n\n\nleading_hand\n\n\nleading_conf\n\n\nuser_id\n\n\n\n\n\n\n00005c9c-3548-4a8f-9d0b-2dd4aff37fc9\n\n\n[[0.23925175, 0.28595301, 0.25055143, 0.20777627]]\n\n\n[call]\n\n\nright\n\n\n1\n\n\n5a389ffe1bed6660a59f4586c7d8fe2770785e5bf79b09334aa951f6f119c024\n\n\n\n\n0020a3db-82d8-47aa-8642-2715d4744db5\n\n\n[[0.5801012999999999, 0.53265105, 0.14562138, 0.12286348]]\n\n\n[call]\n\n\nleft\n\n\n1\n\n\n0d6da2c87ef8eabeda2dcfee2dc5b5035e878137a91b149c754a59804f3dce32\n\n\n\n\n004ac93f-0f7c-49a4-aadc-737e0ad4273c\n\n\n[[0.46294793, 0.26419774, 0.13834939000000002, 0.10784189]]\n\n\n[call]\n\n\nright\n\n\n1\n\n\nd50f05d9d6ca9771938cec766c3d621ff863612f9665b0e4d991c086ec04acc9\n\n\n\n\n006cac69-d3f0-47f9-aac9-38702d038ef1\n\n\n[[0.38799208, 0.44643898, 0.27068787, 0.18277858]]\n\n\n[call]\n\n\nright\n\n\n1\n\n\n998f6ad69140b3a59cb9823ba680cce62bf2ba678058c2fc497dbbb8b22b29fe\n\n\n\n\n00973fac-440e-4a56-b60c-2a06d5fb155d\n\n\n[[0.40980118, 0.38144198, 0.08338464, 0.06229785], [0.6122035100000001, 0.6780825500000001, 0.04700606, 0.07640522]]\n\n\n[call, no_gesture]\n\n\nright\n\n\n1\n\n\n4bb3ee1748be58e05bd1193939735e57bb3c0ca59a7ee38901744d6b9e94632e\n\n\n\n\n\n\n\nNotice that one of the samples contains a no_gesture label to identify an idle hand in the image.\nWe can retrieve the annotation data for a specific image file using its name.\nInspect annotation data for sample image\n# Get the filename without the file extension of the first file in the 'files' list\nfile_id = files[0].stem\n\n# Print the filename\nfile_id\n'00005c9c-3548-4a8f-9d0b-2dd4aff37fc9'\nThe image file names are the index values for the annotation DataFrame.\n# Get the row from the 'annotation_df' DataFrame corresponding to the 'file_id'\nannotation_df.loc[file_id].to_frame()\n\n\n\n\n\n\n\n\n00005c9c-3548-4a8f-9d0b-2dd4aff37fc9\n\n\n\n\n\n\nbboxes\n\n\n[[0.23925175, 0.28595301, 0.25055143, 0.20777627]]\n\n\n\n\nlabels\n\n\n[call]\n\n\n\n\nleading_hand\n\n\nright\n\n\n\n\nleading_conf\n\n\n1\n\n\n\n\nuser_id\n\n\n5a389ffe1bed6660a59f4586c7d8fe2770785e5bf79b09334aa951f6f119c024\n\n\n\n\n\n\nThe bboxes entry contains the [top-left-X-position, top-left-Y-position, width, height] information for any bounding boxes. The values are scaled based on the image dimensions. We multiply top-left-X-position and width values by the image width and multiply top-left-Y-position and height values by the image height to obtain the actual values.\nWe need a font file to annotate the images with class labels. We can download one from Google Fonts.\nDownload font file\n# Define the filename of the font file\nfont_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n\n# If the font file does not exist, download it\nif not os.path.exists(font_file): \n    !wget https://fonts.gstatic.com/s/roboto/v30/$font_file\nAnnotate sample image\n# Import the ImageDraw class from the PIL package\nfrom PIL import ImageDraw\n# Get the width and height of the image\nwidth, height = img.size\n\n# Create a copy of the image\nannotated_img = img.copy()\n\n# Create an ImageDraw object for drawing on the image\ndraw = ImageDraw.Draw(annotated_img)\n\n# Set the font size\nfnt_size = 25\n\n# Get the row from the 'annotation_df' DataFrame corresponding to the 'file_id'\nannotation = annotation_df.loc[file_id]\n\n# Loop through the bounding boxes and labels in the 'annotation' DataFrame\nfor i in range(len(annotation['labels'])):\n    # Get the bounding box coordinates\n    x, y, w, h = annotation['bboxes'][i]\n    \n    # Scale the coordinates to the size of the image\n    x *= width\n    y *= height\n    w *= width\n    h *= height\n    \n    # Create a tuple of coordinates for the bounding box\n    shape = (x, y, x+w, y+h)\n    \n    # Draw the bounding box on the image\n    draw.rectangle(shape, outline='red')\n    \n    # Load the font file\n    fnt = PIL.ImageFont.truetype(font_file, fnt_size)\n    \n    # Draw the label on the image\n    draw.multiline_text((x, y-fnt_size-5), f\"{annotation['labels'][i]}\", font=fnt, fill='red')\n\n# Print the dimensions of the image\nprint(annotated_img.size) \n\n# Show the image\nannotated_img\n(384, 512)\n\n\n\n\n\nWe need to provide IceVision with a class map that maps index values to unique class names.\nCreate a class map\n# Get a list of unique labels in the 'annotation_df' DataFrame\nlabels = annotation_df['labels'].explode().unique().tolist()\n\n# Display labels using a Pandas DataFrame\npd.DataFrame(labels)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\ncall\n\n\n\n\n1\n\n\nno_gesture\n\n\n\n\n2\n\n\ndislike\n\n\n\n\n3\n\n\nfist\n\n\n\n\n4\n\n\nfour\n\n\n\n\n5\n\n\nlike\n\n\n\n\n6\n\n\nmute\n\n\n\n\n7\n\n\nok\n\n\n\n\n8\n\n\none\n\n\n\n\n9\n\n\npalm\n\n\n\n\n10\n\n\npeace\n\n\n\n\n11\n\n\npeace_inverted\n\n\n\n\n12\n\n\nrock\n\n\n\n\n13\n\n\nstop\n\n\n\n\n14\n\n\nstop_inverted\n\n\n\n\n15\n\n\nthree\n\n\n\n\n16\n\n\nthree2\n\n\n\n\n17\n\n\ntwo_up\n\n\n\n\n18\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\nIceVision adds an additional background class at index 0.\n# Create a ClassMap object using the list of labels\nclass_map = ClassMap(labels)\n\n# Print the ClassMap object\nclass_map\n&lt;ClassMap: {'background': 0, 'call': 1, 'no_gesture': 2, 'dislike': 3, 'fist': 4, 'four': 5, 'like': 6, 'mute': 7, 'ok': 8, 'one': 9, 'palm': 10, 'peace': 11, 'peace_inverted': 12, 'rock': 13, 'stop': 14, 'stop_inverted': 15, 'three': 16, 'three2': 17, 'two_up': 18, 'two_up_inverted': 19}&gt;\n\nNote: The background class is not included in the final model."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#create-dataset-parser",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#create-dataset-parser",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Create Dataset Parser",
    "text": "Create Dataset Parser\nTo create a custom dataset parser for object detection, we can use the template for an object detection record and the template for an object detection parser.\nView template for an object detection record\n# Create an ObjectDetectionRecord object\ntemplate_record = ObjectDetectionRecord()\n\n# Print the ObjectDetectionRecord object\ntemplate_record\nBaseRecord\n\ncommon: \n    - Image size None\n    - Record ID: None\n    - Filepath: None\n    - Img: None\ndetection: \n    - Class Map: None\n    - Labels: []\n    - BBoxes: []\nView template for an object detection parser\n# Generate a template parser for an object detection dataset using the ObjectDetectionRecord object\nParser.generate_template(template_record)\nclass MyParser(Parser):\n    def __init__(self, template_record):\n        super().__init__(template_record=template_record)\n    def __iter__(self) -&gt; Any:\n    def __len__(self) -&gt; int:\n    def record_id(self, o: Any) -&gt; Hashable:\n    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n        record.set_img_size(&lt;ImgSize&gt;)\n        record.set_filepath(&lt;Union[str, Path]&gt;)\n        record.detection.set_class_map(&lt;ClassMap&gt;)\n        record.detection.add_labels(&lt;Sequence[Hashable]&gt;)\n        record.detection.add_bboxes(&lt;Sequence[BBox]&gt;)\nAs mentioned earlier, we need the dimensions for an image to scale the corresponding bounding box information. The dataset contains images with different resolutions, so we need to check for each image.\nDefine custom parser class\n# Define a subclass of the 'Parser' class\nclass HagridParser(Parser):\n    # Define the constructor\n    def __init__(self, template_record, annotations_df, img_dict, class_map):\n        # Call the parent class constructor\n        super().__init__(template_record=template_record)\n        \n        # Store the 'img_dict' and 'annotations_df' objects as instance variables\n        self.img_dict = img_dict\n        self.df = annotations_df\n        \n        # Store the 'class_map' object as an instance variable\n        self.class_map = class_map\n        \n    # Define the '__iter__' method\n    def __iter__(self) -&gt; Any:\n        # Yield the rows of the 'annotations_df' DataFrame\n        for o in self.df.itertuples(): yield o\n        \n    # Define the '__len__' method\n    def __len__(self) -&gt; int: \n        # Return the number of rows in the 'annotations_df' DataFrame\n        return len(self.df)\n    \n    # Define the 'record_id' method\n    def record_id(self, o: Any) -&gt; Hashable:\n        # Return the index of the row\n        return o.Index\n    \n    # Define the 'parse_fields' method\n    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n        # Get the file path for the corresponding image\n        filepath = self.img_dict[o.Index]\n        print(filepath)\n        \n        # Open the image and get its width and height\n        width, height = PIL.Image.open(filepath).convert('RGB').size\n        \n         # Set the size of the image in the 'record' object\n        record.set_img_size(ImgSize(width=width, height=height))\n\n        # Set the file path of the image in the 'record' object\n        record.set_filepath(filepath)\n\n        # Set the 'class_map' in the 'record' object\n        record.detection.set_class_map(self.class_map)\n\n        # Add the labels to the 'record' object\n        record.detection.add_labels(o.labels)\n\n        # Create an empty list for the bounding boxes\n        bbox_list = []\n\n        # Loop through the labels\n        for i, label in enumerate(o.labels):\n            # Get the bounding box coordinates\n            x = o.bboxes[i][0]*width\n            y = o.bboxes[i][1]*height\n            w = o.bboxes[i][2]*width\n            h = o.bboxes[i][3]*height\n            # Create a BBox object and add it to the 'bbox_list'\n            bbox_list.append(BBox.from_xywh(x, y, w, h))\n        # Add the bounding boxes to the 'record' object\n        record.detection.add_bboxes(bbox_list)\nWe can then create a parser object using the custom parser class.\nCreate a custom parser object\n# Create a HagridParser object\nparser = HagridParser(template_record, annotation_df, img_dict, class_map)\n\n# Get the number of rows in the 'annotation_df' DataFrame\nnum_rows = len(parser)\n\n# Print the number of rows\nprint(num_rows)\n31833\nWe use the parser object to parse annotations and create records.\nParse annotations to create records\n# Create a 'RandomSplitter' object\ndata_splitter = RandomSplitter([0.8, 0.2])\n\n# Use the 'parse' method to split the data into training and validation sets\ntrain_records, valid_records = parser.parse(data_splitter, cache_filepath=f'{dataset_name}-cache.pkl')\nFinally, we can inspect the training records to ensure the parser works correctly.\nInspect training records\n# Print the first element of the 'train_records'\nprint(train_records[0])\nBaseRecord\n\ncommon: \n    - Filepath: /mnt/980SSD_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_one/2507aacb-43d2-4114-91f1-008e3c7a181c.jpg\n    - Img: None\n    - Record ID: 2507aacb-43d2-4114-91f1-008e3c7a181c\n    - Image size ImgSize(width=640, height=853)\ndetection: \n    - BBoxes: [&lt;BBox (xmin:153.0572608, ymin:197.40873228, xmax:213.5684992, ymax:320.45228481000004)&gt;, &lt;BBox (xmin:474.20276479999995, ymin:563.67557885, xmax:520.8937472, ymax:657.61167499)&gt;]\n    - Class Map: &lt;ClassMap: {'background': 0, 'call': 1, 'no_gesture': 2, 'dislike': 3, 'fist': 4, 'four': 5, 'like': 6, 'mute': 7, 'ok': 8, 'one': 9, 'palm': 10, 'peace': 11, 'peace_inverted': 12, 'rock': 13, 'stop': 14, 'stop_inverted': 15, 'three': 16, 'three2': 17, 'two_up': 18, 'two_up_inverted': 19}&gt;\n    - Labels: [9, 2]\n# Use the 'show_record' function to display the first element of the 'train_records' object with annotations\nshow_record(train_records[0], figsize = (10,10), display_label=True)\n\n\n\n\n\n# Use the 'show_records' function to display the second, third, and fourth elements of the 'train_records' list  with annotations\nshow_records(train_records[1:4], ncols=3,display_label=True)"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#define-dataloader-objects",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#define-dataloader-objects",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Define DataLoader Objects",
    "text": "Define DataLoader Objects\nThe YOLOX model examines an input image using the stride values [8, 16, 32] to detect objects of various sizes.\nThe max number of detections depends on the input resolution and these stride values. Given a 384x512 image, the model will make (384/8)*(512/8) + (384/16)*(512/16) + (384/32)*(512/32) = 4032 predictions. Although, many of those predictions get filtered out during post-processing.\nHere, we can see the difference in results when using a single stride value in isolation with a YOLOX model trained on the COCO dataset.\nStride 8\n\n\n\n\n\nStride 16\n\n\n\n\n\nStride 32\n\n\n\n\n\nDefine stride values\n# Define a list of 'strides'\nstrides = [8, 16, 32]\n\n# Get the maximum value in the 'strides' list\nmax_stride = max(strides)\nWe need to set the input height and width to multiples of the highest stride value (i.e., 32).\nSelect a multiple of the max stride value as the input resolution\n# Show a list of input resolutions by multiplying the maximum stride by numbers in the range 7-20\n[max_stride*i for i in range(7,21)]\n[224, 256, 288, 320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640] \nDefine input resolution\n# Define the size of the image\nimage_size = 384\n\n# Define the presize of the image\npresize = 512\n\nNote: You can lower the image_size to reduce training time at the cost of a potential decrease in accuracy.\n\nIceVision provides several default methods for data augmentation to help the model generalize. It automatically updates the bounding box information for an image based on the applied augmentations.\nDefine Transforms\n# Show the default augmentations included with the 'aug_tfms' function using a Pandas DataFrame\npd.DataFrame(tfms.A.aug_tfms(size=image_size, presize=presize))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nSmallestMaxSize(always_apply=False, p=1, max_size=512, interpolation=1)\n\n\n\n\n1\n\n\nHorizontalFlip(always_apply=False, p=0.5)\n\n\n\n\n2\n\n\nShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.0625, 0.0625), shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-15, 15), interpolation=1, border_mode=4, value=None, mask_value=None)\n\n\n\n\n3\n\n\nRGBShift(always_apply=False, p=0.5, r_shift_limit=(-10, 10), g_shift_limit=(-10, 10), b_shift_limit=(-10, 10))\n\n\n\n\n4\n\n\nRandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True)\n\n\n\n\n5\n\n\nBlur(always_apply=False, p=0.5, blur_limit=(1, 3))\n\n\n\n\n6\n\n\nOneOrOther([RandomSizedBBoxSafeCrop(always_apply=False, p=0.5, height=384, width=384, erosion_rate=0.0, interpolation=1),LongestMaxSize(always_apply=False, p=1, max_size=384, interpolation=1),], p=0.5)\n\n\n\n\n7\n\n\nPadIfNeeded(always_apply=False, p=1.0, min_height=384, min_width=384, pad_height_divisor=None, pad_width_divisor=None, border_mode=0, value=[124, 116, 104], mask_value=None)\n\n\n\n\n\n\n\n# Show the transforms included with the 'resize_and_pad' function using a Pandas DataFrame\npd.DataFrame(tfms.A.resize_and_pad(size=image_size))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nLongestMaxSize(always_apply=False, p=1, max_size=384, interpolation=1)\n\n\n\n\n1\n\n\nPadIfNeeded(always_apply=False, p=1.0, min_height=384, min_width=384, pad_height_divisor=None, pad_width_divisor=None, border_mode=0, value=[124, 116, 104], mask_value=None)\n\n\n\n\n\n\n# Define the 'train_tfms' adapter using the 'Adapter' method and the 'aug_tfms' function\ntrain_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=presize), tfms.A.Normalize()])\n\n# Define the 'valid_tfms' adapter using the 'Adapter' method and the 'resize_and_pad' function\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])\nWe can extract the normalization stats from the tfms.A.Normalize() method for future use. We’ll use these same stats when performing inference with the trained model.\nGet normalization stats\n# Get the mean of the Normalize() transformation\nmean = tfms.A.Normalize().mean\n\n# Get the standard deviation of the Normalize() transformation\nstd = tfms.A.Normalize().std\n\n# Print the mean and standard deviation\nmean, std\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\nNext, we create dataset objects for the training and validation datasets using the defined transforms and normalization stats.\nDefine Datasets\n# Create a Dataset object using the 'train_records' and 'train_tfms' variables\ntrain_ds = Dataset(train_records, train_tfms)\n\n# Create a Dataset object using the 'valid_records' and 'valid_tfms' variables\nvalid_ds = Dataset(valid_records, valid_tfms)\n\n# Print the 'train_ds' and 'valid_ds' objects\ntrain_ds, valid_ds\n(&lt;Dataset with 25466 items&gt;, &lt;Dataset with 6367 items&gt;)\nWe can apply the image augmentations to a sample training image to demonstrate the effects of data augmentation.\nApply augmentations to a training sample\n# Create a list of three samples from the 'train_ds' dataset object\nsamples = [train_ds[0] for _ in range(3)]\n\n# Show the samples using the 'show_samples' function\nshow_samples(samples, ncols=3)\n\n\n\n\n\nOnce the datasets are defined, we can specify YOLOX as the model type for training.\nDefine model type\n# Set the model type to YOLOX\nmodel_type = models.mmdet.yolox\nWe’ll use a model pretrained on the COCO dataset rather than train a new model from scratch.\nDefine backbone\n# Create a YOLOX Tiny backbone for the model\nbackbone = model_type.backbones.yolox_tiny_8x8(pretrained=True)\n\n# Show the backbone information using a Pandas Dataframe\npd.DataFrame.from_dict(backbone.__dict__, orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nmodel_name\n\n\nyolox\n\n\n\n\nconfig_path\n\n\n/home/innom-dt/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs/yolox/yolox_tiny_8x8_300e_coco.py\n\n\n\n\nweights_url\n\n\nhttps://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_tiny_8x8_300e_coco/yolox_tiny_8x8_300e_coco_20210806_234250-4ff3b67e.pth\n\n\n\n\npretrained\n\n\nTrue\n\n\n\n\n\n\nDefine batch size\n# Set the batch size\nbs = 32\n\nNote: Adjust the batch size based on the available GPU memory.\n\nDefine DataLoaders\n# Create a DataLoader for the training set\ntrain_dl = model_type.train_dl(train_ds, batch_size=bs, num_workers=2, shuffle=True)\n\n# Create a DataLoader for the validation set\nvalid_dl = model_type.valid_dl(valid_ds, batch_size=bs, num_workers=2, shuffle=False)\n\nNote: Be careful when increasing the number of workers. There is a bug that significantly increases system memory usage with more workers."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#finetune-the-model",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#finetune-the-model",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Finetune the Model",
    "text": "Finetune the Model\nTo finetune the YOLOX model, we must first instantiate the model and define metrics to track during training.\nInstantiate the model\n# Create a YOLOX Tiny model\nmodel = model_type.model(backbone=backbone(pretrained=True), num_classes=parser.class_map.num_classes) \nDefine metrics\n# Define a list of metrics to evaluate the model\nmetrics = [COCOMetric(metric_type=COCOMetricType.bbox)]\nWe can then create a Learner object to find the learning rate and handle the training loop.\nDefine Learner object\n# Create a fastai learner object to train and evaluate the YOLOX Tiny model\nlearn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)\nFind learning rate\n# Use the learning rate finder to find a good learning rate for the YOLOX Tiny model\nsuggested_lrs = learn.lr_find()\n\n\n\n\n\nDefine learning rate\n# Use the suggested learning rate identified by the learning rate finder\nlr = suggested_lrs.valley\nlr\n0.0008317637839354575\nDefine number of epochs\n# Set the number of epochs to train the YOLOX Tiny model\nepochs = 20\nAfter defining the training parameters, we can finetune the model by training it on the training dataset.\nFinetune model\n# Train the YOLOX Tiny model\nlearn.fine_tune(epochs, lr, freeze_epochs=1)\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nCOCOMetric\n\n\ntime\n\n\n\n\n\n\n0\n\n\n6.054967\n\n\n5.384349\n\n\n0.357238\n\n\n03:24\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nCOCOMetric\n\n\ntime\n\n\n\n\n\n\n0\n\n\n3.794365\n\n\n3.506713\n\n\n0.605573\n\n\n03:40\n\n\n\n\n1\n\n\n3.312004\n\n\n2.977496\n\n\n0.654320\n\n\n03:40\n\n\n\n\n2\n\n\n3.017060\n\n\n3.090266\n\n\n0.606374\n\n\n03:42\n\n\n\n\n3\n\n\n2.881786\n\n\n2.837017\n\n\n0.655119\n\n\n03:48\n\n\n\n\n4\n\n\n2.760416\n\n\n2.978580\n\n\n0.616788\n\n\n03:50\n\n\n\n\n5\n\n\n2.658237\n\n\n2.742451\n\n\n0.660538\n\n\n03:31\n\n\n\n\n6\n\n\n2.595560\n\n\n2.547496\n\n\n0.683073\n\n\n03:34\n\n\n\n\n7\n\n\n2.440215\n\n\n2.707062\n\n\n0.640533\n\n\n03:35\n\n\n\n\n8\n\n\n2.332424\n\n\n2.616575\n\n\n0.658988\n\n\n03:34\n\n\n\n\n9\n\n\n2.292744\n\n\n2.278664\n\n\n0.727411\n\n\n03:33\n\n\n\n\n10\n\n\n2.165260\n\n\n2.263503\n\n\n0.714858\n\n\n03:32\n\n\n\n\n11\n\n\n2.114893\n\n\n2.221797\n\n\n0.724567\n\n\n03:34\n\n\n\n\n12\n\n\n2.048447\n\n\n2.226138\n\n\n0.723726\n\n\n03:33\n\n\n\n\n13\n\n\n1.927701\n\n\n2.126613\n\n\n0.737985\n\n\n03:30\n\n\n\n\n14\n\n\n1.895885\n\n\n2.154254\n\n\n0.733679\n\n\n03:32\n\n\n\n\n15\n\n\n1.869765\n\n\n1.983894\n\n\n0.762880\n\n\n03:33\n\n\n\n\n16\n\n\n1.798780\n\n\n2.019078\n\n\n0.753732\n\n\n03:32\n\n\n\n\n17\n\n\n1.778396\n\n\n2.028802\n\n\n0.751977\n\n\n03:33\n\n\n\n\n18\n\n\n1.748940\n\n\n1.990781\n\n\n0.759491\n\n\n03:36\n\n\n\n\n19\n\n\n1.735546\n\n\n1.973754\n\n\n0.761532\n\n\n03:33"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#prepare-model-for-export",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#prepare-model-for-export",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Prepare Model for Export",
    "text": "Prepare Model for Export\nOnce the model finishes training, we need to modify it before exporting it. First, we’ll prepare an input image to feed to the model.\nDefine method to convert a PIL Image to a Pytorch Tensor\ndef img_to_tensor(img:PIL.Image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    \"\"\"\n    Converts a PIL image to a PyTorch tensor.\n    \n    Args:\n        img: The input PIL image.\n        mean: The mean values for normalization.\n        std: The standard deviation values for normalization.\n    \n    Returns:\n        The normalized tensor.\n    \"\"\"\n    # Convert image to tensor\n    img_tensor = torch.Tensor(np.array(img)).permute(2, 0, 1)\n    # Scale pixels values from [0,255] to [0,1]\n    scaled_tensor = img_tensor.float().div_(255)\n    # Prepare normalization tensors\n    mean_tensor = tensor(mean).view(1,1,-1).permute(2, 0, 1)\n    std_tensor = tensor(std).view(1,1,-1).permute(2, 0, 1)\n    # Normalize tensor    \n    normalized_tensor = (scaled_tensor - mean_tensor) / std_tensor\n    # Batch tensor\n    return normalized_tensor.unsqueeze(dim=0)\nSelect a test image\nannotation_df.iloc[4].to_frame()\n\n\n\n\n\n\n\n\n00973fac-440e-4a56-b60c-2a06d5fb155d\n\n\n\n\n\n\nbboxes\n\n\n[[0.40980118, 0.38144198, 0.08338464, 0.06229785], [0.6122035100000001, 0.6780825500000001, 0.04700606, 0.07640522]]\n\n\n\n\nlabels\n\n\n[call, no_gesture]\n\n\n\n\nleading_hand\n\n\nright\n\n\n\n\nleading_conf\n\n\n1\n\n\n\n\nuser_id\n\n\n4bb3ee1748be58e05bd1193939735e57bb3c0ca59a7ee38901744d6b9e94632e\n\n\n\n\n\n\nGet the test image file path\n# Retrieve the image file path associated with the fifth entry in the 'annotation_df' DataFrame object\ntest_file = img_dict[annotation_df.iloc[4].name]\n\n# Print the test file path\ntest_file\nPath('/home/innom-dt/.fastai/archive/../data/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00973fac-440e-4a56-b60c-2a06d5fb155d.jpg')\nLoad the test image\n# Open the test file\ntest_img = PIL.Image.open(test_file).convert('RGB')\n\n# Display the test image\ntest_img\n\n\n\n\n\nCalculate valid input dimensions\n# Calculate the input height and width for the test image\ninput_h = test_img.height - (test_img.height % max_stride)\ninput_w = test_img.width - (test_img.width % max_stride)\n\n# Print the input height and width\ninput_h, input_w\n(512, 384)\nCrop image to supported resolution\n# Crop and pad the test image to match the input height and width\ntest_img = test_img.crop_pad((input_w, input_h))\n\n# Print the resulting test image\ntest_img\n\n\n\n\n\nConvert image to a normalized tensor\n# Convert the test image to a tensor\ntest_tensor = img_to_tensor(test_img, mean=mean, std=std)\n\n# Print the shape of the resulting tensor\nprint(test_tensor.shape)\ntorch.Size([1, 3, 512, 384])\nBefore making any changes, let’s inspect the current model output.\nInspect raw model output\n# Get the raw model output using the test tensor\nmodel_output = model.cpu().forward_dummy(test_tensor.cpu())\nThe model currently organizes the output into three tuples. The first tuple contains three tensors storing the object class predictions using the three stride values. Recall that there are 19 object classes, excluding the background class added by IceVision.\nThe second tuple contains three tensors with the predicted bounding box coordinates and dimensions using the three stride values.\nThe third tuple contains three tensors with the confidence score for whether an object is present in a given section of the input image using the three stride values.\n# Print the shape for each tensor in the model output\nfor raw_out in model_output:\n    for out in raw_out:\n        print(out.shape)\ntorch.Size([1, 19, 64, 48])\ntorch.Size([1, 19, 32, 24])\ntorch.Size([1, 19, 16, 12])\ntorch.Size([1, 4, 64, 48])\ntorch.Size([1, 4, 32, 24])\ntorch.Size([1, 4, 16, 12])\ntorch.Size([1, 1, 64, 48])\ntorch.Size([1, 1, 32, 24])\ntorch.Size([1, 1, 16, 12])\n\n512/8 = 64, 512/16 = 32, 512/32 = 16\n384/8 = 48, 384/16 = 24, 384/32 = 12\n\nIf we examine the end of a model from the official YOLOX repo, we can see the output looks a bit different.\n\n\n\n\n\nThe official model first passes the tensors with the object class and “objectness” scores through sigmoid functions. It then combines the three tensors for each stride value into a single tensor before combining the resulting three tensors into a single flat array.\nWe can apply these same steps to our model by adding a new forward function using monkey patching.\nDefine custom forward function for exporting the model\ndef forward_export(self, input_tensor):\n    \n    # Get raw model output\n    model_output = self.forward_dummy(input_tensor.cpu())\n    \n    # Get the classification scores from the model output\n    cls_scores = model_output[0]\n    \n    # Get the bounding box predictions from the model output\n    bbox_preds = model_output[1]\n    \n    # Get the objectness scores from the model output\n    objectness = model_output[2]\n    \n    # Process the stride 8 output\n    stride_8_cls = torch.sigmoid(cls_scores[0])\n    stride_8_bbox = bbox_preds[0]\n    stride_8_objectness = torch.sigmoid(objectness[0])\n    stride_8_cat = torch.cat((stride_8_bbox, stride_8_objectness, stride_8_cls), dim=1)\n    stride_8_flat = torch.flatten(stride_8_cat, start_dim=2)\n\n    # Process the stride 16 output\n    stride_16_cls = torch.sigmoid(cls_scores[1])\n    stride_16_bbox = bbox_preds[1]\n    stride_16_objectness = torch.sigmoid(objectness[1])\n    stride_16_cat = torch.cat((stride_16_bbox, stride_16_objectness, stride_16_cls), dim=1)\n    stride_16_flat = torch.flatten(stride_16_cat, start_dim=2)\n\n    # Process the stride 32 output\n    stride_32_cls = torch.sigmoid(cls_scores[2])\n    stride_32_bbox = bbox_preds[2]\n    stride_32_objectness = torch.sigmoid(objectness[2])\n    stride_32_cat = torch.cat((stride_32_bbox, stride_32_objectness, stride_32_cls), dim=1)\n    stride_32_flat = torch.flatten(stride_32_cat, start_dim=2)\n\n    # Concatenate all of the processed outputs\n    full_cat = torch.cat((stride_8_flat, stride_16_flat, stride_32_flat), dim=2)\n\n    # Return the concatenated outputs in a permuted form\n    return full_cat.permute(0, 2, 1)\nAdd custom forward function to model\n# Bind the forward_export method to the model object\nmodel.forward_export = forward_export.__get__(model)\nLet’s verify the new forward function works as intended. The output should have a batch size of 1 and contain 4032 elements, given the input dimensions (calculated earlier), each with 24 values (19 classes + 1 objectness score + 4 bounding box values).\nVerify output shape\n# Call the forward_export method on the model object, passing in the test_tensor as an argument\n# and get the shape of the output tensor\nmodel.forward_export(test_tensor).shape\ntorch.Size([1, 4032, 24])\nWe need to replace the current forward function before exporting the model. We can create a backup of the original forward function just in case.\nCreate a backup of the default model forward function\n# Save the original forward method of the model\norigin_forward = model.forward\nReplace model forward function with custom function\n# Replace the original forward method of the model with the forward_export method\nmodel.forward = model.forward_export\nVerify output shape\n# Call the forward_export method on the model object, passing in the test_tensor as an argument\n# and get the shape of the output tensor\nmodel(test_tensor).shape\ntorch.Size([1, 4032, 24])"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#export-the-model",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#export-the-model",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Export the Model",
    "text": "Export the Model\nThe OpenVINO model conversion script does not support PyTorch models, so we need to export the trained model to ONNX. We can then convert the ONNX model to OpenVINO’s IR format.\nDefine ONNX file name\n# Create a filename for the ONNX model\nonnx_file_name = f\"{dataset_name}-{type(model).__name__}.onnx\"\n\n# Display the filename\nonnx_file_name\n'hagrid-sample-30k-384p-YOLOX.onnx'\nExport trained model to ONNX\n# Export the PyTorch model to ONNX format\ntorch.onnx.export(model,\n                  test_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=11,\n                  do_constant_folding=True,\n                  input_names = ['input'],\n                  output_names = ['output'],\n                  dynamic_axes={'input': {2 : 'height', 3 : 'width'}}\n                 )\nSimplify ONNX model\n# Import the onnx module\nimport onnx\n\n# Import the simplify method from the onnxsim module\nfrom onnxsim import simplify\n# Load the ONNX model from the onnx_file_name\nonnx_model = onnx.load(onnx_file_name)\n\n# Simplify the model\nmodel_simp, check = simplify(onnx_model)\n\n# Save the simplified model to the onnx_file_name\nonnx.save(model_simp, onnx_file_name)\n\nNote: As mentioned earlier, this step is entirely optional.\n\nNow we can export the ONNX model to OpenVINO’s IR format.\nImport OpenVINO Dependencies\n# Import the Core class from the openvino.runtime module\nfrom openvino.runtime import Core\n# Import the Markdown and display classes from the IPython.display module\nfrom IPython.display import Markdown, display\nDefine export directory\n# Create a Path object representing the current directory\noutput_dir = Path('./')\n\n# Print the output_dir object\noutput_dir\nPath('.')\nThe conversion script generates an XML file containing information about the model architecture and a BIN file that stores the trained weights. We need both files to perform inference. OpenVINO uses the same name for the BIN file as provided for the XML file.\nDefine path for OpenVINO IR xml model file\n# Create a Path object representing the IR xml file using the ONNX model file name without the file extension\nir_path = Path(f\"{onnx_file_name.split('.')[0]}.xml\")\n\n# Print the ir_path object\nir_path\nPath('hagrid-sample-30k-384p-YOLOX.xml')\nOpenVINO provides the option to include the normalization stats in the IR model. That way, we don’t need to account for different normalization stats when performing inference with multiple models. We can also convert the model to FP16 precision to reduce file size and improve inference speed.\nDefine arguments for model conversion script\n# Create the Model Optimizer command to convert the ONNX model to OpenVINO\nmo_command = f\"\"\"mo\n                 --input_model \"{onnx_file_name}\"\n                 --input_shape \"[1,3, {image_size}, {image_size}]\"\n                 --mean_values=\"{mean}\"\n                 --scale_values=\"{std}\"\n                 --data_type FP16\n                 --output_dir \"{output_dir}\"\n                 \"\"\"\n\n# Remove extra whitespace from the command string\nmo_command = \" \".join(mo_command.split())\n\n# Print the command and format it as a Bash code block\nprint(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\ndisplay(Markdown(f\"```bash\\n{mo_command}\\n```\"))\nModel Optimizer command to convert the ONNX model to OpenVINO:\nmo --input_model \"hagrid-sample-30k-384p-YOLOX.onnx\" --input_shape \"[1,3, 384, 384]\" --mean_values=\"(0.485, 0.456, 0.406)\" --scale_values=\"(0.229, 0.224, 0.225)\" --data_type FP16 --output_dir \".\"\nConvert ONNX model to OpenVINO IR\n# Check if the IR model file exists\nif not ir_path.exists():\n    # If the IR model file does not exist, export the ONNX model to IR\n    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n    mo_result = %sx $mo_command\n    print(\"\\n\".join(mo_result))\nelse:\n    # If the IR model file already exists, print a message\n    print(f\"IR model {ir_path} already exists.\")\nExporting ONNX model to IR... This may take a few minutes.\nModel Optimizer arguments:\nCommon parameters:\n    - Path to the Input Model:  /media/innom-dt/Samsung_T3/Projects/GitHub/icevision-openvino-unity-tutorial/notebooks/hagrid-sample-30k-384p-YOLOX.onnx\n    - Path for generated IR:    /media/innom-dt/Samsung_T3/Projects/GitHub/icevision-openvino-unity-tutorial/notebooks/.\n    - IR output name:   hagrid-sample-30k-384p-YOLOX\n    - Log level:    ERROR\n    - Batch:    Not specified, inherited from the model\n    - Input layers:     Not specified, inherited from the model\n    - Output layers:    Not specified, inherited from the model\n    - Input shapes:     [1,3, 384, 384]\n    - Source layout:    Not specified\n    - Target layout:    Not specified\n    - Layout:   Not specified\n    - Mean values:  (0.485, 0.456, 0.406)\n    - Scale values:     (0.229, 0.224, 0.225)\n    - Scale factor:     Not specified\n    - Precision of IR:  FP16\n    - Enable fusing:    True\n    - User transformations:     Not specified\n    - Reverse input channels:   False\n    - Enable IR generation for fixed input shape:   False\n    - Use the transformations config file:  None\nAdvanced parameters:\n    - Force the usage of legacy Frontend of Model Optimizer for model conversion into IR:   False\n    - Force the usage of new Frontend of Model Optimizer for model conversion into IR:  False\nOpenVINO runtime found in:  /home/innom-dt/mambaforge/envs/icevision/lib/python3.8/site-packages/openvino\nOpenVINO runtime version:   2022.1.0-7019-cdb9bec7210-releases/2022/1\nModel Optimizer version:    2022.1.0-7019-cdb9bec7210-releases/2022/1\n[ WARNING ]  \nDetected not satisfied dependencies:\n    numpy: installed: 1.23.1, required: &lt; 1.20\n\nPlease install required versions of components or run pip installation\npip install openvino-dev\n[ SUCCESS ] Generated IR version 11 model.\n[ SUCCESS ] XML file: /media/innom-dt/Samsung_T3/Projects/GitHub/icevision-openvino-unity-tutorial/notebooks/hagrid-sample-30k-384p-YOLOX.xml\n[ SUCCESS ] BIN file: /media/innom-dt/Samsung_T3/Projects/GitHub/icevision-openvino-unity-tutorial/notebooks/hagrid-sample-30k-384p-YOLOX.bin\n[ SUCCESS ] Total execution time: 0.47 seconds. \n[ SUCCESS ] Memory consumed: 115 MB. \nIt's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\nFind more information about API v2.0 and IR v11 at https://docs.openvino.ai"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#verify-openvino-inference",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#verify-openvino-inference",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Verify OpenVINO Inference",
    "text": "Verify OpenVINO Inference\nNow, we can verify the OpenVINO model works as desired using the test image.\nGet available OpenVINO compute devices\n# Create an instance of the Core class\nie = Core()\n\n# Get the list of available devices\ndevices = ie.available_devices\n\n# Iterate over the available devices\nfor device in devices:\n    # Get the device name\n    device_name = ie.get_property(device_name=device, name=\"FULL_DEVICE_NAME\")\n    \n    # Print the device and its name\n    print(f\"{device}: {device_name}\")\nCPU: 11th Gen Intel(R) Core(TM) i7-11700K @ 3.60GHz\nPrepare input image for OpenVINO IR model\n# Convert image to tensor\nimg_tensor = torch.Tensor(np.array(test_img)).permute(2, 0, 1)\n# Scale pixels values from [0,255] to [0,1]\nscaled_tensor = img_tensor.float().div_(255)\n# Add an extra dimension to the Tensor\ninput_image = scaled_tensor.unsqueeze(dim=0)\n\n# Print the shape of the input image\ninput_image.shape\ntorch.Size([1, 3, 512, 384])\nTest OpenVINO IR model\n# Create an instance of the Core class\nie = Core()\n\n# Read the IR model file\nmodel_ir = ie.read_model(model=ir_path)\n\n# Reshape the model to match the shape of the input image\nmodel_ir.reshape(input_image.shape)\n\n# Compile the model for the CPU device\ncompiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n\n# Get the input and output layers of the compiled model\ninput_layer_ir = next(iter(compiled_model_ir.inputs))\noutput_layer_ir = next(iter(compiled_model_ir.outputs))\n\n# Run the model on the input image and get the output\nres_ir = compiled_model_ir([input_image])[output_layer_ir]\n# Print the shape of the model output\nres_ir.shape\n(1, 4032, 24)\nThe output shape is correct, meaning we can move on to the post-processing steps."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#define-post-processing-steps",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#define-post-processing-steps",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Define Post-processing Steps",
    "text": "Define Post-processing Steps\nTo process the model output, we need to iterate through each of the 4032 object proposals and save the ones that meet a user-defined confidence threshold (e.g., 50%). We then filter out the redundant proposals (i.e., detecting the same object multiple times) from that subset using Non-Maximum Suppression (NMS).\nWe’ll first define a method that generates offset values based on the input dimensions and stride values, which we can use to traverse the output array.\nDefine method to generate offset values to navigate the raw model output\ndef generate_grid_strides(height, width, strides=[8, 16, 32]):\n    \"\"\"\n    Generate a list of dictionaries containing grid coordinates and strides for a given height and width.\n    \n    Args:\n        height (int): The height of the image.\n        width (int): The width of the image.\n        strides (list): A list of strides to use for generating grid coordinates.\n        \n    Returns:\n        list: A list of dictionaries containing grid coordinates and strides.\n    \"\"\"\n    \n    # Create an empty list to store the grid coordinates and strides\n    grid_strides = []\n\n    # Iterate over the strides\n    for stride in strides:\n        # Calculate the grid height and width\n        grid_height = height // stride\n        grid_width = width // stride\n\n        # Iterate over the grid coordinates\n        for g1 in range(grid_height):\n            for g0 in range(grid_width):\n                # Append a dictionary containing the grid coordinates and stride to the list\n                grid_strides.append({'grid0':g0, 'grid1':g1, 'stride':stride })\n    \n    # Return the list of dictionaries\n    return grid_strides\nGenerate offset values to navigate model output\n# Generate the grid coordinates and strides\ngrid_strides = generate_grid_strides(test_img.height, test_img.width, strides)\n\n# Print the length of the list of grid coordinates and strides\nlen(grid_strides)\n4032\n# Print the first few rows of the list using a DataFrame\npd.DataFrame(grid_strides).head()\n\n\n\n\n\n\n\n\ngrid0\n\n\ngrid1\n\n\nstride\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n8\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n8\n\n\n\n\n2\n\n\n2\n\n\n0\n\n\n8\n\n\n\n\n3\n\n\n3\n\n\n0\n\n\n8\n\n\n\n\n4\n\n\n4\n\n\n0\n\n\n8\n\n\n\n\n\n\nNext, we’ll define a method to iterate through the output array and decode the bounding box information for each object proposal. As mentioned earlier, we’ll only keep the ones with a high enough confidence score. The model predicts the center coordinates of a bounding box, but we’ll store the coordinates for the top-left corner as that is what the ImageDraw.Draw.rectangle() method expects as input.\nDefine method to generate object detection proposals from the raw model output\ndef generate_yolox_proposals(model_output, proposal_length, grid_strides, bbox_conf_thresh=0.3):\n    \"\"\"\n    Generate a list of bounding box proposals from the model output.\n    \n    Args:\n        model_output (numpy array): The output of the YOLOX model.\n        proposal_length (int): The length of each proposal in the model output.\n        grid_strides (list): A list of dictionaries containing grid coordinates and strides.\n        bbox_conf_thresh (float): The confidence threshold for bounding box proposals.\n        \n    Returns:\n        list: A list of bounding box proposals.\n    \"\"\"\n    \n    # Create an empty list to store the bounding box proposals\n    proposals = []\n    \n    # Calculate the number of classes\n    num_classes = proposal_length - 5\n\n    # Iterate over the grid coordinates and strides\n    for anchor_idx in range(len(grid_strides)):\n        \n        # Get the grid coordinates and stride for the current anchor\n        grid0 = grid_strides[anchor_idx]['grid0']\n        grid1 = grid_strides[anchor_idx]['grid1']\n        stride = grid_strides[anchor_idx]['stride']\n\n        # Calculate the starting index for the current anchor in the model output\n        start_idx = anchor_idx * proposal_length\n\n        # Get the coordinates for the center of the predicted bounding box\n        x_center = (model_output[start_idx + 0] + grid0) * stride\n        y_center = (model_output[start_idx + 1] + grid1) * stride\n\n        # Get the dimensions for the predicted bounding box\n        w = np.exp(model_output[start_idx + 2]) * stride\n        h = np.exp(model_output[start_idx + 3]) * stride\n\n        # Calculate the coordinates for the upper left corner of the bounding box\n        x0 = x_center - w * 0.5\n        y0 = y_center - h * 0.5\n\n        # Get the objectness score for the current anchor\n        box_objectness = model_output[start_idx + 4]\n        \n        # Create an empty dictionary to store the bounding box proposal\n        obj = { 'x0':x0, 'y0':y0, 'width':w, 'height':h, 'label':0, 'prob':0 }\n\n        # Iterate over the classes\n        for class_idx in range(num_classes):\n\n            # Calculate the probability of the current class\n            box_cls_score = model_output[start_idx + 5 + class_idx]\n            box_prob = box_objectness * box_cls_score\n\n            # If the probability is greater than the current maximum, update the proposal dictionary\n            if (box_prob &gt; obj['prob']):\n                obj['label'] = class_idx\n                obj['prob'] = box_prob\n\n        # If the bounding box proposal has a probability greater than the specified threshold, add it to the list of proposals\n        if obj['prob'] &gt; bbox_conf_thresh: proposals.append(obj)\n        \n    # Sort the list of bounding box proposals by probability in descending order\n    proposals.sort(key=lambda x:x['prob'], reverse=True)\n    return proposals\nDefine minimum confidence score for keeping bounding box proposals\n# Set the bounding box confidence threshold\nbbox_conf_thresh = 0.5\nProcess raw model output\n# Generate proposals from the model output\nproposals = generate_yolox_proposals(res_ir.flatten(), res_ir.shape[2], grid_strides, bbox_conf_thresh)\n\n# Convert the proposals to a Pandas DataFrame\nproposals_df = pd.DataFrame(proposals)\n\n# Add the label names to the DataFrame\nproposals_df['label'] = proposals_df['label'].apply(lambda x: labels[x])\n\n# Print the proposals Dataframe\nproposals_df\n\n\n\n\n\n\n\n\nx0\n\n\ny0\n\n\nwidth\n\n\nheight\n\n\nlabel\n\n\nprob\n\n\n\n\n\n\n0\n\n\n234.084399\n\n\n345.059397\n\n\n19.638884\n\n\n40.022980\n\n\nno_gesture\n\n\n0.887864\n\n\n\n\n1\n\n\n234.122849\n\n\n344.858476\n\n\n19.512623\n\n\n40.319473\n\n\nno_gesture\n\n\n0.887479\n\n\n\n\n2\n\n\n233.998906\n\n\n344.849410\n\n\n19.742203\n\n\n39.664391\n\n\nno_gesture\n\n\n0.879032\n\n\n\n\n3\n\n\n154.565092\n\n\n193.542165\n\n\n35.063389\n\n\n34.609722\n\n\ncall\n\n\n0.876051\n\n\n\n\n4\n\n\n154.257556\n\n\n193.482616\n\n\n35.451900\n\n\n34.860138\n\n\ncall\n\n\n0.867827\n\n\n\n\n5\n\n\n154.484365\n\n\n193.435712\n\n\n34.926231\n\n\n35.332264\n\n\ncall\n\n\n0.866654\n\n\n\n\n6\n\n\n234.141719\n\n\n344.954988\n\n\n19.724554\n\n\n40.226116\n\n\nno_gesture\n\n\n0.865423\n\n\n\n\n7\n\n\n233.691895\n\n\n344.861304\n\n\n20.142962\n\n\n40.653099\n\n\nno_gesture\n\n\n0.857602\n\n\n\n\n8\n\n\n154.580361\n\n\n193.261580\n\n\n34.681351\n\n\n35.288120\n\n\ncall\n\n\n0.847856\n\n\n\n\n9\n\n\n233.792754\n\n\n344.441489\n\n\n20.184782\n\n\n40.635910\n\n\nno_gesture\n\n\n0.829289\n\n\n\n\n10\n\n\n154.467418\n\n\n193.468482\n\n\n35.273167\n\n\n34.796146\n\n\ncall\n\n\n0.829163\n\n\n\n\n11\n\n\n154.234487\n\n\n193.324329\n\n\n35.518040\n\n\n34.588329\n\n\ncall\n\n\n0.816633\n\n\n\n\n12\n\n\n155.282080\n\n\n193.360302\n\n\n34.524830\n\n\n35.269939\n\n\ncall\n\n\n0.804335\n\n\n\n\n13\n\n\n233.925717\n\n\n344.809189\n\n\n19.701090\n\n\n40.598907\n\n\nno_gesture\n\n\n0.779452\n\n\n\n\n14\n\n\n233.717521\n\n\n344.739007\n\n\n20.083487\n\n\n40.492405\n\n\nno_gesture\n\n\n0.736652\n\n\n\n\n15\n\n\n154.407403\n\n\n193.529026\n\n\n34.728149\n\n\n33.798748\n\n\ncall\n\n\n0.687202\n\n\n\n\n\n\n\nWe know the test image contains one call gesture and one idle hand. The model seems pretty confident about the locations of those two hands as the bounding box values are nearly identical across the no_gesture predictions and among the call predictions.\nWe can filter out the redundant predictions by checking how much the bounding boxes overlap. When two bounding boxes overlap beyond a user-defined threshold, we keep the one with a higher confidence score.\nDefine function to calculate the union area of two bounding boxes\ndef calc_union_area(a, b):\n    # Find the minimum x-coordinate of the two rectangles\n    x = min(a['x0'], b['x0'])\n    \n    # Find the minimum y-coordinate of the two rectangles\n    y = min(a['y0'], b['y0'])\n    \n    # Find the maximum x-coordinate of the two rectangles\n    w = max(a['x0']+a['width'], b['x0']+b['width']) - x\n    \n    # Find the maximum y-coordinate of the two rectangles\n    h = max(a['y0']+a['height'], b['y0']+b['height']) - y\n    \n    # Return the area of the combined rectangle\n    return w*h\nDefine function to calculate the intersection area of two bounding boxes\ndef calc_inter_area(a, b):\n    # Find the maximum x-coordinate of the two rectangles\n    x = max(a['x0'], b['x0'])\n    \n    # Find the maximum y-coordinate of the two rectangles\n    y = max(a['y0'], b['y0'])\n    \n    # Find the minimum x-coordinate of the two rectangles\n    w = min(a['x0']+a['width'], b['x0']+b['width']) - x\n    \n    # Find the minimum y-coordinate of the two rectangles\n    h = min(a['y0']+a['height'], b['y0']+b['height']) - y\n    \n    # Return the area of the intersecting rectangle\n    return w*h\nDefine function to sort bounding box proposals using Non-Maximum Suppression\ndef nms_sorted_boxes(nms_thresh=0.45):\n    # Initialize a list to store the indices of the proposals to keep\n    proposal_indices = []\n\n    # Loop over all proposals in the input list\n    for i in range(len(proposals)):\n        # Get the ith proposal\n        a = proposals[i]\n\n        # Assume that we want to keep this proposal by default\n        keep = True\n\n        # Loop over the indices of the proposals that we want to keep\n        for j in proposal_indices:\n            # Get the jth proposal\n            b = proposals[j]\n\n            # Compute the area of the intersection of the ith and jth proposals\n            inter_area = calc_inter_area(a, b)\n\n            # Compute the area of the union of the ith and jth proposals\n            union_area = calc_union_area(a, b)\n\n            # If the intersection of the ith and jth proposals is more than the specified non-max suppression\n            # threshold, we don't want to keep the ith proposal\n            if inter_area / union_area &gt; nms_thresh:\n                keep = False\n\n        # If we want to keep the ith proposal, append its index to the list of proposal indices\n        if keep:\n            proposal_indices.append(i)\n\n    # Return the list of proposal indices\n    return proposal_indices\nDefine threshold for sorting bounding box proposals\n# Set the non-max suppression threshold\nnms_thresh = 0.45\nSort bouning box proposals using NMS\n# Apply non-max suppression to the list of proposals with the specified non-max suppression threshold\nproposal_indices = nms_sorted_boxes(nms_thresh)\n\n# Print the list of proposal indices\nprint(proposal_indices)\n[0, 3]\nFilter excluded bounding box proposals\n# Print the rows from the proposals DataFrame that correspond to the indices\n# returned by the non-max suppression algorithm\nproposals_df.iloc[proposal_indices]\n\n\n\n\n\n\n\n\nx0\n\n\ny0\n\n\nwidth\n\n\nheight\n\n\nlabel\n\n\nprob\n\n\n\n\n\n\n0\n\n\n234.084399\n\n\n345.059397\n\n\n19.638884\n\n\n40.022980\n\n\nno_gesture\n\n\n0.887864\n\n\n\n\n3\n\n\n154.565092\n\n\n193.542165\n\n\n35.063389\n\n\n34.609722\n\n\ncall\n\n\n0.876051\n\n\n\n\n\n\nNow we have a single prediction for an idle hand and a single prediction for a call sign."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#generate-colormap",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#generate-colormap",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Generate Colormap",
    "text": "Generate Colormap\nBefore we annotate the input image with the predicted bounding boxes, let’s generate a colormap for the object classes.\nImport library for generating color palette\n# Import the distinctipy module\nfrom distinctipy import distinctipy\nGenerate a visually distinct color for each label\n# Use the distinctipy module to generate a list of colors with a length equal to the number of labels\ncolors = distinctipy.get_colors(len(labels))\nDisplay the generated color palette\n# Use the distinctipy module to generate a color swatch using the list of colors\ndistinctipy.color_swatch(colors)\n\n\n\n\n\nSet precision for color values\n# Set the precision to 5 decimal places\nprecision = 5\nRound color values to specified precision\n# Round the values in the list of colors to the specified precision\ncolors = [[np.round(ch, precision) for ch in color] for color in colors]\n\n# Display the rounded list of colors using a Pandas Dataframe\npd.DataFrame(colors)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n\n\n\n\n0\n\n\n0.00000\n\n\n1.00000\n\n\n0.00000\n\n\n\n\n1\n\n\n1.00000\n\n\n0.00000\n\n\n1.00000\n\n\n\n\n2\n\n\n0.00000\n\n\n0.50000\n\n\n1.00000\n\n\n\n\n3\n\n\n1.00000\n\n\n0.50000\n\n\n0.00000\n\n\n\n\n4\n\n\n0.50000\n\n\n0.75000\n\n\n0.50000\n\n\n\n\n5\n\n\n0.32114\n\n\n0.03531\n\n\n0.64056\n\n\n\n\n6\n\n\n0.80830\n\n\n0.00115\n\n\n0.02081\n\n\n\n\n7\n\n\n0.02177\n\n\n0.42475\n\n\n0.33483\n\n\n\n\n8\n\n\n0.72261\n\n\n0.47583\n\n\n0.99531\n\n\n\n\n9\n\n\n0.99715\n\n\n0.97599\n\n\n0.25699\n\n\n\n\n10\n\n\n0.00000\n\n\n1.00000\n\n\n1.00000\n\n\n\n\n11\n\n\n0.00000\n\n\n1.00000\n\n\n0.50000\n\n\n\n\n12\n\n\n0.65521\n\n\n0.34251\n\n\n0.38036\n\n\n\n\n13\n\n\n0.96712\n\n\n0.62955\n\n\n0.52852\n\n\n\n\n14\n\n\n0.48445\n\n\n0.84111\n\n\n0.01565\n\n\n\n\n15\n\n\n0.00000\n\n\n0.00000\n\n\n1.00000\n\n\n\n\n16\n\n\n0.54362\n\n\n0.96123\n\n\n0.90460\n\n\n\n\n17\n\n\n0.36779\n\n\n0.44128\n\n\n0.00059\n\n\n\n\n18\n\n\n0.97231\n\n\n0.10181\n\n\n0.49080\n\n\n\n\n\n\n\nAnnotate image using bounding box proposals\n# Create a copy of the test image\nannotated_img = test_img.copy()\n\n# Create a drawing context for the annotated image\ndraw = ImageDraw.Draw(annotated_img)\n\n# Set the font size for the labels\nfnt_size = 25\n\n# Loop over the indices of the proposals that were selected by the non-max suppression algorithm\nfor i in proposal_indices:\n    # Get the bounding box coordinates, label, and confidence score of the ith proposal\n    x, y, w, h, l, p = proposals[i].values()\n\n    # Compute the shape of the bounding box\n    shape = (x, y, x+w, y+h)\n\n    # Get the color for the ith proposal's label\n    color = tuple([int(ch*255) for ch in colors[proposals[i]['label']]])\n\n    # Draw the bounding box on the annotated image using the computed shape and color\n    draw.rectangle(shape, outline=color)\n\n    # Create a font object using the selected font and font size\n    fnt = PIL.ImageFont.truetype(\"KFOlCnqEu92Fr1MmEU9vAw.ttf\", fnt_size)\n\n    # Draw the label and confidence score on the annotated image using the font, color, and bounding box coordinates\n    draw.multiline_text((x, y-fnt_size*2-5), f\"{labels[l]}\\n{p*100:.2f}%\", font=fnt, fill=color)\n\n# Print the size of the annotated image\nprint(annotated_img.size) \n\n# Display the annotated image\nannotated_img\n(384, 512)\n\n\n\n\n\nBenchmark OpenVINO IR CPU inference speed\n%%timeit\n# Time how long it takes to run the compiled model on the input image\n# and extract the output from the specified layer\ncompiled_model_ir([input_image])[output_layer_ir]\n    12 ms ± 42.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nWe can export the colormap to a JSON file and import it into the Unity project. That way, we can easily swap colormaps for models trained on different datasets without changing any code.\nCreate JSON colormap\n# Initialize the color map with an empty list of items\ncolor_map = {'items': list()}\n\n# Populate the color map with the labels and colors\ncolor_map['items'] = [{'label': label, 'color': color} for label, color in zip(labels, colors)]\n\n# Print the color map\ncolor_map\n{'items': [{'label': 'call', 'color': [0.0, 1.0, 0.0]},\n  {'label': 'no_gesture', 'color': [1.0, 0.0, 1.0]},\n  {'label': 'dislike', 'color': [0.0, 0.5, 1.0]},\n  {'label': 'fist', 'color': [1.0, 0.5, 0.0]},\n  {'label': 'four', 'color': [0.5, 0.75, 0.5]},\n  {'label': 'like', 'color': [0.32114, 0.03531, 0.64056]},\n  {'label': 'mute', 'color': [0.8083, 0.00115, 0.02081]},\n  {'label': 'ok', 'color': [0.02177, 0.42475, 0.33483]},\n  {'label': 'one', 'color': [0.72261, 0.47583, 0.99531]},\n  {'label': 'palm', 'color': [0.99715, 0.97599, 0.25699]},\n  {'label': 'peace', 'color': [0.0, 1.0, 1.0]},\n  {'label': 'peace_inverted', 'color': [0.0, 1.0, 0.5]},\n  {'label': 'rock', 'color': [0.65521, 0.34251, 0.38036]},\n  {'label': 'stop', 'color': [0.96712, 0.62955, 0.52852]},\n  {'label': 'stop_inverted', 'color': [0.48445, 0.84111, 0.01565]},\n  {'label': 'three', 'color': [0.0, 0.0, 1.0]},\n  {'label': 'three2', 'color': [0.54362, 0.96123, 0.9046]},\n  {'label': 'two_up', 'color': [0.36779, 0.44128, 0.00059]},\n  {'label': 'two_up_inverted', 'color': [0.97231, 0.10181, 0.4908]}]}\nExport colormap\n# Import the json module\nimport json\n\n# Set the name of the file to which the color map will be written\ncolor_map_file_name = f\"{dataset_path.name}-colormap.json\"\n\n# Open the file in write mode\nwith open(color_map_file_name, \"w\") as write_file:\n    # Write the color map to the file as JSON\n    json.dump(color_map, write_file)\n    \n# Print the name of the file that the color map was written to\nprint(color_map_file_name)\n'hagrid-sample-30k-384p-colormap.json'"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-1/index.html#summary",
    "href": "posts/icevision-openvino-unity-tutorial/part-1/index.html#summary",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1",
    "section": "Summary",
    "text": "Summary\nWe now have a template to train a YOLOX model using IceVision and export it to OpenVINO. We started by setting up a Conda environment and importing the necessary dependencies. Then, we configured the Kaggle API to download the dataset and created a parser to process the data. We defined DataLoader objects and fine-tuned the model before preparing it for export. We implemented post-processing steps for the model output and generated a colormap to visualize model predictions. In part 2, we will learn how to create a dynamic link library (DLL) file in Visual Studio to perform object detection with our YOLOX model using OpenVINO.\nBeginner Tutorial: Fastai to Unity Beginner Tutorial Pt. 1\nNext: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 2\nAlternative Next: Object Detection for Unity With ONNX Runtime and DirectML Pt. 1\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "",
    "text": "Overview\nInstall OpenVINO\nCreate DLL Project\nConfigure the Project\nAdd Include Directories\nLink Libraries\nPost Build Events\nUpdate Precompiled Header File\nUpdate dllmain File\nBuild Solution\nGather Dependencies\nSummary"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#tutorial-links",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#tutorial-links",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a YOLOX model using IceVision and export it to OpenVINO.\nPart 2: Create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\nPart 3: Perform object detection in a Unity project with OpenVINO.\nGitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#overview",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#overview",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Overview",
    "text": "Overview\nIn part 2 of this tutorial series, we will create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\nWe will begin by installing OpenVINO and creating a DLL project in Visual Studio. Then, we will configure the project by adding the necessary include directories and linking the required libraries. We will also set up post-build events to automatically gather the DLL’s dependencies. Next, we will update the precompiled header and dllmain files to include the necessary code for object detection. Finally, we will build the project to generate the DLL file.\nBy the end of this post, you will have a DLL file that you can use for object detection in Unity.\n\nImportant: This post assumes Visual Studio is present on your system."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#install-openvino",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#install-openvino",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Install OpenVINO",
    "text": "Install OpenVINO\nWe need to download the OpenVINO Toolkit before creating our Visual Studio project. Go to the OpenVINO download page linked below.\n\nOpenVINO Download page\n\nDownload OpenVINO Toolkit\nSelect the options outlined in the image below and click the Download button.\n\n\n\n\n\nDouble-click the file once it finishes downloading and click the Extract button in the popup window.\n\n\n\n\n\nThe installer will then verify the computer meets the system requirements. The toolkit includes the Python scripts for converting models, which require Python 3.6, 3.7, 3.8, or 3.9 to run. We will only use the files for C++ development in this post.\n\n\n\n\n\nWe can stick with the default Recommended Installation option.\n\n\n\n\n\nThe installer will then ask whether Intel can collect some information before starting the installation process.\n\n\n\n\n\n\n\n\n\n\nClick Finish once the installation process completes.\n\n\n\n\n\nInspect OpenVINO Folder\nIf we look at the installation folder for the toolkit, we can see it also includes a version of OpenCV. We’ll use OpenCV to prepare image data from Unity before feeding it to the model.\n\n\n\n\n\nI like to copy the OpenVINO folder to a separate directory with other dependencies for my C++ projects.\n\n\n\n\n\nNow we can create our Visual Studio DLL project."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#create-dll-project",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#create-dll-project",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Create DLL Project",
    "text": "Create DLL Project\nOpen Visual Studio and select the Create a new project option.\n\n\n\n\n\nType DLL into the text box and select the Dynamic-Link Library (DLL) option. This option automatically configures a few parameters for us compared to starting with a standard console application.\n\n\n\n\n\nChoose a name and location for the project and click the Create button. By default, the DLL file will use the project name."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#configure-the-project",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#configure-the-project",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Configure the Project",
    "text": "Configure the Project\nAt the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#add-include-directories",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#add-include-directories",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nWe need to tell Visual Studio where OpenVINO and OpenCV are so we can access their APIs. Right-click the project name in the Solution Explorer panel.\n\n\n\n\n\nSelect the Properties option in the popup menu.\n\n\n\n\n\nIn the Properties Window, open on the C/C++ dropdown. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nAdd the paths for the following folders, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\include\\ie\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\include\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\include\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\include"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#link-libraries",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#link-libraries",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select Input. Select Additional Dependencies and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the paths to the following files, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\lib\\*\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\lib\\intel64\\Release\\*\n&lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\lib\\*.lib"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#post-build-events",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#post-build-events",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Post Build Events",
    "text": "Post Build Events\nOur DLL file will depend on the following DLL files included with the OpenVINO and OpenCV libraries.\nOpenCV DLL files\n\n\n\n\n\nOpenVINO DLL files\n\n\n\n\n\n\n\n\n\n\nWe can add a post-build event in Visual Studio to automatically copy these DLL files to the build folder for the project at compile time. Open the Build Events dropdown in the Properties window and select Post-Build Event. Select Command Line and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the following commands, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenVINO Toolkit, and click OK.\n\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_core453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_imgproc453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\opencv\\bin\\opencv_imgcodecs453.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\bin\\intel64\\Release\\* $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\nxcopy &lt;parent-folder-path&gt;\\openvino_2022.1.0.643\\runtime\\3rdparty\\tbb\\bin\\tbb.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window.\n\n\n\n\n\nWith the dependencies taken care of, we can start modifying the code."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#update-precompiled-header-file",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nWe’ll first update the pch.h Precompiled Header file with the required header files. We can open the pch.h file by selecting it in the Solution Explorer window.\n\n\n\n\n\nComment or remove the #include line for the framework.h header file.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#endif //PCH_H\nAdd required header files\nNext, we’ll add the required header files for OpenVINO and OpenCV below //#include \"framework.h\" line.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#include \"openvino/openvino.hpp\"\n#include &lt;opencv2/opencv.hpp&gt;\n\n#endif //PCH_H"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#update-dllmain-file",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#update-dllmain-file",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Update dllmain File",
    "text": "Update dllmain File\nBy default, the dllmain.cpp file contains the following code.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\nCreate a macro to mark functions we want to make accessible in Unity\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWrap the code in extern “C” to prevent name-mangling issues with the compiler\nThe rest of our code will go inside here.\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n\n}\nDefine variables\nInside the wrapper, we will declare the persistent variables needed for the DLL.\n\nov::Core: represents an OpenVINO runtime Core entity\nov::Model: A user-defined model\nov::CompiledModel: represents a compiled model\nov::InferRequest: an infer request that can be run in asynchronous or synchronous manners\nov::Tensor: API holding host memory\n\n\n// Inference engine instance\nov::Core core;\n// The user define model representation\nstd::shared_ptr&lt;ov::Model&gt; model;\n// A device-specific compiled model\nov::CompiledModel compiled_model;\n\n// List of available compute devices\nstd::vector&lt;std::string&gt; available_devices;\n// An inference request for a compiled model\nov::InferRequest infer_request;\n// Stores the model input data\nov::Tensor input_tensor;\n// A pointer for accessing the input tensor data\nfloat* input_data;\n\n// model has only one output\nov::Tensor output_tensor;\n// A pointer for accessing the output tensor data\nfloat* out_data;\n\n// The current source image width\nint img_w;\n// The current source image height\nint img_h;\n// The current model input width\nint input_w;\n// The current model input height\nint input_h;\n// The total number pixels in the input image\nint n_pixels;\n// The number of color channels \nint n_channels = 3;\n\n// Stores information about a single object prediction\nstruct Object\n{\n    float x0;\n    float y0;\n    float width;\n    float height;\n    int label;\n    float prob;\n};\n\n// Store grid offset and stride values to decode a section of the model output\nstruct GridAndStride\n{\n    int grid0;\n    int grid1;\n    int stride;\n};\n\n// The scale values used to adjust the model output to the source image resolution\nfloat scale_x;\nfloat scale_y;\n\n// The minimum confidence score to consider an object proposal\nfloat bbox_conf_thresh = 0.3;\n// The maximum intersection over union value before an object proposal will be ignored\nfloat nms_thresh = 0.45;\n\n// Stores the grid and stride values to navigate the raw model output\nstd::vector&lt;GridAndStride&gt; grid_strides;\n// Stores the object proposals with confidence scores above bbox_conf_thresh\nstd::vector&lt;Object&gt; proposals;\n// Stores the indices for the object proposals selected using non-maximum suppression\nstd::vector&lt;int&gt; proposal_indices;\n\n// The stride values used to generate the gride_strides vector\nstd::vector&lt;int&gt; strides = { 8, 16, 32 };\nDefine a function to get the number of compute devices\nThe first function we’ll define will create a list of available device names and return the number of devices accessible by OpenVINO. We’ll use this information to select which device to use to perform inference from the Unity application. There might be an option named GNA (Gaussian & Neural Accelerator). GNA is a highly specialized neural coprocessor for tasks like noise cancellation. We’ll exclude it from the list of devices presented to the end user.\n\nov::Core::get_available_devices(): Returns devices available for inference\n\n\n/// &lt;summary&gt;\n/// Get the number of available compute devices\n/// &lt;/summary&gt;\n/// &lt;returns&gt;The number of available devices&lt;/returns&gt;\nDLLExport int GetDeviceCount() \n{\n    // Reset list of available compute devices\n    available_devices.clear();\n\n    // Populate list of available compute devices\n    for (std::string device : core.get_available_devices()) {\n        // Skip GNA device\n        if (device.find(\"GNA\") == std::string::npos) {\n            available_devices.push_back(device);\n        }\n    }\n    // Return the number of available compute devices\n    return available_devices.size();\n}\nDefine a function to get the name of a compute device\nNext, we’ll define a function to return the name of a device at a specified index for the list of available devices.\n/// &lt;summary&gt;\n/// Get the name of the compute device name at the specified index\n/// &lt;/summary&gt;\n/// &lt;param name=\"index\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;The name of the device at the specified index&lt;/returns&gt;\nDLLExport std::string* GetDeviceName(int index) {\n    return &available_devices[index];\n}\nDefine method to generate stride values to navigate the raw model output\nThe method for generating the offset values used to traverse the output array is almost identical to the Python implementation from part 1.\n/// &lt;summary&gt;\n/// Generate offset values to navigate the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"height\"&gt;The model input height&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;The model input width&lt;/param&gt;\nvoid GenerateGridsAndStride(int height, int width)\n{\n    // Remove the values for the previous input resolution\n    grid_strides.clear();\n\n    // Iterate through each stride value\n    for (auto stride : strides)\n    {\n        // Calculate the grid dimensions\n        int grid_height = height / stride;\n        int grid_width = width / stride;\n\n        // Store each combination of grid coordinates\n        for (int g1 = 0; g1 &lt; grid_height; g1++)\n        {\n            for (int g0 = 0; g0 &lt; grid_width; g0++)\n            {\n                grid_strides.push_back(GridAndStride{ g0, g1, stride });\n            }\n        }\n    }\n}\nDefine a function to set the minimum confidence score from Unity\nWe might want to try different confidence thresholds for keeping object proposals from the Unity application, so we’ll add a function to enable this.\n/// &lt;summary&gt;\n/// Set minimum confidence score for keeping bounding box proposals\n/// &lt;/summary&gt;\n/// &lt;param name=\"min_confidence\"&gt;The minimum confidence score for keeping bounding box proposals&lt;/param&gt;\nDLLExport void SetConfidenceThreshold(float min_confidence)\n{\n    bbox_conf_thresh = min_confidence;\n}\nDefine a function to load an OpenVINO model\nOpenVINO needs to compile models for the target device. This process can take several seconds when using GPU inference. We can create a cache directory, so we only need to compile models for a specific resolution-device pair once.\nWe’ll place the code for loading an OpenVINO model inside a try-catch block to avoid crashing the application if we pass an incorrect file path.\nIf the model loads successfully, we’ll attempt to reshape the model input to the desired input dimensions. After reshaping the model input, we can compile the model for the target device.\nWe can get pointers to the model input tensor and create an inference request using the compiled model.\n\nov::Core::set_property(): Sets properties for a device\nov::Core::read_model(): Reads models from IR/ONNX/PDPD formats\nov::Model::reshape(): Updates input shapes and propagates them down to the outputs of the model through all intermediate layers\nov::Core::compile_model(): Creates a compiled model from a source model object\nov::CompiledModel::create_infer_request(): Creates an inference request object used to infer the compiled model\nov::InferRequest::get_input_tensor(): Gets an input tensor for inference\n\n\n/// &lt;summary&gt;\n/// Load a model from the specified file path\n/// &lt;/summary&gt;\n/// &lt;param name=\"model_path\"&gt;The full model path to the OpenVINO IR model&lt;/param&gt;\n/// &lt;param name=\"index\"&gt;The index for the available_devices vector&lt;/param&gt;\n/// &lt;param name=\"image_dims\"&gt;The source image dimensions&lt;/param&gt;\n/// &lt;returns&gt;A status value indicating success or failure to load and reshape the model&lt;/returns&gt;\nDLLExport int LoadModel(char* model_path, int index, int image_dims[2]) \n{\n\n    int return_val = 0;\n    // Set the cache directory for compiled GPU models\n    core.set_property(\"GPU\", ov::cache_dir(\"cache\"));\n\n    // Try loading the specified model\n    try { model = core.read_model(model_path); }\n    catch (...) { return 1; }\n\n    // The dimensions of the source input image\n    img_w = image_dims[0];\n    img_h = image_dims[1];\n    // Calculate new input dimensions based on the max stride value\n    input_w = (int)(strides.back() * std::roundf(img_w / strides.back()));\n    input_h = (int)(strides.back() * std::roundf(img_h / strides.back()));\n    n_pixels = input_w * input_h;\n\n    // Calculate the value used to adjust the model output to the source image resolution\n    scale_x = input_w / (img_w * 1.0);\n    scale_y = input_h / (img_h * 1.0);\n\n    // Generate the grid and stride values based on input resolution\n    grid_strides.clear();\n    GenerateGridsAndStride(input_h, input_w);\n\n    // Try updating the model input dimensions\n    try { model-&gt;reshape({ 1, 3, input_h, input_w }); }\n    catch (...) { return_val = 2; }\n\n    // Create a compiled model for the taret compute device\n    auto compiled_model = core.compile_model(model, \"MULTI\",\n                                             ov::device::priorities(available_devices[index]),\n                                             ov::hint::performance_mode(ov::hint::PerformanceMode::LATENCY),\n                                             ov::hint::inference_precision(ov::element::f32));\n\n    // Create an inference request\n    infer_request = compiled_model.create_infer_request();\n\n    // Get input tensor by index\n    input_tensor = infer_request.get_input_tensor(0);\n    // Get a pointer to the input tensor data\n    input_data = input_tensor.data&lt;float&gt;();\n\n    // Get output tensor\n    output_tensor = infer_request.get_output_tensor();\n    // Get a pointer to the output tensor data\n    out_data = output_tensor.data&lt;float&gt;();\n\n    // Replace the initial input dims with the updated values\n    image_dims[0] = input_w;\n    image_dims[1] = input_h;\n\n    // Return a value of 0 if the model loads successfully\n    return return_val;\n}\nDefine method to generate object detection proposals from the raw model output\nThe method to generate object proposals is nearly identical to the Python implementation from part 1.\n/// &lt;summary&gt;\n/// Generate object detection proposals from the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"out_ptr\"&gt;A pointer to the output tensor data&lt;/param&gt;\nvoid GenerateYoloxProposals(float* out_ptr, int proposal_length)\n{\n    // Remove the proposals for the previous model output\n    proposals.clear();\n\n    // Obtain the number of classes the model was trained to detect\n    int num_classes = proposal_length - 5;\n\n    for (int anchor_idx = 0; anchor_idx &lt; grid_strides.size(); anchor_idx++)\n    {\n        // Get the current grid and stride values\n        int grid0 = grid_strides[anchor_idx].grid0;\n        int grid1 = grid_strides[anchor_idx].grid1;\n        int stride = grid_strides[anchor_idx].stride;\n\n        // Get the starting index for the current proposal\n        int start_idx = anchor_idx * proposal_length;\n\n        // Get the coordinates for the center of the predicted bounding box\n        float x_center = (out_ptr[start_idx + 0] + grid0) * stride;\n        float y_center = (out_ptr[start_idx + 1] + grid1) * stride;\n\n        // Get the dimensions for the predicted bounding box\n        float w = exp(out_ptr[start_idx + 2]) * stride;\n        float h = exp(out_ptr[start_idx + 3]) * stride;\n\n        // Calculate the coordinates for the upper left corner of the bounding box\n        float x0 = x_center - w * 0.5f;\n        float y0 = y_center - h * 0.5f;\n\n        // Get the confidence score that an object is present\n        float box_objectness = out_ptr[start_idx + 4];\n\n        // Initialize object struct with bounding box information\n        Object obj = { x0, y0, w, h, 0, 0 };\n\n        // Find the object class with the highest confidence score\n        for (int class_idx = 0; class_idx &lt; num_classes; class_idx++)\n        {\n            // Get the confidence score for the current object class\n            float box_cls_score = out_ptr[start_idx + 5 + class_idx];\n            // Calculate the final confidence score for the object proposal\n            float box_prob = box_objectness * box_cls_score;\n\n            // Check for the highest confidence score\n            if (box_prob &gt; obj.prob)\n            {\n                obj.label = class_idx;\n                obj.prob = box_prob;\n            }\n        }\n\n        // Only add object proposals with high enough confidence scores\n        if (obj.prob &gt; bbox_conf_thresh) proposals.push_back(obj);\n    }\n\n    // Sort the proposals based on the confidence score in descending order\n    std::sort(proposals.begin(), proposals.end(), [](Object& a, Object& b) -&gt; bool\n              { return a.prob &gt; b.prob; });\n}\nDefine function to sort bounding box proposals using Non-Maximum Suppression\nThe C++ API for OpenCV has built-in functionality to perform comparison operations between rectangles. Therefore, we don’t need to define helper functions to calculate the intersection and union areas of two bounding boxes. Otherwise, the method to sort bounding box proposals using Non-Maximum Suppression is almost identical to the Python implementation from part 1.\n/// &lt;summary&gt;\n/// Filter through a sorted list of object proposals using Non-maximum suppression\n/// &lt;/summary&gt;\nvoid NmsSortedBboxes()\n{\n    // Remove the picked proposals for the previous model outptut\n    proposal_indices.clear();\n\n    // Iterate through the object proposals\n    for (int i = 0; i &lt; proposals.size(); i++)\n    {\n        Object& a = proposals[i];\n\n        // Create OpenCV rectangle for the Object bounding box\n        cv::Rect_&lt;float&gt; rect_a = cv::Rect_&lt;float&gt;(a.x0, a.y0, a.width, a.height);\n\n        bool keep = true;\n\n        // Check if the current object proposal overlaps any selected objects too much\n        for (int j : proposal_indices)\n        {\n            Object& b = proposals[j];\n\n            // Create OpenCV rectangle for the Object bounding box\n            cv::Rect_&lt;float&gt; rect_b = cv::Rect_&lt;float&gt;(b.x0, b.y0, b.width, b.height);\n\n            // Calculate the area where the two object bounding boxes overlap\n            float inter_area = (rect_a & rect_b).area();\n            // Calculate the union area of both bounding boxes\n            float union_area = rect_a.area() + rect_b.area() - inter_area;\n            // Ignore object proposals that overlap selected objects too much\n            if (inter_area / union_area &gt; nms_thresh)\n                keep = false;\n        }\n\n        // Keep object proposals that do not overlap selected objects too much\n        if (keep) proposal_indices.push_back(i);\n    }\n}\nDefine a function to perform inference\nWe will access the pixel data for the input image from Unity with a pointer to a uchar (unsigned 1-byte integer) array and wrap the data in a cv::Mat variable for processing.\nWe don’t need to normalize the input image since the IR model does it internally.\nAfter processing the model output, we’ll return the final number of detected objects to Unity so we can initialize an array of objects.\n\ncv::Mat: n-dimensional dense array class\n\ncv::cvtColor(): Converts an image from one color space to another\nov::InferRequest::infer(): Infers specified input in synchronous mode\nov::InferRequest::get_output_tensor(): Gets an output tensor for inference\n\n\n/// &lt;summary&gt;\n/// Perform inference with the provided texture data\n/// &lt;/summary&gt;\n/// &lt;param name=\"image_data\"&gt;The source image data from Unity&lt;/param&gt;\n/// &lt;returns&gt;The final number of detected objects&lt;/returns&gt;\nDLLExport int PerformInference(uchar* image_data) \n{\n\n    // Store the pixel data for the source input image in an OpenCV Mat\n    cv::Mat input_image = cv::Mat(img_h, img_w, CV_8UC4, image_data);\n\n    // Remove the alpha channel\n    cv::cvtColor(input_image, input_image, cv::COLOR_RGBA2RGB);\n\n    // Resize the input image\n    cv::resize(input_image, input_image, cv::Size(input_w, input_h));\n\n    // Iterate over each pixel in image\n    for (int p = 0; p &lt; n_pixels; p++)\n    {\n        input_data[0*n_pixels + p] = input_image.data[p*n_channels + 0] / 255.0f;\n        input_data[1*n_pixels + p] = input_image.data[p*n_channels + 1] / 255.0f;\n        input_data[2*n_pixels + p] = input_image.data[p*n_channels + 2] / 255.0f;\n    }\n\n    // Perform inference\n    infer_request.infer();\n\n    // Generate new proposals for the current model output\n    GenerateYoloxProposals(out_data, output_tensor.get_shape()[2]);\n\n    // Pick detected objects to keep using Non-maximum Suppression\n    NmsSortedBboxes();\n\n    // return the final number of detected objects\n    return (int)proposal_indices.size();\n}\nDefine a function to populate an array of objects from Unity\nNext, we’ll define a function to populate an array of objects from Unity. We call this function after initializing the list based on the current number of detected objects. We’ll also scale the bounding box information from the input dimensions to the source image resolution.\n/// &lt;summary&gt;\n/// Fill the provided array with the detected objects\n/// &lt;/summary&gt;\n/// &lt;param name=\"objects\"&gt;A pointer to a list of objects from Unity&lt;/param&gt;\nDLLExport void PopulateObjectsArray(Object* objects) \n{\n\n    for (int i = 0; i &lt; proposal_indices.size(); i++)\n    {\n        Object obj = proposals[proposal_indices[i]];\n\n        // Adjust offset to source image resolution and clamp the bounding box\n        objects[i].x0 = std::min(obj.x0 / scale_x, (float)img_w);\n        objects[i].y0 = std::min(obj.y0 / scale_y, (float)img_h);\n        objects[i].width = std::min(obj.width / scale_x, (float)img_w);\n        objects[i].height = std::min(obj.height / scale_y, (float)img_h);\n\n        objects[i].label = obj.label;\n        objects[i].prob = obj.prob;\n    }\n}\nDefine a function to reset the vectors when the Unity application exits\nThis last function will free the memory allocated by the vectors. We’ll call it when the Unity application shuts down.\n/// &lt;summary&gt;\n/// Reset vectors\n/// &lt;/summary&gt;\nDLLExport void FreeResources() \n{\n    available_devices.clear();\n    grid_strides.clear();\n    proposals.clear();\n    proposal_indices.clear();\n}\nThat is all the code needed for the plugin. We can now build the solution to generate the DLL file."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#build-solution",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#build-solution",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Build Solution",
    "text": "Build Solution\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. Visual Studio will generate a new x64 folder in the project directory containing the DLL file and its dependencies."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#gather-dependencies",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#gather-dependencies",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nRight-click the project name in the Solution Explorer panel and select Open Folder in File Explorer from the popup menu.\n\n\n\n\n\nIn the new File Explorer window, go to the parent folder.\n\n\n\n\n\nOpen the x64 → Release subfolder.\n\n\n\n\n\nWe’ll need to copy all the DLL files in this folder and the plugins.xml file to the Unity project."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-2/index.html#summary",
    "href": "posts/icevision-openvino-unity-tutorial/part-2/index.html#summary",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2",
    "section": "Summary",
    "text": "Summary\nIn this post, we learned how to create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO. We installed OpenVINO, created a DLL project in Visual Studio, and configured the project by adding include directories and linking libraries. We also set up post-build events to gather the required dependencies for our DLL file and updated the precompiled header and dllmain files to include the necessary code for object detection. Finally, we built the project to generate the DLL file. In part 3, we will integrate this DLL into a Unity project to perform real-time object detection with a YOLOX model.\nPrevious: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1\nNext: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "",
    "text": "Overview\nCreate New Project\nImport Assets\nAllow Unsafe Code\nCreate Processing Shader\nCreate Object Detector Script\nSet up Unity Scene\nTest in Editor\nSummary"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#tutorial-links",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#tutorial-links",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a YOLOX model using IceVision and export it to OpenVINO.\nPart 2: Create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\nPart 3: Perform object detection in a Unity project with OpenVINO.\nGitHub Repository"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#overview",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#overview",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Overview",
    "text": "Overview\nIn part 3 of this tutorial series, we will integrate the trained YOLOX model into a Unity project to perform real-time object detection. We will begin by creating a new Unity project and importing the necessary assets. Then, we will allow unsafe code in our project to share input data with the DLL file. Next, we will create a processing shader and an object detector script to handle object detection in our Unity project. Lastly, we will set up the Unity scene and test our object detection model in the editor. By the end of this post, you will know how to add object detection functionality to a Unity project.\n\nImportant: This post assumes you already have Unity Hub on your system. Check out this section from a previous tutorial if this is not the case (link)."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-new-project",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-new-project",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Create New Project",
    "text": "Create New Project\nOpen the Unity Hub and click New Project.\n\n\n\n\n\nSelect the target editor version from the Editor Version dropdown menu. We’ll use Unity 2022 for this post, but the current LTS release should also work fine.\n\n\n\n\n\nSelect the 2D Core template.\n\n\n\n\n\nPick a name for the project and a location for the project folder.\n\n\n\n\n\nFinally, click Create Project in the lower right-hand corner."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#import-assets",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#import-assets",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Import Assets",
    "text": "Import Assets\nOnce the project loads, we’ll store the DLL files from part 2 in a new folder called Plugins. Right-click a space in the Assets section and select Create → Folder from the popup menu.\n\n\n\n\n\nThe DLL targets 64-bit x86 architectures, so we need to place the DLL files in a subfolder named x86_64.\n\nPlugins Folder Google Drive\n\n\n\n\n\n\n\nNote: You can place the Plugins folder inside another folder if needed.\n\nCopy all the DLL files and the plugins.xml file into the Assets/Plugins/x86_64 folder. We then need to close and reopen the project for Unity to load the plugin files.\n\n\n\n\n\nBack in the Unity Editor, create a new folder called Colormaps to store the JSON file from part 1.\n\nColormaps Folder Google Drive\n\n\n\n\n\n\nWe place any test images into a new folder called Images.\n\nImages Folder Google Drive\n\n\n\n\n\n\nNext, we’ll create a folder to store the OpenVINO IR models. We need to place the XML and BIN files for the IR models in a StreamingAssets folder to include them in project builds. Create a new folder named StreamingAssets. We’ll place files for each model in a separate folder and put those in a new subfolder called OpenVINOModels to keep things organized.\n\nOpenVINOModels Folder Google Drive\n\n\n\n\n\n\nThe plugins.xml file included with the DLL files contains locations for the DLL files needed for using different types of devices.\nplugins.xml content:\n&lt;ie&gt;\n    &lt;plugins&gt;\n        &lt;plugin name=\"AUTO\" location=\"openvino_auto_plugin.dll\"&gt;\n            &lt;properties&gt;\n                &lt;property key=\"MULTI_WORK_MODE_AS_AUTO\" value=\"YES\"/&gt;\n            &lt;/properties&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"BATCH\" location=\"openvino_auto_batch_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"CPU\" location=\"openvino_intel_cpu_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"GNA\" location=\"openvino_intel_gna_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"GPU\" location=\"openvino_intel_gpu_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"HETERO\" location=\"openvino_hetero_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"MULTI\" location=\"openvino_auto_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"MYRIAD\" location=\"openvino_intel_myriad_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"HDDL\" location=\"openvino_intel_hddl_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n        &lt;plugin name=\"VPUX\" location=\"openvino_intel_vpux_plugin.dll\"&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/ie&gt;\nIt needs to be in the same folder as the DLL files for the plugin to work. However, Unity does not include XML files in the Plugins folder when building the project. We need to store a copy of the plugins.xml file in the StreamingAssets folder and then copy it back to the Plugins/x86_64 folder when first running the built project. We can handle both steps automatically in code."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#allow-unsafe-code",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Allow Unsafe Code",
    "text": "Allow Unsafe Code\nRather than copying the input image from Unity to the OpenVINO plugin, we’ll pass a pointer to the pixel data. First, we need to allow unsafe code for the Unity project. Select Edit → Project Settings... from the top menu.\n\n\n\n\n\nOpen the Player → Other Settings dropdown and scroll down to the Allow 'unsafe' Code checkbox. Enable the setting and close the Project Settings window.\n\n\n\n\n\nNow we can start coding."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-processing-shader",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-processing-shader",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Create Processing Shader",
    "text": "Create Processing Shader\nThe input image gets flipped upside down when we send it to the plugin. We can pre-flip the image in a Compute Shader. We’ll add the Compute Shader in a new folder called Shaders. Right-click a space in the Shaders folder and select Create → Shader → Compute Shader.\n\n\n\n\n\nName the Compute Shader ProcessingShader and open it in the code editor.\nDefault Compute Shader Code\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\nWe need to add a new Texture2D variable to store the pixel data for the input image. We’ll remove the default method and create a new one called FlipXAxis. Replace the default method name in the #pragma kernel line at the top.\nWe need the input image height for the flip operation, which we can access with the Texture2D::GetDimensions function.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel FlipXAxis\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Flip the image around the x-axis\n[numthreads(8, 8, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Stores the InputImage width\n    uint width;\n    // Stores the InputImage height\n    uint height;\n    // Get the dimensions of the InputImage\n    InputImage.GetDimensions(width, height);\n\n    // Update the y value for the pixel coordinates\n    int2 coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-object-detector-script",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#create-object-detector-script",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Create Object Detector Script",
    "text": "Create Object Detector Script\nWe’ll store the C# script that interacts with the OpenVINO plugin in a new Scripts folder. Right-click a space inside it and select Create → C# Script.\n\n\n\n\n\nName the script ObjectDetector and open it in the code editor.\n\n\n\n\n\nDefault script code\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class ObjectDetector : MonoBehaviour\n{\n    // Start is called before the first frame update\n    void Start()\n    {\n        \n    }\n\n    // Update is called once per frame\n    void Update()\n    {\n        \n    }\n}\nAdd required namespaces\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\nSystem.Runtime.InteropServices: Provides a wide variety of members that support COM interop and platform invoke services.\nSystem.IO: Allows reading and writing to files and data streams.\n\n\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Rendering;\nusing System;\nusing UnityEngine.UI;\nusing System.Runtime.InteropServices;\nusing System.IO;\nAdd code to copy plugins.xml file to StreamingAssets folder\nUnity provides an InitializeOnLoad attribute to run code in the Unity Editor without requiring action from the user. This attribute requires the UnityEditor namespace. We can only use this while in the Editor, so we need to wrap the code in Conditional compilation preprocessor directives. We’ll place this code right below the namespaces.\n#if UNITY_EDITOR\nusing UnityEditor;\n\n[InitializeOnLoad]\npublic class Startup\n{\n    static Startup()\n    {\n        // Get all files named \"plugins.xml\"\n        string[] files = Directory.GetFiles(\"./Assets/\", \"plugins.xml\", SearchOption.AllDirectories);\n        // Iterate through each found file\n        foreach (string file in files)\n        {\n            // Check if the file is in the \"x86_64\" folder\n            if (file.Contains(\"x86_64\"))\n            {\n                // Define file path for StreamingAssets folder\n                string targetPath = $\"{Application.streamingAssetsPath}/plugins.xml\";\n                // Print the source file path\n                Debug.Log(file);\n                // Only copy the file to the StreamingAssets folder if it is not already present\n                if (!File.Exists(targetPath)) File.Copy(file, targetPath);\n            }\n        }\n    }\n}\n#endif\nWe use the UNITY_EDITOR scripting symbol to check whether we are in the Unity Editor. We are in the Editor, so it returns true, and the code executes.\n\n\n\n\n\nIf we check if we are not in the Unity Editor, it returns false, and the code block does not execute.\n\n\n\n\n\nWe can verify the code works by saving the script and going to the StreamingAssets folder in the Editor. The plugins.xml file should be present.\n\n\n\n\n\n\nDefine public variables\nWe’ll add the required public variables above the Start method. We will be able to access these variables in the Inspector tab. We can add Header attributes to organize the public variables in the Inspector tab and use Tooltip attributes to provide information about variables.\nDefine scene object variables\nFirst, we need a variable to access the screen object that displays either a test image or webcam input. We may or may not want to mirror the screen based on whether a webcam is facing the user.\n[Header(\"Scene Objects\")]\n[Tooltip(\"The Screen object for the scene\")]\npublic Transform screen;\n[Tooltip(\"Mirror the in-game screen.\")]\npublic bool mirrorScreen = true;\nDefine data processing variables\nNext, we’ll define the variables for processing model input. We can set the default target input resolution to 224 and use it to scale the source resolution while maintaining the original aspect ratio.\nWe’ll also add a public ComputeShader variable to access the ProcessingShader we made earlier.\nWe need to download the pixel data for the input image from the GPU to the CPU before passing it to the plugin. This step can cause a significant performance bottleneck, so we’ll add the option to read the model output asynchronously at the cost of a few frames of latency. This latency might cause the bounding box to trail slightly behind a fast-moving object on the screen. The effect should be minimal, provided the frame rate is high enough.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 224;\n[Tooltip(\"The compute shader for GPU processing\")]\npublic ComputeShader processingShader;\n[Tooltip(\"Asynchronously download input image from the GPU to the CPU.\")]\npublic bool useAsyncGPUReadback = true;\nDefine output processing variables\nWe pass in the JSON file containing the class labels as a TextAsset.\n[Header(\"Output Processing\")]\n[Tooltip(\"A json file containing the colormaps for object classes\")]\npublic TextAsset colormapFile;\n[Tooltip(\"Minimum confidence score for keeping detected objects\")]\n[Range(0,1f)]\npublic float minConfidence = 0.5f;\nDefine variables for debugging\nNext, we’ll add a Boolean variable to toggle printing debug messages to the console.\n[Header(\"Debugging\")]\n[Tooltip(\"Print debugging messages to the console\")]\npublic bool printDebugMessages = true;\nDefine webcam variables\nWe need to specify a desired resolution and framerate when using a webcam as input.\n[Header(\"Webcam\")]\n[Tooltip(\"Use a webcam as input\")]\npublic bool useWebcam = false;\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"The requested webcam framerate\")]\n[Range(0, 60)]\npublic int webcamFPS = 60;\nDefine variables for user interface\nWe’ll make a simple GUI that displays the predicted class, the current framerate, and controls for selecting webcam devices, models, and compute devices.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayBoundingBoxes = true;\n[Tooltip(\"Display number of detected objects\")]\npublic bool displayProposalCount = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.red;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available OpenVINO models\")]\npublic Dropdown modelDropdown;\n[Tooltip(\"The dropdown menu that lists available OpenVINO devices\")]\npublic Dropdown deviceDropdown;\nDefine public variables for the OpenVINO plugin\n[Header(\"OpenVINO\")]\n[Tooltip(\"The name of the openvino models folder\")]\npublic string openvinoModelsDir = \"OpenVINOModels\";\n\n\nDefine private variables\nWe’ll add the required private variables right below the public variables.\nDefine private webcam variables\nWe’ll keep a list of available webcam devices so users can switch between them. Unity renders webcam input to a WebcamTexture.\n// List of available webcam devices\nprivate WebCamDevice[] webcamDevices;\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n// The name of the current webcam  device\nprivate string currentWebcam;\nDefine input variables\nWe’ll update the dimensions and content of the screen object based on the test image or webcam.\nWhen using asynchronous GPU readback, we need one Texture that stores data on the GPU and one that stores data on the CPU.\n// The test image dimensions\nprivate Vector2Int imageDims;\n// The test image texture\nprivate Texture imageTexture;\n// The current screen object dimensions\nprivate Vector2Int screenDims;\n// The model GPU input texture\nprivate RenderTexture inputTextureGPU;\n// The model CPU input texture\nprivate Texture2D inputTextureCPU;\nDefine variable for tracking the current number of detected objects\n// Stores the number of detected objects\nprivate int numObjects;\nDefine variables for storing colormaps\nWe need to create a couple of classes to parse the JSON content.\n// A class for parsing in colormaps from a JSON file\n[System.Serializable]\nclass ColorMap { public string label; public float[] color; }\n// A class for reading in a list of colormaps from a JSON file\n[System.Serializable]\nclass ColorMapList { public List&lt;ColorMap&gt; items; }\n// Stores a list of colormaps from a JSON file\nprivate ColorMapList colormapList;\n// A list of colors that map to class labels\nprivate Color[] colors;\n// A list of single pixel textures that map to class labels\nprivate Texture2D[] colorTextures;\nDefine variables for tracking the framerate\nWe’ll define some variables to track the frame rate.\n// The current frame rate value\nprivate int fps = 0;\n// Controls when the frame rate value updates\nprivate float fpsTimer = 0f;\nDefine private variables for the OpenVINO plugin\n// File paths for the available OpenVINO models\nprivate List&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available OpenVINO models\nprivate List&lt;string&gt; modelNames = new List&lt;string&gt;();\n// Names of the available OpenVINO devices\nprivate List&lt;string&gt; openvinoDevices = new List&lt;string&gt;();\nDefine a struct for reading object information from the OpenVINO plugin\nWe need to create an Object struct for Unity to match the one we defined for the OpenVINO code, along with an array of Object structs that we’ll update with the PopulateObjectsArray() function.\n// Indicate that the members of the struct are laid out sequentially\n[StructLayout(LayoutKind.Sequential)]\n/// &lt;summary&gt;\n/// Stores the information for a single object\n/// &lt;/summary&gt; \npublic struct Object\n{\n    // The X coordinate for the top left bounding box corner\n    public float x0;\n    // The Y coordinate for the top left bounding box cornder\n    public float y0;\n    // The width of the bounding box\n    public float width;\n    // The height of the bounding box\n    public float height;\n    // The object class index for the detected object\n    public int label;\n    // The model confidence score for the object\n    public float prob;\n\n    public Object(float x0, float y0, float width, float height, int label, float prob)\n    {\n        this.x0 = x0;\n        this.y0 = y0;\n        this.width = width;\n        this.height = height;\n        this.label = label;\n        this.prob = prob;\n    }\n}\n\n// Stores information for the current list of detected objects\nprivate Object[] objectInfoArray;\nImport functions from the OpenVINO plugin\nWe pass the pointer to the input pixel data as an IntPtr.\n// Name of the DLL file\nconst string dll = \"OpenVINO_YOLOX_DLL\";\n\n[DllImport(dll)]\nprivate static extern int GetDeviceCount();\n\n[DllImport(dll)]\nprivate static extern IntPtr GetDeviceName(int index);\n\n[DllImport(dll)]\nprivate static extern void SetConfidenceThreshold(float minConfidence);\n\n[DllImport(dll)]\nprivate static extern int LoadModel(string model, int index, int[] inputDims);\n\n[DllImport(dll)]\nprivate static extern int PerformInference(IntPtr inputData);\n\n[DllImport(dll)]\nprivate static extern void PopulateObjectsArray(IntPtr objects);\n\n[DllImport(dll)]\nprivate static extern void FreeResources();\n\n\nDefine Initialization Methods\nWe first need to define some methods to initialize webcams, the screen object, any GUI dropdown menus, and the in-game camera.\nDefine method to initialize a webcam device\n/// &lt;summary&gt;\n/// Initialize the selected webcam device\n/// &lt;/summary&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the selected webcam device&lt;/param&gt;\nprivate void InitializeWebcam(string deviceName)\n{\n    // Stop any webcams already playing\n    if (webcamTexture && webcamTexture.isPlaying) webcamTexture.Stop();\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(deviceName, webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the webcam\n    webcamTexture.Play();\n    // Check if webcam is playing\n    useWebcam = webcamTexture.isPlaying;\n    // Update toggle value\n    useWebcamToggle.SetIsOnWithoutNotify(useWebcam);\n\n    Debug.Log(useWebcam ? \"Webcam is playing\" : \"Webcam not playing, option disabled\");\n}\nDefine method to initialize the in-scene screen object\n/// &lt;summary&gt;\n/// Resize and position an in-scene screen object\n/// &lt;/summary&gt;\nprivate void InitializeScreen()\n{\n    // Set the texture for the screen object\n    screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture = useWebcam ? webcamTexture : imageTexture;\n    // Set the screen dimensions\n    screenDims = useWebcam ? new Vector2Int(webcamTexture.width, webcamTexture.height) : imageDims;\n\n    // Flip the screen around the Y-Axis when using webcam\n    float yRotation = useWebcam && mirrorScreen ? 180f : 0f;\n    // Invert the scale value for the Z-Axis when using webcam\n    float zScale = useWebcam && mirrorScreen ? -1f : 1f;\n\n    // Set screen rotation\n    screen.rotation = Quaternion.Euler(0, yRotation, 0);\n    // Adjust the screen dimensions\n    screen.localScale = new Vector3(screenDims.x, screenDims.y, zScale);\n\n    // Adjust the screen position\n    screen.position = new Vector3(screenDims.x / 2, screenDims.y / 2, 1);\n}\nDefine method to get the available OpenVINO models\n/// &lt;summary&gt;\n/// Get the file paths for available OpenVION models\n/// &lt;/summary&gt;\nprivate void GetOpenVINOModels()\n{\n    // Get the paths for each model folder\n    foreach (string dir in System.IO.Directory.GetDirectories($\"{Application.streamingAssetsPath}/{openvinoModelsDir}\"))\n    {\n        string modelName = dir.Split('\\\\')[1];\n\n        modelNames.Add(modelName.Substring(0, modelName.Length));\n\n        // Get the paths for the XML file for each model\n        foreach (string file in System.IO.Directory.GetFiles(dir))\n        {\n            if (file.EndsWith(\".xml\"))\n            {\n                modelPaths.Add(file);\n            }\n        }\n    }\n}\nDefine method to get the names of available OpenVINO devices\n/// &lt;summary&gt;\n/// Get the names of the available OpenVINO devices\n/// &lt;/summary&gt;\nprivate void GetOpenVINODevices()\n{\n    // Get the number of available OpenVINO devices\n    int deviceCount = GetDeviceCount();\n\n    for (int i = 0; i &lt; deviceCount; i++)\n    {\n        openvinoDevices.Add(Marshal.PtrToStringAnsi(GetDeviceName(i)));\n    }\n}\nDefine method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nprivate void InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach(WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add OpenVINO model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n\n    // Remove default dropdown options\n    deviceDropdown.ClearOptions();\n    // Add OpenVINO device names to menu\n    deviceDropdown.AddOptions(openvinoDevices);\n    // Select the first option in the dropdown\n    deviceDropdown.SetValueWithoutNotify(0);\n}\nDefine method to initialize the in-scene camera object\n/// &lt;summary&gt;\n/// Resize and position the main camera based on an in-scene screen object\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of an in-scene screen object&lt;/param&gt;\nprivate void InitializeCamera(Vector2Int screenDims, string cameraName = \"Main Camera\")\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject camera = GameObject.Find(cameraName);\n    // Adjust the camera position to account for updates to the screenDims\n    camera.transform.position = new Vector3(screenDims.x / 2, screenDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    camera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the screenDims\n    camera.GetComponent&lt;Camera&gt;().orthographicSize = screenDims.y / 2;\n}\nDefine method to update the selected OpenVINO model\n/// &lt;summary&gt;\n/// Update the selected OpenVINO model\n/// &lt;/summary&gt;\npublic void UpdateOpenVINOModel()\n{\n    // Reset objectInfoArray\n    objectInfoArray = new Object[0];\n\n    int[] inputDims = new int[] {\n        inputTextureCPU.width,\n        inputTextureCPU.height\n    };\n\n    Debug.Log($\"Selected Device: {openvinoDevices[deviceDropdown.value]}\");\n\n    // Load the specified OpenVINO model\n    int return_msg = LoadModel(modelPaths[modelDropdown.value], deviceDropdown.value, inputDims);\n\n    SetConfidenceThreshold(minConfidence);\n\n    string[] return_messages = {\n        \"Model loaded and reshaped successfully\", \n        \"Failed to load model\",\n        \"Failed to reshape model input\",\n    };\n\n    Debug.Log($\"Updated input dims: {inputDims[0]} x {inputDims[1]}\");\n    Debug.Log($\"Return message: {return_messages[return_msg]}\");\n}\n\n\nDefine Awake Method\nWe’ll implement the code to copy the plugins.xml file from the StreamingAssets folder to the Plugins/x86_64 folder in the build folder in the Awake() method. The code should be inactive since we are in the Editor.\n// Awake is called when the script instance is being loaded\nprivate void Awake()\n{\n    #if !UNITY_EDITOR\n    // Define the path for the plugins.xml file in the StreamingAssets folder\n    string sourcePath = $\"{Application.streamingAssetsPath}/plugins.xml\";\n    // Define the destination path for the plugins.xml file\n    string targetPath = $\"{Application.dataPath}/Plugins/x86_64/plugins.xml\";\n    // Only copy the file if it is not already present at the destination\n    if (!File.Exists(targetPath)) File.Copy(sourcePath, targetPath);\n    #endif\n}\n\n\nDefine Start Method\nThe Start method is called once before the first frame update, so we’ll perform any required setup steps here.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Initialize list of color maps from JSON file\n    colormapList = JsonUtility.FromJson&lt;ColorMapList&gt;(colormapFile.text);\n    // Initialize the list of colors\n    colors = new Color[colormapList.items.Count];\n    // Initialize the list of color textures\n    colorTextures = new Texture2D[colormapList.items.Count];\n\n    // Populate the color and color texture arrays\n    for (int i = 0; i &lt; colors.Length; i++)\n    {\n        // Create a new color object\n        colors[i] = new Color(\n            colormapList.items[i].color[0],\n            colormapList.items[i].color[1],\n            colormapList.items[i].color[2]);\n        // Create a single-pixel texture\n        colorTextures[i] = new Texture2D(1, 1);\n        colorTextures[i].SetPixel(0, 0, colors[i]);\n        colorTextures[i].Apply();\n\n    }\n\n    // Get the file paths for available OpenVINO models\n    GetOpenVINOModels();\n    // Get the names of available OpenVINO devices\n    GetOpenVINODevices();\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n}\n\n\nDefine Processing Methods\nNext, we need to define methods to process images using the Compute Shader, calculate the input resolution, handle asynchronous GPU readback, and scale the bounding box information.\nDefine method to process images using a compute shader\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The target image RenderTexture&lt;/param&gt;\n/// &lt;param name=\"computeShader\"&gt;The target ComputerShader&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The target ComputeShader function&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, ComputeShader computeShader, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = computeShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = new RenderTexture(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    computeShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release RenderTexture\n    result.Release();\n}\nDefine method to calculate input resolution\n/// &lt;summary&gt;\n/// Scale the source image resolution to the target input dimensions\n/// while maintaing the source aspect ratio.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetDims\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate Vector2Int CalculateInputDims(Vector2Int imageDims, int targetDim)\n{\n    Vector2Int inputDims = new Vector2Int();\n\n    // Calculate the input dimensions using the target minimum dimension\n    if (imageDims.x &gt;= imageDims.y)\n    {\n        inputDims[0] = (int)(imageDims.x / ((float)imageDims.y / (float)targetDim));\n        inputDims[1] = targetDim;\n    }\n    else\n    {\n        inputDims[0] = targetDim;\n        inputDims[1] = (int)(imageDims.y / ((float)imageDims.x / (float)targetDim));\n    }\n\n    return inputDims;\n}\nDefine method to handle asynchronous GPU readback\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Make sure the Texture2D is not null\n    if (inputTextureCPU)\n    {\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        inputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Textur2D\n        inputTextureCPU.Apply();\n    }\n}\nDefine method to send the input texture data to the plugin\n/// &lt;summary&gt;\n/// Pin memory for the input data and pass a reference to the plugin for inference\n/// &lt;/summary&gt;\n/// &lt;param name=\"texture\"&gt;The input texture&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic unsafe int UploadTexture(Texture2D texture)\n{\n    //Pin Memory\n    fixed (byte* p = texture.GetRawTextureData())\n    {\n        // Perform inference and get the number of detected objects\n        numObjects = PerformInference((IntPtr)p);\n    }\n\n    // Initialize the array\n    objectInfoArray = new Object[numObjects];\n\n    // Pin memory\n    fixed (Object* o = objectInfoArray)\n    {\n        // Get the detected objects\n        PopulateObjectsArray((IntPtr)o);\n    }\n\n    return numObjects;\n}\nDefine method to scale bounding boxes to the display resolution\n/// &lt;summary&gt;\n/// Scale the latest bounding boxes to the display resolution\n/// &lt;/summary&gt;\npublic void ScaleBoundingBoxes()\n{\n    // Process new detected objects\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        // The smallest dimension of the screen\n        float minScreenDim = Mathf.Min(screen.transform.localScale.x, screen.transform.localScale.y);\n        // The smallest input dimension\n        int minInputDim = Mathf.Min(inputTextureCPU.width, inputTextureCPU.height);\n        // Calculate the scale value between the in-game screen and input dimensions\n        float minImgScale = minScreenDim / minInputDim;\n        // Calculate the scale value between the in-game screen and display\n        float displayScale = Screen.height / screen.transform.localScale.y;\n\n        // Scale bounding box to in-game screen resolution and flip the bbox coordinates vertically\n        float x0 = objectInfoArray[i].x0 * minImgScale;\n        float y0 = (inputTextureCPU.height - objectInfoArray[i].y0) * minImgScale;\n        float width = objectInfoArray[i].width * minImgScale;\n        float height = objectInfoArray[i].height * minImgScale;\n\n        // Mirror bounding box across screen\n        if (mirrorScreen && useWebcam) x0 = screen.transform.localScale.x - x0 - width;\n\n        // Scale bounding boxes to display resolution\n        objectInfoArray[i].x0 = x0 * displayScale;\n        objectInfoArray[i].y0 = y0 * displayScale;\n        objectInfoArray[i].width = width * displayScale;\n        objectInfoArray[i].height = height * displayScale;\n\n        // Offset the bounding box coordinates based on the difference between the in-game screen and display\n        objectInfoArray[i].x0 += (Screen.width - screen.transform.localScale.x * displayScale) / 2;\n    }\n}\n\n\nDefine Update method\nWe’ll place anything we want to run every frame in the Update method.\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputDims.x} x {inputDims.y}\");\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGBA32, false);\n        // Update the selected OpenVINO model\n        UpdateOpenVINOModel();\n    }\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Flip image before sending to DLL\n    ProcessImageGPU(inputTextureGPU, processingShader, \"FlipXAxis\");\n\n    // Download pixel data from GPU to CPU\n    if (useAsyncGPUReadback)\n    {\n        AsyncGPUReadback.Request(inputTextureGPU, 0, TextureFormat.RGBA32, OnCompleteReadback);\n    }\n    else\n    {\n        RenderTexture.active = inputTextureGPU;\n        inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n        inputTextureCPU.Apply();\n    }\n\n    // Send reference to inputData to DLL\n    numObjects = UploadTexture(inputTextureCPU);\n    if (printDebugMessages) Debug.Log($\"Detected {numObjects} objects\");\n    // Scale bounding boxes\n    ScaleBoundingBoxes();\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n}\n\n\nDefine GUI Methods\nWe need some methods to handle user interactions with the GUI and display the bounding boxes and current framerate.\nDefine method to update webcam usage from GUI\n/// &lt;summary&gt;\n/// This method is called when the value for the webcam toggle changes\n/// &lt;/summary&gt;\n/// &lt;param name=\"useWebcam\"&gt;&lt;/param&gt;\npublic void UpdateWebcamToggle(bool useWebcam)\n{\n    this.useWebcam = useWebcam;\n}\nDefine method to update webcam device from GUI\n/// &lt;summary&gt;\n/// The method is called when the selected value for the webcam dropdown changes\n/// &lt;/summary&gt;\npublic void UpdateWebcamDevice()\n{\n    currentWebcam = webcamDevices[webcamDropdown.value].name;\n    Debug.Log($\"Selected Webcam: {currentWebcam}\");\n    // Initialize webcam if it is not already playing\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n}\nDefine method to update the minimum confidence value\n/// &lt;summary&gt;\n/// Update the minimum confidence score for keeping bounding box proposals\n/// &lt;/summary&gt;\n/// &lt;param name=\"slider\"&gt;&lt;/param&gt;\npublic void UpdateConfidenceThreshold(Slider slider)\n{\n    minConfidence = slider.value;\n    SetConfidenceThreshold(minConfidence);\n}\nDefine OnGUI method\nWe’ll display the predicted bounding boxes and current frame rate in the OnGUI method.\n// OnGUI is called for rendering and handling GUI events.\npublic void OnGUI()\n{\n    // Initialize a rectangle for label text\n    Rect labelRect = new Rect();\n    // Initialize a rectangle for bounding boxes\n    Rect boxRect = new Rect();\n\n    GUIStyle labelStyle = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * 11e-3)\n    };\n    labelStyle.alignment = TextAnchor.MiddleLeft;\n\n    foreach (Object objectInfo in objectInfoArray)\n    {\n        if (!displayBoundingBoxes) break;\n\n        // Skip object if label index is out of bounds\n        if (objectInfo.label &gt; colors.Length - 1) continue;\n\n        // Get color for current class index\n        Color color = colors[objectInfo.label];\n        // Get label for current class index\n        string name = colormapList.items[objectInfo.label].label;\n\n        // Set bounding box coordinates\n        boxRect.x = objectInfo.x0;\n        boxRect.y = Screen.height - objectInfo.y0;\n        // Set bounding box dimensions\n        boxRect.width = objectInfo.width;\n        boxRect.height = objectInfo.height;\n\n        // Scale bounding box line width based on display resolution\n        int lineWidth = (int)(Screen.width * 1.75e-3);\n        // Render bounding box\n        GUI.DrawTexture(\n            position: boxRect,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: lineWidth,\n            borderRadius: 0);\n\n        // Include class label and confidence score in label text\n        string labelText = $\" {name}: {(objectInfo.prob * 100).ToString(\"0.##\")}%\";\n\n        // Initialize label GUI content\n        GUIContent labelContent = new GUIContent(labelText);\n\n        // Calculate the text size.\n        Vector2 textSize = labelStyle.CalcSize(labelContent);\n\n        // Set label text coordinates\n        labelRect.x = objectInfo.x0;\n        labelRect.y = Screen.height - objectInfo.y0 - textSize.y + lineWidth;\n\n        // Set label text dimensions\n        labelRect.width = Mathf.Max(textSize.x, objectInfo.width);\n        labelRect.height = textSize.y;\n        // Set label text and backgound color\n        labelStyle.normal.textColor = color.grayscale &gt; 0.5 ? Color.black : Color.white;\n        labelStyle.normal.background = colorTextures[objectInfo.label];\n        // Render label\n        GUI.Label(labelRect, labelContent, labelStyle);\n\n        Rect objectDot = new Rect();\n        objectDot.height = lineWidth * 5;\n        objectDot.width = lineWidth * 5;\n        float radius = objectDot.width / 2;\n        objectDot.x = (boxRect.x + boxRect.width / 2) - radius;\n        objectDot.y = (boxRect.y + boxRect.height / 2) - radius;\n\n\n        GUI.DrawTexture(\n            position: objectDot,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: radius,\n            borderRadius: radius);\n\n    }\n\n    // Define styling information for GUI elements\n    GUIStyle style = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * (1f / (100f - fontScale)))\n    };\n    style.normal.textColor = textColor;\n\n    // Define screen spaces for GUI elements\n    Rect slot1 = new Rect(10, 10, 500, 500);\n    Rect slot2 = new Rect(10, style.fontSize * 1.5f, 500, 500);\n\n    string content = $\"Objects Detected: {numObjects}\";\n    if (displayProposalCount) GUI.Label(slot1, new GUIContent(content), style);\n\n    // Update framerate value\n    if (Time.unscaledTime &gt; fpsTimer)\n    {\n        fps = (int)(1f / Time.unscaledDeltaTime);\n        fpsTimer = Time.unscaledTime + fpsRefreshRate;\n    }\n\n    // Adjust screen position when not showing predicted class\n    Rect fpsRect = displayProposalCount ? slot2 : slot1;\n    if (displayFPS) GUI.Label(fpsRect, new GUIContent($\"FPS: {fps}\"), style);\n}\n\n\nDefine OnDisable Method\nWe’ll perform any clean-up steps in the OnDisablemethod.\nprivate void OnDisable()\n{\n    FreeResources();\n}"
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#set-up-unity-scene",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#set-up-unity-scene",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Set up Unity Scene",
    "text": "Set up Unity Scene\nNow we can start setting up our Unity scene. We need a screen to display the webcam feed, an empty object to attach the object detector script, dropdown menus for selecting webcams, models, and compute devices, a toggle to activate a webcam feed, and a slider to update the confidence threshold.\nCreate Screen object\nRight-click a space in the Hierarchy tab and select 3D Object → Quad. We can name the new object Screen.\n\n\n\n\n\nNext, drag and drop a test image from the Assets → Images folder onto the Screen object in the Scene view. Note that the Screen looks a bit dim. We need to change the shader for the Screen’s Material so that it does not require an external light source.\n\n\n\n\n\nSelect the Screen in the Hierarchy tab and open the Shader dropdown menu in the Inspector tab. Type Unlit/Texture into the search box and press enter.\n\n\n\n\n\nCreate Inference Manager object\nRight-click a space in the Hierarchy tab and select Create Empty. Name the empty object InferenceManager.\n\n\n\n\n\nWith the InferenceManager object selected, drag the ObjectDetector script into the Inspector tab.\n\n\n\n\n\nNow we can assign the screen object, compute shader, and colormap file in the Inspector tab by dragging them into their respective fields.\nAdd GUI prefab\nWe still need to create the GUI controls. To save time, I made a Prefab that we can drop into the Scene.\n\nGoogle Drive: Canvas Prefab\n\nDrag and drop the Canvas prefab into a new folder called Prefabs.\n\n\n\n\n\nFrom there, drag the prefab into the Hierarchy tab. We can see the GUI by switching to the Game view.\n\n\n\n\n\nConfigure Webcam Toggle On Value Changed function\nNext, we need to pair the WebcamToggle with the UpdateWebcamToggle function in the ObjectDetector script. Expand the Canvas object and select the WebcamToggle.\n\n\n\n\n\nClick and drag the InferenceManager into the On Value Changed field.\n\n\n\n\n\nOpen the No Function dropdown menu and select ObjectDetector → UpdateWebcamToggle.\n\n\n\n\n\nConfigure Webcam Dropdown On Value Changed function\nWe can follow the same steps to pair the WebcamDropdown with the UpdateWebcamDevice function in the ObjectDetector script.\n\n\n\n\n\nThis time select ObjectDetector → UpdateWebcamDevice.\n\n\n\n\n\nConfigure OpenVINOModelDropdown On Value Changed Event\n\n\n\n\n\nConfigure OpenVINODeviceDropdown On Value Changed Event\n\n\n\n\n\nConfigure ConfidenceThresholdSlider On Value Changed Event\n\n\n\n\n\nAssign GUI objects to Inference Manager\nWe can now assign the GUI objects to their respective fields for the ObjectDetector script.\n\n\n\n\n\nAdd Event System\nBefore we can use the GUI, we need to add an Event System. Right-click a space in the Hierarchy tab and select UI → Event System."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#test-in-editor",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#test-in-editor",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Test in Editor",
    "text": "Test in Editor\nClick the play button in the top-middle of the Editor window to test the project.\n\n\n\n\n\nThere should be a bounding box for the call sign and one for the idle hand."
  },
  {
    "objectID": "posts/icevision-openvino-unity-tutorial/part-3/index.html#summary",
    "href": "posts/icevision-openvino-unity-tutorial/part-3/index.html#summary",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3",
    "section": "Summary",
    "text": "Summary\nIn this three-part tutorial series, we learned how to perform end-to-end object detection in Unity using IceVision and OpenVINO. In part 1, we trained a YOLOX model using IceVision and exported it to OpenVINO. In part 2, we created a dynamic link library (DLL) file in Visual Studio to perform object detection with the YOLOX model using OpenVINO. Finally, in this post, we integrated the trained model into a Unity project to perform real-time object detection. You now have a template to perform object detection in Unity that you can adapt to other projects.\nPrevious: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/in-game-style-transfer/index.html",
    "href": "posts/in-game-style-transfer/index.html",
    "title": "In-Game Style Transfer",
    "section": "",
    "text": "I’ve been fascinated with style transfer models ever since I first learned about them a few years ago. Most available style transfer applications are meant for single images. Interestingly, Unity seems to have another use case in mind. The home page for the Barracuda library has an image of what appears to be in-game style transfer.\n\n\n\n\n\nThis was such an intriguing idea that I decided to give it a shot before getting started on my pose estimation project. One concern I had going in was performance. The style transfer models I’d used previously weren’t exactly designed for real-time inference. This concern turned out to be well founded. The frame rates for my first attempt were so low it looked like a stop motion film.\nAt the time, I’d assumed the low frame rates were purely due to the performance demands of the style transfer model. I didn’t realize that my methods for pre and post processing were needlessly inefficient. What’s more, the output looked really bad. I learned much later that this was because I wasn’t implementing the processing steps correctly. I decided to drop the whole endeavor and moved on to pose estimation.\nWorking on my pose estimation project taught me a lot about how to speed up the processing steps in Unity. Most importantly, I learned how to use compute shaders to perform preprocessing steps on the GPU. I decided to see if this was enough to get a style transfer model to run at playable frame rates. I decided to use the demo project for Unity’s Kinematica package to make things more interesting to look at.\n\nFor testing purposes, I started with one of the pretrained fast style transfer models provided in the ONNX model zoo. This particular model was trained to apply the style of the mosaic image below. I plan to use my own style transfer models in the future.\n\n\n\n\n\nMoving the processing steps from the CPU to the GPU allowed me to get “cinematic” frame rates. However, I needed to keep the resolution at or below 720p. Even at those resolutions, my GPU was pinned at 99-100% utilization the whole time. This might make a decent benchmark for the new RTX 3000 cards.\nMoving both the pre and post processing steps to the GPU also made it easier to find out what I did wrong with my initial implementation. First, the RGB values for the input image need to be in the range of [0, 255]. This is worth noting because of an odd characteristic of the Barracuda library. When a tensor is created from a Texture2D (i.e. a regular image) its values are normalized to the range [0, 1].\nIf you need your values to be outside the default range, you have two options. You can create the tensor using a float array, which needs to be done on the CPU. Or, you can use an HDR texture. Fortunately, I already learned how to use HDR textures while working on my pose estimation project.\nThe output of the model is also in the general range of [0, 255]. Although, you do need to clip the values just in case the model overshoots a bit. Since color values in Unity are typically in the range of [0, 1], you also need to scale the output back down to that range. This is the main reason my first attempt looked so bad. Trying to load the raw model output back into a Texture2D made nearly every pixel value pure white. It was relatively easy to implement the proper post processing steps with a compute shader after some research.\nMy current results might need a seizure warning, but they are still way better than my first attempt. The flickering effect is there because the model I’m using doesn’t account for temporal cohesion between frames. Hopefully, I will be able to get a style transfer model working in Unity that’s meant for video in the near future.\n\nMy Results:\n\n\nVideo\n\n\n\n\nFuture Work:\nThis recently released project on GitHub is very promising. Not only does it perform way better with videos, but it’s also possible to modify the style while your using it. I haven’t tried exporting the model to ONNX yet but I plan to fairly soon. Even if it’s fully supported by ONNX, I’ll still need to see if the Barracuda library supports it. But, it should be worth the effort to get it working. You can see just how much smoother video from this new model is in this short sample I made with it.\n\n\nVideo\n\n\nIt’s not 100% perfect. There’s still a tiny bit of flickering in the background. However, this was made with just two sample frames. Also that mosaic image makes it more likely for such flickering to appear. If we use a different style image, the flickering is basically nonexistent. The noise is present because the Gif below was made from an mp4 rather than PNGs."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "",
    "text": "Introduction\nKinematica Image Dataset\nVideo Stylization Model\nUnity’s Implementation: First Impressions\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "Introduction",
    "text": "Introduction\nI spent a bit of time this week messing around with different style transfer experiments. I wanted to see if training the fast_neural_style model on images from the Kinematica demo would improve the output quality. I also got the model from the Interactive Video Stylization Using Few-Shot Patch-Based Training project working in Unity. Lastly, I started exploring Unity’s style transfer project. Let’s just say that calling my implementation basic was an understatement."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html#kinematica-image-dataset",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html#kinematica-image-dataset",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "Kinematica Image Dataset",
    "text": "Kinematica Image Dataset\nI used OBS to record an approximately 13 minute video of the character running around the Kinematica demo. I then split the video into separate frames using ffmpeg. I also created mirrored copies of the frames to double the size of my dataset. I ended up resizing the images to 640 x 480 to speed up training. My experiment resulted in noticeably less flickering when running the demo. Although, I still find the flickering in my results a bit distracting. I’m curious if I could further reduce flickering by training the model for longer. There are also some slight changes in color between the two models. However, the differences were mostly lost when creating the Gifs below.\n\nCOCO 2014 Training Images Dataset\n\n\n\n\n\n\n\nKinematica Demo Image Dataset"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html#video-stylization-model",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html#video-stylization-model",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "Video Stylization Model",
    "text": "Video Stylization Model\nTraining this style transfer model is a bit more involved than the one I’ve been using so far. First, this model doesn’t learn from a source style image like the one below.\n\n\n\n\n\nInstead, you need to provide a few stylized examples of images from your training dataset. I just used the fast_neural_style model to generate these examples. However, you need to put in a bit more work to get the best results. This involves creating masks for each image in the dataset like the one below.\n\n\n\n\n\nYou also need to generate noise for these masks as shown below.\n\n\n\n\n\nYou can technically just make masks and noise for the whole image rather than for specific parts. However, I didn’t feel like doing that just yet. I wanted to see how the model ran in Unity first, so I used one of the sample training datasets provided for the project. Specifically, I used the lynx dataset.\n\n\n\n\n\n\nTraining Results\nAs you can see below, this model produces much less flickering than the fast_neural_style model. The next step was to see how well this transferred to Unity.\n\n\nVideo\n\n\n\n\nUnity Performance\nThe model did a surprisingly okay job stylizing the Kinematica demo despite having only trained on one hundred images of a lynx. Flickering is significantly reduced and it didn’t even give me any headaches importing the ONNX file into Unity. The only catch was performance.\n\n\n\n\n\nOn my desktop, the fast_neural_style model I’ve been using runs 720 x 540 at approximately 25fps. This new model runs the same resolution at about 9fps. You’d probably need to wait a few generations of GPUs before you could get playable frame rates with the model as is. It would take some insane optimization to make this viable for in-game style transfer. Fortunately, Unity has already figured out how to do some insane optimization for their style transfer project."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html#unitys-implementation-first-impressions",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html#unitys-implementation-first-impressions",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "Unity’s Implementation: First Impressions",
    "text": "Unity’s Implementation: First Impressions\nIn short, I have some homework to do. I’ve only glanced through the code for Unity’s project so far, but it’s easy to see why they took so long to release it. They’ve put a lot of work into optimizing the performance of their model. With the default settings, I was consistently getting around 400fps.\nNote: Performance dropped slightly when recording for the Gifs below. Hence, the displayed fps is a bit lower.\n\n\nVideo\n\n\nThe actual scene is about as simple as it gets so I can’t directly compare the lack of flickering just yet. I’ll wait until I get this running in the Kinematica demo for that.\n\n\n\n\n\nHowever, the performance numbers speak for themselves. The team at Unity did a fantastic job with optimization. What’s more, their method for optimizing the performance looks like it should transfer to other models. I want to try applying their method to the PoseNet project as well as the style transfer models I’ve been working with. Although, the optimization process appears quite involved so I’ll need to study it a bit more before attempting that."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-1/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-1/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.1",
    "section": "Conclusion",
    "text": "Conclusion\nMy experiments provided some useful insights in how I should move forward with future style transfer experiments. Training the models on images from the target game seems worthwhile to reduce flickering. I’ll see if letting the model train overnight will further reduce flickering.\nThe performance from the video stylization model was lower than I expected. I thought there might be some decrease in frame rate, but I did not expect it to drop by roughly two thirds. In hindsight, I guess it’s not unreasonable. The video stylization model is double the size at 13MB versus 6.5MB for the fast_neural_style model.\nI’m now even more grateful that Unity has released their example project. It applies a level of expertise that would have taken me a long time to figure out on my own. However, it also shows just how much work still remains in optimizing more sophisticated models for end-user devices. I’m curious if it’s feasible to automate this optimization process."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-2/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-2/index.html",
    "title": "In-Game Style Transfer Experiments Pt.2",
    "section": "",
    "text": "Introduction\nLonger Training Session Results\nUnity’s Implementation in Kinematica Demo\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-2/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-2/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.2",
    "section": "Introduction",
    "text": "Introduction\nTo follow up on the last post, I let the fast_neural_style model train overnight. I also got Unity’s style transfer implementation working in the Kinematica demo so I could directly compare it to other models. The results for both were a bit disappointing."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-2/index.html#longer-training-session-results",
    "href": "posts/in-game-style-transfer-experiments/part-2/index.html#longer-training-session-results",
    "title": "In-Game Style Transfer Experiments Pt.2",
    "section": "Longer Training Session Results",
    "text": "Longer Training Session Results\nThe longer training session actually resulted in more noticeable flickering than the shorter training session. The flickering is still lower than the baseline at least. The model never seemed to settle on how to apply the style during training. The model might be able to get more consistent if I fiddled with the hyperparameters. It would take a long time to find the ideal values, so I’ll hold off on that.\n\nCOCO 2014 Training Images Dataset\n\n\n\n\n\n\n\nKinematica Demo Image Dataset\n\n\n\n\n\n\n\nLonger Training Session"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-2/index.html#unitys-implementation-in-kinematica-demo",
    "href": "posts/in-game-style-transfer-experiments/part-2/index.html#unitys-implementation-in-kinematica-demo",
    "title": "In-Game Style Transfer Experiments Pt.2",
    "section": "Unity’s Implementation in Kinematica Demo",
    "text": "Unity’s Implementation in Kinematica Demo\nThe good news is there is basically no flickering and the frame rate is over 3x higher than the fast_neural_style model at around 80fps. For reference, the Kinematica demo runs at around 120fps with no style transfer. The bad news is the quality of style transfer isn’t that great.\n\n\nVideo\n\n\nThere’s a few things that probably contribute to this difference in quality. First, the model Unity chose is trained to handle different styles without additional training. It’s trained on a wide variety of style images to help it generalize. This makes it more difficult to achieve the same level of quality for a specific style. Second, the team at Unity had to make some tradeoffs when optimizing the model’s performance. They modified the model’s architecture to reduce it’s overall size. This likely had a negative impact on quality. Third, there seems to be some manually tuned parameters in their demo project. I have no idea how much impact these have or how to approach modifying them just yet. However, I’d be surprised if they didn’t affect the quality of the stylized images.\nAs Unity stated in their blog post, improving the quality of style transfer while optimizing the model for real-time use is still an open research question. I think it might be easier to optimize models trained for specific styles rather than trying to use single models that support arbitrary styles. The specialized models don’t take up much disk space and can benefit from the same optimization methods that the Unity team has already developed."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-2/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-2/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.2",
    "section": "Conclusion",
    "text": "Conclusion\nThe video stylization model I tested in the last post seems the most promising for both quality and consistency between frames. The only roadblock is that it requires much more compute power. It might be worthwhile exploring methods to optimize this model. I’d like to try optimization techniques such as quantization and pruning. Both approaches aim to reduce the size of the model and thus reduce hardware requirements. However, I don’t know if the Barracuda library supports quantized models yet. I’m also curious how well the temporal upsampling from Unity’s implementation would work with the video stylization model. I’ve never used any of these optimization techniques before, so I don’t know how long it would take to get them working."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "",
    "text": "Introduction\nModel Quantization\nNetwork Pruning\nUsing a Smaller Model\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "Introduction",
    "text": "Introduction\nI finally got around to exploring different optimization methods to speed up the style transfer models in Unity. I started off with a couple post-training optimization techniques including quantization and pruning. I then tried training with smaller models. I should have started off with the smaller models."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html#model-quantization",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html#model-quantization",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "Model Quantization",
    "text": "Model Quantization\nI followed the example code in PyTorch’s documentation to quantize the fast_neural_style model. The quantized model did seem to perform inference faster in Python. Unfortunately, there was no performance change after importing the quantized model to Unity.\nTurns out it’s not yet possible to export quantized models to ONNX for general use. According to this forum post, quantized models can only be exported for Caffe2. While disappointing, this wasn’t too surprising. Quantization is still in beta for PyTorch.\nI then tried to quantize a regular ONNX model directly. This can be easily done using the Python tool provided with ONNX Runtime. I was able to reduce the size of the model from 6.5MB to 1.65MB by following the example code in the tool’s documentation. As I suspected though, the quantized ONNX model uses operators that are not currently supported by Barracuda. I’d be surprised if supported wasn’t added in the future, so I’ll retry with new releases."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html#network-pruning",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html#network-pruning",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "Network Pruning",
    "text": "Network Pruning\nI went through the pruning tutorial provided by PyTorch and didn’t encounter any issues applying the code to the fast_neural_style model. However, I didn’t encounter any performance improvements. After some more forum diving, I discovered that the pruning module is still an experimental feature. It’s not currently meant as a means to improve inference speed. Also, it turns out that GPUs currently aren’t optimized for the sparse networks that result from pruning. Apparently, this is starting to change with the latest RTX 30 series cards from Nvidia."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html#using-a-smaller-model",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html#using-a-smaller-model",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "Using a Smaller Model",
    "text": "Using a Smaller Model\nSo both of the fancy optimization techniques I wanted to try are currently dead ends for my use case. Fortunately, I finally thought to try the much simpler approach of just starting with a smaller model. This really should have been the first approach I tried.\nI kept the number and types of layers in the model the same but reduced the size for many of the layers. I made no changes to other parts of the code. This turned out to be quite effective.\n\nOriginal Model\n\n\n\n\n\n\n\nSmaller Model\n\n\n\n\n\nI was able to reduce the size of the model from 6.5MB to less than 600KB without any significant visual changes in the output. Frame rates improved significantly compared to the original model at a given resolution. The performance gap narrows as the resolution increases, but frame rates for the smaller model are approximately 2x higher than the original. You can see comparisons of output quality and frame rates at different resolutions below.\n\n\nResolution: 720 x 540\n\nOriginal Model\n\n\n\n\n\n\n\n\n\n\n\n\nSmaller Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nResolution: 1280 x 720\n\nOriginal Model\n\n\n\n\n\n\n\n\n\n\n\n\nSmaller Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nResolution: 1920 x 1080\n\nOriginal Model\n\n\n\n\n\n\n\n\n\n\n\n\nSmaller Model"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-3/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-3/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.3",
    "section": "Conclusion",
    "text": "Conclusion\nNow I’m really curious to see how an RTX 3080 or RX6800 XT would perform with this smaller model. 1080p is still out of reach, but 720p should be easily playable on the latest generation of GPUs. The next step is to see how much I can reduce the size of the video style transfer model I tried previously. That model is roughly twice the size of the original fast_neural_style model. I might need to do more than reduce the size of some of the layers to get playable framerates."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "",
    "text": "Introduction\nShrinking the Model\nResults in Unity\nTraining vs Unity\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "Introduction",
    "text": "Introduction\nI followed up on the results in the last post by testing how much I could shrink the video stylization model. I was initially skeptical since that model is twice the size of the fast_neural_style model. However, the model was easy to modify using the config files provided in the GitHub repository. The hard part turned out to be getting the output in Unity to match the output during training."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html#shrinking-the-model",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html#shrinking-the-model",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "Shrinking the Model",
    "text": "Shrinking the Model\nI was able to shrink the model from 13MB to less than 1MB by modifying two lines in the config file. This involved significantly reducing the number and size of layers in the model.\n\n\n\n\n\nFortunately, this didn’t seem to have any significant impact on the quality of the output. The model did however need to be trained longer to achieve similar results."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html#results-in-unity",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html#results-in-unity",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "Results in Unity",
    "text": "Results in Unity\nThe modified video stylization model has better performance than the original fast_neural_style model, but is still far behind the smaller variant. On the plus side, flickering is still significantly reduced over the fast_neural_style model.\n\nResolution: 720 x 540"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html#training-vs-unity",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html#training-vs-unity",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "Training vs Unity",
    "text": "Training vs Unity\nSomething that immediately stuck out was how differently this model stylizes the scene compared to the fast_neural_style model. This is surprising since the video stylization model was trained using output from the fast neural_style model. The model output during training was much closer to the fast_neural_style model as well.\nAt first, I thought the difference was because I didn’t implement the preprocessing steps correctly. It was a bit of a pain figuring out what preprocessing operations I needed to apply from the source code. It seems to boil down to normalizing the RGB color values to the range [-1,1]. The output didn’t seem to look right any other way, so I stuck with that.\nI believe the difference is due to how this video stylization model is trained. It trains on only a handful of stylized images in a particular scene. It’s possible that the model will require a much larger training set than normal to handle an entire video game level."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-4/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-4/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.4",
    "section": "Conclusion",
    "text": "Conclusion\nI did not expect this model to perform as well as it did with so few layers. It’s likely that the model can be further optimized with a more thoughtful approach. I’m going to conduct more training experiments to see how much I can make this model generalize. Hopefully, the smaller model can handle a wider variety of input."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-5/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-5/index.html",
    "title": "In-Game Style Transfer Experiments Pt.5",
    "section": "",
    "text": "Introduction\nMany-Shot Learning\nDifficult to Learn Styles\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-5/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-5/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.5",
    "section": "Introduction",
    "text": "Introduction\nI’m starting to see a trend where a seemingly difficult problem is easily resolved and something I didn’t think was going to be a problem turns out to be difficult."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-5/index.html#many-shot-learning",
    "href": "posts/in-game-style-transfer-experiments/part-5/index.html#many-shot-learning",
    "title": "In-Game Style Transfer Experiments Pt.5",
    "section": "Many-Shot Learning",
    "text": "Many-Shot Learning\nI was able to get the video stylization model to correctly stylize in Unity by adding a lot more training examples. The creators of the model only needed a few training examples to get great results for a specific scene. I decided to try using around 80 examples to see if that would help it generalize. It did.\n\n\n\n\n\nThis model did produce a bit more flickering than the models trained on fewer examples. It should be easy to reduce the flickering by tuning the quality and quantity of the training examples. Output from this model can even be used as training examples for another model. Although, I actually prefer the color palette from the less accurate model."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-5/index.html#difficult-to-learn-styles",
    "href": "posts/in-game-style-transfer-experiments/part-5/index.html#difficult-to-learn-styles",
    "title": "In-Game Style Transfer Experiments Pt.5",
    "section": "Difficult to Learn Styles",
    "text": "Difficult to Learn Styles\nHaving resolved the discrepancy between the training results and Unity results, I started training models on different styles. I wanted to try styles that were very different from the mosaic image. I started with this sample output from the video stylization model.\n\n\n\n\n\nThe fast_neural_style model wasn’t able to do much more than transfer the color palette from this image. It failed to transfer texture or brush strokes. I tried a range of values for the training parameters but that didn’t really help. I then tried modifying the model architecture to see if I could get it to capture more subtle details. None of these changes produced acceptable results with the above image. This was unexpected since the model does a decent job learning the style of physical paintings.\n\nStarry Night by Vincent van Gogh (link)\n\n\n\n\n\nIt can also learn styles from some pieces of digital art.\n\n\nTotem by Justin Maller (link)\n\n\n\n\n\nThe model might have difficulty capturing the style from the lynx image because of how the style is extracted during the training process. The style of an image is extracted using a model the was pretrained on a bunch of regular images. That model is used to determine whether the style transfer model is doing a good job. It’s possible that the pretrained model being used isn’t recognizing the style features I want. If that’s the case, using a different pretrained model might provide better results."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-5/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-5/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.5",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s annoying that the fast_neural_style model can’t seem to capture certain types of styles. Fortunately, I can use any style transfer model I want to generate the training examples for the video stylization model. I’d prefer to have a model that can produce good results with a wide variety of styles. However, it might be necessary to use multiple models that handle different styles. I want to develop a reliable workflow before I start writing an end-to-end tutorial."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-6/index.html",
    "href": "posts/in-game-style-transfer-experiments/part-6/index.html",
    "title": "In-Game Style Transfer Experiments Pt.6",
    "section": "",
    "text": "Introduction\nArbitrary Style Transfer\nChange in Plans\nConclusion"
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-6/index.html#introduction",
    "href": "posts/in-game-style-transfer-experiments/part-6/index.html#introduction",
    "title": "In-Game Style Transfer Experiments Pt.6",
    "section": "Introduction",
    "text": "Introduction\nEvery time I think I’m ready to wrap up my style transfer project, I come across something that causes me to spend another week experimenting and troubleshooting. I’ve spent way more time on this topic than I intended and the the list future projects keeps growing. Therefore, I’ve made the decision to stop here and move on. Unfortunately, that does mean making some changes to the end-to-end style transfer tutorial."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-6/index.html#arbitrary-style-transfer",
    "href": "posts/in-game-style-transfer-experiments/part-6/index.html#arbitrary-style-transfer",
    "title": "In-Game Style Transfer Experiments Pt.6",
    "section": "Arbitrary Style Transfer",
    "text": "Arbitrary Style Transfer\nAs I mentioned in the weekly recap, I came across an implementation of the same style transfer model that Unity used for its style transfer project. However, this one hadn’t been watered down to run on a PS4 Pro. I was impressed by the results, but it takes days instead of hours to train. Here are a couple examples using style images from Unity’s sample project.\n\n\n\n\n\nThis method uses two deep learning models. One is based the same fast_neural_style model that I used in my first style transfer tutorial. It takes in an image and learns to generate a stylized output based on a target style image. The second model learns to predict values for part of the stylizing model that would enable the stylizing model to produce a stylized image based on a target style. The stylizing model would normally learn these values for a specific style image during training.\nThe models are trained on a large dataset of style images and content images to help them generalize. After training, the predictor model takes in an arbitrary style image and outputs the required values for the stylizing model. The stylizing model takes in those values along with a content image and outputs a stylized version.\nThis method also has an added benefit of allowing the user to adjust the influence of the style image after training. The model in Unity’s sample project seems to have the style influence set pretty low. I believe this is to minimize the flickering that is present when using the regular fast_neural_style model. As I’ve discovered, it’s difficult to leverage the existing methods to maintain consistency between frames and still get playable framerates."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-6/index.html#change-in-plans",
    "href": "posts/in-game-style-transfer-experiments/part-6/index.html#change-in-plans",
    "title": "In-Game Style Transfer Experiments Pt.6",
    "section": "Change in Plans",
    "text": "Change in Plans\nAfter completing the Google Colab notebook for the end-to-end style transfer tutorial, I began testing the code on a variety of styles as well as sample data from Unity. The goal was to identify areas where the models struggled. The good news is I successfully found areas where the models struggled. The bad news is that I probably won’t be able to find a solution for those weaknesses in a reasonable amount of time.\nThe video stylization model that I’ve been working with does a great job maintaining consistency between frames as long as the scene doesn’t change too drastically. However, camera input from 3D video games tend to change a lot between frames. I’ve found that the video stylization model doesn’t maintain frame consistency when moving around in a 3D environment.\nThe GitHub project does provide methods for improving temporal consistency. However, I don’t know if these methods would be practical for stylizing a video game. It’s also unclear if the model would still be able to generalize to different scenes with these methods.\nSince I’m not allowing myself to spend any more time experimenting I’ve decided not to use the video stylization model for the tutorial. Instead, I’ll be using a modified version of the fast_neural_style model. There will be more flickering than I’d like, but the training process will be much simpler."
  },
  {
    "objectID": "posts/in-game-style-transfer-experiments/part-6/index.html#conclusion",
    "href": "posts/in-game-style-transfer-experiments/part-6/index.html#conclusion",
    "title": "In-Game Style Transfer Experiments Pt.6",
    "section": "Conclusion",
    "text": "Conclusion\nI’m disappointed that I wasn’t able to resolve the flickering while maintaining playable frame rates. Flickering can be reduced by using a lower style influence. However, that feels like it defeats the whole point of style transfer. I’ll probably come back to style transfer at some point. For now, it’s time to move on to other projects."
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html",
    "href": "posts/machine-learning-level-generation-notes/index.html",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "",
    "text": "Overview\nExisting Examples\nMainstream Commercial Games\nSome Key Researchers\nInfluential Demos\nMajor Techniques for Level Generation\nHighlights\nImplementation\nRelated Work"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#overview",
    "href": "posts/machine-learning-level-generation-notes/index.html#overview",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Overview",
    "text": "Overview\nI recently started exploring potential applications of machine learning in procedural generations tool and came across this talk given by Ben Berman from the 2017 Roguelike Celebration. Below are some notes I took while watching.\nMachine Learning: Teaching computers using data\n\nPerformance is constrained by data and out ability to communicate what is important to the computer.\n\n\n\n\nMachine_Learning_and_Level_Generation_Diagram_1.png"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#existing-examples",
    "href": "posts/machine-learning-level-generation-notes/index.html#existing-examples",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Existing Examples",
    "text": "Existing Examples\n\nCaves of Qud\n\nUses a Monte Carlo Markov Chain method (MCMC) with “local similarity” (and other) constraints to generate game levels based on a mix of examples and heuristics.\n\n\n\nBotnik\n\nUse a mix of natural language processing tools, “character RNNs” trained on example text, human-specified constraints and human intervention to create mock television scripts, Wikipedia articles, postcards and memes.\n\n\n\nComputational Flaneur\n\nUses a “character RNN” to generate new poetry based on example poetry based on example poetry in response to your environment.\n\nMax Kreminski’s past bots and microsites use NLP, related MCMC methods and a huge number of heuristics to make comedy and poetry.\nDarius Kazemi’s various bots have hijacked natural language processing software like ConceptNet to generate comic tweets and memes.\nIan Holmes’s games and microsites use cellular automata and heuristics and to generate game levels, visuals and text."
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#mainstream-commercial-games",
    "href": "posts/machine-learning-level-generation-notes/index.html#mainstream-commercial-games",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Mainstream Commercial Games",
    "text": "Mainstream Commercial Games\n\nUnsupervised learning techniques used to analyze the client’s UX for League of Legends\nSupercell’s Clash Royale matchmaking widely believed to use, in part, a generalized linear model that considers various player features besides skill level. Not known if the parameters are learned.\nBlizzard institutionalized “pity algorithms” in many loot box mechanics with learned parameters.\nMany free-to-play studios deploy machine learning for ad tech based on a Bayesian customer lifetime value model first outlined in 1987.\nMany virtual slot machines and in-app purchases are optimized using MCMC models.\n\nTakeaway: The research is way ahead of commercial games"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#some-key-researchers",
    "href": "posts/machine-learning-level-generation-notes/index.html#some-key-researchers",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Some Key Researchers",
    "text": "Some Key Researchers\n\nMichael Mateas: Applies many methods, especially recurrent neural networks, with publications on level generation for platformers.\nJulian Togelius: Also applies many methods, especially RNNs, to content generation.\nDan Ritchie (Ex-Pixar Technical Director): Monte Carlo methods in graphics, comprehensive thesis on probabilistic programs and accessible read."
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#influential-demos",
    "href": "posts/machine-learning-level-generation-notes/index.html#influential-demos",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Influential Demos",
    "text": "Influential Demos\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks: Andrej Karpathy’s blog post demonstrating RNNs as content generators. Enormously influential on level generators (and lots more).\nWaveFunctionCollapse: A texture synthesis technique immediately used to generate game levels.\nPhase-Functioned Neural Networks for Character Control: Realistic skinned character animation for arbitrary height map levels.\nDeepDream (or PixelCNN): Neural network-based image and texture synthesis techniques."
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#major-techniques-for-level-generation",
    "href": "posts/machine-learning-level-generation-notes/index.html#major-techniques-for-level-generation",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Major Techniques for Level Generation",
    "text": "Major Techniques for Level Generation\n\nTexture synthesis: Treat the level as a 2D image. Based on original, good levels you designed by hand, learn patterns on a micro and macro scale, then smartly mix these patterns to make them hard to distinguish from the originals.\nRecurrent neural networks: Treat the level as something that you explore over time. Later parts of the level depend on earlier parts, and some fuzzy picture of the whole.\nMonte Carlo Markov Chain design, specifically Hamiltonian Monte Carlo inference: Choose some measurements (design constraints) that are important to you, and learn the values of those measurements from good levels you designed by hand. Then randomly generate levels, tweaking parameters until you get similar measurements"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#highlights",
    "href": "posts/machine-learning-level-generation-notes/index.html#highlights",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Highlights",
    "text": "Highlights\n\nWavefunctionCollapse\n\nEPC2018 - Oskar Stalberg - Wave Function Collapse in Bad North\nEPC2018 - Oskar Stalberg - Wave Function Collapse in Bad North\nStarted with a layout of what a city tile-scape should look like.\nCan be edited by selecting blocks to remove and the layout will automatically update with a suitable alternative.\nHas constraints like certain tiles can only be adjacent to certain tiles.\nLooks at other examples of what city tile-scapes should look like.\n\nAdam Summerville’s Mario level generator\n\nSuper Mario as a String: Platformer Level Generation Via LSTMs\nSuper Mario as a String: Platformer Level Generation Via LSTMs\nBuilding Mario Levels with Machine Learning\nText representation of a level and the understanding that future parts of the level are somehow related to past parts.\nDid not know any of the game rules\n\nDaniel Holden et. al.’s neural network animated character (2017)\n\nSpecify a target trajectory with game controller and the model synthesizes a series of motions that follows that path.\n\nPhase Extraction: Augments the learning process with the timing information of a given motion.\n\nHelps the model learn both periodic motions and when these motions start and end.\n\nTrained model takes very little memory and runs in real-time\nAccomplishes smooth walking, running, jumping, climbing motions and more.\n\nGDC 2018 Character Control with Neural Networks and Machine Learning\n\nGDC Talk\nCharacter Control with Neural Networks and Machine Learning\nPresentation slides\nSlides (PDF)\n\nTwo Minute Papers (2017)\nReal-Time Character Control With Phase-Functioned Neural Networks\n\nDaniel Ritchie’s spaceship generator\n\nUser provides a blocked out shape for spaceship and it generates finer detail versions\nPhD Thesis: Probabilistic Programming for Procedural Modeling and Design\n\nPDF\n\n\nMaking it home, MCMC-based room layout\n\nPlace objects in room and use interior design logic to organize the objects in an appealing way\nTile-based"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#implementation",
    "href": "posts/machine-learning-level-generation-notes/index.html#implementation",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Implementation",
    "text": "Implementation\n\nTips and trick to get working ML-based level generators fast\nObjective:\n\nMake something that looks like it was made by hand, not by a computer\nHow does this contrast with heuristic approaches to level generation?\nThe Heuristic Approach\n\nExtremely time-consuming\n\nExample: Cloudbery Kingdom\n\nTook three years and thousands of man-hours\nLevels still look like they are made by a computer\n\n\n\n\nFocus on what makes things look like they were made by people\nComputer-made Feel: random placement\nHand-Made Feel: Symmetry in Architecture\n\nSymmetry is really simple\n\n\\[\n$-f(x)=f(-x)$\n\\]\n\n\nComputer-made Feel: Overly Precise Numbers\nHand-Made Feel: Small Numbers\n\nModeling: A roguelike level is a 1D platformer in disguise\n\nGenerating linear paths is easier than multidimensional geometry and increases the number of machine learning techniques you can use\n2D levels can be expressed as multiple 1D levels\n\nData: Rip level data from existing Unity games using DevXUnity Pro\n\nColossal amount of content unlocked from the Google Play store\nAlways Google for mods for a game with rich data\nData: Successfully Extracted\n\nRolling Sky (Unity)\nWayward Souls (Apportable)\nHotline Miami, Hotline Miami 2 (GameMaker)\nBotanicula (Adobe AIR)\nGeometry Dash (Cocos 2D)\n\nRNN/CNN approaches don’t need higher level data, seems to work on raw Geometry Dash levels anyway\n\nHexadecimal data\n\n\nUseful Libraries: Try to find an implementation of your approach of choice in TensorFlow/PyTorch (more so PyTorch these days)\n\nMost research is done using PyTorch now\n\nImplementation: LSTM models\n\nBest supported, highest research interest (in 2017)\nTraining models to produce reliably high-quality results in real-time applications is challenging and time consuming\nCharacter-based RNN (LSTM) models have been really successful\nLSTMs take a big chunks of level and try to predict the “next tile”\nAs you show an LSTM chunks, it “remembers” something about which tiles were adjacent to eachother.\nLoss function: A score of how well the system is learning to predict the “next tile” given it memory and the previous tiles.\nThe objective is to minimize loss, because loss is the inverse of the score\nLSTM models are really buggy online (2017)\n\nAccidentally sharing cell states: Many public implementations of “character RNNs” use the same instance of a “cell” object in every layer. This happened because someone started with broken code and adapted it.\nSolution: Don’t use anything more than 6 months old if you are not sure how to read the model.\nFixes convergence issues since this is a colossal bug.\n\n“Chunks”: Nearly every public implementation divides your source level data into chunks whose borders may or may not be the actual start and end points of a level\nBad Encoding: Every Character RNN tool just treasts each character as a class. For levels, this can result in a lot of noise.\nSolution: Semantically-Balanced Hierarchical Encoding. Categorize each tile into a binary string of “traits” (e.g. piece of text), where each trait has real semantic meaning.\n\nTrain an RNN on each binary “view” of your tiles (like a black and white image), then train feed forward networks on for important traits. \\(x_1 = f(x_i \\ \\epsilon \\ traits : i \\neq 1)\\)\nSome experiments saw a 20x improvement in loss Training was immensely faster.\nDefinitely sensitive to your choice of traits.\nNote: models do not work with rarity very well\n\nReward tiles are so rare that you’d need a huge corpus just to learn where they appear\n\nTend to implement convolutions to reinterpret the data that helps the model learn\n\nNoise: Character RNNs produce incredibly noisy results\nSolution: Denoise your level as though it was a texture.\n\nNon-local SSIM (Structural Similarity Index) denoiser works really well for some level textures\nSSIM increases (higher is better) as noisy chunks move towards border, while MSE (Mean Squared Error) stays the same\n\nOr: Include a noise measurement as a part of your loss function, treating some symmetrical part of the level as your “clean” example.\nIdea: Use the mirrored half of the level for your comparison. Denoising and symmetry in one go.\nResults are still preliminary (2017)\nPerformance is Poor: Seems to take a long time to train models. (2017)"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#functional-game-content-like-spells-and-abilities-is-the-holy-grail",
    "href": "posts/machine-learning-level-generation-notes/index.html#functional-game-content-like-spells-and-abilities-is-the-holy-grail",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Functional game content, like spells and abilities, is the holy grail",
    "text": "Functional game content, like spells and abilities, is the holy grail\n\nSpellsource:\n\nA free, open-source fully-hosted multiplayer card game engine for experimentation\nA feature-complete implementation of the Hearthstone ruleset with all 1,312 cards"
  },
  {
    "objectID": "posts/machine-learning-level-generation-notes/index.html#related-work",
    "href": "posts/machine-learning-level-generation-notes/index.html#related-work",
    "title": "Notes on Machine Learning and Level Generation",
    "section": "Related Work",
    "text": "Related Work\n\nAI4Animation: Deep Learning for Character Control\n\nhttps://github.com/sebastianstarke/AI4Animation\nThese AI-Driven Characters Dribble Like Mad! 🏀\n\nGDC 2018 Deep Learning: Beyond the Hype (by SEED/EA)\n\nPresentation\nDeep Learning: Beyond the Hype\n\nLearning Generative Models of 3D Structures (2020)\n\nPaper (PDF)\nCSC2547 Learning Generative Models of 3D Structures\n\n\n​\nReferences:\n\nBen Berman - Machine Learning and Level Generation"
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html",
    "href": "posts/making-money-as-freelance-developer-notes/index.html",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "",
    "text": "Overview\nCharacteristics of a Freelance Developer\nStanding Out From Other Freelance Developers\nHow to Sell Your Services\nWhere to Sell Your Services"
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html#overview",
    "href": "posts/making-money-as-freelance-developer-notes/index.html#overview",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "Overview",
    "text": "Overview\nI decided to watch this video published by freeCodeCamp on business tips for freelance developers. Below are some notes I took while watching."
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html#characteristics-of-a-freelance-developer",
    "href": "posts/making-money-as-freelance-developer-notes/index.html#characteristics-of-a-freelance-developer",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "Characteristics of a Freelance Developer",
    "text": "Characteristics of a Freelance Developer\n\nShould be self-motivated\nShould not need structure provided for you\nMust be able to self-impose deadlines\nMust be able to handle distractions"
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html#standing-out-from-other-freelance-developers",
    "href": "posts/making-money-as-freelance-developer-notes/index.html#standing-out-from-other-freelance-developers",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "Standing Out From Other Freelance Developers",
    "text": "Standing Out From Other Freelance Developers\n\nYou are not a developer, you are a problem solver\nPhrase your capabilities in the context of the problem a potential client needs to be solved\nDo not use technical jargon\nBe an active listener\nDo not talk about your specific technical skills\nTalk about how you are able to address the client’s needs"
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html#how-to-sell-your-services",
    "href": "posts/making-money-as-freelance-developer-notes/index.html#how-to-sell-your-services",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "How to Sell Your Services",
    "text": "How to Sell Your Services\n\nInitial Meeting\n\nMake sure you are in an appropriate venue\n\nExample: Don’t do sales calls while driving\nMake sure you are in a location that you can take business calls during business hours\n\nIdentify the potential client’s problems\nTake time to understand the client’s business\n\nWhat do they do?\nHow do they normally operate?\nWhat are their typical customers like?\nWhat are their goals?\nWhat are some unique characteristics?\n\n\n\n\nProposal\n\nIncorporate the knowledge gained from the initial meeting into a proposal for their unique needs\nSpell out specifically what you are going to do in a contract\n\n\n\nClose\n\nDescribe the features in your proposal and how they address the client’s needs\nDescribe the problems you identified\nDescribe what you are going to do about those problems\nDescribe how what you are going to do will address those problems"
  },
  {
    "objectID": "posts/making-money-as-freelance-developer-notes/index.html#where-to-sell-your-services",
    "href": "posts/making-money-as-freelance-developer-notes/index.html#where-to-sell-your-services",
    "title": "Notes on Making Money as a Freelance Developer",
    "section": "Where to Sell Your Services",
    "text": "Where to Sell Your Services\n\nNetworking Events\n\nGo to networking events where the attendees are not other developers\n\nExample: Networking events with small business owners from wide range of fields\nIt is easier to get clients when you are the only developer in the room\nBusiness Network International\n\nDo not rely on it as a single source of business.\n\n\n\n\n\nThird Party Services\nPros\n\nLots of traffic\n\nCons\n\nLower profits\nHave to pay to get the lead\n\n\n\nPersonal Website\n\nBuild website in a way that tells them you will listen to and address their specific needs\nCan promote through pay per click ads\n\nCan get expensive\nAvoid ad-word management services\n\n\nReferences:\n\nfreeCodeCamp Video: How to make money as a freelance developer - business tips from an expert"
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "",
    "text": "Introduction\nAn Overview of Machine Learning, PyTorch, and Jupyter\nInstalling Mamba\nAccess the Miniforge Prompt\nBenefits of Using Virtual Environments\nCreating a Python Environment\nPackage Overview\nInstalling PyTorch and Jupyter\nTesting PyTorch and Jupyter Notebook Setup\nManaging and Updating Packages with Mamba\nConclusion"
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#introduction",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#introduction",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Introduction",
    "text": "Introduction\nIn this beginner-friendly tutorial, I’ll guide you through installing the Mamba package manager on your local Windows machine and using it to create Python environments. Mamba is a powerful tool that helps you create and manage virtual environments, allowing you to maintain separate configurations for different projects without conflicts. It provides a drop-in replacement for conda and offers higher speed and more reliable environment solutions.\nIn this tutorial, we’ll use a practical example to demonstrate the effectiveness of Mamba. We’ll install PyTorch and Jupyter, two essential tools for machine learning projects. To ensure we set up everything correctly, we’ll verify the PyTorch installation by running a simple test in a Jupyter Notebook. By the end of this tutorial, you’ll be well-equipped to begin your machine-learning projects using Mamba, PyTorch, and Jupyter."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#an-overview-of-machine-learning-pytorch-and-jupyter",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#an-overview-of-machine-learning-pytorch-and-jupyter",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "An Overview of Machine Learning, PyTorch, and Jupyter",
    "text": "An Overview of Machine Learning, PyTorch, and Jupyter\nMachine learning is a subset of artificial intelligence that enables computers to learn from data and make decisions or predictions without being explicitly programmed. It has become an essential tool across various domains, including natural language processing, computer vision, and speech recognition.\nPyTorch is a popular open-source machine learning framework that enables users to perform tensor computations, build dynamic computational graphs, and implement custom machine learning architectures. PyTorch has gained widespread adoption due to its flexibility, ease of use, and strong community support.\nJupyter is an open-source project that provides an interactive computing environment where you can create and share documents containing live code, equations, visualizations, and narrative text. Jupyter Notebook is a web-based platform that allows users to write, run, and debug code. You can also create visualizations and explanatory text.\nThis tutorial will help kickstart your machine-learning projects by guiding you through setting up a local Python environment using Mamba, PyTorch, and Jupyter. If you’re new to machine learning and want to learn more, I recommend the Practical Deep Learning for Coders course. The course uses a hands-on approach to teach students to apply deep learning and machine learning to practical problems using PyTorch and the fastai library."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#installing-mamba",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#installing-mamba",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Installing Mamba",
    "text": "Installing Mamba\n\nDownload the Mambaforge installer for Windows from the official Miniforge GitHub repository.\n\nRun the installer executable.\n\nYou might get a popup from Microsoft Defender SmartScreen like the one below, saying it prevented an unrecognized app from starting. If so, click More info.\n\nThen click the “Run anyway” button.\n\n\nFollow the on-screen instructions in the setup window.\n\nClick the Next &gt; button in the setup window.\n\nClick the I Agree button to accept the license agreement.\n\nSelect Just Me for the installation type and click the Next &gt; button.\n\nStick with the default install location and click the Next &gt; button.\n\nCheck the box to clear the package cache upon completion under installation options and click the Install button to start the installation process.\n\nWait for the installation process to complete, then click the Next &gt; button.\n\nFinally, click the Finish button to close the setup window."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#access-the-miniforge-prompt",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#access-the-miniforge-prompt",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Access the Miniforge Prompt",
    "text": "Access the Miniforge Prompt\nOn Windows, we use Mamba through the Miniforge Prompt. It is a dedicated command-line interface for interacting with conda environments.\nType “Miniforge Prompt” into the Windows search bar and click Open.\n\nA new command prompt will open at the location of your Windows user folder.\n\nAlternatively, you can activate the Miniforge Prompt from any regular command prompt with the following command:\n%USERPROFILE%\\mambaforge\\Scripts\\activate\n\nThe text in front of the current directory path indicates the name of the current conda environment.\n\nWe can get a list of the available conda environments and their locations using the following command:\nmamba env list\n\nThe only one listed is “base.” If we go to the associated directory path in File Explorer, we’ll see the contents for the Mambaforge installation. Mambaforge will store any conda environments we create in the “envs” folder."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#benefits-of-using-virtual-environments",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#benefits-of-using-virtual-environments",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Benefits of Using Virtual Environments",
    "text": "Benefits of Using Virtual Environments\nBefore creating a new Python environment, let’s cover the benefits of using virtual environments and why they are essential when working with Python projects.\n\nIsolation of Dependencies: Virtual environments create isolated spaces for each project, allowing developers to install and manage different package versions without conflicts. This isolation ensures that the specific dependencies required for one project do not interfere with or break the dependencies of another project.\nEasier Project Management: Virtual environments help streamline project management by maintaining separate configurations for different projects. Developers can replicate or share project environments with team members, ensuring consistent behavior across various machines. Shareable project environments help developers collaborate, troubleshoot, and deploy projects.\nSimplified System Maintenance: Using virtual environments helps developers avoid cluttering their system-wide Python installation with numerous packages and varying versions. System maintenance is cleaner as developers can easily add, update, or remove packages within individual project environments without affecting other projects or the system as a whole."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#creating-a-python-environment",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#creating-a-python-environment",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Creating a Python Environment",
    "text": "Creating a Python Environment\nWe can create new Python environments using the mamba create command. The following command will create one called pytorch-env with Python 3.10:\nmamba create --name \"pytorch-env\" python=3.10\n\nMamba will ask you to confirm that you want to install the packages required to create the new conda environment. Type Y into the “Confirm changes” prompt.\n\nAlternatively, you can pre-approve the package installations by adding a -y to the end of the mamba create command.\n   mamba create --name \"pytorch-env\" python=3.10 -y\nLooking in the “envs” directory, we see a new folder for the conda environment we created.\n\nOnce Mamba finishes creating the new environment, it will tell us we can activate and deactivate it using the following commands:\nActivate:\nmamba activate pytorch-env\nDeactivate:\nmamba deactivate\n\nWhen we activate it, the name in front of the current directory will change accordingly."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#package-overview",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#package-overview",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Package Overview",
    "text": "Package Overview\nBefore we install PyTorch and Jupyter, let’s briefly explore the purpose of each package and why they’re essential for machine learning projects.\n\nPyTorch: PyTorch is a fast, flexible, user-friendly, open-source machine learning framework. PyTorch allows developers to perform tensor computations, create dynamic computational graphs, and implement custom machine-learning architectures.\ntorchvision: torchvision provides benchmark datasets, model architectures, and image transformations for computer vision.\ntorchaudio: This package provides various tools and datasets for audio processing, including input/output functions, data loaders for popular audio datasets, and audio-specific transformations.\nJupyter: Jupyter is an open-source project that enables users to create and share documents containing live code, equations, visualizations, and narrative text. Jupyter Notebook is a web-based interactive computing environment where you can write, run, and debug code. You can also include visualizations and explanatory text."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#installing-pytorch-and-jupyter",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#installing-pytorch-and-jupyter",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Installing PyTorch and Jupyter",
    "text": "Installing PyTorch and Jupyter\nWe can install packages in our custom Python environment using mamba or the pip package installer. To use mamba, we replace the word conda in any conda install commands.\nFor example, this is the conda command to install PyTorch on computers with Nvidia GPUs.\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\nFor users without an Nvidia GPU:\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\nHere is the modified command, which uses Mamba.\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\nAs before, type Y into the “Confirm changes” prompt.\n\nAlternatively, you can pre-approve the package installations by adding a -y to the end of the mamba install command.\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\nInstallation time can vary depending on your system’s hardware and Internet speed. Once it finishes, we’ll install Jupyter using the following command:\nmamba install jupyter -y"
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#testing-pytorch-and-jupyter-notebook-setup",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#testing-pytorch-and-jupyter-notebook-setup",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Testing PyTorch and Jupyter Notebook Setup",
    "text": "Testing PyTorch and Jupyter Notebook Setup\nNow that we have PyTorch and Jupyter installed in our Python environment, we can verify everything works as expected by importing PyTorch into a Jupyter Notebook. Before that, I recommend changing the current directory to something more appropriate. I switched to a new folder outside my G drive using the following commands:\nG:\ncd Projects\\Current_Projects\\mamba-tutorial\n\nWe can launch a Jupyter Notebook environment using the following command:\njupyter notebook\n\nThe Jupyter Notebook environment will launch in a new tab in your default web browser.\n\nTo create a new Jupyter Notebook, open the New drop-down menu and select the Python 3 (ipykernel)option.\n\nThe notebook will open in a new tab.\n\nWe can import PyTorch and verify it can use the Nvidia GPU by running the following code in the notebook:\nimport torch\ntorch.cuda.is_available()"
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#managing-and-updating-packages-with-mamba",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#managing-and-updating-packages-with-mamba",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Managing and Updating Packages with Mamba",
    "text": "Managing and Updating Packages with Mamba\nThis section covers the commands to manage and update packages in your Python environment using Mamba. Proper package management is crucial for maintaining project stability and ensuring compatibility between dependencies.\n\nListing Installed Packages\nTo view all the installed packages within your active Python environment, use the following command:\nmamba list\nThis command will display a list of installed packages and their respective versions and channels.\n\n\nUpdating a Package\nUpdating a package to its latest version is simple with Mamba. Use the following command, replacing package-name with the name of the package you want to update:\nmamba update package-name\nMamba will search for the latest version of the package, resolve any dependencies, and prompt you to confirm the update.\n\n\nUpdating All Packages\nTo update all packages within the active Python environment to their latest versions, run the following command:\nmamba update --all\nMamba will check for updates for all installed packages and prompt you to confirm the changes.\n\n\nInstalling a Specific Package Version\nTo install a specific package version, use the “=” sign to append the desired number to the package name. For example, to install version 1.0.0 of a package named example-package, use the following command:\nmamba install example-package=1.0.0\n\n\nRemoving a Package\nTo remove a package from the active Python environment, use the remove command, replacing package-name with the name of the package you want to remove:\nmamba remove package-name\nMamba will prompt you to confirm the removal of the package and its dependencies.\n\n\nSearching for Packages\nYou can search for packages across various channels using the search command. To search for a package named example-package, run:\nmamba search example-package\nThis command will display a list of available versions and channels for the specified package.\nWith these commands, you can effectively manage and update packages in your Python environment using Mamba, ensuring your projects stay up-to-date and compatible with the latest dependencies."
  },
  {
    "objectID": "posts/mamba-getting-started-tutorial-windows/index.html#conclusion",
    "href": "posts/mamba-getting-started-tutorial-windows/index.html#conclusion",
    "title": "Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we successfully installed the Mamba package manager on a Windows machine and used it to create a local Python environment for our machine-learning project. We also installed tools like PyTorch and Jupyter to help us build machine-learning models.\nBy leveraging the power of virtual environments, we can maintain separate configurations for different projects, avoiding dependency conflicts and streamlining project management."
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "",
    "text": "Introduction\nEnvironment Setup\nDataset\nMethodology\nResults\nDiscussion\nConclusion"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#introduction",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#introduction",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Introduction",
    "text": "Introduction\nComputer vision models can learn to recognize complex patterns and objects in images. However, they often struggle to generalize to new input. One solution is data augmentation, a technique that expands the size and diversity of a dataset by creating variations of existing images. The model never sees the exact image twice, helping it learn general features versus memorizing specific examples.\nIn this post, I will explore how different augmentations can impact model performance using a hand gesture dataset. Hand gesture recognition has potential applications in human-computer interaction, robotics, and sign language interpretation. Hand gesture recognition is challenging due to the variability of hand poses, shapes, and lighting conditions, making it a good candidate for image augmentation. The results of these experiments should provide insights into the best practices for training deep learning models to handle diverse input.\nYou can find links to view the training code and run it on Google Colab and Kaggle below.\n\n\n\nJupyter Notebook\nColab\n        Kaggle        \n\n\n\n\nGitHub Repository\nOpen In Colab\nKaggle"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#environment-setup",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#environment-setup",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Environment Setup",
    "text": "Environment Setup\nThe experiment code depends on the miniai library developed for the Practical Deep Learning 2022 Part 2 course. First, we’ll check if we have the miniai package installed. If not, we clone the course repository from GitHub and install the miniai package.\nInstall dependencies\n# %%capture\n# try:\n#     import miniai\n# except ImportError:\n#     !git clone https://github.com/fastai/course22p2.git\n#     !pip install -e ./course22p2\nNext, we need to install several pip packages.\n# %%capture\n# !pip install torch torchvision torchaudio torcheval\n# !pip install numpy pandas pillow wandb\n# !pip install cjm_pil_utils cjm_kaggle_utils cjm_pytorch_utils cjm_pandas_utils\nAfter installing the required dependencies, we can import them into our Jupyter notebook.\nImport dependencies\nfrom pathlib import Path\nimport hashlib\nimport json\nimport random\nimport multiprocessing\nimport math\nimport os\nfrom functools import partial\nfrom copy import copy\nfrom glob import glob\n\nfrom PIL import Image\nimport numpy as np\nimport timm\nfrom tqdm.auto import tqdm\n\n# Import pandas module for data manipulation\nimport pandas as pd\n\n# Set options for Pandas DataFrame display\npd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\npd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\npd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n\n# Import PyTorch dependencies\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import Dataset, DataLoader\nfrom torcheval.tools import get_module_summary\nfrom torcheval.metrics import MulticlassAccuracy\n\n# Import miniai dependencies\nimport fastcore.all as fc\nfrom miniai.datasets import DataLoaders\nfrom miniai.conv import def_device\nfrom miniai.learner import *\nfrom miniai.activations import Hook, ActivationStats\nfrom miniai.accel import MixedPrecision, AccelerateCB\nfrom miniai.init import BatchTransformCB, GeneralRelu, init_weights, set_seed, conv\nfrom miniai.sgd import BatchSchedCB\nfrom miniai.resnet import ResBlock\n\n# Import utility functions\nfrom cjm_kaggle_utils.core import save_kaggle_creds, dl_kaggle\nfrom cjm_pil_utils.core import resize_img, get_img_files, stack_imgs\nfrom cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, tensor_stats_df, get_torch_device, denorm_img_tensor\nfrom cjm_pandas_utils.core import markdown_to_pandas"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#dataset",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#dataset",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Dataset",
    "text": "Dataset\nThe dataset contains images from HaGRID (HAnd Gesture Recognition Image Dataset) modified for image classification. It has 125,912 samples for 18 distinct hand gestures and 27,823 images containing no gestures. The dataset includes a wide range of people, environmental settings, illumination, and camera placement. You can access the dataset on Kaggle at the link below.\n\nHaGRID Classification 512p no_gesture 150k\n\n\n\nReference Images\n\n\n\n\n\n\n\n\nClass\n\n\nImage\n\n\n\n\n\n\ncall\n\n\n\n\n\n\n\ndislike\n\n\n\n\n\n\n\nfist\n\n\n\n\n\n\n\nfour\n\n\n\n\n\n\n\nlike\n\n\n\n\n\n\n\nmute\n\n\n\n\n\n\n\nok\n\n\n\n\n\n\n\none\n\n\n\n\n\n\n\npalm\n\n\n\n\n\n\n\npeace\n\n\n\n\n\n\n\npeace_inverted\n\n\n\n\n\n\n\nrock\n\n\n\n\n\n\n\nstop\n\n\n\n\n\n\n\nstop_inverted\n\n\n\n\n\n\n\nthree\n\n\n\n\n\n\n\nthree2\n\n\n\n\n\n\n\ntwo_up\n\n\n\n\n\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we set our Kaggle credentials by providing our Kaggle username and API token. We save the credentials to a file, which allows us to authenticate and download the dataset from Kaggle.\nSet Kaggle username and API token\nusername = \"\"\nkey = \"\"\nSave Kaggle credentials to file\nsave_kaggle_creds(username, key, overwrite=False)\nEmpty username.\nNext, we define the directories to store the archive files and datasets and create the directories if they don’t already exist.\nSet directory paths\n# Define path to store datasets\ndataset_dir = Path(\"/mnt/980_1TB_2/Datasets/\")\n# Create the dataset directory if it does not exist\ndataset_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Dataset Directory: {dataset_dir}\")\n\n# Define path to store archive files\narchive_dir = dataset_dir/'../Archive'\n# Create the archive directory if it does not exist\narchive_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Archive Directory: {archive_dir}\")\nDataset Directory: /mnt/980_1TB_2/Datasets\nArchive Directory: /mnt/980_1TB_2/Datasets/../Archive\nWe use the Kaggle dataset name to construct the save paths for the archive file and the extracted dataset.\nSet Kaggle dataset\n# Set the name of the dataset\ndataset_name = 'hagrid-classification-512p-no-gesture-150k'\n\n# Construct the Kaggle dataset name by combining the username and dataset name\nkaggle_dataset = f'innominate817/{dataset_name}'\n\n# Create the path to the zip file that contains the dataset\narchive_path = Path(f'{archive_dir}/{dataset_name}.zip')\nprint(f\"Archive Path: {archive_path}\")\n\n# Create the path to the directory where the dataset will be extracted\ndataset_path = Path(f'{dataset_dir}/{dataset_name}')\nprint(f\"Dataset Path: {dataset_path}\")\nArchive Path: /mnt/980_1TB_2/Datasets/../Archive/hagrid-classification-512p-no-gesture-150k.zip\nDataset Path: /mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k\nFinally, we download the Kaggle dataset and extract it to the specified directory.\nDownload Kaggle dataset\ndl_kaggle(kaggle_dataset, archive_path, dataset_path)\nDataset already downloaded\nThe dataset organizes the images for different gesture types into separate subfolders.\nGet image classes\n# Get all subfolders within the dataset_path\nfolders = [folder for folder in dataset_path.glob('*/') if folder.is_dir()]\n\n# Store the folder names\nclass_names = [f.name for f in folders]\n\n# Print the list of class names\npd.DataFrame(class_names)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\ncall\n\n\n\n\n1\n\n\ndislike\n\n\n\n\n2\n\n\nfist\n\n\n\n\n3\n\n\nfour\n\n\n\n\n4\n\n\nlike\n\n\n\n\n5\n\n\nmute\n\n\n\n\n6\n\n\nno_gesture\n\n\n\n\n7\n\n\nok\n\n\n\n\n8\n\n\none\n\n\n\n\n9\n\n\npalm\n\n\n\n\n10\n\n\npeace\n\n\n\n\n11\n\n\npeace_inverted\n\n\n\n\n12\n\n\nrock\n\n\n\n\n13\n\n\nstop\n\n\n\n\n14\n\n\nstop_inverted\n\n\n\n\n15\n\n\nthree\n\n\n\n\n16\n\n\nthree2\n\n\n\n\n17\n\n\ntwo_up\n\n\n\n\n18\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\nAfter obtaining the class names for the dataset, we need to get the paths for all the images in the dataset.\nGet image paths\n# Get a list of all image file paths from the given folders\nimg_paths = [get_img_files(folder) for folder in folders]\n\n# Flatten the list of lists into a single list\nimg_paths = [path for class_paths in img_paths for path in class_paths]\n\n# Get the number of image file paths\nlen(img_paths)\n153735"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#methodology",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#methodology",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Methodology",
    "text": "Methodology\nI tracked the experiment using Weights & Biases and initialized all training runs with the same random seed to ensure the consistency and reproducibility of the results. I then reran the experiment with a few different seed values. You can view the Weights & Biases projects at the links below.\n\nminiai_data_augmentation_seed-1\nminiai_data_augmentation_seed-42\nminiai_data_augmentation_seed-100\nminiai_data_augmentation_seed-1234\n\nYou can view the projects without an account, but you will need one to create new projects.\nLog in to Weights & Biases\nimport wandb\nwandb.login()\nTrue\nSet random number seed\n# seed = 1\n# seed = 42\n# seed = 100\nseed = 1234\nset_seed(seed)\nSet device and data type\ndevice = get_torch_device()\ndtype = torch.float32\ndevice, dtype\n('cuda', torch.float32)\n\nImage Augmentations\nFor this experiment, I tested five different image augmentations. We can visualize each one using a sample image from the dataset.\nDisplay sample image\n# Select a random image path from the list of image paths\nimg_path = random.choice(img_paths)\n\n# Print the name of the class of the image, which is the name of the parent folder\nprint(f\"Class: {img_path.parent.name}\")\n\n# Open the image using the path\nsample_img = Image.open(img_path)\n\n# Display the image\nsample_img\nClass: stop\n\n\n\n\n\nThe first augmentation copies a square section from a random location on the image and pastes it in a random spot. I use random patch sizes ranging from 10-20% of the image dimensions to reduce the chance of obscuring the hand gestures.\n\nRandom Square Copy Transform\ndef rand_square_copy(img_tensor:torch.Tensor, pct:float):\n    \"\"\"\n    Copy data from a randomly selected square region to another randomly selected square region of an image tensor.\n\n    Args:\n    img_tensor (torch.Tensor): The input image tensor.\n    pct (float): The percentage of the image tensor's size to be used as the side length of the square regions.\n\n    Returns:\n    torch.Tensor: The modified input image tensor.\n    \"\"\"\n    \n    # Get the shape of the tensor\n    h, w = img_tensor.shape[-2:]\n    \n    # Calculate the size of the square\n    szx = int(pct * h)\n    szy = int(pct * w)\n    \n    # Calculate the top-left coordinate of the first square\n    stx1 = int(random.random() * (1 - pct) * h)\n    sty1 = int(random.random() * (1 - pct) * w)\n    \n    # Calculate the top-left coordinate of the second square\n    stx2 = int(random.random() * (1 - pct) * h)\n    sty2 = int(random.random() * (1 - pct) * w)\n    \n    # Copy the data from the second square to the first square\n    img_tensor[..., stx1:stx1 + szx, sty1:sty1 + szy] = img_tensor[..., stx2:stx2 + szx, sty2:sty2 + szy]\n    \n    # Return the modified input img_tensor\n    return img_tensor\nclass RandSquareCopy(nn.Module):\n    def __init__(self, pct=0.2, min_num=0, max_num=4):\n        \"\"\"\n        A PyTorch module that applies the `rand_square_copy` function to an input tensor multiple times.\n\n        Args:\n        pct (float, optional): The percentage of the tensor's size to be used as the side length of \n                               the square regions.\n        min_num (int, optional): The minimum number of times to apply the `rand_square_copy` function.\n        max_num (int, optional): The maximum number of times to apply the `rand_square_copy` function.\n        \"\"\"\n        super().__init__()\n        self.pct = pct\n        self.min_num = min_num\n        self.max_num = max_num\n        \n    def forward(self, x): \n        # Apply the `rand_square_copy` function to the input tensor multiple times\n        for i in range(random.randint(self.min_num, self.max_num)):\n            x = rand_square_copy(x, max(0.1,random.random()*self.pct))\n        return x\nTest the transform\nrand_square_copy_tf = RandSquareCopy(pct=0.2, min_num=1, max_num=1)\n\n# Convert the PIL image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)\nprint(img_tensor.shape)\n\n# Apply the `rand_square_copy_tf` transform to the image tensor\ntensor_to_pil(rand_square_copy_tf(img_tensor))\ntorch.Size([3, 512, 512])\n\n\n\n\n\nThe second augmentation copies random pixels and pastes them in different indices. I use random amounts ranging from 0.25-1.5% of the pixels.\n\n\nRandom Pixel Copy Transform\ndef rand_pixel_copy(img_tensor:torch.Tensor, pct:float=0.2):\n    \"\"\"\n    Copy data from a randomly selected set of pixels to another randomly selected set of pixels of a image tensor.\n\n    Args:\n    img_tensor (torch.Tensor): The input image tensor.\n    pct (float, optional): The percentage of the total number of pixels to be selected as the source and target sets\n                           of pixels.\n    \n    Returns:\n    torch.Tensor: The modified input image tensor.\n    \"\"\"\n    \n    src_dim = img_tensor.dim()\n    \n    img_tensor = img_tensor.unsqueeze(0) if src_dim == 3 else img_tensor\n    \n    # Get the shape of the img_tensor\n    b, c, h, w = img_tensor.shape\n    \n    # Calculate the number of pixels to be selected\n    num_pixels = int(img_tensor[-1:].numel() * pct)\n    \n    # Select the source pixel indices\n    source_indices = torch.LongTensor(num_pixels, 2).random_(0, h * w)\n    source_indices[:, 0] = source_indices[:, 0] // w\n    source_indices[:, 1] = source_indices[:, 1] % w\n    \n    # Select the target pixel indices\n    target_indices = torch.LongTensor(num_pixels, 2).random_(0, h * w)\n    target_indices[:, 0] = target_indices[:, 0] // w\n    target_indices[:, 1] = target_indices[:, 1] % w\n    \n    # Get the indices of the channels\n    c_indices = torch.arange(c).repeat(num_pixels, 1).t()\n    \n    # Copy the pixels\n    source_pixels = img_tensor[:, c_indices, source_indices[:, 0], source_indices[:, 1]]\n    img_tensor[:, c_indices, target_indices[:, 0], target_indices[:, 1]] = source_pixels\n    \n    return img_tensor.squeeze(0) if src_dim == 3 else img_tensor\nclass RandPixelCopy(nn.Module):\n    def __init__(self, pct=0.1):\n        \"\"\"\n        A PyTorch module that applies the `rand_pixel_copy` function to an input tensor.\n\n        Args:\n        pct (float, optional): The maximum percentage of the tensor's pixels to be copied.\n        \"\"\"\n        super().__init__()\n        self.pct = pct\n        \n    def forward(self, x):\n        # Apply the `rand_pixel_copy` function to the input tensor\n        return rand_pixel_copy(x, max(0.0025, random.random() * self.pct))\nTest the transform\nrand_pixel_copy_tf = RandPixelCopy(pct=0.015)\n\n# Convert the PIL image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)\nprint(img_tensor.shape)\n\n# Apply the `rand_pixel_copy_tf` transform to the image tensor\ntensor_to_pil(rand_pixel_copy_tf(img_tensor))\ntorch.Size([3, 512, 512])\n\n\n\n\n\nThe third combines the first two approaches by copying random squares and then applying the pixel copy transform on those squares. I use the same patch size range as the first augmentation but copy up to 100% of the pixels.\n\n\nRandom Square Pixel Copy Transform\ndef rand_square_pixel_copy(img_tensor:torch.Tensor, square_pct:float, pixel_pct:float):\n    \"\"\"\n    Copy data from a randomly selected set of pixels of a randomly selected square region to another randomly\n    selected square region of a tensor.\n\n    Args:\n    img_tensor (torch.Tensor): The input tensor.\n    square_pct (float): The percentage of the tensor's size to be used as the side length of the square regions.\n    pixel_pct (float): The percentage of the pixels of the source square region to be copied.\n\n    Returns:\n    torch.Tensor: The modified input tensor.\n    \"\"\"\n    # Get the shape of the tensor\n    h, w = img_tensor.shape[-2:]\n    \n    # Calculate the size of the square\n    szx = int(square_pct * h)\n    szy = int(square_pct * w)\n    \n    # Calculate the top-left coordinate of the first square\n    stx1 = int(random.random() * (1 - square_pct) * h)\n    sty1 = int(random.random() * (1 - square_pct) * w)\n    \n    # Calculate the top-left coordinate of the second square\n    stx2 = int(random.random() * (1 - square_pct) * h)\n    sty2 = int(random.random() * (1 - square_pct) * w)\n    \n    # Copy the data from the second square to the first square\n    source_pixels = rand_pixel_copy(img_tensor[..., stx2:stx2 + szx, sty2:sty2 + szy].clone(), pixel_pct)\n    img_tensor[..., stx1:stx1 + szx, sty1:sty1 + szy] = source_pixels\n    \n    # Return the modified input tensor\n    return img_tensor\nclass RandSquarePixelCopy(nn.Module):\n    def __init__(self, square_pct=0.2, pixel_pct=1.0, min_num=0, max_num=4):\n        \"\"\"\n        A PyTorch module that applies the `rand_square_pixel_copy` function to an input tensor.\n\n        Args:\n        square_pct (float, optional): The percentage of the tensor's size to be used as the side length of the\n                                      square regions.\n        pixel_pct (float, optional): The maximum percentage of the pixels of the source square region to be copied.\n        min_num (int, optional): The minimum number of times the `rand_square_pixel_copy` function is applied to\n                                 the input tensor.\n        max_num (int, optional): The maximum number of times the `rand_square_pixel_copy` function is applied to\n                                 the input tensor.\n        \"\"\"\n        super().__init__()\n        self.square_pct = square_pct\n        self.pixel_pct = pixel_pct\n        self.min_num = min_num\n        self.max_num = max_num\n        \n    def forward(self, x):\n        # Apply the `rand_square_pixel_copy` function to the input tensor\n        for i in range(random.randint(self.min_num, self.max_num)):\n            x = rand_square_pixel_copy(x, \n                                       max(0.1,random.random()*self.square_pct), \n                                       max(0.0025, random.random()*self.pixel_pct))\n        return x\nTest the transform\nrand_square_pixel_copy_tf = RandSquarePixelCopy(square_pct=0.2, pixel_pct=1.0, min_num=1, max_num=1)\n\nimg_tensor = transforms.PILToTensor()(sample_img)\nprint(img_tensor.shape)\n\ntensor_to_pil(rand_square_pixel_copy_tf(img_tensor))\ntorch.Size([3, 512, 512])\n\n\n\n\n\nI also tested the TrivialAugment augmentation from the paper linked below. TrivialAugment applies a single randomly selected transform to each image. I stuck with the default parameters.\n\nTrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation\n\n\n\nTrivial Augmentation\n# Create a TrivialAugmentWide object\ntrivial_aug = transforms.TrivialAugmentWide()\n\n# Convert the image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)[None]\nprint(img_tensor.shape)\n\n# Apply the TrivialAugmentWide to the tensor\ntensor_to_pil(trivial_aug(img_tensor))\ntorch.Size([1, 3, 512, 512])\n\n\n\n\n\nFinally, inspired by the TrivialAugment transform, I created a custom trivial augmentation that randomly applies the random square copy, random rand pixel copy, random square pixel copy, or no transform.\n\n\nCustom Trivial Augmentation Transform\nclass CustomTrivAug(nn.Module):\n    def __init__(self, aug_list):\n        \"\"\"\n        A PyTorch module that applies a random image augmentation.\n\n        Args:\n        aug_list (list): List of functions that implement the desired augmentations.\n        \"\"\"\n        super().__init__()\n        self.aug_list = aug_list\n        \n    def forward(self, x):\n        return random.choice(aug_list)(x)\nTest the transform\naug_list = [\n    # Identity transformation (no augmentation applied)\n    lambda x:x,\n    # Random square copy transformation\n    rand_square_copy_tf,\n    # Random pixel copy transformation\n    rand_pixel_copy_tf,\n    # Random square pixel copy transformation\n    rand_square_pixel_copy_tf\n]\n\n# Create an instance of the CustomTrivAug module\ncustom_triv_aug_tf = CustomTrivAug(aug_list=aug_list)\n\n# Convert a sample image to a tensor\nimg_tensor = transforms.PILToTensor()(sample_img)\nprint(img_tensor.shape)\n\n# Apply a random augmentation and convert the tensor back to an image\ntensor_to_pil(custom_triv_aug_tf(img_tensor))\ntorch.Size([3, 512, 512])\n\n\n\n\n\n\n\n\nModel\nI went with the pretrained ResNet 18-D model from the timm library for its balance of accuracy and speed.\nList available ResNet18 models\npd.DataFrame(timm.list_models('resnet18*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nresnet18\n\n\n\n\n1\n\n\nresnet18d\n\n\n\n\n\n\nInspect config for ResNet18 model\n# Import the resnet module\nfrom timm.models import resnet\n\n# Choose the resnet model\nresnet_model = 'resnet18d'\n\n# Get the default configuration of the chosen model as a Pandas DataFrame\npd.DataFrame.from_dict(resnet.default_cfgs[resnet_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet18d_ra2-48a79e06.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv1.0\n\n\n\n\nclassifier\n\n\nfc\n\n\n\n\n\n\nWe first get the normalization stats used to pre-train the model. We get the mean and standard deviation values for the red, green, and blue color channels from the default_cfgs dictionary.\nGet normalization stats\nnorm_stats = resnet.default_cfgs[resnet_model]['mean'], resnet.default_cfgs[resnet_model]['std']\nnorm_stats\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\nNext, we load the pretrained model and set the number of output classes for the model head. We’ll also update the device and data type of the model to the ones specified earlier.\nLoad ResNet18 model\n# Load the resenet model\nresnet18 = timm.create_model(resnet_model, pretrained=True, num_classes=len(class_names))\n\n# Set the device, data type, and name\nresnet18 = resnet18.to(device=device, dtype=dtype)\nresnet18.device = device\nresnet18.name = resnet_model\nSelect model\nmodel = resnet18\nGet model summary\n# Define the input to the model\ntest_inp = torch.randn(1, 3, 256, 256).to(device)\n\n# Get a summary of the model as a Pandas DataFrame\nsummary_df = markdown_to_pandas(f\"{get_module_summary(model, [test_inp])}\")\n\n# Filter the summary to only contain Conv2d layers and the model\nsummary_df = summary_df[(summary_df.index == 0) | (summary_df['Type'] == 'Conv2d')]\n\n# Remove the column \"Contains Uninitialized Parameters?\"\nsummary_df.drop('Contains Uninitialized Parameters?', axis=1)\n\n\n\n\n\n\n\n\nType\n\n\n# Parameters\n\n\n# Trainable Parameters\n\n\nSize (bytes)\n\n\nForward FLOPs\n\n\nBackward FLOPs\n\n\nIn size\n\n\nOut size\n\n\n\n\n\n\n0\n\n\nResNet\n\n\n11.2 M\n\n\n11.2 M\n\n\n44.9 M\n\n\n2.7 G\n\n\n5.3 G\n\n\n[1, 3, 256, 256]\n\n\n[1, 19]\n\n\n\n\n2\n\n\nConv2d\n\n\n864\n\n\n864\n\n\n3.5 K\n\n\n14.2 M\n\n\n14.2 M\n\n\n[1, 3, 256, 256]\n\n\n[1, 32, 128, 128]\n\n\n\n\n5\n\n\nConv2d\n\n\n9.2 K\n\n\n9.2 K\n\n\n36.9 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 32, 128, 128]\n\n\n[1, 32, 128, 128]\n\n\n\n\n8\n\n\nConv2d\n\n\n18.4 K\n\n\n18.4 K\n\n\n73.7 K\n\n\n301 M\n\n\n603 M\n\n\n[1, 32, 128, 128]\n\n\n[1, 64, 128, 128]\n\n\n\n\n14\n\n\nConv2d\n\n\n36.9 K\n\n\n36.9 K\n\n\n147 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 64, 64, 64]\n\n\n[1, 64, 64, 64]\n\n\n\n\n19\n\n\nConv2d\n\n\n36.9 K\n\n\n36.9 K\n\n\n147 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 64, 64, 64]\n\n\n[1, 64, 64, 64]\n\n\n\n\n23\n\n\nConv2d\n\n\n36.9 K\n\n\n36.9 K\n\n\n147 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 64, 64, 64]\n\n\n[1, 64, 64, 64]\n\n\n\n\n28\n\n\nConv2d\n\n\n36.9 K\n\n\n36.9 K\n\n\n147 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 64, 64, 64]\n\n\n[1, 64, 64, 64]\n\n\n\n\n33\n\n\nConv2d\n\n\n73.7 K\n\n\n73.7 K\n\n\n294 K\n\n\n75.5 M\n\n\n150 M\n\n\n[1, 64, 64, 64]\n\n\n[1, 128, 32, 32]\n\n\n\n\n38\n\n\nConv2d\n\n\n147 K\n\n\n147 K\n\n\n589 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 128, 32, 32]\n\n\n[1, 128, 32, 32]\n\n\n\n\n43\n\n\nConv2d\n\n\n8.2 K\n\n\n8.2 K\n\n\n32.8 K\n\n\n8.4 M\n\n\n16.8 M\n\n\n[1, 64, 32, 32]\n\n\n[1, 128, 32, 32]\n\n\n\n\n46\n\n\nConv2d\n\n\n147 K\n\n\n147 K\n\n\n589 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 128, 32, 32]\n\n\n[1, 128, 32, 32]\n\n\n\n\n51\n\n\nConv2d\n\n\n147 K\n\n\n147 K\n\n\n589 K\n\n\n150 M\n\n\n301 M\n\n\n[1, 128, 32, 32]\n\n\n[1, 128, 32, 32]\n\n\n\n\n56\n\n\nConv2d\n\n\n294 K\n\n\n294 K\n\n\n1.2 M\n\n\n75.5 M\n\n\n150 M\n\n\n[1, 128, 32, 32]\n\n\n[1, 256, 16, 16]\n\n\n\n\n61\n\n\nConv2d\n\n\n589 K\n\n\n589 K\n\n\n2.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 256, 16, 16]\n\n\n[1, 256, 16, 16]\n\n\n\n\n66\n\n\nConv2d\n\n\n32.8 K\n\n\n32.8 K\n\n\n131 K\n\n\n8.4 M\n\n\n16.8 M\n\n\n[1, 128, 16, 16]\n\n\n[1, 256, 16, 16]\n\n\n\n\n69\n\n\nConv2d\n\n\n589 K\n\n\n589 K\n\n\n2.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 256, 16, 16]\n\n\n[1, 256, 16, 16]\n\n\n\n\n74\n\n\nConv2d\n\n\n589 K\n\n\n589 K\n\n\n2.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 256, 16, 16]\n\n\n[1, 256, 16, 16]\n\n\n\n\n79\n\n\nConv2d\n\n\n1.2 M\n\n\n1.2 M\n\n\n4.7 M\n\n\n75.5 M\n\n\n150 M\n\n\n[1, 256, 16, 16]\n\n\n[1, 512, 8, 8]\n\n\n\n\n84\n\n\nConv2d\n\n\n2.4 M\n\n\n2.4 M\n\n\n9.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 512, 8, 8]\n\n\n[1, 512, 8, 8]\n\n\n\n\n89\n\n\nConv2d\n\n\n131 K\n\n\n131 K\n\n\n524 K\n\n\n8.4 M\n\n\n16.8 M\n\n\n[1, 256, 8, 8]\n\n\n[1, 512, 8, 8]\n\n\n\n\n92\n\n\nConv2d\n\n\n2.4 M\n\n\n2.4 M\n\n\n9.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 512, 8, 8]\n\n\n[1, 512, 8, 8]\n\n\n\n\n97\n\n\nConv2d\n\n\n2.4 M\n\n\n2.4 M\n\n\n9.4 M\n\n\n150 M\n\n\n301 M\n\n\n[1, 512, 8, 8]\n\n\n[1, 512, 8, 8]\n\n\n\n\n\n\n\n\nDataset Preparation\nI split the dataset randomly into 90% for training and 10% for validation.\nGet training and validation sets\n# Shuffle the image paths\nrandom.shuffle(img_paths)\n\n# Define the percentage of the images that should be used for training\ntrain_pct = 0.9\nval_pct = 0.1\n\n# Calculate the index at which to split the subset of image paths into training and validation sets\ntrain_split = int(len(img_paths)*train_pct)\nval_split = int(len(img_paths)*(train_pct+val_pct))\n\n# Split the subset of image paths into training and validation sets\ntrain_paths = img_paths[:train_split]\nval_paths = img_paths[train_split:]\n\n# Print the number of images in the training and validation sets\nlen(train_paths), len(val_paths)\n(138361, 15374)\nThe hand gestures in some images are hard to see at low resolutions, so I resized and padded the samples to 288x288 for training to balance computational efficiency and model performance.\nSet training image size\ntrain_sz = (288,288)\nResize Pad Transform\nclass ResizePad(nn.Module):\n    def __init__(self, max_sz=256, padding_mode='edge'):\n        \"\"\"\n        A PyTorch module that resizes an image tensor and adds padding to make it a square tensor.\n\n        Args:\n        max_sz (int, optional): The size of the square tensor.\n        padding_mode (str, optional): The padding mode used when adding padding to the tensor.\n        \"\"\"\n        super().__init__()\n        self.max_sz = max_sz\n        self.padding_mode = padding_mode\n        \n    def forward(self, x):\n        # Get the width and height of the image tensor\n        w, h = TF.get_image_size(x)\n        \n        # Resize the image tensor so that its minimum dimension is equal to `max_sz`\n        size = int(min(w, h) / (max(w, h) / self.max_sz))\n        x = TF.resize(x, size=size)\n        \n        # Add padding to make the image tensor a square\n        w, h = TF.get_image_size(x)\n        offset = (self.max_sz - min(w, h)) // 2\n        padding = [0, offset] if h &lt; w else [offset, 0]\n        x = TF.pad(x, padding=padding, padding_mode=self.padding_mode)\n        x = TF.resize(x, size=[self.max_sz] * 2)\n        \n        return x\nTest the transform\nprint(f\"Source image: {sample_img.size}\")\n\n# Crop the source image\nw, h = sample_img.size\ncropped_img = sample_img.crop([0, h//4, w, h-h//4])\nprint(f\"Cropped image: {cropped_img.size}\")\n\n# Create a `ResizePad` object\nresize_pad = ResizePad(max_sz=max(train_sz))\n\n# Convert the cropped image to a tensor\nimg_tensor = transforms.PILToTensor()(cropped_img)[None]\nprint(f\"Cropped tensor: {img_tensor.shape}\")\n\n# Resize and pad the tensor\nresized_tensor = resize_pad(img_tensor)\nprint(f\"Padded tensor: {resized_tensor.shape}\")\n\n# Apply random pixel copy to the resized and padded tensor\ntensor_to_pil(resized_tensor)\nSource image: (512, 512)\nCropped image: (512, 256)\nCropped tensor: torch.Size([1, 3, 256, 512])\nPadded tensor: torch.Size([1, 3, 288, 288])\n\n\n\n\n\nThe TrivialAugment transform requires PIL Images, so I applied all transforms per item rather than per batch for consistency.\nSet image transforms\n# Set transforms for training set\ntrain_tfms = [\n    ResizePad(max_sz=max(train_sz)),\n#     transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    transforms.Normalize(*norm_stats),\n#     rand_square_copy_tf,\n#     rand_pixel_copy_tf,\n#     rand_square_pixel_copy_tf\n    custom_triv_aug_tf,\n]\n\n# Set transforms for validation set\nvalid_tfms = [\n    ResizePad(max_sz=max(train_sz)),\n    transforms.ToTensor(),\n    transforms.Normalize(*norm_stats),\n]\nWe pass the list of transforms into a custom Dataset class along with the image paths and class names.\nDefine training dataset class\nclass ImageDataset(Dataset):\n    def __init__(self, img_paths, classes, tfms):\n        # Store the arguments as instance variables\n        self.img_paths = img_paths\n        self.classes = classes\n        self.tfms = tfms\n        \n        # Create a mapping from class names to class indices\n        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n\n    def __len__(self):\n        # Return the number of images in the dataset\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n        # Get the path of the image at the given index\n        img_path = self.img_paths[index]\n        \n        # Get the label of the image\n        label = self.class_to_idx[img_path.parent.name]\n        \n        # Open the image\n        image = Image.open(img_path).convert('RGB')\n        \n        return self.tfms(image), label\nInitialize datasets\n# create the training dataset using the composed transformations\ntrain_dataset = ImageDataset(img_paths=train_paths, \n                             classes=class_names,\n                             tfms=transforms.Compose(train_tfms))\n\n# create the validation dataset\nvalid_dataset = ImageDataset(img_paths=val_paths, \n                             classes=class_names,\n                             tfms=transforms.Compose(valid_tfms))\n\n# print the length of the training and validation datasets\nlen(train_dataset), len(valid_dataset)\n(138361, 15374)\nInspect training set sample\ntensor_to_pil(denorm_img_tensor(train_dataset[0][0], *norm_stats))\n\n\n\n\n\nInspect validation set sample\ntensor_to_pil(denorm_img_tensor(valid_dataset[0][0], *norm_stats))\n\n\n\n\n\nSet training batch size\nbs = 32\nInitialzie dataloaders\ntrain_dataloader = DataLoader(train_dataset, \n                              batch_size=bs, \n                              shuffle=True, \n                              num_workers=multiprocessing.cpu_count())\n\nvalid_dataloader = DataLoader(valid_dataset, batch_size=bs)\n\ndls = DataLoaders(train_dataloader, valid_dataloader)\n\n# Print the number of batches in the training and validation dataloaders\nlen(dls.train), len(dls.valid)\n(4324, 481)\n\n\nTraining\nWe can define a custom callback to integrate W&B with the miniai library. The one below tracks the loss and accuracy values for the training and validation sets.\nDefine training callbacks\nclass WandBCB(MetricsCB):\n    \"\"\"\n    A Callback class that integrates with the Weights and Biases (W&B) platform to log training \n    and validation metrics, as well as sample figures during training.\n    \"\"\"\n    order=100\n    \n    def __init__(self, config, *ms, project:str, name:str=None, **metrics):\n        \"\"\"\n        Initialize the WandBCB class.\n        \n        Parameters:\n            config (dict): Configuration parameters for the W&B run.\n            ms (list): List of metrics to be logged.\n            project (str): The name of the W&B project.\n            name (str, optional): The name of the W&B run. Defaults to None.\n            metrics (dict, optional): Additional metrics to be logged.\n        \"\"\"\n        fc.store_attr()\n        self.train = False\n        self.run = None\n        super().__init__(*ms, **metrics)\n        \n    def before_fit(self, learn):\n        \"\"\"\n        Initialize the W&B run.\n        \n        Parameters:\n            learn (miniai.learner.Learner): The miniai Learner object.\n        \"\"\"\n        # If a name for the run is provided, use it, otherwise use the default name\n        if self.name:\n            self.run = wandb.init(project=self.project, name=self.name, config=self.config)\n        else:\n            self.run = wandb.init(project=self.project, config=self.config)\n            \n    def after_fit(self, learn):\n        \"\"\"\n        Finalize the W&B run.\n        \n        Parameters:\n            learn (miniai.learner.Learner): The miniai Learner object.\n        \"\"\"\n        wandb.finish()\n\n    def _log(self, d):\n        \"\"\"\n        Log the metrics to W&B.\n        \n        Parameters:\n            d (dict): Dictionary of metrics.\n        \"\"\"\n        # Log the metrics with a prefix of \"train_\" if in training mode, otherwise \"val_\"\n        if self.train: \n            wandb.log({'train_'+m:float(d[m]) for m in self.all_metrics})\n        else: \n            wandb.log({'val_'+m:float(d[m]) for m in self.all_metrics})\n\n    def after_batch(self, learn):\n        \"\"\"\n        Log the loss after a batch\n        \"\"\"\n        super().after_batch(learn)\n        # Log the batch loss to W&B\n        wandb.log({'loss':learn.loss})\n        \n    def after_epoch(self, learn):\n        \"\"\"\n        Update the training mode flag and call the parent class's `after_epoch` method.\n\n        Parameters:\n            learn (miniai.learner.Learner): The miniai Learner object.\n        \"\"\"\n        # Update the training mode flag\n        self.train = learn.training\n        super().after_epoch(learn)\nclass OutOfBoundsCB(Callback):\n    \"\"\"\n    A callback that exits the training session if the loss is not NaN or infinite.\n    \"\"\"\n    def __init__(self, msg=\"Loss out of bounds\"):\n        fc.store_attr()\n        \n    def after_loss(self, learn):\n        \"\"\"\n        Check if the loss is NaN or infinite.\n        \"\"\"\n        if learn.loss.detach().isnan() or learn.loss.detach().isinf(): \n            print(self.msg)\n            raise CancelFitException()\nInitialize callbacks\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\n\ncbs = [MixedPrecision(), \n       OutOfBoundsCB(),\n       DeviceCB(), \n       metrics, \n       ProgressCB(plot=False),\n      ]\nSet optimizer\nopt_func = partial(torch.optim.AdamW, eps=1e-5)\nSet learning rate scheduler\nlr = 1e-3\nepochs = 3\n\ntmax = epochs * len(dls.train)\nsched = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs.append(BatchSchedCB(sched))\nConfigure Weights & Biases project\nproject_name = f\"miniai_data_augmentation_seed-{seed}\"\n# run_name = \"baseline\"\n# run_name = \"rand-square-cp-1\"\n# run_name = \"rand-pixel-cp-1\"\n# run_name = \"rand-square-pixel-cp-1\"\n# run_name = \"trivial-aug-item-1\"\nrun_name = \"custom-trivial-aug-1\"\n\nproject_config = {'lr':lr, 'epochs':epochs}\n\nwandbcb =  WandBCB(config=project_config, \n                   project=project_name,\n                   name=run_name,\n                   accuracy=MulticlassAccuracy()\n                  )\ncbs.append(wandbcb)\nInitialize learner\nlearn = TrainLearner(model, dls, torch.nn.functional.cross_entropy, lr=lr, cbs=cbs, opt_func=opt_func)\nTrain model\nlearn.fit(epochs)\n\n\n\n\n\n\naccuracy\n\n\nloss\n\n\nepoch\n\n\ntrain\n\n\n\n\n\n\n0.905\n\n\n0.343\n\n\n0\n\n\ntrain\n\n\n\n\n0.966\n\n\n0.113\n\n\n0\n\n\neval\n\n\n\n\n0.976\n\n\n0.077\n\n\n1\n\n\ntrain\n\n\n\n\n0.989\n\n\n0.038\n\n\n1\n\n\neval\n\n\n\n\n0.994\n\n\n0.021\n\n\n2\n\n\ntrain\n\n\n\n\n0.994\n\n\n0.021\n\n\n2\n\n\neval\n\n\n\n\n\n\nSet checkpoint directory\n# Create a directory to store the checkpoints if it does not already exist\ncheckpoint_dir = Path(f\"./{project_name}/\")\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\n\n# Print the checkpoint path\ncheckpoint_dir\nPath('miniai_data_augmentation_seed-1234')\nSave model checkpoint\ntorch.save(learn.model.state_dict(), checkpoint_dir/f\"{model.name}-{run_name}.pth\")\nTest model\ninfer_sz = max(train_sz)\n\n# Choose a random image from the list of image paths\nimg_path = random.choice(img_paths)\n\n# Print the actual class of the chosen image\nprint(f\"Class: {img_path.parent.name}\")\n\n# Open the image and resize it\nsample_img = Image.open(img_path)\ninp_img = resize_img(sample_img.copy(), infer_sz)\n\n# Convert the image to a tensor and move it to the device\nimg_tensor = pil_to_tensor(inp_img, *norm_stats).to(device=device)\n\n# Make a prediction with the model\nwith torch.no_grad():\n    pred = learn.model(img_tensor)\n\n# Get the predicted class index and convert it to the class name\npred_class = train_dataset.classes[torch.argmax(torch.softmax(pred, dim=1))]\n\n# Print the predicted class\nprint(f\"Predicted Class: {pred_class}\")\n\n# Display the image\nsample_img\nClass: rock\nPredicted Class: rock"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#results",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#results",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Results",
    "text": "Results\nAll training runs, including the baseline no augmentation run, ended with over 99% accuracy across all random seeds.\n\nValidation Set\nThe TrivialAugmentWide transform had the highest validation accuracy across three of four random seeds, but the scores were too close to name a definitive winner.\nSeed: 1\n\n\n\n\n\nSeed: 42\n\n\n\n\n\nSeed: 100\n\n\n\n\n\nSeed: 1234\n\n\n\n\n\n\n\nTest Set\nI also ended each run by testing the models without augmentation on the entire dataset. I used a minimum resolution of 288p, as I did for training but did not enforce square aspect ratios. The results were quite different from the validation sets.\nPerform inference on dataset\n# Initialize a list to store the file paths of the images that were incorrectly classified\nwrong_imgs = []\n\n# Iterate through the test image paths\nfor path in tqdm(img_paths):\n    # Get the actual class of the image\n    target_cls = path.parent.name\n\n    # Open and resize the image\n    sample_img = Image.open(path)\n    sample_img = resize_img(sample_img, infer_sz)\n\n    # Convert the image to a tensor and move it to the device\n    img_tensor = pil_to_tensor(sample_img, *norm_stats).to(device=device)\n\n    # Make a prediction with the model\n    with torch.no_grad():\n        pred = learn.model(img_tensor)\n\n    # Get the predicted class index and convert it to the class name\n    pred_cls = train_dataset.classes[torch.argmax(torch.softmax(pred, dim=1))]\n\n    # If the prediction is incorrect, add the file path to the list of wrong images\n    if pred_cls != target_cls: \n        wrong_imgs.append(path)\n\nlen(wrong_imgs)\n251\nInspect the number of wrong predictions per class\n# Create a DataFrame from the list of incorrectly classified images\nwrong_imgs_df = pd.DataFrame(wrong_imgs)\n\n# Add a column to the DataFrame with the actual class of each image\nwrong_imgs_df['class'] = wrong_imgs_df.apply(lambda row: Path(row[0]).parent.stem, axis=1)\n\n# Create a DataFrame with the class distribution of the incorrectly classified images\nclass_dist_df = wrong_imgs_df['class'].value_counts().to_frame().rename(columns={\"class\":run_name})\n\n# Set the index name to \"class\"\nclass_dist_df.rename_axis(\"class\", inplace=True)\n\n# Display the class distribution DataFrame\nclass_dist_df\n\n\n\n\n\n\n\n\ncustom-trivial-aug-1\n\n\n\n\nclass\n\n\n\n\n\n\n\n\npeace\n\n\n27\n\n\n\n\none\n\n\n23\n\n\n\n\nthree\n\n\n22\n\n\n\n\nno_gesture\n\n\n22\n\n\n\n\nstop\n\n\n19\n\n\n\n\nfour\n\n\n17\n\n\n\n\ntwo_up\n\n\n15\n\n\n\n\ncall\n\n\n15\n\n\n\n\nstop_inverted\n\n\n13\n\n\n\n\nlike\n\n\n12\n\n\n\n\nrock\n\n\n12\n\n\n\n\nfist\n\n\n10\n\n\n\n\npalm\n\n\n9\n\n\n\n\nok\n\n\n8\n\n\n\n\npeace_inverted\n\n\n7\n\n\n\n\ntwo_up_inverted\n\n\n6\n\n\n\n\nmute\n\n\n5\n\n\n\n\ndislike\n\n\n5\n\n\n\n\nthree2\n\n\n4\n\n\n\n\n\n\n\nSave DataFrame\nclass_dist_df_path = checkpoint_dir/f\"class_dist_df-{run_name}.json\"\nclass_dist_df.to_json(class_dist_df_path)\nWith this seed, the TrivialAugmentWide model missed the second-most number of predictions during inference, even though it had the highest validation accuracy.\nLoad and concatenate DataFrames\ndef get_class_dist_agg_df(folder, ext='json'):\n    # Get a list of paths to `ext` files in the folder\n    df_paths = glob(os.path.join(folder, f\"*.{ext}\"))\n    \n    # Create a generator that reads the class distribution data from each file in df_paths\n    class_dist_dfs = (pd.read_json(f).rename_axis('class') for f in df_paths)\n\n    # Concatenate the class distribution data into a single DataFrame\n    return pd.concat(class_dist_dfs, axis=1, sort=False).fillna(0).astype(int)\n# Concatenate the class distribution data into a single DataFrame\nclass_dist_agg_df = get_class_dist_agg_df(checkpoint_dir)\n\n# Compute the sum of the class distribution data for each run\nsums = class_dist_agg_df.sum(axis=0)\n\n# Add a row to the DataFrame with the totals\nclass_dist_agg_df.loc['Total'] = sums\n\n# Fill any missing values with 0 and convert the values to integers\nclass_dist_agg_df.sort_values(by='Total', axis=1)\n\n\n\n\n\n\n\n\nrand-square-pixel-cp-1\n\n\nbaseline\n\n\nrand-square-cp-1\n\n\ncustom-trivial-aug-1\n\n\ntrivial-aug-item-1\n\n\nrand-pixel-cp-1\n\n\n\n\nclass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncall\n\n\n6\n\n\n15\n\n\n9\n\n\n15\n\n\n16\n\n\n13\n\n\n\n\ndislike\n\n\n8\n\n\n5\n\n\n5\n\n\n5\n\n\n6\n\n\n10\n\n\n\n\nfist\n\n\n11\n\n\n8\n\n\n9\n\n\n10\n\n\n13\n\n\n21\n\n\n\n\nfour\n\n\n25\n\n\n19\n\n\n15\n\n\n17\n\n\n22\n\n\n31\n\n\n\n\nlike\n\n\n18\n\n\n11\n\n\n15\n\n\n12\n\n\n19\n\n\n14\n\n\n\n\nmute\n\n\n5\n\n\n3\n\n\n6\n\n\n5\n\n\n4\n\n\n10\n\n\n\n\nno_gesture\n\n\n18\n\n\n23\n\n\n23\n\n\n22\n\n\n44\n\n\n25\n\n\n\n\nok\n\n\n5\n\n\n15\n\n\n11\n\n\n8\n\n\n8\n\n\n7\n\n\n\n\none\n\n\n14\n\n\n22\n\n\n28\n\n\n23\n\n\n21\n\n\n19\n\n\n\n\npalm\n\n\n11\n\n\n10\n\n\n7\n\n\n9\n\n\n23\n\n\n12\n\n\n\n\npeace\n\n\n20\n\n\n17\n\n\n26\n\n\n27\n\n\n24\n\n\n45\n\n\n\n\npeace_inverted\n\n\n1\n\n\n9\n\n\n7\n\n\n7\n\n\n6\n\n\n7\n\n\n\n\nrock\n\n\n8\n\n\n8\n\n\n6\n\n\n12\n\n\n12\n\n\n6\n\n\n\n\nstop\n\n\n14\n\n\n18\n\n\n11\n\n\n19\n\n\n15\n\n\n23\n\n\n\n\nstop_inverted\n\n\n6\n\n\n10\n\n\n10\n\n\n13\n\n\n12\n\n\n21\n\n\n\n\nthree\n\n\n20\n\n\n22\n\n\n31\n\n\n22\n\n\n37\n\n\n20\n\n\n\n\nthree2\n\n\n5\n\n\n11\n\n\n7\n\n\n4\n\n\n8\n\n\n4\n\n\n\n\ntwo_up\n\n\n10\n\n\n11\n\n\n12\n\n\n15\n\n\n16\n\n\n18\n\n\n\n\ntwo_up_inverted\n\n\n8\n\n\n6\n\n\n8\n\n\n6\n\n\n6\n\n\n11\n\n\n\n\nTotal\n\n\n213\n\n\n243\n\n\n246\n\n\n251\n\n\n312\n\n\n317\n\n\n\n\n\n\n\nEven though the TrivialAugmentWide transform seemed like the winner across the validation sets, it performed the worst overall during inference. The custom trivial augmentation transform performed the best overall, with the random square copy transform a close second.\nCompare performance across random seeds\n# Get project folders\nfolder_prefix = project_name.split('-')[0]\nproject_folders = [f for f in os.listdir(\".\") if f.startswith(folder_prefix) and os.path.isdir(f)]\n\n# Get the class distribution data for each random seed\ndf_list = [get_class_dist_agg_df(f).sum(axis=0).rename(int(f.split('-')[-1])) for f in project_folders]\n\n# Concatenate the class distribution data across seeds into a single DataFrame\nmissing_imgs_df = pd.concat(df_list, axis=1, sort=False).transpose().sort_index()\n\n# Compute the mean of the class distribution data for each run\nmean_vals = missing_imgs_df.mean(axis=0).rename('Mean')\n\n# Fill any missing values with 0, convert the values to integers, and sort the columns\nmean_vals_df = mean_vals.to_frame().transpose().fillna(0).astype(int)\n\n# Append mean values to DataFrame\npd.concat([missing_imgs_df, mean_vals_df], axis=0).sort_values(by='Mean', axis=1)\n\n\n\n\n\n\n\n\ncustom-trivial-aug-1\n\n\nrand-square-cp-1\n\n\nrand-square-pixel-cp-1\n\n\nbaseline\n\n\nrand-pixel-cp-1\n\n\ntrivial-aug-item-1\n\n\n\n\n\n\n1\n\n\n231\n\n\n215\n\n\n261\n\n\n257\n\n\n272\n\n\n321\n\n\n\n\n42\n\n\n231\n\n\n238\n\n\n290\n\n\n284\n\n\n309\n\n\n312\n\n\n\n\n100\n\n\n229\n\n\n255\n\n\n224\n\n\n238\n\n\n352\n\n\n307\n\n\n\n\n1234\n\n\n251\n\n\n246\n\n\n213\n\n\n243\n\n\n317\n\n\n312\n\n\n\n\nMean\n\n\n235\n\n\n238\n\n\n247\n\n\n255\n\n\n312\n\n\n313\n\n\n\n\n\n\nSort runs for each random seed\n# Define a lambda function to sort the column names based on their values in each row and return a new DataFrame\nsort_cols = lambda row: pd.DataFrame({row.name: row.sort_values().index.tolist()}).transpose()\n\n# Apply the lambda function to each row in the DataFrame and concatenate the results into a new DataFrame\nsorted_cols_df = pd.concat(missing_imgs_df.apply(sort_cols, axis=1).tolist())\n\nsorted_cols_df\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n1\n\n\nrand-square-cp-1\n\n\ncustom-trivial-aug-1\n\n\nbaseline\n\n\nrand-square-pixel-cp-1\n\n\nrand-pixel-cp-1\n\n\ntrivial-aug-item-1\n\n\n\n\n42\n\n\ncustom-trivial-aug-1\n\n\nrand-square-cp-1\n\n\nbaseline\n\n\nrand-square-pixel-cp-1\n\n\nrand-pixel-cp-1\n\n\ntrivial-aug-item-1\n\n\n\n\n100\n\n\nrand-square-pixel-cp-1\n\n\ncustom-trivial-aug-1\n\n\nbaseline\n\n\nrand-square-cp-1\n\n\ntrivial-aug-item-1\n\n\nrand-pixel-cp-1\n\n\n\n\n1234\n\n\nrand-square-pixel-cp-1\n\n\nbaseline\n\n\nrand-square-cp-1\n\n\ncustom-trivial-aug-1\n\n\ntrivial-aug-item-1\n\n\nrand-pixel-cp-1"
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#discussion",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#discussion",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Discussion",
    "text": "Discussion\nThe random square copy augmentation performed consistently well throughout my initial testing and final set of experiments. I thought this one might not work well with the hand gesture dataset as it could copy or cover parts of a hand. I assumed that would be an issue if the result were too close to another gesture class.\nThe random pixel copy transform did much worse than I expected on the validation sets and during inference. I thought it would be one of the better-performing augmentations since it was unlikely to obscure a hand. I am curious if some of the training images were already so noisy that adding more was too much.\nGiven how poorly the pixel copy transform performed relative to the others, I was surprised my custom trivial augmentation transform did the best on average.\nSomething I found interesting is that the trivial augmentation models initially performed far better during inference than the others. However, it lost that lead as I cleaned up the dataset. I wonder if the more extreme augmentations included with that transform help with a dirty dataset? With the cleaned-up dataset, I believe some of the included transforms altered the images too much.\nLooking to the future, recent improvements in fine-grained control for generative models like ControlNet for Stable Diffusion may make synthetic datasets more comparable to real-world images. I plan to explore this approach in future experiments to see if it can improve the performance of deep learning models trained on visual data."
  },
  {
    "objectID": "posts/miniai-data-augmentation-experiments/part-1/index.html#conclusion",
    "href": "posts/miniai-data-augmentation-experiments/part-1/index.html#conclusion",
    "title": "Exploring the Impact of Different Image Augmentations on Hand Gesture Recognition",
    "section": "Conclusion",
    "text": "Conclusion\nMy findings suggest that different datasets may require different augmentation strategies, and it’s important to test multiple augmentations to find the best one for a particular dataset. In addition, factors such as the complexity and quality of the dataset and the type of deep learning model may influence the performance of image augmentations.\nHowever, the square-copy transform delivered the best bang for the buck in these experiments. It is nearly tied with the custom trivial augmentation for overall performance and is simple to implement.\nOf course, experiments with a known dataset are no substitute for real-world testing. Therefore, I encourage you to try the in-browser demos I’ve provided to test the models trained with different augmentations using your webcam.\n\nLive Demos\n\n\n\nData Augmentation\nDemo\n\n\n\n\nBaseline (no augmentation)\nhttps://cj-mills.github.io/miniai-resnet18d-baseline-demo/\n\n\nRandom Square Copy\nhttps://cj-mills.github.io/miniai-resnet18d-rand-square-cp-demo/\n\n\nRandom Pixel Copy\nhttps://cj-mills.github.io/miniai-resnet18d-rand-pixel-cp-demo/\n\n\nRandom Square Pixel Copy\nhttps://cj-mills.github.io/miniai-resnet18d-rand-square-pixel-cp-demo/\n\n\nTrivialAugmentWide\nhttps://cj-mills.github.io/miniai-resnet18d-trivial-aug-demo/\n\n\nCustom Trivial Augment\nhttps://cj-mills.github.io/miniai-resnet18d-custom-trivial-aug-demo/\n\n\n\nThe models run locally in your browser using the Barracuda inference library for Unity. I selected the best-performing runs for each image augmentation to give them their best shot. Let me know which one performs best for you in the comments below."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html",
    "href": "posts/no-ml-degree-book-notes/index.html",
    "title": "Notes on No ML Degree Book",
    "section": "",
    "text": "Key Points\nProgramming\nMachine Learning\nA Base Portfolio\nTalent Projects\nSelf Evaluation\nIdeas\nWorkflow\nJob Hunting\nReferences"
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#key-points",
    "href": "posts/no-ml-degree-book-notes/index.html#key-points",
    "title": "Notes on No ML Degree Book",
    "section": "Key Points",
    "text": "Key Points\n\nSelf-learning ML\n\nStart with software engineering, then transition to machine learning.\nUse free peer-to-peer CS schools to learn programming and Fastai to learn machine learning.\n\n\n\nHireability\n\nSearch for small companies, companies with specific needs, and organizations with practical interviews.\nEmployers hire self-learners based on validated real-world results.\n\n\n\nPortfolio\n\nCreate a solid portfolio that requires high-effort focus and develop rigorous work habits.\nThe safest portfolio projects involve publishing papers, machine learning competitions, and contributing to open-source projects.\nThe second-best projects are creating live ML products, collaborating with people in the industry, and developing ML content with high engagement.\nResult-based portfolio projects have metrics or testimonials, a context, and third-party validation.\nImprove promising existing projects instead of coming up with gut project ideas."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#programming",
    "href": "posts/no-ml-degree-book-notes/index.html#programming",
    "title": "Notes on No ML Degree Book",
    "section": "Programming",
    "text": "Programming\n\nStart with Programming\n\nThere are far more entry-level positions in software development.\nAim for at least six months to two years of study and work experience.\n\n\n\nNo-degree Tech Schools and Online Courses\n\n\n\nSchools & Courses\nDescription\n\n\n\n\nCodecademy\nLearn to code for free.\n\n\nScrimba\nInteractive courses for frontend development.\n\n\nfreeCodeCamp\nLearn to code for free, building projects.\n\n\n42\n42 is a tuition-free, peer-to-peer, project-based, online computer science training program.\n\n\nHolberton School\nLearn software development in a collaborative, project-based environment.\n\n\n\n\n\nBoot Camps\n\n\n\nBoot Camps\nDescription\n\n\n\n\nBloom Institute of Technology\nBloom Institute of Technology is an online tech school that offers a deferred tuition program.\n\n\n\n\n\nComputer Science\n\n90% of today’s models train on and deploy to servers.\nMost work focuses on making the data, training, and production process faster by improving efficiency and organization.\nA practical computer science curriculum that focuses on projects and programming provides a solid base.\n\n\n\nFront-end and Mobile\n\nRunning ML models on personal computers and phones provides compelling cost, latency, and privacy benefits.\nMajor shifts on the client side include human-in-the-loop, prompt engineering, and active learning.\nCreating smaller intermediate models, workflows, and programs to interact with server-side models is crucial.\n\n\n\n\nTools\nDescription\n\n\n\n\nTensorFlow.js\nDevelop ML models in JavaScript, and use ML directly in the browser or in Node.js.\n\n\nONNX Runtime Web\nONNX Runtime Web is a Javascript library for running ONNX models on browsers and on Node.js.\n\n\nEigen (C++) compiled with Web Assembly\nEigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\n\n\nPyScript\nPyScript is a framework that allows users to create rich Python applications in the browser using HTML’s interface and the power of Pyodide, WASM, and modern web technologies."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#machine-learning",
    "href": "posts/no-ml-degree-book-notes/index.html#machine-learning",
    "title": "Notes on No ML Degree Book",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nLearning Machine Learning\n\nPrioritize creating a great resume instead of building competitive interview skills.\n\nA strong portfolio weighs heavier than a boot camp graduation.\n\nFocus on ML opportunities that have practical interviews and light theory requirements.\nLearn data-centric problem-solving tools.\nIdentify, scope, communicate and solve problems.\nBuild a portfolio with externally validated results.\nGain a light overview of ML and statistics.\nMany companies look for strong programmers and offer on-the-job ML training.\n\n\n\nPractical ML Courses\n\nPick a practical ML course and study it for one month.\n\nAfter the first month, 90% of your focus should be on your portfolio.\n\nClassic machine learning is still prevalent in the industry and often shows up in interviews.\n\nExplore one or two videos each evening from StatQuest’s Statistics Fundamentals and Machine Learning playlists.\n\n\n\n\n\nCourses\nDescription\n\n\n\n\nFast.ai: Practical Deep Learning for Coders\nFast.ai provides a practical, application-first approach to deep learning.\n\n\nKaggle: 30 Days of ML\nMachine learning beginner → Kaggle competitor in 30 days.\n\n\nMade With ML Course\nLearn how to responsibly deliver value with ML.\n\n\n\n\n\n\n\n\n\nSkills\n\n\n\n\nGet comfortable working with lots of tools mixing off-the-shelf library calls with dabbling in the source code, context switching, and debugging.\n\n\nSpot potential risks and weaknesses with your solutions and how to mitigate them.\n\n\nLearn the types of problems machine learning can and cannot solve.\n\n\nLearn when to use paid APIs, open-source, or custom solutions.\n\n\nLearn rudimentary awareness of how your model impacts a business, including privacy, UI/UX, legal, ethics, and the business model.\n\n\nCommunicate expectations and timelines to technical and non-technical stakeholders.\n\n\nLearn how and when to mitigate risk from your inexperience.\n\n\nUnderstand what data is available and how to get more.\n\n\nExtract, visualize, clean, and load data.\n\n\nUnderstand the data and use it to make informed decisions.\n\n\nUnderstand the type of problem and how to find a solution.\n\n\nSet and measure appropriate objectives and success criteria.\n\n\nDevelop baseline models.\n\n\nTrain models with state-of-the-art results.\n\n\nQuickly and efficiently debug models.\n\n\nVisualize model performance.\n\n\nDeploy models and understand memory, cost, queries-per-second, and latency."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#a-base-portfolio",
    "href": "posts/no-ml-degree-book-notes/index.html#a-base-portfolio",
    "title": "Notes on No ML Degree Book",
    "section": "A Base Portfolio",
    "text": "A Base Portfolio\n\nWeak Portfolio Projects\n\nDon’t include toy projects like MNIST on your resume.\nML projects that are too hard for the recruiter to evaluate or lack results don’t help your resume.\nSelf-learners need to differentiate themselves from fake and low-effort portfolios.\n\n\n\nDegree Equivalent Portfolio Projects\n\nDegree equivalent projects are 1-3 months long, results-driven projects that provide evidence you can do the job and are easy for recruiters to understand.\nA non-expert recruiter needs third-party validation that you didn’t copy-paste your projects.\n\n\nPrimary Options\n\nAchieve a high-ranking score in an ML competition.\nContribute to a popular ML open-source project.\nWrite a paper that gets published (this is mostly for transitioning STEM researchers).\n\n\n\nSecondary Options (require more effort for recruiters)\n\nCreate an ML project with real users (ideally, a deployed model with a UI).\nCreate an industry-specific solution with a mentor that provides testimonials.\nCreate ML content with high engagement, such as blogging, podcasts, and videos.\n\n\n\n\nHigh-effort Projects\n\nSmaller ML competitions like niche competitions on Kaggle, Numerai, ML conference competitions, or company competitions are great portfolio projects.\nOpen-source contributions to up-and-coming projects are often the best way to collaborate and get to know people in ML.\n\n\n\n\nOpen-Source Projects\nDescription\n\n\n\n\nFFCV\nFFCV is a drop-in data loading system that dramatically increases data throughput in model training.\n\n\nEleutherAI\nEleutherAI is a grassroots collective of researchers working to open source AI research.\n\n\nHugging Face\nThe AI community building the future.\n\n\nPyTorch Lightning\nScale your PyTorch models, without the boilerplate.\n\n\nLAION\nThe Large-scale Artificial Intelligence Open Network\n\n\nReplicate\nReplicate makes it easy to share your machine learning model.\n\n\ntimm\nPyTorch image models, scripts, pretrained weights\n\n\nSegmentation Models\nSegmentation models with pretrained backbones.\n\n\nOpenAI Gym\nGym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n\n\nAlbumentations\nAlbumentations is a computer vision tool that boosts the performance of deep convolutional neural networks.\n\n\neinops\neinops provides flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, jax, and others.\n\n\nFLAX\nFlax is a neural network library for JAX that is designed for flexibility.\n\n\nfast.ai\nfastai simplifies training fast and accurate neural nets using modern best practices.\n\n\nONNX Runtime\nONNX Runtime is a cross-platform inference and training machine-learning accelerator.\n\n\nBest-of Machine Learning with Python\nA ranked list of awesome machine learning Python libraries. Updated weekly.\n\n\n\n\n\nIndustry Portfolio Projects\n\nPortfolio projects that solve a real problem need someone that vouches for your solution.\nEmail ten to twenty ML engineers at startups you respect and ask them for industry problems with accessible data you can tackle.\nTry people on Twitter with less than 10k followers and a blog.\nML engineers can put you in a good starting point, scope the project, help you when you get stuck, and potentially hire or recommend you later.\nApproach people who post data-related freelance projects on freelancer marketplaces and look at sites that post pro-bono data projects.\n\n\nEmail Example\nTitle: Industry ML problems\nHi Jane,\nI’m self-studying deep learning [Link to github] and I’m looking\nfor problems I can tackle for my portfolio.\nGiven your interesting work on Twitters’s recommendation\nsystem [link to their blog], I thought you could have exposure to\nother unique industry problems.\nI’m thinking of using Twitter’s API to do an NLP analysis to\ndetect the percentage of bots on Twitter. Is that a good entry-\nlevel problem to tackle or can you think of something else?\nCheers,\nBob"
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#talent-projects",
    "href": "posts/no-ml-degree-book-notes/index.html#talent-projects",
    "title": "Notes on No ML Degree Book",
    "section": "Talent Projects",
    "text": "Talent Projects\n\nTalent projects are 1-4 week open, result-driven projects that help you stand out once in an interview.\nTalent projects focus on novelty and result in a demo, blog post, or visual.\nTalent projects indicate a passion for a particular topic, establish personal branding, and help create a developer advocacy skillset.\nTalent projects are hard to execute and introduce more noise for recruiters.\n\n\n\n\nX-factor Projects\nDescription\n\n\n\n\nDIY Self Driving - A Holiday Side Project\nDIY Self Driving - A Holiday Side Project\n\n\nlucidrains GitHub repositories\nHow to turn novel papers into prototypes\n\n\nHow to collect data in the wild and create irl demos\nThe cold start problem: how to build your machine learning portfolio\n\n\n\n\nDeveloper Advocacy Projects\n\nDeveloper advocacy roles focus on engagement through content and can work as a transition into more technical roles.\nExternal excitement is an indicator that you made a unique contribution."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#self-evaluation",
    "href": "posts/no-ml-degree-book-notes/index.html#self-evaluation",
    "title": "Notes on No ML Degree Book",
    "section": "Self Evaluation",
    "text": "Self Evaluation\n\nMetric-based Portfolio Items\n\nPortfolio items need metrics, context, and third-party validation.\nThird-party validation reduces the doubt you made a mistake or made things up.\n\n\nExample Portfolio Item\nA skin cancer classification model with 90% accuracy on\nBenchmark X with a previous SOTA of 85%. Published in\nMachine Learning Conference X as the first author.\n\n\n\nTestimonial-Based Portfolio Items\n\nContributing to popular frameworks demonstrates you understand what they need and understand the framework enough to improve and pass a technical review for your submission.\n\n\nExample Portfolio Item\nA released open-source contribution to PyTorch, the LAMB\noptimizer [link], and a blog post [link].\n\nYou can also request a quote from the framework’s team.\n\"X made a fast and well-documented implementation of\nthe LAMB optimizer in PyTorch.\", Employee X at\nFacebook Meta. [endorsement link], [commit link] and a\nblog post [link]\nPublic endorsements on Twitter, LinkedIn, or GitHub make them verifiable.\n\n\n\n\nProduct Portfolio Items\n\nLive demos allow recruiters to test projects without technical expertise.\nUsing a recent or custom model is more likely to convince cynical recruiters your project is original.\nOpen-ended projects need a cluster of validation to compensate for less interpretable results.\nWhen applicable, deploy the model on a scalable back-end on a large cloud provider and show evidence it supports at least 100 QPS.\n\n\nExample Portfolio Item\nA super-resolution model in production and a live UI. [link]\nOptimized deployment taking the original RAM footprint\nfrom 1 GB to 150 MB, and the CPU inference from 4\nseconds to 30 ms. [Google Colab benchmark link]. 100\nweekly users [Stats screenshot], 250 stars on GitHub [link],\nand seen on Hacker News [link] and recommended by X,\nat Famous company. [link to tweet]"
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#ideas",
    "href": "posts/no-ml-degree-book-notes/index.html#ideas",
    "title": "Notes on No ML Degree Book",
    "section": "Ideas",
    "text": "Ideas\n\nBase Portfolio Ideas\n\nAvoid gut ideas until you have a few years of experience to tell if it’s good.\nGood base portfolio ideas are validated problems and allow you to translate hard work into outcomes with as few risks as possible.\n\n\n\nTalent Project Ideas\n\nIt is better to overdeliver on a tiny real problem than to make a vague attempt on an ambitious one.\nSuccessful talent projects lead to short and clear stories.\nTry ranking a few dozen project areas rather than deciding between a few specific ideas. Ideas that appear in any mainstream channel are likely overused.\n\n\n\nSourcing Ideas\n\nStarting Points\n\nTop ML conferences from 1987 to 2007\nStanford’s CS224n, CS329S & CS231n projects\nFastAI student projects\nTwitter likes and GitHub stars\nCreative projects like AK, ML x Art, NeurIPS gallery\nEdge devices and hardware projects\nKaggle Kernels\nTop 15-40% of papers on Arxiv Sanity\n\nAim for projects made by people early in their careers.\n\n\n\nRanking Ideas\n\nTry to gather at least 20-30 project ideas before ranking them.\nCan you impress a non-technical person in less than 30 seconds?\nCan you find a quick way to run the model?\nDo you have enough computing resources and knowledge for the project?\nIs there an apparent angle to improve the project?\nDoes the project excite you?\nRanking a project often depends on whether you will use the project for your resume, personal marketing, getting a developer advocacy role, or something else.\nNarrow down the list to five projects and pick the most exciting one.\nIf you can’t create a baseline within the first week, move on to something else.\n\n\n\nPromoting Projects\n\nHave something highly visual or a few-click online demo to make it more shareable on social media.\nSpread good vibes on Twitter and have a good reply game to build an audience.\nShare projects that use specific tools/products in their Slack channels and Discord groups.\nAdd models to model platforms like Huggingface Spaces, Replicate, Modelplace, and Runway models.\nLook into using Google’s Keyword tool, other mainstream SEO tools, Google’s trending topics, and Youtube’s Keyword tool to increase search traffic."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#workflow",
    "href": "posts/no-ml-degree-book-notes/index.html#workflow",
    "title": "Notes on No ML Degree Book",
    "section": "Workflow",
    "text": "Workflow\n\nHigh-effort Focus\n\nStarting with a blank page and trying to build your first project requires high-effort focus.\nResource for building high-effort focus\n\nAtomic Habits\nDeep Work\n\nBuilding high-effort focus requires good sleep, exercise, and food routines.\nSleep at least 8 hours, ideally 9 hours, in a quiet, dark, and cool environment.\nExercise at least 20 minutes per day to elevate your heart rate.\nEat healthy food that does not spike your sugar levels.\nUse tiny habits to gradually increase your capacity for high-effort focus to 1-3 hours per day.\n\n\n\nLearning Schedule\n\n8:00am-2:00pm: High-effort focus (scoping, coding, major refactoring)\n2:00pm-6:00pm: Low-effort focus (light debugging and simple refactoring)\n6:00pm-10:00pm: Mid-effort focus (learning gaps + skimming)\nTry taking long breaks during lunch for exercise and leisure to reenergize for another high-effort session."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#job-hunting",
    "href": "posts/no-ml-degree-book-notes/index.html#job-hunting",
    "title": "Notes on No ML Degree Book",
    "section": "Job Hunting",
    "text": "Job Hunting\n\nThe ideal hiring process for self-learners is specialized, practical, or small-scale.\n\nThis hiring process is more prevalent with smaller organizations, startups, companies with specific cultures, or specialized teams within larger organizations.\nHiring managers are technical, and questions cater to each candidate’s work are reflect skills for the job.\n\n\n\nHigh-growth Startups and Small Organizations\n\nSmaller companies that have technical hiring managers, technical founders, and few applicants are good choices for self-learners.\nThese companies need people who can add value on day one.\nThe hiring process can vary more between small companies, and you often need to do more adjacent work related to ML.\nPlaces to look for startups\n\nProduct Hunt\nY Combinator’s list of startups\nAngel list\nAsk HN: Who is hiring? monthly threads\nremote jobs\nlocal incubators and offices\nportfolios of angel investors\n\nReach out to your professional, social, and personal networks.\nResearch institutions often need people who can do the more engineering-heavy side of their ML research.\n\n\n\nMidsized and Large Companies\n\nSearch for no-degree graduates on LinkedIn and see where they work or ask them directly.\nLook for companies that actively look for non-degree candidates.\nAttract companies with online and social media presence.\nUse interview-as-a-service companies like TripleByte.\nBrowse Glassdoor and look for companies with practical interviews.\n\n\n\nResume\n\nAim for half a page with essential contact information, tech jobs, and one or two bullet points with your most impressive ML projects.\nDon’t refer to yourself as an ML enthusiast, add jargon, or make the resume more than one page.\n\n\n\nEmail Templates for Contacting Startups\n\nEmail the company’s founder and CEO and send two follow-up emails one week apart if they don’t reply.\nEmail companies regardless of whether they have open positions.\n\n\nEmail Template\nTitle: Entry-level ML positions\nHi John,\nI hope you’ve had an excellent week so far!\nI first saw your product on Product Hunt. I loved the user\ninterface, and I was impressed by the quality of the generative\nmodel. I’m currently looking for an entry-level ML position.\nI’ve made open source contributions to PyTorch and ranked in\nthe top 5% in a popular image segmentation competition on\nKaggle. You can find more details in my portfolio [github] and\n[linkedin] here.\nIf you have any opportunities at [company] or know anyone else\nhiring, please let me know.\nCheers,\nJane\n\n\n\nInterview Prep\n\nPractice describing yourself concisely in 30 seconds and giving a 30 seconds overview of each critical project.\nDo some of the easy LeetCode questions in Python and practice mock interviews with friends.\nInvest a few hours researching the problems companies you like are working on and get data on how they are likely to approach them.\n\nHave specific questions and the ability to discuss their problems in detail.\n\n\n\n\nPlan B\n\nApply for software roles closely related to machine learning.\nApply for software roles in companies that do a lot of ML.\nApply for developer advocacy and content marketing roles in ML companies.\nApply for product manager and analytic roles related to ML.\nBid on ML contracting opportunities or software projects related to ML."
  },
  {
    "objectID": "posts/no-ml-degree-book-notes/index.html#references",
    "href": "posts/no-ml-degree-book-notes/index.html#references",
    "title": "Notes on No ML Degree Book",
    "section": "References",
    "text": "References\n\nNo ML Degree: How to Land Your First Machine Learning Job Without a Degree"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nDownload OpenCV\nCreate DLL Project\nConfigure the Project\nAdd Include Directories\nLink Libraries\nPost Build Events\nInstall ONNX Runtime\nUpdate Precompiled Header File\nUpdate dllmain File\nBuild Solution\nGather Dependencies\nSummary"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#tutorial-links",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#tutorial-links",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nTraining Tutorial: Train a hand gesture classifier using fastai and export it to TensorFlow.js.\nPart 1: Create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using ONNX Runtime and DirectML.\nPart 2: Perform object detection in a Unity project with ONNX Runtime and DirectML.\nGitHub Repository"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#introduction",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nIn this two-part tutorial series, I will show you how to implement real-time object detection in Unity using ONNX Runtime and DirectML. ONNX Runtime is a cross-platform model accelerator that works with several hardware acceleration libraries. DirectML is a hardware-accelerated DirectX 12 library for machine learning on Windows.\nIn Part 1, we will create a dynamic link library (DLL) file in Visual Studio to perform object detection with ONNX Runtime and DirectML. In Part 2, we will integrate this DLL file into a Unity project and perform real-time object detection. By the end of this tutorial series, you will have the skills and knowledge to leverage ONNX Runtime and DirectML in your Unity projects.\nThis tutorial uses an ONNX model exported in a previous tutorial series, so no modifications to the training code are required.\n\nModel Training Tutorial: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1\n\n\nUnity Demo\n\n\nVideo"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#overview",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#overview",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of this tutorial series, we will create a dynamic link library (DLL) file in Visual Studio to perform object detection using ONNX Runtime and DirectML. We will first download OpenCV, then create a DLL project and configure it. Next, we will include dependency directories and link libraries and set up post-build events. After that, we will install ONNX Runtime and update the precompiled header and dllmain files. Finally, we will build the project to generate the DLL file. By the end of this post, you will have a DLL file ready to be integrated into a Unity project.\n\nImportant: This post assumes Visual Studio is present on your system."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#download-opencv",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#download-opencv",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Download OpenCV",
    "text": "Download OpenCV\nWe’ll use the OpenCV library to process image data from Unity. The tutorial uses OpenCV 4.6.0, which is available at the link below.\n\nOpenCV 4.6.0 GitHub\n\nSelect the opencv-4.6.0-vc14_vc15.exe option from the Assets list.\n\n\n\n\n\nRun the executable once it finishes downloading. You might get a warning from Windows that the executable is an unrecognized app. We can bypass this by clicking the More info text, then the Run anyway button.\n\n\n\n\n\nThen, click the Run anyway button.\n\n\n\n\n\nThe executable will prompt us to select a location to extract the opencv folder. We’ll need to give Visual Studio this location to access the library’s functionality. I tend to place my C++ dependencies in a dedicated folder for consistency.\n\n\n\n\n\nIf we open the opencv folder, we can see a build folder and a source folder. Everything we need is in the build folder."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#create-dll-project",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#create-dll-project",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Create DLL Project",
    "text": "Create DLL Project\nOpen Visual Studio and select the Create a new project option.\n\n\n\n\n\nType DLL into the text box and select the Dynamic-Link Library (DLL) option. This option automatically configures a few parameters for us compared to starting with a standard console application.\n\n\n\n\n\nChoose a name and location for the project and click the Create button. By default, the DLL file will use the project name."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#configure-the-project",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#configure-the-project",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Configure the Project",
    "text": "Configure the Project\nAt the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#add-include-directories",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#add-include-directories",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nWe need to tell Visual Studio where OpenCV is so we can access its APIs. Right-click the project name in the Solution Explorer panel.\n\n\n\n\n\nSelect the Properties option in the popup menu.\n\n\n\n\n\nIn the Properties Window, open on the C/C++ dropdown. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nAdd the path for the following folder, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenCV library, and click OK.\n\n&lt;parent-folder-path&gt;\\opencv\\build\\include"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#link-libraries",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#link-libraries",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select Input. Select Additional Dependencies and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the following path, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenCV library, and click OK.\n\n&lt;parent-folder-path&gt;\\opencv\\build\\x64\\vc15\\lib\\*"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#post-build-events",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#post-build-events",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Post Build Events",
    "text": "Post Build Events\nOur DLL file will depend on the following DLL file included with OpenCV.\nOpenCV DLL files\n\n\n\n\n\nWe can add a post-build event in Visual Studio to automatically copy these DLL files to the build folder for the project at compile time. Open the Build Events dropdown in the Properties window and select Post-Build Event. Select Command Line and click &lt;Edit..&gt;.\n\n\n\n\n\nAdd the following commands, replacing &lt;parent-folder-path&gt; with the full path to the parent folder for the OpenCV library, and click OK.\n\nxcopy &lt;parent-folder-path&gt;\\opencv\\build\\x64\\vc15\\bin\\opencv_world452.dll $(SolutionDir)$(Platform)\\$(Configuration)\\ /c /y\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#install-onnx-runtime",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#install-onnx-runtime",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Install ONNX Runtime",
    "text": "Install ONNX Runtime\nWe can install ONNX Runtime with DirectML directly into our project via the NuGet package manager integrated with Visual Studio. Open the Project menu and select Manage NuGet Packages... from the dropdown menu.\n\n\n\n\n\nSelect the Browse option and enter Microsoft.ML.OnnxRuntime.DirectML into the search box.\n\n\n\n\n\nWith the package selected, click the Install button. The project uses version 1.12.1.\n\n\n\n\n\nClick OK in the popup window.\n\n\n\n\n\nWith the dependencies taken care of, we can start modifying the code."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#update-precompiled-header-file",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#update-precompiled-header-file",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nWe’ll first update the pch.h Precompiled Header file with the required header files. We can open the pch.h file by selecting it in the Solution Explorer window.\n\n\n\n\n\nComment or remove the #include line for the framework.h header file.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n\n#endif //PCH_H\nAdd required header files\nNext, we’ll add the required header files for ONNX Runtime and OpenCV below //#include \"framework.h\" line.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n//#include \"framework.h\"\n#include &lt;onnxruntime_cxx_api.h&gt;\n#include \"dml_provider_factory.h\"\n#include &lt;opencv2/opencv.hpp&gt;\n\n#endif //PCH_H"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#update-dllmain-file",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#update-dllmain-file",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Update dllmain File",
    "text": "Update dllmain File\nBy default, the dllmain.cpp file contains the following code.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\nCreate a macro to mark functions we want to make accessible in Unity\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWrap the code in extern “C” to prevent name-mangling issues with the compiler\nThe rest of our code will go inside here.\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n\n}\nDefine variables\nInside the wrapper, we will declare the persistent variables needed for the DLL.\n\nOrtApi: ONNX Runtime API interface\nOrtEnv: Holds the logging state for the ONNX Runtime objects\nOrtSessionOptions: Holds the options used when creating a new ONNX Runtime session\nOrtSession: The ONNX Runtime session\n\n\n// The current source image width\nint img_w;\n// The current source image height\nint img_h;\n// The current model input width\nint input_w;\n// The current model input height\nint input_h;\n// The total number pixels in the input image\nint n_pixels;\n// The number of color channels \nint n_channels = 3;\n\n// Stores information about a single object prediction\nstruct Object\n{\n    float x0;\n    float y0;\n    float width;\n    float height;\n    int label;\n    float prob;\n};\n\n// Store grid offset and stride values to decode a section of the model output\nstruct GridAndStride\n{\n    int grid0;\n    int grid1;\n    int stride;\n};\n\n// The scale values used to adjust the model output to the source image resolution\nfloat scale_x;\nfloat scale_y;\n\n// The minimum confidence score to consider an object proposal\nfloat bbox_conf_thresh = 0.3;\n// The maximum intersection over union value before an object proposal will be ignored\nfloat nms_thresh = 0.45;\n\n// Stores the grid and stride values to navigate the raw model output\nstd::vector&lt;GridAndStride&gt; grid_strides;\n// Stores the object proposals with confidence scores above bbox_conf_thresh\nstd::vector&lt;Object&gt; proposals;\n// Stores the indices for the object proposals selected using non-maximum suppression\nstd::vector&lt;int&gt; proposal_indices;\n\n// The stride values used to generate the gride_strides vector\nstd::vector&lt;int&gt; strides = { 8, 16, 32 };\n\n// The mean of the ImageNet dataset used to train the model\nconst float mean[] = { 0.485f, 0.456f, 0.406f };\n// The standard deviation of the ImageNet dataset used to train the model\nconst float std_dev[] = { 0.229f, 0.224f, 0.225f };\n\n// ONNX Runtime API interface\nconst OrtApi* ort = NULL;\n\n// List of available execution providers\nchar** provider_names;\nint provider_count;\n\n// Holds the logging state for the ONNX Runtime objects\nOrtEnv* env;\n// Holds the options used when creating a new ONNX Runtime session\nOrtSessionOptions* session_options;\n// The ONNX Runtime session\nOrtSession* session;\n\n// The name of the model input\nchar* input_name;\n// The name of the model output\nchar* output_name;\n\n// A pointer to the raw input data\nfloat* input_data;\n// The memory size of the raw input data\nint input_size;\nDefine a function to convert char data to wchar_t\nONNX Runtime expects paths to ONNX model files to be in wchar_t format. We receive the file paths in char format, so we’ll make a function to convert them.\n/// &lt;summary&gt;\n/// Convert char data to wchar_t\n/// &lt;/summary&gt;\n/// &lt;param name=\"text\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nstatic wchar_t* charToWChar(const char* text)\n{\n    const size_t size = strlen(text) + 1;\n    wchar_t* wText = new wchar_t[size];\n    size_t converted_chars;\n    mbstowcs_s(&converted_chars, wText, size, text, _TRUNCATE);\n    return wText;\n}\nDefine a function to initialize the interface to the ONNX Runtime API\n\nOrtGetApiBase: The Onnxruntime library’s entry point to access the C API.\nGetAvailableProviders: Get the names of all available providers.\n\n\n/// &lt;summary&gt;\n/// Initialize the ONNX Runtime API interface and get the available execution providers\n/// &lt;/summary&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nDLLExport void InitOrtAPI() {\n\n    ort = OrtGetApiBase()-&gt;GetApi(ORT_API_VERSION);\n\n    ort-&gt;GetAvailableProviders(&provider_names, &provider_count);\n}\nDefine a function to get the number of execution providers\nThe next function we’ll define will create a list of available execution providers and return the number of providers accessible by ONNX Runtime. We’ll use this information to select which device to use to perform inference from the Unity application.\n/// &lt;summary&gt;\n/// Get the number of available execution providers\n/// &lt;/summary&gt;\n/// &lt;returns&gt;The number of available devices&lt;/returns&gt;\nDLLExport int GetProviderCount()\n{\n    // Return the number of available execution providers\n    return provider_count;\n}\nDefine a function to get the name of an execution provider\nNext, we’ll define a function to return the name at a specified index in the list of available execution providers.\n/// &lt;summary&gt;\n/// Get the name of the execution provider at the specified index\n/// &lt;/summary&gt;\n/// &lt;param name=\"index\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;The name of the execution provider at the specified index&lt;/returns&gt;\nDLLExport char* GetProviderName(int index) {\n    return provider_names[index];\n}\nDefine method to generate stride values to navigate the raw model output\nThe method for generating the offset values used to traverse the output array is almost identical to the Python implementation from the training tutorial.\n/// &lt;summary&gt;\n/// Generate offset values to navigate the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"height\"&gt;The model input height&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;The model input width&lt;/param&gt;\nvoid GenerateGridsAndStride(int height, int width)\n{\n    // Remove the values for the previous input resolution\n    grid_strides.clear();\n\n    // Iterate through each stride value\n    for (auto stride : strides)\n    {\n        // Calculate the grid dimensions\n        int grid_height = height / stride;\n        int grid_width = width / stride;\n\n        // Store each combination of grid coordinates\n        for (int g1 = 0; g1 &lt; grid_height; g1++)\n        {\n            for (int g0 = 0; g0 &lt; grid_width; g0++)\n            {\n                grid_strides.push_back(GridAndStride{ g0, g1, stride });\n            }\n        }\n    }\n}\nDefine a function to set the minimum confidence score from Unity\nWe might want to try different confidence thresholds for keeping object proposals from the Unity application, so we’ll add a function to enable this.\n/// &lt;summary&gt;\n/// Set minimum confidence score for keeping bounding box proposals\n/// &lt;/summary&gt;\n/// &lt;param name=\"min_confidence\"&gt;The minimum confidence score for keeping bounding box proposals&lt;/param&gt;\nDLLExport void SetConfidenceThreshold(float min_confidence)\n{\n    bbox_conf_thresh = min_confidence;\n}\nDefine function to refresh memory when switching models or execution providers\n\nReleaseSession\nReleaseEnv\n\n\n/// &lt;summary&gt;\n/// Refresh memory when switching models or execution providers\n/// &lt;/summary&gt;\nDLLExport void RefreshMemory() {\n    if (input_data) free(input_data);\n    if (session) ort-&gt;ReleaseSession(session);\n    if (env) ort-&gt;ReleaseEnv(env);\n}\nDefine a function to load an ONNX model\n\nCreateEnv: Create an OrtEnv.\nDisableTelemetryEvents: Disable telemetry.\nCreateSessionOptions: Create an OrtSessionOptions object.\nDisableMemPattern: Disable the memory pattern optimization.\nSetSessionExecutionMode: Controls whether you want to execute operators in your graph sequentially or in parallel.\nOrtSessionOptionsAppendExecutionProvider_DML: Add a DirectML execution provider.\nCreateSession: Create an OrtSession from a model file.\nReleaseSessionOptions: Free an OrtSessionOptions object.\nAllocatorWithDefaultOptions: Create the default allocator.\nSessionGetInputName: Get input name.\nSessionGetOutputName: Get output name.\n\n\n/// &lt;summary&gt;\n/// Load a model from the specified file path\n/// &lt;/summary&gt;\n/// &lt;param name=\"model_path\"&gt;The full model path to the ONNX model&lt;/param&gt;\n/// &lt;param name=\"execution_provider\"&gt;The name for the desired execution_provider&lt;/param&gt;\n/// &lt;param name=\"image_dims\"&gt;The source image dimensions&lt;/param&gt;\n/// &lt;returns&gt;A status value indicating success or failure to load and reshape the model&lt;/returns&gt;\nDLLExport int LoadModel(char* model_path, char* execution_provider, int image_dims[2])\n{\n    int return_val = 0;\n\n    // Initialize the ONNX runtime environment\n    std::string instance_name = \"yolox-inference\";\n    ort-&gt;CreateEnv(ORT_LOGGING_LEVEL_WARNING, instance_name.c_str(), &env);\n\n    // Disable telemetry\n    ort-&gt;DisableTelemetryEvents(env);\n\n    // Add the selected execution provider\n    ort-&gt;CreateSessionOptions(&session_options);\n    std::string provider_name = execution_provider;\n    \n    // Add the specified execution provider\n    if (provider_name.find(\"CPU\") != std::string::npos) {\n        return_val = 1;\n    }\n    else if (provider_name.find(\"Dml\") != std::string::npos) {\n        ort-&gt;DisableMemPattern(session_options);\n        ort-&gt;SetSessionExecutionMode(session_options, ExecutionMode::ORT_SEQUENTIAL);\n        OrtSessionOptionsAppendExecutionProvider_DML(session_options, 0);\n    }\n    else return_val = 1;\n\n    // Create a new inference session\n    ort-&gt;CreateSession(env, charToWChar(model_path), session_options, &session);\n    ort-&gt;ReleaseSessionOptions(session_options);\n\n    Ort::AllocatorWithDefaultOptions allocator;\n\n    // Get input and output names\n    ort-&gt;SessionGetInputName(session, 0, allocator, &input_name);\n    ort-&gt;SessionGetOutputName(session, 0, allocator, &output_name);\n\n    // The dimensions of the source input image\n    img_w = image_dims[0];\n    img_h = image_dims[1];\n    // Calculate new input dimensions based on the max stride value\n    input_w = (int)(strides.back() * std::roundf(img_w / strides.back()));\n    input_h = (int)(strides.back() * std::roundf(img_h / strides.back()));\n    n_pixels = input_w * input_h;\n\n    // Calculate the value used to adjust the model output to the source image resolution\n    scale_x = input_w / (img_w * 1.0);\n    scale_y = input_h / (img_h * 1.0);\n\n    // Generate the grid and stride values based on input resolution\n    GenerateGridsAndStride(input_h, input_w);\n\n    // Replace the initial input dims with the updated values\n    image_dims[0] = input_w;\n    image_dims[1] = input_h;\n\n    // Allocate memory for the raw input data\n    input_size = n_pixels * n_channels * (int)sizeof(float);\n    input_data = (float*)malloc((size_t)input_size * sizeof(float*));\n    if (input_data != NULL) memset(input_data, 0, input_size);\n\n    // Return a value of 0 if the model loads successfully\n    return return_val;\n}\nDefine method to generate object detection proposals from the raw model output\nThe method to generate object proposals is nearly identical to the Python implementation from the training tutorial.\n/// &lt;summary&gt;\n/// Generate object detection proposals from the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"out_ptr\"&gt;A pointer to the output tensor data&lt;/param&gt;\nvoid GenerateYoloxProposals(float* out_ptr, int proposal_length)\n{\n    // Remove the proposals for the previous model output\n    proposals.clear();\n\n    // Obtain the number of classes the model was trained to detect\n    int num_classes = proposal_length - 5;\n\n    for (int anchor_idx = 0; anchor_idx &lt; grid_strides.size(); anchor_idx++)\n    {\n        // Get the current grid and stride values\n        int grid0 = grid_strides[anchor_idx].grid0;\n        int grid1 = grid_strides[anchor_idx].grid1;\n        int stride = grid_strides[anchor_idx].stride;\n\n        // Get the starting index for the current proposal\n        int start_idx = anchor_idx * proposal_length;\n\n        // Get the coordinates for the center of the predicted bounding box\n        float x_center = (out_ptr[start_idx + 0] + grid0) * stride;\n        float y_center = (out_ptr[start_idx + 1] + grid1) * stride;\n\n        // Get the dimensions for the predicted bounding box\n        float w = exp(out_ptr[start_idx + 2]) * stride;\n        float h = exp(out_ptr[start_idx + 3]) * stride;\n\n        // Calculate the coordinates for the upper left corner of the bounding box\n        float x0 = x_center - w * 0.5f;\n        float y0 = y_center - h * 0.5f;\n\n        // Get the confidence score that an object is present\n        float box_objectness = out_ptr[start_idx + 4];\n\n        // Initialize object struct with bounding box information\n        Object obj = { x0, y0, w, h, 0, 0 };\n\n        // Find the object class with the highest confidence score\n        for (int class_idx = 0; class_idx &lt; num_classes; class_idx++)\n        {\n            // Get the confidence score for the current object class\n            float box_cls_score = out_ptr[start_idx + 5 + class_idx];\n            // Calculate the final confidence score for the object proposal\n            float box_prob = box_objectness * box_cls_score;\n\n            // Check for the highest confidence score\n            if (box_prob &gt; obj.prob)\n            {\n                obj.label = class_idx;\n                obj.prob = box_prob;\n            }\n        }\n\n        // Only add object proposals with high enough confidence scores\n        if (obj.prob &gt; bbox_conf_thresh) proposals.push_back(obj);\n    }\n\n    // Sort the proposals based on the confidence score in descending order\n    std::sort(proposals.begin(), proposals.end(), [](Object& a, Object& b) -&gt; bool\n              { return a.prob &gt; b.prob; });\n}\nDefine function to sort bounding box proposals using Non-Maximum Suppression\nThe C++ API for OpenCV has built-in functionality to perform comparison operations between rectangles. Therefore, we don’t need to define helper functions to calculate the intersection and union areas of two bounding boxes. Otherwise, the method to sort bounding box proposals using Non-Maximum Suppression is almost identical to the Python implementation from part 1.\n/// &lt;summary&gt;\n/// Filter through a sorted list of object proposals using Non-maximum suppression\n/// &lt;/summary&gt;\nvoid NmsSortedBboxes()\n{\n    // Remove the picked proposals for the previous model outptut\n    proposal_indices.clear();\n\n    // Iterate through the object proposals\n    for (int i = 0; i &lt; proposals.size(); i++)\n    {\n        Object& a = proposals[i];\n\n        // Create OpenCV rectangle for the Object bounding box\n        cv::Rect_&lt;float&gt; rect_a = cv::Rect_&lt;float&gt;(a.x0, a.y0, a.width, a.height);\n\n        bool keep = true;\n\n        // Check if the current object proposal overlaps any selected objects too much\n        for (int j : proposal_indices)\n        {\n            Object& b = proposals[j];\n\n            // Create OpenCV rectangle for the Object bounding box\n            cv::Rect_&lt;float&gt; rect_b = cv::Rect_&lt;float&gt;(b.x0, b.y0, b.width, b.height);\n\n            // Calculate the area where the two object bounding boxes overlap\n            float inter_area = (rect_a & rect_b).area();\n            // Calculate the union area of both bounding boxes\n            float union_area = rect_a.area() + rect_b.area() - inter_area;\n            // Ignore object proposals that overlap selected objects too much\n            if (inter_area / union_area &gt; nms_thresh)\n                keep = false;\n        }\n\n        // Keep object proposals that do not overlap selected objects too much\n        if (keep) proposal_indices.push_back(i);\n    }\n}\nDefine a function to perform inference\nWe will access the pixel data for the input image from Unity with a pointer to a uchar (unsigned 1-byte integer) array and wrap the data in a cv::Mat variable for processing.\nAfter processing the model output, we’ll return the final number of detected objects to Unity so we can initialize an array of objects.\n\ncv::Mat: n-dimensional dense array class\n\ncv::cvtColor(): Converts an image from one color space to another\nOrtMemoryInfo:\nCreateCpuMemoryInfo: Create an OrtMemoryInfo for CPU memory.\nOrtValue\nCreateTensorWithDataAsOrtValue: Create a tensor backed by a user supplied buffer.\nReleaseMemoryInfo\nRun: Run the model in an OrtSession.\nOrtTensorTypeAndShapeInfo\nGetTensorTypeAndShape: Get type and shape information from a tensor OrtValue.\nGetDimensionsCount: Get dimension count in OrtTensorTypeAndShapeInfo.\nGetDimensions: Get dimensions in OrtTensorTypeAndShapeInfo.\nGetTensorMutableData: Get a pointer to the raw data inside a tensor.\n\n\n/// &lt;summary&gt;\n/// Perform inference with the provided texture data\n/// &lt;/summary&gt;\n/// &lt;param name=\"image_data\"&gt;The source image data from Unity&lt;/param&gt;\n/// &lt;returns&gt;The final number of detected objects&lt;/returns&gt;\nDLLExport int PerformInference(uchar* image_data)\n{\n    // Store the pixel data for the source input image in an OpenCV Mat\n    cv::Mat input_image = cv::Mat(img_h, img_w, CV_8UC4, image_data);\n    // Remove the alpha channel\n    cv::cvtColor(input_image, input_image, cv::COLOR_RGBA2RGB);\n    // Resize the image to the model input dimensions\n    cv::resize(input_image, input_image, cv::Size(input_w, input_h));\n\n    // Iterate over each pixel in image\n    for (int p = 0; p &lt; n_pixels; p++)\n    {\n        for (int ch = 0; ch &lt; n_channels; ch++) {\n            // Scale and normalize each value\n            input_data[ch * n_pixels + p] = ((input_image.data[p * n_channels + ch] / 255.0f) - mean[ch]) / std_dev[ch];\n        }\n    }\n\n    // Initialize list of input and output names\n    const char* input_names[] = { input_name };\n    const char* output_names[] = { output_name };\n    // Initialize the list of model input dimension\n    int64_t input_shape[] = { 1, 3, input_h, input_w };\n    int input_shape_len = sizeof(input_shape) / sizeof(input_shape[0]);\n\n    // Initialize an input tensor object with the input_data\n    OrtMemoryInfo* memory_info;\n    ort-&gt;CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info);\n\n    OrtValue* input_tensor = NULL;\n    ort-&gt;CreateTensorWithDataAsOrtValue(memory_info, input_data, input_size, input_shape,\n                                        input_shape_len, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,\n                                        &input_tensor);\n\n    ort-&gt;ReleaseMemoryInfo(memory_info);\n\n\n    OrtValue* output_tensor = NULL;\n    // Perform inference\n    ort-&gt;Run(session, NULL, input_names, (const OrtValue* const*)&input_tensor, 1, output_names, 1,\n             &output_tensor);\n\n    // Make sure the output tensor is not NULL to avoid potential crashes\n    if (output_tensor == NULL) {\n        ort-&gt;ReleaseValue(input_tensor);\n        ort-&gt;ReleaseValue(output_tensor);\n        return -1;\n    }\n\n    // Get the length of a single object proposal (i.e., number of object classes + 5)\n    OrtTensorTypeAndShapeInfo* output_tensor_info;\n    ort-&gt;GetTensorTypeAndShape(output_tensor, &output_tensor_info);\n    size_t output_length[1] = {};\n    ort-&gt;GetDimensionsCount(output_tensor_info, output_length);\n    int64_t output_dims[3] = {};\n    ort-&gt;GetDimensions(output_tensor_info, output_dims, *output_length);\n\n    // Access model output\n    float* out_data;\n    ort-&gt;GetTensorMutableData(output_tensor, (void**)&out_data);\n\n    // Generate new proposals for the current model output\n    GenerateYoloxProposals(out_data, output_dims[2]);\n\n    // Pick detected objects to keep using Non-maximum Suppression\n    NmsSortedBboxes();\n\n    // Free memory for input and output tensors\n    ort-&gt;ReleaseValue(input_tensor);\n    ort-&gt;ReleaseValue(output_tensor);\n\n    // return the final number of detected objects\n    return (int)proposal_indices.size();\n}\nDefine a function to populate an array of objects from Unity\nNext, we’ll define a function to populate an array of objects from Unity. We call this function after initializing the list based on the current number of detected objects. We’ll also scale the bounding box information from the input dimensions to the source image resolution.\n/// &lt;summary&gt;\n/// Fill the provided array with the detected objects\n/// &lt;/summary&gt;\n/// &lt;param name=\"objects\"&gt;A pointer to a list of objects from Unity&lt;/param&gt;\nDLLExport void PopulateObjectsArray(Object* objects) \n{\n\n    for (int i = 0; i &lt; proposal_indices.size(); i++)\n    {\n        Object obj = proposals[proposal_indices[i]];\n\n        // Adjust offset to source image resolution and clamp the bounding box\n        objects[i].x0 = std::min(obj.x0 / scale_x, (float)img_w);\n        objects[i].y0 = std::min(obj.y0 / scale_y, (float)img_h);\n        objects[i].width = std::min(obj.width / scale_x, (float)img_w);\n        objects[i].height = std::min(obj.height / scale_y, (float)img_h);\n\n        objects[i].label = obj.label;\n        objects[i].prob = obj.prob;\n    }\n}\nDefine a function to clear memory when the Unity application exits\nThis last function will free the memory allocated by the plugin. We’ll call it when the Unity application shuts down.\n/// &lt;summary&gt;\n/// Free memory\n/// &lt;/summary&gt;\nDLLExport void FreeResources()\n{\n    grid_strides.clear();\n    proposals.clear();\n    proposal_indices.clear();\n\n    free(input_data);\n    ort-&gt;ReleaseSession(session);\n    ort-&gt;ReleaseEnv(env);\n}\nThat is all the code needed for the plugin. We can now build the solution to generate the DLL file."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#build-solution",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#build-solution",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Build Solution",
    "text": "Build Solution\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. Visual Studio will generate a new x64 folder in the project directory containing the DLL file and its dependencies."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#gather-dependencies",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#gather-dependencies",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nRight-click the project name in the Solution Explorer panel and select Open Folder in File Explorer from the popup menu.\n\n\n\n\n\nIn the new File Explorer window, go to the parent folder.\n\n\n\n\n\nOpen the x64 → Release subfolder.\n\n\n\n\n\nWe’ll need to copy all the DLL files in this folder to the Unity project."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-1/index.html#summary",
    "href": "posts/onnx-directml-unity-tutorial/part-1/index.html#summary",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1",
    "section": "Summary",
    "text": "Summary\nIn this post, we created a dynamic link library (DLL) file in Visual Studio to perform object detection using ONNX Runtime and DirectML. We started by downloading OpenCV and creating a DLL project. Then, we configured the project by adding include directories, linking libraries, and setting up post-build events. After, we installed ONNX Runtime and updated the precompiled header and dllmain files. Finally, we built the project to generate the DLL file. In Part 2, we will cover how to integrate this DLL file into a Unity project and perform real-time object detection.\nTraining Tutorial: End-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1\nNext: Object Detection for Unity With ONNX Runtime and DirectML Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "",
    "text": "Overview\nCreate New Project\nImport Assets\nAllow Unsafe Code\nCreate Processing Shader\nCreate Object Detector Script\nSet up Unity Scene\nTest in Editor\nSummary"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#tutorial-links",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#tutorial-links",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nTraining Tutorial: Train a hand gesture classifier using fastai and export it to TensorFlow.js.\nPart 1: Create a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using ONNX Runtime and DirectML.\nPart 2: Perform object detection in a Unity project with ONNX Runtime and DirectML.\nGitHub Repository"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#overview",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#overview",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Overview",
    "text": "Overview\nIn Part 2 of this tutorial series, we will integrate our DLL file into a Unity project to perform real-time object detection. We will start by creating a new Unity project and importing the necessary assets. Then, we will allow unsafe code in the project to share input data with the DLL. Next, we will create a processing shader and an object detector script to handle object detection in our Unity project. Finally, we will set up the Unity scene and test the object detection model in the Unity editor. By the end of this post, you will have a working real-time object detection system in Unity for Windows.\n\nImportant: This post assumes you already have Unity Hub on your system. Check out this section from a previous tutorial if this is not the case (link)."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-new-project",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-new-project",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Create New Project",
    "text": "Create New Project\nOpen the Unity Hub and click New Project.\n\n\n\n\n\nSelect the target editor version from the Editor Version dropdown menu. We’ll use Unity 2022 for this post, but the current LTS release should also work fine.\n\n\n\n\n\nSelect the 2D Core template.\n\n\n\n\n\nPick a name for the project and a location for the project folder before clicking Create Project in the lower right-hand corner."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#import-assets",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#import-assets",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Import Assets",
    "text": "Import Assets\nOnce the project loads, we’ll store the DLL files from part 2 in a new folder called Plugins. Right-click a space in the Assets section and select Create → Folder from the popup menu.\n\n\n\n\n\nThe DLL targets 64-bit x86 architectures, so we need to place the DLL files in a subfolder named x86_64.\n\nPlugins Folder Google Drive\n\n\n\n\n\n\n\nNote: You can place the Plugins folder inside another folder if needed.\n\nCopy all the DLL files into the Assets/Plugins/x86_64 folder. We then need to close and reopen the Unity Editor to load the plugin files.\n\n\n\n\n\nAfter restarting the Unity Editor, create a new folder called Colormaps to store the JSON file from the training tutorial.\n\nColormaps Folder Google Drive\n\n\n\n\n\n\nWe’ll place any test images into a new folder called Images.\n\nImages Folder Google Drive\n\n\n\n\n\n\nNext, we’ll create a folder to store the ONNX models. We need to place the .onnx files in a StreamingAssets folder to include them in project builds. Create a new folder named StreamingAssets. We’ll place each model file in a separate folder and put those in a new subfolder called ONNXModels to keep things organized.\n\nONNXModels Folder Google Drive"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#allow-unsafe-code",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#allow-unsafe-code",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Allow Unsafe Code",
    "text": "Allow Unsafe Code\nRather than copying the input image from Unity to the plugin, we’ll pass a pointer to the pixel data. First, we need to allow unsafe code for the Unity project. Select Edit → Project Settings... from the top menu.\n\n\n\n\n\nOpen the Player → Other Settings dropdown and scroll down to the Allow 'unsafe' Code checkbox. Enable the setting and close the Project Settings window.\n\n\n\n\n\nNow we can start coding."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-processing-shader",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-processing-shader",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Create Processing Shader",
    "text": "Create Processing Shader\nThe input image gets flipped upside down when we send it to the plugin. We can pre-flip the image in a Compute Shader. We’ll add the Compute Shader in a new folder called Shaders. Right-click a space in the Shaders folder and select Create → Shader → Compute Shader.\n\n\n\n\n\nName the Compute Shader ProcessingShader and open it in the code editor.\nDefault Compute Shader Code\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\nWe need to add a new Texture2D variable to store the pixel data for the input image. We’ll remove the default method and create a new one called FlipXAxis. Replace the default method name in the #pragma kernel line at the top.\nWe need the input image height for the flip operation, which we can access with the Texture2D::GetDimensions function.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel FlipXAxis\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Flip the image around the x-axis\n[numthreads(8, 8, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Stores the InputImage width\n    uint width;\n    // Stores the InputImage height\n    uint height;\n    // Get the dimensions of the InputImage\n    InputImage.GetDimensions(width, height);\n\n    // Update the y value for the pixel coordinates\n    int2 coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-object-detector-script",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#create-object-detector-script",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Create Object Detector Script",
    "text": "Create Object Detector Script\nWe’ll store the C# script that interacts with the plugin in a new Scripts folder. Right-click a space inside it and select Create → C# Script.\n\n\n\n\n\nName the script ObjectDetector and open it in the code editor.\n\n\n\n\n\nDefault script code\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class ObjectDetector : MonoBehaviour\n{\n    // Start is called before the first frame update\n    void Start()\n    {\n        \n    }\n\n    // Update is called once per frame\n    void Update()\n    {\n        \n    }\n}\nAdd required namespaces\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\nSystem.Runtime.InteropServices: Provides a wide variety of members that support COM interop and platform invoke services.\nSystem.IO: Allows reading and writing to files and data streams.\n\n\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Rendering;\nusing System;\nusing UnityEngine.UI;\nusing System.Runtime.InteropServices;\nusing System.IO;\nAdd code to copy DirectML.dll file to editor folder\nWe must copy the DirectML.dll file from the Plugins folder to the parent folder for the Unity Editor application to use DirectML in the Editor. We’ll also need to copy that file to the build folder when building the Unity project. We can handle both steps automatically in code.\nWe can obtain the path to the current Unity Editor from the EditorApplication.applicationpath variable.\nUnity provides an InitializeOnLoad attribute to run code in the Unity Editor without requiring action from the user. This attribute requires the UnityEditor namespace. We can only use this while in the Editor, so we need to wrap the code in Conditional compilation preprocessor directives. We’ll place this code right below the namespaces.\n#if UNITY_EDITOR\nusing UnityEditor;\n\n[InitializeOnLoad]\npublic class Startup\n{\n    static Startup()\n    {\n        // Get all files named \"DirectML.dll\" in the Assets directory\n        string[] files = Directory.GetFiles(\"./Assets/\", \"DirectML.dll\", SearchOption.AllDirectories);\n        // Iterate through each found file\n        foreach (string file in files)\n        {\n            // Check if the file is in the \"x86_64\" folder\n            if (file.Contains(\"x86_64\"))\n            {\n                // Get the file path for the Editor application\n                string editorPath = EditorApplication.applicationPath;\n                // Extract the parent folder for the Editor application\n                string editorDir = Directory.GetParent(editorPath).ToString();\n                // Define target file path\n                string targetPath = $\"{editorDir}/DirectML.dll\";\n                // Only copy the file to the Editor application folder if it is not already present\n                if (!File.Exists(targetPath)) File.Copy(file, targetPath);\n            }\n        }\n    }\n}\n#endif\nWe use the UNITY_EDITOR scripting symbol to check whether we are in the Unity Editor. We are in the Editor, so it returns true, and the code executes.\n\n\n\n\n\nIf we check if we are not in the Unity Editor, it returns false, and the code block does not execute.\n\n\n\n\n\nWe can verify the code works by saving the script and going to the parent folder for Editor application. The DirectML.dll file should be present.\n\n\n\n\n\n\nNote: I install Unity editors to a location that does not require Administrator access. You might need to manually copy the file if this is not the case for you.\n\n\nDefine public variables\nWe’ll add the required public variables above the Start method. We will be able to access these variables in the Inspector tab. We can add Header attributes to organize the public variables in the Inspector tab and use Tooltip attributes to provide information about variables.\nDefine scene object variables\nFirst, we need a variable to access the screen object that displays either a test image or webcam input. We may or may not want to mirror the screen based on whether a webcam is facing the user.\n[Header(\"Scene Objects\")]\n[Tooltip(\"The Screen object for the scene\")]\npublic Transform screen;\n[Tooltip(\"Mirror the in-game screen.\")]\npublic bool mirrorScreen = true;\nDefine data processing variables\nNext, we’ll define the variables for processing model input. We can set the default target input resolution to 224 and use it to scale the source resolution while maintaining the original aspect ratio.\nWe’ll also add a public ComputeShader variable to access the ProcessingShader we made earlier.\nWe need to download the pixel data for the input image from the GPU to the CPU before passing it to the plugin. This step can cause a significant performance bottleneck, so we’ll add the option to read the model output asynchronously at the cost of a few frames of latency. This latency might cause the bounding box to trail slightly behind a fast-moving object on the screen. The effect should be minimal, provided the frame rate is high enough.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 224;\n[Tooltip(\"The compute shader for GPU processing\")]\npublic ComputeShader processingShader;\n[Tooltip(\"Asynchronously download input image from the GPU to the CPU.\")]\npublic bool useAsyncGPUReadback = true;\nDefine output processing variables\nWe pass in the JSON file containing the class labels as a TextAsset.\n[Header(\"Output Processing\")]\n[Tooltip(\"A json file containing the colormaps for object classes\")]\npublic TextAsset colormapFile;\n[Tooltip(\"Minimum confidence score for keeping detected objects\")]\n[Range(0,1f)]\npublic float minConfidence = 0.5f;\nDefine variables for debugging\nNext, we’ll add a Boolean variable to toggle printing debug messages to the console.\n[Header(\"Debugging\")]\n[Tooltip(\"Print debugging messages to the console\")]\npublic bool printDebugMessages = true;\nDefine webcam variables\nWe need to specify a desired resolution and framerate when using a webcam as input.\n[Header(\"Webcam\")]\n[Tooltip(\"Use a webcam as input\")]\npublic bool useWebcam = false;\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"The requested webcam framerate\")]\n[Range(0, 60)]\npublic int webcamFPS = 60;\nDefine variables for user interface\nWe’ll make a simple GUI that displays the predicted class, the current framerate, and controls for selecting webcam devices, models, and execution providers.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayBoundingBoxes = true;\n[Tooltip(\"Display number of detected objects\")]\npublic bool displayProposalCount = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.red;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available ONNX models\")]\npublic Dropdown modelDropdown;\n[Tooltip(\"The dropdown menu that lists available ONNX execution providers\")]\npublic Dropdown executionProviderDropdown;\nDefine public variables for the ONNX plugin\n[Header(\"ONNX\")]\n[Tooltip(\"The name of the ONNX models folder\")]\npublic string onnxModelsDir = \"ONNXModels\";\n\n\nDefine private variables\nWe’ll add the required private variables right below the public variables.\nDefine private webcam variables\nWe’ll keep a list of available webcam devices so users can switch between them. Unity renders webcam input to a WebcamTexture.\n// List of available webcam devices\nprivate WebCamDevice[] webcamDevices;\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n// The name of the current webcam  device\nprivate string currentWebcam;\nDefine input variables\nWe’ll update the dimensions and content of the screen object based on the test image or webcam.\nWhen using asynchronous GPU readback, we need one Texture that stores data on the GPU and one that stores data on the CPU.\n// The test image dimensions\nprivate Vector2Int imageDims;\n// The test image texture\nprivate Texture imageTexture;\n// The current screen object dimensions\nprivate Vector2Int screenDims;\n// The model GPU input texture\nprivate RenderTexture inputTextureGPU;\n// The model CPU input texture\nprivate Texture2D inputTextureCPU;\nDefine variable for tracking the current number of detected objects\n// Stores the number of detected objects\nprivate int numObjects;\nDefine variables for storing colormaps\nWe need to create a couple of classes to parse the JSON content.\n// A class for parsing in colormaps from a JSON file\n[System.Serializable]\nclass ColorMap { public string label; public float[] color; }\n// A class for reading in a list of colormaps from a JSON file\n[System.Serializable]\nclass ColorMapList { public List&lt;ColorMap&gt; items; }\n// Stores a list of colormaps from a JSON file\nprivate ColorMapList colormapList;\n// A list of colors that map to class labels\nprivate Color[] colors;\n// A list of single pixel textures that map to class labels\nprivate Texture2D[] colorTextures;\nDefine variables for tracking the framerate\nWe’ll define some variables to track the frame rate.\n// The current frame rate value\nprivate int fps = 0;\n// Controls when the frame rate value updates\nprivate float fpsTimer = 0f;\nDefine private variables for the plugin\n// File paths for the available ONNX models\nprivate List&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available ONNX models\nprivate List&lt;string&gt; modelNames = new List&lt;string&gt;();\n// Names of the available ONNX execution providers\nprivate List&lt;string&gt; onnxExecutionProviders = new List&lt;string&gt;();\nDefine a struct for reading object information from the plugin\nWe need to create an Object struct for Unity to match the one we defined for the ONNX Runtime code, along with an array of Object structs that we’ll update with the PopulateObjectsArray() function.\n// Indicate that the members of the struct are laid out sequentially\n[StructLayout(LayoutKind.Sequential)]\n/// &lt;summary&gt;\n/// Stores the information for a single object\n/// &lt;/summary&gt; \npublic struct Object\n{\n    // The X coordinate for the top left bounding box corner\n    public float x0;\n    // The Y coordinate for the top left bounding box cornder\n    public float y0;\n    // The width of the bounding box\n    public float width;\n    // The height of the bounding box\n    public float height;\n    // The object class index for the detected object\n    public int label;\n    // The model confidence score for the object\n    public float prob;\n\n    public Object(float x0, float y0, float width, float height, int label, float prob)\n    {\n        this.x0 = x0;\n        this.y0 = y0;\n        this.width = width;\n        this.height = height;\n        this.label = label;\n        this.prob = prob;\n    }\n}\n\n// Stores information for the current list of detected objects\nprivate Object[] objectInfoArray;\nImport functions from the plugin\nWe pass the pointer to the input pixel data as an IntPtr.\n// Name of the DLL file\nconst string dll = \"ONNX_YOLOX_DLL\";\n\n[DllImport(dll)]\nprivate static extern int InitOrtAPI();\n\n[DllImport(dll)]\nprivate static extern int GetProviderCount();\n\n[DllImport(dll)]\nprivate static extern IntPtr GetProviderName(int index);\n\n[DllImport(dll)]\nprivate static extern void SetConfidenceThreshold(float minConfidence);\n\n[DllImport(dll)]\nprivate static extern void RefreshMemory();\n\n[DllImport(dll)]\nprivate static extern int LoadModel(string model, string execution_provider, int[] inputDims);\n\n[DllImport(dll)]\nprivate static extern int PerformInference(IntPtr inputData);\n\n[DllImport(dll)]\nprivate static extern void PopulateObjectsArray(IntPtr objects);\n\n[DllImport(dll)]\nprivate static extern void FreeResources();\n\n\nDefine Initialization Methods\nWe first need to define some methods to initialize webcams, the screen object, any GUI dropdown menus, and the in-game camera.\nDefine method to initialize a webcam device\n/// &lt;summary&gt;\n/// Initialize the selected webcam device\n/// &lt;/summary&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the selected webcam device&lt;/param&gt;\nprivate void InitializeWebcam(string deviceName)\n{\n    // Stop any webcams already playing\n    if (webcamTexture && webcamTexture.isPlaying) webcamTexture.Stop();\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(deviceName, webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the webcam\n    webcamTexture.Play();\n    // Check if webcam is playing\n    useWebcam = webcamTexture.isPlaying;\n    // Update toggle value\n    useWebcamToggle.SetIsOnWithoutNotify(useWebcam);\n\n    Debug.Log(useWebcam ? \"Webcam is playing\" : \"Webcam not playing, option disabled\");\n}\nDefine method to initialize the in-scene screen object\n/// &lt;summary&gt;\n/// Resize and position an in-scene screen object\n/// &lt;/summary&gt;\nprivate void InitializeScreen()\n{\n    // Set the texture for the screen object\n    screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture = useWebcam ? webcamTexture : imageTexture;\n    // Set the screen dimensions\n    screenDims = useWebcam ? new Vector2Int(webcamTexture.width, webcamTexture.height) : imageDims;\n\n    // Flip the screen around the Y-Axis when using webcam\n    float yRotation = useWebcam && mirrorScreen ? 180f : 0f;\n    // Invert the scale value for the Z-Axis when using webcam\n    float zScale = useWebcam && mirrorScreen ? -1f : 1f;\n\n    // Set screen rotation\n    screen.rotation = Quaternion.Euler(0, yRotation, 0);\n    // Adjust the screen dimensions\n    screen.localScale = new Vector3(screenDims.x, screenDims.y, zScale);\n\n    // Adjust the screen position\n    screen.position = new Vector3(screenDims.x / 2, screenDims.y / 2, 1);\n}\nDefine method to get the available ONNX models\n/// &lt;summary&gt;\n/// Get the file paths for available ONNX models\n/// &lt;/summary&gt;\nprivate void GetONNXModels()\n{\n    // Get the paths for each model folder\n    foreach (string dir in System.IO.Directory.GetDirectories($\"{Application.streamingAssetsPath}/{onnxModelsDir}\"))\n    {\n        // Extract the model folder name\n        string modelName = dir.Split('\\\\')[1];\n        // Add name to list of model names\n        modelNames.Add(modelName);\n\n        // Get the paths for the ONNX file for each model\n        foreach (string file in System.IO.Directory.GetFiles(dir))\n        {\n            if (file.EndsWith(\".onnx\"))\n            {\n                modelPaths.Add(file);\n            }\n        }\n    }\n}\nDefine method to get the names of available execution providers\n/// &lt;summary&gt;\n/// Get the names of the available ONNX execution providers\n/// &lt;/summary&gt;\nprivate void GetONNXExecutionProviders()\n{\n    // Get the number of available ONNX execution providers\n    int providerCount = GetProviderCount();\n    Debug.Log($\"Provider Count: {providerCount}\");\n\n    for (int i = 0; i &lt; providerCount; i++)\n    {\n        string providerName = Marshal.PtrToStringAnsi(GetProviderName(i));\n        Debug.Log(providerName);\n        providerName = providerName.Replace(\"ExecutionProvider\", \"\");\n        onnxExecutionProviders.Add(providerName);\n    }\n    onnxExecutionProviders.Reverse();\n}\nDefine method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nprivate void InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach (WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add ONNX model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n\n    // Remove default dropdown options\n    executionProviderDropdown.ClearOptions();\n    // Add ONNX provider names to menu\n    executionProviderDropdown.AddOptions(onnxExecutionProviders);\n    // Select the first option in the dropdown\n    executionProviderDropdown.SetValueWithoutNotify(0);\n}\nDefine method to initialize the in-scene camera object\n/// &lt;summary&gt;\n/// Resize and position the main camera based on an in-scene screen object\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of an in-scene screen object&lt;/param&gt;\nprivate void InitializeCamera(Vector2Int screenDims, string cameraName = \"Main Camera\")\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject camera = GameObject.Find(cameraName);\n    // Adjust the camera position to account for updates to the screenDims\n    camera.transform.position = new Vector3(screenDims.x / 2, screenDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    camera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the screenDims\n    camera.GetComponent&lt;Camera&gt;().orthographicSize = screenDims.y / 2;\n}\nDefine method to update the selected ONNX model\n/// &lt;summary&gt;\n/// Update the selected ONNX model\n/// &lt;/summary&gt;\npublic void UpdateONNXModel()\n{\n    // Reset objectInfoArray\n    objectInfoArray = new Object[0];\n\n    int[] inputDims = new int[] {\n        inputTextureCPU.width,\n        inputTextureCPU.height\n    };\n\n    Debug.Log($\"Source input dims: {inputDims[0]} x {inputDims[1]}\");\n\n    // Load the specified ONNX model\n    int return_msg = LoadModel(\n        modelPaths[modelDropdown.value], \n        onnxExecutionProviders[executionProviderDropdown.value], \n        inputDims);\n\n    SetConfidenceThreshold(minConfidence);\n\n    string[] return_messages = {\n        \"Using DirectML\",\n        \"Using CPU\",\n    };\n\n    Debug.Log($\"Updated input dims: {inputDims[0]} x {inputDims[1]}\");\n    Debug.Log($\"Return message: {return_messages[return_msg]}\");\n}\n\n\nDefine Awake Method\nWe’ll implement the code to copy the DirectML.dll file from the Plugins/x86_64 folder to the root of the build folder in the Awake() method. The code should be inactive since we are in the Editor.\n// Awake runs when the script instance is being loaded\nprivate void Awake()\n{\n    #if !UNITY_EDITOR\n        // Define the path for the DirectML.dll file in the StreamingAssets folder\n        string sourcePath = $\"{Application.dataPath}/Plugins/x86_64/DirectML.dll\";\n\n    string dataPath = Application.dataPath;\n    string buildDir = Directory.GetParent(dataPath).ToString();\n\n    // Define the destination path for the DirectML.dll file\n    string targetPath = $\"{buildDir}/DirectML.dll\";\n    // Only copy the file if it is not already present at the destination\n    if (!File.Exists(targetPath)) File.Copy(sourcePath, targetPath);\n    #endif\n}\n\n\nDefine Start Method\nThe Start method is called once before the first frame update, so we’ll perform any required setup steps here.\n// Start runs before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Initialize list of color maps from JSON file\n    colormapList = JsonUtility.FromJson&lt;ColorMapList&gt;(colormapFile.text);\n    // Initialize the list of colors\n    colors = new Color[colormapList.items.Count];\n    // Initialize the list of color textures\n    colorTextures = new Texture2D[colormapList.items.Count];\n\n    // Populate the color and color texture arrays\n    for (int i = 0; i &lt; colors.Length; i++)\n    {\n        // Create a new color object\n        colors[i] = new Color(\n            colormapList.items[i].color[0],\n            colormapList.items[i].color[1],\n            colormapList.items[i].color[2]);\n        // Create a single-pixel texture\n        colorTextures[i] = new Texture2D(1, 1);\n        colorTextures[i].SetPixel(0, 0, colors[i]);\n        colorTextures[i].Apply();\n\n    }\n\n    // Get the file paths for available ONNX models\n    GetONNXModels();\n    // Initialize the ONNX Runtime API\n    InitOrtAPI();\n    // Get the names of available ONNX execution providers\n    GetONNXExecutionProviders();\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n}\n\n\nDefine Processing Methods\nNext, we need to define methods to process images using the Compute Shader, calculate the input resolution, handle asynchronous GPU readback, and scale the bounding box information.\nDefine method to process images using a compute shader\n/// &lt;summary&gt;\n/// Process the provided image using the specified function on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The target image RenderTexture&lt;/param&gt;\n/// &lt;param name=\"computeShader\"&gt;The target ComputerShader&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The target ComputeShader function&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate void ProcessImageGPU(RenderTexture image, ComputeShader computeShader, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = computeShader.FindKernel(functionName);\n    // Define a temporary HDR RenderTexture\n    RenderTexture result = new RenderTexture(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    computeShader.SetTexture(kernelHandle, \"InputImage\", image);\n\n    // Execute the ComputeShader\n    computeShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, image);\n\n    // Release RenderTexture\n    result.Release();\n}\nDefine method to calculate input resolution\n/// &lt;summary&gt;\n/// Scale the source image resolution to the target input dimensions\n/// while maintaing the source aspect ratio.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetDims\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nprivate Vector2Int CalculateInputDims(Vector2Int imageDims, int targetDim)\n{\n    Vector2Int inputDims = new Vector2Int();\n\n    // Calculate the input dimensions using the target minimum dimension\n    if (imageDims.x &gt;= imageDims.y)\n    {\n        inputDims[0] = (int)(imageDims.x / ((float)imageDims.y / (float)targetDim));\n        inputDims[1] = targetDim;\n    }\n    else\n    {\n        inputDims[0] = targetDim;\n        inputDims[1] = (int)(imageDims.y / ((float)imageDims.x / (float)targetDim));\n    }\n\n    return inputDims;\n}\nDefine method to handle asynchronous GPU readback\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Make sure the Texture2D is not null\n    if (inputTextureCPU)\n    {\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        inputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Textur2D\n        inputTextureCPU.Apply();\n    }\n}\nDefine method to send the input texture data to the plugin\n/// &lt;summary&gt;\n/// Pin memory for the input data and pass a reference to the plugin for inference\n/// &lt;/summary&gt;\n/// &lt;param name=\"texture\"&gt;The input texture&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\npublic unsafe int UploadTexture(Texture2D texture)\n{\n    //Pin Memory\n    fixed (byte* p = texture.GetRawTextureData())\n    {\n        // Perform inference and get the number of detected objects\n        numObjects = PerformInference((IntPtr)p);\n    }\n\n    // Initialize the array\n    objectInfoArray = new Object[numObjects];\n\n    // Pin memory\n    fixed (Object* o = objectInfoArray)\n    {\n        // Get the detected objects\n        PopulateObjectsArray((IntPtr)o);\n    }\n\n    return numObjects;\n}\nDefine method to scale bounding boxes to the display resolution\n/// &lt;summary&gt;\n/// Scale the latest bounding boxes to the display resolution\n/// &lt;/summary&gt;\npublic void ScaleBoundingBoxes()\n{\n    // Process new detected objects\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        // The smallest dimension of the screen\n        float minScreenDim = Mathf.Min(screen.transform.localScale.x, screen.transform.localScale.y);\n        // The smallest input dimension\n        int minInputDim = Mathf.Min(inputTextureCPU.width, inputTextureCPU.height);\n        // Calculate the scale value between the in-game screen and input dimensions\n        float minImgScale = minScreenDim / minInputDim;\n        // Calculate the scale value between the in-game screen and display\n        float displayScale = Screen.height / screen.transform.localScale.y;\n\n        // Scale bounding box to in-game screen resolution and flip the bbox coordinates vertically\n        float x0 = objectInfoArray[i].x0 * minImgScale;\n        float y0 = (inputTextureCPU.height - objectInfoArray[i].y0) * minImgScale;\n        float width = objectInfoArray[i].width * minImgScale;\n        float height = objectInfoArray[i].height * minImgScale;\n\n        // Mirror bounding box across screen\n        if (mirrorScreen && useWebcam) x0 = screen.transform.localScale.x - x0 - width;\n\n        // Scale bounding boxes to display resolution\n        objectInfoArray[i].x0 = x0 * displayScale;\n        objectInfoArray[i].y0 = y0 * displayScale;\n        objectInfoArray[i].width = width * displayScale;\n        objectInfoArray[i].height = height * displayScale;\n\n        // Offset the bounding box coordinates based on the difference between the in-game screen and display\n        objectInfoArray[i].x0 += (Screen.width - screen.transform.localScale.x * displayScale) / 2;\n    }\n}\n\n\nDefine Update method\nWe’ll place anything we want to run every frame in the Update method.\n// Update runs once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputDims.x} x {inputDims.y}\");\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGBHalf);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGBA32, false);\n        // Update the selected ONNX model\n        UpdateONNXModel();\n    }\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Flip image before sending to DLL\n    ProcessImageGPU(inputTextureGPU, processingShader, \"FlipXAxis\");\n\n    // Download pixel data from GPU to CPU\n    if (useAsyncGPUReadback)\n    {\n        AsyncGPUReadback.Request(inputTextureGPU, 0, TextureFormat.RGBA32, OnCompleteReadback);\n    }\n    else\n    {\n        RenderTexture.active = inputTextureGPU;\n        inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n        inputTextureCPU.Apply();\n    }\n\n    // Send reference to inputData to DLL\n    numObjects = UploadTexture(inputTextureCPU);\n    if (printDebugMessages) Debug.Log($\"Detected {numObjects} objects\");\n    // Scale bounding boxes\n    ScaleBoundingBoxes();\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n}\n\n\nDefine GUI Methods\nWe need some methods to handle user interactions with the GUI and display the bounding boxes and current framerate.\nDefine method that to handle switching ONNX models and execution providers\n/// &lt;summary&gt;\n/// This method runs when the value for an ONNX dropdown changes\n/// &lt;/summary&gt;\npublic void ONNXDropdownUpdate()\n{\n    // Only call plugin methods after initializing the input texture\n    if (inputTextureCPU)\n    {\n        RefreshMemory();\n        UpdateONNXModel();\n    }\n}\nDefine method to update webcam usage from GUI\n/// &lt;summary&gt;\n/// This method is called when the value for the webcam toggle changes\n/// &lt;/summary&gt;\n/// &lt;param name=\"useWebcam\"&gt;&lt;/param&gt;\npublic void UpdateWebcamToggle(bool useWebcam)\n{\n    this.useWebcam = useWebcam;\n}\nDefine method to update webcam device from GUI\n/// &lt;summary&gt;\n/// The method is called when the selected value for the webcam dropdown changes\n/// &lt;/summary&gt;\npublic void UpdateWebcamDevice()\n{\n    currentWebcam = webcamDevices[webcamDropdown.value].name;\n    Debug.Log($\"Selected Webcam: {currentWebcam}\");\n    // Initialize webcam if it is not already playing\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n}\nDefine method to update the minimum confidence value\n/// &lt;summary&gt;\n/// Update the minimum confidence score for keeping bounding box proposals\n/// &lt;/summary&gt;\n/// &lt;param name=\"slider\"&gt;&lt;/param&gt;\npublic void UpdateConfidenceThreshold(Slider slider)\n{\n    minConfidence = slider.value;\n    SetConfidenceThreshold(minConfidence);\n}\nDefine OnGUI method\nWe’ll display the predicted bounding boxes and current frame rate in the OnGUI method.\n// OnGUI handles and renders GUI events.\npublic void OnGUI()\n{\n    // Initialize a rectangle for label text\n    Rect labelRect = new Rect();\n    // Initialize a rectangle for bounding boxes\n    Rect boxRect = new Rect();\n\n    GUIStyle labelStyle = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * 11e-3)\n    };\n    labelStyle.alignment = TextAnchor.MiddleLeft;\n\n    foreach (Object objectInfo in objectInfoArray)\n    {\n        if (!displayBoundingBoxes) break;\n\n        // Skip object if label index is out of bounds\n        if (objectInfo.label &gt; colors.Length - 1) continue;\n\n        // Get color for current class index\n        Color color = colors[objectInfo.label];\n        // Get label for current class index\n        string name = colormapList.items[objectInfo.label].label;\n\n        // Set bounding box coordinates\n        boxRect.x = objectInfo.x0;\n        boxRect.y = Screen.height - objectInfo.y0;\n        // Set bounding box dimensions\n        boxRect.width = objectInfo.width;\n        boxRect.height = objectInfo.height;\n\n        // Scale bounding box line width based on display resolution\n        int lineWidth = (int)(Screen.width * 1.75e-3);\n        // Render bounding box\n        GUI.DrawTexture(\n            position: boxRect,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: lineWidth,\n            borderRadius: 0);\n\n        // Include class label and confidence score in label text\n        string labelText = $\" {name}: {(objectInfo.prob * 100).ToString(\"0.##\")}%\";\n\n        // Initialize label GUI content\n        GUIContent labelContent = new GUIContent(labelText);\n\n        // Calculate the text size.\n        Vector2 textSize = labelStyle.CalcSize(labelContent);\n\n        // Set label text coordinates\n        labelRect.x = objectInfo.x0;\n        labelRect.y = Screen.height - objectInfo.y0 - textSize.y + lineWidth;\n\n        // Set label text dimensions\n        labelRect.width = Mathf.Max(textSize.x, objectInfo.width);\n        labelRect.height = textSize.y;\n        // Set label text and backgound color\n        labelStyle.normal.textColor = color.grayscale &gt; 0.5 ? Color.black : Color.white;\n        labelStyle.normal.background = colorTextures[objectInfo.label];\n        // Render label\n        GUI.Label(labelRect, labelContent, labelStyle);\n\n        Rect objectDot = new Rect();\n        objectDot.height = lineWidth * 5;\n        objectDot.width = lineWidth * 5;\n        float radius = objectDot.width / 2;\n        objectDot.x = (boxRect.x + boxRect.width / 2) - radius;\n        objectDot.y = (boxRect.y + boxRect.height / 2) - radius;\n\n\n        GUI.DrawTexture(\n            position: objectDot,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: radius,\n            borderRadius: radius);\n\n    }\n\n    // Define styling information for GUI elements\n    GUIStyle style = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * (1f / (100f - fontScale)))\n    };\n    style.normal.textColor = textColor;\n\n    // Define screen spaces for GUI elements\n    Rect slot1 = new Rect(10, 10, 500, 500);\n    Rect slot2 = new Rect(10, style.fontSize * 1.5f, 500, 500);\n\n    string content = $\"Objects Detected: {numObjects}\";\n    if (displayProposalCount) GUI.Label(slot1, new GUIContent(content), style);\n\n    // Update framerate value\n    if (Time.unscaledTime &gt; fpsTimer)\n    {\n        fps = (int)(1f / Time.unscaledDeltaTime);\n        fpsTimer = Time.unscaledTime + fpsRefreshRate;\n    }\n\n    // Adjust screen position when not showing predicted class\n    Rect fpsRect = displayProposalCount ? slot2 : slot1;\n    if (displayFPS) GUI.Label(fpsRect, new GUIContent($\"FPS: {fps}\"), style);\n}\nDefine method to exit the application using the GUI\n/// &lt;summary&gt;\n/// This method runs when the user clicks the GUI Quit button\n/// &lt;/summary&gt;\npublic void Quit()\n{\n    Application.Quit();\n}\n\n\nDefine OnApplicationQuit Method\nWe’ll perform any clean-up steps in the OnApplicationQuitmethod.\nprivate void OnApplicationQuit()\n{\n    FreeResources();\n}"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Set up Unity Scene",
    "text": "Set up Unity Scene\nNow we can start setting up our Unity scene. We need a screen to display the webcam feed, an empty object to attach the object detector script, dropdown menus for selecting webcams, models, and execution providers, a toggle to activate a webcam feed, and a slider to update the confidence threshold.\nCreate Screen object\nRight-click a space in the Hierarchy tab and select 3D Object → Quad. We can name the new object Screen.\n\n\n\n\n\nNext, drag and drop a test image from the Assets → Images folder onto the Screen object in the Scene view. Note that the Screen looks a bit dim. We need to change the shader for the Screen’s Material so that it does not require an external light source.\n\n\n\n\n\nSelect the Screen in the Hierarchy tab and open the Shader dropdown menu in the Inspector tab. Type Unlit/Texture into the search box and press enter.\n\n\n\n\n\nCreate Inference Manager object\nRight-click a space in the Hierarchy tab and select Create Empty. Name the empty object InferenceManager.\n\n\n\n\n\nWith the InferenceManager object selected, drag the ObjectDetector script into the Inspector tab.\n\n\n\n\n\nNow we can assign the screen object, compute shader, and colormap file in the Inspector tab by dragging them into their respective fields.\nAdd GUI prefab\nWe still need to create the GUI controls. To save time, I made a Prefab that we can drop into the Scene.\n\nGoogle Drive: Canvas Prefab\n\nDrag and drop the Canvas prefab into a new folder called Prefabs.\n\n\n\n\n\nFrom there, drag the prefab into the Hierarchy tab. We can see the GUI by switching to the Game view.\n\n\n\n\n\nConfigure Webcam Toggle On Value Changed function\nNext, we need to pair the WebcamToggle with the UpdateWebcamToggle function in the ObjectDetector script. Expand the Canvas object and select the WebcamToggle.\n\n\n\n\n\nClick and drag the InferenceManager into the On Value Changed field.\n\n\n\n\n\nOpen the No Function dropdown menu and select ObjectDetector → UpdateWebcamToggle.\n\n\n\n\n\nConfigure Webcam Dropdown On Value Changed function\nWe can follow the same steps to pair the WebcamDropdown with the UpdateWebcamDevice function in the ObjectDetector script.\n\n\n\n\n\nThis time select ObjectDetector → UpdateWebcamDevice.\n\n\n\n\n\nConfigure ONNXModelDropdown On Value Changed Event\n\n\n\n\n\nConfigure ONNXExecutionProviderDropdown On Value Changed Event\n\n\n\n\n\nConfigure ConfidenceThresholdSlider On Value Changed Event\n\n\n\n\n\nConfigure QuitButton On Click Event\n\n\n\n\n\nAssign GUI objects to Inference Manager\nWe can now assign the GUI objects to their respective fields for the ObjectDetector script.\n\n\n\n\n\nAdd Event System\nBefore we can use the GUI, we need to add an Event System. Right-click a space in the Hierarchy tab and select UI → Event System."
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#test-in-editor",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#test-in-editor",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Test in Editor",
    "text": "Test in Editor\nClick the play button in the top-middle of the Editor window to test the project.\n\n\n\n\n\nThere should be a bounding box for the call sign and one for the idle hand.\nCPU Execution Provider\n\n\n\n\n\nDirectML Execution Provider"
  },
  {
    "objectID": "posts/onnx-directml-unity-tutorial/part-2/index.html#summary",
    "href": "posts/onnx-directml-unity-tutorial/part-2/index.html#summary",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 2",
    "section": "Summary",
    "text": "Summary\nIn this two-part tutorial series, you learned how to implement real-time object detection in Unity using ONNX Runtime and DirectML. In Part 1, we created a dynamic link library (DLL) file in Visual Studio to perform object detection with ONNX Runtime and DirectML. In this post, we integrated this DLL file into a Unity project and performed real-time object detection. You should now have the skills and knowledge to leverage ONNX Runtime and DirectML in your Unity projects.\nPrevious: Object Detection for Unity With ONNX Runtime and DirectML Pt. 1\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-1/index.html",
    "href": "posts/openvino-unity-plugin/part-1/index.html",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.1",
    "section": "",
    "text": "Overview\nPrerequisites\nConvert ONNX Model to OpenVINO IR\nConclusion"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-1/index.html#overview",
    "href": "posts/openvino-unity-plugin/part-1/index.html#overview",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.1",
    "section": "Overview",
    "text": "Overview\nIn this tutorial series, we will cover how to create a plugin that leverages the OpenVINO™ Toolkit for the Unity game engine. We will first create a Dynamic link library (DLL) in Visual Studio to perform inference with a pretrained deep learning model. We will then demonstrate how to access this DLL inside a Unity application.\nWe will be using a computationally demanding style transfer model to demonstrate the potential performance benefits from using the OpenVINO inference engine. The model takes in a single RGB image as input and has been trained to generate a stylized version of the image based on a specific style image. The steps for training this type of model are covered in an earlier tutorial.\nIn this first part, we will ensure the prerequisite software is installed on our system and convert a pretrained model from ONNX format to the OpenVINO Intermediate Representation format."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-1/index.html#prerequisites",
    "href": "posts/openvino-unity-plugin/part-1/index.html#prerequisites",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.1",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe following prerequisites are required to complete this tutorial.\n\nExperience\nThis tutorial does not assume any prior experience with the OpenVINO™ Toolkit or Unity. However, some basic experience with Unity would be beneficial.\n\n\nSystem Requirements\nThe target platform for this project is Windows 10 64-bit. The OpenVINO™ Toolkit does not appear to support 32-bit versions. Given that the OpenVINO™ Toolkit is designed for Intel hardware, an Intel CPU and/or GPU is highly recommended.\n\n\nUnity\nThe first prerequisite we will want to set up is Unity. If you don’t already have Unity installed, you can download Unity Hub from the link below.\n\nUnity Hub: (download)\n\nOnce Unity Hub is set up, you can install Unity 2020.3.12f1 (LTS) from the link below.\n\nUnity Download Archive: (download)\nDownload Unity 2020.3.12f1 (LTS): (download)\n\nNote: The installation process will also install Visual Studio, one of the other prerequisites.\nIf you have never used Unity before, you can become acquainted with the basics by following the tutorial below. It will walk you through the installation process all the way to making an Angry Birds clone.\n\nHow to Make a Game - Unity Beginner Tutorial\n\n\n\nVisual Studio\nVisual Studio will be installed during the installation process for Unity. However it can also be downloaded directly from the link below.\n\nVisual Studio Community 2019: (download)\n\n\n\nVisual C++ Redistributables\nThe Visual C++ Redistributables should be installed along with Visual Studio. However, you can also download them from the link below.\n\nLatest C++ Redistributables: (link)\n\n\n\nCMake\nCMake is listed as a requirement for the OpenVINO™ Toolkit. However, it will not be needed for this tutorial. The download link for the latest release of CMake 64-bit is still provided below.\n\nCMake: link\n\nNote: Make sure to select one of the Add CMake to the system PATH options during the installation process.\n\n\n\n\n\n\n\nPython\nWe will need Python 3.6, 3.7, or 3.8 64-bit to convert the provided model from ONNX format to OpenVINO’s intermediate representation. We can install Python 3.8 from the Windows Store. This method automatically sets up the Python installation to be accessible from the command line.\n\nWindows Store Python 3.8: (link)\n\n\n\nOpenVINO\nWe now have all the required prerequisites to install OpenVINO. We’ll be using OpenVINO 2021.3 for this tutorial. You will need to fill out a registration form to download the toolkit.\n\nRegistration Link\nDownload Link"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-1/index.html#convert-onnx-model-to-openvino-ir",
    "href": "posts/openvino-unity-plugin/part-1/index.html#convert-onnx-model-to-openvino-ir",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.1",
    "section": "Convert ONNX Model to OpenVINO IR",
    "text": "Convert ONNX Model to OpenVINO IR\nBefore we write any code, we need to convert the trained model to OpenVINO’s Intermediate Representation format. We will use the style transfer model from this tutorial series as an example.\n\nMesh Shader Style Transfer Model\nOther models\n\nOpen the File Explorer and Navigate to C:\\Program Files (x86)\\Intel\\openvino_2021\\deployment_tools\\model_optimizer\n\n\n\n\n\nType cmd into the address bar and press Enter to open a command line prompt.\n\n\n\n\n\nThe mo.py script requires a few additional python packages. You can install them via pip with the following commands.\n\nNumPy: pip install numpy\ndefusedxml: pip install defusedxml\nNetworkX: pip install networkx\nONNX: pip install onnx\n\nOnce those are installed, we’ll run the mo.py script with the following parameters.\npython mo.py --input_model &lt;path-to-ONNX-model&gt;  --model_name &lt;openvino-model-name&gt; --output_dir &lt;path-to-save-openvino-model&gt; --data_type FP16\nNote: FP16 or half precision is recommended for GPU inference. It reduces the size of the model and can increase inference speed.\nHere is an example where User_Name would be replaced with the current Windows username.\npython mo.py --input_model C:\\Users\\User_Name\\Downloads\\final.onnx  --model_name mesh-shader_fp16 --output_dir C:\\Users\\User_Name\\Downloads\\ --data_type FP16\nWe can also specify the input resolution with the --input_shape [N,C,H,W] parameter. For example, we could specify an input resolution of 960 x 540 with --input_shape [1,3,540,960]. However, this is not required as we will be updating the input resolution from the Unity application at runtime.\nThe script will generate three files:\n\nmesh-shader_fp16.bin\nmesh-shader_fp16.mapping\nmesh-shader_fp16.xml\n\nWe will need the .bin and .xml files. The .xml file describes the network topology, including the layer operations and flow of data through the network. Here is a snippet from the top of the generated .xml file.\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;net name=\"mesh-shader_fp16\" version=\"10\"&gt;\n    &lt;layers&gt;\n        &lt;layer id=\"0\" name=\"input.1\" type=\"Parameter\" version=\"opset1\"&gt;\n            &lt;data shape=\"1,3,960,540\" element_type=\"f16\"/&gt;\n            &lt;output&gt;\n                &lt;port id=\"0\" precision=\"FP16\" names=\"input.1\"&gt;\n                    &lt;dim&gt;1&lt;/dim&gt;\n                    &lt;dim&gt;3&lt;/dim&gt;\n                    &lt;dim&gt;960&lt;/dim&gt;\n                    &lt;dim&gt;540&lt;/dim&gt;\n                &lt;/port&gt;\n            &lt;/output&gt;\n        &lt;/layer&gt;\n        &lt;layer id=\"1\" name=\"Pad_0/Cast_111275_const\" type=\"Const\" version=\"opset1\"&gt;\n            &lt;data offset=\"0\" size=\"32\" shape=\"4\" element_type=\"i64\"/&gt;\n            &lt;output&gt;\n                &lt;port id=\"0\" precision=\"I64\"&gt;\n                    &lt;dim&gt;4&lt;/dim&gt;\n                &lt;/port&gt;\n            &lt;/output&gt;\n        &lt;/layer&gt;\nThe .bin file stores the constant values for the model learned during the training process.\n\nOther models"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-1/index.html#conclusion",
    "href": "posts/openvino-unity-plugin/part-1/index.html#conclusion",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.1",
    "section": "Conclusion",
    "text": "Conclusion\nThat takes care of the required setup. In the next part, we will cover how to create a Dynamic link library (DLL) in Visual Studio to perform inference with the OpenVINO IR model.\nProject Resources:\nGitHub Repository\n\nNext: Part 2"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html",
    "href": "posts/openvino-unity-plugin/part-2/index.html",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "",
    "text": "Overview\nCreate a New Visual Studio Project\nConfigure Project\nAdd Include Directories\nLink Libraries\nClear Default Code\nUpdate Precompiled Header File\nUpdate dllmain\nBuild Solution\nGather Dependencies\nConclusion"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#overview",
    "href": "posts/openvino-unity-plugin/part-2/index.html#overview",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of the tutorial, we first installed Unity, OpenVINO, and its prerequisite software. We then demonstrated how to use the python conversion script included with the OpenVINO™ Toolkit to convert a pretrained model from ONNX format to the OpenVINO Intermediate Representation format.\nIn this part, we will walk through the steps needed to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pretrained deep learning model."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#create-a-new-visual-studio-project",
    "href": "posts/openvino-unity-plugin/part-2/index.html#create-a-new-visual-studio-project",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Create a New Visual Studio Project",
    "text": "Create a New Visual Studio Project\nOpen Visual Studio and select Create a new project.\n\n\n\n\n\nType DLL into the search bar. Select the Dynamic-Link Library (DLL) option and press Next.\n\n\n\n\n\nIn the next window, we’ll name the new project OpenVINO_Plugin. Take note of the Location the project will be saved to and click Create. The default location can be replaced, but we will need to access the project folder to get the generated DLL file."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#configure-project",
    "href": "posts/openvino-unity-plugin/part-2/index.html#configure-project",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Configure Project",
    "text": "Configure Project\nWe need to update the default project configuration to access the OpenVINO™ Toolkit and build the project with it.\n\nSet Build Configuration and Platform\nThe OpenVINO™ Toolkit does not support x86 builds. We will need to set the project to build for x64. At the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#add-include-directories",
    "href": "posts/openvino-unity-plugin/part-2/index.html#add-include-directories",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nVisual Studio needs to be told where the OpenVINO™ Toolkit is located, so we can access its APIs. In the Solution Explorer panel, right-click the project name.\n\n\n\n\n\nSelect Properties in the popup menu.\n\n\n\n\n\nIn the Properties Window, open the C++ dropdown and click on All Options. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nWe need to add the include directories for the OpenVINO inference engine and the OpenCV libraries included with the OpenVINO™ Toolkit.\nAdd the following lines and then click OK. Feel free to open these folders in the File Explorer and see what exactly they provide access to.\n\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\inference_engine\\include\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\opencv\\include"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#link-libraries",
    "href": "posts/openvino-unity-plugin/part-2/index.html#link-libraries",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select All Options. Scroll up to the top of the All Options section and select Additional Dependencies.\n\n\n\n\n\nAdd the following lines for the OpenVINO and OpenCV libraries, then click OK. The * at the end tells Visual Studio to add all the .lib files contained in those folders. We do not technically need every single one, but this is more convenient than manually typing the specific file names.\n\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\inference_engine\\lib\\intel64\\Release\\*\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\opencv\\lib\\*\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#clear-default-code",
    "href": "posts/openvino-unity-plugin/part-2/index.html#clear-default-code",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Clear Default Code",
    "text": "Clear Default Code\nNow, we can finally start coding. The default code for the dllmain.cpp file is as follows.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\""
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#update-precompiled-header-file",
    "href": "posts/openvino-unity-plugin/part-2/index.html#update-precompiled-header-file",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nThe pch.h file is a Precompiled Header file that is generated by Visual Studio. We can place any header files that won’t be updated here and they will only be compiled once. This can reduce build times for larger projects. We can open the pch.h file by selecting that line and pressing F12.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n#include \"framework.h\"\n\n\n#endif //PCH_H\nWe’ll add the required header files below #include \"framework.h\". Each one can be explored by selecting that line and pressing F12 as well.\n// add headers that you want to pre-compile here\n#include \"framework.h\"\n// A header file that provides a set minimal required Inference Engine API.\n#include &lt;inference_engine.hpp&gt;\n// A header file that provides the API for the OpenCV modules.\n#include &lt;opencv2/opencv.hpp&gt;\n// Regular expressions standard header\n#include &lt;regex&gt;"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#update-dllmain",
    "href": "posts/openvino-unity-plugin/part-2/index.html#update-dllmain",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Update dllmain",
    "text": "Update dllmain\nBack in the dllmain.cpp file, we’ll add the InferenceEngine namespace and create a macro to mark functions we want to make accessible in Unity.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nusing namespace InferenceEngine;\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWe need to wrap the code in extern \"C\" to prevent name-mangling issues with the compiler.\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\n\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n    \n}\n\nDeclare Variables\nInside the wrapper, we’ll declare the variables needed for the DLL.\nWe need to keep track of the available compute devices for OpenVINO, so we can select them in Unity. Create an std::vector&lt;std::string&gt; variable named availableDevices. This will store the names of supported devices found by OpenVINO on the system. We’ll combine the list of available devices into a single std::string variable to send it to Unity.\nNext, create a cv::Mat to store the input image data from Unity.\nTo use the OpenVINO inference engine, we first need to create a Core instance called ie. We’ll use this variable to read the model file, get the available compute devices, change configuration settings, and load the model onto the target compute device.\nWe’ll store the information from the .xml and .bin file in a CNNNetwork variable called network.\nWe need to create an executable version of the network before we can perform inference. Create an ExecutableNetwork variable called executable_network.\nAfter that, we will create an InferRequest variable called infer_request. We’ll use this variable to initiate inference for the model.\nOnce we create the inference request, we will need write access to the input tensor for the model and read access to the output tensor for the model. This is how we will update the input and read the output when performing inference. Create a MemoryBlob::Ptr variable called minput and a MemoryBlob::CPtr variable called moutput.\nSince the input and output dimensions are the same, we can use the same size variables when iterating through the input and output data. Create two size_t variables to store the number of color channels and number of pixels for the input image.\nLastly, we will create an std::vector&lt;float&gt; called data_img that will be used for processing the raw model output.\n\nCode :\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n    // List of available compute devices\n    std::vector&lt;std::string&gt; availableDevices;\n    // An unparsed list of available compute devices\n    std::string allDevices = \"\";\n    // The name of the input layer of Neural Network \"input.1\"\n    std::string firstInputName;\n    // The name of the output layer of Neural Network \"140\"\n    std::string firstOutputName;\n\n    // Stores the pixel data for model input image and output image\n    cv::Mat texture;\n\n    // Inference engine instance\n    Core ie;\n    // Contains all the information about the Neural Network topology and related constant values for the model\n    CNNNetwork network;\n    // Provides an interface for an executable network on the compute device\n    ExecutableNetwork executable_network;\n    // Provides an interface for an asynchronous inference request\n    InferRequest infer_request;\n    \n    // A pointer to the input tensor for the model\n    MemoryBlob::Ptr minput;\n    // A pointer to the output tensor for the model\n    MemoryBlob::CPtr moutput;\n\n    // The number of color channels \n    size_t num_channels;\n    // The number of pixels in the input image\n    size_t nPixels;\n\n    // A vector for processing the raw model output\n    std::vector&lt;float&gt; data_img;\n}\n\n\n\nCreate GetAvailableDevices() Function\nWe’ll create a function that returns the available OpenVINO compute devices so that we can view and select them in Unity. This function simply combines the list of available devices into a single, comma separated string that will be parsed in Unity. We need to add the DLLExport macro since we’ll be calling this function from Unity.\n\nCode :\n// Returns an unparsed list of available compute devices\nDLLExport const std::string* GetAvailableDevices() {\n    // Add all available compute devices to a single string\n    for (auto&& device : availableDevices) {\n        allDevices += device;\n        allDevices += ((device == availableDevices[availableDevices.size() - 1]) ? \"\" : \",\");\n    }\n    return &allDevices;\n}\n\n\n\nCreate SetDeviceCache() Function\nIt can take over 20 seconds to upload the OpenVINO model to a GPU. This is because OpenCL kernels are being compiled for the specific model and GPU at runtime. There isn’t much we can do about this the first time a model is loaded to the GPU. However, we can eliminate this load time in future uses by storing cache files for the model. The cache files are specific to each GPU. Additional cache files will also be created when using a new input resolution for a model. We do not need to add the DLLExport macro as this function will only be called by other functions in the DLL.\nWe’ll use a regular expression to confirm a compute device is a GPU before attempting to set a cache directory for it.\nWe can specify the directory to store cache files for each available GPU using the ie.SetConfig() method. We’ll just name the directory, cache.\nBy default, the cache directory will be created in the same folder as the executable file that will be generated from the Unity project.\n\nCode :\n// Configure the cache directory for GPU compute devices\nvoid SetDeviceCache() {\n    std::regex e(\"(GPU)(.*)\");\n    // Iterate through the available compute devices\n    for (auto&& device : availableDevices) {\n        // Only configure the cache directory for GPUs\n        if (std::regex_match(device, e)) {\n            ie.SetConfig({ {CONFIG_KEY(CACHE_DIR), \"cache\"} }, device);\n        }\n    }\n}\n\n\n\nCreate PrepareBlobs() Function\nThe next function will get the names of the input and output layers for the model and set the precision for them. We can access information about the input and output layers with network.getInputsInfo() and network.getOutputsInfo() respectively.\nThe model only has one input and output, so we can access them directly with .begin() rather than using a for loop. There are two values stored for each layer. The first contains the name of the layer and the second provides access to get and set methods for the layer.\n\nCode :\n// Get the names of the input and output layers and set the precision\nDLLExport void PrepareBlobs() {\n    // Get information about the network input\n    InputsDataMap inputInfo(network.getInputsInfo());\n    // Get the name of the input layer\n    firstInputName = inputInfo.begin()-&gt;first;\n    // Set the input precision\n    inputInfo.begin()-&gt;second-&gt;setPrecision(Precision::U8);\n\n    // Get information about the network output\n    OutputsDataMap outputInfo(network.getOutputsInfo());\n    // Get the name of the output layer\n    firstOutputName = outputInfo.begin()-&gt;first;\n    // Set the output precision\n    outputInfo.begin()-&gt;second-&gt;setPrecision(Precision::FP32);\n}\n\n\n\nCreate InitializeOpenVINO() Function\nThis is where we will make the preparations for performing inference and will be the first function called from the plugin in Unity. The function will take in a path to an OpenVINO model and read in the network information. We’ll then set the batch size for the network using network.setBatchSize() and call the PrepareBlobs() function.\nWe can initialize our list of available devices by calling ie.GetAvailableDevices(). Any available GPUs will be stored last, so we’ll want to reverse the list. The first GPU found (typically integrated graphics) would be named GPU.0. The second would be named GPU.1 and so on.\nLastly, we will call the SetDeviceCache() function now that we know what devices are available.\n\nCode :\n// Set up OpenVINO inference engine\nDLLExport void InitializeOpenVINO(char* modelPath) {\n    // Read network file\n    network = ie.ReadNetwork(modelPath);\n    // Set batch size to one image\n    network.setBatchSize(1);\n    // Get the output name and set the output precision\n    PrepareBlobs();\n    // Get a list of the available compute devices\n    availableDevices = ie.GetAvailableDevices();\n    // Reverse the order of the list\n    std::reverse(availableDevices.begin(), availableDevices.end());\n    // Specify the cache directory for GPU inference\n    SetDeviceCache();\n}\n\n\n\nCreate SetInputDims() Function\nNext, we’ll make a function to update the input resolution for the model from Unity. The function will take in a width and height value. The output resolution (i.e. the amount of values the model needs to predict) is determined by the input resolution. As a result, the input resolution has a significant impact on both inference speed and output quality.\nOpenVINO provides the InferenceEngine::CNNNetwork::reshape method to update the input dimensions at runtime. This method also propagates the changes down to the outputs.\nTo use it, we first need to create an InferenceEngine::SizeVector variable and assign the new dimensions. We can then pass the SizeVector as input to network.reshape().\nWe’ll also want to initialize the dimensions of the texture variable with the provided width and height values.\n\nCode :\n// Manually set the input resolution for the model\nDLLExport void SetInputDims(int width, int height) {\n\n    // Collect the map of input names and shapes from IR\n    auto input_shapes = network.getInputShapes();\n\n    // Set new input shapes\n    std::string input_name;\n    InferenceEngine::SizeVector input_shape;\n    // create a tuple for accessing the input dimensions\n    std::tie(input_name, input_shape) = *input_shapes.begin();\n    // set batch size to the first input dimension\n    input_shape[0] = 1;\n    // changes input height to the image one\n    input_shape[2] = height;\n    // changes input width to the image one\n    input_shape[3] = width;\n    input_shapes[input_name] = input_shape;\n\n    // Call reshape\n    // Perform shape inference with the new input dimensions\n    network.reshape(input_shapes);\n    // Initialize the texture variable with the new dimensions\n    texture = cv::Mat(height, width, CV_8UC4);\n}\n\n\n\nCreate UploadModelToDevice() Function\nIn this function, we will create an executable version of the network and create an inference request for it. This function will take as input an index for the availableDevices variable. This will allow us to specify and switch between compute devices in the Unity project at runtime.\nOnce we have the inference request, we can get pointers to the input and output tensors using the .GetBlob() method. We need to cast each Blob as a MemoryBlob. The dimensions of the input tensor can be accessed using the minput-&gt;getTensorDesc().getDims() method.\nWe will return the name of the device the model will be executed on back to Unity.\n\nCode :\n// Create an executable network for the target compute device\nDLLExport std::string* UploadModelToDevice(int deviceNum) {\n\n    // Create executable network\n    executable_network = ie.LoadNetwork(network, availableDevices[deviceNum]);\n    // Create an inference request object\n    infer_request = executable_network.CreateInferRequest();\n    \n    // Get a pointer to the input tensor for the model\n    minput = as&lt;MemoryBlob&gt;(infer_request.GetBlob(firstInputName));\n    // Get a pointer to the ouptut tensor for the model\n    moutput = as&lt;MemoryBlob&gt;(infer_request.GetBlob(firstOutputName));\n\n    // Get the number of color channels \n    num_channels = minput-&gt;getTensorDesc().getDims()[1];\n    // Get the number of pixels in the input image\n    size_t H = minput-&gt;getTensorDesc().getDims()[2];\n    size_t W = minput-&gt;getTensorDesc().getDims()[3];\n    nPixels = W * H;\n\n    // Filling input tensor with image data\n    data_img = std::vector&lt;float&gt;(nPixels * num_channels);\n    \n    // Return the name of the current compute device\n    return &availableDevices[deviceNum];;\n}\n\n\n\nCreate PerformInference() Function\nThe last function in our DLL will take a pointer to raw pixel data from a Unity Texture2D as input. It will then prepare the input for the model, execute the model on the target device, process the raw output, and copy the processed output back to the memory location for the raw pixel data from Unity.\nWe first need to assign the inputData to the texture.data. The inputData from Unity will have an RGBA color format. However, the model is expecting an RGB color format. We can use the cv::cvtColor() method to convert the color format for the texture variable.\nWe can get write-only access to the input tensor for the model with minput-&gt;wmap().\nThe pixel values are stored in a different order in the OpenCV Mat compared to the input tensor for the model. The Mat stores the red, green, and blue color values for a given pixel next to each other. In contrast, the input tensor stores all the red values for the entire image next to each other, then the green values, then the blue. We need to take this into account when writing values from texture to the input tensor and when reading values from the output tensor.\nOnce we have updated the input tensor with the current inputData, we can execute the model with infer_request.Infer(). This will execute the model in synchronous mode.\nWhen inference is complete, we can get read-only access to the output tensor with moutput-&gt;rmap().\nValid color values are in the range [0, 255]. However, the model might output values slightly outside of that range. We need to clamp the output values to this range. If we don’t, the output in Unity will look like the image below where pixels near pure black or white are discolored.\n\n\n\n\n\nWe will perform this post processing step using the std::vector&lt;float&gt; data_img we declared earlier, before assigning the values back into texture.\nWe need to use the cv::cvtColor() method again to add an alpha channel back to texture\nFinally, we can copy the pixel data from texture back to the Unity texture data using the std::memcpy() method.\n\nCode :\n// Perform inference with the provided texture data\nDLLExport void PerformInference(uchar* inputData) {\n\n    // Assign the inputData to the OpenCV Mat\n    texture.data = inputData;\n    // Remove the alpha channel\n    cv::cvtColor(texture, texture, cv::COLOR_RGBA2RGB);\n\n    // locked memory holder should be alive all time while access to its buffer happens\n    LockedMemory&lt;void&gt; ilmHolder = minput-&gt;wmap();\n\n    // Filling input tensor with image data\n    auto input_data = ilmHolder.as&lt;PrecisionTrait&lt;Precision::U8&gt;::value_type*&gt;();\n\n    // Iterate over each pixel in image\n    for (size_t p = 0; p &lt; nPixels; p++) {\n        // Iterate over each color channel for each pixel in image\n        for (size_t ch = 0; ch &lt; num_channels; ++ch) {\n            input_data[ch * nPixels + p] = texture.data[p * num_channels + ch];\n        }\n    }\n\n    // Perform inference\n    infer_request.Infer();\n\n    // locked memory holder should be alive all time while access to its buffer happens\n    LockedMemory&lt;const void&gt; lmoHolder = moutput-&gt;rmap();\n    const auto output_data = lmoHolder.as&lt;const PrecisionTrait&lt;Precision::FP32&gt;::value_type*&gt;();\n\n    // Iterate through each pixel in the model output\n    for (size_t p = 0; p &lt; nPixels; p++) {\n        // Iterate through each color channel for each pixel in image\n        for (size_t ch = 0; ch &lt; num_channels; ++ch) {\n            // Get values from the model output\n            data_img[p * num_channels + ch] = static_cast&lt;float&gt;(output_data[ch * nPixels + p]);\n\n            // Clamp color values to the range [0, 255]\n            if (data_img[p * num_channels + ch] &lt; 0) data_img[p * num_channels + ch] = 0;\n            if (data_img[p * num_channels + ch] &gt; 255) data_img[p * num_channels + ch] = 255;\n\n            // Copy the processed output to the OpenCV Mat\n            texture.data[p * num_channels + ch] = data_img[p * num_channels + ch];\n        }\n    }\n\n    // Add alpha channel\n    cv::cvtColor(texture, texture, cv::COLOR_RGB2RGBA);\n    // Copy values from the OpenCV Mat back to inputData\n    std::memcpy(inputData, texture.data, texture.total() * texture.channels());\n}"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#build-solution",
    "href": "posts/openvino-unity-plugin/part-2/index.html#build-solution",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Build Solution",
    "text": "Build Solution\nNow that the code is complete, we just need to build the solution to generate the .dll file.\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. This will generate a new x64 folder in the project’s directory.\n\n\n\n\n\nNavigate to that folder in the File Explorer and open the Release child folder. Inside, you will find the .dll file along with a few other files that will not be needed."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#gather-dependencies",
    "href": "posts/openvino-unity-plugin/part-2/index.html#gather-dependencies",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nThe .dll file generated by our project is still dependent on other .dll files from both OpenVINO and OpenCV. Those .dll files have dependencies of their own as well. We will need to copy these dependencies along with the OpenVINO_Plugin.dll file into a new folder called x86_64 for the Unity project.\nHere are the dependencies needed to use our .dll.\n\nclDNNPlugin.dll\ninference_engine.dll\ninference_engine_ir_reader.dll\ninference_engine_legacy.dll\ninference_engine_lp_transformations.dll\ninference_engine_preproc.dll\ninference_engine_transformations.dll\nlibhwloc-5.dll\nMKLDNNPlugin.dll\nngraph.dll\nopencv_core_parallel_tbb452_64.dll\nopencv_core452.dll\nopencv_imgcodecs452.dll\nopencv_imgproc452.dll\nplugins.xml\ntbb.dll\n\nThe required dependencies can be found in the following directories.\n\nOpenVINO: C:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\inference_engine\\bin\\intel64\\Release\nnGraph: C:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\ngraph\\lib\nTBB: C:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\inference_engine\\external\\tbb\\bin\nOpenCV: C:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\opencv\\bin\n\nYou can download a folder containing the OpenVINO_Plugin.dll file and its dependencies from the link below.\n\nx86_64 folder"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-2/index.html#conclusion",
    "href": "posts/openvino-unity-plugin/part-2/index.html#conclusion",
    "title": "OpenVINO Plugin for Unity Tutorial Pt. 2",
    "section": "Conclusion",
    "text": "Conclusion\nThat is everything we need for the OpenVINO functionality. In the next part, we will demonstrate how to access this functionality as a plugin inside a Unity project.\nProject Resources:\nGitHub Repository\n\nNext: Part 3"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html",
    "href": "posts/openvino-unity-plugin/part-3/index.html",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "",
    "text": "Overview\nCreate New Project\nInstall Barracuda Package\nAdd GUI\nCreate Video Player\nSet Up Style Transfer Code\nEnable Unsafe Code\nAttach Script to Camera\nAssign UI Events\nBuild the Project\nAdd Models Folder\nAdd Plugins folder\nRun the Application\nConclusion"
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#overview",
    "href": "posts/openvino-unity-plugin/part-3/index.html#overview",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of the tutorial, we first installed Unity, OpenVINO, and its prerequisite software. We then demonstrated how to use the python conversion script included with the OpenVINO™ Toolkit to convert a pretrained model from ONNX format to the OpenVINO Intermediate Representation format. In Part 2, we walked through the steps needed to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pretrained deep learning model. In this part, we will demonstrate how to access the DLL as a plugin inside a Unity project."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#create-new-project",
    "href": "posts/openvino-unity-plugin/part-3/index.html#create-new-project",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Create New Project",
    "text": "Create New Project\nOpen Unity Hub and click the New button.\n\n\n\n\n\nStick with the default 3D template and name the project OpenVINO_Plugin_Demo. There appears to be some compatibility issues with the Barracuda library and some of the packages included with the 2D template.\nTake note of where the project will be generated and click Create."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#install-barracuda-package",
    "href": "posts/openvino-unity-plugin/part-3/index.html#install-barracuda-package",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Install Barracuda Package",
    "text": "Install Barracuda Package\nTo benchmark our plugin’s performance, we’ll be comparing the Intel-focused OpenVINO inference engine to Unity’s cross-platform Barracuda inference engine.\nOpen the Window menu at the top of the Unity Editor and select Package Manager.\n\n\n\n\n\nIn the Package Manager window, open the small dropdown menu in the left corner and select Add package from git URL....\n\n\n\n\n\nType com.unity.barracuda into the text box and click Add. This will install Barracuda version 1.0.4. Go ahead and close the Package manager window once Barracuda finishes installing."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#add-gui",
    "href": "posts/openvino-unity-plugin/part-3/index.html#add-gui",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Add GUI",
    "text": "Add GUI\nAt the moment, we cannot use our OpenVINO plugin inside the Unity editor. There appears to be a dependency conflict between one of the dependencies for OpenVINO and the Unity editor. If we look in the folder where the Unit editor executable is located, we can see that the editor also uses tbb.dll. However, it is a different version than the one required for OpenVINO.\nWe will need to build our Unity project without the plugin files, and then add them to the build folder where the project executable file is located.\nSince we can’t use the performance metrics and controls inside the editor, we will need to add our own.\nThe user interface will contain the following:\n\nInput fields for updating the height and width dimensions for the model input\nA dropdown menu for switching between the OpenVINO and Barracuda inference engines\nA dropdown menu for switching between style transfer models\nA toggle to turn image stylization on and off\nA toggle to turn asynchronous GPU readback on and off (more on this later)\nA button to exit the application\nA console readout for debugging\nA frame rate counter to measure performance\n\nWe will be using the free Graphy asset to display frame rate and other performance metrics. Everything else is already set up in the prefab linked below.\n\nCanvas Prefab\n\n\nImport Canvas Prefab\nDownload the Canvas.prefab file from the above link and drop it into the assets folder in the Unity editor.\n\n\n\n\n\nDrag and drop the prefab from the Assets folder into the Hierarchy tab. A TMP Importer popup window will appear. Click Import TMP Essentials. Close the popup window once the import is complete.\n\n\n\n\n\nIf we select the Game tab, we can see the interface we just added. Don’t worry if it looks squished.\n\n\n\n\n\n\n\nAdd Graphy Package\nOpen the link to the Grapy Unity Store page below and click Add to My Assets. You will need to be signed in to your Unity account.\n\nGraphy Asset Store Page: (link)\n\n\n\n\n\n\nBack in Unity, open the Package Manager again and select the Packages: In Project dropdown. Switch the selection to My Assets. You will need to be signed in to view your assets.\n\n\n\n\n\nType Graphy into the search bar and click download.\n\n\n\n\n\nClick Import once the package finished downloading. An Import Unity Package popup window will appear.\n\n\n\n\n\nClick Import in the popup window. Close the Package Manager window once the import is complete. There should now be a new folder called Graphy - Ultimate Stats Monitor in the Assets folder.\n\n\n\n\n\nInside the new folder, open the Prefab folder and drag the [Graphy] prefab into the Hierarchy tab. You will see that our game scene gets updated.\n\n\n\n\n\nWith the [Graphy] object still selected in the Hierarchy tab. Scroll down in the Inspector tab to Graphy Manager (Script) section. Open the Graph modules position dropdown and select TOP_LEFT. Nothing will change in the game view, but the position will be updated when we build the project.\n\n\n\n\n\n\n\nAdd Event System\nWe need to add an Event System so that we can send events to GameObjects and their scripts based on user input. Right click an empty space in the Hierarchy tab. Open the UI submenu and select Event System.\n\n\n\n\n\n\n\nMake a Test Build\nBefore continuing, let’s make a test build to confirm our GUI looks correct. First, press Ctrl+s to save the project.\nOpen the File menu and select Build Settings...\n\n\n\n\n\nClick build in the popup window. You will be prompted to select a folder to store the files generated during the build.\n\n\n\n\n\nCreate a new folder in the default location and name it Build. Click select folder.\n\n\n\n\n\nOnce the build is complete, a File Explorer window will open with the project executable selected. Double-click the executable to run it.\n\n\n\n\n\nYou should get output similar to below. The buttons are not connected to any code just yet, so you will need to press Alt+F4 to exit the application."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#create-video-player",
    "href": "posts/openvino-unity-plugin/part-3/index.html#create-video-player",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Create Video Player",
    "text": "Create Video Player\nFor this demo we’ll just have a video file play in the background. While this demo can run on older integrated graphics, the frame rate would not be high enough to interact with a game environment. The newer discrete GPUs from Intel would be needed for playable frame rates. Since those are not widely available at the time of writing, we’ll stick with a video.\n\nImport Video Files\nWe’ll be using this video available on Pexels, a free stock photos & videos site. Download the video in Full HD resolution.\n\nWoman Dancing\nNote: Renamed to pexels_woman_dancing\n\n\n\nCreate the Videos Folder\nIn the Assets window, right-click an empty space, select the Create option, and click Folder. Name the folder Videos.\n\n\n\n\n\nDouble-click the Videos folder to open it.\n\n\nAdd Video Files\nDrag and drop the video file from the file explorer into the Videos folder.\n\n\n\n\n\n\n\nCreate the Screen GameObject\nWe need to make a “screen” in Unity to watch the video. We’ll use a Quad object for the screen. Right click an empty space in the Hierarchy tab, select the 3D Object section and click Quad. We can just name it VideoScreen.\n\n\n\n\n\n\n\nResize the Screen\nWith the VideoScreen object selected, we need to adjust the Scale parameter in the Inspector tab. Set the X value to 16 and the Y value to 9. Leave the Z value at 1.\n\n\n\n\n\n\n\nUpdate Screen Material\nCreate a new folder in the Assets section named Materials. Inside the Materials folder, right-click an empty space and select Material from the popup menu. We’ll name the new material video.\n\n\n\n\n\nWith the VideoScreen object selected in the Hierarchy tab, open the Materials dropdown in the Inspector tab. Drag and drop the video material into the Element 0 slot.\n\n\n\n\n\nWith the VideoScreen object still selected, scroll down to the bottom of the Inspector tab and open the Shader dropdown menu. Select the Unlit option.\n\n\n\n\n\nSelect Texture from the submenu. The video screen should now be white.\n\n\n\n\n\n\n\nCreate the Video Player\nIn the Hierarchy tab, right-click an empty area, select the Video section, and click Video Player. This will create a new GameObject called Video Player.\n\n\n\n\n\n\n\nSet Video Clip\nSelect the Video Player object in the Hierarchy tab. Then, drag and drop the pexels_woman_dancing file into the Video Clip parameter in the Inspector tab.\n\n\n\n\n\n\n\nMake the Video Loop\nTick the Loop checkbox in the Inspector tab to make the video repeat when the project is running.\n\n\n\n\n\n\n\nSet Render Mode\nOpen the Render Mode dropdown and Material Override.\n\n\n\n\n\n\n\nSpecify Renderer\nWith the Video Player object still selected, drag and drop the VideoScreen object from the Hierarchy tab into the Render slot in the Inspector tab.\n\n\n\n\n\n\n\nAdjust the Camera\nBefore playing the video, we need to reposition and resize the Main Camera object.\nSelect the Main Camera object in the Hierarchy tab. Set the Y value for the Position to Y and the Z value for the Position to -9.\nFinally, we need to set the Projection setting to Orthographic and adjust the Size parameter to 4.5 in the Inspector tab."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#set-up-style-transfer-code",
    "href": "posts/openvino-unity-plugin/part-3/index.html#set-up-style-transfer-code",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Set Up Style Transfer Code",
    "text": "Set Up Style Transfer Code\nNow, we can implement the code for using both the Barracuda and OpenVINO inference engines.\n\nCreate Style Transfer Folder\nWe’ll place all our additions to the project in a new asset folder called Style_Transfer. This will help keep things organized.\n\n\nImport ONNX Files to Assets\nNext, we need to import the original ONN file that we used earlier. This will be used by the Barracuda inference engine. Open the Style_Transfer folder and make a new folder called Models.\nDrag and drop the ONNX file into the Models folder.\n\n\n\n\n\n\n\nStyleTransferShader Compute Shader\nWe can perform both the preprocessing and postprocessing operations for the Barracuda model on the GPU since both the input and output are images. We’ll implement these steps in a compute shader.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Shaders. Enter the Shaders folder and right-click an empty space. Select Shader in the Create submenu and click Compute Shader. We’ll name it StyleTransferShader.\nimage here\n\n\n\n\n\n\n\nRemove the Default Code\nOpen the StyleTransferShader in your code editor. By default, the ComputeShader will contain the following.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\nDelete the CSMain function along with the #pragma kernel CSMain. Next, we need to add a Texture2D variable to store the input image. Name it InputImage and give it a data type of &lt;half4&gt;. Use the same data type for the Result variable as well.\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n\nCreate ProcessInput Function\nThe style transfer models expect RGB channel values to be in the range [0, 255]. Color values in Unity are in the range [0,1]. Therefore, we need to scale the three channel values for the InputImage by 255. We’ll perform this step in a new function called ProcessInput as shown below.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel ProcessInput\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n[numthreads(4, 4, 1)]\nvoid ProcessInput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((InputImage[id.xy].x * 255.0h),\n        (InputImage[id.xy].y * 255.0h),\n        (InputImage[id.xy].z * 255.0h), 1.0h);\n}\n\n\nCreate ProcessOutput Function\nAs we did in the DLL, we need to clamp the model output values to the range [0, 255]. We can use the built-in clamp() method to make sure all values are in the correct range. We’ll then scale the values back down to [0, 1] for Unity. We’ll perform these steps in a new function called ProcessOutput as shown below.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel ProcessInput\n#pragma kernel ProcessOutput\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n[numthreads(4, 4, 1)]\nvoid ProcessInput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((InputImage[id.xy].x * 255.0h),\n        (InputImage[id.xy].y * 255.0h),\n        (InputImage[id.xy].z * 255.0h), 1.0h);\n}\n\n[numthreads(4, 4, 1)]\nvoid ProcessOutput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((clamp(InputImage[id.xy].x, 0.0h, 255.0h) / 255.0h),\n        (clamp(InputImage[id.xy].y, 0.0f, 255.0h) / 255.0h),\n        (clamp(InputImage[id.xy].z, 0.0f, 255.0h) / 255.0h), 1.0h);\n}\n\n\nCreate FlipXAxis() Function\nThe pixel data from Unity gets flipped when loaded into the cv::Mat texture variable in the DLL. We will need to flip the image before sending it to the plugin so that it will be in the correct orientation for the model. We will need to flip the image again when we receive the update pixel data back from the DLL.\nWe need to know the height and width of the image to swap the pixel values. We’ll store these values in int variables. We’ll store the new (x,y) coordinates for individual pixel values in an int2 variable.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel ProcessInput\n#pragma kernel ProcessOutput\n#pragma kernel FlipXAxis\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n[numthreads(4, 4, 1)]\nvoid ProcessInput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((InputImage[id.xy].x * 255.0h),\n        (InputImage[id.xy].y * 255.0h),\n        (InputImage[id.xy].z * 255.0h), 1.0h);\n}\n\n[numthreads(4, 4, 1)]\nvoid ProcessOutput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((clamp(InputImage[id.xy].x, 0.0h, 255.0h) / 255.0h),\n        (clamp(InputImage[id.xy].y, 0.0f, 255.0h) / 255.0h),\n        (clamp(InputImage[id.xy].z, 0.0f, 255.0h) / 255.0h), 1.0h);\n}\n    \n// The height of the input image\nint height;\n// The width of the input image\nint width;\n// Stores the new location for individual pixel values\nint2 coords;\n\n[numthreads(4, 4, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Update the y value for the pixel coordinates\n    coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}\n\n\n\nCreate StyleTransfer Script\nWe need to make a new C# script to perform inference with the style transfer model. This script will load the model, process the input, run the model, and process the output.\n\nCreate the Asset File\nOpen the Style_Transfer folder and create a new folder called Scripts. In the Scripts folder, right-click an empty space and select C# Script in the Create submenu. Name the script StyleTransfer.\n\n\n\n\n\n\n\nAdd Required Namespaces\nWe will start by adding the required namespaces at the top of the script.\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nSystem.Runtime.InteropServices: Provides a wide variety of members that support COM interop and platform invoke services.\nUnityEngine.UI: Provides access to UI elements.\nUnity.Barracuda: Provides access to the Barracuda library API.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\n\n\nCode\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing System;\nusing System.Runtime.InteropServices;\nusing UnityEngine.UI;\nusing Unity.Barracuda;\nusing UnityEngine.Rendering;\n\n\n\nAdd Public Variables\nNext, we add the publicly accessible variables inside the StyleTransfer class.\nWe need to add a ComputeShader variable so that we can access the StyleTransferShader we made earlier.\nThere are several parts of the user interface that we will need to access inside the script.\n\ninference engine dropdown\ncompute device dropdown\nmodel dropdown\nstylize toggle\nuseAsync toggle\ninput resolution fields\nconsole readout text\n\nThe last public variables will be a list of NNmodel variables to store the ONNX models for the Barracuda engine and a WorkerFactory.Type variable to specify the backend for the Barracuda engine.\n\nCode\npublic class StyleTransfer : MonoBehaviour\n{\n    [Tooltip(\"Performs the preprocessing and postprocessing steps\")]\n    public ComputeShader styleTransferShader;\n\n    [Tooltip(\"Toggle between OpenVINO and Barracuda\")]\n    public TMPro.TMP_Dropdown inferenceEngineDropdown;\n\n    [Tooltip(\"Switch between the available compute devices for OpenVINO\")]\n    public TMPro.TMP_Dropdown deviceDropdown;\n\n    [Tooltip(\"Switch between the available OpenVINO models\")]\n    public TMPro.TMP_Dropdown modelDropdown;\n\n    [Tooltip(\"Turn stylization on and off\")]\n    public Toggle stylize;\n\n    [Tooltip(\"Turn AsyncGPUReadback on and off\")]\n    public Toggle useAsync;\n\n    [Tooltip(\"Text box for the input width\")]\n    public TMPro.TMP_InputField widthText;\n    [Tooltip(\"Text box for the input height\")]\n    public TMPro.TMP_InputField heightText;\n    \n    [Tooltip(\"Text area to display console output\")]\n    public Text consoleText;\n\n    [Tooltip(\"The model asset file that will be used when performing inference\")]\n    public NNModel[] modelAssets;\n\n    [Tooltip(\"The backend used when performing inference\")]\n    public WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\n\n\n\nCreate DLL Method Declarations\nWe need to specify the name of the .dll file. We’ll store this in a const string variable called dll.\nWe can indicate that a given method is from a DLL by using the DllImport attribute.\nNote: The DLL functions that return string values have IntPtr as their return values here. We will use the Marshal.PtrToStringAnsi() method to get that string value at the memory location stored in the pointer.\nWe will be accessing the following methods from the DLL:\n\nGetAvailableDevices\nInitializeOpenVINO\nSetInputDims\nPrepareBlobs\nUploadModelToDevice\nPerformInference\n\n\nCode\n\n    // Name of the DLL file\n    const string dll = \"OpenVINO_Plugin\";\n\n    [DllImport(dll)]\n    private static extern IntPtr GetAvailableDevices();\n\n    [DllImport(dll)]\n    private static extern void InitializeOpenVINO(string modelPath);\n\n    [DllImport(dll)]\n    private static extern bool SetInputDims(int width, int height);\n\n    [DllImport(dll)]\n    private static extern void PrepareBlobs();\n\n    [DllImport(dll)]\n    private static extern IntPtr UploadModelToDevice(int deviceNum = 0);\n\n    [DllImport(dll)]\n    private static extern void PerformInference(IntPtr inputData);\n\n\n\nAdd Private Variables\nLastly, we will add the variables that do not need to be publicly accessible.\nThe models in the modelAssets variable will need to be compiled into runtime models to perform inference with them. We’ll store the compiled models in a new list of Model variables called m_RuntimeModels.\nWe will create a separate inference engine interface for each of the Barracuda models. These will be stored in a list of IWorker variables called engines.\nWe will need to resize and flip the input image in Unity before sending it to the OpenVINO plugin. We’ll perform these operations using a temporary RenderTexture variable called tempTex.\nFor this tutorial, the pixel data currently needs to be downloaded from the GPU to the CPU before sending it to the plugin as well. We’ll read the pixel data from tempTex to a Texture2D variable called inputTex.\nWe will store the raw pixel data from inputTex in a byte[] variable called inputData. We will be passing a pointer to this array as input to the PerformInference() method.\nNext , we will create two int variables to store the current width and height values for the input dimensions. We’ll give the width a default value of 960 and height a default value of 540.\nWe also need two string variables to store the unparsed list of available devices for OpenVINO and the current device being used.\nFinally, we need four List&lt;string&gt; variables to store the parsed list of available devices, the file paths for the OpenVINO models, the names of the OpenVINO models, and the names of the ONNX models.\n\nCode\n    // The compiled model used for performing inference\n    private Model[] m_RuntimeModels;\n\n    // The interface used to execute the neural network\n    private IWorker[] engines;\n\n    // Contains the resized input texture\n    private RenderTexture tempTex;\n    // Contains the input texture that will be sent to the OpenVINO inference engine\n    private Texture2D inputTex;\n    // Stores the raw pixel data for inputTex\n    private byte[] inputData;\n\n    // Input image width\n    private int width = 960;\n    // Input image height\n    private int height = 540;\n\n    // Unparsed list of available compute devices for OpenVINO\n    private string openvinoDevices;\n    // Current compute device for OpenVINO\n    private string currentDevice;\n    // Parsed list of compute devices for OpenVINO\n    private List&lt;string&gt; deviceList = new List&lt;string&gt;();\n\n    // File paths for the OpenVINO IR models\n    private List&lt;string&gt; openVINOPaths = new List&lt;string&gt;();\n    // Names of the OpenVINO IR model\n    private List&lt;string&gt; openvinoModels = new List&lt;string&gt;();\n    // Names of the ONNX models\n    private List&lt;string&gt; onnxModels = new List&lt;string&gt;();\n\n\n\nDefine Start() Method\nHere we will perform all the set up steps needed to initialize the user interface, OpenVINO plugin, and Barracuda inference engine.\nNote: We are assuming that the OpenVINO models are stored in the same directory as the application executable, in a folder named models.\n\n\nMethod Steps\n\nGet the name of the CPU and GPU\nCreate a list of the available OpenVINO IR models\nUpdate model dropdown menu with list of OpenVINO models\nClear list of available compute devices\nCheck if either the CPU or GPU is made by Intel\n\nif true, initialize OpenVINO and update the list of compute devices\nif false, use the Barracuda inference engine\n\nInitialize the tempTex and inputTex variables with the default resolution\nInitialize the m_RuntimeModels variable\nInitialize the engines variable\nCompile the models in the modelAssets variable and initialize the inference engines for each.\n\n\nCode\n    // Start is called before the first frame update\n    void Start()\n    {\n        string processorType = SystemInfo.processorType.ToString();\n        Debug.Log($\"Processor Type: {processorType}\");\n        string graphicsDeviceName = SystemInfo.graphicsDeviceName.ToString();\n        Debug.Log($\"Graphics Device Name: {graphicsDeviceName}\");\n\n        string[] openVINOFiles = System.IO.Directory.GetFiles(\"models\");\n        Debug.Log(\"Available OpenVINO Models\");\n        foreach (string file in openVINOFiles)\n        {\n            if (file.EndsWith(\".xml\"))\n            {\n                Debug.Log(file);\n                openVINOPaths.Add(file);\n                string modelName = file.Split('\\\\')[1];\n                openvinoModels.Add(modelName.Substring(0, modelName.Length-4));\n            }\n        }\n\n        // Remove default dropdown options\n        modelDropdown.ClearOptions();\n        // Add OpenVINO models to menu\n        modelDropdown.AddOptions(openvinoModels);\n        // Select the first option in the dropdown\n        modelDropdown.SetValueWithoutNotify(0);\n\n        // Remove default dropdown options\n        deviceDropdown.ClearOptions();\n\n        // Check if either the CPU of GPU is made by Intel\n        if (processorType.Contains(\"Intel\") || graphicsDeviceName.Contains(\"Intel\"))\n        {\n            Debug.Log(\"Initializing OpenVINO\");\n            InitializeOpenVINO(openVINOPaths[0]);\n            Debug.Log($\"Setting Input Dims to W: {width} x H: {height}\");\n            SetInputDims(width, height);\n            Debug.Log(\"Uploading IR Model to Compute Device\");\n            currentDevice = Marshal.PtrToStringAnsi(UploadModelToDevice());\n            Debug.Log($\"OpenVINO using: {currentDevice}\");\n\n            // Get an unparsed list of available \n            openvinoDevices = Marshal.PtrToStringAnsi(GetAvailableDevices());\n            \n            Debug.Log($\"Available Devices:\");\n            // Parse list of available compute devices\n            foreach (string device in openvinoDevices.Split(','))\n            {\n                // Add device name to list\n                deviceList.Add(device);\n                Debug.Log(device);\n            }\n\n            // Add OpenVINO compute devices to dropdown\n            deviceDropdown.AddOptions(deviceList);\n            // Set the value for the dropdown to the current compute device\n            deviceDropdown.SetValueWithoutNotify(deviceList.IndexOf(currentDevice));\n        }\n        else\n        {\n            // Use Barracuda inference engine if not on Intel hardware\n            inferenceEngineDropdown.SetValueWithoutNotify(1);\n        }\n        \n        // Initialize textures with default input resolution\n        tempTex = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.ARGB32);\n        inputTex = new Texture2D(width, height, TextureFormat.RGBA32, false);\n\n        // Initialize list of Barracuda models\n        m_RuntimeModels = new Model[modelAssets.Length];\n        // Initialize list of Barracuda inference engines\n        engines = new IWorker[modelAssets.Length];\n        for(int i = 0; i &lt; modelAssets.Length; i++)\n        {\n            // Add names of available ONNX models to list\n            onnxModels.Add(modelAssets[i].name);\n            Debug.Log($\"ModelAsset Name: {modelAssets[i].name}\");\n            // Compile the model asset into an object oriented representation\n            m_RuntimeModels[i] = ModelLoader.Load(modelAssets[i]);\n            // Create a worker that will execute the model with the selected backend\n            engines[i] = WorkerFactory.CreateWorker(workerType, m_RuntimeModels[i]);\n        }\n    }\n\n\n\nCreate ProcessImage() Method\nNext, we’ll make a new method to execute the ProcessInput(), ProcessOutput() and FlipXAxis() functions in our ComputeShader. This method will take in the image that needs to be processed as well as a function name to indicate which function we want to execute. We’ll need to store the processed images in textures with HDR formats. This will allow us to use color values outside the default range of [0, 1]. As mentioned previously, the model expects values in the range of [0, 255].\n\nMethod Steps\n\nGet the ComputeShader index for the specified function\nCreate a temporary RenderTexture with random write access enabled to store the processed image\nExecute the ComputeShader\nCopy the processed image back into the original RenderTexture\nRelease the temporary RenderTexture\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Process the provided image using the specified function on the GPU\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"image\"&gt;&lt;/param&gt;\n    /// &lt;param name=\"functionName\"&gt;&lt;/param&gt;\n    /// &lt;returns&gt;The processed image&lt;/returns&gt;\n    private void ProcessImage(RenderTexture image, string functionName)\n    {\n        // Specify the number of threads on the GPU\n        int numthreads = 4;\n        // Get the index for the specified function in the ComputeShader\n        int kernelHandle = styleTransferShader.FindKernel(functionName);\n        // Define a temporary HDR RenderTexture\n        RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n        // Enable random write access\n        result.enableRandomWrite = true;\n        // Create the HDR RenderTexture\n        result.Create();\n\n        // Set the value for the Result variable in the ComputeShader\n        styleTransferShader.SetTexture(kernelHandle, \"Result\", result);\n        // Set the value for the InputImage variable in the ComputeShader\n        styleTransferShader.SetTexture(kernelHandle, \"InputImage\", image);\n        // Set the value for the height variable in the ComputeShader\n        styleTransferShader.SetInt(\"height\", image.height);\n        // Set the value for the width variable in the ComputeShader\n        styleTransferShader.SetInt(\"width\", image.width);\n\n        // Execute the ComputeShader\n        styleTransferShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n        // Copy the result into the source RenderTexture\n        Graphics.Blit(result, image);\n\n        // Release the temporary RenderTexture\n        RenderTexture.ReleaseTemporary(result);\n    }\n\n\n\nCreate StylizeImage() Method\nWe’ll create a new method to handle stylizing individual frames from the camera with the Barracuda engine. This method will take in the src RenderTexture from the game camera and copy the stylized image back into that same RenderTexture.\n\nMethod Steps:\n\nResize the camera input to the targetHeight\nIf the height of src is larger than the targetHeight, we’ll calculate the new dimensions to downscale the camera input. We’ll then adjust the new dimensions to be multiples of 8. This is to make sure we don’t lose parts of the image after applying the processing steps with the Compute shader.\nApply preprocessing steps to the image\nWe’ll call the ProcessImage() method and pass rTex along with the name for the ProcessInput() function in the ComputeShader. The result will be stored in rTex.\nExecute the model\nWe’ll use the engine.Execute() method to run the model with the current input. We can store the raw output from the model in a new Tensor.\nApply the postprocessing steps to the model output\nWe’ll call the ProcessImage() method and pass rTex along with the name for the ProcessOutput() function in the ComputeShader. The result will be stored in rTex.\nCopy the stylized image to the src RenderTexture\nWe’ll use the Graphics.Blit() method to copy the final stylized image into the src RenderTexture.\nRelease the temporary RenderTexture\nFinally, we’ll release the temporary RenderTexture.\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Stylize the provided image\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"src\"&gt;&lt;/param&gt;\n    /// &lt;returns&gt;&lt;/returns&gt;\n    private void StylizeImage(RenderTexture src)\n    {\n        // Create a new RenderTexture variable\n        RenderTexture rTex;\n\n        // Check if the target display is larger than the height\n        // and make sure the height is at least 4\n        if (src.height &gt; height && height &gt;= 4)\n        {\n            // Adjust the target image dimensions to be multiples of 4\n            int targetHeight = height - (height % 4);\n            int targetWidth = width - (width % 4);\n\n            // Assign a temporary RenderTexture with the new dimensions\n            rTex = RenderTexture.GetTemporary(targetWidth, targetHeight, 24, src.format);\n        }\n        else\n        {\n            // Assign a temporary RenderTexture with the src dimensions\n            rTex = RenderTexture.GetTemporary(src.width, src.height, 24, src.format);\n        }\n\n        // Copy the src RenderTexture to the new rTex RenderTexture\n        Graphics.Blit(src, rTex);\n\n        // Apply preprocessing steps\n        ProcessImage(rTex, \"ProcessInput\");\n\n        // Create a Tensor of shape [1, rTex.height, rTex.width, 3]\n        Tensor input = new Tensor(rTex, channels: 3);\n\n        // Execute neural network with the provided input\n        //engine.Execute(input);\n        engines[modelDropdown.value].Execute(input);\n\n        // Get the raw model output\n        Tensor prediction = engines[modelDropdown.value].PeekOutput();\n\n        // Release GPU resources allocated for the Tensor\n        input.Dispose();\n\n        // Make sure rTex is not the active RenderTexture\n        RenderTexture.active = null;\n        // Copy the model output to rTex\n        prediction.ToRenderTexture(rTex);\n        // Release GPU resources allocated for the Tensor\n        prediction.Dispose();\n\n        // Apply post processing steps\n        ProcessImage(rTex, \"ProcessOutput\");\n        // Copy rTex into src\n        Graphics.Blit(rTex, src);\n\n        // Release the temporary RenderTexture\n        RenderTexture.ReleaseTemporary(rTex);\n    }\n\n\n\nCreate SetInferenceEngine() Method\nThis method will be called when a new option is selected from the inference engine dropdown menu.\n\nMethod Steps\n\nCreate a temporary list of the models for the selected inference engine\nUpdate the model dropdown menu with the temporary list\nCheck if the model that was last used for the previous inference engine is available for the selected inference engine.\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called when an inference engine option is selected from dropdown\n    /// &lt;/summary&gt;\n    public void SetInferenceEngine()\n    {\n        // Get the list of models for the selected inference engine\n        List&lt;string&gt; currentList = inferenceEngineDropdown.value == 0 ? openvinoModels : onnxModels;\n\n        // Remove current dropdown options\n        modelDropdown.ClearOptions();\n        // Add current inference engine models to menu\n        modelDropdown.AddOptions(currentList);\n        // Select the first option in the dropdown\n        modelDropdown.SetValueWithoutNotify(0);\n        \n        // Get index for current model selection\n        int index = modelDropdown.value;\n        // Get the name for the previously selected inference engine model\n        string previousModel = inferenceEngineDropdown.value == 0 ? onnxModels[index] : openvinoModels[index];\n        // Check if the model for the previous inference engine is available for the current engine\n        if (currentList.Contains(previousModel))\n        {\n            modelDropdown.SetValueWithoutNotify(openvinoModels.IndexOf(previousModel));\n        }\n    }\n\n\n\nCreate SetDevice() Method\nThis method will be called when a new option is selected from the device dropdown menu.\n\nMethod Steps\n\nCall UploadModelToDevice() method and save the returned value to currentDevice\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called when a compute device is selected from dropdown\n    /// &lt;/summary&gt;\n    public void SetDevice()\n    {\n        // Uploading model to device\n        currentDevice = Marshal.PtrToStringAnsi(UploadModelToDevice(deviceDropdown.value));\n    }\n\n\n\nCreate UpdateInputDims() Method\nThis method will be called when the input dimensions are updated from the user interface.\n\nMethod Steps\n\nGet integer values from user input for the height and width input fields\nUpdate tempTex and inputTex with the new height and width values\nCall SetInputDims() method.\nCall SetDevice() method\nCall PrepareBlobs() method\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called when the input resolution is updated\n    /// &lt;/summary&gt;\n    public void UpdateInputDims()\n    {\n        // Get the integer value from the width input\n        int.TryParse(widthText.text, out width);\n        // Get the integer value from the height input\n        int.TryParse(heightText.text, out height);\n\n        // Update tempTex with the new dimensions\n        tempTex = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.ARGB32);\n        // Update inputTex with the new dimensions\n        inputTex = new Texture2D(width, height, TextureFormat.RGBA32, false);\n        \n\n        // Set input resolution width x height for the OpenVINO model\n        Debug.Log($\"Setting Input Dims to W: {width} x H: {height}\");\n        SetInputDims(width, height);\n        SetDevice();\n\n        // Preparing Output Blobs\n        PrepareBlobs();\n    }\n\n\n\nCreate UpdateModel() Method\nThis method will be called when a new option is selected from the model dropdown menu.\n\nMethod Steps\n\nCheck if the OpenVINO engine is currently selected\n\nCall InitializeOpenVINO() for selected the model.\nCall UpdateInputDims() method\n\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called when a model option is selected from the dropdown\n    /// &lt;/summary&gt;\n    public void UpdateModel()\n    {\n        Debug.Log($\"Selected Model: {modelDropdown.value}\");\n        // Initialize the selected OpenVINO model\n        if (inferenceEngineDropdown.value == 0)\n        {\n            InitializeOpenVINO(openVINOPaths[modelDropdown.value]);\n            UpdateInputDims();\n        }\n    }\n\n\n\nCreate UpdateTexture() Method\nThis method is where we will send the current pixel data from inputTex to the OpenVINO plugin. We need to use the unsafe keyword since we’ll be creating a pointer to the inputData array. Unity does not allow unsafe code by default, so we will need to enable it in the Project Settings.\n\nMethod Steps\n\nPin the memory for inputData using a fixed statement and get a pointer to the variable\nCall the PerformInference() method with the pointer as input\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Pin memory for the input data and send it to OpenVINO for inference\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"inputData\"&gt;&lt;/param&gt;\n    public unsafe void UpdateTexture(byte[] inputData)\n    {\n        //Pin Memory\n        fixed (byte* p = inputData)\n        {\n            // Perform inference with OpenVINO\n            PerformInference((IntPtr)p);\n        }\n    }\n\n\n\nCreate OnCompleteReadback() Callback\nAs mentioned earlier, we currently need to download the pixel data for tempTex from the GPU to the CPU. This normally causes a pipeline stall as Unity prevents any execution on the main thread to prevent the data from changing before it has finished downloading to the CPU. This can cause a noticeable performance bottleneck that increases with the amount of pixel data there is to download.\nUnity provides an alternative approach with AsyncGPUReadback that does not block the main thread. However it adds a few frames of latency. This may or may not matter depending on the specific application.\nThis function will be called once the AsyncGPUReadback has completed. We can load the raw pixel data from the request directly to inputTex.\nNote: It may be possible to access the underlying DirectX11 texture data directly using OpenVINO’s Remote Blob API. However, this potential approach will need to be explored in a future post.\n\nMethod Steps\n\nCheck if an error occurred during the GPU readback.\nLoad the raw texture data from the readback into inputTex.\nApply the changes to inputTex.\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called once AsyncGPUReadback has been completed\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"request\"&gt;&lt;/param&gt;\n    void OnCompleteReadback(AsyncGPUReadbackRequest request)\n    {\n        if (request.hasError)\n        {\n            Debug.Log(\"GPU readback error detected.\");\n            return;\n        }\n\n        // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n        inputTex.LoadRawTextureData(request.GetData&lt;uint&gt;());\n        // Apply changes to Texture2D\n        inputTex.Apply();\n    }\n\n\n\nDefine OnRenderImage() Method\nThe OnRenderImage() method gets called after the Camera has finished rendering. This gives us access to the RenderTexture for the game camera as well as the RenderTexture for the target display. It allows us to stylize the final image from the in-game that would be displayed to the user. We’ll only stylize the current frame if the stylize toggle is on.\n\nMethod Steps:\n\nCheck if the stylize UI toggle is ticked.\n\nCheck which inference engine is currently selected\n\nOpenVINO:\n\nCopy the RenderTexture for the camera to tempTex\nFlip the image\nRead pixel data from tempTex to inputTex either asynchronously or synchronously\nCall UpdateTexture() method\nLoad the updated raw pixel data from inputData to inputTex\nApply the changes to inputTex\nCopy the pixel data from inputTex to tempTex\nFlip the image again\nCopy the pixel data from tempTex to the RenderTexture for the camera\n\nBarracuda:\n\nCall StylizeImage() method\n\n\nCopy the RenderTexture for the camera to the RenderTexture for the target display.\n\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// OnRenderImage is called after the Camera had finished rendering \n    /// &lt;/summary&gt;\n    /// &lt;param name=\"src\"&gt;Input from the Camera&lt;/param&gt;\n    /// &lt;param name=\"dest\"&gt;The texture for the target display&lt;/param&gt;\n    void OnRenderImage(RenderTexture src, RenderTexture dest)\n    {\n        // Only stylize current frame if Stylize toggle is on\n        if (stylize.isOn)\n        {\n            // Check which inference engine to use\n            if (inferenceEngineDropdown.value == 0)\n            {\n                // Copy current frame to smaller temporary texture\n                Graphics.Blit(src, tempTex);\n                // Flip image before sending to DLL\n                ProcessImage(tempTex, \"FlipXAxis\");\n\n                if (useAsync.isOn)\n                {\n                    AsyncGPUReadback.Request(tempTex, 0, TextureFormat.RGBA32, OnCompleteReadback);\n                }\n                else\n                {\n                    RenderTexture.active = tempTex;\n                    inputTex.ReadPixels(new Rect(0, 0, tempTex.width, tempTex.height), 0, 0);\n                    inputTex.Apply();\n                }\n\n                // Get raw data from Texture2D\n                inputData = inputTex.GetRawTextureData();\n                // Send reference to inputData to DLL\n                UpdateTexture(inputData);\n                // Load the new image data from the DLL to the texture\n                inputTex.LoadRawTextureData(inputData);\n                // Apply the changes to the texture\n                inputTex.Apply();\n                // Copy output image to temporary texture\n                Graphics.Blit(inputTex, tempTex);\n                // Flip output image from DLL\n                ProcessImage(tempTex, \"FlipXAxis\");\n                // Copy the temporary texture to the source resolution texture\n                Graphics.Blit(tempTex, src);\n            }\n            else\n            {\n                StylizeImage(src);\n            }\n        }\n        \n        Graphics.Blit(src, dest);\n    }\n\n\n\nCreate Log() Method\nSince we can’t run the project in the Unity Editor with the plugin, we don’t have direct access to the console. However, we can capture the text that gets sent to the console and display it in the user interface. This can be useful for debugging purposes.\n\nMethod Steps\n\nAdd the most recent console text to the console readout text area.\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Updates on screen console text\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"logString\"&gt;&lt;/param&gt;\n    /// &lt;param name=\"stackTrace\"&gt;&lt;/param&gt;\n    /// &lt;param name=\"type\"&gt;&lt;/param&gt;\n    public void Log(string logString, string stackTrace, LogType type)\n    {\n        consoleText.text = consoleText.text + \"\\n \" + logString;\n    }\n\n\n\nDefine OnEnable() Method\nThe OnEnable() method is called when the GameObject the script is attached to becomes enabled or active.\n\nMethod Steps\n\nAdd Log() method to Application.logMessageReceived event.\n\n\n\nCode\n    // Called when the object becomes enabled and active\n    void OnEnable()\n    {\n        Application.logMessageReceived += Log;\n    }\n\n\n\nDefine OnDestroy() Method\nWe need to manually release the resources that get allocated for the inference engine interfaces and release the temporary RenderTexture. This should be one of the last actions performed. Therefore, we will do it in the OnDestroy() method.\n\nMethod Steps\n\nRemove the inputTex object.\nRelease the tempTex RenderTexture.\nRelease the resources allocated for each Barracuda inference engine interface.\nRemove the Log function from the Application.logMessageReceived event.\n\n\n\nCode\n    // Called when the MonoBehaviour will be destroyed\n    private void OnDestroy()\n    {\n        Destroy(inputTex);\n        RenderTexture.ReleaseTemporary(tempTex);\n\n        // Release the resources allocated for the inference engines\n        foreach (IWorker engine in engines)\n        {\n            engine.Dispose();\n        }\n        \n        Application.logMessageReceived -= Log;\n    }\n\n\n\nCreate Quit() Method\nThis method will be called when the Quit button is clicked in the user interface and will cause the application to exit.\n\nMethod Steps\n\nCall the Application.Quit() method.\n\n\n\nCode\n    /// &lt;summary&gt;\n    /// Called when the Quit button is clicked.\n    /// &lt;/summary&gt;\n    public void Quit()\n    {\n        // Causes the application to exit\n        Application.Quit();\n    }\nThat takes care of the required code for this project. However, we still need to enable unsafe code in the Project Settings for the UpdateTexture() method to be allowed."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#enable-unsafe-code",
    "href": "posts/openvino-unity-plugin/part-3/index.html#enable-unsafe-code",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Enable Unsafe Code",
    "text": "Enable Unsafe Code\nOpen the Edit menu at the top of the Unity Editor and select Project Settings.\n\nSelect Player from the side menu and open the Other Settings dropdown menu.\n\n\n\n\n\nScroll down until you see the Allow 'unsafe' Code checkbox. Enable the setting and close the Project Settings window."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#attach-script-to-camera",
    "href": "posts/openvino-unity-plugin/part-3/index.html#attach-script-to-camera",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Attach Script to Camera",
    "text": "Attach Script to Camera\nTo run the StyleTransfer script, we need to attach it to the active Camera in the scene.\nSelect the Main Camera object in the Hierarchy Tab and click the little lock icon at the top of the Inspector tab. This will keep the Main Camera in the Inspector tab.\n\n\n\n\n\nDrag and drop the StyleTransfer.cs into the bottom of the Inspector tab. Now can assign the UI elements inside the Canvas object as well as the compute shader and any ONNX models. We can do so by dragging dropping them into their associated variable spots in the Inspector tab.\nThe actual text area for the Console View object is located at Console View &gt; Viewport &gt; Content in the Hierarchy tab.\n\n\n\n\n\nWe also need to select the Compute Precompiled option from the Worker Type dropdown menu.\nimage here\n\n\n\n\n\nThe fully populated script should look like the image below.\n\n\n\n\n\nClick the lock icon again to unlock the Inspector tab."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#assign-ui-events",
    "href": "posts/openvino-unity-plugin/part-3/index.html#assign-ui-events",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Assign UI Events",
    "text": "Assign UI Events\nNow we just need to assign the associated methods from the StyleTransfer.cs script to the UI event triggers.\n\nUpdate Button\nSelect the Update object inside the Canvas object.\n\n\n\n\n\nScroll down in the Inspector tab to the On Click section.\n\n\n\n\n\nDrag and drop the Main Camera object into the None (Object) spot. Open the No Function dropdown menu. Open the StyleTransfer option and select UpdateInputDims().\n\n\n\n\n\n\n\nInference Engine Dropdown\nSelect the Inference Engines object inside the Canvas object.\nScroll down in the Inspector tab to the On Value Changed (Int32) section and press the little plus sign.\n\n\n\n\n\nDrag and drop the Main Camera object into the None (Object) spot.\n\n\n\n\n\nThis time, select the StyleTransfer.SetInferenceEngine method.\n\n\n\n\n\n\n\nDevice Dropdown\nSelect the Device object inside the Canvas object and follow the same steps as the inference engine dropdown. This time, select the StyleTransfer.SetDevice method.\n\n\n\n\n\n\n\nModel Dropdown\nSelect the Model object inside the Canvas object and follow the same steps as the device dropdown. This time, select the StyleTransfer.UpdateModel method.\n\n\n\n\n\n\n\nQuit Button\nSelect the Quit object inside the Canvas object and follow the same steps as the Update button. This time select the StyleTransfer.Quit method."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#build-the-project",
    "href": "posts/openvino-unity-plugin/part-3/index.html#build-the-project",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Build the Project",
    "text": "Build the Project\nNow we can build the completed project. Open the Build Settings window like before and press Build. We’ll use the same folder as last time. If we did everything correctly, there should not be any build errors."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#add-models-folder",
    "href": "posts/openvino-unity-plugin/part-3/index.html#add-models-folder",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Add Models Folder",
    "text": "Add Models Folder\nCreate a new folder in the same directory as the OpenVINO_Plugin_Demo.exe file and name it models.\n\n\n\n\n\nWe’ll place the .xml and .bin files for any OpenVINO models in this folder."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#add-plugins-folder",
    "href": "posts/openvino-unity-plugin/part-3/index.html#add-plugins-folder",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Add Plugins folder",
    "text": "Add Plugins folder\nGo back to the main build folder and open the OpenVINO_Plugin_Demo_Data folder.\n\n\n\n\n\nFrom there, open the Plugins folder.\n\n\n\n\n\nCopy and paste the x86_64 from earlier that contains all the .dll files and dependencies in here."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#run-the-application",
    "href": "posts/openvino-unity-plugin/part-3/index.html#run-the-application",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Run the Application",
    "text": "Run the Application\nNow we can finally test the application. Go back to the main build folder and double-click the OpenVINO_Plugin_Demo.exe file to run it. Remember that the first time the application launches will be slow as the cache files are generated."
  },
  {
    "objectID": "posts/openvino-unity-plugin/part-3/index.html#conclusion",
    "href": "posts/openvino-unity-plugin/part-3/index.html#conclusion",
    "title": "OpenVINO Plugin for Unity Tutorial Pt.3",
    "section": "Conclusion",
    "text": "Conclusion\nWe now have a general workflow for leveraging the OpenVINO™ Toolkit inside the Unity game engine. The style transfer model used in this tutorial is extremely demanding and can push even high-end graphics cards to their limit. The performance improvements from using OpenVINO make these types of models accessible to a wider audience. Less demanding models can even be offloaded to the integrated graphics or CPU. Tools like Unity are capable of much more than making games and it is exciting to see what will be possible by combining efficient deep learning models with real-time development platforms.\nProject Resources:\nGitHub Repository"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html",
    "href": "posts/openvino-yolox-unity/in-editor/index.html",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "",
    "text": "This tutorial is outdated. Use the new version at the link below.\nEnd-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#overview",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#overview",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Overview",
    "text": "Overview\nIn this follow-up to the YOLOX tutorial, we will update the OpenVINO and Unity code so that we can use the plugin directly in the Unity Editor. We will also make some additional changes to remove the need to manually move any files when building the Unity project."
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#install-new-openvino-version",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#install-new-openvino-version",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Install New OpenVINO Version",
    "text": "Install New OpenVINO Version\nTo use the OpenVINO plugin inside the Unity editor, we will first update the OpenVINO project to OpenVINO 2021.4.2. This resolves the dependency conflict mentioned in the last part of this tutorial series.\nNew users can register for OpenVINO or download it directly from the links below.\n• Registration Link\n• Download Link"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#update-openvino-project",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#update-openvino-project",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Update OpenVINO Project",
    "text": "Update OpenVINO Project\nThe code from the previous tutorial can be found at the repository linked below.\n• GitHub Repository\nOnce the download is complete, open the OpenVINO_YOLOX_DLL folder and double-click the OpenVINO_YOLOX_DLL.sln file to open the project in Visual Studio.\n\nUpdate Include Directories\nWe need to tell Visual Studio where the new version of OpenVINO is located. In the Solution Explorer panel, right-click the project name.\n\n\n\n\n\nSelect the Properties in the popup menu.\n\n\n\n\n\nIn the Properties Window, open the C++ dropdown and click on All Options. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nReplace the existing Include Directories with the ones below.\n• C:Files (x86)_2021.4.752_tools_engine\n• C:Files (x86)_2021.4.752\n\n\nUpdate Link Libraries\nNext, open the Linker dropdown in the Properties window and select All Options. Scroll up to the top of the All Options section and select Additional Dependencies.\n\n\n\n\n\nReplace the existing Link Libraries with the ones below.\n• C:Files (x86)_2021.4.752_tools_engine*\n• C:Files (x86)_2021.4.752*\nBack in the Properties window, click the Apply button and close the window.\n\n\nReplace GetAvailableDevices()\nWe will replace the existing GetAvailableDevices() method with a new method called FindAvailableDevices(). We need to use a different method than storing the available device names in a comma separated string which has resulted in some string conversion issues in Unity.\nFirst, we will make sure the available_devices vector is empty.\nWe will then iterate through each device name returned from ie.GetAvailableDevices() and add any CPU or GPU compute devices to available_devices.\nWe will skip any devices that contain GNA as the Gaussian & Neural Accelerator only supports a limited set of layer types.\nWe will still reverse available_devices so any GPU devices are selected first.\nThe method to set the location for the cache directory has changed from previous versions of OpenVINO. Now, we can set the directory for all GPUs using a single line.\nLastly, we will return the final number of available compute devices to Unity.\n\nCode:\n// Returns an unparsed list of available compute devices\nDLLExport int FindAvailableDevices(std::string* device_list) {\n    available_devices.clear();\n\n    for (auto&& device : ie.GetAvailableDevices()) {\n        if (device.find(\"GNA\") != std::string::npos) continue;\n\n        available_devices.push_back(device);\n    }\n\n    // Reverse the order of the list\n    std::reverse(available_devices.begin(), available_devices.end());\n\n    // Configure the cache directory for GPU compute devices\n    ie.SetConfig({ {CONFIG_KEY(CACHE_DIR), \"cache\"} }, \"GPU\");\n\n    return available_devices.size();\n}\n\n\n\nCreate GetDeviceName()\nNext, we will create a function that returns the name for a device at a given index in available_devices. In Unity, we will use this function to iterate through the devices in available_devices. This will help us avoid the string conversion issues mentioned above.\n\nCode:\nDLLExport std::string* GetDeviceName(int index) {\n    return &available_devices[index];\n}\n\n\n\nCreate FreeResources()\nThe memory used for the variables might not get automatically freed when running the project in the Unity Editor. This can cause issues when entering Play mode in the editor multiple times. To avoid this, we will manually clear the memory for the vectors by calling the clear() method for each.\n\nCode:\nDLLExport void FreeResources() {\n    available_devices.clear();\n    grid_strides.clear();\n    proposals.clear();\n    picked.clear();\n}\nNow we can build the project as we did in the previous tutorial. Open the Build menu at the top of the Visual Studio window and click Build Solution.\nNavigate to the x64 build folder in the File Explorer and open the Release child folder. Inside, we can see the new OpenVINO_YOLOX_DLL.dll file.\n\n\n\nGather Dependencies\nWe need to gather the dependencies for the new OpenVINO version.\nHere are the dependencies needed to use our new .dll:\n• clDNNPlugin.dll\n• inference_engine.dll\n• inference_engine_ir_reader.dll\n• inference_engine_legacy.dll\n• inference_engine_lp_transformations.dll\n• inference_engine_preproc.dll\n• inference_engine_transformations.dll\n• MKLDNNPlugin\n• ngraph.dll\n• opencv_core_parallel_tbb453_64.dll\n• opencv_core453.dll\n• opencv_imgcodecs453.dll\n• opencv_imgproc453.dll\n• plugins.xml\n• tbb.dll\nThe required dependencies can be found in the following directories.\n• OpenVINO: C:Files (x86)_2021.4.752_engine\n• nGraph: C:Files (x86)_2021.4.752_tools\n• TBB: C:Files (x86)_2021.4.752_tools_engine\n• OpenCV: C:Files (x86)_2021.4.752\nA folder containing the OpenVINO_YOLOX_DLL.dll file and its dependencies is also available to download at the link below.\n• Plugins folder"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#modify-unity-project",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#modify-unity-project",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Modify Unity Project",
    "text": "Modify Unity Project\nNow we can update the Unity project to accommodate the changes we made to the OpenVINO project. There are also some additional changes we will implement to make things easier when building the project.\n\nOpen the Project\nOpen the OpenVINO_YOLOX_Demo project in the Unity Editor.\n\n\nAdd Editor Tools\nNext, create a new folder in the Assets section called Editor. Inside the Editor folder, create a new script called PrepareAssets. Open the script in the code editor.\n\nRequired Namespaces\nUnityEditor: Implements the editor-specific APIs in Unity. It cannot be referenced by runtime code compiled into players.\nusing UnityEngine;\nusing UnityEditor;\n\n\nCreate Refresh() Method\nBy default, plugin files are only loaded when the Unity Editor launches. That means we would need to restart the editor whenever we added new .dll files. However, we can avoid this by calling the AssetDatabase.Refresh() method.\n[MenuItem(\"Tools/OpenVINO/Refresh\")]\nstatic void Refresh()\n{\n    AssetDatabase.Refresh();\n    Debug.Log(\"Refreshing Asset Database.\");\n}\n\n\nCreate CopyToStreamingAssets() Method\nThe YOLOX model files need to be copied to the StreamingAssets folder in order to be saved when the project is built.\nThe OpenVINO inference engine needs the plugins.xml file to know where the .dll files needed to perform inference with a CPU or GPU are located. However, Unity does not save .xml files in the Plugins folder when the project is built. We need to copy the plugins.xml file to the StreamingAssets folder so that the file gets saved in the build folder. We then need to copy the file back to the Plugins folder when the application first runs.\n[MenuItem(\"Tools/OpenVINO/Copy to StreamingAssets\")]\nstatic void CopyToStreamingAssets()\n{\n    if (AssetDatabase.IsValidFolder(\"Assets/StreamingAssets\") == false)\n    {\n        Debug.Log(\"Creating StreamingAssets folder.\");\n        AssetDatabase.CreateFolder(\"Assets\", \"StreamingAssets\");\n    }\n\n    if (AssetDatabase.IsValidFolder(\"Assets/StreamingAssets/models\") == false)\n    {\n        Debug.Log(\"Copying models folder to StreamingAssets folder.\");\n        bool success = AssetDatabase.CopyAsset(\"Assets/OpenVINO/models\", \"Assets/StreamingAssets/models\");\n        Debug.Log(success);\n    }\n    else\n    {\n        Debug.Log(\"models folder already exists in StreamingAssets folder\");\n    }\n\n    AssetDatabase.CopyAsset(\"Assets/OpenVINO/Plugins/x86_64/plugins.xml\", \"Assets/StreamingAssets/plugins.xml\");\n}\n\n\n\nImport OpenVINO Plugins\nBack in the Assets section, create a new folder called OpenVINO. Drag and drop the Plugins folder from the file explorer into the OpenVINO folder.\n\nImport YOLOX Models\nDownload the folder containing the models from the link below.\n• Google Drive Link\nExtract the models folder from models.tar and drag it into the OpenVINO folder.\nOpen the Tools → OpenVINO submenu at the top of the Editor. Click the Refresh button to ensure the DLL files are loaded.\nThen click the Copy to StreamingAssets button to copy the models folder and plugins.xml file to the StreamingAssets folder.\n\n\n\nRemove Graphy Tool (Optional)\nWe can remove the Graphy tool since we can now use the in-editor performance metrics. Make sure to remove any references in the ObjectDetector.cs script.\n\n\nRemove the Console View (Optional)\nLikewise, we can remove the Console View object in the Canvas. Again, make sure to remove any references in the ObjectDetector.cs script.\n\n\nModify ObjectDetector Script\nOpen the ObjectDetector.cs script in the code editor.\n\nUpdate DLL Method Declarations\nNext, we need to replace the old DLL method declarations with the new functions we added earlier.\n// Name of the DLL file\nconst string dll = \"OpenVINO_YOLOX_DLL\";\n\n[DllImport(dll)]\nprivate static extern int FindAvailableDevices();\n\n[DllImport(dll)]\nprivate static extern IntPtr GetDeviceName(int index);\n\n[DllImport(dll)]\nprivate static extern IntPtr InitOpenVINO(string model, int width, int height, int device);\n\n[DllImport(dll)]\nprivate static extern void PerformInference(IntPtr inputData);\n\n[DllImport(dll)]\nprivate static extern void PopulateObjectsArray(IntPtr objects);\n\n[DllImport(dll)]\nprivate static extern int GetObjectCount();\n\n[DllImport(dll)]\nprivate static extern void SetNMSThreshold(float threshold);\n\n[DllImport(dll)]\nprivate static extern void SetConfidenceThreshold(float threshold);\n\n[DllImport(dll)]\nprivate static extern void FreeResources();\n\n\nDefine Awake() Method\nWhen the built application first starts, we will check if the plugins.xml file is in the Plugins folder. If not, we will move the file from the StreamingAssets folder to the Plugins folder.\n\nCode:\npublic void Awake()\n{\n    #if UNITY_EDITOR_WIN\n        return;\n    #else\n\n        Debug.Log(\"Checking for plugins.xml file\");\n\n    string sourcePath = $\"{Application.streamingAssetsPath}/plugins.xml\";\n    string targetPath = $\"{Application.dataPath}/Plugins/x86_64/plugins.xml\";\n\n    if (File.Exists(targetPath))\n    {\n        Debug.Log(\"plugins.xml already in folder\");\n    }\n    else\n    {\n        Debug.Log(\"Moving plugins.xml file from StreamingAssets to Plugins folder.\");\n        File.Move(sourcePath, targetPath);\n    }\n\n    #endif\n}\n\n\n\nUpdate GetOpenVINOModels() Method\nWe need to modify the GetOpenVINOModels() so that it searches the StreamingAssets folder. The path for the StreamingAssets folder is stored in the Application.streamingAssetsPath property.\n\nCode:\n/// &lt;summary&gt;\n/// Get the list of available OpenVINO models\n/// &lt;/summary&gt;\nprivate void GetOpenVINOModels()\n{\n    // Get the subdirectories containing the available models\n    string[] modelDirs = System.IO.Directory.GetDirectories(Application.streamingAssetsPath + \"/models\");\n\n    // Get the model files in each subdirectory\n    List&lt;string&gt; openVINOFiles = new List&lt;string&gt;();\n    foreach (string dir in modelDirs)\n    {\n        openVINOFiles.AddRange(System.IO.Directory.GetFiles(dir));\n    }\n\n    // Get the paths for the .xml files for each model\n    Debug.Log(\"Available OpenVINO Models:\");\n    foreach (string file in openVINOFiles)\n    {\n        if (file.EndsWith(\".xml\"))\n        {\n            openVINOPaths.Add(file);\n            string modelName = file.Split('\\\\')[1];\n            openvinoModels.Add(modelName.Substring(0, modelName.Length));\n\n            Debug.Log($\"Model Name: {modelName}\");\n            Debug.Log($\"File Path: {file}\");\n        }\n    }\n    Debug.Log(\"\");\n}\n\n\n\nUpdate Start() Method\nWe need to update the if statement in the Start() method so that it calls the new FindAvailableDevices() and GetDeviceName() functions we created earlier.\n\nCode:\nif (processorType.Contains(\"Intel\") || graphicsDeviceName.Contains(\"Intel\"))\n{\n    // Get the list of available models\n    GetOpenVINOModels();\n\n    Debug.Log(\"Available Devices:\");\n    int deviceCount = FindAvailableDevices();\n    for (int i = 0; i &lt; deviceCount; i++)\n    {\n        deviceList.Add(Marshal.PtrToStringAnsi(GetDeviceName(i)));\n        Debug.Log(deviceList[i]);\n    }\n}\nelse\n{\n    inference.isOn = performInference = inference.enabled = false;\n    Debug.Log(\"No Intel hardware detected\");\n}\n\n\n\nCall FreeResources() Method\nWe will call the FreeResources() method we created earlier in the OnDisable() method.\n private void OnDisable()\n {\n FreeResources();\n }"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#run-in-editor",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#run-in-editor",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Run In Editor",
    "text": "Run In Editor\nThose are all the changes we need to make. Now we can try out the project in the editor.\n\n\nVideo"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-editor/index.html#build-the-project",
    "href": "posts/openvino-yolox-unity/in-editor/index.html#build-the-project",
    "title": "OpenVINO Object Detection in the Unity Editor (Outdated)",
    "section": "Build the Project",
    "text": "Build the Project\nWith the modifications we made, we can now build and run the project like any other Unity project without needing to manually copy files anymore.\n\nNext: In-Game Camera\nProject Resources:\nGitHub Repository"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "",
    "text": "Overview\nUpdate BoundingBox Script\nUpdate ObjectDetector Script\nAttach Script to Camera\nUpdate GUI Events\nTest it Out"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#overview",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#overview",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Overview",
    "text": "Overview\nIn this follow-up post, we will update the Unity code so that we can use the in-game camera as input for the YOLOX model. This can be useful for performing object detection on the in-game environment as shown below.\n\n\n\n\n\nThe baseline models used in this series were trained exclusively on real-world images. While the larger model variants are somewhat accurate in video game environments, they would need to be trained on images similar to the target game environment to maintain reliable accuracy. Therefore, we will be sticking with the demo project that has been used so far in this tutorial series."
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#update-boundingbox-script",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#update-boundingbox-script",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Update BoundingBox Script",
    "text": "Update BoundingBox Script\nOpen the BoundingBox script in the code editor.\n\nDelete the LineRenderer\nWe need to update how the bounding box is created so that the bounding boxes are not included in the current camera frame. Otherwise, the bounding box will be included in the input images to the model, reducing the models accuracy. This means we can no longer use the LineRenderer component.\n\n\nAdd New Public Properties\nWe need to add a few new public properties that will be accessed in the ObjectDetector script.\n\nrenderBox: Indicates whether to render the bounding box on screen\nboxRect: The bounding box defined in GUI space\nboxTex: The texture used for rendering the bounding box on screen\n\n// Indicates whether to render the bounding box on screen\npublic bool renderBox = false;\n// The bounding box\npublic Rect boxRect = new Rect();\n// The texture used for rendering the bounding box on screen\npublic Texture2D boxTex = Texture2D.whiteTexture;\nAlso, set the color and lineWidth variables to public.\n// The object class color\npublic Color color;\n// The adjusted line width for the bounding box\npublic int lineWidth = (int)(Screen.width * 1.75e-3);\n\n\nUpdate ToggleBBox Method\nWe need to update the value for renderBox inside the ToggleBox method.\n/// &lt;summary&gt;\n/// Toggle the visibility for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"show\"&gt;&lt;/param&gt;\npublic void ToggleBBox(bool show)\n{\n    renderBox = show;\n    bbox.SetActive(show);\n    text.SetActive(show);\n}\n\n\nUpdate InitializeLabel Method\nSince we are already working in screen space, we no longer need to convert from world to screen coordinates.\n/// &lt;summary&gt;\n/// Initialize the label for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"label\"&gt;&lt;/param&gt;\nprivate void InitializeLabel()\n{\n    // Set the label text\n    textContent.text = $\"{text.name}: {(info.prob * 100).ToString(\"0.##\")}%\";\n    // Set the text color\n    textContent.color = color;\n    // Set the text alignment\n    textContent.alignment = TextAlignmentOptions.MidlineLeft;\n    // Set the font size\n    textContent.fontSize = fontSize;\n    // Resize the text area\n    RectTransform rectTransform = text.GetComponent&lt;RectTransform&gt;();\n    rectTransform.sizeDelta = new Vector2(250, 50);\n    // Position the label above the top left corner of the bounding box\n    Vector3 textPos = new Vector3(info.x0, info.y0, -10f);\n    float xOffset = rectTransform.rect.width / 2;\n    textPos = new Vector3(textPos.x + xOffset, textPos.y + textContent.fontSize, textPos.z);\n    text.transform.position = textPos;\n}\n\n\nUpdate InitializeBBox Method\nSince we are not using a LineRenderer, we just need to update the boxRect variable with the new starting position and dimensions.\n/// &lt;summary&gt;\n/// Initialize the position and dimensions for the bounding box\n/// &lt;/summary&gt;\nprivate void InitializeBBox()\n{\n    // Set the position and dimensions\n    boxRect = new Rect(info.x0, Screen.height - info.y0, info.width, info.height);\n\n    // Make sure the bounding box is rendered\n    ToggleBBox(true);\n}\n\n\nUpdate the Constructor\nLastly, we need to remove any references to the lineRenderer variable.\n/// &lt;summary&gt;\n/// Constructor for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"objectInfo\"&gt;&lt;/param&gt;\npublic BoundingBox(Utils.Object objectInfo)\n{\n    // Add a text componenet to store the label text\n    textContent = text.AddComponent&lt;TextMeshProUGUI&gt;();\n    // Assign text object to the label canvas\n    text.transform.SetParent(canvas.transform);\n\n    // Update the object info for the bounding box\n    SetObjectInfo(objectInfo);\n}"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#update-objectdetector-script",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#update-objectdetector-script",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Update ObjectDetector Script",
    "text": "Update ObjectDetector Script\nNext, open the ObjectDetector script. We don’t need to add any new variables for this script.\n\nUpdate InitializeTextures Method\nSince we are receiving input from the current Camera frame, we need to initialize the textures using the screen dimensions instead of the videoTexture dimensions.\n/// &lt;summary&gt;\n/// Calculate the dimensions for the input image\n/// &lt;/summary&gt;\n/// &lt;param name=\"newVideo\"&gt;&lt;/param&gt;\nprivate void InitializeTextures(bool newVideo = false)\n{\n    // Adjust the input dimensions to maintain the current aspect ratio\n    if (imageDims.x != targetDims.x)\n    {\n        imageDims.x = targetDims.x;\n        aspectRatioScale = (float)Screen.height / Screen.width;\n        imageDims.y = (int)(targetDims.x * aspectRatioScale);\n        targetDims.y = imageDims.y;\n\n    }\n    if (imageDims.y != targetDims.y)\n    {\n        imageDims.y = targetDims.y;\n        aspectRatioScale = (float)Screen.width / Screen.height;\n        imageDims.x = (int)(targetDims.y * aspectRatioScale);\n        targetDims.x = imageDims.x;\n\n    }\n\n\n    // Initialize the RenderTexture that will store the processed input image\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGB32);\n    // Update inputTex with the new dimensions\n    inputTex = new Texture2D(imageDims.x, imageDims.y, TextureFormat.RGBA32, false);\n\n    // Update the values for the width and height input fields\n    Debug.Log($\"Setting Input Dims to W: {imageDims.x} x H: {imageDims.y}\");\n    width.text = $\"{imageDims.x}\";\n    height.text = $\"{imageDims.y}\";\n}\n\n\nModify UpdateBoundingBoxes Method\nLikewise, in the UpdateBoundingBoxes method we now need to determine the value for minDimension using the screen dimensions instead of videoTexture.\n/// &lt;summary&gt;\n/// Update the list of bounding boxes based on the latest output from the model\n/// &lt;/summary&gt;\nprivate void UpdateBoundingBoxes()\n{\n    // Process new detected objects\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        // The smallest dimension of the screen\n        int minDimension = Mathf.Min(Screen.width, Screen.height);\n        // The value used to scale the bbox locations up to the source resolution\n        float scale = (float)minDimension / Mathf.Min(imageDims.x, imageDims.y);\n\n        // Flip the bbox coordinates vertically\n        objectInfoArray[i].y0 = rTex.height - objectInfoArray[i].y0;\n\n        objectInfoArray[i].x0 *= scale;\n        objectInfoArray[i].y0 *= scale;\n        objectInfoArray[i].width *= scale;\n        objectInfoArray[i].height *= scale;\n\n        // Update bounding box list with new object info\n        try\n        {\n            boundingBoxes[i].SetObjectInfo(objectInfoArray[i]);\n        }\n        catch\n        {\n            // Add a new bounding box object when needed\n            boundingBoxes.Add(new BoundingBox(objectInfoArray[i]));\n        }\n    }\n\n    // Turn off extra bounding boxes\n    for (int i = 0; i &lt; boundingBoxes.Count; i++)\n    {\n        if (i &gt; objectInfoArray.Length - 1)\n        {\n            boundingBoxes[i].ToggleBBox(false);\n        }\n    }\n}\n\n\nModify Update Method\nWe will be moving most of the code in the Update() method to the OnRenderImage() method.\n// Update is called once per frame\nvoid Update()\n{\n    // Toggle the user interface\n    if (Input.GetKeyDown(\"space\"))\n    {\n        canvas.SetActive(!canvas.activeInHierarchy);\n    }\n\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam.isOn) Graphics.Blit(webcamTexture, videoTexture);\n}\n\n\nDefine OnRenderImage Method\nInstead of copying the contents of videoTexture to rTex, we will copy the contents of the current camera frame. We will not be modifying the source texture, so we can copy it directly to the destination texture.\npublic void OnRenderImage(RenderTexture source, RenderTexture destination)\n{\n    if (performInference == true)\n    {\n        // Copy the source texture to the rTex RenderTexture\n        Graphics.Blit(source, rTex);\n        \n        // Flip image before sending to DLL\n        FlipImage(rTex, \"FlipXAxis\");\n\n        // Download pixel data from GPU to CPU\n        if (useAsync.isOn)\n        {\n            AsyncGPUReadback.Request(rTex, 0, TextureFormat.RGBA32, OnCompleteReadback);\n        }\n        else\n        {\n            RenderTexture.active = rTex;\n            inputTex.ReadPixels(new Rect(0, 0, rTex.width, rTex.height), 0, 0);\n            inputTex.Apply();\n        }\n\n        // Send reference to inputData to DLL\n        UploadTexture(inputTex.GetRawTextureData());\n\n        // Update bounding boxes with new object info\n        UpdateBoundingBoxes();\n    }\n    Graphics.Blit(source, destination);\n}\n\n\nDefine OnGUI Method\nWe will render all the bounding boxes using the GUI.DrawTexture method. This method can only be called in the OnGUI method. When calling GUI.DrawTexture(), will pass in the boxRect, boxTex, color, and lineWidth properties for the current bounding box.\npublic void OnGUI()\n{\n    if (performInference == false) return;\n\n    foreach (BoundingBox boundingBox in boundingBoxes)\n    {\n        if (boundingBox.renderBox)\n        {\n            GUI.DrawTexture(boundingBox.boxRect, boundingBox.boxTex, ScaleMode.StretchToFill, true, 0, boundingBox.color, 3, boundingBox.lineWidth);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#attach-script-to-camera",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#attach-script-to-camera",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Attach Script to Camera",
    "text": "Attach Script to Camera\nNow we need to attach the ObjectDetector script to the main camera, so we can use the OnRenderImage() method. Select the Object Detector object in the Hierarchy tab. In the Inspector tab, click and hold on the Object Detector (Script) component.\n\n\n\n\n\nThen, drag and drop the component onto the Main Camera object in the Hierarchy tab. The component should now be attached to the Main Camera object.\n\n\n\n\n\nWe can delete the Object Detector object, as it will not be used anymore."
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#update-gui-events",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#update-gui-events",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Update GUI Events",
    "text": "Update GUI Events\nLastly, we need to reassign all the GUI events as show here. This time, we drag and drop the Main Camera object from the Hierarchy tab."
  },
  {
    "objectID": "posts/openvino-yolox-unity/in-game-camera/index.html#test-it-out",
    "href": "posts/openvino-yolox-unity/in-game-camera/index.html#test-it-out",
    "title": "OpenVINO Object Detection in Unity Using the In-Game Camera",
    "section": "Test it Out",
    "text": "Test it Out\nIf we run the project now, the results should be basically the same.\nProject Resources:\nGitHub Repository"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html",
    "href": "posts/openvino-yolox-unity/part-1/index.html",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "",
    "text": "This tutorial is outdated. Use the new version at the link below.\nEnd-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1\n\n\n\nOverview\nPrerequisites\nDownload OpenVINO IR Models\nDownload Test Videos\nConclusion"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#experience",
    "href": "posts/openvino-yolox-unity/part-1/index.html#experience",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "Experience",
    "text": "Experience\nThis tutorial does not assume any prior experience with the OpenVINO™ Toolkit or Unity. However, some basic experience with Unity would be beneficial."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#system-requirements",
    "href": "posts/openvino-yolox-unity/part-1/index.html#system-requirements",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "System Requirements",
    "text": "System Requirements\nThe target platform for this project is Windows 10 64-bit. The OpenVINO™ Toolkit does not appear to support 32-bit versions. Given that the OpenVINO™ Toolkit is designed for Intel hardware, an Intel CPU and/or GPU is highly recommended."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#unity",
    "href": "posts/openvino-yolox-unity/part-1/index.html#unity",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "Unity",
    "text": "Unity\nThe first prerequisite we will want to set up is Unity. The Unity Editor can be installed through the Unity Hub, which can be downloaded from the link below.\n\nUnity Hub: (download)\n\nWe will be using Unity 2020 LTS. The exact version can be downloaded from the links below.\n\nUnity LTS Releases: (download)\nDownload Unity 2020.3.18 (LTS): (download)\n\nNote: The installation process will also install Visual Studio, one of the other prerequisites.\nThe tutorial below walks through the basics of Unity, from the installation process all the way to making an Angry Birds clone.\n\nHow to Make a Game - Unity Beginner Tutorial"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#visual-studio",
    "href": "posts/openvino-yolox-unity/part-1/index.html#visual-studio",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "Visual Studio",
    "text": "Visual Studio\nUnity automatically includes Visual Studio when installing the Editor. However it can also be downloaded directly from the link below.\n\nVisual Studio Community 2019: (download)"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#visual-c-redistributables",
    "href": "posts/openvino-yolox-unity/part-1/index.html#visual-c-redistributables",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "Visual C++ Redistributables",
    "text": "Visual C++ Redistributables\nThe Visual C++ Redistributables should be installed along with Visual Studio. If not, they can be downloaded from the link below.\n\nLatest C++ Redistributables: (link)"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#cmake",
    "href": "posts/openvino-yolox-unity/part-1/index.html#cmake",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "CMake",
    "text": "CMake\nThe official OpenVINO™ installation guide lists CMake as a requirement. However, we do not need it for this project. Still, the latest release of CMake 64-bit is available at the link below.\n\nCMake: link\n\nNote: Make sure to select one of the Add CMake to the system PATH options during the installation process."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#python",
    "href": "posts/openvino-yolox-unity/part-1/index.html#python",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "Python",
    "text": "Python\nPython 3.6, 3.7, or 3.8 64-bit are needed to convert a model from ONNX format to OpenVINO’s intermediate representation (IR). We can install Python 3.8 from the Windows Store. This method automatically configures the Python installation to be accessible from the command line.\n\nWindows Store Python 3.8: (link)\n\nThe YOLOX models are already available in OpenVINO IR format, so Python is not required for this tutorial. However, models trained on custom datasets will need to be converted. The steps for converting models from ONNX format to OpenVINO IR are covered in a previous tutorial. The YOLOX models are also available in ONNX format on GitHub."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-1/index.html#openvino",
    "href": "posts/openvino-yolox-unity/part-1/index.html#openvino",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)",
    "section": "OpenVINO",
    "text": "OpenVINO\nWe now have all the required prerequisites to install OpenVINO. We’ll be using OpenVINO 2021.3 for this tutorial. First time users need to fill out a registration form to download the toolkit.\n\nRegistration Link\nDownload Link"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html",
    "href": "posts/openvino-yolox-unity/part-2/index.html",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "",
    "text": "This tutorial is outdated. Use the new version at the link below.\nEnd-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#overview",
    "href": "posts/openvino-yolox-unity/part-2/index.html#overview",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of the tutorial, we first installed Unity, OpenVINO, and its prerequisite software. We then downloaded some pretrained models that had been converted to the OpenVINO Intermediate Representation format, along with some test videos.\nIn this part, we will walk through the steps needed to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pretrained deep learning model."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#create-a-new-visual-studio-project",
    "href": "posts/openvino-yolox-unity/part-2/index.html#create-a-new-visual-studio-project",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Create a New Visual Studio Project",
    "text": "Create a New Visual Studio Project\nOpen Visual Studio and select Create a new project.\n\n\n\n\n\nType DLL into the search bar. Select the Dynamic-Link Library (DLL) option and press Next.\n\n\n\n\n\nIn the next window, we’ll name the new project OpenVINO_YOLOX_DLL. Take note of the Location the project will be saved to and click Create. The default location can be replaced, but we will need to access the project folder to get the generated DLL file."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#configure-project",
    "href": "posts/openvino-yolox-unity/part-2/index.html#configure-project",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Configure Project",
    "text": "Configure Project\nWe need to update the default project configuration to access the OpenVINO™ Toolkit and build the project with it.\n\nSet Build Configuration and Platform\nThe OpenVINO™ Toolkit does not support x86 builds. We will need to set the project to build for x64. At the top of the window, open the Solution Configurations dropdown menu, and select Release.\n\n\n\n\n\nThen, open the Solution Platform dropdown menu and select x64."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#add-include-directories",
    "href": "posts/openvino-yolox-unity/part-2/index.html#add-include-directories",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Add Include Directories",
    "text": "Add Include Directories\nVisual Studio needs to be told where the OpenVINO™ Toolkit is located, so we can access its APIs. In the Solution Explorer panel, right-click the project name.\n\n\n\n\n\nSelect Properties in the popup menu.\n\n\n\n\n\nIn the Properties Window, open the C++ dropdown and click on All Options. Select the Additional Include Directories section and click on &lt;Edit..&gt; in the dropdown.\n\n\n\n\n\nWe need to add the include directories for the OpenVINO inference engine and the OpenCV libraries included with the OpenVINO™ Toolkit.\nAdd the following lines and then click OK. Feel free to open these folders in the File Explorer and see what exactly they provide access to.\n\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\inference_engine\\include\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\opencv\\include"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#link-libraries",
    "href": "posts/openvino-yolox-unity/part-2/index.html#link-libraries",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Link Libraries",
    "text": "Link Libraries\nNext, open the Linker dropdown in the Properties window and select All Options. Scroll up to the top of the All Options section and select Additional Dependencies.\n\n\n\n\n\nAdd the following lines for the OpenVINO and OpenCV libraries, then click OK. The * at the end tells Visual Studio to add all the .lib files contained in those folders. We do not technically need every single one, but this is more convenient than manually typing the specific file names.\n\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\deployment_tools\\inference_engine\\lib\\intel64\\Release\\*\nC:\\Program Files (x86)\\Intel\\openvino_2021.3.394\\opencv\\lib\\*\n\n\n\n\n\n\nFinally, click the Apply button and close the Properties window."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#clear-default-code",
    "href": "posts/openvino-yolox-unity/part-2/index.html#clear-default-code",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Clear Default Code",
    "text": "Clear Default Code\nNow, we can finally start coding. The default code for the dllmain.cpp file is as follows.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nBOOL APIENTRY DllMain( HMODULE hModule,\n                       DWORD  ul_reason_for_call,\n                       LPVOID lpReserved\n                     )\n{\n    switch (ul_reason_for_call)\n    {\n    case DLL_PROCESS_ATTACH:\n    case DLL_THREAD_ATTACH:\n    case DLL_THREAD_DETACH:\n    case DLL_PROCESS_DETACH:\n        break;\n    }\n    return TRUE;\n}\nWe can delete everything below the #include \"pch.h\" line.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\""
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#update-precompiled-header-file",
    "href": "posts/openvino-yolox-unity/part-2/index.html#update-precompiled-header-file",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Update Precompiled Header File",
    "text": "Update Precompiled Header File\nThe pch.h file is a Precompiled Header file that is generated by Visual Studio. We can place any header files that won’t be updated here and they will only be compiled once. This can reduce build times for larger projects. We can open the pch.h file by selecting that line and pressing F12.\n// pch.h: This is a precompiled header file.\n// Files listed below are compiled only once, improving build performance for future builds.\n// This also affects IntelliSense performance, including code completion and many code browsing features.\n// However, files listed here are ALL re-compiled if any one of them is updated between builds.\n// Do not add files here that you will be updating frequently as this negates the performance advantage.\n\n#ifndef PCH_H\n#define PCH_H\n\n// add headers that you want to pre-compile here\n#include \"framework.h\"\n\n\n#endif //PCH_H\nWe’ll add the required header files below #include \"framework.h\". Each one can be explored by selecting that line and pressing F12 as well.\n// add headers that you want to pre-compile here\n#include \"framework.h\"\n// A header file that provides a set minimal required Inference Engine API.\n#include &lt;inference_engine.hpp&gt;\n// A header file that provides the API for the OpenCV modules.\n#include &lt;opencv2/opencv.hpp&gt;\n// Regular expressions standard header\n#include &lt;regex&gt;"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#update-dllmain",
    "href": "posts/openvino-yolox-unity/part-2/index.html#update-dllmain",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Update dllmain",
    "text": "Update dllmain\nBack in the dllmain.cpp file, we’ll add the InferenceEngine namespace and create a macro to mark functions we want to make accessible in Unity.\n// dllmain.cpp : Defines the entry point for the DLL application.\n#include \"pch.h\"\n\nusing namespace InferenceEngine;\n\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\nWe need to wrap the code in extern “C” to prevent name - mangling issues with the compiler.\n// Create a macro to quickly mark a function for export\n#define DLLExport __declspec (dllexport)\n\n// Wrap code to prevent name-mangling issues\nextern \"C\" {\n\n}\n\nDeclare Variables\nInside the wrapper, we’ll declare the persistent variables needed for the DLL.\n\nObject: The YOLOX model predicts the coordinates and dimensions for a bounding box that contains a detected object, along with the predicted object class and the confidence for that prediction. We will store this information for each prediction in a struct called Object. We will create a corresponding struct in Unity.\nGridAndStride: The model input is divided into grid cells which correspond to sections of the input image. We need to keep track of these grid cells to scale the predicted object locations back up to the source input resolution. We will keep track of this information using a new struct called GridAndStride.\nimg_w: We need to keep track of the width of the input image to scale the model output back up to the source resolution.\nimg_h: We need to keep track of the height of the input image to scale the model output back up to the source resolution.\ninput_w: We need to keep track of the current input width for the model to generate the GridAndStride values as well as padding and resizing the input images.\ninput_h: We need to keep track of the current input height for the model to generate the GridAndStride values as well as padding and resizing the input images.\ncount: We will be passing the object predictions to Unity by populating an Object array from Unity. We need to know how many objects have been detected, so that we can initialize the array before filling it.\nscale: We need to keep track of the difference between the input image and the input dimensions of the model, so that we can scale up the model output back to the source resolution.\nbbox_conf_thresh: We will only consider model predictions with confidence scores above a specified threshold.\nnms_thresh: The way that the model detects objects makes it possible to detect the same object more than once. We can filter through multiple detections of the same object by checking how much two predicted bounding boxes overlap. If a bounding box overlaps one with a higher confidence too much, we can ignore it. This technique is called Non Maximum Suppression (NMS).\navailable_devices_str: We will pass a list of compute devices available for performing inference to Unity as a comma separated string. We need to make this string a global variable to keep the data persistent in memory.\navailable_devices: We will store the unparsed list of available devices in a vector.\ngrid_strides: We will store the grid and stride values for scaling predicted object locations back up to the current input dimensions in a GridAndStride vector.\nproposals: We will store the object proposals with high enough confidence scores in an Object vector.\npicked: We will keep track of which object proposals we want to keep in a separate vector.\nie: To use the OpenVINO inference engine, we first need to create a Core instance called ie. We’ll use this variable to read the model file, get the available compute devices, change configuration settings, and load the model onto the target compute device.\nnetwork: We’ll store the information from the .xml and .bin file in a CNNNetwork variable.\nexecutable_network: We need to create an executable version of the network before we can perform inference.\ninfer_request: After that, we will create an InferRequest variable to initiate inference for the model.\nminput: Once we create the inference request, we will require write access to the input tensor for the model. We can access the input tensor using a MemoryBlob::Ptr variable.\nmoutput: After executing the model, we will need read access to the output tensor for the model. We can access the output tensor using a MemoryBlob::CPtr variable.\n\n\nCode :\n// Wrap code to prevent name-mangling issues*\nextern \"C\" {\n    // Stores information about a single object prediction*\n    struct Object\n    {\n        float x0;\n        float y0;\n        float width;\n        float height;\n        int label;\n        float prob;\n    };\n\n    // Store grid offset and stride values to decode a section of the model output*\n    struct GridAndStride\n    {\n        int grid0;\n        int grid1;\n        int stride;\n    };\n\n    // The width of the source input image*\n    int img_w;\n    // The height of the source input image*\n    int img_h;\n    // The input width for the model*\n    int input_w = 640;\n    // The input height for the model*\n    int input_h = 640;\n    // Stores the final number of objects detected after decoding the model output*\n    int count = 0;\n\n    // The scale value used to adjust the model output to the original unpadded image*\n    float scale;\n    // The minimum confidence score to consider an object proposal*\n    float bbox_conf_thresh = 0.3;\n    // The maximum intersection over union value before an object proposal will be ignored*\n    float nms_thresh = 0.45;\n\n    // An unparsed list of available compute devices*\n    std::string available_devices_str = \"\";\n\n    // List of available compute devices*\n    std::vector&lt;std::string&gt; available_devices;\n    // Stores the grid and stride values*\n    std::vector&lt;GridAndStride&gt; grid_strides;\n    // Stores the object proposals with confidence scores above bbox_conf_thresh*\n    std::vector&lt;Object&gt; proposals;\n    // Stores the indices for the object proposals selected using non-maximum suppression*\n    std::vector&lt;int&gt; picked;\n\n    // Inference engine instance*\n    Core ie;\n    // Contains all the information about the Neural Network topology and related constant values for the model*\n    CNNNetwork network;\n    // Provides an interface for an executable network on the compute device*\n    ExecutableNetwork executable_network;\n    // Provides an interface for an asynchronous inference request*\n    InferRequest infer_request;\n\n    // A pointer to the input tensor for the model*\n    MemoryBlob::Ptr minput;\n    // A pointer to the output tensor for the model*\n    MemoryBlob::CPtr moutput;\n}\n\n\n\nCreate GetAvailableDevices() Function\nWe might want to compare the performance of executing the model on the CPU, GPU, or iGPU. Being able to manually select the compute device gives us the option to offload less demanding models to the integrated graphics, which would otherwise go unused.\nThis function combines the list of available devices into a single, comma separated string that will be parsed in Unity. We will use this list to manually select which device is used to execute the model.\nIt can take over 20 seconds to upload the OpenVINO model to a GPU or iGPU. This is because OpenCL kernels are being compiled for the specific model and GPU at runtime. There isn’t much we can do about this the first time a model is loaded to the GPU. However, we can eliminate this load time in future uses by storing cache files for the model. The cache files are specific to each GPU. Additional cache files will also be created when using a new input resolution for a model.\nWe’ll use a regular expression to confirm a compute device is a GPU before attempting to set a cache directory for it.\nWe can specify the directory to store cache files for each available GPU using the ie.SetConfig() method. We’ll just name the directory, cache. By default, the cache directory will be created in the same folder as the executable file that will be generated from the Unity project.\nWe need to add the DLLExport macro since we’ll be calling this function from Unity.\n\nCode :\n// Returns an unparsed list of available compute devices\nDLLExport const std::string* GetAvailableDevices() {\n\n    // Get a list of the available compute devices\n    available_devices = ie.GetAvailableDevices();\n    // Reverse the order of the list\n    std::reverse(available_devices.begin(), available_devices.end());\n\n    // Configure the cache directory for GPU compute devices\n    std::regex e(\"(GPU)(.*)\");\n    // Iterate through the available compute devices\n    for (auto&& device : available_devices) {\n        // Only configure the cache directory for GPUs\n        if (std::regex_match(device, e)) {\n            ie.SetConfig({ {CONFIG_KEY(CACHE_DIR), \"cache\"} }, device);\n        }\n    }\n\n    // Add all available compute devices to a single string\n    for (auto&& device : available_devices) {\n        available_devices_str += device;\n        available_devices_str += ((device == available_devices[available_devices.size() - 1]) ? \"\" : \",\");\n    }\n    return &available_devices_str;\n}\n\n\n\nCreate GetObjectCount() Function\nWe need to get the final number of detected objects so that we can initialize the size of the Object array in Unity before populating it with the latest predictions. This function will simply return the value for the count variable to Unity.\n\nCode:\n// Get the final number of objects detected in the current model output\nDLLExport int GetObjectCount() {\n    return count;\n}\n\n\n\nCreate SetConfidenceThreshold() Function\nNext, we will create a simple function to update the minimum confidence threshold from inside the Unity application.\n\nCode:\n// Set the minimum confidence score\nDLLExport void SetConfidenceThreshold(float threshold) {\n    bbox_conf_thresh = threshold;\n}\n\n\n\nCreate SetNMSThreshold() Function\nWe will also create a similar function to update the nms_threshold value.\n\nCode:\n// Set the maximum intersection over union value\nDLLExport void SetNMSThreshold(float threshold) {\n    nms_thresh = threshold;\n}\n\n\n\nCreate GenerateGridsAndStride() Function\nThe YOLOX model examines the input image using different stride values (i.e. different resolutions). This allows the model to better detect objects of different sizes in the image. A smaller stride value (e.g. 8) is used to detect smaller objects while a larger value (e.g. 32) is used to detect larger objects. Here, we can see the difference in results when using a single stride value in isolation.\nStride: 8\n\n\n\n\n\nStride: 16\n\n\n\n\n\nStride: 32\n\n\n\n\n\nWe need to keep track of these stride values along with the grid cells to properly scale the model output back up to the source resolution.\nThe specific strides values used by a model are determined during training. All the models used in this tutorial use stride values of 8, 16, and 32. The number of grid cells is dependent on the current input dimensions and stride value.\nIn this function, we will generate each combination of grid cells and stride values and store them in the grid_strides vector. This function will not be directly called from Unity, so we do not need to add the DLLExport macro.\n\nCode:\n// Generate the grid and stride values\nvoid GenerateGridsAndStride() {\n\n    // The stride values used to generate the gride_strides vector\n    const int strides[] = { 8, 16, 32 };\n\n    // Iterate through each stride value\n    for (auto stride : strides)\n    {\n        // Calculate the grid dimensions\n        int grid_height = input_h / stride;\n        int grid_width = input_w / stride;\n\n        // Store each combination of grid coordinates\n        for (int g1 = 0; g1 &lt; grid_height; g1++)\n        {\n            for (int g0 = 0; g0 &lt; grid_width; g0++)\n            {\n                grid_strides.push_back(GridAndStride{ g0, g1, stride });\n            }\n        }\n    }\n}\n\n\n\nCreate SetInputDims() Function\nNext, we’ll make a function to update the input resolution for the model from Unity. The function will take in a width and height value. The input resolution has a significant impact on both inference speed and model accuracy, so this should be tuned based on the target hardware and use case.\nThere are a few steps we need to take each time we use a different input resolution.\nWe first need to calculate the padded input dimensions for the model. We cannot use purely arbitrary dimensions for the model input, without causing an error. Therefore, we will pad the image resolution so that the height and width are multiples of 32.\nWe also need to calculate the value to scale the model output from the input dimensions to the source resolution.\nOnce we have the new input dimensions, we can update the grid_strides vector.\nFinally, we perform shape inference to update the model dimensions. OpenVINO provides the InferenceEngine::CNNNetwork::reshape method to update the input dimensions at runtime. This method also propagates the changes down to the outputs.\nTo use it, we first need to create an InferenceEngine::SizeVector variable and assign the new dimensions. We can then pass the SizeVector as input to network.reshape().\n\nCode:\n// Manually set the input resolution for the model\nvoid SetInputDims(int width, int height) {\n\n    img_w = width;\n    img_h = height;\n\n    // Calculate the padded model input dimensions\n    input_w = (int)(32 * std::roundf(img_w / 32));\n    input_h = (int)(32 * std::roundf(img_h / 32));\n\n    // Calculate the value used to adjust the model output to the original unpadded image\n    scale = std::min(input_w / (img_w * 1.0), input_h / (img_h * 1.0));\n\n    // Generate grid_strides for the padded model input dimensions\n    grid_strides.clear();\n    GenerateGridsAndStride();\n\n    // Collect the map of input names and shapes from IR\n    auto input_shapes = network.getInputShapes();\n\n    // Set new input shapes\n    std::string input_name_1;\n    InferenceEngine::SizeVector input_shape;\n    // Create a tuple for accessing the input dimensions\n    std::tie(input_name_1, input_shape) = *input_shapes.begin();\n    // Set batch size to the first input dimension\n    input_shape[0] = 1;\n    // Update the height for the input dimensions\n    input_shape[2] = input_h;\n    // Update the width for the input dimensions\n    input_shape[3] = input_w;\n    input_shapes[input_name_1] = input_shape;\n\n    // Perform shape inference with the new input dimensions\n    network.reshape(input_shapes);\n}\n\n\n\nCreate UploadModelToDevice() Function\nAfter updating the network with the current input dimensions, we can create an executable version of the network. This function will take as input an index for the availableDevices variable. This will allow us to specify and switch between compute devices in the Unity project at runtime.\nAfter uploading the model to the compute device, we can create an inference request for it.\nOnce we have the inference request, we can get pointers to the input and output tensors using the .GetBlob() method. We need to cast each Blob as a MemoryBlob. The dimensions of the input tensor can be accessed using the minput-&gt;getTensorDesc().getDims() method.\nWe will return the name of the device the model will be executed on back to Unity.\n\nCode:\n// Create an executable network for the target compute device\nstd::string* UploadModelToDevice(int deviceNum) {\n\n    // Create executable network\n    executable_network = ie.LoadNetwork(network, available_devices[deviceNum]);\n    // Create an inference request object\n    infer_request = executable_network.CreateInferRequest();\n\n    // Get the name of the input layer\n    std::string input_name = network.getInputsInfo().begin()-&gt;first;\n    // Get a pointer to the input tensor for the model\n    minput = as&lt;MemoryBlob&gt;(infer_request.GetBlob(input_name));\n\n    // Get the name of the output layer\n    std::string output_name = network.getOutputsInfo().begin()-&gt;first;\n    // Get a pointer to the output tensor for the model\n    moutput = as&lt;MemoryBlob&gt;(infer_request.GetBlob(output_name));\n\n    // Return the name of the current compute device\n    return &available_devices[deviceNum];;\n}\n\n\n\nCreate InitOpenVINO() Function\nWe will call both the SetInputDims() and UploadModelToDevice() functions from a new function called InitOpenVINO(). This is the function that will be called from Unity to initialize the required OpenVINO variables. The function will take a path to a .xml file for a model along with the input resolution and the desired compute device.\n\nCode:\n// Set up OpenVINO inference engine\nDLLExport std::string* InitOpenVINO(char* modelPath, int width, int height, int deviceNum) {\n\n    // Read network file\n    network = ie.ReadNetwork(modelPath);\n\n    SetInputDims(width, height);\n\n    return UploadModelToDevice(deviceNum);\n}\n\n\n\nCreate StaticResize() Function\nAs mentioned earlier, we need to pass the input image so that the dimensions are multiples of 32. In this function, we will pad and resize the image resolution to match the model input dimensions calculated in the SetInputDims() function.\nWe can do so by creating a new cv::Mat with the padded input dimensions and copying the pixel data to it. This means there will almost always be a small section of the model input that does not have anything in it. Fortunately, the padded section is small enough that the extra computation is negligible.\nFor example, this is how much padding is added at the default target input resolution of 640x360 (actual dimensions are 640x352).\n\n\n\n\n\n\nCode:\n// Resize and pad the source input image to the dimensions expected by the model\ncv::Mat StaticResize(cv::Mat& img) {\n    // Calculate the unpadded input dimensions\n    float r = std::min(input_w / (img.cols * 1.0), input_h / (img.rows * 1.0));\n    int unpad_w = r * img.cols;\n    int unpad_h = r * img.rows;\n\n    // Scale the input image to the unpadded input dimensions\n    cv::Mat re(unpad_h, unpad_w, CV_8UC3);\n    cv::resize(img, re, re.size());\n\n    // Create a new Mat with the padded input dimensions\n    cv::Mat out(input_h, input_w, CV_8UC3);\n    // Copy the unpadded image data to the padded Mat\n    re.copyTo(out(cv::Rect(0, 0, re.cols, re.rows)));\n    return out;\n}\n\n\n\nCreate GenerateYoloxProposals() Function\nWe need to use the grid/stride combinations to decode the model output.\nThe dimensions for the model output are dependent on the number of object classes the model was trained to detect. We can extract this information from the moutput pointer which points to the output tensor for the model.\nOnce we have this information, we can iterate through the model output using the grid/stride combinations. Each grid cell/stride value pair in grid_strides corresponds to a single bounding box prediction.\nEach bounding box prediction contains the following:\n\nThe X coordinate for the center of the bounding box\nThe Y coordinate for the center of the bounding box\nThe width of the bounding box\nThe height of the bounding box\nThe likelihood that an object is present \nA confidence score for each of the object classes\n\nWe need to first offset the (x,y) coordinates by the grid cell indices and then scale the coordinates using the stride value. The width and height values need to be scaled by the stride value as well.\nWhile the model predicts the coordinates for the center of the bounding box, we will be using the coordinates for the top-left corner to draw the bounding box in Unity. We can obtain these coordinates by subtracting half the width and height from the x and y coordinates respectively.\nWe determine what type of object has been predicted by finding the object class with the highest confidence score.\nLastly, we will add any bounding box predictions with confidence scores higher than bbox_conf_thresh to the proposals vector.\n\nCode:\n// Create object proposals for all model predictions with high enough confidence scores\nvoid GenerateYoloxProposals(const float* feat_ptr) {\n\n    const int num_anchors = grid_strides.size();\n\n    // Obtain the length of a single bounding box proposal\n    const int proposal_length = moutput-&gt;getTensorDesc().getDims()[2];\n\n    // Obtain the number of classes the model was trained to detect\n    const int num_classes = proposal_length - 5;\n\n    for (int anchor_idx = 0; anchor_idx &lt; num_anchors; anchor_idx++)\n    {\n        // Get the current grid and stride values\n        const int grid0 = grid_strides[anchor_idx].grid0;\n        const int grid1 = grid_strides[anchor_idx].grid1;\n        const int stride = grid_strides[anchor_idx].stride;\n\n        // Get the starting index for the current proposal\n        const int basic_pos = anchor_idx * proposal_length;\n\n        // Get the coordinates for the center of the predicted bounding box\n        float x_center = (feat_ptr[basic_pos + 0] + grid0) * stride;\n        float y_center = (feat_ptr[basic_pos + 1] + grid1) * stride;\n\n        // Get the dimensions for the predicte bounding box\n        float w = exp(feat_ptr[basic_pos + 2]) * stride;\n        float h = exp(feat_ptr[basic_pos + 3]) * stride;\n\n        // Calculate the coordinates for the upper left corner of the bounding box\n        float x0 = x_center - w * 0.5f;\n        float y0 = y_center - h * 0.5f;\n\n        // Get the confidence score that an object is present\n        float box_objectness = feat_ptr[basic_pos + 4];\n\n        // Initialize object struct with bounding box information\n        Object obj = { x0 , y0, w, h, 0, 0 };\n\n        // Find the object class with the highest confidence score\n        for (int class_idx = 0; class_idx &lt; num_classes; class_idx++)\n        {\n            // Get the confidence score for the current object class\n            float box_cls_score = feat_ptr[basic_pos + 5 + class_idx];\n            // Calculate the final confidence score for the object proposal\n            float box_prob = box_objectness * box_cls_score;\n\n            // Check for the highest confidence score\n            if (box_prob &gt; obj.prob) {\n                obj.label = class_idx;\n                obj.prob = box_prob;\n            }\n        }\n\n        // Only add object proposals with high enough confidence scores\n        if (obj.prob &gt; bbox_conf_thresh) proposals.push_back(obj);\n    }\n}\n\n\n\nCreate NmsSortedBboxes() Function\nAs mentioned earlier, it is possible for the model to detect the same object multiple times. This function will implement Non Maximum Suppression (NMS) to sort through the generated object proposals and filter out the extra detections.\nNote: This approach does not exclusively target duplicates of the same object. It is possible to filter out proposals for different objects if the second object overlaps too much of an object proposal with a higher confidence score. This is more likely when nms_thresh is set to a small value.\nWe can determine how much two bounding boxes overlap each other by dividing the area of where they overlap (i.e. the intersection) by the total area taken up by both bounding boxes (i.e. the union).\nWe can easily determine the intersection and union areas by creating OpenCV Rectangles using the bounding box information for the two object proposals.\n\nCode:\n// Filter through a sorted list of object proposals using Non-maximum suppression\nvoid NmsSortedBboxes() {\n\n    const int n = proposals.size();\n\n    // Iterate through the object proposals\n    for (int i = 0; i &lt; n; i++)\n    {\n        const Object& a = proposals[i];\n\n        // Create OpenCV rectangle for the Object bounding box\n        cv::Rect_&lt;float&gt; aRect = cv::Rect_&lt;float&gt;(a.x0, a.y0, a.width, a.height);\n        // Get the bounding box area\n        const float aRect_area = aRect.area();\n\n        bool keep = true;\n\n        // Check if the current object proposal overlaps any selected objects too much\n        for (int j = 0; j &lt; (int)picked.size(); j++)\n        {\n            const Object& b = proposals[picked[j]];\n\n            // Create OpenCV rectangle for the Object bounding box\n            cv::Rect_&lt;float&gt; bRect = cv::Rect_&lt;float&gt;(b.x0, b.y0, b.width, b.height);\n\n            // Calculate the area where the two object bounding boxes overlap\n            float inter_area = (aRect & bRect).area();\n            // Calculate the union area of both bounding boxes\n            float union_area = aRect_area + bRect.area() - inter_area;\n            // Ignore object proposals that overlap selected objects too much\n            if (inter_area / union_area &gt; nms_thresh) keep = false;\n        }\n\n        // Keep object proposals that do not overlap selected objects too much\n        if (keep) picked.push_back(i);\n    }\n}\n\n\n\nCreate DecodeOutputs() Function\nThis is where we will call the GenerateYoloxProposals() function with the raw model output, sort the generated proposals, and filter out the duplicate proposals with the NmsSortedBboxes() function.\nWe need to clear the proposals vector before calling the GenerateYoloxProposals() function, so that we do not add to the proposals from the previous model input.\nWe will use the std::sort() to sort the object proposals, before filtering out the duplicates. First, we need to create a comparison function to use str::sort(). This comparison function will simply check if the confidence score of the first object proposal is greater than the second.\n\nCode:\n// The comparison function for sorting the object proposals\nbool CompareProposals(const Object& a, const Object& b) {\n\n    return a.prob &gt; b.prob;\n}\nAs with the proposals vector, we need to clear the picked vector before calling the NmsSortedBboxes() function.\nOnce we have filtered the object proposals, we can update the count value with the final number of selected proposals.\n\n\nCode:\n// Process the model output to determine detected objects\nvoid DecodeOutputs(const float* prob) {\n\n    // Remove the proposals for the previous model output\n    proposals.clear();\n\n    // Generate new proposals for the current model output\n    GenerateYoloxProposals(prob);\n\n    // Sort the generated proposals based on their confidence scores\n    std::sort(proposals.begin(), proposals.end(), CompareProposals);\n\n    // Remove the picked proposals for the previous model output\n    picked.clear();\n    // Pick detected objects to keep using Non-maximum Suppression\n    NmsSortedBboxes();\n    // Update the number of objects detected\n    count = picked.size();\n}\n\n\n\nCreate PerformInference() Function\nWe will access the pixel data for the input image from Unity with a pointer to a uchar (unsigned 1 byte integer) array. We will store the pixel data in a cv::Mat variable.\nThe pixel data from Unity is in RGBA format while the model expects RGB format. Therefore, we need to first remove the alpha channel using the cv::cvtColor() method. We can then pass the cv::Mat to the StaticResize() function and store the result in a new cv::Mat.\nNext, we need to lock the memory for the model input so that we can fill it with the processed pixel data. We can get write-only access to the input tensor for the model with minput-&gt;wmap().\nThe pixel values are stored in a different order in the OpenCV Mat compared to the input tensor for the model. The Mat stores the red, green, and blue color values for a given pixel next to each other. In contrast, the input tensor stores all the red values for the entire image next to each other, then the green values, then the blue. We need to take this into account when writing values from texture to the input tensor.\nThe color values need to be scaled from the range [0, 255] to [0, 1]. We then need to standardize the color data using the mean and standard deviation values for the ImageNet dataset.\nAfter that, we can finally execute the model.\nAs with the model input, we need to lock the memory for the model output tensor, so that we can read the predictions. We can get read-only access to the output tensor with moutput-&gt;rmap().\nWe will store a reference to the model predictions in a float pointer and pass it to the DecodeOutputs() function.\n\nCode:\n// Perform inference with the provided texture data\nDLLExport void PerformInference(uchar* inputData) {\n\n    // Store the pixel data for the source input image\n    cv::Mat texture = cv::Mat(img_h, img_w, CV_8UC4);\n\n    // Assign the inputData to the OpenCV Mat\n    texture.data = inputData;\n    // Remove the alpha channel\n    cv::cvtColor(texture, texture, cv::COLOR_RGBA2RGB);\n    // Resize and pad the input image\n    cv::Mat pr_img = StaticResize(texture);\n\n    // The number of color channels \n    int num_channels = pr_img.channels();\n    // Get the number of pixels in the input image\n    int H = minput-&gt;getTensorDesc().getDims()[2];\n    int W = minput-&gt;getTensorDesc().getDims()[3];\n    int nPixels = W * H;\n\n    // locked memory holder should be alive all time while access to its buffer happens\n    LockedMemory&lt;void&gt; ilmHolder = minput-&gt;wmap();\n\n    // Filling input tensor with image data\n    float* input_data = ilmHolder.as&lt;float*&gt;();\n\n    // The mean of the ImageNet dataset used to train the model\n    const float mean[] = { 0.485, 0.456, 0.406 };\n    // The standard deviation of the ImageNet dataset used to train the model\n    const float standard_dev[] = { 0.229, 0.224, 0.225 };\n\n    // Iterate over each pixel in image\n    for (int p = 0; p &lt; nPixels; p++) {\n        // Iterate over each color channel for each pixel in image\n        for (int ch = 0; ch &lt; num_channels; ++ch) {\n            input_data[ch * nPixels + p] = (pr_img.data[p * num_channels + ch] / 255.0f - mean[ch]) / standard_dev[ch];\n        }\n    }\n\n    // Perform inference\n    infer_request.Infer();\n\n    // locked memory holder should be alive all time while access to its buffer happens\n    LockedMemory&lt;const void&gt; moutputHolder = moutput-&gt;rmap();\n    const float* net_pred = moutputHolder.as&lt;const PrecisionTrait&lt;Precision::FP32&gt;::value_type*&gt;();\n\n    // Process the model output\n    DecodeOutputs(net_pred);\n}\n\n\n\nCreate PopulateObjectsArray() Function\nOnce we have processed the model output we can initialize the Object array in Unity by calling the GetObjectCount() function. We can then pass a reference to the array and populate it with the proposals vector.\nBefore adding an object to the array, we will scale the bounding box information of the object proposal to the original image resolution.\nIt is possible that the final image coordinates may be slightly outside the actual image dimensions. We handle these edge cases by clamping the bounding box coordinates between 0 and the image dimensions.\n\nCode:\n// Fill the provided array with the detected objects\nDLLExport void PopulateObjectsArray(Object* objects) {\n\n    for (int i = 0; i &lt; count; i++)\n    {\n        Object object = proposals[picked[i]];\n\n        // Adjust offset to original unpadded dimensions\n        float x0 = (object.x0) / scale;\n        float y0 = (object.y0) / scale;\n\n        // Clamp the image coordinates to the original image dimensions\n        x0 = std::max(std::min(x0, (float)(img_w - 1)), 0.f);\n        y0 = std::max(std::min(y0, (float)(img_h - 1)), 0.f);\n\n        // Save the final object information\n        object.x0 = x0;\n        object.y0 = y0;\n\n        objects[i] = object;\n    }\n}"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#build-solution",
    "href": "posts/openvino-yolox-unity/part-2/index.html#build-solution",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Build Solution",
    "text": "Build Solution\nNow that the code is complete, we just need to build the solution to generate the .dll file.\nOpen the Build menu at the top of the Visual Studio window and click Build Solution. This will generate a new x64 folder in the project’s directory.\n\n\n\n\n\nNavigate to that folder in the File Explorer and open the Release child folder. Inside, we can see the .dll file along with a few other files that will not be needed."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#gather-dependencies",
    "href": "posts/openvino-yolox-unity/part-2/index.html#gather-dependencies",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Gather Dependencies",
    "text": "Gather Dependencies\nThe .dll file generated by our project is still dependent on other .dll files from both OpenVINO and OpenCV. Those .dll files have dependencies of their own as well. We will need to copy these dependencies along with the OpenVINO_Plugin.dll file into a new folder called x86_64 for the Unity project.\nHere are the dependencies needed to use our .dll.\n\nclDNNPlugin.dll\ninference_engine.dll\ninference_engine_ir_reader.dll\ninference_engine_legacy.dll\ninference_engine_lp_transformations.dll\ninference_engine_preproc.dll\ninference_engine_transformations.dll\nlibhwloc-5.dll\nMKLDNNPlugin.dll\nngraph.dll\nopencv_core_parallel_tbb452_64.dll\nopencv_core452.dll\nopencv_imgcodecs452.dll\nopencv_imgproc452.dll\nplugins.xml\ntbb.dll\n\nThe required dependencies can be found in the following directories.\n\nOpenVINO: C:Files (x86)_2021.3.394_engine\nnGraph: C:Files (x86)_2021.3.394_tools\nTBB: C:Files (x86)_2021.3.394_tools_engine\nOpenCV: C:Files (x86)_2021.3.394\n\nA folder containing the OpenVINO_Plugin.dll file and its dependencies is also available to download at the link below.\n\nPlugins folder"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-2/index.html#conclusion",
    "href": "posts/openvino-yolox-unity/part-2/index.html#conclusion",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)",
    "section": "Conclusion",
    "text": "Conclusion\nThat is everything we need for the OpenVINO™ functionality. In the next part, we will demonstrate how to access this functionality as a plugin inside a Unity project.\nProject Resources:\nGitHub Repository\n\nNext: Part 3"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html",
    "href": "posts/openvino-yolox-unity/part-3/index.html",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "",
    "text": "This tutorial is outdated. Use the new version at the link below.\nEnd-to-End Object Detection for Unity With IceVision and OpenVINO Pt. 1"
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#overview",
    "href": "posts/openvino-yolox-unity/part-3/index.html#overview",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of the tutorial, we first installed Unity, OpenVINO™, and the prerequisite software. We then downloaded some pretrained models in the OpenVINO™ Intermediate Representation format, along with some videos to test the models.\nIn Part 2, we walked through the steps to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pre-trained deep learning models.\nIn this part, we will demonstrate how to create a Unity project to access the DLL as a plugin."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#create-new-project",
    "href": "posts/openvino-yolox-unity/part-3/index.html#create-new-project",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Create New Project",
    "text": "Create New Project\nOpen Unity Hub and click the New button.\n\n\n\n\n\nWe can stick with the default 3D template and name the project OpenVINO_YOLOX_Demo. Take note of where the project will be generated and click Create."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#add-gui",
    "href": "posts/openvino-yolox-unity/part-3/index.html#add-gui",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Add GUI",
    "text": "Add GUI\nAt the moment, we cannot use our OpenVINO plugin inside the Unity editor. There appears to be a dependency conflict between one of the dependencies for this version of OpenVINO and the Unity editor. However, this should be resolved for future versions of OpenVINO.\nFor now, we need to build our Unity project without the plugin files, and then add them to the build folder where the project executable file is located.\nWe will be using the free Graphy asset to display frame rate and other performance metrics at runtime. A simple user interface is already set up in the prefab linked below.\n\nCanvas Prefab\n\n\nImport Canvas Prefab\nDownload the Canvas.prefab file from the above link and drop it into the assets folder in the Unity editor.\n\n\n\n\n\nDrag and drop the prefab from the Assets folder into the Hierarchy tab. A TMP Importer popup window will appear. Click Import TMP Essentials. Close the popup window once the import is complete.\n\n\n\n\n\nIf we select the Game tab, we can see the interface we just added. Don’t worry if it looks squished.\n\n\n\n\n\n\n\nInstall Graphy Package\nThis next step requires being signed into a Unity account. Open the link to the Graphy Unity Store page below and click Add to My Assets.\n\nGraphy Asset Store Page: (link)\n\n\n\n\n\n\nThe Add to My Assets button should change to Open In Unity. Click the button again to open the asset in the Package Manager back in the editor.\nThe Package Manager window should popup in the editor with the Graphy asset selected. Click the download button in the bottom right corner.\n\n\n\n\n\nClick Import once the package is finished downloading. An Import Unity Package popup window will appear.\n\n\n\n\n\nClick Import in the popup window. Close the Package Manager window once the import is complete. There should now be a new folder called Graphy - Ultimate Stats Monitor in the Assets folder.\n\n\n\n\n\nInside the new folder, open the Prefab folder and drag the [Graphy] prefab into the Hierarchy tab. We will see that our game scene gets updated.\n\n\n\n\n\nWith the [Graphy] object still selected in the Hierarchy tab. Scroll down in the Inspector tab to the Graphy Manager (Script) section. Open the Graph modules position dropdown and select TOP_LEFT. Nothing will change in the game view, but the position will be updated when we build the project."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#create-video-player",
    "href": "posts/openvino-yolox-unity/part-3/index.html#create-video-player",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Create Video Player",
    "text": "Create Video Player\nFor this demo, we will obtain our input images from a video or webcam feed playing in the scene.\n\nCreate the Videos Folder\nIn the Assets window, right-click an empty space, select the Create option, and click Folder. Name the folder Videos.\n\n\n\n\n\nDouble-click the Videos folder to open it.\n\n\nAdd Video Files\nDrag and drop any video files from the file explorer into the Videos folder. We will be using the file names to populate the dropdown menu in the UI, so rename the files according to their target object class.\n\n\n\n\n\n\n\nCreate the Video Screen\nWe will use a Quad object for the screen. Right-click an empty space in the Hierarchy tab. Select the 3D Object section and click Quad. We can just name it VideoScreen.\n\n\n\n\n\nWe will be updating the VideoScreen dimensions in code based on the resolution of the video or webcam feed.\n\n\nAdd Video Player Component\nUnity has a Video Player component that provides the functionality to attach video files to the VideoScreen. With the VideoScreen object selected in the Hierarchy tab, click the Add Component button in the Inspector tab.\n\n\n\n\n\nType video into the search box and select Video Player from the search results.\n\n\n\n\n\nWe do not need to manually assign a video clip as this will be done in code.\n\n\nMake the Video Loop\nTick the Loop checkbox in the Inspector tab to make the video repeat when the project is running."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#add-label-canvas",
    "href": "posts/openvino-yolox-unity/part-3/index.html#add-label-canvas",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Add Label Canvas",
    "text": "Add Label Canvas\nWe will attach the labels for each bounding box to a separate Canvas rather than the one containing the user interface. This will allow us to hide the user interface without hiding the labels.\nRight-click an empty space in the Hierarchy tab. Select the UI section and click Canvas. We can just name it Label Canvas."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#enable-unsafe-code",
    "href": "posts/openvino-yolox-unity/part-3/index.html#enable-unsafe-code",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Enable Unsafe Code",
    "text": "Enable Unsafe Code\nIn order to pass references to Unity variables to the OpenVINO plugin we will need to enable unsafe code. Open the Edit menu at the top of the Unity Editor and select Project Settings.\n\n\n\n\n\nSelect Player from the side menu and open the Other Settings dropdown menu.\n\n\n\n\n\nScroll down to the Allow ‘unsafe’ Code checkbox and enable it."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#include-unlit-shaders",
    "href": "posts/openvino-yolox-unity/part-3/index.html#include-unlit-shaders",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Include Unlit Shaders",
    "text": "Include Unlit Shaders\nWe will be using Unlit shaders for both the video screen and the bounding boxes. By default, Unlit shaders are not included in project builds. We need to manually include them in the project settings.\nWhile still in the Project Settings window, select the Graphics submenu and scroll down to the Always Included Shaders section. Update the Size value to add an extra Element spot.\n\n\n\n\n\nSelect the new bottom shader spot.\n\n\n\n\n\nType Unlit/Texture shader into the Select Shader window and select Unlit/Texture from the available options. We can then close the Select Shader window.\n\n\n\n\n\nWe will also need the Unlit/Color shader for the bounding boxes so repeat these steps to add it as well.\n\n\n\n\n\nNow we can close the project settings window."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#set-background-color",
    "href": "posts/openvino-yolox-unity/part-3/index.html#set-background-color",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Set Background Color",
    "text": "Set Background Color\nOne last thing we can do to make things a bit cleaner is to set the background color to a solid color. This will look a bit nicer when we play videos that have a different aspect ratio than the screen.\nSelect the Main Camera object in the Hierarchy tab. Over in the Inspector tab, open the Clear Flags dropdown and select Solid Color.\n\n\n\n\n\nClick on the color bar next to Background to set the color.\n\n\n\n\n\nWe can make it pure black by setting the RGB color values to 0."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#set-up-project-code",
    "href": "posts/openvino-yolox-unity/part-3/index.html#set-up-project-code",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Set Up Project Code",
    "text": "Set Up Project Code\nNow, we can implement the code for the project.\n\nImageProcessing Compute Shader\nThe pixel data from Unity gets flipped when loaded into the cv::Mat texture variable in the DLL. We will need to flip the image before sending it to the plugin so that it will be in the correct orientation for the model. We’ll implement these steps on the GPU in a compute shader.\n\nCreate the Asset File\nCreate a new assets folder called Shaders. Enter the Shaders folder and right-click an empty space. Select Shader in the Create submenu and click Compute Shader. We’ll name it ImageProcessing.\n\n\nRemove the Default Code\nOpen the ImageProcessing file in the code editor. By default, the ComputeShader will contain the following code.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel CSMain\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;float4&gt; Result;\n\n[numthreads(8,8,1)]\nvoid CSMain (uint3 id : SV_DispatchThreadID)\n{\n    // TODO: insert actual code here!\n\n    Result[id.xy] = float4(id.x & id.y, (id.x & 15)/15.0, (id.y & 15)/15.0, 0.0);\n}\nDelete the CSMain function along with the #pragma kernel CSMain.\n\n\nCreate FlipXAxis() Function\nNext, we need to add a Texture2D variable to store the input image. Name it InputImage and give it a data type of . Use the same data type for the Result variable as well.\nWe also need to know the height and width of the image to swap the pixel values. We’ll store these values in int variables and store the new (x,y) coordinates for individual pixel values in an int2 variable.\n\nCode:\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel FlipXAxis\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n// The height of the input image\nint height;\n// Stores the new location for individual pixel values\nint2 coords;\n\n[numthreads(4, 4, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    // Update the y value for the pixel coordinates\n    coords = int2(id.x, height - id.y);\n    Result[id.xy] = float4(InputImage[coords].x, InputImage[coords].y, InputImage[coords].z, 1.0f);\n}\n\n\n\n\nCreate Utils Script\nWe need to create an Object struct for Unity to match the one we defined for the OpenVINO code. We will place the Object and the list of object classes inside a separate C# script called Utils so that we can access them from multiple scripts.\n\nCreate the Asset File\nCreate a new assets folder called Scripts. In the Scripts folder, right-click an empty space and select C# Script in the Create submenu. Name the script Utils.\n\n\nRequired Namespaces\nWe will start by adding the required namespaces at the top of the script.\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nSystem.Runtime.InteropServices: Provides a wide variety of members that support COM interop and platform invoke services.\n\nusing UnityEngine;\nusing System;\nusing System.Runtime.InteropServices;\n\n\nObject Struct\nThe Object struct in Unity has the exact same variables as the one in the DLL. We need to specify how the data in the struct is organized so that we can properly modify the Object array in the PopulateObjectsArray() function. We can use the StructLayout attribute to indicate how the data is laid out. In our case, the data is laid out sequentially, so we specify LayoutKind.Sequential.\n\nCode:\n// Indicate that the members of the struct are laid out sequentially\n[StructLayout(LayoutKind.Sequential)]\n/// &lt;summary&gt;\n/// Stores the information for a single object\n/// &lt;/summary&gt; \npublic struct Object\n{\n    // The X coordinate for the top left bounding box corner\n    public float x0;\n    // The Y coordinate for the top left bounding box corner\n    public float y0;\n    // The width of the bounding box\n    public float width;\n    // The height of the bounding box\n    public float height;\n    // The object class index for the detected object\n    public int label;\n    // The model confidence score for the object\n    public float prob;\n\n    public Object(float x0, float y0, float width, float height, int label, float prob)\n    {\n        this.x0 = x0;\n        this.y0 = y0;\n        this.width = width;\n        this.height = height;\n        this.label = label;\n        this.prob = prob;\n    }\n}\n\n\n\nobject_classes Tuple\nWe will store the ordered list of object classes along with unique color codes in a Tuple array. The color codes will be used to set the color for the bounding boxes.\nNote: The object classes need to be in the same order as what the model was trained on. This array will need to be updated when using models trained on a different dataset.\n\nCode:\n/// &lt;summary&gt;\n/// The color coded, ordered list of object classes the model was trained to detect\n/// &lt;/summary&gt;\npublic static Tuple&lt;string, Color&gt;[] object_classes = new Tuple&lt;string, Color&gt;[] \n{\n    Tuple.Create(\"person\",         new Color(0.000f, 0.447f, 0.741f)),\n    Tuple.Create(\"bicycle\",        new Color(0.850f, 0.325f, 0.098f)),\n    Tuple.Create(\"car\",            new Color(0.929f, 0.694f, 0.125f)),\n    Tuple.Create(\"motorcycle\",     new Color(0.494f, 0.184f, 0.556f)),\n    Tuple.Create(\"airplane\",       new Color(0.466f, 0.674f, 0.188f)),\n    Tuple.Create(\"bus\",            new Color(0.301f, 0.745f, 0.933f)),\n    Tuple.Create(\"train\",          new Color(0.635f, 0.078f, 0.184f)),\n    Tuple.Create(\"truck\",          new Color(0.300f, 0.300f, 0.300f)),\n    Tuple.Create(\"boat\",           new Color(0.600f, 0.600f, 0.600f)),\n    Tuple.Create(\"traffic light\",  new Color(1.000f, 0.000f, 0.000f)),\n    Tuple.Create(\"fire hydrant\",   new Color(1.000f, 0.500f, 0.000f)),\n    Tuple.Create(\"stop sign\",      new Color(0.749f, 0.749f, 0.000f)),\n    Tuple.Create(\"parking meter\",  new Color(0.000f, 1.000f, 0.000f)),\n    Tuple.Create(\"bench\",          new Color(0.000f, 0.000f, 1.000f)),\n    Tuple.Create(\"bird\",           new Color(0.667f, 0.000f, 1.000f)),\n    Tuple.Create(\"cat\",            new Color(0.333f, 0.333f, 0.000f)),\n    Tuple.Create(\"dog\",            new Color(0.333f, 0.667f, 0.000f)),\n    Tuple.Create(\"horse\",          new Color(0.333f, 1.000f, 0.000f)),\n    Tuple.Create(\"sheep\",          new Color(0.667f, 0.333f, 0.000f)),\n    Tuple.Create(\"cow\",            new Color(0.667f, 0.667f, 0.000f)),\n    Tuple.Create(\"elephant\",       new Color(0.667f, 1.000f, 0.000f)),\n    Tuple.Create(\"bear\",           new Color(1.000f, 0.333f, 0.000f)),\n    Tuple.Create(\"zebra\",          new Color(1.000f, 0.667f, 0.000f)),\n    Tuple.Create(\"giraffe\",        new Color(1.000f, 1.000f, 0.000f)),\n    Tuple.Create(\"backpack\",       new Color(0.000f, 0.333f, 0.500f)),\n    Tuple.Create(\"umbrella\",       new Color(0.000f, 0.667f, 0.500f)),\n    Tuple.Create(\"handbag\",        new Color(0.000f, 1.000f, 0.500f)),\n    Tuple.Create(\"tie\",            new Color(0.333f, 0.000f, 0.500f)),\n    Tuple.Create(\"suitcase\",       new Color(0.333f, 0.333f, 0.500f)),\n    Tuple.Create(\"frisbee\",        new Color(0.333f, 0.667f, 0.500f)),\n    Tuple.Create(\"skis\",           new Color(0.333f, 1.000f, 0.500f)),\n    Tuple.Create(\"snowboard\",      new Color(0.667f, 0.000f, 0.500f)),\n    Tuple.Create(\"sports ball\",    new Color(0.667f, 0.333f, 0.500f)),\n    Tuple.Create(\"kite\",           new Color(0.667f, 0.667f, 0.500f)),\n    Tuple.Create(\"baseball bat\",   new Color(0.667f, 1.000f, 0.500f)),\n    Tuple.Create(\"baseball glove\", new Color(1.000f, 0.000f, 0.500f)),\n    Tuple.Create(\"skateboard\",     new Color(1.000f, 0.333f, 0.500f)),\n    Tuple.Create(\"surfboard\",      new Color(1.000f, 0.667f, 0.500f)),\n    Tuple.Create(\"tennis racket\",  new Color(1.000f, 1.000f, 0.500f)),\n    Tuple.Create(\"bottle\",         new Color(0.000f, 0.333f, 1.000f)),\n    Tuple.Create(\"wine glass\",     new Color(0.000f, 0.667f, 1.000f)),\n    Tuple.Create(\"cup\",            new Color(0.000f, 1.000f, 1.000f)),\n    Tuple.Create(\"fork\",           new Color(0.333f, 0.000f, 1.000f)),\n    Tuple.Create(\"knife\",          new Color(0.333f, 0.333f, 1.000f)),\n    Tuple.Create(\"spoon\",          new Color(0.333f, 0.667f, 1.000f)),\n    Tuple.Create(\"bowl\",           new Color(0.333f, 1.000f, 1.000f)),\n    Tuple.Create(\"banana\",         new Color(0.667f, 0.000f, 1.000f)),\n    Tuple.Create(\"apple\",          new Color(0.667f, 0.333f, 1.000f)),\n    Tuple.Create(\"sandwich\",       new Color(0.667f, 0.667f, 1.000f)),\n    Tuple.Create(\"orange\",         new Color(0.667f, 1.000f, 1.000f)),\n    Tuple.Create(\"broccoli\",       new Color(1.000f, 0.000f, 1.000f)),\n    Tuple.Create(\"carrot\",         new Color(1.000f, 0.333f, 1.000f)),\n    Tuple.Create(\"hot dog\",        new Color(1.000f, 0.667f, 1.000f)),\n    Tuple.Create(\"pizza\",          new Color(0.333f, 0.000f, 0.000f)),\n    Tuple.Create(\"donut\",          new Color(0.500f, 0.000f, 0.000f)),\n    Tuple.Create(\"cake\",           new Color(0.667f, 0.000f, 0.000f)),\n    Tuple.Create(\"chair\",          new Color(0.833f, 0.000f, 0.000f)),\n    Tuple.Create(\"couch\",          new Color(1.000f, 0.000f, 0.000f)),\n    Tuple.Create(\"potted plant\",   new Color(0.000f, 0.167f, 0.000f)),\n    Tuple.Create(\"bed\",            new Color(0.000f, 0.333f, 0.000f)),\n    Tuple.Create(\"dining table\",   new Color(0.000f, 0.500f, 0.000f)),\n    Tuple.Create(\"toilet\",         new Color(0.000f, 0.667f, 0.000f)),\n    Tuple.Create(\"tv\",             new Color(0.000f, 0.833f, 0.000f)),\n    Tuple.Create(\"laptop\",         new Color(0.000f, 1.000f, 0.000f)),\n    Tuple.Create(\"mouse\",          new Color(0.000f, 0.000f, 0.167f)),\n    Tuple.Create(\"remote\",         new Color(0.000f, 0.000f, 0.333f)),\n    Tuple.Create(\"keyboard\",       new Color(0.000f, 0.000f, 0.500f)),\n    Tuple.Create(\"cell phone\",     new Color(0.000f, 0.000f, 0.667f)),\n    Tuple.Create(\"microwave\",      new Color(0.000f, 0.000f, 0.833f)),\n    Tuple.Create(\"oven\",           new Color(0.000f, 0.000f, 1.000f)),\n    Tuple.Create(\"toaster\",        new Color(0.000f, 0.000f, 0.000f)),\n    Tuple.Create(\"sink\",           new Color(0.143f, 0.143f, 0.143f)),\n    Tuple.Create(\"refrigerator\",   new Color(0.286f, 0.286f, 0.286f)),\n    Tuple.Create(\"book\",           new Color(0.429f, 0.429f, 0.429f)),\n    Tuple.Create(\"clock\",          new Color(0.571f, 0.571f, 0.571f)),\n    Tuple.Create(\"vase\",           new Color(0.714f, 0.714f, 0.714f)),\n    Tuple.Create(\"scissors\",       new Color(0.857f, 0.857f, 0.857f)),\n    Tuple.Create(\"teddy bear\",     new Color(0.000f, 0.447f, 0.741f)),\n    Tuple.Create(\"hair drier\",     new Color(0.314f, 0.717f, 0.741f)),\n    Tuple.Create(\"toothbrush\",     new Color(0.50f, 0.5f, 0f))\n};\n\n\n\n\nCreate BoundingBox Script\nWe will implement the functionality for creating bounding boxes in a new script called BoundingBox. The BoundingBox class will handle creating a single bounding box along with updating its object class, position, and size. We will be creating as many BoundingBox instances as needed to handle the number of objects specified by the GetObjectCount() function.\n\nRequired Namespaces\n\nTMPro: Provides advanced text rendering capabilities\n\nusing UnityEngine;\nusing TMPro;\n\n\nProperties\n\nbbox: A new GameObject will be created for each bounding box.\ntext: The text showing the labe and confidence score will be stored in a separate GameObject as it needs to be attached to a Canvas\ncanvas: We will store a reference to the Label Canvas object so that we can attach the text object to it.\ninfo: This will store the Object struct for the bounding box.\ncolor: This will store the color specific to the current object class\nlineWidth: We shall set the line thickness for the bounding box based on the current screen size. Feel free to make this larger or smaller based on personal preference.\nfontSize: We shall also set the font size for the bounding box text based on the current screen size. Again, feel free to adjust this based on personal preference.\ntextContent: We can display the text for the bounding using the TextMeshProUGUI component. We will attach this component to the text object.\nlineRenderer: We can draw the bounding box itself using a LineRenderer component. We will attach this component to the bbox object.\n\n\nCode:\n// Contains the bounding box\nprivate GameObject bbox = new GameObject();\n// Contains the label text\nprivate GameObject text = new GameObject();\n// The canvas on which the bounding box labels will be drawn\nprivate GameObject canvas = GameObject.Find(\"Label Canvas\");\n\n// The object information for the bounding box\nprivate Utils.Object info;\n\n// The object class color\nprivate Color color;\n\n// The adjusted line width for the bounding box\nprivate int lineWidth = (int)(Screen.width * 1.75e-3);\n// The adjusted font size based on the screen size\nprivate float fontSize = (float)(Screen.width * 9e-3);\n\n// The label text\nprivate TextMeshProUGUI textContent;\n\n// Draws the bounding box\nprivate LineRenderer lineRenderer;\n\n\n\nInitializeLabel()\nThe text for each bounding box will contain the name of the object class and the confidence score from the model output.\nWe shall make the text the color specified by the object class, left-justified, and aligned in the middle of the text box. The default size of the text box might be too small for some of the longer object names, so we shall make it a bit wider as well.\nThe text will be positioned just above the top-left corner of the bounding box. We can use the WorldToScreenPoint() method to map the desired in-game position to screen space.\n\nCode:\n/// &lt;summary&gt;\n/// Initialize the label for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"label\"&gt;&lt;/param&gt;\nprivate void InitializeLabel()\n{\n    // Set the label text\n    textContent.text = $\"{text.name}: {(info.prob * 100).ToString(\"0.##\")}%\";\n    // Set the text color\n    textContent.color = color;\n    // Set the text alignment\n    textContent.alignment = TextAlignmentOptions.MidlineLeft;\n    // Set the font size\n    textContent.fontSize = fontSize;\n    // Resize the text area\n    RectTransform rectTransform = text.GetComponent&lt;RectTransform&gt;();\n    rectTransform.sizeDelta = new Vector2(250, 50);\n    // Position the label above the top left corner of the bounding box\n    Vector3 textPos = Camera.main.WorldToScreenPoint(new Vector3(info.x0, info.y0, -10f));\n    float xOffset = rectTransform.rect.width / 2;\n    textPos = new Vector3(textPos.x + xOffset, textPos.y + textContent.fontSize, textPos.z);\n    text.transform.position = textPos;\n}\n\n\n\nToggleBBox()\nWe can toggle the visibility for the bounding box and text using the SetActive() method. We will use this to hide any extra bounding boxes and to unhide them when they are needed again.\n\nCode:\n/// &lt;summary&gt;\n/// Toggle the visibility for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"show\"&gt;&lt;/param&gt;\npublic void ToggleBBox(bool show)\n{\n    bbox.SetActive(show);\n    text.SetActive(show);\n}\n\n\n\nInitializeBBox()\nThe bounding box will be the color specified by the object class. The lineRender will consist of five points to create a closed box; four for each corner and one to close the box.\n\nPoint #0: Top-left corner\nPoint #1: Top-right corner\nPoint #2: Bottom-right corner\nPoint #3: Bottom-left corner\nPoint #4: Top-left corner (close loop)\n\n\nCode:\n/// &lt;summary&gt;\n/// Initialize the position and dimensions for the bounding box\n/// &lt;/summary&gt;\nprivate void InitializeBBox()\n{\n    // Set the material color\n    lineRenderer.material.color = color;\n\n    // The bbox will consist of five points\n    lineRenderer.positionCount = 5;\n\n    // Set the width from the start point\n    lineRenderer.startWidth = lineWidth;\n    // Set the width from the end point\n    lineRenderer.endWidth = lineWidth;\n\n    // Get object information\n    float x0 = info.x0;\n    float y0 = info.y0;\n    float width = info.width;\n    float height = info.height;\n\n    // Offset value to align the bounding box points\n    float offset = lineWidth / 2;\n\n    // Top left point\n    Vector3 pos0 = new Vector3(x0, y0, 0);\n    lineRenderer.SetPosition(0, pos0);\n    // Top right point\n    Vector3 pos1 = new Vector3(x0 + width, y0, 0);\n    lineRenderer.SetPosition(1, pos1);\n    // Bottom right point\n    Vector3 pos2 = new Vector3(x0 + width, (y0 - height) + offset, 0);\n    lineRenderer.SetPosition(2, pos2);\n    // Bottom left point\n    Vector3 pos3 = new Vector3(x0 + offset, (y0 - height) + offset, 0);\n    lineRenderer.SetPosition(3, pos3);\n    // Closing Point\n    Vector3 pos4 = new Vector3(x0 + offset, y0 + offset, 0);\n    lineRenderer.SetPosition(4, pos4);\n\n    // Make sure the bounding box is visible\n    ToggleBBox(true);\n}\n\n\n\nSetObjectInfo()\nWe will call the InitializeLabel() and InitializeBBox() methods from a new method called SetObjectInfo(). This method will also update the names for the bbox and text objects along with the color. We will use this method to update the bounding boxes based on the latest model output.\n\nCode:\n/// &lt;summary&gt;\n/// Update the object info for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"objectInfo\"&gt;&lt;/param&gt;\npublic void SetObjectInfo(Utils.Object objectInfo)\n{\n    // Set the object info\n    info = objectInfo;\n    // Get the object class label\n    bbox.name = Utils.object_classes[objectInfo.label].Item1;\n    text.name = bbox.name;\n    // Get the object class color\n    color = Utils.object_classes[objectInfo.label].Item2;\n\n    // Initialize the label\n    InitializeLabel();\n    // Initialize the position and dimensions\n    InitializeBBox();\n}\n\n\n\nConstructor\nThere are a few steps we need to take whenever we create a new BoundingBox instance.\nFirst, we need to attach the textContent component to the text object and make the text object a child of the Label Canvas.\nNext, we need to add the lineRender component to the bbox object and give it a new material with an Unlit/Color shader.\nLastly, we call the SetObjectInfo() method to update the bounding box and text.\n\nCode:\n/// &lt;summary&gt;\n/// Constructor for the bounding box\n/// &lt;/summary&gt;\n/// &lt;param name=\"objectInfo\"&gt;&lt;/param&gt;\npublic BoundingBox(Utils.Object objectInfo)\n{\n    // Add a text component to store the label text\n    textContent = text.AddComponent&lt;TextMeshProUGUI&gt;();\n    // Assign text object to the label canvas\n    text.transform.SetParent(canvas.transform);\n\n    // Add a line renderer to draw the bounding box\n    lineRenderer = bbox.AddComponent&lt;LineRenderer&gt;();\n    // Make LineRenderer Shader Unlit\n    lineRenderer.material = new Material(Shader.Find(\"Unlit/Color\"));\n\n    // Update the object info for the bounding box\n    SetObjectInfo(objectInfo);\n}\n\n\n\n\nCreate ObjectDetector Script\nWe will implement the functionality for calling the plugin functions and handling the model output in a new script. Open the Scripts folder in the Assets section and create a new C# script called ObjectDetector.\n\nAdd Required Namespaces\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nSystem.Runtime.InteropServices: Provides a wide variety of members that support COM interop and platform invoke services.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Video: Provides access to the functionality for the Video Player component.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\n\n\nCode:\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.Video;\nusing UnityEngine.Rendering;\nusing System;\nusing System.Runtime.InteropServices;\nusing UnityEngine.UI;\n\n\n\nAdd Public Variables\n\nvideoScreen: We need a reference to the VideoScreen object so that we can update its dimensions and access the VideoPlayer component.\nimageProcessingShader: We need a reference to the imageProcessing shader so that we can access the FlipXAxis function.\ndeviceDropdown: We need access to the device dropdown menu, and update the list of available compute devices.\nmodelDropdown: The script will look for available model files when the application first starts. We will update the menu options based on what model files are found.\nvideoDropdown:\ninference: The Inference toggle button in the UI will be used to indicate whether we want to execute the mode or just play the video feed.\nuseWebcam: The Webcam toggle button in the UI will be used to switch between using a video file or webcam feed as input for the model.\nuseAsync: The Use Async toggle button will be used to turn asynchronous GPU readback on and off (more on this later).\nconsoleText: We don’t have direct access to the console while running the application. However, we can capture the text that gets sent to the console and display it in the user interface. This can be useful for debugging purposes.\nvideoClips: We will select which video files we want to have available in the Inspector tab.\n\n\nCode:\n[Tooltip(\"The screen for viewing preprocessed images\")]\npublic Transform videoScreen;\n\n[Tooltip(\"Performs the preprocessing steps\")]\npublic ComputeShader imageProcessingShader;\n\n[Tooltip(\"Switch between the available compute devices for OpenVINO\")]\npublic TMPro.TMP_Dropdown deviceDropdown;\n\n[Tooltip(\"Switch between the available OpenVINO models\")]\npublic TMPro.TMP_Dropdown modelDropdown;\n\n[Tooltip(\"Switch between the available video files\")]\npublic TMPro.TMP_Dropdown videoDropdown;\n\n[Tooltip(\"Turn stylization on and off\")]\npublic Toggle inference;\n\n[Tooltip(\"Use webcam feed as input\")]\npublic Toggle useWebcam;\n\n[Tooltip(\"Turn AsyncGPUReadback on and off\")]\npublic Toggle useAsync;\n\n[Tooltip(\"Text area to display console output\")]\npublic Text consoleText;\n\n[Tooltip(\"List of available video files\")]\npublic VideoClip[] videoClips;\n\n\n\nCreate DLL Method Declarations\nWe need to specify the name of the .dll file. We’ll store this in a const string variable called dll.\nWe can indicate that a given method is from a DLL by using the DllImport attribute.\nNote: The DLL functions that return string values have IntPtr as their return values here. We will use the Marshal.PtrToStringAnsi() method to get that string value at the memory location stored in the pointer.\n\nCode:\n// Name of the DLL file\nconst string dll = \"OpenVINO_YOLOX_DLL\";\n\n[DllImport(dll)]\nprivate static extern IntPtr GetAvailableDevices();\n\n[DllImport(dll)]\nprivate static extern IntPtr InitOpenVINO(string model, int width, int height, int device);\n\n[DllImport(dll)]\nprivate static extern void PerformInference(IntPtr inputData);\n\n[DllImport(dll)]\nprivate static extern void PopulateObjectsArray(IntPtr objects);\n\n[DllImport(dll)]\nprivate static extern int GetObjectCount();\n\n[DllImport(dll)]\nprivate static extern void SetNMSThreshold(float threshold);\n\n[DllImport(dll)]\nprivate static extern void SetConfidenceThreshold(float threshold);\n\n\n\nAdd Private Variables\n\nwebcamDims: We will set the target resolution for a webcam to 1280x720\nvideoDims: We need to keep track of the dimensions of the video screen for different methods in the script.\ntargetDims: We will set the default target dims to 640x640. However, this will get updated to maintain the aspect ratio for the current video source.\nimageDims: This will store the unpadded dimensions of the image being fed to the model.\nwebcamTexture: This will provide access to live video input from a webcam.\nvideoTexture: This will store the current source video texture.\nrTex: This will store the texture used to create the input texture.\ninputTex: This contains the input texture that will be sent to the OpenVINO inference engine.\nperformInference: Keeps track of whether to execute the OpenVINO model.\nwebcamFPS: We will set the target frame rate for the webcam to 60fps.\naspectRatioScale: Used to scale the input image dimensions while maintaining aspect ratio.\ncurrentDevice: Current compute device for OpenVINO.\ninputData: Stores the raw pixel data for inputTex.\nobjectInfoArray: This is the Object array that we will update using the PopulateObjectsArray() function in the plugin.\nboundingBoxes: Stores the bounding boxes for detected objects.\ndeviceList: Parsed list of compute devices for OpenVINO.\nopenVINOPaths: File paths for the OpenVINO IR models\nopenvinoModels: Names of the OpenVINO IR model\nvideoNames: Names of the available video files\ncanvas: A reference to the canvas for the user interface\ngraphy: A reference to the Graphy on-screen metrics\nwidth: A references to the width input field for the target image dimensions\nheight: A references to the height input field for the target image dimensions\n\n\nCode:\n// The requested webcam dimensions\nprivate Vector2Int webcamDims = new Vector2Int(1280, 720);\n// The dimensions of the current video source\nprivate Vector2Int videoDims;\n// The targrt resolution for input images\nprivate Vector2Int targetDims = new Vector2Int(640, 640);\n// The unpadded dimensions of the image being fed to the model\nprivate Vector2Int imageDims = new Vector2Int(0, 0);\n\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n\n// The source video texture\nprivate RenderTexture videoTexture;\n// The texture used to create the input texture\nprivate RenderTexture rTex;\n\n// Contains the input texture that will be sent to the OpenVINO inference engine\nprivate Texture2D inputTex;\n\n// Keeps track of whether to execute the OpenVINO model\nprivate bool performInference = true;\n\n// The requested webcam frame rate\nprivate int webcamFPS = 60;\n\n// Used to scale the input image dimensions while maintaining aspect ratio\nprivate float aspectRatioScale;\n\n// Current compute device for OpenVINO\nprivate string currentDevice;\n\n// Stores the raw pixel data for inputTex\nprivate byte[] inputData;\n// Stores information about detected objects\nprivate Utils.Object[] objectInfoArray;\n\n// Stores the bounding boxes for detected objects\nprivate List&lt;BoundingBox&gt; boundingBoxes = new List&lt;BoundingBox&gt;();\n// Parsed list of compute devices for OpenVINO\nprivate List&lt;string&gt; deviceList = new List&lt;string&gt;();\n// File paths for the OpenVINO IR models\nprivate List&lt;string&gt; openVINOPaths = new List&lt;string&gt;();\n// Names of the OpenVINO IR model\nprivate List&lt;string&gt; openvinoModels = new List&lt;string&gt;();\n// Names of the available video files \nprivate List&lt;string&gt; videoNames = new List&lt;string&gt;();\n\n// A reference to the canvas for the user interface\nprivate GameObject canvas;\n// A reference to the Graphy on-screen metrics\nprivate GameObject graphy;\n// References to input fields for the target image dimensions\nprivate TMPro.TMP_InputField width;\nprivate TMPro.TMP_InputField height;\n\n\n\nCreate Log() Method\nWe can capture the console output using the Applications.logMessageReceived callback. We just need to append a function call to this callback that updates consoleText with the latest console message. This function will be called every time a log message is received. All it does is append the latest console message to the existing text for consoleText.\n\nCode:\n/// &lt;summary&gt;\n/// Updates on-screen console text\n/// &lt;/summary&gt;\n/// &lt;param name=\"logString\"&gt;&lt;/param&gt;\n/// &lt;param name=\"stackTrace\"&gt;&lt;/param&gt;\n/// &lt;param name=\"type\"&gt;&lt;/param&gt;\npublic void Log(string logString, string stackTrace, LogType type)\n{\n    consoleText.text = consoleText.text + \"\\n \" + logString;\n}\n\n\n\nDefine OnEnable() Method\nWe will append the Log() method to the Applications.logMessageReceived callback in the OnEnable() method. The OnEnable() method is called when the GameObject the script is attached to becomes enabled.\n\nCode:\n// Called when the object becomes enabled and active\nvoid OnEnable() \n{\n    Application.logMessageReceived += Log;\n}\n\n\n\nCreate InitializeVideoScreen() Method\nWe will update the position, orientation, and size of the VideoScreen object in a new method called InitializeVideoScreen. The method will take in width and height values.\nFirst, we will set the video player component to render to a RenderTexture and set videoTexture as the target texture.\nThe default shader assigned to the VideoScreen object needs to be replaced with an Unlit/Texture shader. This will remove the need for the screen to be lit by an in-game light.\nLastly, we will update the dimension and position of the VideoScreen object.\n\nCode:\n/// &lt;summary&gt;\n/// Prepares the videoScreen GameObject to display the chosen video source.\n/// &lt;/summary&gt;\n/// &lt;param name=\"width\"&gt;&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;&lt;/param&gt;\nprivate void InitializeVideoScreen(int width, int height)\n{\n    // Set the render mode for the video player\n    videoScreen.GetComponent&lt;VideoPlayer&gt;().renderMode = VideoRenderMode.RenderTexture;\n\n    // Use new videoTexture for Video Player\n    videoScreen.GetComponent&lt;VideoPlayer&gt;().targetTexture = videoTexture;\n\n    // Apply the new videoTexture to the VideoScreen Gameobject\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.shader = Shader.Find(\"Unlit/Texture\");\n    videoScreen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.SetTexture(\"_MainTex\", videoTexture);\n    // Adjust the VideoScreen dimensions for the new videoTexture\n    videoScreen.localScale = new Vector3(width, height, videoScreen.localScale.z);\n    // Adjust the VideoScreen position for the new videoTexture\n    videoScreen.position = new Vector3(width / 2, height / 2, 1);\n}\n\n\n\nCreate InitializeWebcam() Method\nWhen using a webcam, we will first initialize the webcamTexture with the target resolution and framerate. We will not know if the target resolution is supported by the webcam until we try playing it.\nIf there is no physical webcam available, Unity may try using a virtual camera from applications like OBS. We can tell when this happens by checking the actual dimensions of the webcamTexture. If the resolution is set to 16x16, then it is safe to say something went wrong. When this happens, we will disable the webcam functionality.\nIf the webcamTexture is properly initialized, we will limit the target framerate to the same as the target frame rate for the webcam. This can help the application run more smoothly.\nWe will also disable the Video Player component. Lastly, we will update the values for videoDims with the final dimensions for the webcamTexture.\n\nCode:\n/// &lt;summary&gt;\n/// Try to initialize and start a webcam\n/// &lt;/summary&gt;\nprivate void InitializeWebcam()\n{\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the Camera\n    webcamTexture.Play();\n\n    if (webcamTexture.width == 16)\n    {\n        webcamTexture.Stop();\n        Debug.Log(\"\\nUnable to initialize a webcam. Disabling option.\\n\");\n        useWebcam.isOn = false;\n        useWebcam.enabled = false;\n    }\n    else\n    {\n        // Limit application framerate to the target webcam framerate\n        Application.targetFrameRate = webcamFPS;\n\n        // Deactivate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = false;\n\n        // Update the videoDims.y\n        videoDims.y = webcamTexture.height;\n        // Update the videoDims.x\n        videoDims.x = webcamTexture.width;\n    }\n}\n\n\n\nCreate InitializeCamera() Method\nOnce the VideoScreen has been updated, either for a video or webcam feed, we need to resize and reposition the in-game camera. We will do so in a new method called InitializeCamera.\nWe can access the Main Camera object with GameObject.Find(“Main Camera”). We will set the X and Y coordinates to the same as the VideoScreen position.\nThe camera also needs to be set to orthographic mode to remove perspective.\nLastly, we need to update the size of the camera. The orthographicSize attribute is actually the half size, so we need to divide videoDims.y (i.e. the height) by 2 as well.\n\nCode:\n/// &lt;summary&gt;\n/// Resizes and positions the in-game Camera to accommodate the video dimensions\n/// &lt;/summary&gt;\nprivate void InitializeCamera()\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject mainCamera = GameObject.Find(\"Main Camera\");\n    // Adjust the camera position to account for updates to the VideoScreen\n    mainCamera.transform.position = new Vector3(videoDims.x / 2, videoDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    mainCamera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the VideoScreen\n    int orthographicSize;\n    if (((float)Screen.width / Screen.height) &lt; ((float)videoDims.x / videoDims.y)){\n        float scale = ((float)Screen.width / Screen.height) /\n            ((float)videoDims.x / videoDims.y);\n        orthographicSize = (int)((videoDims.y / 2) / scale);\n    }\n    else\n    {\n        orthographicSize = (int)(videoDims.y / 2);\n    }\n\n    Debug.Log($\"Orthographic Size: {orthographicSize}\");\n    mainCamera.GetComponent&lt;Camera&gt;().orthographicSize = orthographicSize;\n}\n\n\n\nCreate InitializeTextures() Method\nWhenever the target input resolution is updated, we need to make sure that it maintains the same aspect ratio as the source video or webcam feed. Feeding a stretched or squashed image to the model could impact the model’s accuracy.\nOnce we have adjusted the target input resolution, we can update rTex and inputTex with the new dimensions. We will also update the width and height input fields to make sure the user knows the adjusted resolution.\n\nCode:\n/// &lt;summary&gt;\n/// Calculate the dimensions for the input image\n/// &lt;/summary&gt;\n/// &lt;param name=\"newVideo\"&gt;&lt;/param&gt;\nprivate void InitializeTextures(bool newVideo = false)\n{\n    if (newVideo)\n    {\n        // Calculate scale for new  aspect ratio\n        int min = Mathf.Min(videoTexture.width, videoTexture.height);\n        int max = Mathf.Max(videoTexture.width, videoTexture.height);\n        aspectRatioScale = (float)min / max;\n\n        // Adjust the smallest input dimension to maintain the new aspect ratio\n        if (max == videoTexture.height)\n        {\n            imageDims.x = (int)(targetDims.y * aspectRatioScale);\n            imageDims.y = targetDims.y;\n        }\n        else\n        {\n            imageDims.y = (int)(targetDims.x * aspectRatioScale);\n            imageDims.x = targetDims.x;\n        }\n    }\n    else\n    {\n        // Adjust the input dimensions to maintain the current aspect ratio\n        if (imageDims.x != targetDims.x)\n        {\n            imageDims.x = targetDims.x;\n            aspectRatioScale = (float)videoTexture.height / videoTexture.width;\n            imageDims.y = (int)(targetDims.x * aspectRatioScale);\n            targetDims.y = imageDims.y;\n\n        }\n        if (imageDims.y != targetDims.y)\n        {\n            imageDims.y = targetDims.y;\n            aspectRatioScale = (float)videoTexture.width / videoTexture.height;\n            imageDims.x = (int)(targetDims.y * aspectRatioScale);\n            targetDims.x = imageDims.x;\n\n        }\n    }\n\n    // Initialize the RenderTexture that will store the processed input image\n    rTex = RenderTexture.GetTemporary(imageDims.x, imageDims.y, 24, RenderTextureFormat.ARGB32);\n    // Update inputTex with the new dimensions\n    inputTex = new Texture2D(imageDims.x, imageDims.y, TextureFormat.RGBA32, false);\n\n    // Update the values for the width and height input fields\n    Debug.Log($\"Setting Input Dims to W: {imageDims.x} x H: {imageDims.y}\");\n    width.text = $\"{imageDims.x}\";\n    height.text = $\"{imageDims.y}\";\n}\n\n\n\nCreate InitializeDropDowns() Method\nThis method will simply clear the default options for the dropdown menus in the UI, add the current options, and select the first option in the list.\n\nCode:\n/// &lt;summary&gt;\n/// Initialize the options for the dropdown menus\n/// &lt;/summary&gt;\nprivate void InitializeDropdowns()\n{\n    // Remove default dropdown options\n    deviceDropdown.ClearOptions();\n    // Add OpenVINO compute devices to dropdown\n    deviceDropdown.AddOptions(deviceList);\n    // Set the value for the dropdown to the current compute device\n    deviceDropdown.SetValueWithoutNotify(deviceList.IndexOf(currentDevice));\n\n    // Remove default dropdown options\n    videoDropdown.ClearOptions();\n    // Add OpenVINO models to menu\n    videoDropdown.AddOptions(videoNames);\n    // Select the first option in the dropdown\n    videoDropdown.SetValueWithoutNotify(0);\n\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add OpenVINO models to menu\n    modelDropdown.AddOptions(openvinoModels);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n}\n\n\n\nCreate InitializeOpenVINO() Method\nWe will call the InitiOpenVINO() function from the plugin in a new method called InitializeOpenVINO(). The InitiOpenVINO() function will only be called when the inference toggle is checked.\n\nCode:\n/// &lt;summary&gt;\n/// Called when a model option is selected from the dropdown\n/// &lt;/summary&gt;\npublic void InitializeOpenVINO()\n{\n    // Only initialize OpenVINO when performing inference\n    if (performInference == false) return;\n\n    Debug.Log(\"Initializing OpenVINO\");\n    Debug.Log($\"Selected Model: {openvinoModels[modelDropdown.value]}\");\n    Debug.Log($\"Selected Model Path: {openVINOPaths[modelDropdown.value]}\");\n    Debug.Log($\"Setting Input Dims to W: {imageDims.x} x H: {imageDims.y}\");\n    Debug.Log(\"Uploading IR Model to Compute Device\");\n\n    // Set up the neural network for the OpenVINO inference engine\n    currentDevice = Marshal.PtrToStringAnsi(InitOpenVINO(\n        openVINOPaths[modelDropdown.value],\n        inputTex.width,\n        inputTex.height,\n        deviceDropdown.value));\n\n    Debug.Log($\"OpenVINO using: {currentDevice}\");\n}\n\n\n\nCreate InitializationSteps() Method\nWe will call the initialization methods in a new method called InitializationSteps(). This method will be called each time the model input is updated. This could be from changing the target input resolution, selecting a different video, or switching between using a webcam.\n\nCode:\n/// &lt;summary&gt;\n/// Perform the initialization steps required when the model input is updated\n/// &lt;/summary&gt;\nprivate void InitializationSteps()\n{\n    if (useWebcam.isOn)\n    {\n        // Initialize webcam\n        InitializeWebcam();\n    }\n    else\n    {\n        Debug.Log($\"Selected Video: {videoDropdown.value}\");\n\n        // Set Initial video clip\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().clip = videoClips[videoDropdown.value];\n        // Update the videoDims.y\n        videoDims.y = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().height;\n        // Update the videoDims.x\n        videoDims.x = (int)videoScreen.GetComponent&lt;VideoPlayer&gt;().width;\n    }\n\n    // Create a new videoTexture using the current video dimensions\n    videoTexture = RenderTexture.GetTemporary(videoDims.x, videoDims.y, 24, RenderTextureFormat.ARGB32);\n\n    // Initialize the videoScreen\n    InitializeVideoScreen(videoDims.x, videoDims.y);\n    // Adjust the camera based on the source video dimensions\n    InitializeCamera();\n    // Initialize the textures that store the model input\n    InitializeTextures(true);\n    // Set up the neural network for the OpenVINO inference engine\n    InitializeOpenVINO();\n}\n\n\n\nCreate GetOpenVINOModels()\nBefore we can perform the initialization steps, we need to get a list of available OpenVINO models. The models folder from part 1 of this tutorial will be placed in the same directory as the executable for the Unity application. We will search through the subfolders to obtain the paths to the .xml model files.\nThe actual .xml files are all named yolox_10.xml, so we will use the names of their respective parent directories for the dropdown menu options.\n\nCode:\n/// &lt;summary&gt;\n/// Get the list of available OpenVINO models\n/// &lt;/summary&gt;\nprivate void GetOpenVINOModels()\n{\n    // Get the subdirectories containing the available models\n    string[] modelDirs = System.IO.Directory.GetDirectories(\"models\");\n\n    // Get the model files in each subdirectory\n    List&lt;string&gt; openVINOFiles = new List&lt;string&gt;();\n    foreach (string dir in modelDirs)\n    {\n        openVINOFiles.AddRange(System.IO.Directory.GetFiles(dir));\n    }\n\n    // Get the paths for the .xml files for each model\n    Debug.Log(\"Available OpenVINO Models:\");\n    foreach (string file in openVINOFiles)\n    {\n        if (file.EndsWith(\".xml\"))\n        {\n            openVINOPaths.Add(file);\n            string modelName = file.Split('\\\\')[1];\n            openvinoModels.Add(modelName.Substring(0, modelName.Length));\n\n            Debug.Log($\"Model Name: {modelName}\");\n            Debug.Log($\"File Path: {file}\");\n        }\n    }\n    Debug.Log(\"\");\n}\n\n\n\nDefine Start() Method\nWhen the application starts, we will get references to the canvas, graphy, width, and height objects in the Hierarchy tab.\nThe OpenVINO plugin project has only been tested on Intel hardware, so we will confirm that Intel hardware is available. If it is, we will get the available OpenVINO models and call the GetAvailableDevices() function to see what compute devices are available for OpenVINO.\nIf there is no Intel hardware available, we will disable the option to perform inference, but still call the other initialization steps.\n\nCode:\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get references to GameObjects in hierarchy\n    canvas = GameObject.Find(\"Canvas\");\n    graphy = GameObject.Find(\"[Graphy]\");\n    width = GameObject.Find(\"Width\").GetComponent&lt;TMPro.TMP_InputField&gt;();\n    height = GameObject.Find(\"Height\").GetComponent&lt;TMPro.TMP_InputField&gt;();\n\n    // Check if either the CPU of GPU is made by Intel\n    string processorType = SystemInfo.processorType.ToString();\n    string graphicsDeviceName = SystemInfo.graphicsDeviceName.ToString();\n    if (processorType.Contains(\"Intel\") || graphicsDeviceName.Contains(\"Intel\"))\n    {\n        // Get the list of available models\n        GetOpenVINOModels();\n\n        // Get an unparsed list of available \n        string openvinoDevices = Marshal.PtrToStringAnsi(GetAvailableDevices());\n\n        Debug.Log($\"Available Devices:\");\n        // Parse list of available compute devices\n        foreach (string device in openvinoDevices.Split(','))\n        {\n            // Add device name to list\n            deviceList.Add(device);\n            Debug.Log(device);\n        }\n    }\n    else\n    {\n        inference.isOn = performInference = inference.enabled = false;\n        Debug.Log(\"No Intel hardware detected\");\n    }\n\n    // Get the names of the video clips\n    foreach (VideoClip clip in videoClips) videoNames.Add(clip.name);\n\n    // Initialize the dropdown menus\n    InitializeDropdowns();\n    // Perform the required \n    InitializationSteps();\n}\n\n\n\nCreate FlipImage() Method\nNext, we’ll make a new method to execute the FlipXAxis() function in our ComputeShader. This method will take in the image that needs to be processed as well as a function name to indicate which function we want to execute.\n\nMethod Steps\n\nGet the ComputeShader index for the specified function\nCreate a temporary RenderTexture with random write access enabled to store the processed image\nExecute the ComputeShader\nCopy the processed image back into the original RenderTexture\nRelease the temporary RenderTexture\n\n\n\nCode:\n/// &lt;summary&gt;\n/// Perform a flip operation of the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The image to be flipped&lt;/param&gt;\n/// &lt;param name=\"tempTex\"&gt;Stores the flipped image&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The name of the function to execute in the compute shader&lt;/param&gt;\nprivate void FlipImage(RenderTexture image, string functionName)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 4;\n    // Get the index for the PreprocessResNet function in the ComputeShader\n    int kernelHandle = imageProcessingShader.FindKernel(functionName);\n\n    /// Allocate a temporary RenderTexture\n    RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, image.format);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    imageProcessingShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    imageProcessingShader.SetTexture(kernelHandle, \"InputImage\", image);\n    // Set the value for the height variable in the ComputeShader\n    imageProcessingShader.SetInt(\"height\", image.height);\n    // Set the value for the width variable in the ComputeShader\n    imageProcessingShader.SetInt(\"width\", image.width);\n\n    // Execute the ComputeShader\n    imageProcessingShader.Dispatch(kernelHandle, image.width / numthreads, image.height / numthreads, 1);\n\n    // Copy the flipped image to tempTex\n    Graphics.Blit(result, image);\n\n    // Release the temporary RenderTexture\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\n\nCreate OnCompleteReadback() Callback\nWe currently need to download the pixel data for tempTex from the GPU to the CPU. This normally causes a pipeline stall as Unity prevents any execution on the main thread to prevent the data from changing before it has finished downloading to the CPU. This can cause a noticeable performance bottleneck that increases with the amount of pixel data there is to download.\nUnity provides an alternative approach with AsyncGPUReadback that does not block the main thread. However it adds a few frames of latency. This may or may not matter depending on the specific application.\nThis function will be called once the AsyncGPUReadback has completed. We can load the raw pixel data from the request directly to inputTex.\n\nCode:\n/// &lt;summary&gt;\n/// Called once AsyncGPUReadback has been completed\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;&lt;/param&gt;\nvoid OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    // Fill Texture2D with raw data from the AsyncGPUReadbackRequest\n    inputTex.LoadRawTextureData(request.GetData&lt;uint&gt;());\n    // Apply changes to Texture2D\n    inputTex.Apply();\n}\n\n\n\nCreate UploadTexture() Method\nThis method is where we will send the current pixel data from inputTex to the OpenVINO plugin. We need to use the unsafe keyword since we’ll be creating a pointer to the inputData array. Unity does not allow unsafe code by default, so we will need to enable it in the Project Settings.\nWe need to pin the memory for inputData using a fixed statement and get a pointer to the variable. We can then call the PerformInference() method with the pointer as input.\nAfter executing the model, we will call the GetObjectCount() so that we can initialize the objectInfoArray.\nLastly, we need to pin the memory for the objectInfoArray, before calling the PopulateObjectsArray() function.\n\nCode:\n/// &lt;summary&gt;\n/// Pin memory for the input data and send it to OpenVINO for inference\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputData\"&gt;&lt;/param&gt;\npublic unsafe void UploadTexture(byte[] inputData)\n{\n    //Pin Memory\n    fixed (byte* p = inputData)\n    {\n        // Perform inference\n        PerformInference((IntPtr)p);\n    }\n\n    // Get the number of detected objects\n    int numObjects = GetObjectCount();\n    // Initialize the array\n    objectInfoArray = new Utils.Object[numObjects];\n\n    // Pin memory\n    fixed (Utils.Object* o = objectInfoArray)\n    {\n        // Get the detected objects\n        PopulateObjectsArray((IntPtr)o);\n    }\n}\n\n\n\nCreate UpdateBoundingBoxes() Method\nAfter getting the object info for the current output predictions, we can update the boundingBoxes array with the new info. The position and dimensions for the bounding boxes need to be scaled based on the dimensions of the videoTexture. We also need to flip the bounding box coordinates vertically.\nSince the model does not keep track of unique objects across video frames, we will just reuse the existing bounding box objects. We will only add new bounding boxes as needed.\nWe are unlikely to need the same number of bounding boxes all the time, so we will deactivate any extras.\n\nCode:\n/// &lt;summary&gt;\n/// Update the list of bounding boxes based on the latest output from the model\n/// &lt;/summary&gt;\nprivate void UpdateBoundingBoxes()\n{\n    // Process new detected objects\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        // The smallest dimension of the videoTexture\n        int minDimension = Mathf.Min(videoTexture.width, videoTexture.height);\n\n        // The value used to scale the bbox locations up to the source resolution\n        float scale = (float)minDimension / Mathf.Min(imageDims.x, imageDims.y);\n\n        // Flip the bbox coordinates vertically\n        objectInfoArray[i].y0 = rTex.height - objectInfoArray[i].y0;\n\n        objectInfoArray[i].x0 *= scale;\n        objectInfoArray[i].y0 *= scale;\n        objectInfoArray[i].width *= scale;\n        objectInfoArray[i].height *= scale;\n\n        // Update bounding box list with new object info\n        try\n        {\n            boundingBoxes[i].SetObjectInfo(objectInfoArray[i]);\n        }\n        catch\n        {\n            // Add a new bounding box object when needed\n            boundingBoxes.Add(new BoundingBox(objectInfoArray[i]));\n        }\n    }\n\n    // Turn off extra bounding boxes\n    for (int i = 0; i &lt; boundingBoxes.Count; i++)\n    {\n        if (i &gt; objectInfoArray.Length - 1)\n        {\n            boundingBoxes[i].ToggleBBox(false);\n        }\n    }\n}\n\n\n\nDefine Update() Method\nThe UI and performance metrics can take up a lot of the screen. We can toggle the visibility of the UI and metrics using the SetActive() method.\nIf we are using a webcam, we need to copy the pixel data from the webcamTexture to the videoTexture.\nWhen not performing inference, we will just skip the rest of this method.\nWhen we are performing inference, we need to copy the videoTexture to rTex so that we don’t manipulate the video feed displayed to the viewers.\nWe can then flip the image, download the pixel data from the CPU to GPU, execute the model, and update the bounding boxes.\n\nCode:\n// Update is called once per frame\nvoid Update()\n{\n    // Toggle the user interface\n    if (Input.GetKeyDown(\"space\"))\n    {\n        canvas.SetActive(!canvas.activeInHierarchy);\n        graphy.SetActive(!graphy.activeInHierarchy);\n    }\n\n    // Copy webcamTexture to videoTexture if using webcam\n    if (useWebcam.isOn) Graphics.Blit(webcamTexture, videoTexture);\n\n    // Toggle whether to perform inference\n    if (performInference == false) return;\n\n    // Copy the videoTexture to the rTex RenderTexture\n    Graphics.Blit(videoTexture, rTex);\n\n    // Flip image before sending to DLL\n    FlipImage(rTex, \"FlipXAxis\");\n\n    // Download pixel data from GPU to CPU\n    if (useAsync.isOn)\n    {\n        AsyncGPUReadback.Request(rTex, 0, TextureFormat.RGBA32, OnCompleteReadback);\n    }\n    else\n    {\n        RenderTexture.active = rTex;\n        inputTex.ReadPixels(new Rect(0, 0, rTex.width, rTex.height), 0, 0);\n        inputTex.Apply();\n    }\n\n    // Send reference to inputData to DLL\n    UploadTexture(inputTex.GetRawTextureData());\n\n    // Update bounding boxes with new object info\n    UpdateBoundingBoxes();\n}\n\n\n\nCreate UpdateInputDims() Method\nThis method will be called when the input dimensions are updated from the user interface. We first need to parse the user input for the width and height input fields and use the values to update targetDims. We can then call InitializeTextures() and InitializeOpenVINO().\n\nCode:\n/// &lt;summary&gt;\n/// Called when the input dimensions are updated in the GUI\n/// &lt;/summary&gt;\npublic void UpdateInputDims()\n{\n    // Pares the new width value\n    int newWidth;\n    int.TryParse(width.text, out newWidth);\n    // Parse the new height value\n    int newHeight;\n    int.TryParse(height.text, out newHeight);\n    // Update target dims\n    targetDims = new Vector2Int(newWidth, newHeight);\n    // Initialize the textures that store the model input\n    InitializeTextures();\n    // Set up the neural network for the OpenVINO inference engine\n    InitializeOpenVINO();\n}\n\n\n\nCreate UpdateInferenceValue() Method\nThis method will be called when the user interacts with the inference toggle. If the toggle is turned on, we will call the InitializeOpenVINO() method. Otherwise, we will disable all the bounding boxes.\n\nCode:\n/// &lt;summary&gt;\n/// Called when the value for the Inference toggle is updated\n/// &lt;/summary&gt;\npublic void UpdateInferenceValue()\n{\n    // Only update the performInference value if the canvas is active\n    performInference = inference.isOn;\n\n    if (performInference)\n    {\n        InitializeOpenVINO();\n    }\n    else\n    {\n        // Hide all bounding boxes when not performing inference\n        for (int i = 0; i &lt; boundingBoxes.Count; i++)\n        {\n            boundingBoxes[i].ToggleBBox(false);\n        }\n    }\n}\n\n\n\nCreate UpdateNMSThreshold() Method\nThis method will be called when the NMS threshold value is updated. It simply parses the user input and then calls the SetNMSThreshold() function in the plugin.\n\nCode:\n/// &lt;summary&gt;\n/// Called when the NMS threshold value is updated in the GUI\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputField\"&gt;&lt;/param&gt;\npublic void UpdateNMSThreshold(TMPro.TMP_InputField inputField)\n{\n    // Parse the input field value\n    float threshold;\n    float.TryParse(inputField.text, out threshold);\n    // Clamp threshold value between 0 and 1\n    threshold = Mathf.Min(threshold, 1f);\n    threshold = Mathf.Max(0f, threshold);\n    // Update the threshold value\n    inputField.text = $\"{threshold}\";\n    SetNMSThreshold(threshold);\n}\n\n\n\nCreate UpdateConfidenceThreshold() Method\nLikewise, this method is called when the confidence score threshold is updated, and calls the SetConfidenceThreshold() function in the plugin.\n\nCode:\n/// &lt;summary&gt;\n/// Called when the confidence threshold is updated in the GUI\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputField\"&gt;&lt;/param&gt;\npublic void UpdateConfThreshold(TMPro.TMP_InputField inputField)\n{\n    // Parse the input field value\n    float threshold;\n    float.TryParse(inputField.text, out threshold);\n    // Clamp threshold value between 0 and 1\n    threshold = Mathf.Min(threshold, 1f);\n    threshold = Mathf.Max(0f, threshold);\n    // Update the threshold value\n    inputField.text = $\"{threshold}\";\n    SetConfidenceThreshold(threshold);\n}\n\n\n\nCreate UpdateVideo() Method\nThis method will be called when a new video is selected from the video dropdown menu. We will only call the InitializationSteps() method if we are using the video player and not a webcam.\n\nCode:\n/// &lt;summary&gt;\n/// Called when a model option is selected from the dropdown\n/// &lt;/summary&gt;\npublic void UpdateVideo()\n{\n    if (videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled == false) return;\n\n    Debug.Log($\"Selected Video: {videoDropdown.value}\");\n    InitializationSteps();\n}\n\n\n\nCreate UseWebcam() Method\nThis method will be called when the user interacts with the useWebcam toggle. If the user wants to use a webcam, we will first confirm that there is a webcam available. If there is not, we will disable the option to use a webcam.\nIf the user does not want to use a webcam, we will stop the webcam feed and re-enable the video player.\nWe need to call the InitializationSteps() method whether we are starting or stopping the webcam.\n\nCode:\n/// &lt;summary&gt;\n/// Called when the value for the Use Webcam toggle is updated\n/// &lt;/summary&gt;\npublic void UseWebcam()\n{\n    if (useWebcam.isOn)\n    {\n        WebCamDevice[] devices = WebCamTexture.devices;\n        for (int i = 0; i &lt; devices.Length; i++)\n        {\n            Debug.Log(devices[i].name);\n        }\n\n        if (WebCamTexture.devices.Length == 0)\n        {\n            Debug.Log(\"No webcam device detected.\");\n            useWebcam.SetIsOnWithoutNotify(false);\n        }\n    }\n    else\n    {\n        // Stop the webcam\n        webcamTexture.Stop();\n        // Activate the Video Player\n        videoScreen.GetComponent&lt;VideoPlayer&gt;().enabled = true;\n    }\n\n    InitializationSteps();\n}\n\n\n\nDefine OnDestroy() Method\nWhen the script stops running, we will remove the Log() method from the logMessageReceived callback.\n\nCode:\n// Called when the MonoBehaviour will be destroyed\nprivate void OnDestroy()\n{        \n    Application.logMessageReceived -= Log;\n}\n\n\n\nCreate Quit() Method\nThis method will be called when the Quit button is clicked in the user interface and will cause the application to exit.\n\nCode:\n/// &lt;summary&gt;\n/// Called when the Quit button is clicked.\n/// &lt;/summary&gt;\npublic void Quit()\n{\n    // Causes the application to exit\n    Application.Quit();\n}\nThat takes care of the required code for this project."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#attach-script-to-gameobject",
    "href": "posts/openvino-yolox-unity/part-3/index.html#attach-script-to-gameobject",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Attach Script to GameObject",
    "text": "Attach Script to GameObject\nTo use the ObjectDetector script, we need to attach it to a GameObject. Right-click an empty space in the Hierarchy tab and select Create Empty. Name the new object Object Detector.\n\n\n\n\n\nWith the object still selected, drag and drop the ObjectDetector script into the Hierarchy tab.\n\n\n\n\n\nNow we can drag and drop the objects in the Hierarchy tab into their associated spots in the Inspector tab.\n\n\n\n\n\nNext, we will need to click the small lock button at the top of the Inspector tab to keep the Object Detector selected.\n\n\n\n\n\nNow we can add the video files by selecting all of them in the Assets section and dragging them onto the Video Clips slot in the Inspector tab. We can unlock the Inspector tab after we have added the videos."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#assign-ui-events",
    "href": "posts/openvino-yolox-unity/part-3/index.html#assign-ui-events",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Assign UI Events",
    "text": "Assign UI Events\nThe last step needed before we can build the project is to assign the UI events.\n\nInput Dims\nOpen the UpdateInputDims container inside the Canvas object and select Width.\n\n\n\n\n\nIn the Inspector tab, scroll down to On End Edit.\n\n\n\n\n\nDrag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot.\n\n\n\n\n\nClick on the No Function dropdown menu and open the ObjectDetector section. Select the UpdateInputDims() method from the list.\n\n\n\n\n\nPerform the same steps for the Height object.\n\n\n\n\n\n\n\nNMS Threshold\nOpen the UpdateNMSThreshold container inside the Canvas object and select Threshold.\n\n\n\n\n\nIn the Inspector tab, scroll down to On End Edit. Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the UpdateNMSThreshold(TMP_InputField) option from the function menu.\n\n\nConfidence Threshold\nOpen the UpdateConfidenceThreshold container inside the Canvas object and select Threshold. In the Inspector tab, scroll down to On End Edit. Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the UpdateConfThreshold(TMP_InputField) option from the function menu.\n\n\nDevice Dropdown\nSelect the Device object inside the Canvas. In the Inspector tab, scroll down to On Value Changed (Int32). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the InitializeOpenVINO() option from the function menu.\n\n\n\n\n\n\n\nModel Dropdown\nSelect the Model object inside the Canvas. In the Inspector tab, scroll down to On Value Changed (Int32). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. Again, select the InitializeOpenVINO() option from the function menu.\n\n\nVideo Dropdown\nSelect the Video object inside the Canvas. In the Inspector tab, scroll down to On Value Changed (Int32). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the UpdateVideo() option from the function menu.\n\n\nWebcam Toggle\nSelect the Webcam object inside the Canvas. In the Inspector tab, scroll down to On Value Changed (Boolean). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the UseWebcam() option from the function menu.\n\n\nInference\nSelect the Inference object inside the Canvas. In the Inspector tab, scroll down to On Value Changed (Boolean). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the UpdateInferenceValue() option from the function menu.\n\n\nQuit\nSelect the Quit object inside the Canvas. In the Inspector tab, scroll down to On Click(). Drag and drop the Object Detector object from the Hierarchy tab onto the None (Object) slot. This time, select the Quite() option from the function menu."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#build-the-project",
    "href": "posts/openvino-yolox-unity/part-3/index.html#build-the-project",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Build the Project",
    "text": "Build the Project\nNow we can build the completed project. First, press Ctrl+s to save the project. Open the File menu and select Build Settings…\n\n\n\n\n\nClick build in the popup window. You will be prompted to select a folder to store the files generated during the build.\n\n\n\n\n\nCreate a new folder in the default location and name it Build. Click Select Folder.\n\n\n\n\n\nOnce the build is complete, a File Explorer window will open with the project executable selected."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#add-models-folder",
    "href": "posts/openvino-yolox-unity/part-3/index.html#add-models-folder",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Add Models Folder",
    "text": "Add Models Folder\nCopy and paste models folder from part 1 into the folder with the project executable."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#add-the-plugins-folder",
    "href": "posts/openvino-yolox-unity/part-3/index.html#add-the-plugins-folder",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Add the Plugins folder",
    "text": "Add the Plugins folder\nOpen the OpenVINO_YOLOX_Demo_Data folder inside the Build directory. Copy and paste the Plugins folder from part 2."
  },
  {
    "objectID": "posts/openvino-yolox-unity/part-3/index.html#run-the-application",
    "href": "posts/openvino-yolox-unity/part-3/index.html#run-the-application",
    "title": "OpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)",
    "section": "Run the Application",
    "text": "Run the Application\nAt last, we can test our project. Double-click the executable to run it. Remember that the first time the application launches will be slow as the cache files are generated."
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html",
    "href": "posts/practical-procedural-generation-notes/index.html",
    "title": "Notes on Practical Procedural Generation",
    "section": "",
    "text": "Overview\nExamples\nSteps\nThe IKEA Catalog of Generativity\nMaking use of Generativity\nFurther Reading"
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#overview",
    "href": "posts/practical-procedural-generation-notes/index.html#overview",
    "title": "Notes on Practical Procedural Generation",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Kate Compton’s talk covering practical procedural generation techniques."
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#examples",
    "href": "posts/practical-procedural-generation-notes/index.html#examples",
    "title": "Notes on Practical Procedural Generation",
    "section": "Examples",
    "text": "Examples\nMinecraft Official Site\nNo Man’s Sky\nBay 12 Games: Dwarf Fortress\nThese Monsters\nCameron’s Yavalath Page\n\nMade a system that could generate game rules\nMade a player that could play arbitrary games\nHad virtual players play thousands of games, until he found a game that was pretty well balanced\n\nPANORAMICAL on Steam\nIt is as if you were playing chess\nFitzwilliam Darcy’s Dance Challenge\nThe Treachery of Sanctuary - CHRIS MILK\nKinematics Dress\nV&A Design a Wig\nToca Hair Salon - The Power of Play - Toca Boca\n\nLots of generative content uses extremely sophisticated and brilliant AI and fails anyway\nSome of the best generative content is simple\nThe hardest part of procedural content is design"
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#steps",
    "href": "posts/practical-procedural-generation-notes/index.html#steps",
    "title": "Notes on Practical Procedural Generation",
    "section": "Steps",
    "text": "Steps\n\nUnderstand the design space\nEnumerate your constraints\nUnderstand the process\nPick a generative method\nIterate and be flexible\n\nA lot of great generative projects are things that were tried because it is a stupid idea\n\n\nWhat are you making?\n\nBe specific\n\nLevel generator\nCharacter creator\nAbstract art generator\ncocktail recipe generator\ngame title generator\nconversational character\npoetry generator\ntwitterbot\n\n\nMaking an artist-in-a-box\n\nteaching an algorithm to make art like an artist\nFind and expert (or read their writing)\n\nHow do they think through a problem?\nExample Question: “If you are designing a creature, what do you do?”\nExample Answer: They start by drawing a bean shape as a base for the creature, and hangs a mouth on it.\n\n\nAdditive and Subtractive Methods\n\nBuild up a space of good stuff\n(optional) Remove bad stuff\nVocab:\n\nPossibility space\nExpressive range"
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#the-ikea-catalog-of-generativity",
    "href": "posts/practical-procedural-generation-notes/index.html#the-ikea-catalog-of-generativity",
    "title": "Notes on Practical Procedural Generation",
    "section": "The IKEA Catalog of Generativity",
    "text": "The IKEA Catalog of Generativity\n\nA catalog of generative methods and why you might chose each\n\n\nAdditive Methods\n\nTiles\n\nWorks well for\n\nSomething you can break into (equal-sized) regions\nwhere tile-to-tile placement don’t need to be constrained\n\nCan use WaveFunctionCollapse when placement needs to be constrained\n\nbut you can still get emergence from the placement of tiles\none of the oldest forms\n\n\n\n\nGrammars\n\nRecursively make things from other things\nTracery and other templating systems (for text)\nL-Systems (for geometry)\nReplacement grammars\nLevel design as model transformation - Proceedings of the 2nd International Workshop on Procedural Content Generation in Games\n\n\n\nDistribution\n\nput down a bunch of stuff\ncan use random numbers (actual randomness does not look good)\nreal distributions are hierarchical and clustered, but also maintain spacing\nBarnacling: when you have a large object in your world, there should be medium sized objects around it and smaller objects around those\nFooting: When two things intersect, there should be an awareness of them intersecting\n\nExample: If you stick tree in the ground, there will be dirt piled up around it\n\nGreebling: cosmetic detailing added to the surface of an larger object that makes it appear more complex or technologically advanced\nOptions\n\nstart with a grid, and offset a bit\n\n(less obvious with a hex grid)\n\nUse a voronoi diagram with easing\nDo it properly with a Halton Sequence\n\n\n\n\nParametric\n\nAn array of floats representing settings, “morph handles”\nmodellable as points in an N-dimensional cube\nAny position is a valid artifact\nYou can do genetic algorithms\n\nor use directed walks through the space\nor “regionize” the space\n\n\n\n\nInterpretive\n\nStart with an input\n\nRun an algorithm to process data into some other data\n\nYou have a simple structure\n\nsome distribution of points, a skeleton, a connectivity map, a curve or path and want to make it more complex\n\nExamples:\n\nNoise (Perlin/simplex)\nVoronoi/Delaunay\nConstructive Solid Geometry Extrusion, revolution\nMetaballs\nFractals, mathematical models of impossible shapes\n(Hypernom, Miegakure)\n\nlow control, high weirdness, not suitable for most games\n\n\n\n\n\nSimulations\n\nParticle trails\n\nsimulate particle path responding to forces\n\ndraw directly\nOR record path and use for extrusions or distributions (Photoshop brushes)\nGoes great with user input (Leapmotion, Kinect)\nCellular automata\nAgent-based simulations\nPhysics simulation\n\n\n\n\nSubtractive Methods\n\nSaving Seeds\n\nSeeded random numbers\n\nSame seed, same random generation\n\nMake sure nothing is framerate or input dependent\n\n\nWhitelist a catalog of known good content\n\nIt’s faster to verify questionable content than to build a testing function\n\n\n\n\nGenerate and test\n\nIf you can write an algorithm to judge “quality”\n\nThrowaway vs ranking/prioritization\n\nUse ranking/prioritization\n\nTest for brokenness/connectivity\n\nBeware of false functions\n\nbeware the “fun equation”\n\n\n\n\nComputationally exploring the possibility space\n\nAlso called “search”\n\nBrute force search\n\n“Find the tallest creature that the tool can make”\n“Make a level that has these properties”\n\nHill-climbing\n\nGenetic algorithms\nWorks best with parametric methods\n\n\n\n\n\nConstraint-solving\n\nYou can describe a possibility space and constraints, just find the valid parameters.\nInverse Kinematics-solving\nAnswer set solving\n\nPotassco Clingo\nDO NOT WRITE YOUR OWN SOLVER\n\nBrute force\n\npay attention to exponential growth"
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#making-use-of-generativity",
    "href": "posts/practical-procedural-generation-notes/index.html#making-use-of-generativity",
    "title": "Notes on Practical Procedural Generation",
    "section": "Making use of Generativity",
    "text": "Making use of Generativity\n\nYou can generate many things\nThey are all mathematically unique\nBut they aren’t perceived as unique\nIs this a problem?\n\nDo not boast about really big numbers\n\n\n\nDifferent kinds of generative content\n\nBackground\n\nIn-fill (don’t be empty)\n\nPerceptual differentiation\nPerceptual uniqueness\nCharacterful\n\nTest: Would you write a fanfic for this generated item?\n\n\n\n\nOwnership: MSG for PCG\n\nAllow users to name content\nShowing off content with their name attached, to a large audience\n\nThe “victoriain explorers club” model\n\npromote players\nLet players take credit for your generativity\n\ncreators, curators, retellers\n\n\n\n\nData Structures: Make your life easier\n\nA/B test generators\nRelease new generative content safely\nCreate editors and run user-made generators safely\nVisualize your generators"
  },
  {
    "objectID": "posts/practical-procedural-generation-notes/index.html#further-reading",
    "href": "posts/practical-procedural-generation-notes/index.html#further-reading",
    "title": "Notes on Practical Procedural Generation",
    "section": "Further Reading",
    "text": "Further Reading\n\nEncyclopedia of Generativity\nSo you want to build a generator…\nICCC’21: Int. Conference on Computational Creativity\nMy Liner Notes for Spore - Chris Hecker’s Website\nA Brief History of Spore\nDanesh: A tool to help people explore, explain and experiment with procedural generators\n\nReferences:\n\nPractical Procedural Generation for Everyone"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html",
    "href": "posts/procedural-map-generation-techniques-notes/index.html",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "",
    "text": "Overview\nInfluential Games\nSimple Room-Placement\nBinary Space Partition Rooms\nCellular Automata\nDrunkard’s Walk\nDiffusion Limited Aggregation\nDLA with a Central Attractor\nVoronoi Diagrams\nPerlin and Simplex Noise\nYou can use more than one technique\nRemoving Unreachable Areas\nThe Hot Path\nTelling a Story"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#overview",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#overview",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Overview",
    "text": "Overview\nMy notes on Herbert Wolverson’s talk on procedural map generation techniques from the 2020 virtual Roguelike Celebration."
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#influential-games",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#influential-games",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Influential Games",
    "text": "Influential Games\n\nRogue (1980)\n\nOne of the first uses of procedural generation\nGenerates up to 9 rooms and connects them randomly\nUsed procedural generation because they needed to keep the game small\nDifferent map every time the game is started\nEffectively infinite replay\n\n\n\nDwarf Fortress (2006 - Present)\n\nProbably crammed the most procedural generation into one game\nProcedurally Generates:\n\nMassive overworld with sweeping mountain ranges, forests, volcanoes, demon-infested fortresses\nCivilizations that either like or hate each other\n\nCan drill down to a single person and their procedurally generated backstory\n\nMid-scale\n\nCan zoom into any particular block on the map to find it is beautifully rendered and still matches the overall shape of the overworld\nTrees gain and lose foliage depending on their type and biome\n\nTheir type spawns the appropriate biome\n\n\n\n\nTakeaway: The randomness does not define the above games. The randomness is fed into an algorithm the generates something that approximates what you want to get, but ensures that it is different every time"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#simple-room-placement",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#simple-room-placement",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Simple Room-Placement",
    "text": "Simple Room-Placement\n\n\n\n\n\n\nStart with a solid map (random rectangle)\nFill the map with walls.\nRandomly pick a room location.\n\nIf the map location is not already occupied by another room, add the room\n\nKeep picking rooms.\nJoin the rooms you kept with corridors.\n\nExample: Using a simple dog leg algorithm that randomly switches between being either vertical first or horizontal first."
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#binary-space-partition-rooms",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#binary-space-partition-rooms",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Binary Space Partition Rooms",
    "text": "Binary Space Partition Rooms\n\n\n\n\n\n\nSimilar results to random room placement, better spaced out.\n\nUsed in Nethack\n\n\n\nDivide map into two. Randomly decide whether to divide vertically or horizontally.\nDivide area into two.\nRepeat.\nUse divided space for room.\n\n\nAdd a gutter of one tile around to avoid rooms joining together (unless desired)"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#cellular-automata",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#cellular-automata",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Cellular Automata",
    "text": "Cellular Automata\n\n\n\n\n\n\nEvolve order from chaos.\nPopularized in Conway’s Game of Life.\n\n\nMake a random map.\nMake a copy of it.\nApply cell life rules to each tile.\n\nIterate every tile that isn’t on the edge and count the number of neighbors, including diagonals.\n\nIf there are no neighbors, then it becomes a wall\nIf there is one to four neighbors, it becomes empty\nIf there are five or more neighbors, it becomes a wall\nTweak rules to suit specific game\n\n\nRepeat.\n\n\nSimple\nFast\nDeterministic (same random seed generates the same results)"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#drunkards-walk",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#drunkards-walk",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Drunkard’s Walk",
    "text": "Drunkard’s Walk\n\n\n\n\n\n\nFind Umber Hulk. Insert beer.\nPlace Hulk randomly on solid map. See what he smashes\nHulks stop when they leave the map, or pass out after n steps.\n\n\nStart with a solid map\nRandom walk through map\nTiles get removed based on walking path\nPick maximum number of walking steps\nRepeat.\n\n\nGuarantees the map will be contiguous\nTends to generate maps that look like it was carved out by water.\n\nIdeal for creating limestone caverns and similar."
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#diffusion-limited-aggregation",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#diffusion-limited-aggregation",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Diffusion Limited Aggregation",
    "text": "Diffusion Limited Aggregation\n\n\n\n\n\n\nExplanation\nStart with a targeted seed.\nRandomly - or not - fire particles at it.\nDig out the last edge the particle hit.\n\n\nStart by digging out a small target seed\nPick a random point anywhere on the map\nPick a random direction\nShoot a particle\n\nKeep shooting until you hit something\nIf you hit a target area, carve out the last solid area you passed through\n\n\n\nTends to give you a very winding open map\nGuaranteed to be contiguous\nLots of ways to tweak the algorithm to make things more interesting"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#dla-with-a-central-attractor",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#dla-with-a-central-attractor",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "DLA with a Central Attractor",
    "text": "DLA with a Central Attractor\n\n\n\n\n\n\nMore likely to always hit the target\nRandomly spawn your starting point and then shoot the particle directly at the middle of the map\nHelps ensure your get an open space in the middle\n\nIdeal, for example, to put a dragon with his hoard\n\nMore interesting pattern around the edges of the map\nCan also apply symmetry down the vertical\n\nUse sparingly"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#voronoi-diagrams",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#voronoi-diagrams",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Voronoi Diagrams",
    "text": "Voronoi Diagrams\n\n\n\n\n\n\nRandomly (or deliberately) placed seeds.\nEach tile joins the closest seed.\nVary distance heuristic for different effects.\nIterate every point on the map and it joins the area belonging to the closest seed.\n\nExample Algorithms:\n\nDelauney triangulations\nBrute force\n\n\nCan customize the result using a different distance algorithm to determine which group every tile joins\n\nPythagorean distance\nManhattan distance\n\nFind the edges, place walls there and wind up with an alien cell structure\nCan be used to determine spawning placement/behavior based on cell location\nCan be used for effective city generation\n\nApocalypse Taxi\n\n\nUses the edges of the generated cells to determine where the roads went\nRandomly populated the content of each cell with something like “heavy industrial city”, “light industrial city”, etc.\n\n\nApocalypse Taxi\nCan be combine with other techniques"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#perlin-and-simplex-noise",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#perlin-and-simplex-noise",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Perlin and Simplex Noise",
    "text": "Perlin and Simplex Noise\n\n\n\n\n\n\nBasically a bunch of gradients combined together with a few variables\nCan generate it in either two or three dimensions\nX/Y Value: gives a number in the range \\([-1,1]\\)\nSmoothly moving either up or down\nContinuous\nOctaves: number of gradients being mixed in.\nGain: how long the various gradients last\nLacunarity: adds in randomness\nFrequency: how frequently each of the various octaves peaks\nCommonly used to make an overworld/terrain map\nProblem: The gradients are kind of dull\n\nCan be addressed by adding a second layer of noise that is more “bumpy”\n\nInterpolate between smooth and bumpy gradients as you zoom in and out\n\n\nEasy to implement\nCan also be used to generate realistic looking clouds, particles, wood grain"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#you-can-use-more-than-one-technique",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#you-can-use-more-than-one-technique",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "You can use more than one technique",
    "text": "You can use more than one technique\n\nCan help generate maps that tell a story\nExample: Use BSP to generate a more structured part of the map leads into a more chaotic section generated using cellular automata\nExample: Use DLA for erosion\n\nTake map and then use DLA to fire particles at it to blast parts of the map away\nMap becomes more organic-looking while keeping its basic structure\n\nExample: Mix procedurally generated content with human-made prefabs"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#dijkstra-maps",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#dijkstra-maps",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Dijkstra Maps",
    "text": "Dijkstra Maps\n\n\n\n\n\n\nExplanation\nStart with 1 or more starting points.\nRest of the map ” sentinel” value - unreachable\nSet points adjacent to start to 1.\nPoints adjacent to those 2.\n\nKeep going until whole map walked"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#removing-unreachable-areas",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#removing-unreachable-areas",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Removing Unreachable Areas",
    "text": "Removing Unreachable Areas\n\nCellular automata can give you chunks of the map that you can’t get to.\n\n\nFind Central Start\nRun Dijkstra\nCull tiles without a valid distance.\n\nOr hide it for underground levels\n\n\nFinding a Starting Point\n\nFind a desired starting point\nFind closest open tile for actual start.\n\nFinding an Endpoint\n\nUse distance to target\nUse Dijkstra to find farthest point"
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#the-hot-path",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#the-hot-path",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "The Hot Path",
    "text": "The Hot Path\n\nPath-find from start to end\nDijkstra Map with the path as starting points.\n\\(&lt;n\\) distance is “hot path”\nCan use A* algorithm\nCan be used to minimize branching in game map by culling irrelevant parts of the map outside the hot path.\nOr “bonus” content to reward exploration of the hot path."
  },
  {
    "objectID": "posts/procedural-map-generation-techniques-notes/index.html#telling-a-story",
    "href": "posts/procedural-map-generation-techniques-notes/index.html#telling-a-story",
    "title": "Notes on Procedural Map Generation Techniques",
    "section": "Telling a Story",
    "text": "Telling a Story\n\nRooms are ordered.\nStory progression is in order, but RNG is retained\nMaybe room 5 has a locked door, meaning the key must be in rooms 1-4.\n\nTakeaway: Guide the randomness and use algorithms to check the randomness.\nReferences:\n\nHerbert Wolverson - Procedural Map Generation Techniques\nSource Code for Talk: GitHub Repository\nOnline Book: Roguelike Tutorial - In Rust"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "",
    "text": "Overview\nIntroduction\nGoal of the pipeline\nAvailable procedural tools\nUser point of view\nPipeline\nCliffs tool in detail\nBiome tool in detail\nConclusion"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#overview",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#overview",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Overview",
    "text": "Overview\nI recently started learning about tools and techniques used for procedural generation in game development. I came across a presentation from 2018 given by Etienne Carrier, a technical artist at Ubisoft. He provides an overview the tools he developed for the production of Far Cry 5. Below are some notes I took while watching."
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#introduction",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#introduction",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Introduction",
    "text": "Introduction\n\nHow to maintain quality of features like forests through multiple terrain iterations without manually updating them."
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#goal-of-the-pipeline",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#goal-of-the-pipeline",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Goal of the pipeline",
    "text": "Goal of the pipeline\n\nMacro management tool to fill up the world with natural looking content\nMaintain content consistency with the terrain topology\nAutomatable\nDeterministic (same result given the same input)\nUser friendly"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#available-procedural-tools",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#available-procedural-tools",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Available procedural tools",
    "text": "Available procedural tools\n\nFreshwater tool: generates lakes, rivers, streams, and waterfalls\nFences and power-lines\nCliff generation on steep terrain surfaces\nBiome tool: generates vegetation throughout the world\nFog density map generation (2D map based on topology, forests, freshwater, etc)\nWorldmap terrain"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#user-point-of-view",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#user-point-of-view",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "User point of view",
    "text": "User point of view\n\nTerrain Terraforming Pass\nFreshwater: Artist lays down fresh water network using curves and splines\nCliffs: based on terrain slope\nVegetation: Artist uses biome painter → color density = forest density?\n\nTool naturally distributes grass and other vegetation\nReacts to water proximity\nAvoids adding vegetation on cliff erosion lines\nAltitude affects forest density\n\nPoints of Interest: Artist can manually tweak results and specific locations\n\nExample: lay down road splines\nExample: clear out spots for bases\nAssets like houes, sheds, tools, etc. still need to be manually placed\n\nMight require tweaking of local biome\n\nAdd fences using splines\nAdd power-line networks using splines\n\nTransformer boxes automatically added where required\nRefresh biome to account for new power-lines\n\n\nTerrain can be adjusted at any time and is non-destructive"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#pipeline",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#pipeline",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Pipeline",
    "text": "Pipeline\n\nDunia2 (FarCry Game Engine/Editor) Inputs → Houdini\nHoudini Outputs → Dunia2\nInputs via Python Scripts:\n\nWorld information\nFile Paths\nTerrain Sectors\nSplines and Shapes\n\nInputs from Disk:\n\nHeight maps (.raw)\nBiome painter (.png)\n2D terrain masks (.png)\nHoudini Geometry (.geo or .bgo)\n\nOutputs saved (data is saved temporarily as buffers on disk):\n\nEntity point cloud\n\nExported with object id\nCould be anything that has a position in editor\n\nVegetation assets\nRocks\nCollectibles\nDecals\nVFX\nPrefabs\n\n\nTerrain texture layers\nTerrain height map layers\n2D terrain data\nGeometry\nTerrain Logic zones\n\nTools Interconnectivity\n\nEach tool will output necessary masks to affect the next ones\nCooking order is important if one tool requires input from a previous one\n\nfreshwater → roads → fences & power-lines → cliffs → biomes → fog → world map"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#cliffs-tool-in-detail",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#cliffs-tool-in-detail",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Cliffs tool in detail",
    "text": "Cliffs tool in detail\n\nTool input\n\nslope terrain data → slope threshold (is it too steep to walk on?) → cliffs input\nprepare geometry by remeshing to get uniform mesh triangles\n\n\n\nStratification\nstratification\n\nVisible horizontal lines formed by the accumulation of sedimentary rock and soil\nSlice input geometry into strata chunks\n\nEach strata has a random thickness\nAssign strata id to each slice\nControl stata angle with RGB painter\nSplit noise (on low res mesh) to split mesh into two groups\n\nrun stratification tool on both groups with different seed values to break up strata lines\n\nExtrude and displace strata\nReduce mesh triangle count\nExported geometry is divided per sector\n\n\n\n\nCliffs are shaded in-game (same texture as terrain underneath)\n\n\nErosion\n\nRun a flow simulation\n\nPoints scattered on cliff surfaces that will flow down the slope to create an erosion effect\n\nUse erosion data to scatter crumbled rocks on the erosion surfaces\n\nexport as point cloud\n\nuse noise to mix two different cliff textures\n\n\n\nVegetation growing surfaces\n\nScatter vegetation on viable cliff surfaces\n\nClear above or not using raycast\n\n\n\n\nExported data\n\nCliffs geometry\nEntities point cloud\nTerrain Texture IDs\n2D Cliffs color\n2D Cliffs mask"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#biome-tool-in-detail",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#biome-tool-in-detail",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Biome tool in detail",
    "text": "Biome tool in detail\n\nInput\n\nGenerate terrain from height map\n\n\n\nTerrain abiotic data\n\nPhysical features of the land that are generated from terrain topology\n\nOcclusion\nFlow\nSlope\nCurvature\nIllumination\nAltitude\nLatitude\nLongitude\nWind\n\nImporting 2D data\n\nBiome painter data\nProcedurally generated data\n\nFreshwater masks\nRoads masks\nFences mask\nPower lines mask\nCliffs mask\n\n\n\n\n\nProcessing main biomes\n\nBiome and sub-biome\n\nMain biome (e.g. Mountain) large scale\n\nsub biome: mountain grass\nsub biome: mountain forest\n\nMain biome processes power line clearings\nSub-biomes recipes\n\nnode-based\nmountain forest\n\ningredient: aspen undergrowth\ningredient: dead conifer\ningredient: hemlock\n\nmountain grass\nintermountain prarie\n\nViability\n\nEach species is fighting for ground to grow and thrive\nviability is defined by setting up favored terrain attributes for each species\nspecies that accumulate the most viability will win over others\nFactors\n\nocclusion terrain data\nflow map\nviability radius\n\nExample: a tree with a lower viability will be discarded if it is withing the radius of a tree with a higher viability\n\npriority radius\n\nUseful for populating bushes, etc. under larger trees\nfiltering will process priority first\nif priority is equal, the viability will be used instead\n\n\n\n\n\n\n\nCombine terrain data\n\nMix terrain abiotic data to achieve specific distribution patterns\n\nOcclusion\nAltitude\nFlow map\nNoise (e.g. perlin noise)\nExclusion masks (generated from fresh water maps, power lines, etc.)\nUsed as viability for species\n\n\n\n\nSizes\n\nmultiple sizes for same species\ndriven by viability\n\nasset size value linked to viability value\n\nsmall and young trees more likely to spawn at edge of forest\ntaller trees more likely near the center\nAltitude\nSizes variation\n\nseveral assets of the same size\nprobability control on each variation\nforest canopy\n\necological succession\nOverstory\nMidstory\nWoody understory\nHerbaceous understory\n\nAge parameter\n\nsine-distance field generated from viability data\nramp for profile shape\n\nDensity\n\nramp from size, age, or viability\nslope aspect effect on density\nillumination (how much light does the vegetation receive)\n\n\n\n\n\nEntities Color\n\nper instance color ramp variation\n\n\n\nRotation\n\norient on terrain slope\n\ne.g. grass leaning towards water\npre-bended tree trunks from growing on a slope\n\ngrassland oriented on wind vector map\npercent angle of terrain\nrotation jitter\n\n\n\nTerrain elements affected by vegetation in biomes\n\nTerrain deformation\n\nterrain elevation around trunks\nheight map layer\nneed to blend assets with terrain\n\ngenerate matching terrain texture underneath asset\n\n\nTerrain textures\n\npine needles\nacorns\ndead branches\nshadows affect how light other vegetation receives\n\nTerrain data output\nTerrain color\n\nterrain texture tint\n\n\n\n\nExported data\n\nreuse data on following species (e.g. viability map)\nspecies age output\nterrain height map\nentities point cloud\nterrain texture IDs\nterrain color\nforest mask"
  },
  {
    "objectID": "posts/procedural-tools-far-cry-5-notes/index.html#conclusion",
    "href": "posts/procedural-tools-far-cry-5-notes/index.html#conclusion",
    "title": "Notes on the Procedural Tools Used to Make Far Cry 5",
    "section": "Conclusion",
    "text": "Conclusion\n\nLessons Learned\n\nprocedural tools can generate a lot of data\ngives a lot of control over performance, game-play, and art\ndesign elegant tools that open up possibilities\nkeep things simple\nlisten to users\n\nmight prefer more manual controls instead of automation\n\nbe flexible\nbalance between control and automation\n\nReferences:\n\nProcedural World Generation of Ubisoft’s Far Cry 5"
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html",
    "href": "posts/pytorch-cuda-wsl2/index.html",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "",
    "text": "Introduction\nInstalling WSL\nSetting Up Ubuntu\nThe Headaches\nConclusion"
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html#introduction",
    "href": "posts/pytorch-cuda-wsl2/index.html#introduction",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "Introduction",
    "text": "Introduction\nI spent a couple days figuring out how to train deep learning models on Microsoft’s Windows Subsystem for Linux (WSL). The process was a bit of a hassle. While the official installation guides are adequate, there were some headaches that came up during regular use. This post summarizes my experience making it work.\n\nWhat is WSL\nWSL is a compatibility layer that let’s you run Linux environments directly on Windows. You can run Linux command-line tools and applications, invoke Windows applications from the Linux command-line, and access Windows drives through the Linux file system. The most recent version, WSL2, uses a real Linux kernel. This provides support for more applications such as Docker. More importantly for my purposes, it also enables GPU accelerated applications.\n\n\nMotivation\nI’ve been dual-booting Windows and Linux for a while now. I prefer Linux for coding and training models while Windows is supported by more applications. This setup didn’t have any drawbacks for me until I started working with the Barracuda library for Unity. Unity is installed on Windows but my environment for training deep learning models is on Linux. This is inconvenient when I want to test out a newly trained model in Unity. I decided to try WSL2 in the hopes that it would remove the need to switch between operating systems."
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html#installing-wsl",
    "href": "posts/pytorch-cuda-wsl2/index.html#installing-wsl",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "Installing WSL",
    "text": "Installing WSL\nThe install process for most WSL2 use cases is straightforward. You just need to enable a few features and install your preferred Linux distribution from the Microsoft Store. However, the process for enabling CUDA support is a bit more involved.\n\nInstall Windows Insider Build\nCUDA applications are only supported in WSL2 on Windows build versions 20145 or higher. These are currently only accessible through the Dev Channel for the Windows Insider Program. I confirmed it does not work with the latest public release. Microsoft requires you to enable Full telemetry collection to install Insider builds for Windows. This was annoying since the first thing I do when installing Windows is disable every accessible telemetry setting. Fortunately, I only needed to temporarily enable a couple of the settings to install an Insider build.\n\n\nInstall Nvidia’s Preview Driver\nNvidia provides a preview Windows display driver for their graphics cards that enables CUDA on WSL2. This Windows driver includes both the regular driver components for Windows and WSL. We’re not supposed to install display drivers on the Linux distribution itself.\n\nNvidia Drivers for CUDA on WSL\n\n\n\nInstall WSL\nYou can install WSL with one line in the command window if you install a preview build first. I did it backwards so I had to use the slightly longer manual installation. I went with Ubuntu 20.04 for my distribution since that’s what I currently have installed on my desktop."
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html#setting-up-ubuntu",
    "href": "posts/pytorch-cuda-wsl2/index.html#setting-up-ubuntu",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "Setting Up Ubuntu",
    "text": "Setting Up Ubuntu\nThe set up process was basically the same as regular Ubuntu with the exception of no display drivers.\n\nUpdate Ubuntu\nAs usual, I first checked for any updates. There were quite a few.\nsudo apt update\nsudo apt upgrade\n\n\nInstall CUDA Toolkit\nThe next step was to install the CUDA toolkit. Nvidia lists WSL-Ubuntu as a separate distribution. I don’t know what makes it functionally different than the regular Ubuntu distribution. Both worked and performed the same for me when training models. You can view the instructions I followed for both by clicking the links below.\n\nUbuntu\nWSL-Ubuntu\n\n\n\nInstall Anaconda\nI like to use Anaconda, so I downloaded the latest available release to the home directory and installed it like normal.\ncd ~\nwget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\nchmod +x Anaconda3-2020.11-Linux-x86_64.sh\n./Anaconda3-2020.11-Linux-x86_64.sh\nI had to restart bash to use the new python interpreter like normal as well.\nexec bash\nAfter that, the interactive python interpreter started without issue.\nPython 3.8.5 (default, Sep  4 2020, 07:30:14)\n[GCC 7.3.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n\n\nInstall Fastai Library\nI installed the fastai library which is built on top of PyTorch to test whether I could access the GPU. The installation went smoothly.\nconda install -c fastai -c pytorch -c anaconda fastai gh anaconda\nI was able to confirm that PyTorch could access the GPU using the torch.cuda.is_available() method.\nPython 3.8.5 (default, Sep  4 2020, 07:30:14)\n[GCC 7.3.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.cuda.is_available()\nTrue\n&gt;&gt;&gt;\nI opened up a jupyter notebook and trained a ResNet50 model to confirm that the GPU was actually being used. The Task Manager in Windows accurately displays the available GPU memory and temperature but not GPU usage for WSL applications. The nvidia-smi command doesn’t work yet in WSL either. I believe Nvidia is planning on adding that functionality in a future release. However, the nvidia-smi.exe command does accurately show GPU usage."
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html#the-headaches",
    "href": "posts/pytorch-cuda-wsl2/index.html#the-headaches",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "The Headaches",
    "text": "The Headaches\nEverything seemed to be working as I’d hoped. However, I started encountering some issues the more I used WSL.\n\nMemory Usage\nBy default, WSL distributions will take up as much system memory as is available and not release it. This problem is compounded since Windows already takes up a decent chuck of memory. This seems to be something Microsoft is still working on. However, you can limit the amount of memory WSL can access. The workaround involves creating a .wslconfig file and adding it to you Windows user folder (e.g. C:\\Users\\Username). You can see the contents for an example config file below.\n[wsl2]\nmemory=6GB\nGPU memory usage doesn’t suffer from this problem, so it wasn’t too big of an issue for me.\n\n\nFile Permissions\nThis is where things started to get more inconvenient for my use case. The way in which WSL handles permissions for files in attached drives isn’t readily apparent for new users. I didn’t have any problem accessing the previously mentioned jupyter notebook or the image dataset I used to train the model. However, I couldn’t access the images in a different dataset when training a different model.\nI tried adding the necessary permissions in Ubuntu but that didn’t work. I even tried copying the dataset to the Ubuntu home directory. I ended up finding a solution on Stack Exchange. It involves adding another config file, this time to Ubuntu. I needed to create a wsl.conf file in the /etc/ directory. This one enables metadata for the files so that changes in permission actually work.\n[automount]\nenabled = true\nroot = /mnt/\noptions = \"metadata,umask=22,fmask=11\"\nI had to restart my computer after creating the file for it to take effect. You can learn more about wsl.conf files and the settings in the above example at the links below.\n\nAutomatically Configuring WSL\nChmod/Chown WSL Improvements\nFile Permissions for WSL\n\n\n\nDisk Space\nThis is the one that killed the whole endeavor for me. I deleted the copy of the dataset I made in the Ubuntu home directory after I was able to access the original. I noticed that my disk usage didn’t decrease after I deleted the 48GB of images. This is also a known problem with WSL. There is another workaround where you can manually release unused disk space that involves the following steps.\n\nOpen PowerShell as an Administrator.\nNavigate to the folder containing the virtual hard drive file for your distribution.\nShutdown WSL.\nRun optimize-vhd for the virtual hard drive.\n\ncd C:\\Users\\UserName_Here\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\nwsl --shutdown\noptimize-vhd -Path .\\ext4.vhdx -Mode full\nYou currently need to do this every time you want to reclaim disk space from WSL. By this point, any convenience I’d gain over a dual-boot setup had been wiped out."
  },
  {
    "objectID": "posts/pytorch-cuda-wsl2/index.html#conclusion",
    "href": "posts/pytorch-cuda-wsl2/index.html#conclusion",
    "title": "Using PyTorch with CUDA on WSL2",
    "section": "Conclusion",
    "text": "Conclusion\nI’m excited about the future of WSL. Having such tight integration between Windows and Linux has a lot of potential. Unfortunately, it’s not at a point where I’d feel comfortable switching over from a dual-boot setup. I’m hoping that the issues I encountered will get resolved in 2021. I’ll give it another shot when CUDA support comes out of preview."
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html",
    "href": "posts/deep-learning-unity-intro/index.html",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "",
    "text": "Introduction\nThe Barracuda Library\nExporting Models to ONNX\nLoading Models\nExecuting Models\nWorking with Data\nSummary"
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#introduction",
    "href": "posts/deep-learning-unity-intro/index.html#introduction",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Introduction",
    "text": "Introduction\nMost deep learning models get deployed to servers instead of user devices. Server-side inference comes with many advantages, like complete control over the runtime environment and the option to scale computing resources up and down as needed. It can also be the only feasible way to run extremely-large models like GPT-3.\nHowever, running models on user devices can provide compelling cost, latency, and privacy benefits. There are no servers to maintain, no lag from poor internet connections, and no user data to protect. The latency benefits can be particularly significant for interactive real-time applications.\nUnity is one of the best platforms for developing real-time 2D, 3D, VR, and AR applications. Its core competency is game development, but it also works well for other immersive and interactive applications.\nThere are many potential ways to leverage deep learning in Unity applications, including mapping user movement to virtual avatars, generating character dialogue, and powering enemy AI to name a few. Below are some examples from personal projects.\n\nIn-Game Style Transfer\n\n\nVideo\n\n\n\n\nPose Estimation\n\n\nVideo\n\n\n\n\nObject Detection\n\n\nVideo\n\n\nThese examples only scratch the surface of what’s possible by combining deep learning models with powerful real-time creation tools like Unity and Unreal Engine. The Barracuda library makes it easy to start exploring these possibilities."
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#the-barracuda-library",
    "href": "posts/deep-learning-unity-intro/index.html#the-barracuda-library",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "The Barracuda Library",
    "text": "The Barracuda Library\nBarracuda is a neural network inference library for the Unity game engine. It initially focused on models trained with Unity’s Deep Reinforcement Learning toolkit, ML-Agents, but has expanded support over time.\nBarracuda provides multiple backends for both CPU and GPU inference. The fastest CPU backend uses the Burst compiler, which translates IL/.NET bytecode into highly-optimized native code using LLVM. The most performant GPU backend uses Compute shaders. Compute shaders are programs written in High-level shader language (HLSL) that run on the GPU, outside the standard rendering pipeline.\nSome platforms don’t support Compute shaders, so Unity recently added a Pixel Shader backend to enable GPU inference on platforms where Compute shaders are not supported. While faster than CPU inference, it is significantly slower than the Compute shader backend in my testing.\nOne of Barracuda’s greatest strengths is its cross-platform support. As of writing, Barracuda does not support specialized inference hardware, quantization, or even FP16 precision. However, it runs wherever Unity does, which is nearly everywhere."
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#exporting-models-to-onnx",
    "href": "posts/deep-learning-unity-intro/index.html#exporting-models-to-onnx",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Exporting Models to ONNX",
    "text": "Exporting Models to ONNX\nBarracuda works with models in the ONNX file format. PyTorch provides built-in support to export models to ONNX.\ntorch.onnx.export(learn.model.cpu(),\n                  batched_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=9,\n                  do_constant_folding=True,\n                  input_names = ['input'],\n                  output_names = ['output'],\n                 )\nWe can use the tf2onnx python package to convert TensorFlow models.\npython -m tf2onnx.convert --saved-model ./savedmodel --opset 10 --output model.onnx\nBarracuda maps ONNX operators to backend-specific implementations, so model support depends on what operators Unity implements for a given inference backend. One could theoretically implement missing operations themselves, but it would probably make more sense to explore other inference options at that point. Another option is to tweak the model architecture to ensure it only uses supported operations."
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#loading-models",
    "href": "posts/deep-learning-unity-intro/index.html#loading-models",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Loading Models",
    "text": "Loading Models\nUnity imports ONNX models as an NNModel asset.\n[Tooltip(\"The Barracuda/ONNX asset file\")]\npublic NNModel modelAsset;\nThese then compile into a Model object at runtime.\n// Get an object oriented representation of the model\nm_RunTimeModel = ModelLoader.Load(modelAsset);"
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#executing-models",
    "href": "posts/deep-learning-unity-intro/index.html#executing-models",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Executing Models",
    "text": "Executing Models\nBarracuda has an IWorker interface that abstracts implementation details for different inference backends. It is responsible for translating the Model object into a set of operations and executing them.\n// Create a worker to execute the model using the selected backend\nIWorker engine = WorkerFactory.CreateWorker(workerType, m_RunTimeModel);\nBarracuda can run models in a single frame or across multiple using Coroutines. The latter option can help maintain smooth frame rates when using more demanding models.\n// Execute the model with the input Tensor\nengine.Execute(input);"
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#working-with-data",
    "href": "posts/deep-learning-unity-intro/index.html#working-with-data",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Working with Data",
    "text": "Working with Data\nBarracuda stores data in multi-dimensional array-like objects called Tensors.\n\nInitializing Tensors\nWe can initialize an input Tensor from an array for CPU data, a ComputeBuffer for general GPU data, or a Texture2D or RenderTexture for image data.\nInitialize from an array\n// Normal single-dimensional array\nfloat[] tensorData = new float[]\n{\n    0f, 1f, 2f, \n    3f, 4f, 5f,\n    6f, 7f, 8f \n};\n\nTensor tensor = new Tensor(n: 1, h: 3, w: 3, c: 1, tensorData);\nInitialize from image data\n// Initialize a Tensor using the inputTexture\nTensor input = new Tensor(inputTexture, channels: 3);\n\n\nAccessing Tensor Elements\nWe can access Tensor elements using multi-dimensional array operators.\n// Normal single-dimensional array\nfloat[] tensorData = new float[]\n{\n    0f, 1f, 2f, \n    3f, 4f, 5f,\n    6f, 7f, 8f \n};\n\n// Batch size: 1, Height: 3, Width: 3, Channels: 1\nTensor tensor = new Tensor(n: 1, h: 3, w: 3, c: 1, tensorData);\n\nDebug.Log($\"Tensor shape: {tensor.shape}\");\nDebug.Log($\"First element in flat array: {tensor[0]}\");\nDebug.Log($\"Second row, third column: {tensor[0, 1, 2, 0]}\");\n\ntensor.Dispose();\n\n// Batch size: 1, Height: 1, Width: 3, Channels: 3\ntensor = new Tensor(n: 1, h: 1, w: 3, c: 3, tensorData);\n\nDebug.Log($\"Tensor shape: {tensor.shape}\");\nDebug.Log($\"First element in flat array: {tensor[0]}\");\nDebug.Log($\"First row, first column, second channel: {tensor[0, 0, 0, 1]}\");\nDebug.Log($\"First row, second column, third channel: {tensor[0, 0, 1, 2]}\");\n\ntensor.Dispose();\nOutput\nTensor shape: (n:1, h:3, w:3, c:1)\nFirst element in flat array: 0\nSecond row, third column: 5\n\nTensor shape: (n:1, h:1, w:3, c:3)\nFirst element in flat array: 0\nFirst row, first column, second channel: 1\nFirst row, second column, third channel: 5\n\n\nRetrieving Model Output\nWe can download model output to the CPU or copy it to a RenderTexture to keep the data on the GPU, as shown below.\n// Get raw model output\nTensor output = engine.PeekOutput(outputLayer);\n\n// Copy model output to a RenderTexture\noutput.ToRenderTexture(outputTextureGPU);\nReading model output from the GPU to the CPU causes a pipeline stall as Unity prevents any execution on the main thread to prevent the data from changing before it has finished downloading to the CPU. The pipeline stall can cause a noticeable performance bottleneck that increases with the amount of data we need to download.\nThis performance bottleneck is not an issue when the model output can stay on the GPU like when performing artistic style transfer. However, reading the prediction of a simple image classifier to the CPU can cap GPU utilization from approximately 100% to around 60%.\n\nStandard GPU Readback\n\n\n\n\n\nFortunately, Unity provides a method to read data from the GPU asynchronously called AsyncGPUReadback.Request(). The one drawback to this method is that it adds a few frames of latency. That should not be noticeable as long as the frame rate is high enough.\n\n\nAsynchronous GPU Readback\n\n\n\n\n\n\n\n\nProcessing Data\nWe typically need to manually implement preprocessing steps like applying ImageNet normalization to input images. We can implement these preprocessing steps on the CPU using C# scripts or on the GPU using Compute shaders (when supported) or Fragment Shaders. Naturally, we want to perform image preprocessing on the GPU when possible.\n\nImageNet Normalization Compute shader\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel NormalizeImageNet\n\n// The pixel data for the input image\nTexture2D&lt;float4&gt; InputImage;\n// The pixel data for the processed image\nRWTexture2D&lt;float4&gt; Result;\n\n// Apply the ImageNet normalization stats from PyTorch to an image\n[numthreads(8, 8, 1)]\nvoid NormalizeImageNet(uint3 id : SV_DispatchThreadID)\n{\n    // Set the pixel color values for the processed image\n    Result[id.xy] = float4(\n        // Normalize the red color channel values\n        (InputImage[id.xy].r - 0.4850f) / 0.2290f,\n        // Normalize the green color channel values\n        (InputImage[id.xy].g - 0.4560f) / 0.2240f,\n        // Normalize the blue color channel values\n        (InputImage[id.xy].b - 0.4060f) / 0.2250f,\n        // Ignore the alpha/transparency channel\n        InputImage[id.xy].a);\n}\nWe can sometimes use Barracuda to handle postprocessing by adding additional layers to the end of models, like Sigmoid, Softmax, and Argmax, at runtime.\n// Create a model builder to modify the m_RunTimeModel\nModelBuilder modelBuilder = new ModelBuilder(m_RunTimeModel);\n\n// Add a new Softmax layer\nmodelBuilder.Softmax(softmaxLayer, outputLayer);\n// Add a new Argmax layer\nmodelBuilder.Reduce(Layer.Type.ArgMax, argmaxLayer, softmaxLayer);\n\n// Create a worker to execute the model using the selected backend\nIWorker engine = WorkerFactory.CreateWorker(workerType, modelBuilder.model);\nOtherwise, we need to implement those manually as well.\n// Get raw model output\nTensor output = engine.PeekOutput(outputLayer);\n\n// Initialize vector for coordinates\nVector2 coords = new Vector2();\n\n// Process estimated point coordinates\nfor (int i = 0; i &lt; output.length; i++)\n{\n    coords[i] = ((output[i] + 1) / 2) * inputDims[i] * (imageDims[i] / inputDims[i]);\n}"
  },
  {
    "objectID": "posts/deep-learning-unity-intro/index.html#summary",
    "href": "posts/deep-learning-unity-intro/index.html#summary",
    "title": "Getting Started With Deep Learning in Unity",
    "section": "Summary",
    "text": "Summary\nThis post introduced the Barracuda inference library for the Unity game engine. Barracuda is not the only option to perform inference in Unity, but it provides a good starting point. A follow-up tutorial series will walk through training a model using the fastai library, exporting it to ONNX format, and performing inference with it in a Unity project using the Barracuda library.\nNext: Fastai to Unity Tutorial Pt. 1"
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "",
    "text": "Overview\nMotivation\nGenerative models\nStructure-Aware Representations\nApplication"
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html#overview",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html#overview",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "Overview",
    "text": "Overview\nI wanted to get an idea of where the research is at for using deep learning models to generate 3D models for applications in procedural generation tools and creating synthetic datasets. I came across a video going over the 2020 paper, Learning Generative Models of 3D Structures. Below are some notes I took while watching."
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html#motivation",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html#motivation",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "Motivation",
    "text": "Motivation\n\n3D Graphics are now critical to many industries\nHuge cost in data capture and human labeling leads to lack of training data"
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html#generative-models",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html#generative-models",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "Generative models",
    "text": "Generative models\n\ngenerative: \\[\nP(X) \\ vs \\ discriminative: P(Y|X)\n\\]\nInstead of learning to predict some attribute Y given an input X, the generative model learns the entire input distribution, enabling them to sample objects directly from X\nCan be useful in simulating real-world environments and synthetically generating training data"
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html#structure-aware-representations",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html#structure-aware-representations",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "Structure-Aware Representations",
    "text": "Structure-Aware Representations\n\nScope: learned generative models of structured 3D content\n\n\nLearned:\n\nDetermined with data ↔︎ By hand or rules\n\n\n\nStructured:\n\n3D shapes and scenes that are decomposed into sub-structures ↔︎ a monolithic chunk of geometry\n\n\n\n\n\n\n\n\nStructure-Aware\n\nExpress 3D shapes and scenes using abstractions that allow manipulation of their high-level structure\nrepresent the geometry of the atomic structural elements\nrepresent the structural patterns\n\n\n\nStructure-Aware Representations\n\nRepresentations of Part/Object Geometry\n\nVoxel Grid\nPoint Cloud\nImplicit Surface\n\nA function that determines whether a point is inside or outside a surface\n\nTriangle Mesh\n\nRepresentations of Structure\n\nSegmented geometry\n\nLinks a label to each part of the entity’s geometry\n\nPart sets\n\nan unordered set of atoms (pieces)\n\nRelationship graphs\n\nWith edges between different parts of a scene or object\n\nHierarchies (trees)\nHierarchical Graphs\n\nCombine relationship graphs and hierarchies\n\nDeterministic Programs\n\nCan be made to output any of the above representations\nBeneficial for making patterns clear\nAllows editing by users"
  },
  {
    "objectID": "posts/learning-generative-models-of-3d-structures-notes/index.html#methodologies",
    "href": "posts/learning-generative-models-of-3d-structures-notes/index.html#methodologies",
    "title": "Notes on Learning Generative Models of 3D Structures",
    "section": "Methodologies",
    "text": "Methodologies\n\n\n\n\n\n\nProgram synthesis\n\nConstrain-based program synthesis\n\nUsed when only a few training examples are available\nTries to find the minimum cost program while satisfying some constraints\n\n\n\n\nClassical Probabilistic Models\n\nProbabilistic graphical models\n\nInput Type:\n\nSmall dataset, not large enough to train a deep learning model\nFixed structure\n\nExamples:\n\nFactor graph\nBayesian network\nMarkov random field\n\n\nProbabilistic grammars\n\nInput Type:\n\nSmall dataset, not large enough to train a deep learning model\nDynamic, tree-like structure\n\nExamples:\n\nContext-free grammar (CFG)\n\nUsed in natural language processing\na start symbol\na set of terminals and non-terminals\na set of rules that map a non-terminal to another layout\ngenerates a tree where the leaf nodes are terminals\n\nProbabilistic CFG (PCFG)\n\nAdds a probability of each rule\n\n\n\n\n\n\nDeep Generative Models\n\nInput Type:\n\nBig dataset\n\nAutoregressive models\n\nInput Type:\n\nNot globally-coherent\n\nIteratively consumes it’s output from one iteration as input for the next iteration\n\n\n\n\n\nWeakness:\n\nIf one step drifts from the training data, it can cause subsequent output to diverge further\n\n\nDeep latent variable models\n\nInput Type:\n\nGlobally-coherent\n\nVariational AutoEncoders (VAE)\nGenerative Adversarial Networks (GAN)\nCode Idea:\n\nSample over a low dimensional latent space in a trained generator that maps latent vectors to actual 3D shapes which are hard to sample.\nUse a global latent variable to control the generation\nTrained with a reconstruction loss between the input and generated output\nOften perform better than autoregressive models in terms of global coherence\n\n\n\n\n\nStructure Type\n\nRecurrent Neural Network\n\nData represented as a linear chain\n\nRecursive Neural Network RvNN\n\nData represented as a tree\n\nGraph Convolutional Network\n\nData represented as a graph\n\nNeural Program Synthesis\n\n​\n## Application\n\nSynthesize a plausible program that recreates an existing piece of 3D content\nRecover shape-generating programs from an existing 3D shape\nLearning Shape Abstractions by Assembling Volumetric Primitives (2017)\n\nLearned to reconstruct 3D shapes with simple geometric primitives\nDecompose shapes into primitives and used chamfer distance as a loss function\nhttps://github.com/shubhtuls/volumetricPrimitives\n\nLearning Shape Abstractions\nLearning Shape Abstractions by Assembling Volumetric Primitives\nLearning to Infer and Execute 3D Shape Programs (2019)\n\nModel can output a 3D shape program consisting of loops and other high level structures\n\n\n\n\n\n\n\nhttps://github.com/HobbitLong/shape2prog\n\nLearning to Infer and Execute 3D Shape Programs\nSuperquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids\n\nhttps://github.com/paschalidoud/superquadric_parsing\n\nPerform visual program induction directly from 2D images\n\nLiu et al. 2019 - Other Applications:\n\nPart-based shape synthesis\nIndoor scene synthesis\n\nReferences:\n\nCSC2547 Learning Generative Models of 3D Structures\nLearning Generative Models of 3D Structures (2020) (PDF)"
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html",
    "href": "posts/shape-key-motion-graphic-bpy/index.html",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "",
    "text": "Introduction\nImport Dependencies\nDefine Helper Functions\nSet up Scene\nCreate and Position Camera\nCreate Material With Emission Shader\nCreate a Plane With the Material\nCut Out Center From Plane\nAdd Shape Keys\nAdd Keyframes\nConclusion"
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#introduction",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#introduction",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Introduction",
    "text": "Introduction\nI decided to recreate this short tutorial from YouTube to practice using the Blender Python API. This post goes through the code I came up with to replicate the tutorial plus some small additions."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#import-dependencies",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#import-dependencies",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nThe only dependencies strictly required for this tutorial are bpy and bmesh. The bpy package is the base API for Blender and the bmesh module provides access to Blender’s internal mesh editing API. I also used the math module from the Python Standard Library for one of my helper functions."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#define-helper-functions",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#define-helper-functions",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Define Helper Functions",
    "text": "Define Helper Functions\nI made some wrapper functions for the standard location, rotation, and scale transformations as well as getting the name of the active object.\nYou can get the name of the active object with bpy.context.active_object.name.\nThe three standard transformations can be accessed for individual objects with the following:\n\nbpy.data.objects[\"object_name\"].location\nbpy.data.objects[\"object_name\"].rotation_euler\nbpy.data.objects[\"object_name\"].scale\n\nI also made a function to empty the default collection so that nothing gets duplicated. Collections can be accessed with bpy.data.collections[\"collection_name\"] or bpy.data.collections[index].\nLastly, I made a function to easily add sequences of keyframes to a given object. The function uses the built-in setattr() method to set the desired value for the target object and uses the object.keyframe_insert() method to add the keyframe."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#set-up-scene",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#set-up-scene",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Set up Scene",
    "text": "Set up Scene\nThe first thing I do is set the Color Management property, View Transform, from the default value of Filmic to Standard. This setting can be accessed at bpy.data.scenes[\"Scene\"].view_settings.view_transform.\nNext, I set the background to the desired color. In my case, it’s pure black. The background color is stored in bpy.data.worlds['World'].node_tree.nodes[\"Background\"].inputs[0].default_value.\nThe last setup step is to clear any objects added from the last time the script was run with the clear_collection() function."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#create-and-position-camera",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#create-and-position-camera",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Create and Position Camera",
    "text": "Create and Position Camera\nCameras can be added using the bpy.ops.object.camera_add() method. I then positioned the camera using the wrapper functions I defined earlier."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#create-material-with-emission-shader",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#create-material-with-emission-shader",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Create Material With Emission Shader",
    "text": "Create Material With Emission Shader\nI decided to add some color to the motion graphic so I needed to create a new material. It is recommended to check if the material exists before trying to create it. This can be done in one line as shown below.\nmaterial = bpy.data.materials.get(material_name) or bpy.data.materials.new(material_name)\nSince there’s is no light, I’ll add an Emission shader. This requires enabling nodes for the material with material.use_nodes = True.\nNext, I remove the default Principled_BSDF node as well as any Emission nodes from earlier runs. Nodes can be removed using the material.node_tree.nodes.remove() method.\nThe Emission node needs to be linked to the first slot in the Material Output node. Nodes are linked using the material.node_tree.links.new() method."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#create-a-plane-with-the-material",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#create-a-plane-with-the-material",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Create a Plane With the Material",
    "text": "Create a Plane With the Material\nThe object in the above motion graphic is a plain. Plains can be added using the bpy.ops.mesh.primitive_plane_add() method.\nI then assign the previously created material to the plane. Materials can be added to an object with object.data.materials.append(material)."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#cut-out-center-from-plane",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#cut-out-center-from-plane",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Cut Out Center From Plane",
    "text": "Cut Out Center From Plane\nThe next step is to make a square hole in the plane like in the above Gif. This requires modifying the mesh for the plane.\nMesh data for the currently selected object is stored at bpy.context.object.data.\nTo edit the mesh, we need to get a BMesh representation. We first create an empty BMesh with bm = bmesh.new() and then fill it with the mesh using bm.from_mesh(mesh).\nWe can make the square by adding a new inset to the plane using the bmesh.ops.inset_individual() method. Then, we delete the new face that gets created with bmesh.ops.delete().\nThe mesh then needs to be updated with these alterations using bm.to_mesh(mesh). Finally, we need to free the BMesh representation we created with bm.free()."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#add-shape-keys",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#add-shape-keys",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Add Shape Keys",
    "text": "Add Shape Keys\nWe can add shape keys with the bpy.ops.object.shape_key_add() method. To deform the plane, we need to access its vertices. We can do this in edit mode with the bmesh module.\nWe first enter edit mode for the plane with bpy.ops.object.mode_set(mode=\"EDIT\"). We can then create a new BMesh representation for the current mesh in edit mode using bm = bmesh.from_edit_mesh(mesh).\nThe vertices are stored in bm.verts, but we need to create our own list since we can’t index it directly.\nUnlike the tutorial video, I just set the positions for the inner vertices directly. It took some trial and error to determine the correct indices for the inner vertices.\nAfter freeing the BMesh representation, we can enter object mode with bpy.ops.object.mode_set(mode=\"OBJECT\").\n\nFirst Shape Key\n\n\n\n\n\n\n\nSecond Shape Key\nThe process for the second shape key is identical except it only moves two of the inner vertices."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#add-keyframes",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#add-keyframes",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Add Keyframes",
    "text": "Add Keyframes\nBefore adding the keyframes, I set the render frame rate as well the start and end frames for the scene. The frame rate is stored at bpy.context.scene.render.fps.\nThe start and end frames are stored in bpy.data.scenes['Scene'].frame_start and bpy.data.scenes['Scene'].frame_end respectively.\n\n\n\n\n\n\nShape Keys\nThe shape keys for the plane are stored in bpy.context.selected_objects[0].data.shape_keys. Individual shape keys can be accessed with bpy.context.selected_objects[0].data.shape_keys.key_blocks[index].\n\nFirst Shape Key\n\n\n\n\n\n\n\nSecond Shape Key\n\n\n\n\n\n\n\n\nPlane Rotation\n\n\n\n\n\n\n\nMaterial Color\nThe color for the Emision shader can be accessed at material.node_tree.nodes[\"Emission\"].inputs[\"Color\"].default_value."
  },
  {
    "objectID": "posts/shape-key-motion-graphic-bpy/index.html#conclusion",
    "href": "posts/shape-key-motion-graphic-bpy/index.html#conclusion",
    "title": "Create a Shape Key Motion Graphic with the Blender Python API",
    "section": "Conclusion",
    "text": "Conclusion\nI feel like this exercise was worthwhile as it forced me to learn about multiple parts of the API. Although, it took quite a bit longer than the nine minute length of the tutorial video to track down all the required parts of the API. Finding out how to properly add the Emission shader was particularly time consuming. I did not realize that the name used to create the Emission shader was different than the name used to reference it. Fortunately, Blender has been around for a while and someone on the internet had already asked how to do most of the individual steps.\nTutorial Resources: GitHub Repository"
  },
  {
    "objectID": "posts/unity-barracuda-inference-yolox-walkthrough/index.html",
    "href": "posts/unity-barracuda-inference-yolox-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Barracuda Inference YOLOX Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#introduction",
    "href": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Barracuda Inference YOLOX Package",
    "section": "Introduction",
    "text": "Introduction\nThe Barracuda Inference YOLOX package extends the functionality of unity-barracuda-inference-base to perform object detection using YOLOX models.\nObject detection has numerous potential uses in Unity applications, including giving NPCs a more realistic perception of their environment, gesture-based controls, and augmented reality, to name a few. Here is a demo video from a project that uses this package to detect hand gestures.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#package-overview",
    "href": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Barracuda Inference YOLOX Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains two C# scripts.\n\nYOLOXObjectDetector.cs: This script provides functionality to perform object detection with YOLOX models using the Barracuda inference engine.\nPackageInstaller.cs: An Editor utility script for automatically installing a list of dependency packages defined in a JSON file."
  },
  {
    "objectID": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#code-explanation",
    "href": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Barracuda Inference YOLOX Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Barracuda Inference YOLOX package by examining the purpose and functionality of each C# script.\n\nYOLOXObjectDetector.cs\nThis script defines the YOLOXObjectDetector class, which extends the BarracudaModelRunner class from the Barracuda Inference Base package to perform object detection using YOLOX models. This class also depends on the bounding-box-2d-toolkit and unity-yolox-utils packages. The complete code is available on GitHub at the link below.\n\nYOLOXObjectDetector.cs\n\n\nSerialized Fields\nThe YOLOXObjectDetector class includes a field to add a color map from a JSON file.\n// Output Processing configuration and variables\n[Header(\"Output Processing\")]\n// JSON file containing the color map for bounding boxes\n[SerializeField, Tooltip(\"JSON file with bounding box colormaps\")]\nprivate TextAsset colormapFile;\nIt also includes a field to control how often to unload memory assets when using Barracuda’s Pixel Shader backend. The Pixel Shader backend enables GPU inference on platforms that don’t support Compute Shaders. However, there seems to be a bug in the current version of Barracuda, which does not release unused assets when using this backend. Left unchecked, this can fill up both system and GPU memory. We can address this by manually freeing memory. Doing that every frame can hurt performance, so we’ll only do it at set intervals.\n[Header(\"Settings\")]\n[Tooltip(\"Interval (in frames) for unloading unused assets with Pixel Shader backend\")]\n[SerializeField] private int pixelShaderUnloadInterval = 100;\n\n\nSerializable Classes\nThe Colormap and ColormapList classes help store color map information from a JSON file, which we then use for generating bounding boxes.\n// Serializable classes to store color map information from JSON\n[System.Serializable]\nclass Colormap\n{\n    public string label;\n    public List&lt;float&gt; color;\n}\n\n[System.Serializable]\nclass ColormapList\n{\n    public List&lt;Colormap&gt; items;\n}\n\n\nPrivate Variables\n// A counter for the number of frames processed.\nprivate int frameCounter = 0;\n\n// Indicates if the system supports asynchronous GPU readback\nprivate bool supportsAsyncGPUReadback = false;\n\n// Stride values used by the YOLOX model\nprivate static readonly int[] Strides = { 8, 16, 32 };\n\n// Number of fields in each bounding box\nprivate const int NumBBoxFields = 5;\n\n// Layer names for the Transpose, Flatten, and TransposeOutput operations\nprivate const string TransposeLayer = \"transpose\";\nprivate const string FlattenLayer = \"flatten\";\nprivate const string TransposeOutputLayer = \"transposeOutput\";\nprivate string defaultOutputLayer;\n\n// Texture formats for output processing\nprivate TextureFormat textureFormat = TextureFormat.RHalf;\nprivate RenderTextureFormat renderTextureFormat = RenderTextureFormat.RHalf;\n\n// List to store label and color pairs for each class\nprivate List&lt;(string, Color)&gt; colormapList = new List&lt;(string, Color)&gt;();\n\n// Output textures for processing on CPU and GPU\nprivate Texture2D outputTextureCPU;\nprivate RenderTexture outputTextureGPU;\n\n// List to store grid and stride information for the YOLOX model\nprivate List&lt;GridCoordinateAndStride&gt; gridCoordsAndStrides = new List&lt;GridCoordinateAndStride&gt;();\n\n// Length of the proposal array for YOLOX output\nprivate int proposalLength;\n\n\nStart\nThis method runs at the start of the script. It performs several initializations, including checking for async GPU readback support, loading the color map list, and initializing the output texture.\n// Called at the start of the script\nprotected override void Start()\n{\n    base.Start();\n    CheckAsyncGPUReadbackSupport(); // Check if async GPU readback is supported\n    LoadColorMapList(); // Load colormap information from JSON file\n    CreateOutputTexture(1, 1); // Initialize output texture\n\n    proposalLength = colormapList.Count + NumBBoxFields; // Calculate proposal length\n}\n\n\nCheckAsyncGPUReadbackSupport\nThis method checks if the system supports asynchronous GPU readback\n// Check if the system supports async GPU readback\npublic bool CheckAsyncGPUReadbackSupport()\n{\n    supportsAsyncGPUReadback = SystemInfo.supportsAsyncGPUReadback && supportsAsyncGPUReadback;\n    return supportsAsyncGPUReadback;\n}\n\n\nLoadAndPrepareModel\nThis method loads and prepares the YOLOX model by setting worker types and applying transpose and flatten operations.\n// Load and prepare the YOLOX model\nprotected override void LoadAndPrepareModel()\n{\n    base.LoadAndPrepareModel();\n\n    defaultOutputLayer = modelBuilder.model.outputs[0];\n    WorkerFactory.Type bestType = WorkerFactory.ValidateType(WorkerFactory.Type.Auto);\n    bool supportsComputeBackend = bestType == WorkerFactory.Type.ComputePrecompiled;\n\n    // Set worker type for WebGL\n    if (Application.platform == RuntimePlatform.WebGLPlayer)\n    {\n        workerType = WorkerFactory.Type.PixelShader;\n    }\n\n    // Apply transpose operation on the output layer\n    modelBuilder.Transpose(TransposeLayer, defaultOutputLayer, new[] { 0, 3, 2, 1, });\n    defaultOutputLayer = TransposeLayer;\n\n    // Apply Flatten and TransposeOutput operations if supported\n    if (supportsComputeBackend && (workerType != WorkerFactory.Type.PixelShader))\n    {\n        modelBuilder.Flatten(FlattenLayer, TransposeLayer);\n        modelBuilder.Transpose(TransposeOutputLayer, FlattenLayer, new[] { 0, 1, 3, 2 });\n        modelBuilder.Output(TransposeLayer);\n        defaultOutputLayer = TransposeOutputLayer;\n    }\n}\n\n\nInitializeEngine\nThis method initializes the Barracuda engine and checks if asynchronous GPU readback is supported.\n/// &lt;summary&gt;\n/// Initialize the Barracuda engine\n/// &lt;summary&gt;\nprotected override void InitializeEngine()\n{\n    base.InitializeEngine();\n\n    // Check if async GPU readback is supported by the engine\n    supportsAsyncGPUReadback = engine.Summary().Contains(\"Unity.Barracuda.ComputeVarsWithSharedModel\");\n}\n\n\nLoadColorMapList\nThis method loads the color map list from a JSON file.\n/// &lt;summary&gt;\n/// Load the color map list from the JSON file\n/// &lt;summary&gt;\nprivate void LoadColorMapList()\n{\n    if (IsColorMapListJsonNullOrEmpty())\n    {\n        Debug.LogError(\"Class labels JSON is null or empty.\");\n        return;\n    }\n\n    ColormapList colormapObj = DeserializeColorMapList(colormapFile.text);\n    UpdateColorMap(colormapObj);\n}\n\n\nIsColorMapListJsonNullOrEmpty\nThis method checks if the provided color map JSON file is null or empty.\n/// &lt;summary&gt;\n/// Check if the color map JSON file is null or empty\n/// &lt;summary&gt;\nprivate bool IsColorMapListJsonNullOrEmpty()\n{\n    return colormapFile == null || string.IsNullOrWhiteSpace(colormapFile.text);\n}\n\n\nDeserializeColorMapList\nThis method deserializes the provided color map JSON string to a ColormapList object.\n/// &lt;summary&gt;\n/// Deserialize the color map list from the JSON string\n/// &lt;summary&gt;\nprivate ColormapList DeserializeColorMapList(string json)\n{\n    try\n    {\n        return JsonUtility.FromJson&lt;ColormapList&gt;(json);\n    }\n    catch (Exception ex)\n    {\n        Debug.LogError($\"Failed to deserialize class labels JSON: {ex.Message}\");\n        return null;\n    }\n}\n\n\nUpdateColorMap\nThis method updates the colormapList array with the provided ColormapList object.\n/// &lt;summary&gt;\n/// Update the color map list with deserialized data\n/// &lt;summary&gt;\nprivate void UpdateColorMap(ColormapList colormapObj)\n{\n    if (colormapObj == null)\n    {\n        return;\n    }\n\n    // Add label and color pairs to the colormap list\n    foreach (Colormap colormap in colormapObj.items)\n    {\n        Color color = new Color(colormap.color[0], colormap.color[1], colormap.color[2]);\n        colormapList.Add((colormap.label, color));\n    }\n}\n\n\nCreateOutputTexture\nThis method creates an output texture with the specified width and height.\n/// &lt;summary&gt;\n/// Create an output texture with the specified width and height.\n/// &lt;/summary&gt;\nprivate void CreateOutputTexture(int width, int height)\n{\n    outputTextureCPU = new Texture2D(width, height, textureFormat, false);\n}\n\n\nExecuteModel\nThis method executes the YOLOX model with a given input texture.\n/// &lt;summary&gt;\n/// Execute the YOLOX model with the given input texture.\n/// &lt;/summary&gt;\npublic void ExecuteModel(RenderTexture inputTexture)\n{\n    using (Tensor input = new Tensor(inputTexture, channels: 3))\n    {\n        base.ExecuteModel(input);\n    }\n\n    // Update grid_strides if necessary\n    if (engine.PeekOutput(defaultOutputLayer).length / proposalLength != gridCoordsAndStrides.Count)\n    {\n        gridCoordsAndStrides = YOLOXUtility.GenerateGridCoordinatesWithStrides(Strides, inputTexture.height, inputTexture.width);\n    }\n}\n\n\nProcessOutput\nThis method processes the output array from the YOLOX model, applying Non-Maximum Suppression (NMS), and returns an array of BBox2DInfo objects with class labels and colors.\n/// &lt;summary&gt;\n/// Process the output array from the YOLOX model, applying Non-Maximum Suppression (NMS) and\n/// returning an array of BBox2DInfo objects with class labels and colors.\n/// &lt;/summary&gt;\n/// &lt;param name=\"outputArray\"&gt;The output array from the YOLOX model&lt;/param&gt;\n/// &lt;param name=\"confidenceThreshold\"&gt;The minimum confidence score for a bounding box to be considered&lt;/param&gt;\n/// &lt;param name=\"nms_threshold\"&gt;The threshold for Non-Maximum Suppression (NMS)&lt;/param&gt;\n/// &lt;returns&gt;An array of BBox2DInfo objects containing the filtered bounding boxes, class labels, and colors&lt;/returns&gt;\npublic BBox2DInfo[] ProcessOutput(float[] outputArray, float confidenceThreshold = 0.5f, float nms_threshold = 0.45f)\n{\n    // Generate bounding box proposals from the output array\n    List&lt;BBox2D&gt; proposals = YOLOXUtility.GenerateBoundingBoxProposals(outputArray, gridCoordsAndStrides, colormapList.Count, NumBBoxFields, confidenceThreshold);\n\n    // Apply Non-Maximum Suppression (NMS) to the proposals\n    List&lt;int&gt; proposal_indices = BBox2DUtility.NMSSortedBoxes(proposals, nms_threshold);\n\n    // Create an array of BBox2DInfo objects containing the filtered bounding boxes, class labels, and colors\n    return proposal_indices\n        .Select(index =&gt; proposals[index])\n        .Select(bbox =&gt; new BBox2DInfo(bbox, colormapList[bbox.index].Item1, colormapList[bbox.index].Item2))\n        .ToArray();\n}\n\n\nCopyOutputToArray\nThis method copies the model output to a float array.\n/// &lt;summary&gt;\n/// Copy the model output to a float array.\n/// &lt;/summary&gt;\npublic float[] CopyOutputToArray()\n{\n    using (Tensor output = engine.PeekOutput(defaultOutputLayer))\n    {\n        if (workerType == WorkerFactory.Type.PixelShader)\n        {\n            frameCounter++;\n            if (frameCounter % pixelShaderUnloadInterval == 0)\n            {\n                Resources.UnloadUnusedAssets();\n                frameCounter = 0;\n            }\n        }\n        return output.data.Download(output.shape);\n    }\n}\n\n\nCopyOutputToTexture\nThis method copies the model output to a texture.\n/// &lt;summary&gt;\n/// Copy the model output to a texture.\n/// &lt;/summary&gt;\npublic void CopyOutputToTexture()\n{\n    using (Tensor output = engine.PeekOutput(TransposeLayer))\n    {\n        if (output.width != outputTextureCPU.width || output.height != outputTextureCPU.height)\n        {\n            CreateOutputTexture(output.width, output.height);\n            outputTextureGPU = RenderTexture.GetTemporary(output.width, output.height, 0, renderTextureFormat);\n        }\n        output.ToRenderTexture(outputTextureGPU);\n    }\n}\n\n\nCopyOutputWithAsyncReadback\nThis method copies the model output using asynchronous GPU readback if the platform supports it.\n/// &lt;summary&gt;\n/// Copy the model output using async GPU readback. If not supported, defaults to synchronous readback.\n/// &lt;/summary&gt;\npublic float[] CopyOutputWithAsyncReadback()\n{\n    if (!supportsAsyncGPUReadback)\n    {\n        Debug.Log(\"Async GPU Readback not supported. Defaulting to synchronous readback\");\n        return CopyOutputToArray();\n    }\n\n    CopyOutputToTexture();\n\n    AsyncGPUReadback.Request(outputTextureGPU, 0, textureFormat, OnCompleteReadback);\n\n    Color[] outputColors = outputTextureCPU.GetPixels();\n    float[] outputArray = outputColors.Select(color =&gt; color.r).Reverse().ToArray();\n\n    // Reverse the order of each proposal in the output array\n    for (int i = 0; i &lt; outputArray.Length; i += proposalLength)\n    {\n        Array.Reverse(outputArray, i, proposalLength);\n    }\n\n    return outputArray;\n}\n\n\nCropInputDims\nThis method crops input dimensions to be divisible by the maximum stride.\n/// &lt;summary&gt;\n/// Crop input dimensions to be divisible by the maximum stride.\n/// &lt;/summary&gt;\npublic Vector2Int CropInputDims(Vector2Int inputDims)\n{\n    inputDims[0] -= inputDims[0] % Strides.Max();\n    inputDims[1] -= inputDims[1] % Strides.Max();\n\n    return inputDims;\n}\n\n\nOnCompleteReadback\nThis method handles the completion of an async GPU readback request.\n/// &lt;summary&gt;\n/// Handle the completion of an async GPU readback request.\n/// &lt;/summary&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    if (outputTextureCPU != null)\n    {\n        try\n        {\n            // Load readback data into the output texture and apply changes\n            outputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n            outputTextureCPU.Apply();\n        }\n        catch (UnityException ex)\n        {\n            if (ex.Message.Contains(\"LoadRawTextureData: not enough data provided (will result in overread).\"))\n            {\n                Debug.Log(\"Updating input data size to match the texture size.\");\n            }\n            else\n            {\n                Debug.LogError($\"Unexpected UnityException: {ex.Message}\");\n            }\n        }\n    }\n}\n\n\nOnDisable\nThis method cleans up resources when the script is disabled.\n/// &lt;summary&gt;\n/// Clean up resources when the script is disabled.\n/// &lt;/summary&gt;\nprotected override void OnDisable()\n{\n    base.OnDisable();\n    // Release the temporary render texture\n    RenderTexture.ReleaseTemporary(outputTextureGPU);\n}\n\n\n\n\nPackageInstaller.cs\nIn this section, we will go through the PackageInstaller.cs script and explain how each part of the code works to install the required packages. The complete code is available on GitHub at the link below.\n\nPackageInstaller.cs\n\n\nSerializable Classes\nThe script defines two serializable classes to hold package data.\n// Serializable class to hold package data\n[System.Serializable]\npublic class PackageData\n{\n    public string packageName;\n    public string packageUrl;\n}\n\n// Serializable class to hold a list of PackageData objects\n[System.Serializable]\npublic class PackageList\n{\n    public List&lt;PackageData&gt; packages;\n}\nThese classes are for deserializing the JSON file containing the list of packages to install.\n\n\nPackageInstaller Class Variables\nThe PackageInstaller class contains several private static fields.\n// Stores the AddRequest object for the current package to install.\nprivate static AddRequest addRequest;\n// A list of PackageData objects to install.\nprivate static List&lt;PackageData&gt; packagesToInstall;\n// The index of the current package to install.\nprivate static int currentPackageIndex;\n\n// GUID of the JSON file containing the list of packages to install\nprivate const string PackagesJSONGUID = \"02aec9cd479b4b758a7afde0032230ec\";\n\n\nInstallDependencies\nThe InstallDependencies() method executes when Unity loads without action from the user. It reads the package JSON file and calls the InstallNextPackage() method to install the packages.\n// Method called on load to install packages from the JSON file\n[InitializeOnLoadMethod]\npublic static void InstallDependencies()\n{\n    // Read the package JSON file\n    packagesToInstall = ReadPackageJson().packages;\n    // Initialize the current package index\n    currentPackageIndex = 0;\n    // Start installing the packages\n    InstallNextPackage();\n}\n\n\nInstallNextPackage\nThis method installs the next package in the list.\n// Method to install the next package in the list\nprivate static void InstallNextPackage()\n{\n    // Iterate through package list\n    if (currentPackageIndex &lt; packagesToInstall.Count)\n    {\n        PackageData packageData = packagesToInstall[currentPackageIndex];\n\n        // Check if the package is already installed\n        if (!IsPackageInstalled(packageData.packageName))\n        {\n            // Attempt to install package\n            addRequest = Client.Add(packageData.packageUrl);\n            EditorApplication.update += PackageInstallationProgress;\n        }\n        else\n        {\n            // Increment the current package index\n            currentPackageIndex++;\n            // Recursively call InstallNextPackage\n            InstallNextPackage();\n        }\n    }\n}\n\n\nPackageInstallationProgress\nThis method monitors the progress of the package installation and logs whether it was successful. It then triggers the installation process for the next package in the list.\n// Method to monitor the progress of package installation\nprivate static void PackageInstallationProgress()\n{\n    if (addRequest.IsCompleted)\n    {\n        // Log whether the package installation was successful\n        if (addRequest.Status == StatusCode.Success)\n        {\n            UnityEngine.Debug.Log($\"Successfully installed: {addRequest.Result.packageId}\");\n        }\n        else if (addRequest.Status &gt;= StatusCode.Failure)\n        {\n            UnityEngine.Debug.LogError($\"Failed to install package: {addRequest.Error.message}\");\n        }\n\n        // Unregister the method from the EditorApplication.update \n        EditorApplication.update -= PackageInstallationProgress;\n        // Increment the current package index\n        currentPackageIndex++;\n        // Install the next package in the list\n        InstallNextPackage();\n    }\n}\n\n\nIsPackageInstalled\nThis method verifies whether a package has already been installed or not.\n// Method to check if a package is already installed\nprivate static bool IsPackageInstalled(string packageName)\n{\n    // List the installed packages\n    var listRequest = Client.List(true, false);\n    while (!listRequest.IsCompleted) { }\n\n    if (listRequest.Status == StatusCode.Success)\n    {\n        // Check if the package is already installed\n        return listRequest.Result.Any(package =&gt; package.name == packageName);\n    }\n    else\n    {\n        UnityEngine.Debug.LogError($\"Failed to list packages: {listRequest.Error.message}\");\n    }\n\n    return false;\n}\n\n\nReadPackageJson\nThis method reads the JSON file containing the list of packages to install and returns a PackageList object.\n// Method to read the JSON file and return a PackageList object\nprivate static PackageList ReadPackageJson()\n{\n    // Convert the PackagesJSONGUID to an asset path\n    string assetPath = AssetDatabase.GUIDToAssetPath(PackagesJSONGUID);\n    // Read the JSON file content as a string\n    string jsonString = File.ReadAllText(assetPath);\n    // Deserialize the JSON string into a PackageList object\n    return JsonUtility.FromJson&lt;PackageList&gt;(jsonString);\n}"
  },
  {
    "objectID": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#conclusion",
    "href": "posts/unity-barracuda-inference-yolox-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Barracuda Inference YOLOX Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Barracuda Inference YOLOX package. The package extends the functionality of unity-barracuda-inference-base to perform object detection using YOLOX models.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-barracuda-inference-yolox\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html",
    "href": "posts/streamlit-api-notes/index.html",
    "title": "Notes on the Streamlit API",
    "section": "",
    "text": "Overview\nStreamlit\nMagic Commands\nDisplay Text\nDisplay Data\nDisplay Charts\nDisplay Media\nAdd Widgets to Sidebar\nColumns\nControl Flow\nDisplay Interactive Widgets\nCommand Line\nMutate Data\nDisplay and Execute Code\nPlaceholders, help, and options\nPage Configuration\nCache Data Objects\nCache Non-data Objects\nDisplay Progress and Status"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#overview",
    "href": "posts/streamlit-api-notes/index.html#overview",
    "title": "Notes on the Streamlit API",
    "section": "Overview",
    "text": "Overview\nHere are some notes and reference code for working with the Streamlit API."
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#streamlit",
    "href": "posts/streamlit-api-notes/index.html#streamlit",
    "title": "Notes on the Streamlit API",
    "section": "Streamlit",
    "text": "Streamlit\n\nStreamlit - The fastest way to build and share data apps\nTurns data scripts into shareable web apps\npip install streamlit\nTest Installation: streamlit hello\nRun apps: streamlit run main.py\nFormat text using Markdown"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#magic-commands",
    "href": "posts/streamlit-api-notes/index.html#magic-commands",
    "title": "Notes on the Streamlit API",
    "section": "Magic Commands",
    "text": "Magic Commands\nreplit: https://replit.com/@innominate817/streamlit-magic-commands#main.py\nimport streamlit as st\nimport pandas as pd\n\nd = {'col1': [1,2], 'col2': [3,4]}\ndata = pd.DataFrame(data=d)\n\n# Magic commands implicitly call st.write()\n'_This_ is some **Markdown***'\nnum = 3\n'dataframe:', data"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-text",
    "href": "posts/streamlit-api-notes/index.html#display-text",
    "title": "Notes on the Streamlit API",
    "section": "Display Text",
    "text": "Display Text\nreplit: https://replit.com/@innominate817/streamlit-display-text#main.py\nimport streamlit as st\n\nst.text('Fixed width text')\nst.markdown('_Markdown_') # see *\nst.caption('Balloons. Hundreds of them...')\nst.latex(r''' e^{i\\pi} + 1 = 0 ''')\nst.write('Most objects') # df, err, func, keras!\nst.write(['st', 'is &lt;', 3]) # see *\nst.title('My title')\nst.header('My header')\nst.subheader('My sub')\nst.code('for i in range(8): foo()')"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-data",
    "href": "posts/streamlit-api-notes/index.html#display-data",
    "title": "Notes on the Streamlit API",
    "section": "Display Data",
    "text": "Display Data\nreplit: https://replit.com/@innominate817/streamlit-display-data\nimport streamlit as st\nimport pandas as pd\n\nd = [{\n    'a': 1,\n    'b': 2,\n    'c': 3,\n    'd': 4\n}, {\n    'a': 100,\n    'b': 200,\n    'c': 300,\n    'd': 400\n}, {\n    'a': 1000,\n    'b': 2000,\n    'c': 3000,\n    'd': 4000\n}]\ndata = pd.DataFrame(data=d)\n\nst.dataframe(data)\nst.table(data.iloc[0:2])\nst.json({'foo': 'bar', 'fu': 'ba'})\nst.metric(label=\"Temp\", value=\"273 K\", delta=\"1.2 K\")"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-charts",
    "href": "posts/streamlit-api-notes/index.html#display-charts",
    "title": "Notes on the Streamlit API",
    "section": "Display Charts",
    "text": "Display Charts\nreplit #1: https://replit.com/@innominate817/streamlit-display-charts#main.py\nimport streamlit as st\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nimport plotly.figure_factory as ff\nfrom bokeh.plotting import figure\n\nchart_data = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n\nst.line_chart(chart_data)\n\nchart_data = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n\nst.area_chart(chart_data)\n\nchart_data = pd.DataFrame(np.random.randn(50, 3), columns=[\"a\", \"b\", \"c\"])\n\nst.bar_chart(chart_data)\n\narr = np.random.normal(1, 1, size=100)\nfig, ax = plt.subplots()\nax.hist(arr, bins=20)\n\nst.pyplot(fig)\n\ndf = pd.DataFrame(np.random.randn(200, 3), columns=['a', 'b', 'c'])\n\nc = alt.Chart(df).mark_circle().encode(x='a',\n                                       y='b',\n                                       size='c',\n                                       color='c',\n                                       tooltip=['a', 'b', 'c'])\n\nst.altair_chart(c, use_container_width=True)\n\ndf = pd.DataFrame(np.random.randn(200, 3), columns=['a', 'b', 'c'])\n\nst.vega_lite_chart(\n    df, {\n        'mark': {\n            'type': 'circle',\n            'tooltip': True\n        },\n        'encoding': {\n            'x': {\n                'field': 'a',\n                'type': 'quantitative'\n            },\n            'y': {\n                'field': 'b',\n                'type': 'quantitative'\n            },\n            'size': {\n                'field': 'c',\n                'type': 'quantitative'\n            },\n            'color': {\n                'field': 'c',\n                'type': 'quantitative'\n            },\n        },\n    })\n\n# Add histogram data\nx1 = np.random.randn(200) - 2\nx2 = np.random.randn(200)\nx3 = np.random.randn(200) + 2\n\n# Group data together\nhist_data = [x1, x2, x3]\n\ngroup_labels = ['Group 1', 'Group 2', 'Group 3']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=[.1, .25, .5])\n\n# Plot!\nst.plotly_chart(fig, use_container_width=True)\n\nx = [1, 2, 3, 4, 5]\ny = [6, 7, 2, 4, 5]\n\np = figure(title='simple line example', x_axis_label='x', y_axis_label='y')\n\np.line(x, y, legend_label='Trend', line_width=2)\n\nst.bokeh_chart(p, use_container_width=True)\nreplit #2: https://replit.com/@innominate817/streamlit-display-charts-2#main.py\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport graphviz as graphviz\n\ndf = pd.DataFrame(np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n                  columns=['lat', 'lon'])\n\nst.pydeck_chart(\n    pdk.Deck(\n        map_style='mapbox://styles/mapbox/light-v9',\n        initial_view_state=pdk.ViewState(\n            latitude=37.76,\n            longitude=-122.4,\n            zoom=11,\n            pitch=50,\n        ),\n        layers=[\n            pdk.Layer(\n                'HexagonLayer',\n                data=df,\n                get_position='[lon, lat]',\n                radius=200,\n                elevation_scale=4,\n                elevation_range=[0, 1000],\n                pickable=True,\n                extruded=True,\n            ),\n            pdk.Layer(\n                'ScatterplotLayer',\n                data=df,\n                get_position='[lon, lat]',\n                get_color='[200, 30, 0, 160]',\n                get_radius=200,\n            ),\n        ],\n    ))\n\n# Create a graphlib graph object\ngraph = graphviz.Digraph()\ngraph.edge('run', 'intr')\ngraph.edge('intr', 'runbl')\ngraph.edge('runbl', 'run')\ngraph.edge('run', 'kernel')\ngraph.edge('kernel', 'zombie')\ngraph.edge('kernel', 'sleep')\ngraph.edge('kernel', 'runmem')\ngraph.edge('sleep', 'swap')\ngraph.edge('swap', 'runswap')\ngraph.edge('runswap', 'new')\ngraph.edge('runswap', 'runmem')\ngraph.edge('new', 'runmem')\ngraph.edge('sleep', 'runmem')\n\nst.graphviz_chart(graph)\n\ndf = pd.DataFrame(np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n                  columns=['lat', 'lon'])\nst.map(df)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-media",
    "href": "posts/streamlit-api-notes/index.html#display-media",
    "title": "Notes on the Streamlit API",
    "section": "Display Media",
    "text": "Display Media\nreplit: https://replit.com/@innominate817/streamlit-display-media#main.py\nNote: audio does not seem to pass through in replit\nimport streamlit as st\nfrom PIL import Image\n\nimage = Image.open('sunrise.jpg')\nst.image(image, caption='Sunrise by the mountains')\n\naudio_file = open('audio_sample.mp3', 'rb')\naudio_bytes = audio_file.read()\nst.audio(audio_bytes, format='audio/mp3')\n\nvideo_file = open('video_sample.mp4', 'rb')\nvideo_bytes = video_file.read()\nst.video(video_bytes)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#add-widgets-to-sidebar",
    "href": "posts/streamlit-api-notes/index.html#add-widgets-to-sidebar",
    "title": "Notes on the Streamlit API",
    "section": "Add Widgets to Sidebar",
    "text": "Add Widgets to Sidebar\nreplit: https://replit.com/@innominate817/streamlit-add-widgets-to-sidebar#main.py\nimport streamlit as st\n\n# Add individual widget to side bar\nradio1 = st.sidebar.radio('Option #1:', [1, 2])\n\n# Add group of widgets to sidebar\nwith st.sidebar:\n    radio2 = st.radio('Option #2:', [3, 4])\n    checkbox1 = st.checkbox('Check me')\n\nif radio2 == 3:\n    st.write('You selected number 3.')\nelse:\n    st.write(\"You selected number 4.\")\n\nif checkbox1:\n    st.write(\"checkbox checked\")\nelse:\n    st.write(\"checkbox unchecked\")"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#columns",
    "href": "posts/streamlit-api-notes/index.html#columns",
    "title": "Notes on the Streamlit API",
    "section": "Columns",
    "text": "Columns\nreplit: https://replit.com/@innominate817/streamlit-columns#main.py\nimport streamlit as st\n\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.header(\"A cat\")\n    st.image(\"https://images.pexels.com/photos/1170986/pexels-photo-1170986.jpeg?cs=srgb&dl=pexels-evg-culture-1170986.jpg&fm=jpg&w=640&h=960\")\n\nwith col2:\n    st.header(\"A dog\")\n    st.image(\"https://images.pexels.com/photos/2252311/pexels-photo-2252311.jpeg?cs=srgb&dl=pexels-laura-stanley-2252311.jpg&fm=jpg&w=640&h=959\")\n\nwith col3:\n    st.header(\"An owl\")\n    st.image(\"https://images.pexels.com/photos/5883285/pexels-photo-5883285.jpeg?cs=srgb&dl=pexels-mehmet-turgut-kirkgoz-5883285.jpg&fm=jpg&w=640&h=960\")"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#control-flow",
    "href": "posts/streamlit-api-notes/index.html#control-flow",
    "title": "Notes on the Streamlit API",
    "section": "Control Flow",
    "text": "Control Flow\nreplit: https://replit.com/@innominate817/streamlit-control-flow#main.py\nimport streamlit as st\n\nname = st.text_input('Name')\nif not name:\n    st.warning('Please input a name.')\n    st.stop()\nst.success('Thank you for inputting a name.')\n\nwith st.form(\"my_form\"):\n    st.write(\"Inside the form\")\n    slider_val = st.slider(\"Form slider\")\n    checkbox_val = st.checkbox(\"Form checkbox\")\n\n    # Every form must have a submit button.\n    submitted = st.form_submit_button(\"Submit\")\n    if submitted:\n        st.write(\"slider\", slider_val, \"checkbox\", checkbox_val)\n\nst.write(\"Outside the form\")"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-interactive-widgets",
    "href": "posts/streamlit-api-notes/index.html#display-interactive-widgets",
    "title": "Notes on the Streamlit API",
    "section": "Display Interactive Widgets",
    "text": "Display Interactive Widgets\nreplit: https://replit.com/@innominate817/streamlit-interactive-widgets#main.py\nimport streamlit as st\nimport numpy as np\nimport pandas as pd\nimport io\n\nst.header(\"Button\")\nif st.button('Say hello'):\n    st.write('Why hello there')\nelse:\n    st.write('Goodbye')\n\nst.header(\"Download Buttons\")\ndf = pd.DataFrame(np.random.randn(200, 3), columns=['a', 'b', 'c'])\n\n\n@st.cache\ndef convert_df(df):\n    # IMPORTANT: Cache the conversion to prevent computation on every rerun\n    return df.to_csv().encode('utf-8')\n\n\ncsv = convert_df(df)\n\nst.download_button(\n    label=\"Download data as CSV\",\n    data=csv,\n    file_name='large_df.csv',\n    mime='text/csv',\n)\n\ntext_contents = '''This is some text'''\nst.download_button('Download some text', text_contents)\n\nbinary_contents = b'example content'\n# Defaults to 'application/octet-stream'\nst.download_button('Download binary file', binary_contents)\n\nwith open(\"flower.jpg\", \"rb\") as file:\n    btn = st.download_button(label=\"Download image\",\n                             data=file,\n                             file_name=\"flower.png\",\n                             mime=\"image/png\")\n\nst.header(\"Checkbox\")\nagree = st.checkbox('I agree')\n\nif agree:\n    st.write('Great!')\n\nst.header(\"Radio Button\")\ngenre = st.radio(\"What's your favorite movie genre\",\n                 ('Comedy', 'Drama', 'Documentary'))\n\nif genre == 'Comedy':\n    st.write('You selected comedy.')\nelse:\n    st.write(\"You didn't select comedy.\")\n\nst.header(\"Selectbox\")\noption = st.selectbox('How would you like to be contacted?',\n                      ('Email', 'Home phone', 'Mobile phone'))\n\nst.write('You selected:', option)\n\nst.header(\"Multiselect\")\noptions_multi = st.multiselect('What are your favorite colors',\n                               ['Green', 'Yellow', 'Red', 'Blue'],\n                               ['Yellow', 'Red'])\n\nst.write('You selected:', options_multi)\n\nst.header(\"Sliders\")\nst.subheader('Basic')\nst.slider('Slide me', min_value=0, max_value=10)\nst.subheader('Range Slider')\nvalues = st.slider('Select a range of values', 0.0, 100.0, (25.0, 75.0))\nst.write('Values:', values)\n\nst.subheader('Range Time Slider')\nfrom datetime import time\n\nappointment = st.slider(\"Schedule your appointment:\",\n                        value=(time(11, 30), time(12, 45)))\nst.write(\"You're scheduled for:\", appointment)\n\nst.subheader('Datetime Slider')\nfrom datetime import datetime\n\nstart_time = st.slider(\"When do you start?\",\n                       value=datetime(2020, 1, 1, 9, 30),\n                       format=\"MM/DD/YY - hh:mm\")\nst.write(\"Start time:\", start_time)\n\nst.header(\"Select Sliders\")\ncolor = st.select_slider(\n    'Select a color of the rainbow',\n    options=['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet'])\nst.write('My favorite color is', color)\n\nstart_color, end_color = st.select_slider(\n    'Select a range of color wavelength',\n    options=['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet'],\n    value=('red', 'blue'))\nst.write('You selected wavelengths between', start_color, 'and', end_color)\n\nst.header(\"Text Input\")\ntitle = st.text_input('Movie title', 'Life of Brian')\nst.write('The current movie title is', title)\n\nst.header(\"Number Input\")\nnumber = st.number_input('Insert a number')\nst.write('The current number is ', number)\n\nst.header(\"Text Area\")\ntxt = st.text_area(\n    'Text to analyze', '''\n     It was the best of times, it was the worst of times, it was\n     the age of wisdom, it was the age of foolishness, it was\n     the epoch of belief, it was the epoch of incredulity, it\n     was the season of Light, it was the season of Darkness, it\n     was the spring of hope, it was the winter of despair, (...)\n     ''')\nst.write('First word:', txt.split(',')[0:2])\n\nst.header(\"Date Input\")\nfrom datetime import date\n\nd = st.date_input(\"When's your birthday\", date(2019, 7, 6))\nst.write('Your birthday is:', d)\n\nst.header(\"Time Input\")\nt = st.time_input('Set an alarm for', time(8, 45))\nst.write('Alarm is set for', t)\n\nst.header(\"File Uploader\")\nst.subheader(\"Single File\")\nuploaded_file = st.file_uploader(\"Choose a file\")\nif uploaded_file is not None:\n    # To read file as bytes:\n    bytes_data = uploaded_file.getvalue()\n    st.write(bytes_data)\n\n    # To convert to a string based IO:\n    stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n    st.write(stringio)\n\n    # To read file as string:\n    string_data = stringio.read()\n    st.write(string_data)\n\n    # Can be used wherever a \"file-like\" object is accepted:\n    dataframe = pd.read_csv(uploaded_file)\n    st.write(dataframe)\nst.subheader(\"Multiple Files\")\nuploaded_files = st.file_uploader(\"Choose a CSV file\",\n                                  accept_multiple_files=True)\nfor uploaded_file in uploaded_files:\n    bytes_data = uploaded_file.read()\n    st.write(\"filename:\", uploaded_file.name)\n    st.write(bytes_data)\n\nst.header(\"Color Picker\")\ncolor = st.color_picker('Pick A Color', '#00f900')\nst.write('The current color is', color)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#command-line",
    "href": "posts/streamlit-api-notes/index.html#command-line",
    "title": "Notes on the Streamlit API",
    "section": "Command Line",
    "text": "Command Line\nstreamlit --help\nstreamlit run your_script.py\nstreamlit hello\nstreamlit config show\nstreamlit cache clear\nstreamlit docs\nstreamlit --version"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#mutate-data",
    "href": "posts/streamlit-api-notes/index.html#mutate-data",
    "title": "Notes on the Streamlit API",
    "section": "Mutate Data",
    "text": "Mutate Data\nreplit: https://replit.com/@innominate817/streamlit-mutate-data#main.py\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.header(\"Mutate Table\")\ndf1 = pd.DataFrame(np.random.randn(50, 20),\n                   columns=('col %d' % i for i in range(20)))\n\nmy_table = st.table(df1)\n\ndf2 = pd.DataFrame(np.random.randn(50, 20),\n                   columns=('col %d' % i for i in range(20)))\n\nmy_table.add_rows(df2)\n# Now the table shown in the Streamlit app contains the data for\n# df1 followed by the data for df2.\n\nst.header(\"Mutate Chart\")\n# Assuming df1 and df2 from the example above still exist...\nmy_chart = st.line_chart(df1)\nmy_chart.add_rows(df2)\n# Now the chart shown in the Streamlit app contains the data for\n# df1 followed by the data for df2."
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-and-execute-code",
    "href": "posts/streamlit-api-notes/index.html#display-and-execute-code",
    "title": "Notes on the Streamlit API",
    "section": "Display and Execute Code",
    "text": "Display and Execute Code\nreplit: https://replit.com/@innominate817/streamlit-echo-code#main.py\nimport streamlit as st\n\nwith st.echo():\n    st.write('This code will be printed')\n    num = 2 + 2\n    st.write(f'Sum: {num}')"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#placeholders-help-and-options",
    "href": "posts/streamlit-api-notes/index.html#placeholders-help-and-options",
    "title": "Notes on the Streamlit API",
    "section": "Placeholders, help, and options",
    "text": "Placeholders, help, and options\nreplit: https://replit.com/@innominate817/streamlit-placeholders-help-options#main.py\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.set_page_config(layout='wide')\n\n# Replace any single element.\nelement = st.empty()\nchart_data = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\nelement.line_chart(chart_data)\nelement.text_input(\"New Text\")  # Replaces previous.\n\n# Insert out of order.\nelements = st.container()\nelements.line_chart(chart_data)\nst.write(\"Hello\")\nelements.text_input(\"Some more new text\")  # Appears above \"Hello\".\n\nst.help(pd.DataFrame)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#page-configuration",
    "href": "posts/streamlit-api-notes/index.html#page-configuration",
    "title": "Notes on the Streamlit API",
    "section": "Page Configuration",
    "text": "Page Configuration\nreplit: https://replit.com/@innominate817/streamlit-page-configuration#main.py\nimport streamlit as st\n\nst.set_page_config(page_title=\"Ex-stream-ly Cool App\",\n                   page_icon=\"🧊\",\n                   layout=\"wide\",\n                   initial_sidebar_state=\"expanded\",\n                   menu_items={\n                       'Get Help':\n                       'https://www.extremelycoolapp.com/help',\n                       'Report a bug':\n                       \"https://www.extremelycoolapp.com/bug\",\n                       'About':\n                       \"# This is a header. This is an *extremely* cool app!\"\n                   })"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#cache-data-objects",
    "href": "posts/streamlit-api-notes/index.html#cache-data-objects",
    "title": "Notes on the Streamlit API",
    "section": "Cache Data Objects",
    "text": "Cache Data Objects\nimport streamlit\nimport pandas as pd\nimport numpy as np\n\n# E.g. Dataframe computation, storing downloaded data, etc.\n@st.experimental_memo\ndef foo(bar):\n    # Do something expensive and return data\n    chart_data = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n    return chart_data\n# Executes foo\nd1 = foo(ref1)\n# Does not execute foo\n# Returns cached item by value, d1 == d2\nd2 = foo(ref1)\n# Different arg, so function foo executes\nd3 = foo(ref2)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#cache-non-data-objects",
    "href": "posts/streamlit-api-notes/index.html#cache-non-data-objects",
    "title": "Notes on the Streamlit API",
    "section": "Cache Non-data Objects",
    "text": "Cache Non-data Objects\nimport streamlit as st\n\n# E.g. TensorFlow session, database connection, etc.\n@st.experimental_singleton\ndef foo(bar):\n    # Create and return a non-data object\n    return session\n# Executes foo\ns1 = foo(ref1)\n# Does not execute foo\n# Returns cached item by reference, d1 == d2\ns2 = foo(ref1)\n# Different arg, so function foo executes\ns3 = foo(ref2)"
  },
  {
    "objectID": "posts/streamlit-api-notes/index.html#display-progress-and-status",
    "href": "posts/streamlit-api-notes/index.html#display-progress-and-status",
    "title": "Notes on the Streamlit API",
    "section": "Display Progress and Status",
    "text": "Display Progress and Status\nreplit: https://replit.com/@innominate817/streamlit-display-progress-and-status#main.py\nimport streamlit as st\nimport time\n\nwith st.spinner('Wait for it...'):\n    time.sleep(5)\nst.success('Done!')\n\nmy_bar = st.progress(0)\n\nfor percent_complete in range(100):\n    time.sleep(0.1)\n    my_bar.progress(percent_complete + 1)\n\nst.balloons()\n\nst.error('Error message')\n\nst.warning('Warning message')\n\nst.info('Info message')\n\nst.success('Success message')\n\ne = RuntimeError('This is an exception of type RuntimeError')\nst.exception(e)\nReferences:\n\nStreamlit Documentation"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html",
    "href": "posts/stylegan-v2-notes/index.html",
    "title": "Notes on StyleGANv2",
    "section": "",
    "text": "Overview\nNotable StyleGANv1 Characteristics\nNotable StyleGANv2 Changes\nStyleGANv1 Recap\nStyleGANv1 Artifacts\nOverview of StyleGANv2 Changes\nRecap"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#overview",
    "href": "posts/stylegan-v2-notes/index.html#overview",
    "title": "Notes on StyleGANv2",
    "section": "Overview",
    "text": "Overview\nI recently started learning more about generative deep learning models for some potential projects and decided to check out this video by Henry AI Labs covering StyleGANv2. Below are some notes I took while watching."
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#notable-styleganv1-characteristics",
    "href": "posts/stylegan-v2-notes/index.html#notable-styleganv1-characteristics",
    "title": "Notes on StyleGANv2",
    "section": "Notable StyleGANv1 Characteristics",
    "text": "Notable StyleGANv1 Characteristics\n\nAdaptive instance normalization\nA mapping network from the latent vector \\(z \\ \\epsilon \\ Z\\) into \\(w \\ \\epsilon \\ W\\)\nUses progressive GAN growing (starts with 4x4 input image and iteratively doubles dimensions)"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#notable-styleganv2-changes",
    "href": "posts/stylegan-v2-notes/index.html#notable-styleganv2-changes",
    "title": "Notes on StyleGANv2",
    "section": "Notable StyleGANv2 Changes",
    "text": "Notable StyleGANv2 Changes\n\nRestructures the use of adaptive instance normalization\nGets away from progressive growing to get away from the artifacts introduced in v1\n\nWater droplet effects\nFixed position of eyes and noses in generated faces\n\nPerceptual path-length normalization term in the loss function to improve on latent space interpolation\n\nLatent space interpolation: the changes in the generated image when changing the latent vector \\(Z\\)\nYou want small changes in the latent vector to have small semantic perceptual changes in the generated image\nThe interpolation is so smooth in v2 that you can create an animated GIF\nCan combine the vectors of two generated images and combine them to create an in-between image\n\nIntroduces a deep deep fake detection algorithm to project the generated images back into latent space to try to see if you can contribute the generated image to the network that created it"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#styleganv1-recap",
    "href": "posts/stylegan-v2-notes/index.html#styleganv1-recap",
    "title": "Notes on StyleGANv2",
    "section": "StyleGANv1 Recap",
    "text": "StyleGANv1 Recap\n\nMapping Network\n\nLatent vector \\(Z\\): a random vector that is passed to a network of eight fully connected layers that maps to the \\(w \\ \\epsilon \\ W\\) latent space\n\\(W\\) latent code: used to control the features in the generative adversarial network using the adaptive instance normalization layers\n\nThe feature maps are normalized with the mean and variance parameters of the feature maps (is this channel wide or feature map wide)\nThe feature maps are then scaled using the \\(W\\) parameters and shifted using the mean of the \\(W\\) vector\n\n\n\n\nUses progressive growing for the GAN\n\nStarts at a small model that generates 4x4 images and iteratively adds layers to increase the output resolution up to 1024x1024\n\n\n\nPerceptual path length quality loss metric\n\nmeasures how smooth the semantic change is to the output image when changing the latent vector \\(Z\\)\nTakes the baseline of the progressively growing GAN again with an FID score on the FFHQ dataset\nIntroduce tuning of the bi-linear up and down sampling\nAdd mapping and styles\nRemove traditional input: instead of using a latent vector \\(Z\\), they start with a constant value\nAdd noise inputs\nMixing regularization\n\n\n\nStyleGANv1 Artifacts\n\nDroplet Artifacts\n\ncommonly produces shiny blobs that look somewhat like water splotches on old photographic prints\noften show up at the interface between hair and the background\nattributed to the way the adaptive instance normalization is structured\nCan be used to distinguish between a real and generated image\nstarts to appear at the 64x64 resolution scale and persists all the way up to the final 1024x1024 resolution\n\n\n\nPhase Artifacts\n\nFeatures like mouths, eyes, and noses are fixed in place across generated images\nAppear as images are scaled up and walk along the latent space\nattributed to the structure of the progressive growing and having intermediate scales and intermediate low resolution maps that have to be used to produce images that fool the discriminator"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#overview-of-styleganv2-changes",
    "href": "posts/stylegan-v2-notes/index.html#overview-of-styleganv2-changes",
    "title": "Notes on StyleGANv2",
    "section": "Overview of StyleGANv2 Changes",
    "text": "Overview of StyleGANv2 Changes\n\nStart with baseline StyleGAN\nAdd weight demodulation\nAdd lazy regularization\nAdd path length regularization\nNo growing, new Generator and Discriminator architecture\nLarge networks\n\n\nRemoving Normalization Artifacts\n\nAdaptive Instance Normalization (StyleGANv1):\n\\(AdaIN(x,y) = \\sigma(y) \\ \\left(\\frac{x - \\mu(x)}{\\sigma(x)}\\right) + \\mu(y)\\) - Introduced in Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\n\nUsed in StyleGANv1 to have the latent vector \\(W\\) influence the features of the generator model\n\nThe latent vector \\(W\\) controls the scaling \\(\\sigma(y)\\) and shifting \\(\\mu(y)\\) parameters of the normalization of the intermediate features maps of the generator\nThey are separating the normalization of the feature maps\n\n\n\n\nStyleGANv2 Changes\n\nSeparate out the addition of the Gaussian noise \\(B\\) with the adaptive instance normalization layer\n\nReasoning: they might have conflicting effects\n\nSwitch from using adaptive instance normalization to weight demodulation layers\n\nScale the weight parameters by using \\(w^\\prime_{ijk}=s_i \\cdot w_{ijk}\\) where \\(s_i\\) is from the adaptive instance normalization from the \\(W\\) latent vector\nDemodulate it to assume that the features have unit variance (dividing all values by the standard deviation?)\n\n\\[\nw^{\\prime\\prime}_{ijk} = w^{\\prime}_{ijk} / {\\sqrt{\\sum_{i,k}{w^\\prime_{ijk}}^2+\\epsilon}}\n\\]\n\nChange the weight parameters of the 3x3 kernel size convolutional layer instead of having an intermediate modulation and normalizing layer\nremoving weight demodulation results in strange artifacts when interpolating between images\n\nAdd perceptual path length regularization metric to the loss function for the generator\n\nMake sure changes in the latent vector \\(Z\\) result in proportional semantic changes in the output image\nSmall changes in the latent vector \\(Z\\) should result in smooth changes in the output image\nreferenced a paper that found ImageNet-trained CNNs are biased towards texture and that increasing shape bias improves accuracy and robustness\n\nTraditional metrics relied on using pretrained image classifiers that are biased towards texture rather than shape detection\nImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\nTexture vs Shape: The bias in CNNs\nStylized Imgaenet\ntexture-vs-shape\n\n\\[\n  \\mathbb{E}_{w,y  N(0,I)} \\left(||J^{T}_{w}y||_{2}-a\\right)^{2}\n  \\]\nJacobian matrix \\(J_{w} = \\partial g(w)/\\partial w\\) ​ - sort of seeing the partial derivatives of output with respect to small changes in the latent vector that produces the images ​ - Use the small changes and the Jacobian matrix and multiply it by a random image \\(Y\\) and is randomly sampled at each iteration ​\nLazy regularization: only perform regularization every 16 steps\n\nGet away from progressive growing of the GAN\n\nprogressive growing requires a lot of hyper parameter search for the \\(\\alpha\\) value used to perform the element wise sum for the upsampled image\n\ncomplicates training\n\nInspired by MSG-GAN: Multi-Scale Gradient for Generative Adversarial Networks\n\nEnforces the intermediate feature maps in the generator by generating images from one (e.g. 4x4, 8x8, 16x16) and providing them as additional features to the discriminator\nMSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks\nBMSG-GAN\nStyleGANv2 does not use the exact same technique as MSG-GAN\n\nInstead of feeding intermediate features from the generator to the discriminator, they have more of a ResNet style architecture\n\nNot a traditional skip connection: They flatten each intermediate feature map (e.g. 256x256, 512x512) to 3-channel RGB format and feed those into the skip connection\n\nAllows the model to focus more on the larger feature maps\n\n\nDeep fake detection algorithm\n\nProjects generated images back into the latent space\nGoal is to find the latent \\(W\\) vector that produced the generated image\n\nThis allows the the generated image to be attributed to the generator model\nThe deep fake detection algorithm cannot find the latent vector would reproduce the real images\n\nNote: Might not be a robust solution for an actual deepfake detector"
  },
  {
    "objectID": "posts/stylegan-v2-notes/index.html#recap",
    "href": "posts/stylegan-v2-notes/index.html#recap",
    "title": "Notes on StyleGANv2",
    "section": "Recap",
    "text": "Recap\n\nStyleGANv2 Changes\n\nRestructured Adaptive Instance Normalization\nReplaced Progressive Growing with skip connections\nPerceptual path length (PPL) normalization\nPPL norm results in easier latent space projection (Deepfake Detection)\n\n\n\nTraining Speed Gains (1024x1024 resolution)\n\nStyleGANv1 → 37 images per second\nV2 Config E → 61 images per second (40% faster)\nV2 Config F → 31 images per second (larger networks)\nV2 Config F → 9 days on 8 Tesla V100 GPUs for FFHQ dataset, 13 days for LSUN CAR dataset\n\n\n\n\nConfiguration\nResolution\nTotal kimg\n1 GPU\nGPU Memory\n\n\n\n\nconfig-f\n1024x1024\n25000\n69d 23h\n13.3 GB\n\n\nconfig-f\n1024x1024\n10000\n27d 23h\n13.3 GB\n\n\nconfig-e\n1024x1024\n25000\n35d 11h\n8.6 GB\n\n\nconfig-e\n1024x1024\n10000\n14d 4h\n8.6 GB\n\n\nconfig-f\n256x256\n25000\n32d 13h\n6.4 GB\n\n\nconfig-f\n256x256\n10000\n13d 0h\n6.4 GB\n\n\n\nReferences:\n\nHenry AI Labs Video: StyleGANv2 Explained!\nStyleGANv2 Paper: Analyzing and Improving the Image Quality of StyleGAN"
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html",
    "href": "posts/targeted-in-game-style-transfer/index.html",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "",
    "text": "Overview\nDownload Unity Project\nMethod\nCreate Depth Cameras\nUpdate StyleTransferShader\nUpdate StyleTransfer Script\nAssign Depth Cameras\nAssign Stylize Layer With Code"
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#overview",
    "href": "posts/targeted-in-game-style-transfer/index.html#overview",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Overview",
    "text": "Overview\nIn this follow up to the in-game style transfer tutorial, we’ll cover a method to stylize specific GameObjects in a Unity. This tutorial builds on the project from the previous tutorial. If you did not follow that tutorial, you can follow the steps to download the project below."
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#download-unity-project",
    "href": "posts/targeted-in-game-style-transfer/index.html#download-unity-project",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Download Unity Project",
    "text": "Download Unity Project\nNote: You can skip this section If you followed the previous tutorial and already have the project.\nYou can download the project from the previous tutorial by clicking on the link below.\n\nGitHub Repository: (download)\n\nOnce the download is complete, extract the project folder.\nOpen Unity Hub and click the Add button.\n\n\n\n\n\nNavigate to the extracted End-to-End-In-Game-Style-Transfer-Tutorial-Intel-main folder and select the Kinematica_Demo_0.8.0-preview folder.\nWe’ll be working in Unity 2019.4.20, but later versions are also supported.\nThe project will take several minutes to open the first time as it imports the project assets.\n\nFix AssetImporter Error\nYou might get an error like the one below after opening the project in Unity.\nAssetImporter is referencing an asset from the previous import. This should not happen.\n\n\n\n\n\nYou can fix this issue by rebuilding the Unit asset. Open the Kinematica folder in the Assets section and double-click on the Unit asset. This will open the Kinematica Asset Builder window.\n\n\n\n\n\nClick Build in the pop-up window. You can close the pop-up window once the build is complete.\n\n\n\n\n\nBack in the Assets section, open the Biped scene in the Scenes folder. The project should run normally now. However, there might be some stuttering the first time it is run."
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#method",
    "href": "posts/targeted-in-game-style-transfer/index.html#method",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Method",
    "text": "Method\nFor this method, we’ll still be using the entire camera frame as the input for the style transfer model. However, we can use Layers to identify what parts of the final image will be from the stylized version of the current frame.\nWe’ll need to capture depth data from the scene to determine what needs to be stylized at runtime. We can’t use the regular color data because the lighting is different when a camera only renders certain parts of the scene.\n\nStandard Scene\n\n\n\n\n\n\n\nCharacter Only\n\n\n\n\n\nHere is an example of depth data captured from the camera that has been edited to be more easily viewable. Depth data is stored in the red color channel and can be difficult to see."
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#create-depth-cameras",
    "href": "posts/targeted-in-game-style-transfer/index.html#create-depth-cameras",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Create Depth Cameras",
    "text": "Create Depth Cameras\nIn the Hierarchy tab, select the Main Camera object and press Ctrl-d twice to create two copies.\n\n\n\n\n\nHold Ctrl and select both of the camera copies. Drag them onto the original Main Camera object to make them child objects.\n\n\n\n\n\nWith the duplicates still selected, we will remove every component except the Camera component. You can remove components by right-clicking on the component name and selecting Remove Component.\n\n\n\n\n\n\n\n\n\n\nAlso, make sure that the position and rotation values for the Transform component for each of the depth cameras are set to to zero. We need to make sure they are in exactly the same position as the Main Camera, or the depth values will be off.\n\n\n\n\n\nOne of the duplicate cameras will capture the depth data for the GameObjects we want to stylize. We’ll call this one Style Depth. The second duplicate will capture the depth data for the standard scene, and be called Source Depth.\n\n\n\n\n\n\nCreate Stylize Layer\nWe’ll identify the GameObjects we want to stylize by assigning a new Layer to these objects. Select the Layers drop-down menu and click Edit Layers....\n\n\n\n\n\nIn the Inspector tab open the Layers drop-down and enter Stylize in the User Layer 12 slot.\n\n\n\n\n\n\nSet Style Depth Culling Mask\nLastly, with the Stylize Depth camera selected, set the Culling Mask to Stylize only in the Inspector tab.\n\n\n\n\n\n\n\n\nAssign Stylize Layer\nWe can assign the Stylize layer to just about any GameObject we wish. To start, let’s assign it to the Unit parent object in the Hierarchy tab which contains the GameObjects for the character model. With the Unit object selected in the Hierarchy tab, select the Stylize layer from the Layer drop-down in the Inspector tab.\nNote: You might get a bunch of errors in the console when you select the Unit object. You can ignore these.\n\n\n\n\n\nYou will be prompted to chose whether to set the layer for the child objects as well. Select Yes, change children."
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#update-styletransfershader",
    "href": "posts/targeted-in-game-style-transfer/index.html#update-styletransfershader",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Update StyleTransferShader",
    "text": "Update StyleTransferShader\nNext, we will add a new function to our compute shader that will select what parts of the stylize frame to keep. In the Assets section, open the Shaders subfolder located inside the Style_Transfer folder. Open the StyleTransferShader in your code editor.\n\n\n\n\n\nFirst, we need to add three new Texture2D variables. Two of these will store the depth data from the Style Depth and Source Depth cameras. The third will store the original image for the current frame.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel ProcessInput\n#pragma kernel ProcessOutput\n\n// Create a RenderTexture with enableRandomWrite flag and set it\n// with cs.SetTexture\nRWTexture2D&lt;half4&gt; Result;\n// Stores the input image and is set with cs.SetTexture\nTexture2D&lt;half4&gt; InputImage;\n\n// Stores the depth data from the StyleDepth camera\nTexture2D&lt;half4&gt; StyleDepth;\n// Stores the depth data from the SourceDepth camera\nTexture2D&lt;half4&gt; SrcDepth;\n// Store the original image from the current frame\nTexture2D&lt;half4&gt; SrcImage;\n\n[numthreads(8, 8, 1)]\nvoid ProcessInput(uint3 id : SV_DispatchThreadID)\n{\n    Result[id.xy] = half4((InputImage[id.xy].x * 255.0h),\n        (InputImage[id.xy].y * 255.0h),\n        (InputImage[id.xy].z * 255.0h), 1.0h);\n}\nNow we can add the new function. We’ll call it Merge since it merges the original and stylized frame. This function compares the pixel values from the StyleDepth and SrcDepth textures. If they match, that means a target GameObject is present and there is nothing in front of it. However, this does not account for parts of the scene with infinite depth like the sky. We can add another check to see if the current pixel value for the StyleDepth texture is greater than zero.\n[numthreads(8, 8, 1)]\nvoid Merge(uint3 id : SV_DispatchThreadID)\n{\n    bool match = StyleDepth[id.xy].x == SrcDepth[id.xy].x;\n\n    if (match && (StyleDepth[id.xy].x &gt; 0)) {\n        Result[id.xy] = half4(InputImage[id.xy].r, InputImage[id.xy].g, InputImage[id.xy].b, InputImage[id.xy].a);\n    }\n    else {\n        Result[id.xy] = half4(SrcImage[id.xy].r, SrcImage[id.xy].g, SrcImage[id.xy].b, SrcImage[id.xy].a);\n    }\n}\nNow we just need to add the #pragma kernel Merge line at the top of the file.\n// Each #kernel tells which function to compile; you can have many kernels\n#pragma kernel ProcessInput\n#pragma kernel ProcessOutput\n#pragma kernel Merge"
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#update-styletransfer-script",
    "href": "posts/targeted-in-game-style-transfer/index.html#update-styletransfer-script",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Update StyleTransfer Script",
    "text": "Update StyleTransfer Script\nThe next step is to modify the StyleTransfer script. The script is located in the Scripts subfolder inside the Style_Transfer folder.\n\n\n\n\n\n\nAdd Depth Camera Variables\nThe only new variables we need to add are for the two depth cameras. We’ll name them styleDepth and sourceDepth respectively. We can also add another bool variable to toggle the targeted stylization.\npublic class StyleTransfer : MonoBehaviour\n{\n    [Tooltip(\"Performs the preprocessing and postprocessing steps\")]\n    public ComputeShader styleTransferShader;\n\n    [Tooltip(\"Stylize the camera feed\")]\n    public bool stylizeImage = true;\n\n    [Tooltip(\"Stylize only specified GameObjects\")]\n    public bool targetedStylization = true;\n\n    [Tooltip(\"The height of the image being fed to the model\")]\n    public int targetHeight = 540;\n\n    [Tooltip(\"The model asset file that will be used when performing inference\")]\n    public NNModel modelAsset;\n\n    [Tooltip(\"The backend used when performing inference\")]\n    public WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\n\n    [Tooltip(\"Captures the depth data for the target GameObjects\")]\n    public Camera styleDepth;\n\n    [Tooltip(\"Captures the depth data for the entire scene\")]\n    public Camera sourceDepth;\n\n\nAssign Depth Textures\nCurrently, the depth cameras are just capturing standard color data. We need to manually assign Depth textures to the targetTexture property for both of the cameras. We’ll do this at the top of the Start() method. The textures need to have the same dimensions as target screen. We can access this in Screen.width and Screen.height.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the screen dimensions\n    int width = Screen.width;\n    int height = Screen.height;\n\n    // Force the StyleDepth Camera to render to a Depth texture\n    styleDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n    styleDepth.forceIntoRenderTexture = true;\n    // Force the SourceDepth Camera to render to a Depth texture\n    sourceDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n    sourceDepth.forceIntoRenderTexture = true;\n\n    // Compile the model asset into an object oriented representation\n    m_RuntimeModel = ModelLoader.Load(modelAsset);\n\n    // Create a worker that will execute the model with the selected backend\n    engine = WorkerFactory.CreateWorker(workerType, m_RuntimeModel);\n}\n\n\nUpdate Depth Texture Dimensions\nIf the screen resolution changes while the project is running, the resolution for the depth textures will need to be updated as well. We can check if the screen resolution has changed in the Update() method.\nprivate void Update()\n{\n    if (styleDepth.targetTexture.width != Screen.width || styleDepth.targetTexture.height != Screen.height)\n    {\n        // Get the screen dimensions\n        int width = Screen.width;\n        int height = Screen.height;\n        // Assign depth textures with the new dimensions\n        styleDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n        sourceDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n    }\n}\n\n\nRelease Depth Textures\nWe will release the temporary Depth textures in the OnDisable() method.\n// OnDisable is called when the MonoBehavior becomes disabled or inactive\nprivate void OnDisable()\n{\n    // Release the resources allocated for the inference engine\n    engine.Dispose();\n    // Release the Depth texture for the StyleDepth camera\n    RenderTexture.ReleaseTemporary(styleDepth.targetTexture);\n    // Release the Depth texture for the SourceDepth camera\n    RenderTexture.ReleaseTemporary(sourceDepth.targetTexture);\n}\n\n\nCreate Merge() Method\nNext, we need to add a new method to dispatch the Merge function in the compute shader. This method will be nearly identical to the existing ProcessImage() method except that it will also set the values for the StyleDepth, SrcDepth, and SrcImage variables.\n/// &lt;summary&gt;\n/// Merge the stylized frame and the original frame on the GPU\n/// &lt;/summary&gt;\n/// &lt;param name=\"styleImage\"&gt;&lt;/param&gt;\n/// &lt;param name=\"sourceImage\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;The merged image&lt;/returns&gt;\nprivate void Merge(RenderTexture styleImage, RenderTexture sourceImage)\n{\n    // Specify the number of threads on the GPU\n    int numthreads = 8;\n    // Get the index for the specified function in the ComputeShader\n    int kernelHandle = styleTransferShader.FindKernel(\"Merge\");\n    // Define a temporary HDR RenderTexture\n    int width = styleImage.width;\n    int height = styleImage.height;\n    RenderTexture result = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.ARGBHalf);\n    // Enable random write access\n    result.enableRandomWrite = true;\n    // Create the HDR RenderTexture\n    result.Create();\n\n    // Set the value for the Result variable in the ComputeShader\n    styleTransferShader.SetTexture(kernelHandle, \"Result\", result);\n    // Set the value for the InputImage variable in the ComputeShader\n    styleTransferShader.SetTexture(kernelHandle, \"InputImage\", styleImage);\n    // Set the value for the StyleDepth variable in the ComputeShader\n    styleTransferShader.SetTexture(kernelHandle, \"StyleDepth\", styleDepth.activeTexture);\n    // Set the value for the SrcDepth variable in the ComputeShader\n    styleTransferShader.SetTexture(kernelHandle, \"SrcDepth\", sourceDepth.activeTexture);\n    // Set the value for the SrcImage variable in the ComputeShader\n    styleTransferShader.SetTexture(kernelHandle, \"SrcImage\", sourceImage);\n\n    // Execute the ComputeShader\n    styleTransferShader.Dispatch(kernelHandle, result.width / numthreads, result.height / numthreads, 1);\n\n    // Copy the result into the source RenderTexture\n    Graphics.Blit(result, styleImage);\n\n    // Release the temporary RenderTexture\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nUpdate OnRenderImage() Method\nWe’ll call the Merge() method inside the OnRenderImage() method. First, we need to create a copy of the current frame before it gets stylized. The Merge() method will only be called when targetedStylization is set to true and stylizeImage is set to true. Lastly, we need to release the temporary RenderTexture for the copy of the current frame.\n/// &lt;summary&gt;\n/// OnRenderImage is called after the Camera had finished rendering \n/// &lt;/summary&gt;\n/// &lt;param name=\"src\"&gt;Input from the Camera&lt;/param&gt;\n/// &lt;param name=\"dest\"&gt;The texture for the targer display&lt;/param&gt;\nvoid OnRenderImage(RenderTexture src, RenderTexture dest)\n{\n    // Create a temporary RenderTexture to store copy of the current frame\n    RenderTexture sourceImage = RenderTexture.GetTemporary(src.width, src.height, 24, src.format);\n    // Copyt the current frame\n    Graphics.Blit(src, sourceImage);\n\n    if (stylizeImage)\n    {\n        StylizeImage(src);\n\n        if (targetedStylization)\n        {\n            // Merge the stylized frame and origina frame\n            Merge(src, sourceImage);\n        }\n    }\n\n    Graphics.Blit(src, dest);\n\n    // Release the temporary RenderTexture\n    RenderTexture.ReleaseTemporary(sourceImage);\n}"
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#assign-depth-cameras",
    "href": "posts/targeted-in-game-style-transfer/index.html#assign-depth-cameras",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Assign Depth Cameras",
    "text": "Assign Depth Cameras\nThe last step is to assign the depth cameras to the StyleTransfer script in the Inspector tab. Select the Main Camera object in the Hierarchy tab. Drag and drop each of the depth cameras into their respective spots in the Inspector tab.\n\n\n\n\n\n\nTest it Out\nIf you hit the play button, you should see that only the GameObjects with the Stylize layer are stylized."
  },
  {
    "objectID": "posts/targeted-in-game-style-transfer/index.html#assign-stylize-layer-with-code",
    "href": "posts/targeted-in-game-style-transfer/index.html#assign-stylize-layer-with-code",
    "title": "Targeted In-Game Style Transfer Tutorial",
    "section": "Assign Stylize Layer With Code",
    "text": "Assign Stylize Layer With Code\nYou might want to change which objects are stylized depending on certain conditions. As an example, we’ll demonstrate how to toggle stylization on and off for a GameObject when they are clicked on with the mouse at runtime.\n\nMethod\nA common method to select GameObjects with the mouse is to use the Physics.Raycast() method. This approach involves casting a ray (i.e. a line) from the camera to the current mouse position. If that ray makes contact with a GameObject that has a collider component, we can identify that object and manipulate it in code.\nThis method will not work if the GameObject does not have a collider, so keep that in mind when using this approach in other projects.\nThe Kinematica Demo is not ideally structured for selecting individual GameObjects with the mouse. For example, the colliders for the level boundaries are not directly attached to the GameObjects like the one outlined below. That means we can’t click on any of the four level boundaries without adding new collider components.\n\n\n\n\n\nThe floor panels already have their own colliders attached and will work out of the box.\n\n\n\n\n\nThe obstacles that we can climb over are actually children of GameObjects that have a collider. That means every child object will be stylized rather than the specific child object we click on.\n\n\n\n\n\n\n\nSteps\nWe’ll implement the following steps in the Update() method.\n\nCheck if the mouse button has been clicked with Input.GetMouseButtonUp() method.\nCreate a new Ray variable that contains a ray going from the main camera to the current mouse position\nCreate a new RaycastHit variable.\nCheck if the ray has hit any colliders with the Physics.Raycast() method.\n\nGet a list of the child objects of the GameObject with the collider, including GameObject itself.\nIterate through the list of child objects.\n\nCheck if the child object has a MeshRenderer component that is enabled.\n\nToggle the objects layer between Default and Stylize.\n\n\n\n\nif (styleDepth.targetTexture.width != Screen.width || styleDepth.targetTexture.height != Screen.height)\n{\n    // Get the screen dimensions\n    int width = Screen.width;\n    int height = Screen.height;\n\n    // Assign depth textures with the new dimensions\n    styleDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n    sourceDepth.targetTexture = RenderTexture.GetTemporary(width, height, 24, RenderTextureFormat.Depth);\n}\n\nif (Input.GetMouseButtonUp(0))\n{\n    Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition);\n    RaycastHit hit;\n    if (Physics.Raycast(ray, out hit))\n    {\n        Transform[] allChildren = hit.transform.gameObject.GetComponentsInChildren&lt;Transform&gt;();\n        for (int i = 0; i &lt; allChildren.Length; i++)\n        {\n            MeshRenderer meshRenderer = allChildren[i].GetComponent&lt;MeshRenderer&gt;();\n            if (meshRenderer != null && meshRenderer.enabled)\n            {\n                if (allChildren[i].gameObject.layer == 12)\n                {\n                    allChildren[i].gameObject.layer = 0;\n                }\n                else\n                {\n                    allChildren[i].gameObject.layer = 12;\n                }\n            }\n        }\n    }\n}\nIf we run the game now, we can click on GameObjects with colliders to toggle the stylization effect."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "",
    "text": "Introduction\nOverview\nInstall Dependencies\nImport Dependencies\nSelect a Model\nDownload the Dataset\nInspect the Dataset\nDefine DataLoaders\nFinetune the Model\nClean Dataset (Optional)\nTest the Model\nExport the Model\nExport Inference Data\nSummary"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#tutorial-links",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#tutorial-links",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a hand gesture classifier using fastai and export it to TensorFlow.js.\nPart 2: Create a TensorFlow.js plugin for the Unity game engine.\nPart 3: Build a Unity project as a WebGL application and host it using GitHub Pages.\nGitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#introduction",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#introduction",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Introduction",
    "text": "Introduction\nIn this three-part tutorial series, we will use fastai and TensorFlow.js to create an in-browser hand gesture recognition system in Unity. In Part 1, we will train a hand gesture classifier using fastai and export it to TensorFlow.js. In Part 2, we will create a TensorFlow.js plugin for the Unity game engine. Finally, in Part 3, we will host the Unity project as a live demo on GitHub Pages. By the end of this tutorial series, you will have a hand gesture recognition system that you can use in your Unity projects.\nIn-Browser Demo: Hand Gesture Classifier\nThe tutorial uses a downscaled version of HaGRID (HAnd Gesture Recognition Image Dataset) that I modified for image classification tasks. The dataset contains images for 18 distinct hand gestures and an additional no_gesture class to account for idle hands.\n\n\n\nReference Images\n\n\n\n\n\n\n\n\nClass\n\n\nImage\n\n\n\n\n\n\ncall\n\n\n\n\n\n\n\ndislike\n\n\n\n\n\n\n\nfist\n\n\n\n\n\n\n\nfour\n\n\n\n\n\n\n\nlike\n\n\n\n\n\n\n\nmute\n\n\n\n\n\n\n\nok\n\n\n\n\n\n\n\none\n\n\n\n\n\n\n\npalm\n\n\n\n\n\n\n\npeace\n\n\n\n\n\n\n\npeace_inverted\n\n\n\n\n\n\n\nrock\n\n\n\n\n\n\n\nstop\n\n\n\n\n\n\n\nstop_inverted\n\n\n\n\n\n\n\nthree\n\n\n\n\n\n\n\nthree2\n\n\n\n\n\n\n\ntwo_up\n\n\n\n\n\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\n\n\n\n\nWe can use a model trained on this dataset to map hand gestures to user input via a webcam in Unity."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#overview",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#overview",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Overview",
    "text": "Overview\nIn Part 1 of this tutorial series, we finetune an image classifier from the timm library using fastai and export it to TensorFlow.js. We will start by installing and importing the necessary dependencies. Then, we will select a model to use and download a dataset to train it. After inspecting the dataset, we will define data loaders to use for training. Finally, we will finetune and export the model. We also demonstrate how to clean the dataset to improve training. By the end of this post, you will have a trained hand gesture classifier that you can use in web applications.\nYou can find links to view the training code and run it on Google Colab and Kaggle below.\n\n\n\nJupyter Notebook\nColab\nKaggle\n\n\n\n\nGitHub Repository\nOpen In Colab\nOpen in Kaggle"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#install-dependencies",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#install-dependencies",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Install Dependencies",
    "text": "Install Dependencies\nThe training code requires PyTorch for the fastai library, the fastai library itself for training, and the Kaggle API Python package for downloading the dataset. The timm library provides access to a wide range of pretrained image models.\nInstall model training dependencies\n# %%capture\n# !pip install -U torch torchvision torchaudio\n# !pip install -U fastai==2.7.9\n# !pip install -U kaggle\n# !pip install -U Pillow\n# !pip install -U timm\nThe model conversion process involves exporting the PyTorch model to ONNX, converting the ONNX model to a TensorFlow SavedModel, then converting the SavedModel to TensorFlow.js web format.\nInstall Tensorflow.js conversion dependencies\n# %%capture\n# !pip install -U onnxruntime\n# !pip install onnx-tf\n# !pip install tensorflowjs\n# !pip install tensorflow_probability\n# !pip install onnx-simplifier\n# !pip install simple-onnx-processing-tools\n# !pip install -U onnx_graphsurgeon --index-url https://pypi.ngc.nvidia.com"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#import-dependencies",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#import-dependencies",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nImport all fastai computer vision functionality\nfrom fastai.vision.all import *\nImport pandas and disable column and row limits\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nImport timm library\nimport timm\ntimm.__version__\n'0.6.7'"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#select-a-model",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#select-a-model",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Select a Model",
    "text": "Select a Model\nTimm provides many pretrained models, but not all are fast enough for real-time applications. We can filter through the available models using the timm.list_models() function.\nView available ResNet models\npd.DataFrame(timm.list_models('resnet*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nresnet10t\n\n\n\n\n1\n\n\nresnet14t\n\n\n\n\n2\n\n\nresnet18\n\n\n\n\n3\n\n\nresnet18d\n\n\n\n\n4\n\n\nresnet26\n\n\n\n\n5\n\n\nresnet26d\n\n\n\n\n6\n\n\nresnet26t\n\n\n\n\n7\n\n\nresnet32ts\n\n\n\n\n8\n\n\nresnet33ts\n\n\n\n\n9\n\n\nresnet34\n\n\n\n\n10\n\n\nresnet34d\n\n\n\n\n11\n\n\nresnet50\n\n\n\n\n12\n\n\nresnet50_gn\n\n\n\n\n13\n\n\nresnet50d\n\n\n\n\n14\n\n\nresnet51q\n\n\n\n\n15\n\n\nresnet61q\n\n\n\n\n16\n\n\nresnet101\n\n\n\n\n17\n\n\nresnet101d\n\n\n\n\n18\n\n\nresnet152\n\n\n\n\n19\n\n\nresnet152d\n\n\n\n\n20\n\n\nresnet200d\n\n\n\n\n21\n\n\nresnetaa50\n\n\n\n\n22\n\n\nresnetblur50\n\n\n\n\n23\n\n\nresnetrs50\n\n\n\n\n24\n\n\nresnetrs101\n\n\n\n\n25\n\n\nresnetrs152\n\n\n\n\n26\n\n\nresnetrs200\n\n\n\n\n27\n\n\nresnetrs270\n\n\n\n\n28\n\n\nresnetrs350\n\n\n\n\n29\n\n\nresnetrs420\n\n\n\n\n30\n\n\nresnetv2_50\n\n\n\n\n31\n\n\nresnetv2_50d_evos\n\n\n\n\n32\n\n\nresnetv2_50d_gn\n\n\n\n\n33\n\n\nresnetv2_50x1_bit_distilled\n\n\n\n\n34\n\n\nresnetv2_50x1_bitm\n\n\n\n\n35\n\n\nresnetv2_50x1_bitm_in21k\n\n\n\n\n36\n\n\nresnetv2_50x3_bitm\n\n\n\n\n37\n\n\nresnetv2_50x3_bitm_in21k\n\n\n\n\n38\n\n\nresnetv2_101\n\n\n\n\n39\n\n\nresnetv2_101x1_bitm\n\n\n\n\n40\n\n\nresnetv2_101x1_bitm_in21k\n\n\n\n\n41\n\n\nresnetv2_101x3_bitm\n\n\n\n\n42\n\n\nresnetv2_101x3_bitm_in21k\n\n\n\n\n43\n\n\nresnetv2_152x2_bit_teacher\n\n\n\n\n44\n\n\nresnetv2_152x2_bit_teacher_384\n\n\n\n\n45\n\n\nresnetv2_152x2_bitm\n\n\n\n\n46\n\n\nresnetv2_152x2_bitm_in21k\n\n\n\n\n47\n\n\nresnetv2_152x4_bitm\n\n\n\n\n48\n\n\nresnetv2_152x4_bitm_in21k\n\n\n\n\n\n\n\nThe smaller ResNet models are both fast and sufficiently accurate in most settings. Unfortunately, the resnet10t and resnet14t models contain operations unsupported by the TensorFlow.js conversion script. We’ll instead use the resnet18 model for our lightweight option.\nInspect config for specific model\nEach model comes with a set of default configuration parameters. We must keep track of the mean and std values used to normalize the model input. Many pretrained models use the ImageNet normalization stats, but others, like MobileViT, do not.\nfrom timm.models import resnet\nresnet_model = 'resnet18'\npd.DataFrame.from_dict(resnet.default_cfgs[resnet_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://download.pytorch.org/models/resnet18-5c106cde.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.875\n\n\n\n\ninterpolation\n\n\nbilinear\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nconv1\n\n\n\n\nclassifier\n\n\nfc\n\n\n\n\n\n\nView available ConvNeXt models\npd.DataFrame(timm.list_models('convnext*', pretrained=True))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nconvnext_base\n\n\n\n\n1\n\n\nconvnext_base_384_in22ft1k\n\n\n\n\n2\n\n\nconvnext_base_in22ft1k\n\n\n\n\n3\n\n\nconvnext_base_in22k\n\n\n\n\n4\n\n\nconvnext_large\n\n\n\n\n5\n\n\nconvnext_large_384_in22ft1k\n\n\n\n\n6\n\n\nconvnext_large_in22ft1k\n\n\n\n\n7\n\n\nconvnext_large_in22k\n\n\n\n\n8\n\n\nconvnext_nano\n\n\n\n\n9\n\n\nconvnext_small\n\n\n\n\n10\n\n\nconvnext_small_384_in22ft1k\n\n\n\n\n11\n\n\nconvnext_small_in22ft1k\n\n\n\n\n12\n\n\nconvnext_small_in22k\n\n\n\n\n13\n\n\nconvnext_tiny\n\n\n\n\n14\n\n\nconvnext_tiny_384_in22ft1k\n\n\n\n\n15\n\n\nconvnext_tiny_hnf\n\n\n\n\n16\n\n\nconvnext_tiny_in22ft1k\n\n\n\n\n17\n\n\nconvnext_tiny_in22k\n\n\n\n\n18\n\n\nconvnext_xlarge_384_in22ft1k\n\n\n\n\n19\n\n\nconvnext_xlarge_in22ft1k\n\n\n\n\n20\n\n\nconvnext_xlarge_in22k\n\n\n\n\n\n\n\nThe convnext_nano model is highly accurate for its size and is a good choice when compute power is less constrained.\nInspect config for specific model\nfrom timm.models import convnext\nconvnext_model = 'convnext_nano'\npd.DataFrame.from_dict(convnext.default_cfgs[convnext_model], orient='index')\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nurl\n\n\nhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_nano_d1h-7eb4bdea.pth\n\n\n\n\nnum_classes\n\n\n1000\n\n\n\n\ninput_size\n\n\n(3, 224, 224)\n\n\n\n\npool_size\n\n\n(7, 7)\n\n\n\n\ncrop_pct\n\n\n0.95\n\n\n\n\ninterpolation\n\n\nbicubic\n\n\n\n\nmean\n\n\n(0.485, 0.456, 0.406)\n\n\n\n\nstd\n\n\n(0.229, 0.224, 0.225)\n\n\n\n\nfirst_conv\n\n\nstem.0\n\n\n\n\nclassifier\n\n\nhead.fc\n\n\n\n\ntest_input_size\n\n\n(3, 288, 288)\n\n\n\n\ntest_crop_pct\n\n\n1.0\n\n\n\n\n\n\n\nSelect a model\n# model_type = resnet\n# model_name = resnet_model\nmodel_type = convnext\nmodel_name = convnext_model\nExtract normalization stats from model config\nmean = model_type.default_cfgs[model_name]['mean']\nstd = model_type.default_cfgs[model_name]['std']\nmean, std\n((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#download-the-dataset",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#download-the-dataset",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Download the Dataset",
    "text": "Download the Dataset\nThe Kaggle API tool requires an API Key for a Kaggle account. Sign in or create a Kaggle account using the link below, then click the Create New API Token button.\n\nKaggle Account Settings: https://www.kaggle.com/me/account\n\n\n\n\n\n\nKaggle will generate and download a kaggle.json file containing your username and new API token. Initialize the creds variable with the values for each.\nEnter Kaggle username and API token\ncreds = '{\"username\":\"\",\"key\":\"\"}'\nSave Kaggle credentials if none are present\n\nSource: https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\n# Save API key to a json file if it does not already exist\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\nImport Kaggle API\nfrom kaggle import api\nSelect a dataset\nNow that we have our Kaggle credentials set, we need to define the dataset and where to store it. I made three different-sized versions of the dataset available on Kaggle.\n\nHaGRID Classification 512p no_gesture 150k\n\nImages: 154,816\nSize: 4 GB\n\nHaGRID Classification 512p no_gesture 300k\n\nImages: 309,632\nSize: 8 GB\n\nHaGRID Classification 512p no_gesture\n\nImages: 619,264\nSize: 15 GB\n\n\ndataset_name = 'hagrid-classification-512p-no-gesture-150k'\n# dataset_name = 'hagrid-classification-512p-no-gesture-300k'\n# dataset_name = 'hagrid-classification-512p-no-gesture'\n\nkaggle_dataset = f'innominate817/{dataset_name}'\nDefine path to dataset\nWe will use the default archive and data folders for the fastai library to store the compressed and uncompressed datasets.\narchive_dir = URLs.path()\ndataset_dir = archive_dir/'../data'\narchive_path = Path(f'{archive_dir}/{dataset_name}.zip')\ndataset_path = Path(f'{dataset_dir}/{dataset_name}')\nDefine method to extract the dataset from an archive file\ndef file_extract(fname, dest=None):\n    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n    if dest is None: dest = Path(fname).parent\n    fname = str(fname)\n    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n    else: raise Exception(f'Unrecognized archive: {fname}')\nDownload the dataset if it is not present\nif not archive_path.exists():\n    api.dataset_download_cli(kaggle_dataset, path=archive_dir)\n    file_extract(fname=archive_path, dest=dataset_path)"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#inspect-the-dataset",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#inspect-the-dataset",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Inspect the Dataset",
    "text": "Inspect the Dataset\nWe can start inspecting the dataset once it finishes downloading.\nInspect the dataset path\npd.DataFrame(list(dataset_path.ls()))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-classification-512p-no-gesture-150k/hagrid-classification-512p-no-gesture-150k\n\n\n\n\n\n\nGet image file paths\nimg_dir = dataset_path/dataset_name\nfiles = get_image_files(img_dir)\nlen(files)\n154816\nInspect files\nThe dataset indicates the image class in the parent folder names.\npd.DataFrame([files[0], files[-1]])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-classification-512p-no-gesture-150k/hagrid-classification-512p-no-gesture-150k/stop/3ac51cf4-cd81-4803-a608-76a55b36df26.jpeg\n\n\n\n\n1\n\n\n/home/innom-dt/.fastai/archive/../data/hagrid-classification-512p-no-gesture-150k/hagrid-classification-512p-no-gesture-150k/two_up/d5a0a30d-92aa-4a7c-9621-1fed0e8f0b66.jpeg\n\n\n\n\n\n\nInspect class folder names\nThere are 19 class folders, and the dataset does not predefine a training-validation split.\nfolder_names = [path.name for path in Path(img_dir).ls()]\nif 'models' in folder_names: \n    os.removedirs(img_dir/'models')\n    folder_names.remove('models')\nfolder_names.sort()\nprint(f\"Num classes: {len(folder_names)}\")\npd.DataFrame(folder_names)\nNum classes: 19\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\ncall\n\n\n\n\n1\n\n\ndislike\n\n\n\n\n2\n\n\nfist\n\n\n\n\n3\n\n\nfour\n\n\n\n\n4\n\n\nlike\n\n\n\n\n5\n\n\nmute\n\n\n\n\n6\n\n\nno_gesture\n\n\n\n\n7\n\n\nok\n\n\n\n\n8\n\n\none\n\n\n\n\n9\n\n\npalm\n\n\n\n\n10\n\n\npeace\n\n\n\n\n11\n\n\npeace_inverted\n\n\n\n\n12\n\n\nrock\n\n\n\n\n13\n\n\nstop\n\n\n\n\n14\n\n\nstop_inverted\n\n\n\n\n15\n\n\nthree\n\n\n\n\n16\n\n\nthree2\n\n\n\n\n17\n\n\ntwo_up\n\n\n\n\n18\n\n\ntwo_up_inverted\n\n\n\n\n\n\n\nInspect one of the training images\nimport PIL\nimg = PIL.Image.open(files[0])\nprint(f\"Class: {files[0].parent.name}\")\nprint(f\"Image Dims: {img.shape}\")\nimg\n    Class: stop\n    Image Dims: (512, 512)"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#define-dataloaders",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#define-dataloaders",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Define DataLoaders",
    "text": "Define DataLoaders\nNext, we need to define the Transforms for the DataLoaders object.\nDefine target input dimensions\nThe Unity project will take input from a webcam, which will likely not have a square aspect ratio. We can still train the models with a square aspect ratio, and training at 256x256 (65,536 pixels) is more efficient than training at 384x216 (82,944 pixels) for a 16:9 aspect ratio.\nThe ResNet and ConvNeXt models handle arbitrary input dimensions well. However, we must export some models like MobileViT with the exact input dimensions used for inference.\nsize_1_1 = (256, 256)\nsize_16_9 = (216, 384)\nsize = size_1_1\nDefine Transforms\nWe can leave most of the options in aug_transforms at their default values. The HaGRID dataset is diverse enough that we don’t need to add too much augmentation. However, we will disable the max_rotate option as orientation is relevant for gesture recognition.\nitem_tfms = [Resize(size, method=ResizeMethod.Pad, pad_mode=PadMode.Border)]\n\nbatch_tfms = [\n    *aug_transforms(\n        size=size, \n        mult=1.0,\n        do_flip=True,\n        flip_vert=False,\n        max_rotate=0.0,\n        min_zoom=1.0,\n        max_zoom=1.1,\n        max_lighting=0.2,\n        max_warp=0.2, \n        p_affine=0.75,\n        pad_mode=PadMode.Border)\n]\nNote: The fastai library automatically normalizes input for timm models as of version 2.7.5.\nDefine batch size\nbs = 32\nDefine DataLoaders object\nWe can use the from_folder method to instantiate the DataLoaders object.\ndls = ImageDataLoaders.from_folder(\n    path=img_dir, \n    valid_pct=0.2, \n    bs=bs, \n    item_tfms=item_tfms, \n    batch_tfms=batch_tfms\n)\nVerify DataLoaders object\nLet’s verify the DataLoaders object works as expected before training a model.\ndls.train.show_batch()"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#finetune-the-model",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#finetune-the-model",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Finetune the Model",
    "text": "Finetune the Model\nNow we can define the Learner object and finetune the selected model.\nDefine metrics\nmetrics = [error_rate, accuracy]\nDefine Learner object\nlearn = vision_learner(dls, model_name, metrics=metrics)\nDefine model checkpoint file path\ncheckpoint_path = Path(f\"{dataset_path.name}-{model_name}.pth\")\ncheckpoint_path\nPath('hagrid-classification-512p-no-gesture-150k-convnext_nano.pth')\nLoad existing checkpoint (Optional)\n# if checkpoint_path.exists():\n#     print(\"Loading checkpoint...\")\n#     checkpoint = torch.load(checkpoint_path)\n#     learn.model.load_state_dict(checkpoint)\nFind learning rate\nsuggested_lrs = learn.lr_find()\n\n\n\n\n\nDefine learning rate\nWe can use a slightly higher learning rate than the learning rate finder recommends to speed up training.\nlr = suggested_lrs.valley*3\nlr\n0.003606793354265392\nDefine number of epochs\nWe should not need to train for more than a few epochs.\nepochs = 4\nDefine callbacks\nTraining with mixed precision can significantly reduce training time on modern GPUs. However, the older GPUs on the free tiers for Google Colab and Kaggle will likely not benefit from it.\ncbs = [MixedPrecision()]\nFine tune model\nlearn.fine_tune(epochs, base_lr=lr, cbs=cbs)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.372442\n\n\n0.189683\n\n\n0.061299\n\n\n0.938701\n\n\n04:22\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.096614\n\n\n0.054749\n\n\n0.017214\n\n\n0.982786\n\n\n05:29\n\n\n\n\n1\n\n\n0.048555\n\n\n0.033598\n\n\n0.010012\n\n\n0.989988\n\n\n05:34\n\n\n\n\n2\n\n\n0.030899\n\n\n0.018264\n\n\n0.005555\n\n\n0.994445\n\n\n05:32\n\n\n\n\n3\n\n\n0.018128\n\n\n0.015447\n\n\n0.004877\n\n\n0.995123\n\n\n05:30\n\n\n\n\n\n\nSave model checkpoint\ntorch.save(learn.model.state_dict(), checkpoint_path)\nMake predictions for a group of images\nlearn.show_results()\n\n\n\n\n\nDefine an Interpretation object\nOnce the model finishes training, we can create an Interpretation object to see where it struggles. An Interpretation object is also helpful to see if there are any mislabeled/low-quality training images.\ninterp = Interpretation.from_learner(learn)\nPlot top losses\ninterp.plot_top_losses(k=9, figsize=(15,10))"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#clean-dataset-optional",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#clean-dataset-optional",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Clean Dataset (Optional)",
    "text": "Clean Dataset (Optional)\nI spent some time cleaning the modified HaGRID datasets, but there are likely some training images that should still be moved or deleted. If the model accuracy is insufficient, consider using the ImageClassifierCleaner widget to edit the dataset.\nImport fastai ImageClassifierCleaner\n# from fastai.vision.widgets import ImageClassifierCleaner\nMark images to delete or move\n# cleaner = ImageClassifierCleaner(learn)\n# cleaner\nInspect samples to change\n# cleaner.change()\nMove selected samples to target class folder\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), img_dir/cat)\nInspect samples to delete\n# cleaner.delete()\nDelete selected samples\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\nNote: Restart the notebook and finetune the trained model after cleaning the dataset."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#test-the-model",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#test-the-model",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Test the Model",
    "text": "Test the Model\nNext, we will test the model on a single image.\nSelect a test image\nimport PIL\ntest_file = files[0]\ntest_file.parent.name, test_file.name\n('stop', '3ac51cf4-cd81-4803-a608-76a55b36df26.jpeg')\ntest_img = PIL.Image.open(test_file)\nprint(f\"Source image size: {test_img.size}\")\ntest_img\nSource image size: (512, 512)\n\n\n\n\n\nSet target size\nWe will test the model with the target inference resolution to verify it performs as desired.\nsize = size_16_9\nsize\n(216, 384)\nOption 1: Pad to target input dims\npadded_img = PIL.ImageOps.pad(test_img, [size[1], size[0]], method=PIL.Image.Resampling.BICUBIC)\nprint(f\"Padded image size: {padded_img.size}\")\npadded_img\nPadded image size: (384, 216)\n\n\n\n\n\nOption 2: Stretch to target input dims\nstretched_img = test_img.resize([size[1], size[0]])\nprint(f\"Stretched image size: {stretched_img.size}\")\nstretched_img\nStretched image size: (384, 216)\n\n\n\n\n\nMake a prediction on padded image using a fastai.vision.core.PILImage\npred = learn.predict(PILImage(padded_img))\npred, pred[2].max()\n    (('stop',\n      TensorBase(13),\n      TensorBase([5.6481e-08, 2.9167e-09, 1.7013e-08, 1.0619e-07, 7.0342e-09, 4.3362e-10,\n              1.9056e-08, 7.8481e-07, 1.9450e-08, 5.0064e-06, 3.8272e-09, 8.2519e-11,\n              4.3597e-08, 9.9999e-01, 2.3984e-08, 1.0935e-08, 2.4180e-09, 2.1497e-08,\n              5.9654e-10])),\n     TensorBase(1.0000))\nMake a prediction on stretched image using a fastai.vision.core.PILImage\npred = learn.predict(PILImage(stretched_img))\npred, pred[2].max()\n(('stop',\n  TensorBase(13),\n  TensorBase([1.3940e-06, 4.6373e-07, 1.3970e-04, 6.4621e-07, 6.8510e-08, 9.9468e-09,\n          1.1748e-07, 1.3881e-06, 1.1672e-06, 3.1076e-04, 9.0491e-09, 7.7261e-10,\n          8.4245e-08, 9.9954e-01, 1.7898e-07, 2.6569e-08, 3.4712e-08, 2.2750e-08,\n          6.5716e-09])),\n TensorBase(0.9995))\nWhen we are satisfied with the model, we can start preparing for implementing it in TensorFlow.js. We will need to apply some of the preprocessing and post-processing that fastai applies automatically.\nInspect the after_item pipeline\nWe do not need to worry about padding the input image as both the ResNet and ConvNeXt models handle arbitrary input dimensions.\nlearn.dls.after_item\nPipeline: Resize -- {'size': (256, 256), 'method': 'pad', 'pad_mode': 'border', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nInspect the after_batch pipeline\nThe after_batch pipeline first scales the image color channel values from [0,255] to [0,1]. We will need to do the same for the TensorFlow.js plugin. We will also need to normalize the input image with the relevant normalization stats.\nlearn.dls.after_batch\nPipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Flip -- {'size': (256, 256), 'mode': 'bilinear', 'pad_mode': 'border', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -&gt; Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} -&gt; Normalize -- {'mean': tensor([[[[0.4850]],\n\n         [[0.4560]],\n\n         [[0.4060]]]], device='cuda:0'), 'std': tensor([[[[0.2290]],\n\n         [[0.2240]],\n\n         [[0.2250]]]], device='cuda:0'), 'axes': (0, 2, 3)}\nConvert image to tensor\nWe’ll first prepare the input image by converting it to a tensor, batching it, and moving it to the GPU.\nimg_tensor = tensor(padded_img).permute(2, 0, 1).float().unsqueeze(dim=0).cuda()\nimg_tensor.shape, img_tensor\n    (torch.Size([1, 3, 216, 384]),\n     tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               ...,\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.]],\n     \n              [[0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               ...,\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.]],\n     \n              [[0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               ...,\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.],\n               [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'))\nScale tensor values\nWe’ll then scale the values from [0, 255] to [0, 1].\nscaled_tensor = img_tensor / 255\nNext, we’ll prepare the normalization values.\nPrepare mean values\nmean_tensor = tensor(mean).view(1,1,-1).permute(2, 0, 1).unsqueeze(dim=0).cuda()\nmean_tensor.shape, mean_tensor\n    (torch.Size([1, 3, 1, 1]),\n     tensor([[[[0.4850]],\n     \n              [[0.4560]],\n     \n              [[0.4060]]]], device='cuda:0'))\nPrepare std_dev values\nstd_tensor = tensor(std).view(1,1,-1).permute(2, 0, 1).unsqueeze(dim=0).cuda()\nstd_tensor.shape, std_tensor\n    (torch.Size([1, 3, 1, 1]),\n     tensor([[[[0.2290]],\n     \n              [[0.2240]],\n     \n              [[0.2250]]]], device='cuda:0'))\nWe can integrate the normalization preprocessing step and the Softmax postprocessing function into the exported model by creating a custom forward method. This way, we don’t need to store the normalization stats for each model in a separate file.\nCreate a backup of the default model forward function\nWe first need to create a backup of the current forward method.\norigin_forward = learn.model.forward\nDefine custom forward function for exporting the model\nThe custom forward method will normalize the input tensor, feed it to the original forward method and pass the raw output through a Softmax function.\ndef forward_export(self, x):\n    \n    # Normalize input\n    normalized_tensor = (x - mean_tensor) / std_tensor\n    \n    # Get raw model output\n    preds = origin_forward(normalized_tensor)\n    \n    # Apply SoftMax function\n    return torch.nn.functional.softmax(preds, dim=1)\nNote: This custom forward method might also be a suitable spot to resize input images for models like MobileViT with fixed input dimensions.\nAdd custom forward function to model\nWe then add the custom forward method to the model using monkey patching.\nlearn.model.forward_export = forward_export.__get__(learn.model)\nTest custom forward function\nNow we can test the custom forward method to verify it returns the expected prediction.\nwith torch.no_grad():\n    preds = learn.model.forward_export(scaled_tensor)\npreds.cpu().argmax(), preds.cpu()\n    (TensorBase(13),\n     TensorBase([[1.8443e-08, 1.8163e-09, 1.3866e-08, 2.6368e-08, 1.0109e-09, 6.3904e-10,\n              2.6506e-09, 7.7717e-09, 3.7365e-10, 1.0260e-06, 9.1487e-11, 4.4600e-11,\n              4.3488e-10, 1.0000e+00, 1.8129e-08, 3.4815e-09, 3.7684e-10, 1.1454e-08,\n              1.1459e-10]]))\nGet the class labels\nlearn.dls.vocab\n    ['call', 'dislike', 'fist', 'four', 'like', 'mute', 'no_gesture', 'ok', 'one', 'palm', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted', 'three', 'three2', 'two_up', 'two_up_inverted']\nGet the predicted class label\nlearn.dls.vocab[preds.cpu().argmax()]\n    'stop'\nReplace model forward function with custom function\nLastly, we need to replace the current forward method with the custom one before exporting the model to ONNX.\nlearn.model.forward = learn.model.forward_export"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#export-the-model",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#export-the-model",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Export the Model",
    "text": "Export the Model\nNow we can begin the process of converting the PyTorch model to TensorFlow.js.\nDefine ONNX opset version\nopset = 15\nDefine ONNX file name\nonnx_file_name = f\"{dataset_path.name}-{learn.arch}-opset{opset}.onnx\"\nprint(onnx_file_name)\nhagrid-classification-512p-no-gesture-150k-convnext_nano-opset15.onnx\nExport trained model to ONNX\nWe will also unlock the input dimensions for the model to give ourselves more flexibility in Unity. This setting will not matter for models like MobileViT that require exact input dimensions.\ntorch.onnx.export(learn.model.cpu(),\n                  batched_tensor,\n                  onnx_file_name,\n                  export_params=True,\n                  opset_version=opset,\n                  do_constant_folding=True,\n                  input_names = ['input'],\n                  output_names = ['output'],\n                  dynamic_axes={'input': {2 : 'height', 3 : 'width'}}\n                 )\nImport dependencies for TensorFlow.js conversion\nimport onnx\nfrom scc4onnx import order_conversion\nfrom onnxsim import simplify\nfrom onnx_tf.backend import prepare\nLoad ONNX model\nonnx_model = onnx.load(onnx_file_name)\nInspect model input\nLooking at the input layer for the ONNX model, we can see that it is channels-first. However, the channels-last format is more straightforward for preparing model input in JavaScript. We can switch the model input to channels-last format using the scc4onnx package.\nonnx_model.graph.input[0]\nname: \"input\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n      }\n      dim {\n        dim_value: 3\n      }\n      dim {\n        dim_param: \"height\"\n      }\n      dim {\n        dim_param: \"width\"\n      }\n    }\n  }\n}\nGet input name\ninput_name = onnx_model.graph.input[0].name\ninput_name\n'input'\nConvert model input to channels-last format\nonnx_model = order_conversion(\n    onnx_graph=onnx_model,\n    input_op_names_and_order_dims={f\"{input_name}\": [0,2,3,1]},\n    non_verbose=True\n)\nInspect updated model input\nIf we look at the input layer again, we can see it is now in channels-last format.\nonnx_model.graph.input[0]\nname: \"input\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n      }\n      dim {\n        dim_param: \"height\"\n      }\n      dim {\n        dim_param: \"width\"\n      }\n      dim {\n        dim_value: 3\n      }\n    }\n  }\n}\nSimplify ONNX model\nThe ONNX models generated by PyTorch are not always the most concise. We can use the onnx-simplifier package to tidy up the exported model. This step is entirely optional.\nonnx_model, check = simplify(onnx_model)\ncheck\nTrue\nPrepare ONNX model for Tensorflow Backend\nNext, we need to convert the ONNX model to an internal representation of the computational graph.\ntf_rep = prepare(onnx_model)\nDefine path for TensorFlow saved model directory\ntf_model_dir = f\"./{onnx_file_name.split('.')[0]}\"\ntf_model_dir\n'./hagrid-classification-512p-no-gesture-150k-convnext_nano-opset15'\nWe can now export the internal representation to a Tensorflow proto file.\nExport backend representation to a Tensorflow proto file\ntf_rep.export_graph(tf_model_dir)\nDefine directory path to store tfjs model files\ntfjs_model_dir = f\"{tf_model_dir}-tfjs-uint8\"\ntfjs_model_dir\n'./hagrid-classification-512p-no-gesture-150k-convnext_nano-opset15-tfjs-uint8'\nDefine arguments for tfjs converter script\nThe TensorFlow.js conversion script provides a few quantization options. These can significantly reduce the model file size. The file size matters since users download the models when loading the web demo. However, using the quantization options on small models like MobileNet can hurt accuracy.\nfrom IPython.display import Markdown, display\ntfjs_convert_command = f\"\"\"tensorflowjs_converter\n                 --input_format=tf_saved_model \n                 --output_format=tfjs_graph_model \n                 --signature_name=serving_default \n                 --saved_model_tags=serve \n                 \"{tf_model_dir}\" \n                 \"{tfjs_model_dir}\"\n                 \"--quantize_uint8\"\n                 \"\"\"\ntfjs_convert_command = \" \".join(tfjs_convert_command.split())\ndisplay(Markdown(f\"```bash\\n{tfjs_convert_command}\\n```\"))\ntensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --signature_name=serving_default --saved_model_tags=serve \"./hagrid-classification-512p-no-gesture-150k-convnext_nano-opset15\" \"./hagrid-classification-512p-no-gesture-150k-convnext_nano-opset15-tfjs-uint8\" \"--quantize_uint8\"\nExport SavedModel to TFJS format\nThe conversion script will create a new folder containing a model.json file describing the model architecture and some BIN files storing the model weights.\nprint(\"Exporting TensorFlow SavedModel to TensorFlow.js Graph model...\")\nconversion_result = %sx $tfjs_convert_command\nprint(\"\\n\".join(conversion_result))"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#export-inference-data",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#export-inference-data",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Export Inference Data",
    "text": "Export Inference Data\nWe can export the list of class labels to a JSON file and import it into the Unity project. That way, we don’t have to hardcode them, and we can easily swap in models trained on different datasets.\nimport json\nExport class labels\nlabels = list(learn.dls.vocab)\nlabels\n['call',\n 'dislike',\n 'fist',\n 'four',\n 'like',\n 'mute',\n 'no_gesture',\n 'ok',\n 'one',\n 'palm',\n 'peace',\n 'peace_inverted',\n 'rock',\n 'stop',\n 'stop_inverted',\n 'three',\n 'three2',\n 'two_up',\n 'two_up_inverted']\nclass_labels = {\"classes\": labels}\nclass_labels_file_name = f\"{dataset_path.name}-classes.json\"\n\nwith open(class_labels_file_name, \"w\") as write_file:\n    json.dump(class_labels, write_file)"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-1/index.html#summary",
    "href": "posts/tensorflow-js-unity-tutorial/part-1/index.html#summary",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1",
    "section": "Summary",
    "text": "Summary\nIn this post, we finetuned an image classifier from the timm library using fastai and exported it to TensorFlow.js. We started by installing and importing the necessary dependencies, then selected a model to use and downloaded a dataset to train it. After inspecting the dataset, we defined data loaders to use for training. Finally, we finetuned and exported the model. We also demonstrated how to clean the dataset to improve training. With this completed, we are ready to move on to Part 2 of this tutorial series, where we will create a TensorFlow.js plugin for the Unity game engine.\nNext: In-Browser Hand Gesture Recognition for Unity with Fastai and TensorFlow.js Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "",
    "text": "Overview\nSet Up Unity Hub\nInstall Unity Editor\nCreate New Project\nImport Assets\nCreate JavaScript Utility File\nCreate jslib Plugin\nImport Plugin Functions\nCreate Image Classifier Script\nSet up Unity Scene\nTest in Browser\nSummary"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#tutorial-links",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#tutorial-links",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a hand gesture classifier using fastai and export it to TensorFlow.js.\nPart 2: Create a TensorFlow.js plugin for the Unity game engine.\nPart 3: Build a Unity project as a WebGL application and host it using GitHub Pages.\nGitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#overview",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#overview",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Overview",
    "text": "Overview\nIn Part 2 of this tutorial series, we first install Unity Hub, download a Unity Editor, and create a new Unity project. Next, we will import any required assets and add a JavaScript utility file and jslib plugin. We will then create an image classifier script to use our trained model in Unity and set up the Unity scene to use it. Finally, we will test our project in a browser. By the end of this post, you will have a working TensorFlow.js plugin for Unity that you can use to recognize hand gestures in your projects."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#set-up-unity-hub",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#set-up-unity-hub",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Set Up Unity Hub",
    "text": "Set Up Unity Hub\nBefore creating a project, we must install Unity Hub, create a UnityID account, and activate a (free) Unity license. The beginner Unity tutorial at the link below covers all these steps and how to create a simple flappy bird-style game.\n\nMake Your First Video Game - Ultimate Beginner Unity3D Tutorial\n\nThe link opens to the part covering how to install Unity for the first time, but I recommend watching the entire tutorial for those new to Unity."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#install-unity-editor",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#install-unity-editor",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Install Unity Editor",
    "text": "Install Unity Editor\nAfter installing Unity Hub and activating a license, we must add a version of the Unity Editor. We will use the latest 2022.1+ release. The tutorial uses 2022.1.19, which you can install directly from the link below.\n\nUnity download archive: Unity 2022.1.19\n\nOpen Unity Hub and select the Installs section in the side panel. Then click the Install Editor button in the upper right-hand corner.\n\n\n\n\n\nClick the Install button next to the latest 2022.1 version under Other Versions.\n\n\n\n\n\nScroll down the Add modules selection menu and click the check box next to WebGL Build Support. Click the Install button after selecting all desired modules.\n\n\n\n\n\nUnity Hub will begin downloading and installing the selected editor version."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-new-project",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-new-project",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Create New Project",
    "text": "Create New Project\nGo back to the Projects section after the editor finishes installing and click New Project.\n\n\n\n\n\nSelect the target editor version from the Editor Version dropdown menu.\n\n\n\n\n\nSelect the 2D Core template.\n\n\n\n\n\nPick a name for the project and a location for the project folder.\n\n\n\n\n\nFinally, click Create Project in the lower right-hand corner.\n\n\n\n\n\nThe Unity Editor may take a few minutes to open the new project."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#import-assets",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#import-assets",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Import Assets",
    "text": "Import Assets\nInside the editor window, we will first import any class labels, test images, and TensorFlow.js models into the Assets folder.\nImport class labels\nRight-click a space in the Assets section and select Create → Folder from the popup menu.\n\n\n\n\n\nName the new folder ClassLabels.\n\n\n\n\n\nDrag and drop any JSON class label files from the operating system’s file explorer into the ClassLabels folder. Sample files are available in the Google Drive link below.\n\nGoogle Drive: ClassLabels\n\n\n\n\n\n\nImport sample images\nNext, create an Images folder and drop any test images into it. Sample files are available in the Google Drive link below.\nGoogle Drive: Images\n\n\n\n\n\nUnity automatically imports images as a Sprite (2D and UI) texture type. We don’t need to change it for our purposes.\nImport TFJS models\nWe need to place the TensorFlow.js models in a StreamingAssets folder to include them in the final application. Create a new folder named StreamingAssets. Let’s put the folders for each model in a new subfolder called TFJSModels to keep things organized.\n\nGoogle Drive: TFJSModels\n\n\n\n\n\n\nInside each subfolder are the JSON and BIN files for a TensorFlow.js model."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-javascript-utility-file",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-javascript-utility-file",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Create JavaScript Utility File",
    "text": "Create JavaScript Utility File\nBefore creating the jslib plugin, we’ll add a JavaScript file with a utility function to the StreamingAssets folder. Open the StreamingAssets folder in a code editor and create a new JavaScript file called utils.js.\n\n\n\n\n\nDefine function to perform inference asynchronously\nThis function will take in the model, input data, and shape for the input tensor as input.\nWe can use the tf. tidy() method included with TensorFlow.js for automatic memory cleanup. TensorFlow.js also provides a built-in ArgMax to extract the predicted class index.\n// Perform inference with the provided model and input data\nasync function PerformInferenceAsync(model, float32Data, shape) {\n\n    const outputData = tf.tidy(() =&gt; {\n        // Initialize the input tensor\n        const input_tensor = tf.tensor(float32Data, shape, 'float32');\n        // Make a prediction.\n        return model.predict(input_tensor);\n    });\n    // Pass raw output through a SoftMax function\n    let results = await outputData.data();\n    // Extract the predicted class from the model output\n    let index = await tf.argMax(results).data();\n    return [index, results[index]];\n}"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-jslib-plugin",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-jslib-plugin",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Create jslib Plugin",
    "text": "Create jslib Plugin\nNow we can create the jslib plugin to use the TensorFlow.js models. In the Assets section, create a new folder called Plugins. Then create a subfolder called WebGL.\n\n\n\n\n\nOpen the WebGL folder in a code editor and create a new file called WebGLPlugin.jslib. For formatting purposes, we can set the jslib file as JavaScript.\n\n\n\n\n\nBasic plugin format\nHere is the basic format for the jslib plugin. We’ll place any functions we want accessible from Unity inside the plugin variable.\n// Define plugin functions\nlet plugin = {\n    \n}\n\n// Add plugin functions\nmergeInto(LibraryManager.library, plugin);\nDefine function to add external JavaScript dependencies\nWe first need to add the script tag for the TensorFlow.js npm package and the utils.js script in the StreamingAssets folder.\n// Add additional JavaScript dependencies to the html page\nGetExternalJS: function () {\n\n    // Add base TensorFlow.js dependencies\n    let tfjs_script = document.createElement(\"script\");\n    tfjs_script.src = \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js\";\n    document.head.appendChild(tfjs_script);\n\n    // Add custom utility functions\n    let script = document.createElement(\"script\");\n    script.src = \"./StreamingAssets/utils.js\";\n    document.head.appendChild(script);\n},\nDefine function to set the TensorFlow.js compute backend\nNext, we’ll add a function to update the compute backend. At the time of writing, I only recommend using the WebGL backend. The base CPU backend is not suitable for real-time applications. The WASM backend enables multi-threaded CPU inference but does not have full operator support. Lastly, the WebGPU backend is not yet officially supported by browsers.\n// Set the TFJS inference backend\nSetTFJSBackend: function (backend) {\n  let backend_str = UTF8ToString(backend);\n  try {\n     tf.setBackend(backend_str).then(() =&gt; { });\n     console.log(`Successfully set ${backend_str} backend.`);\n  } catch (error) {\n     console.log(\"Error occurred. Falling back to WebGL backend.\");\n     tf.setBackend('webgl');\n  }\n},\nDefine function to Initialize a TensorFlow.js model\nWe’ll pass a file path for a model.json file and an array to store the predicted class index and the corresponding confidence score. We’ll be able to access the updated array values in Unity.\n// Load a TFJS model\nInitTFJSModel: async function (model_path, output_data) {\n\n    // Convert bytes to the text\n    let model_path_str = UTF8ToString(model_path);\n    // Load the TensorFlow.js model at the provided file path\n    this.model = await tf.loadGraphModel(model_path_str, { fromTFHub: false });\n\n    // Check the model input shape\n    const input_shape = this.model.inputs[0].shape;\n    console.log(`Input Shape: ${input_shape}`);\n\n    // Initialize an array to store the predicted class index and confidence score\n    this.prediction = new Float32Array(buffer, output_data, 2);\n},\nDefine function to perform inference with a input image\nLoading the model can take several seconds, depending on the user’s internet connection. We don’t want to perform inference until after it initializes.\nDue to the way Unity reads texture data from the GPU to the CPU, we need to read the image data from the bottom up to make the image correctly oriented.\nAfter populating the input array, we’ll wait for the asynchronous inference function to finish before updating the prediction array.\n// Perform inference with the provided image data\nPerformInference: function (image_data, size, width, height) {\n\n    // Only perform inference after loading a model\n    if (typeof this.model == 'undefined') {\n        console.log(\"Model not defined yet\");\n        return false;\n    }\n\n    // Initialize an array with the raw image data\n    const uintArray = new Uint8ClampedArray(buffer, image_data, size, width, height);\n\n    // Channels-last order\n    const [input_array] = new Array(new Array());\n\n    // Flip input image from Unity\n    for (let row = height - 1; row &gt;= 0; row--) {\n        let slice = uintArray.slice(row * width * 3, (row * width * 3) + (width * 3));\n        // Scale color values from [0,255] to [0,1]\n        for (let col = 0; col &lt; slice.length; col += 3) {\n            input_array.push(slice[col + 0]/255);\n            input_array.push(slice[col + 1]/255);\n            input_array.push(slice[col + 2]/255);\n        }\n    }\n\n    // Initialize the input array with the preprocessed input data\n    const float32Data = Float32Array.from(input_array);\n    const shape = [1, height, width, 3];\n\n    // Pass preprocessed input to the model\n    PerformInferenceAsync(this.model, float32Data, shape).then(output =&gt; {\n\n        // Store class index and confidence value\n        this.prediction[0] = output[0];\n        this.prediction[1] = output[1];\n    })\n    return true;\n},\nThat’s it for the plugin code. Next, we need to import the functions in Unity."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#import-plugin-functions",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#import-plugin-functions",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Import Plugin Functions",
    "text": "Import Plugin Functions\nWe’ll import the plugin functions in a dedicated C# script and make them accessible as a static class. Right-click a space inside the WebGL folder and select Create → C# Script.\n\n\n\n\n\nName the new script WebGLPlugin.\n\n\n\n\n\nDefault script code\nBy default, C# scripts contain the following code. We don’t need any of it in this case.\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class ImageClassifier : MonoBehaviour\n{\n    // Start is called before the first frame update\n    void Start()\n    {\n        \n    }\n\n    // Update is called once per frame\n    void Update()\n    {\n        \n    }\n}\nWe need the System.Runtime.InteropServices namespace to handle interactions with the jslib plugin.\nusing System.Runtime.InteropServices;\n\n/// &lt;summary&gt;\n/// Class with JavaScript plugin functions for WebGL.\n/// &lt;/summary&gt;\npublic static class WebGLPlugin\n{\n    // Import \"GetExternalJS\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void GetExternalJS();\n    // Import \"SetTFJSBackend\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void SetTFJSBackend(string backend);\n    // Import \"InitTFJSModel\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void InitTFJSModel(string model_path, float[] output_data, int output_size);\n    // Import \"PerformInference\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern bool PerformInference(byte[] image_data, int size, int width, int height);\n}"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-image-classifier-script",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#create-image-classifier-script",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Create Image Classifier Script",
    "text": "Create Image Classifier Script\nNow we can create the main script for the Unity project. This script will handle getting input images, sending them to the model, and using the returned predictions to update on-screen readouts. We’ll store it in a new Scripts folder and name it ImageClassifier.\n\n\n\n\n\nAdd required namespaces\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\nSystem.IO: Contains types that allow reading and writing to files and data streams, and types that provide basic file and directory support.\nUnityEngine.Networking: Provides access to the UnityWebRequest module to communicate with http services.\n\n\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing System;\nusing UnityEngine.UI;\nusing System.IO;\nusing UnityEngine.Networking;\nAdd code to create a list of available TFJS models\nWe can’t browse for available TFJS models while running the Unity application in the browser, so we need to create a list of model names and file paths to check at runtime. We can do this automatically within the Unity Editor.\nUnity provides an InitializeOnLoad attribute to run code in the Unity Editor without requiring action from the user. This attribute requires the UnityEditor namespace. We can only use this while in the Editor, so we need to wrap the code in Conditional compilation preprocessor directives. This code will go right below the namespaces.\nWe use the UNITY_EDITOR scripting symbol to check whether we are in the Unity Editor. When we are in the Editor, it returns true, and the code executes.\n#if UNITY_EDITOR\nusing UnityEditor;\n\n[InitializeOnLoad]\npublic class Startup\n{\n    // A helper class that stores the name and file path for a TensorFlow.js model\n    [System.Serializable]\n    class ModelData\n    {\n        public string name;\n        public string path;\n\n        public ModelData(string name, string path)\n        {\n            this.name = name;\n            this.path = path;\n        }\n    }\n\n    // A helper class that stores a list of TensorFlow.js model names and file paths\n    [System.Serializable]\n    class ModelList\n    {\n        public List&lt;ModelData&gt; models;\n\n        public ModelList(List&lt;ModelData&gt; models)\n        {\n            this.models = models;\n        }\n    }\n\n    static Startup()\n    {\n        string tfjsModelsDir = \"TFJSModels\";\n        List&lt;ModelData&gt; models = new List&lt;ModelData&gt;();\n\n        Debug.Log(\"Available models\");\n        // Get the paths for each model folder\n        foreach (string dir in Directory.GetDirectories($\"{Application.streamingAssetsPath}/{tfjsModelsDir}\"))\n        {\n            string dirStr = dir.Replace(\"\\\\\", \"/\");\n            // Extract the model folder name\n            string[] splits = dirStr.Split('/');\n            string modelName = splits[splits.Length - 1];\n\n            // Get the paths for the model.json file for each model\n            foreach (string file in Directory.GetFiles(dirStr))\n            {\n                if (file.EndsWith(\"model.json\"))\n                {\n                    string fileStr = file.Replace(\"\\\\\", \"/\").Replace(Application.streamingAssetsPath, \"\");\n                    models.Add(new ModelData(modelName, fileStr));\n                }\n            }\n        }\n\n        ModelList modelList = new ModelList(models);\n        // Format the list of available models as a string in JSON format\n        string json = JsonUtility.ToJson(modelList);\n        Debug.Log($\"Model List JSON: {json}\");\n        // Write the list of available TensorFlow.js models to a JSON file\n        using StreamWriter writer = new StreamWriter($\"{Application.streamingAssetsPath}/models.json\");\n        writer.Write(json);\n    }\n}\n#endif\nThis code runs after opening the project in the Unity Editor or saving changes to this script. We can verify the code works by saving the script and going to the StreamingAssets folder in the Editor. The models.json file should be present.\n\n\n\n\n\n\nDefine public variables\nWe’ll add the required public variables above the Start method. We will be able to access these variables in the Inspector tab. We can add Header attributes to organize the public variables in the Inspector tab and use Tooltip attributes to provide information about variables.\nDefine scene object variables\nFirst, we need a variable to access the screen object that displays either a test image or webcam input.\n[Header(\"Scene Objects\")]\n[Tooltip(\"The Screen object for the scene\")]\npublic Transform screen;\nDefine data processing variables\nWe can set the default target input resolution to 216 and use it to scale the source resolution while maintaining the original aspect ratio.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 216;\nDefine output processing variables\nWe pass in the JSON file containing the class labels as a TextAsset.\n[Header(\"Output Processing\")]\n[Tooltip(\"A json file containing the class labels\")]\npublic TextAsset classLabels;\n[Tooltip(\"Minimum confidence score for keeping predictions\")]\n[Range(0, 1f)]\npublic float minConfidence = 0.5f;\nDefine variables for debugging\nNext, we’ll add a Boolean variable to toggle printing debug messages to the console. These messages get printed to the console in the browser as well.\n[Header(\"Debugging\")]\n[Tooltip(\"Print debugging messages to the console\")]\npublic bool printDebugMessages = true;\nDefine webcam variables\nWe need to specify a desired resolution and framerate when using a webcam as input.\n[Header(\"Webcam\")]\n[Tooltip(\"Use a webcam as input\")]\npublic bool useWebcam = false;\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"The requested webcam framerate\")]\n[Range(0, 60)]\npublic int webcamFPS = 60;\nDefine variables for user interface\nWe’ll make a simple GUI that displays the predicted class, the current framerate, and controls for selecting webcam devices, models, and backends.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayPredictedClass = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.yellow;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available TFJS models\")]\npublic Dropdown modelDropdown;\n[Tooltip(\"The dropdown menu that lists available TFJS backends\")]\npublic Dropdown backendDropdown;\nDefine TensorFlow.js variables\nWe’ll store the path to the StreamingAssets sub-folder containing the TensorFlow.js models in a string variable.\n[Header(\"TFJS\")]\n[Tooltip(\"The name of the TFJS models folder\")]\npublic string tfjsModelsDir = \"TFJSModels\";\n\n\nDefine private variables\nWe’ll add the required private variables right below the public variables.\nDefine private webcam variables\nWe’ll keep a list of available webcam devices so users can switch between them. Unity renders webcam input to a WebcamTexture.\n// List of available webcam devices\nprivate WebCamDevice[] webcamDevices;\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n// The name of the current webcam  device\nprivate string currentWebcam;\nDefine input variables\nWe’ll update the dimensions and content of the screen object based on the test image or webcam.\n// The test image dimensions\nVector2Int imageDims;\n// The test image texture\nTexture imageTexture;\n// The current screen object dimensions\nVector2Int screenDims;\n// The model GPU input texture\nRenderTexture inputTextureGPU;\n// The model CPU input texture\nTexture2D inputTextureCPU;\nDefine variables for class labels\nWe need to create a little class that indicates the structure of the JSON content. Our JSON file only contains a single array of strings. We can store this array in a dedicated variable.\n// A class for reading in class labels from a JSON file\nclass ClassLabels { public string[] classes; }\n// The ordered list of class names\nprivate string[] classes;\nDefine variable to track whether a model is initialized\nWe’ll track whether a model is ready for inference so we can tell the user if a model is still loading.\n// Stores whether the TensorFlow.js model is ready for inference\nbool modelInitialized;\nDefine variables for tracking the framerate\nLastly, we need to define a couple of variables for the custom fps counter.\n// The current frame rate value\nprivate int fps = 0;\n// Controls when the frame rate value updates\nprivate float fpsTimer = 0f;\nDefine variables to store values for GUI dropdowns\nWe’ll update the lists of model paths and names at runtime using the models.json file in the StreamingAssets folder.\nAs mentioned previously, I only recommend using the WebGL backend at the time of writing, so we’ll hardcode that option here. In the future, we can check if each backend type is available in the jslib plugin and update the list in Unity.\n// File paths for the available TFJS models\nList&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available TFJS models\nList&lt;string&gt; modelNames = new List&lt;string&gt;();\n// Names of the available TFJS backends\nList&lt;string&gt; tfjsBackends = new List&lt;string&gt; { \"webgl\" };\nDefine variable to store the current inference output\nWe’ll share the memory for this array with the jslib plugin. We’ll update the values in the plugin and access them in this script.\n// Stores the latest model prediction and confidence score\nfloat[] output_data = new float[2];\nDefine variables for reading the models.json file\nWe need to create a couple of helper classes that indicate the structure of the models.json file.\n// A helper class to store the name and file path of a TensorFlow.js model\n[System.Serializable]\nclass ModelData { public string name; public string path; }\n// A helper class to store a read a list of available TensorFlow.js models from a JSON file\n[System.Serializable]\nclass ModelList { public List&lt;ModelData&gt; models; }\n\n\nDefine Initialization Methods\nWe first need to define some methods to initialize webcams, the screen object, any GUI dropdown menus, and the in-game camera.\nDefine method to initialize a webcam device\n/// &lt;summary&gt;\n/// Initialize the selected webcam device\n/// &lt;/summary&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the selected webcam device&lt;/param&gt;\nvoid InitializeWebcam(string deviceName)\n{\n    // Stop any webcams already playing\n    if (webcamTexture && webcamTexture.isPlaying) webcamTexture.Stop();\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(deviceName, webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the webcam\n    webcamTexture.Play();\n    // Check if webcam is playing\n    useWebcam = webcamTexture.isPlaying;\n    // Update toggle value\n    useWebcamToggle.SetIsOnWithoutNotify(useWebcam);\n\n    Debug.Log(useWebcam ? \"Webcam is playing\" : \"Webcam not playing, option disabled\");\n}\nDefine method to initialize the in-scene screen object\n/// &lt;summary&gt;\n/// Resize and position an in-scene screen object\n/// &lt;/summary&gt;\nvoid InitializeScreen()\n{\n    // Set the texture for the screen object\n    screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture = useWebcam ? webcamTexture : imageTexture;\n    // Set the screen dimensions\n    screenDims = useWebcam ? new Vector2Int(webcamTexture.width, webcamTexture.height) : imageDims;\n\n    // Flip the screen around the Y-Axis when using webcam\n    float yRotation = useWebcam ? 180f : 0f;\n    // Invert the scale value for the Z-Axis when using webcam\n    float zScale = useWebcam ? -1f : 1f;\n\n    // Set screen rotation\n    screen.rotation = Quaternion.Euler(0, yRotation, 0);\n    // Adjust the screen dimensions\n    screen.localScale = new Vector3(screenDims.x, screenDims.y, zScale);\n\n    // Adjust the screen position\n    screen.position = new Vector3(screenDims.x / 2, screenDims.y / 2, 1);\n}\nDefine method to switch TensorFlow.js models\n/// &lt;summary&gt;\n/// Load a TensorFlow.js model\n/// &lt;/summary&gt;\npublic void UpdateTFJSModel()\n{\n    // Load TensorFlow.js model in JavaScript plugin\n    WebGLPlugin.InitTFJSModel(modelPaths[modelDropdown.value], output_data, output_data.Length);\n}\nDefine method to read the list of available TensorFlow.js models\nWe can parse the raw JSON content from the models.json file using the JsonUtility.FromJson() method. We’ll then update the associated GUI dropdown with the available model names.\n/// &lt;summary&gt;\n/// Get the names and paths of the available TensorFlow.js models\n/// &lt;/summary&gt;\n/// &lt;param name=\"json\"&gt;&lt;/param&gt;\nvoid GetTFJSModels(string json)\n{\n    ModelList modelList = JsonUtility.FromJson&lt;ModelList&gt;(json);\n    foreach (ModelData model in modelList.models)\n    {\n        //Debug.Log($\"{model.name}: {model.path}\");\n        modelNames.Add(model.name);\n        string path = $\"{Application.streamingAssetsPath}{model.path}\";\n        modelPaths.Add(path);\n    }\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add TFJS model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n}\nDefine method to download the list of available TensorFlow.js models\nAt runtime, we must fetch the models.json file from the hosting server before parsing its content. Getting the file is an asynchronous operation, so we’ll use a coroutine. Once we have the file, we’ll pass it to the GetTFJSModels method.\n/// &lt;summary&gt;\n/// Download the JSON file with the available TFJS model information\n/// &lt;/summary&gt;\n/// &lt;param name=\"uri\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nIEnumerator GetRequest(string uri)\n{\n    using (UnityWebRequest webRequest = UnityWebRequest.Get(uri))\n    {\n        // Request and wait for the desired page.\n        yield return webRequest.SendWebRequest();\n\n        string[] pages = uri.Split('/');\n        int page = pages.Length - 1;\n\n        switch (webRequest.result)\n        {\n            case UnityWebRequest.Result.ConnectionError:\n            case UnityWebRequest.Result.DataProcessingError:\n                Debug.LogError(pages[page] + \": Error: \" + webRequest.error);\n                break;\n            case UnityWebRequest.Result.ProtocolError:\n                Debug.LogError(pages[page] + \": HTTP Error: \" + webRequest.error);\n                break;\n            case UnityWebRequest.Result.Success:\n                Debug.Log(pages[page] + \":\\nReceived: \" + webRequest.downloadHandler.text);\n\n                // Extract the available model names and file paths from the JSON string\n                GetTFJSModels(webRequest.downloadHandler.text);\n                // Initialize one of the available TensorFlow.js models\n                UpdateTFJSModel();\n                break;\n        }\n    }\n}\nDefine method to initialize GUI dropdown menu options\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nvoid InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach (WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Get the available TensorFlow.js models\n    string modelListPath = $\"{Application.streamingAssetsPath}/models.json\";\n    StartCoroutine(GetRequest(modelListPath));\n\n    // Remove default dropdown options\n    backendDropdown.ClearOptions();\n    // Add TFJS backend names to menu\n    backendDropdown.AddOptions(tfjsBackends);\n    // Select the first option in the dropdown\n    backendDropdown.SetValueWithoutNotify(0);\n}\nDefine method to initialize the in-scene camera object\n/// &lt;summary&gt;\n/// Resize and position the main camera based on an in-scene screen object\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of an in-scene screen object&lt;/param&gt;\nvoid InitializeCamera(Vector2Int screenDims, string cameraName = \"Main Camera\")\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject camera = GameObject.Find(cameraName);\n    // Adjust the camera position to account for updates to the screenDims\n    camera.transform.position = new Vector3(screenDims.x / 2, screenDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    camera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the screenDims\n    camera.GetComponent&lt;Camera&gt;().orthographicSize = screenDims.y / 2;\n}\n\n\nDefine Awake method\nWe’ll call the GetExternalJS function from the jslib plugin in the Awake() method.\n// Awake is called when the script instance is being loaded\nvoid Awake()\n{\n    WebGLPlugin.GetExternalJS();\n}\n\n\nDefine Start method\nThe Start method is called once before the first frame update, so we’ll perform any required setup steps here.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n    // Initialize list of class labels from JSON file\n    classes = JsonUtility.FromJson&lt;ClassLabels&gt;(classLabels.text).classes;\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n\n    // Update the current TensorFlow.js compute backend\n    WebGLPlugin.SetTFJSBackend(tfjsBackends[backendDropdown.value]);\n}\n\n\nDefine Processing Methods\nNext, we need to define a preprocessing method to calculate the input resolution.\nDefine method to calculate input resolution\n/// &lt;summary&gt;\n/// Scale the source image resolution to the target input dimensions\n/// while maintaing the source aspect ratio.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetDims\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nVector2Int CalculateInputDims(Vector2Int imageDims, int targetDim)\n{\n    // Clamp the minimum dimension value to 64px\n    targetDim = Mathf.Max(targetDim, 64);\n\n    Vector2Int inputDims = new Vector2Int();\n\n    // Calculate the input dimensions using the target minimum dimension\n    if (imageDims.x &gt;= imageDims.y)\n    {\n        inputDims[0] = (int)(imageDims.x / ((float)imageDims.y / (float)targetDim));\n        inputDims[1] = targetDim;\n    }\n    else\n    {\n        inputDims[0] = targetDim;\n        inputDims[1] = (int)(imageDims.y / ((float)imageDims.x / (float)targetDim));\n    }\n\n    return inputDims;\n}\n\n\nDefine Update method\nWe’ll place anything we want to run every frame in the Update method.\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n    // Scale the source image resolution\n    Vector2Int inputDims = CalculateInputDims(screenDims, targetDim);\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGB32);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGB24, false);\n    }\n\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputTextureGPU.width}x{inputTextureGPU.height}\");\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Download pixel data from GPU to CPU\n    RenderTexture.active = inputTextureGPU;\n    inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n    inputTextureCPU.Apply();\n\n    // Get the current input dimensions\n    int width = inputTextureCPU.width;\n    int height = inputTextureCPU.height;\n    int size = width * height * 3;\n\n    // Pass the input data to the plugin to perform inference\n    modelInitialized = WebGLPlugin.PerformInference(inputTextureCPU.GetRawTextureData(), size, width, height);\n\n    // Check if index is valid\n    if (printDebugMessages) Debug.Log(modelInitialized ? $\"Predicted Class: {classes[(int)output_data[0]]}\" : \"Not Initialized\");\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n}\n\n\nDefine GUI Methods\nWe need some methods to handle user interactions with the GUI and display the predicted class and current framerate.\nDefine method to update webcam usage from GUI\n/// &lt;summary&gt;\n/// This method is called when the value for the webcam toggle changes\n/// &lt;/summary&gt;\n/// &lt;param name=\"useWebcam\"&gt;&lt;/param&gt;\npublic void UpdateWebcamToggle(bool useWebcam)\n{\n    this.useWebcam = useWebcam;\n}\nDefine method to update webcam device from GUI\n/// &lt;summary&gt;\n/// The method is called when the selected value for the webcam dropdown changes\n/// &lt;/summary&gt;\npublic void UpdateWebcamDevice()\n{\n    currentWebcam = webcamDevices[webcamDropdown.value].name;\n    Debug.Log($\"Selected Webcam: {currentWebcam}\");\n    // Initialize webcam if it is not already playing\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n}\nDefine method to update the TensorFlow.js backend\n/// &lt;summary&gt;\n/// Update the TensorFlow.js compute backend\n/// &lt;/summary&gt;\npublic void UpdateTFJSBackend()\n{\n    WebGLPlugin.SetTFJSBackend(tfjsBackends[backendDropdown.value]);\n}\nDefine method to update the confidence threshold\n/// &lt;summary&gt;\n/// Update the minimum confidence score for keeping predictions\n/// &lt;/summary&gt;\n/// &lt;param name=\"slider\"&gt;&lt;/param&gt;\npublic void UpdateConfidenceThreshold(Slider slider)\n{\n    minConfidence = slider.value;\n}\nDefine OnGUI method\nWe’ll display the predicted class and current frame rate in the OnGUI method. We’ll show a different message while the model is still loading.\n// OnGUI is called for rendering and handling GUI events.\npublic void OnGUI()\n{\n    // Define styling information for GUI elements\n    GUIStyle style = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * (1f / (100f - fontScale)))\n    };\n    style.normal.textColor = textColor;\n\n    // Define screen spaces for GUI elements\n    Rect slot1 = new Rect(10, 10, 500, 500);\n    Rect slot2 = new Rect(10, style.fontSize * 1.5f, 500, 500);\n\n    // Verify predicted class index is valid\n    string labelText = $\"{classes[(int)output_data[0]]} {(output_data[1] * 100).ToString(\"0.##\")}%\";\n    if (output_data[1] &lt; minConfidence) labelText = \"None\";\n    string content = modelInitialized ? $\"Predicted Class: {labelText}\" : \"Loading Model...\";\n    if (displayPredictedClass) GUI.Label(slot1, new GUIContent(content), style);\n\n    // Update framerate value\n    if (Time.unscaledTime &gt; fpsTimer)\n    {\n        fps = (int)(1f / Time.unscaledDeltaTime);\n        fpsTimer = Time.unscaledTime + fpsRefreshRate;\n    }\n\n    // Adjust screen position when not showing predicted class\n    Rect fpsRect = displayPredictedClass ? slot2 : slot1;\n    if (displayFPS) GUI.Label(fpsRect, new GUIContent($\"FPS: {fps}\"), style);\n}\nThat’s it for the code."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#set-up-unity-scene",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Set up Unity Scene",
    "text": "Set up Unity Scene\nWe can, at last, start setting up our Unity scene. We need a screen to display the webcam feed, an empty object to attach the ImageClassifier script, a dropdown menu for selecting webcam devices, and a toggle to switch between the test image and a webcam feed.\nCreate Screen object\nRight-click a space in the Hierarchy tab and select 3D Object → Quad. We can name the new object Screen.\n\n\n\n\n\nNext, drag and drop a test image from the Assets → Images folder onto the Screen object in the Scene view. Note that the Screen looks a bit dim. We need to change the shader for the Screen’s Material so that it does not require an external light source.\n\n\n\n\n\nSelect the Screen in the Hierarchy tab and open the Shader dropdown menu in the Inspector tab. Type Unlit/Texture into the search box and press enter.\n\n\n\n\n\nCreate Inference Manager object\nRight-click a space in the Hierarchy tab and select Create Empty. Name the empty object InferenceManager.\n\n\n\n\n\nWith the InferenceManager object selected, drag the ImageClassifier script into the Inspector tab.\n\n\n\n\n\nNow we can assign the Screen and class labels file in the Inspector tab by dragging them into their respective fields.\n\n\n\n\n\nAdd GUI prefab\nWe still need to create the GUI toggle and dropdown menu. To save time, I made a Prefab that we can drop into the Scene.\n\nGoogle Drive: Prefabs folder\n\nDrag and drop the Prefabs folder into the Assets section.\n\n\n\n\n\nOpen the Prefabs folder and drag the Canvas prefab into the Hierarchy tab. We can see the GUI by switching to the Game view.\n\n\n\n\n\nConfigure Webcam Toggle On Value Changed function\nNext, we need to pair the WebcamToggle with the UpdateWebcamToggle function in the ImageClassifier script. Expand the Canvas object and select the WebcamToggle.\n\n\n\n\n\nClick and drag the InferenceManager into the On Value Changed field.\n\n\n\n\n\nOpen the No Function dropdown menu and select ImageClassifier → UpdateWebcamToggle.\n\n\n\n\n\nConfigure Webcam Dropdown On Value Changed function\nWe can follow the same steps to pair the WebcamDropdown with the UpdateWebcamDevice function in the ImageClassifier script.\n\n\n\n\n\nThis time select ImageClassifier → UpdateWebcamDevice.\n\n\n\n\n\nConfigure TFJSModelDropdown On Value Changed function\n\n\n\n\n\nConfigure TFJSBackendDropdown On Value Changed function\n\n\n\n\n\nConfigure ConfidenceThresholdSlider On Value Changed Event\n\n\n\n\n\nAssign GUI objects to Inference Manager\nWe can now assign the WebcamToggle and WebcamDropdown objects to their respective fields for the ImageClassifier script.\n\n\n\n\n\nAdd Event System\nBefore we can use the GUI, we need to add an Event System. Right-click a space in the Hierarchy tab and select UI → Event System."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#test-in-browser",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#test-in-browser",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Test in Browser",
    "text": "Test in Browser\nNow, we can build the project and test it in a web browser. In the Unity project, select File → Build Settings... in the top menu bar to open the Build Settings window.\n\n\n\n\n\nSelect WebGL from the list of platforms and click Switch Platform.\n\n\n\n\n\nUnity enables compression by default for WebGL builds, which GitHub Pages does not support. We can disable compression in the Player Settings. Click the Player Settings... button in the bottom-left corner of the Build Settings window.\n\n\n\n\n\nSelect Disabled from the Compression Format dropdown menu and close the Project Settings window.\n\n\n\n\n\nWe can test the WebGL build locally by clicking Build and Run in the Build Settings window.\n\n\n\n\n\nUnity will prompt us to select a folder to store the build files. Create a new folder called Build. Open the folder and click Select Folder to start the build process.\n\n\n\n\n\nUnity caps the framerate to the default target framerate for the platform. My desktop display maxes out at 60fps.\nTest ConvNeXt nano performance\nPerformance for the ConvNeXt nano model fluctuates in the mid to high 50s. That is significantly lower than native inference options like OpenVINO or DirectML but still usable. GPU utilization seems to max out around 34% on my desktop.\n\n\n\n\n\nTest ResNet18 performance\nThe smaller ResNet18 model reaches the display’s 60fps max refresh rate but likely also falls far short of native inference options."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-2/index.html#summary",
    "href": "posts/tensorflow-js-unity-tutorial/part-2/index.html#summary",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2",
    "section": "Summary",
    "text": "Summary\nIn this post, we installed Unity Hub, downloaded a Unity Editor, and created a new Unity project. We then imported the required assets and added a JavaScript utility file and jslib plugin. Next, we created an image classifier script to use our trained model in Unity and set up the Unity scene to use it. Finally, we tested our project in a browser. With this completed, we are ready to move on to Part 3 of this tutorial series, where we will host our Unity project as a live demo on GitHub Pages.\nPrevious: In-Browser Hand Gesture Recognition for Unity with Fastai and TensorFlow.js Pt. 1\nNext: In-Browser Hand Gesture Recognition for Unity with Fastai and TensorFlow.js Pt. 3\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "",
    "text": "Overview\nCreate GitHub Pages Repository\nAdd Unity Build Files\nTest Live Demo\nSummary"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#tutorial-links",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#tutorial-links",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Tutorial Links",
    "text": "Tutorial Links\n\nPart 1: Train a hand gesture classifier using fastai and export it to TensorFlow.js.\nPart 2: Create a TensorFlow.js plugin for the Unity game engine.\nPart 3: Build a Unity project as a WebGL application and host it using GitHub Pages.\nGitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#overview",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#overview",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Overview",
    "text": "Overview\nIn Part 3 of this tutorial series, we will create a GitHub Pages repository to host our Unity project as a live demo. We will start by creating a repository and adding the Unity build files. Then, we will test the live demo to verify it works properly. By the end of this post, you will have a live demo of your hand gesture recognition system that you can share with others."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#create-github-pages-repository",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#create-github-pages-repository",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Create GitHub Pages Repository",
    "text": "Create GitHub Pages Repository\nWe first need to create a new GitHub repository to store the WebGL build. We can do this on GitHub or locally using Git, GitHub Desktop, or another tool.\n\n\n\n\n\nOpen the Settings tab for the new repository on GitHub.\n\n\n\n\n\nOpen the Pages section. Switch the source branch to main and click the Save button to start the automated build process.\n\n\n\n\n\nWe can check the GitHub Pages build progress under the Actions tab for the repository.\n\n\n\n\n\nThe web page will be accessible once the pages build and deployment workflow completes. Although, we don’t have any web pages at the moment.\n\n\n\n\n\nGitHub will provide a URL for accessing the web demo in the GitHub Pages section once it finishes building."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#add-unity-build-files",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#add-unity-build-files",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Add Unity Build Files",
    "text": "Add Unity Build Files\nNow let’s add the WebGL build from the previous post. Open the Build folder from part 2.\n\n\n\n\n\nCopy the folder content to the repository for the GitHub Pages demo.\n\n\n\n\n\nWe can push the local changes to GitHub, which will automatically trigger the pages build and deployment workflow."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#test-live-demo",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#test-live-demo",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Test Live Demo",
    "text": "Test Live Demo\nWe can test the web demo at the URL provided by GitHub once the build workflow completes."
  },
  {
    "objectID": "posts/tensorflow-js-unity-tutorial/part-3/index.html#summary",
    "href": "posts/tensorflow-js-unity-tutorial/part-3/index.html#summary",
    "title": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3",
    "section": "Summary",
    "text": "Summary\nIn this tutorial series, we demonstrated how to use fastai and TensorFlow.js to create a hand gesture recognition system in Unity. In Part 1, we trained a hand gesture classifier using fastai and exported it to TensorFlow.js. In Part 2, we created a TensorFlow.js plugin for the Unity game engine. Finally, in this post, we hosted the Unity project as a live demo on GitHub Pages. With this tutorial series, you now have the tools and knowledge to use TensorFlow.js models in Unity applications.\nPrevious: In-Browser Hand Gesture Recognition for Unity with Fastai and TensorFlow.js Pt. 2\nProject Resources: GitHub Repository"
  },
  {
    "objectID": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html",
    "href": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html",
    "title": "How to Convert TensorFlow Models to ONNX with tf2onnx",
    "section": "",
    "text": "To follow along with this example, you will need:\n\nA python environment with tensorflow (tip: Use conda)\n\n  conda install tensorflow\nor\n  conda install tensorflow-gpu\n\nA TensorFlow SavedModel (download)\n\nThe TensorFlow model used for this tutorial is a PoseNet model with a ResNet architecture. You can download the exact model here."
  },
  {
    "objectID": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#requirements",
    "href": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#requirements",
    "title": "How to Convert TensorFlow Models to ONNX with tf2onnx",
    "section": "",
    "text": "To follow along with this example, you will need:\n\nA python environment with tensorflow (tip: Use conda)\n\n  conda install tensorflow\nor\n  conda install tensorflow-gpu\n\nA TensorFlow SavedModel (download)\n\nThe TensorFlow model used for this tutorial is a PoseNet model with a ResNet architecture. You can download the exact model here."
  },
  {
    "objectID": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#usage",
    "href": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#usage",
    "title": "How to Convert TensorFlow Models to ONNX with tf2onnx",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nYou can install the library using pip:\npip install -U tf2onnx\n\n\nSteps\n\nMake sure the SavedModel file is named saved_model.pb\nAt a minimum, you need to specify the source model format, the path to the folder containing the SavedModel, and a name for the ONNX file.\nFor example:\n\nModel Format: --saved-model\nModel Folder: ./savedmodel\nNote: Do not include a / at the end of the path.\nOutput Name: model.onnx\n\n\npython -m tf2onnx.convert --saved-model ./savedmodel --opset 10 --output model.onnx\nWith these parameters you might receive some warnings, but the output should include something like this.\n2020-10-21 12:54:11,024 - INFO - Using tensorflow=2.3.0, onnx=1.7.0, tf2onnx=1.6.3/d4abc8\n2020-10-21 12:54:11,024 - INFO - Using opset &lt;onnx, 10&gt;\n2020-10-21 12:54:12,423 - INFO - Optimizing ONNX model\n2020-10-21 12:54:14,047 - INFO - After optimization: Add -4 (20-&gt;16), Const -1 (115-&gt;114), Identity -4 (4-&gt;0), Transpose -117 (122-&gt;5)\n2020-10-21 12:54:14,138 - INFO -\n2020-10-21 12:54:14,138 - INFO - Successfully converted TensorFlow model ./savedmodel to ONNX\n2020-10-21 12:54:14,215 - INFO - ONNX model is saved at model.onnx"
  },
  {
    "objectID": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#next-steps",
    "href": "posts/tensorflow-savedmodel-to-onnx-tutorial/index.html#next-steps",
    "title": "How to Convert TensorFlow Models to ONNX with tf2onnx",
    "section": "Next Steps",
    "text": "Next Steps\nBe sure to check out the GitHub repo if you want to learn what else you can do with the tool. The README page goes in to greater detail about the following:\n\nCurrent TensorFlow support\nParameter options\nAdvanced use cases\nHow the tool converts TensorFlow Models"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "",
    "text": "Overview\nConvert Model to TFJS\nImport Assets\nUpdate JavaScript Utility File\nUpdate jslib Plugin\nImport Plugin Functions\nCreate Object Detector Script\nUpdate Unity Scene\nTest in Browser\nSummary"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#overview",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#overview",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Overview",
    "text": "Overview\nThis follow-up post shows you how to modify an existing Unity project from a previous tutorial to use TensorFlow.js for in-browser object detection. We will begin by converting a pre-existing model to the TensorFlow.js format and importing it into Unity. Next, we will update the JavaScript utility file and the jslib plugin from the original project. After setting up the plugin, we will create a script to implement the object detector in Unity and update our scene to use it. Finally, we will test the project in-browser to ensure it works properly.\nIn-Browser Demo: Hand Gesture Detector\nProject Resources\n\nGitHub Repository: The final Unity project for this tutorial.\nColormaps: A JSON colormap file that maps each object class to a distinct color.\nTFJSModels: A YOLOX model in TensorFlow.js format.\nIn-Browser Hand Gesture Recognition for Unity with Fastai and TensorFlow.js: The previous tutorial."
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#convert-model-to-tfjs",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#convert-model-to-tfjs",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Convert Model to TFJS",
    "text": "Convert Model to TFJS\nWe first need a YOLOX model in TensorFlow.js format. We can use the same steps from the original tutorial to convert the YOLOX model from a previous tutorial. The YOLOX model detects the same hand gestures as the image classifier from the original tutorial.\nA link to the model conversion notebook is below, along with links for running the notebook on Google Colab and Kaggle.\n\n\n\nJupyter Notebook\nColab\nKaggle\n\n\n\n\nGitHub Repository\nOpen In Colab\nOpen in Kaggle"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#import-assets",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#import-assets",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Import Assets",
    "text": "Import Assets\nOpen the Unity project from the original tutorial in the Unity Editor. Alternatively, you can create a copy of the project and rename it to something more appropriate. We will first import the colormap and TensorFlow.js model into the Assets folder.\nImport color map\nWe’ll store the colormap JSON file in a new folder called Colormaps. Drag and drop the JSON colormap file from the operating system’s file explorer into the Colormaps folder.\n\nGoogle Drive: Colormaps\n\n\n\n\n\n\nImport TFJS model\nWe need to replace the existing TensorFlow.js models in the StreamingAssets folder with the YOLOX model.\n\nGoogle Drive: TFJSModels"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#update-javascript-utility-file",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#update-javascript-utility-file",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Update JavaScript Utility File",
    "text": "Update JavaScript Utility File\nWe’ll update the function in the utils.js file in the StreamingAssets folder to return the raw model output.\nUpdate function to perform inference asynchronously with YOLOX model\n// Perform inference with the provided model and input data\nasync function PerformInferenceAsync(model, float32Data, shape) {\n\n    const outputData = tf.tidy(() =&gt; {\n        const input_tensor = tf.tensor(float32Data, shape, 'float32');\n        // Make a prediction.\n        return model.predict(input_tensor);\n    });\n    return await outputData.data();\n}"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#update-jslib-plugin",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#update-jslib-plugin",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Update jslib Plugin",
    "text": "Update jslib Plugin\nWe also need to make a few updates to the WebGLPlugin.jslib file.\nUpdate function to Initialize TFJS model\nWe’ll update the InitTFJSModel function to take the path to the YOLOX model.json file and the normalization stats to the plugin.\n// Load a TFJS YOLOX model\nInitTFJSModel: async function (model_path, mean, std_dev) {\n\n    // Convert bytes to the text\n    let model_path_str = UTF8ToString(model_path);\n    // Load the TensorFlow.js model at the provided file path\n    this.model = await tf.loadGraphModel(model_path_str, { fromTFHub: false });\n\n    // Check the model input shape\n    const input_shape = this.model.inputs[0].shape;\n    console.log(`Input Shape: ${input_shape}`);\n\n    // Store normalization stats\n    this.mean = new Float32Array(buffer, mean, 3);\n    this.std_dev = new Float32Array(buffer, std_dev, 3);\n},\nDefine function to update output array\nThe size of the raw model output depends on the input resolution. That means we need to update the output array each time we update the input resolution.\n// Update the array which stores the raw model output\nUpdateOutputArray: function (array_data, size) {\n    delete this.output_array;\n    this.output_array = new Float32Array(buffer, array_data, size);\n    console.log(`New output size JS: ${this.output_array.length}`);\n},\nUpdate inference function for YOLOX model\nLastly, we need to update the PerformInference function to normalize the input data and fill the shared output array with the raw model output.\n// Perform inference with the provided image data\nPerformInference: function (image_data, size, width, height) {\n\n    // Only perform inference after loading a model\n    if (typeof this.model == 'undefined') {\n        console.log(\"Model not defined yet\");\n        return false;\n    }\n\n    // Initialize an array with the raw image data\n    const uintArray = new Uint8ClampedArray(buffer, image_data, size, width, height);\n\n    // Channels-last order\n    const [input_array] = new Array(new Array());\n\n    // Flip input image from Unity\n    for (let row = height - 1; row &gt;= 0; row--) {\n        let slice = uintArray.slice(row * width * 3, (row * width * 3) + (width * 3));\n        for (let col = 0; col &lt; slice.length; col += 3) {\n            input_array.push(((slice[col + 0] / 255.0) - this.mean[0]) / this.std_dev[0]);\n            input_array.push(((slice[col + 1] / 255.0) - this.mean[1]) / this.std_dev[1]);\n            input_array.push(((slice[col + 2] / 255.0) - this.mean[2]) / this.std_dev[2]);\n        }\n    }\n\n    // Initialize the input array with the preprocessed input data\n    const float32Data = Float32Array.from(input_array);\n    const shape = [1, height, width, 3];\n\n    // Pass preprocessed input to the model\n    PerformInferenceAsync(this.model, float32Data, shape).then(output =&gt; {\n        if (output_array.length == output.length) {\n            this.output_array.set(output);\n        }\n        else {\n            console.log(`Model output size JS: ${output.length}`);\n            this.output_array.fill(0);\n        }\n    })\n    return true;\n},\nThat’s it for the changes to the plugin code. Next, we need to update the script where we import the functions in Unity."
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#import-plugin-functions",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#import-plugin-functions",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Import Plugin Functions",
    "text": "Import Plugin Functions\nWe need to update the import line for InitTFJSModel and import the new UpdateOutputArray function in the WebGLPlugin.cs script.\nusing System.Runtime.InteropServices;\n\n/// &lt;summary&gt;\n/// Class with JavaScript plugin functions for WebGL.\n/// &lt;/summary&gt;\npublic static class WebGLPlugin\n{\n    // Import \"GetExternalJS\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void GetExternalJS();\n    // Import \"SetTFJSBackend\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void SetTFJSBackend(string backend);\n    // Import \"InitTFJSModel\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void InitTFJSModel(string model_path, float[] mean, float[] std);\n    // Import \"UpdateOutputArray\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern void UpdateOutputArray(float[] output_data, int size);\n    // Import \"PerformInference\" plugin function\n    [DllImport(\"__Internal\")]\n    public static extern bool PerformInference(byte[] image_data, int size, int width, int height);\n}\nChanging the import line for InitTFJSModel will cause an error in the ImageClassifier script, but we will resolve this in the next section."
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#create-object-detector-script",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#create-object-detector-script",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Create Object Detector Script",
    "text": "Create Object Detector Script\nWe’ll replace the ImageClassifier script from the original tutorial with a new script called ObjectDetector. It will handle getting input images, sending them to the model, processing the model output, and drawing bounding boxes around detected objects. Delete the existing ImageClassifier script.\n\n\n\n\n\nAdd required namespaces\n\nSystem: Contains fundamental classes and base classes that define commonly-used value and reference data types, events and event handlers, interfaces, attributes, and processing exceptions.\nUnityEngine.UI: Provides access to UI elements.\nUnityEngine.Rendering: Provides access to the elements of the rendering pipeline.\nSystem.IO: Contains types that allow reading and writing to files and data streams, and types that provide basic file and directory support.\nUnityEngine.Networking: Provides access to the UnityWebRequest module to communicate with http services.\nSystem.Linq: Provides classes and interfaces that support queries that use Language-Integrated Query (LINQ).\n\n\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing System;\nusing UnityEngine.UI;\nusing System.IO;\nusing UnityEngine.Networking;\nusing System.Linq;\nAdd code to create a list of available TFJS models\nWe can use the same code to create a list of available TFJS models used in the ImageClassifier script. This code will go right below the namespaces.\n#if UNITY_EDITOR\nusing UnityEditor;\n\n[InitializeOnLoad]\npublic class Startup\n{\n    // A helper class that stores the name and file path for a TensorFlow.js model\n    [System.Serializable]\n    class ModelData\n    {\n        public string name;\n        public string path;\n\n        public ModelData(string name, string path)\n        {\n            this.name = name;\n            this.path = path;\n        }\n    }\n\n    // A helper class that stores a list of TensorFlow.js model names and file paths\n    [System.Serializable]\n    class ModelList\n    {\n        public List&lt;ModelData&gt; models;\n\n        public ModelList(List&lt;ModelData&gt; models)\n        {\n            this.models = models;\n        }\n    }\n\n    static Startup()\n    {\n        string tfjsModelsDir = \"TFJSModels\";\n        List&lt;ModelData&gt; models = new List&lt;ModelData&gt;();\n\n        Debug.Log(\"Available models\");\n        // Get the paths for each model folder\n        foreach (string dir in Directory.GetDirectories($\"{Application.streamingAssetsPath}/{tfjsModelsDir}\"))\n        {\n            string dirStr = dir.Replace(\"\\\\\", \"/\");\n            // Extract the model folder name\n            string[] splits = dirStr.Split('/');\n            string modelName = splits[splits.Length - 1];\n\n            // Get the paths for the model.json file for each model\n            foreach (string file in Directory.GetFiles(dirStr))\n            {\n                if (file.EndsWith(\"model.json\"))\n                {\n                    string fileStr = file.Replace(\"\\\\\", \"/\").Replace(Application.streamingAssetsPath, \"\");\n                    models.Add(new ModelData(modelName, fileStr));\n                }\n            }\n        }\n\n        ModelList modelList = new ModelList(models);\n        // Format the list of available models as a string in JSON format\n        string json = JsonUtility.ToJson(modelList);\n        Debug.Log($\"Model List JSON: {json}\");\n        // Write the list of available TensorFlow.js models to a JSON file\n        using StreamWriter writer = new StreamWriter($\"{Application.streamingAssetsPath}/models.json\");\n        writer.Write(json);\n    }\n}\n#endif\n\nDefine public variables\nThe required public variables are mostly the same as the ImageClassifier script.\nDefine scene object variables\nWe’ll add a new variable to indicate whether to mirror the in-scene screen.\n[Header(\"Scene Objects\")]\n[Tooltip(\"The Screen object for the scene\")]\npublic Transform screen;\n[Tooltip(\"Mirror the in-game screen.\")]\npublic bool mirrorScreen = true;\nDefine data processing variables\nWe’ll increase the default target input resolution from 216 to 224.\n[Header(\"Data Processing\")]\n[Tooltip(\"The target minimum model input dimensions\")]\npublic int targetDim = 224;\nDefine output processing variables\nWe’ll replace the classLabels TextAsset with a variable for the colormap JSON file.\n[Header(\"Output Processing\")]\n[Tooltip(\"A json file containing the colormaps for object classes\")]\npublic TextAsset colormapFile;\n[Tooltip(\"Minimum confidence score for keeping predictions\")]\n[Range(0, 1f)]\npublic float minConfidence = 0.5f;\nDefine variables for debugging (unchanged)\n[Header(\"Debugging\")]\n[Tooltip(\"Print debugging messages to the console\")]\npublic bool printDebugMessages = true;\nDefine webcam variables (unchanged)\n[Header(\"Webcam\")]\n[Tooltip(\"Use a webcam as input\")]\npublic bool useWebcam = false;\n[Tooltip(\"The requested webcam dimensions\")]\npublic Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"The requested webcam framerate\")]\n[Range(0, 60)]\npublic int webcamFPS = 60;\nDefine variables for user interface\nWe’ll replace the displayPredictedClass GUI variable with two new variables that control displaying bounding boxes and the number of detected objects.\n[Header(\"GUI\")]\n[Tooltip(\"Display predicted class\")]\npublic bool displayBoundingBoxes = true;\n[Tooltip(\"Display number of detected objects\")]\npublic bool displayProposalCount = true;\n[Tooltip(\"Display fps\")]\npublic bool displayFPS = true;\n[Tooltip(\"The on-screen text color\")]\npublic Color textColor = Color.yellow;\n[Tooltip(\"The scale value for the on-screen font size\")]\n[Range(0, 99)]\npublic int fontScale = 50;\n[Tooltip(\"The number of seconds to wait between refreshing the fps value\")]\n[Range(0.01f, 1.0f)]\npublic float fpsRefreshRate = 0.1f;\n[Tooltip(\"The toggle for using a webcam as the input source\")]\npublic Toggle useWebcamToggle;\n[Tooltip(\"The dropdown menu that lists available webcam devices\")]\npublic Dropdown webcamDropdown;\n[Tooltip(\"The dropdown menu that lists available TFJS models\")]\npublic Dropdown modelDropdown;\n[Tooltip(\"The dropdown menu that lists available TFJS backends\")]\npublic Dropdown backendDropdown;\nDefine TensorFlow.js variables (unchanged)\n[Header(\"TFJS\")]\n[Tooltip(\"The name of the TFJS models folder\")]\npublic string tfjsModelsDir = \"TFJSModels\";\n\n\nDefine private variables\nWe’ll add the required private variables right below the public variables.\nDefine private webcam variables (unchanged)\n// List of available webcam devices\nprivate WebCamDevice[] webcamDevices;\n// Live video input from a webcam\nprivate WebCamTexture webcamTexture;\n// The name of the current webcam  device\nprivate string currentWebcam;\nDefine input variables (unchanged)\n// The test image dimensions\nVector2Int imageDims;\n// The test image texture\nTexture imageTexture;\n// The current screen object dimensions\nVector2Int screenDims;\n// The model GPU input texture\nRenderTexture inputTextureGPU;\n// The model CPU input texture\nTexture2D inputTextureCPU;\nDefine variables for colormap\nWe’ll create a couple of classes to parse the JSON colormap content.\n// A class for parsing in colormaps from a JSON file\n[System.Serializable]\nclass ColorMap { public string label; public float[] color; }\n// A class for reading in a list of colormaps from a JSON file\n[System.Serializable]\nclass ColorMapList { public List&lt;ColorMap&gt; items; }\n// Stores a list of colormaps from a JSON file\nprivate ColorMapList colormapList;\n// A list of colors that map to class labels\nprivate Color[] colors;\n// A list of single pixel textures that map to class labels\nprivate Texture2D[] colorTextures;\nDefine variable to track whether a model is initialized (unchanged)\n// Stores whether the TensorFlow.js model is ready for inference\nbool modelInitialized;\nDefine variables for tracking the framerate (unchanged)\n// The current frame rate value\nprivate int fps = 0;\n// Controls when the frame rate value updates\nprivate float fpsTimer = 0f;\nDefine variables to store values for GUI dropdowns (unchanged)\n// File paths for the available TFJS models\nList&lt;string&gt; modelPaths = new List&lt;string&gt;();\n// Names of the available TFJS models\nList&lt;string&gt; modelNames = new List&lt;string&gt;();\n// Names of the available TFJS backends\nList&lt;string&gt; tfjsBackends = new List&lt;string&gt; { \"webgl\" };\nDefine variables for reading the models.json file (unchanged)\n// A helper class to store the name and file path of a TensorFlow.js model\n[System.Serializable]\nclass ModelData { public string name; public string path; }\n// A helper class to store a read a list of available TensorFlow.js models from a JSON file\n[System.Serializable]\nclass ModelList { public List&lt;ModelData&gt; models; }\nDefine variables for normalization stats\nThe YOLOX model uses the standard ImageNet normalization stats from PyTorch.\n// The normalization stats for the YOLOX model\nfloat[] mean = new float[] { 0.485f, 0.456f, 0.406f };\nfloat[] std = new float[] { 0.229f, 0.224f, 0.225f };\nDefine struct to store information for object predictions\n/// &lt;summary&gt;\n/// Stores the information for a single object\n/// &lt;/summary&gt; \npublic struct Object\n{\n    // The X coordinate for the top left bounding box corner\n    public float x0;\n    // The Y coordinate for the top left bounding box cornder\n    public float y0;\n    // The width of the bounding box\n    public float width;\n    // The height of the bounding box\n    public float height;\n    // The object class index for the detected object\n    public int label;\n    // The model confidence score for the object\n    public float prob;\n\n    public Object(float x0, float y0, float width, float height, int label, float prob)\n    {\n        this.x0 = x0;\n        this.y0 = y0;\n        this.width = width;\n        this.height = height;\n        this.label = label;\n        this.prob = prob;\n    }\n}\nDefine struct to store grid offset and stride values\n// Store grid offset and stride values to decode a section of the model output\npublic struct GridAndStride\n{\n    public int grid0;\n    public int grid1;\n    public int stride;\n\n    public GridAndStride(int grid0, int grid1, int stride)\n    {\n        this.grid0 = grid0;\n        this.grid1 = grid1;\n        this.stride = stride;\n    }\n}\nDefine variables for processing model output\n// Stores information for the current list of detected objects\nprivate Object[] objectInfoArray;\n// Stores the grid and stride values to navigate the raw model output\nList&lt;GridAndStride&gt; grid_strides = new List&lt;GridAndStride&gt;();\n// The stride values used to generate the gride_strides vector\nint[] strides = new int[] { 8, 16, 32 };\n// Stores the raw model output\nfloat[] output_array;\n// The scale values used to adjust the model output to the source image resolution\nfloat scale_x;\nfloat scale_y;\n// Stores the number of detected objects\nprivate int numObjects;\n\n\nDefine Initialization Methods\nThe initialization methods are identical to those from the ImageClassifier script except for UpdateTFJSModel.\nDefine method to initialize a webcam device (unchanged)\n/// &lt;summary&gt;\n/// Initialize the selected webcam device\n/// &lt;/summary&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the selected webcam device&lt;/param&gt;\nvoid InitializeWebcam(string deviceName)\n{\n    // Stop any webcams already playing\n    if (webcamTexture && webcamTexture.isPlaying) webcamTexture.Stop();\n\n    // Create a new WebCamTexture\n    webcamTexture = new WebCamTexture(deviceName, webcamDims.x, webcamDims.y, webcamFPS);\n\n    // Start the webcam\n    webcamTexture.Play();\n    // Check if webcam is playing\n    useWebcam = webcamTexture.isPlaying;\n    // Update toggle value\n    useWebcamToggle.SetIsOnWithoutNotify(useWebcam);\n\n    Debug.Log(useWebcam ? \"Webcam is playing\" : \"Webcam not playing, option disabled\");\n}\nDefine method to initialize the in-scene screen object (unchanged)\n/// &lt;summary&gt;\n/// Resize and position an in-scene screen object\n/// &lt;/summary&gt;\nvoid InitializeScreen()\n{\n    // Set the texture for the screen object\n    screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture = useWebcam ? webcamTexture : imageTexture;\n    // Set the screen dimensions\n    screenDims = useWebcam ? new Vector2Int(webcamTexture.width, webcamTexture.height) : imageDims;\n\n    // Flip the screen around the Y-Axis when using webcam\n    float yRotation = useWebcam ? 180f : 0f;\n    // Invert the scale value for the Z-Axis when using webcam\n    float zScale = useWebcam ? -1f : 1f;\n\n    // Set screen rotation\n    screen.rotation = Quaternion.Euler(0, yRotation, 0);\n    // Adjust the screen dimensions\n    screen.localScale = new Vector3(screenDims.x, screenDims.y, zScale);\n\n    // Adjust the screen position\n    screen.position = new Vector3(screenDims.x / 2, screenDims.y / 2, 1);\n}\nDefine method to switch TensorFlow.js models\n/// &lt;summary&gt;\n/// Load a TensorFlow.js model\n/// &lt;/summary&gt;\npublic void UpdateTFJSModel()\n{\n    // Load TensorFlow.js model in JavaScript plugin\n    WebGLPlugin.InitTFJSModel(modelPaths[modelDropdown.value], mean, std);\n}\nDefine method to read the list of available TensorFlow.js models (unchanged)\n/// &lt;summary&gt;\n/// Get the names and paths of the available TensorFlow.js models\n/// &lt;/summary&gt;\n/// &lt;param name=\"json\"&gt;&lt;/param&gt;\nvoid GetTFJSModels(string json)\n{\n    ModelList modelList = JsonUtility.FromJson&lt;ModelList&gt;(json);\n    foreach (ModelData model in modelList.models)\n    {\n        //Debug.Log($\"{model.name}: {model.path}\");\n        modelNames.Add(model.name);\n        string path = $\"{Application.streamingAssetsPath}{model.path}\";\n        modelPaths.Add(path);\n    }\n    // Remove default dropdown options\n    modelDropdown.ClearOptions();\n    // Add TFJS model names to menu\n    modelDropdown.AddOptions(modelNames);\n    // Select the first option in the dropdown\n    modelDropdown.SetValueWithoutNotify(0);\n}\nDefine method to download the list of available TensorFlow.js models (unchanged)\n/// &lt;summary&gt;\n/// Download the JSON file with the available TFJS model information\n/// &lt;/summary&gt;\n/// &lt;param name=\"uri\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nIEnumerator GetRequest(string uri)\n{\n    using (UnityWebRequest webRequest = UnityWebRequest.Get(uri))\n    {\n        // Request and wait for the desired page.\n        yield return webRequest.SendWebRequest();\n\n        string[] pages = uri.Split('/');\n        int page = pages.Length - 1;\n\n        switch (webRequest.result)\n        {\n            case UnityWebRequest.Result.ConnectionError:\n            case UnityWebRequest.Result.DataProcessingError:\n                Debug.LogError(pages[page] + \": Error: \" + webRequest.error);\n                break;\n            case UnityWebRequest.Result.ProtocolError:\n                Debug.LogError(pages[page] + \": HTTP Error: \" + webRequest.error);\n                break;\n            case UnityWebRequest.Result.Success:\n                Debug.Log(pages[page] + \":\\nReceived: \" + webRequest.downloadHandler.text);\n\n                // Extract the available model names and file paths from the JSON string\n                GetTFJSModels(webRequest.downloadHandler.text);\n                // Initialize one of the available TensorFlow.js models\n                UpdateTFJSModel();\n                break;\n        }\n    }\n}\nDefine method to initialize GUI dropdown menu options (unchanged)\n/// &lt;summary&gt;\n/// Initialize the GUI dropdown list\n/// &lt;/summary&gt;\nvoid InitializeDropdown()\n{\n    // Create list of webcam device names\n    List&lt;string&gt; webcamNames = new List&lt;string&gt;();\n    foreach (WebCamDevice device in webcamDevices) webcamNames.Add(device.name);\n\n    // Remove default dropdown options\n    webcamDropdown.ClearOptions();\n    // Add webcam device names to dropdown menu\n    webcamDropdown.AddOptions(webcamNames);\n    // Set the value for the dropdown to the current webcam device\n    webcamDropdown.SetValueWithoutNotify(webcamNames.IndexOf(currentWebcam));\n\n    // Get the available TensorFlow.js models\n    string modelListPath = $\"{Application.streamingAssetsPath}/models.json\";\n    StartCoroutine(GetRequest(modelListPath));\n\n    // Remove default dropdown options\n    backendDropdown.ClearOptions();\n    // Add TFJS backend names to menu\n    backendDropdown.AddOptions(tfjsBackends);\n    // Select the first option in the dropdown\n    backendDropdown.SetValueWithoutNotify(0);\n}\nDefine method to initialize the in-scene camera object (unchanged)\n/// &lt;summary&gt;\n/// Resize and position the main camera based on an in-scene screen object\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of an in-scene screen object&lt;/param&gt;\nvoid InitializeCamera(Vector2Int screenDims, string cameraName = \"Main Camera\")\n{\n    // Get a reference to the Main Camera GameObject\n    GameObject camera = GameObject.Find(cameraName);\n    // Adjust the camera position to account for updates to the screenDims\n    camera.transform.position = new Vector3(screenDims.x / 2, screenDims.y / 2, -10f);\n    // Render objects with no perspective (i.e. 2D)\n    camera.GetComponent&lt;Camera&gt;().orthographic = true;\n    // Adjust the camera size to account for updates to the screenDims\n    camera.GetComponent&lt;Camera&gt;().orthographicSize = screenDims.y / 2;\n}\n\n\nDefine Awake method (unchanged)\n// Awake is called when the script instance is being loaded\nvoid Awake()\n{\n    WebGLPlugin.GetExternalJS();\n}\n\n\nDefine Start method\nIn the Start method, we need to initialize the color map variables with the JSON file.\n// Start is called before the first frame update\nvoid Start()\n{\n    // Get the source image texture\n    imageTexture = screen.gameObject.GetComponent&lt;MeshRenderer&gt;().material.mainTexture;\n    // Get the source image dimensions as a Vector2Int\n    imageDims = new Vector2Int(imageTexture.width, imageTexture.height);\n\n    // Initialize list of available webcam devices\n    webcamDevices = WebCamTexture.devices;\n    foreach (WebCamDevice device in webcamDevices) Debug.Log(device.name);\n    currentWebcam = webcamDevices[0].name;\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Initialize webcam\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n\n\n    // Initialize list of color maps from JSON file\n    colormapList = JsonUtility.FromJson&lt;ColorMapList&gt;(colormapFile.text);\n    // Initialize the list of colors\n    colors = new Color[colormapList.items.Count];\n    // Initialize the list of color textures\n    colorTextures = new Texture2D[colormapList.items.Count];\n\n    // Populate the color and color texture arrays\n    for (int i = 0; i &lt; colors.Length; i++)\n    {\n        // Create a new color object\n        colors[i] = new Color(\n            colormapList.items[i].color[0],\n            colormapList.items[i].color[1],\n            colormapList.items[i].color[2]);\n        // Create a single-pixel texture\n        colorTextures[i] = new Texture2D(1, 1);\n        colorTextures[i].SetPixel(0, 0, colors[i]);\n        colorTextures[i].Apply();\n\n    }\n\n    // Initialize the webcam dropdown list\n    InitializeDropdown();\n\n    // Update the current TensorFlow.js compute backend\n    WebGLPlugin.SetTFJSBackend(tfjsBackends[backendDropdown.value]);\n}\n\n\nDefine Processing Methods\nThe preprocessing methods are identical to those from the ImageClassifier script. However, we need to add some post-processing methods to extract object predictions from the raw model output.\nDefine method to calculate input resolution (unchanged)\n/// &lt;summary&gt;\n/// Scale the source image resolution to the target input dimensions\n/// while maintaing the source aspect ratio.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;&lt;/param&gt;\n/// &lt;param name=\"targetDims\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nVector2Int CalculateInputDims(Vector2Int imageDims, int targetDim)\n{\n    // Clamp the minimum dimension value to 64px\n    targetDim = Mathf.Max(targetDim, 64);\n\n    Vector2Int inputDims = new Vector2Int();\n\n    // Calculate the input dimensions using the target minimum dimension\n    if (imageDims.x &gt;= imageDims.y)\n    {\n        inputDims[0] = (int)(imageDims.x / ((float)imageDims.y / (float)targetDim));\n        inputDims[1] = targetDim;\n    }\n    else\n    {\n        inputDims[0] = targetDim;\n        inputDims[1] = (int)(imageDims.y / ((float)imageDims.x / (float)targetDim));\n    }\n\n    return inputDims;\n}\nDefine method to scale bounding boxes to the display resolution\n/// &lt;summary&gt;\n/// Scale the latest bounding boxes to the display resolution\n/// &lt;/summary&gt;\npublic void ScaleBoundingBoxes()\n{\n    // Process new detected objects\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        // The smallest dimension of the screen\n        float minScreenDim = Mathf.Min(screen.transform.localScale.x, screen.transform.localScale.y);\n        // The smallest input dimension\n        int minInputDim = Mathf.Min(inputTextureCPU.width, inputTextureCPU.height);\n        // Calculate the scale value between the in-game screen and input dimensions\n        float minImgScale = minScreenDim / minInputDim;\n        // Calculate the scale value between the in-game screen and display\n        float displayScale = Screen.height / screen.transform.localScale.y;\n\n        // Scale bounding box to in-game screen resolution and flip the bbox coordinates vertically\n        float x0 = objectInfoArray[i].x0 * minImgScale;\n        float y0 = (inputTextureCPU.height - objectInfoArray[i].y0) * minImgScale;\n        float width = objectInfoArray[i].width * minImgScale;\n        float height = objectInfoArray[i].height * minImgScale;\n\n        // Mirror bounding box across screen\n        if (mirrorScreen && useWebcam)\n        {\n            x0 = screen.transform.localScale.x - x0 - width;\n        }\n\n        // Scale bounding boxes to display resolution\n        objectInfoArray[i].x0 = x0 * displayScale;\n        objectInfoArray[i].y0 = y0 * displayScale;\n        objectInfoArray[i].width = width * displayScale;\n        objectInfoArray[i].height = height * displayScale;\n\n        // Offset the bounding box coordinates based on the difference between the in-game screen and display\n        objectInfoArray[i].x0 += (Screen.width - screen.transform.localScale.x * displayScale) / 2;\n    }\n}\nDefine method to generate stride values to navigate the raw model output\nWe’ll generate offset values based on the input dimensions and stride values, which we can use to traverse the output array.\n/// &lt;summary&gt;\n/// Generate offset values to navigate the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"height\"&gt;The model input height&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;The model input width&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nList&lt;GridAndStride&gt; GenerateGridStrides(int height, int width)\n{\n    List&lt;GridAndStride&gt; grid_strides = new List&lt;GridAndStride&gt;();\n\n    // Iterate through each stride value\n    foreach (int stride in strides)\n    {\n        // Calculate the grid dimensions\n        int grid_height = height / stride;\n        int grid_width = width / stride;\n\n        Debug.Log($\"Gride: {grid_height} x {grid_width}\");\n\n        // Store each combination of grid coordinates\n        for (int g1 = 0; g1 &lt; grid_height; g1++)\n        {\n\n            for (int g0 = 0; g0 &lt; grid_width; g0++)\n            {\n                grid_strides.Add(new GridAndStride(g0, g1, stride));\n            }\n        }\n    }\n    return grid_strides;\n}\nDefine method to generate object detection proposals from the raw model output\nNext, we define a method to iterate through the output array and decode the bounding box information for each object proposal. We only want to keep the ones with a high enough confidence score. The model predicts the center coordinates of a bounding box, but we store the coordinates for the top-left corner as we use this to draw the rectangles on the screen.\n/// &lt;summary&gt;\n/// Generate object detection proposals from the raw model output\n/// &lt;/summary&gt;\n/// &lt;param name=\"model_output\"&gt;The raw model output&lt;/param&gt;\n/// &lt;param name=\"proposal_length\"&gt;The length of a single proposal&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nList&lt;Object&gt; GenerateYOLOXProposals(float[] model_output, int proposal_length)\n{\n    List&lt;Object&gt; proposals = new List&lt;Object&gt;();\n\n    // Obtain the number of classes the model was trained to detect\n    int num_classes = proposal_length - 5;\n\n    for (int anchor_idx = 0; anchor_idx &lt; grid_strides.Count; anchor_idx++)\n    {\n        // Get the current grid and stride values\n        var grid0 = grid_strides[anchor_idx].grid0;\n        var grid1 = grid_strides[anchor_idx].grid1;\n        var stride = grid_strides[anchor_idx].stride;\n\n        // Get the starting index for the current proposal\n        var start_idx = anchor_idx * proposal_length;\n\n        // Get the coordinates for the center of the predicted bounding box\n        var x_center = (model_output[start_idx + 0] + grid0) * stride;\n        var y_center = (model_output[start_idx + 1] + grid1) * stride;\n\n        // Get the dimensions for the predicted bounding box\n        var w = Mathf.Exp(model_output[start_idx + 2]) * stride;\n        var h = Mathf.Exp(model_output[start_idx + 3]) * stride;\n\n        // Calculate the coordinates for the upper left corner of the bounding box\n        var x0 = x_center - w * 0.5f;\n        var y0 = y_center - h * 0.5f;\n\n        x0 /= scale_x;\n        y0 /= scale_y;\n        w /= scale_x;\n        h /= scale_y;\n\n        // Get the confidence score that an object is present\n        var box_objectness = model_output[start_idx + 4];\n\n        // Initialize object struct with bounding box information\n        Object obj = new Object(x0, y0, w, h, 0, 0);\n\n        // Find the object class with the highest confidence score\n        for (int class_idx = 0; class_idx &lt; num_classes; class_idx++)\n        {\n            // Get the confidence score for the current object class\n            var box_cls_score = model_output[start_idx + 5 + class_idx];\n            // Calculate the final confidence score for the object proposal\n            var box_prob = box_objectness * box_cls_score;\n\n            // Check for the highest confidence score\n            if (box_prob &gt; obj.prob)\n            {\n                obj.label = class_idx;\n                obj.prob = box_prob;\n            }\n        }\n\n        // Only add object proposals with high enough confidence scores\n        if (obj.prob &gt; minConfidence) proposals.Add(obj);\n    }\n\n    // Sort the proposals based on the confidence score in descending order\n    proposals = proposals.OrderByDescending(x =&gt; x.prob).ToList();\n\n    return proposals;\n}\nDefine function to calculate the union area of two bounding boxes\n/// &lt;summary&gt;\n/// Calculate the union area of two bounding boxes\n/// &lt;/summary&gt;\n/// &lt;param name=\"a\"&gt;&lt;/param&gt;\n/// &lt;param name=\"b\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nfloat CalcUnionArea(Object a, Object b)\n{\n    var x = Mathf.Min(a.x0, b.x0);\n    var y = Mathf.Min(a.y0, b.y0);\n    var w = Mathf.Max(a.x0 + a.width, b.x0 + b.width) - x;\n    var h = Mathf.Max(a.y0 + a.height, b.y0 + b.height) - y;\n    return w * h;\n}\nDefine function to calculate the intersection area of two bounding boxes\n/// &lt;summary&gt;\n/// Calculate the intersection area of two bounding boxes\n/// &lt;/summary&gt;\n/// &lt;param name=\"a\"&gt;&lt;/param&gt;\n/// &lt;param name=\"b\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nfloat CalcInterArea(Object a, Object b)\n{\n    var x = Mathf.Max(a.x0, b.x0);\n    var y = Mathf.Max(a.y0, b.y0);\n    var w = Mathf.Min(a.x0 + a.width, b.x0 + b.width) - x;\n    var h = Mathf.Min(a.y0 + a.height, b.y0 + b.height) - y;\n    return w * h;\n}\nDefine function to sort bounding box proposals using Non-Maximum Suppression\n/// &lt;summary&gt;\n/// Sort bounding box proposals using Non-Maximum Suppression\n/// &lt;/summary&gt;\n/// &lt;param name=\"proposals\"&gt;&lt;/param&gt;\n/// &lt;param name=\"nms_thresh\"&gt;&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nList&lt;int&gt; NMSSortedBoxes(List&lt;Object&gt; proposals, float nms_thresh = 0.45f)\n{\n    List&lt;int&gt; proposal_indices = new List&lt;int&gt;();\n\n    for (int i = 0; i &lt; proposals.Count; i++)\n    {\n        var a = proposals[i];\n        bool keep = true;\n\n        // Check if the current object proposal overlaps any selected objects too much\n        foreach (int j in proposal_indices)\n        {\n            var b = proposals[j];\n\n            // Calculate the area where the two object bounding boxes overlap\n            var inter_area = CalcInterArea(a, b);\n\n            // Calculate the union area of both bounding boxes\n            var union_area = CalcUnionArea(a, b);\n\n            // Ignore object proposals that overlap selected objects too much\n            if (inter_area / union_area &gt; nms_thresh) keep = false;\n        }\n\n        // Keep object proposals that do not overlap selected objects too much\n        if (keep) proposal_indices.Add(i);\n    }\n\n    return proposal_indices;\n}\n\n\nDefine Update method\nMost of the Update method is identical to the one from the ImageClassifier script, besides the post-processing steps. Although, there is an odd quirk with part of the YOLOX model in TensorFlow.js.\nThe YOLOX model requires input dimensions that are multiples of 32. However, if we perform inference with such an input resolution, we get an error like the one below.\nUncaught (in promise) Error: Error in concat4D: Shape of tensors[1] (1,3,111,112) does not match the shape of the rest (1,3,112,112) along the non-concatenated axis 1.\nI don’t know why this occurs in TensorFlow.js, but we can resolve it by adding 1 to each input dimension. So instead of an input resolution of 224x224, we have 225x225.\n// Update is called once per frame\nvoid Update()\n{\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    if (useWebcam)\n    {\n        // Initialize webcam if it is not already playing\n        if (!webcamTexture || !webcamTexture.isPlaying) InitializeWebcam(currentWebcam);\n\n        // Skip the rest of the method if the webcam is not initialized\n        if (webcamTexture.width &lt;= 16) return;\n\n        // Make sure screen dimensions match webcam resolution when using webcam\n        if (screenDims.x != webcamTexture.width)\n        {\n            // Resize and position the screen object using the source image dimensions\n            InitializeScreen();\n            // Resize and position the main camera using the source image dimensions\n            InitializeCamera(screenDims);\n        }\n    }\n    else if (webcamTexture && webcamTexture.isPlaying)\n    {\n        // Stop the current webcam\n        webcamTexture.Stop();\n\n        // Resize and position the screen object using the source image dimensions\n        InitializeScreen();\n        // Resize and position the main camera using the source image dimensions\n        InitializeCamera(screenDims);\n    }\n\n\n    // Scale the source image resolution\n    Vector2Int sourceDims = CalculateInputDims(screenDims, targetDim);\n    Vector2Int inputDims = sourceDims;\n    inputDims[0] = (inputDims[0] - inputDims[0] % 32) + 1;\n    inputDims[1] = (inputDims[1] - inputDims[1] % 32) + 1;\n    scale_x = inputDims[0] / (float)sourceDims[0];\n    scale_y = inputDims[1] / (float)sourceDims[1];\n\n\n    // Initialize the input texture with the calculated input dimensions\n    inputTextureGPU = RenderTexture.GetTemporary(inputDims.x, inputDims.y, 24, RenderTextureFormat.ARGB32);\n\n    if (!inputTextureCPU || inputTextureCPU.width != inputTextureGPU.width)\n    {\n        inputTextureCPU = new Texture2D(inputDims.x, inputDims.y, TextureFormat.RGB24, false);\n        grid_strides = new List&lt;GridAndStride&gt;();\n        grid_strides = GenerateGridStrides(inputDims[1], inputDims[0]);\n\n        int output_size = grid_strides.Count * (colors.Length + 5);\n        output_array = new float[output_size];\n        WebGLPlugin.UpdateOutputArray(output_array, output_size);\n        Debug.Log($\"Updating output array to {output_size}\");\n        Debug.Log($\"Input Dims: {inputTextureCPU.width}x{inputTextureCPU.height}\");\n    }\n\n    if (printDebugMessages) Debug.Log($\"Input Dims: {inputTextureGPU.width}x{inputTextureGPU.height}\");\n\n    // Copy the source texture into model input texture\n    Graphics.Blit((useWebcam ? webcamTexture : imageTexture), inputTextureGPU);\n\n    // Download pixel data from GPU to CPU\n    RenderTexture.active = inputTextureGPU;\n    inputTextureCPU.ReadPixels(new Rect(0, 0, inputTextureGPU.width, inputTextureGPU.height), 0, 0);\n    inputTextureCPU.Apply();\n\n    // Get the current input dimensions\n    int width = inputTextureCPU.width;\n    int height = inputTextureCPU.height;\n    int size = width * height * 3;\n\n    // Pass the input data to the plugin to perform inference\n    modelInitialized = WebGLPlugin.PerformInference(inputTextureCPU.GetRawTextureData(), size, width, height);\n\n    // Release the input texture\n    RenderTexture.ReleaseTemporary(inputTextureGPU);\n\n    if (modelInitialized == false) return;\n\n    List&lt;Object&gt; proposals = GenerateYOLOXProposals(output_array, colors.Length + 5);\n\n    List&lt;int&gt; proposal_indices = NMSSortedBoxes(proposals);\n    numObjects = proposal_indices.Count;\n    objectInfoArray = new Object[numObjects];\n    for (int i = 0; i &lt; objectInfoArray.Length; i++)\n    {\n        objectInfoArray[i] = proposals[proposal_indices[i]];\n    }\n    ScaleBoundingBoxes();\n\n}\n\n\nDefine GUI Methods\nDefine method to update webcam usage from GUI (unchanged)\n/// &lt;summary&gt;\n/// This method is called when the value for the webcam toggle changes\n/// &lt;/summary&gt;\n/// &lt;param name=\"useWebcam\"&gt;&lt;/param&gt;\npublic void UpdateWebcamToggle(bool useWebcam)\n{\n    this.useWebcam = useWebcam;\n}\nDefine method to update webcam device from GUI (unchanged)\n/// &lt;summary&gt;\n/// The method is called when the selected value for the webcam dropdown changes\n/// &lt;/summary&gt;\npublic void UpdateWebcamDevice()\n{\n    currentWebcam = webcamDevices[webcamDropdown.value].name;\n    Debug.Log($\"Selected Webcam: {currentWebcam}\");\n    // Initialize webcam if it is not already playing\n    if (useWebcam) InitializeWebcam(currentWebcam);\n\n    // Resize and position the screen object using the source image dimensions\n    InitializeScreen();\n    // Resize and position the main camera using the source image dimensions\n    InitializeCamera(screenDims);\n}\nDefine method to update the TensorFlow.js backend (unchanged)\n/// &lt;summary&gt;\n/// Update the TensorFlow.js compute backend\n/// &lt;/summary&gt;\npublic void UpdateTFJSBackend()\n{\n    WebGLPlugin.SetTFJSBackend(tfjsBackends[backendDropdown.value]);\n}\nDefine method to update the confidence threshold (unchanged)\n/// &lt;summary&gt;\n/// Update the minimum confidence score for keeping predictions\n/// &lt;/summary&gt;\n/// &lt;param name=\"slider\"&gt;&lt;/param&gt;\npublic void UpdateConfidenceThreshold(Slider slider)\n{\n    minConfidence = slider.value;\n}\nDefine OnGUI method\nWe’ll display the predicted bounding boxes and current frame rate in the OnGUI method. We’ll show a different message while the model is still loading.\n// OnGUI is called for rendering and handling GUI events.\npublic void OnGUI()\n{\n    // Initialize a rectangle for label text\n    Rect labelRect = new Rect();\n    // Initialize a rectangle for bounding boxes\n    Rect boxRect = new Rect();\n\n    GUIStyle labelStyle = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * 11e-3)\n    };\n    labelStyle.alignment = TextAnchor.MiddleLeft;\n\n    foreach (Object objectInfo in objectInfoArray)\n    {\n        if (!displayBoundingBoxes) break;\n\n        // Skip object if label index is out of bounds\n        if (objectInfo.label &gt; colors.Length - 1) continue;\n\n        // Get color for current class index\n        Color color = colors[objectInfo.label];\n        // Get label for current class index\n        string name = colormapList.items[objectInfo.label].label;\n\n        // Set bounding box coordinates\n        boxRect.x = objectInfo.x0;\n        boxRect.y = Screen.height - objectInfo.y0;\n        // Set bounding box dimensions\n        boxRect.width = objectInfo.width;\n        boxRect.height = objectInfo.height;\n\n        // Scale bounding box line width based on display resolution\n        int lineWidth = (int)(Screen.width * 1.75e-3);\n        // Render bounding box\n        GUI.DrawTexture(\n            position: boxRect,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: lineWidth,\n            borderRadius: 0);\n\n        // Include class label and confidence score in label text\n        string labelText = $\" {name}: {(objectInfo.prob * 100).ToString(\"0.##\")}%\";\n\n        // Initialize label GUI content\n        GUIContent labelContent = new GUIContent(labelText);\n\n        // Calculate the text size.\n        Vector2 textSize = labelStyle.CalcSize(labelContent);\n\n        // Set label text coordinates\n        labelRect.x = objectInfo.x0;\n        labelRect.y = Screen.height - objectInfo.y0 - textSize.y + lineWidth;\n\n        // Set label text dimensions\n        labelRect.width = Mathf.Max(textSize.x, objectInfo.width);\n        labelRect.height = textSize.y;\n        // Set label text and backgound color\n        labelStyle.normal.textColor = color.grayscale &gt; 0.5 ? Color.black : Color.white;\n        labelStyle.normal.background = colorTextures[objectInfo.label];\n        // Render label\n        GUI.Label(labelRect, labelContent, labelStyle);\n\n        Rect objectDot = new Rect();\n        objectDot.height = lineWidth * 5;\n        objectDot.width = lineWidth * 5;\n        float radius = objectDot.width / 2;\n        objectDot.x = (boxRect.x + boxRect.width / 2) - radius;\n        objectDot.y = (boxRect.y + boxRect.height / 2) - radius;\n\n\n        GUI.DrawTexture(\n            position: objectDot,\n            image: Texture2D.whiteTexture,\n            scaleMode: ScaleMode.StretchToFill,\n            alphaBlend: true,\n            imageAspect: 0,\n            color: color,\n            borderWidth: radius,\n            borderRadius: radius);\n\n    }\n\n    // Define styling information for GUI elements\n    GUIStyle style = new GUIStyle\n    {\n        fontSize = (int)(Screen.width * (1f / (100f - fontScale)))\n    };\n    style.normal.textColor = textColor;\n\n    // Define screen spaces for GUI elements\n    Rect slot1 = new Rect(10, 10, 500, 500);\n    Rect slot2 = new Rect(10, style.fontSize * 1.5f, 500, 500);\n\n    string content = $\"Objects Detected: {numObjects}\";\n    if (displayProposalCount) GUI.Label(slot1, new GUIContent(modelInitialized ? content : \"Loading Model...\"), style);\n\n    // Update framerate value\n    if (Time.unscaledTime &gt; fpsTimer)\n    {\n        fps = (int)(1f / Time.unscaledDeltaTime);\n        fpsTimer = Time.unscaledTime + fpsRefreshRate;\n    }\n\n    // Adjust screen position when not showing predicted class\n    Rect fpsRect = displayProposalCount ? slot2 : slot1;\n    if (displayFPS) GUI.Label(fpsRect, new GUIContent($\"FPS: {fps}\"), style);\n}\nThat’s it for the code updates."
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#update-unity-scene",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#update-unity-scene",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Update Unity Scene",
    "text": "Update Unity Scene\nAll that’s left is to swap out the ImageClassifier script in the Unity scene.\nUpdate Inference Manager object\nWith the InferenceManager object selected, drag the ObjectDetector script into the Inspector tab.\n\n\n\n\n\nRemove the empty script component left over from the ImageClassifier script.\n\n\n\n\n\nNow we can assign the Screen object and colormap file in the Inspector tab by dragging them into their respective fields.\n\n\n\n\n\nConfigure Webcam Toggle On Value Changed function\nNext, we need to pair the WebcamToggle with the UpdateWebcamToggle function in the ObjectDetector script. Expand the Canvas object and select the WebcamToggle.\n\n\n\n\n\nClick and drag the InferenceManager into the On Value Changed field.\n\n\n\n\n\nOpen the dropdown menu and select ObjectDetector → UpdateWebcamToggle.\n\n\n\n\n\nConfigure Webcam Dropdown On Value Changed function\n\n\n\n\n\nConfigure TFJSModelDropdown On Value Changed function\n\n\n\n\n\nConfigure TFJSBackendDropdown On Value Changed function\n\n\n\n\n\nConfigure ConfidenceThresholdSlider On Value Changed Event\n\n\n\n\n\nAssign GUI objects to Inference Manager"
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#test-in-browser",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#test-in-browser",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Test in Browser",
    "text": "Test in Browser\nNow, we can build the project and test it in a web browser. In the Unity project, select File → Build Settings... in the top menu bar to open the Build Settings window.\n\n\n\n\n\nSelect WebGL from the list of platforms and click Switch Platform if it is not already the target platform.\n\n\n\n\n\nWe can test the WebGL build locally by clicking Build and Run in the Build Settings window.\n\n\n\n\n\nUnity will prompt us to select a folder to store the build files or default to the Build folder from the original tutorial. If it’s the former, create a new folder called Build. Open the folder and click Select Folder to start the build process.\n\n\n\n\n\nUnity caps the framerate to the default target framerate for the platform. My desktop display maxes out at 60fps.\nTest YOLOX Tiny performance\nAs with the image classifier models in the original tutorial, performance is far lower in WebGL with the YOLOX model than when using native inference options.\n\n\n\n\n\nFull-screen\nPerformance seems slightly better when in full-screen mode."
  },
  {
    "objectID": "posts/tfjs-yolox-unity-tutorial/index.html#summary",
    "href": "posts/tfjs-yolox-unity-tutorial/index.html#summary",
    "title": "Using TensorFlow.js for In-Browser Object Detection in Unity",
    "section": "Summary",
    "text": "Summary\nIn this follow-up tutorial, we showed you how to use TensorFlow.js for in-browser object detection in Unity. We began by converting our model to TFJS and importing the necessary assets. Next, we updated our JavaScript and jslib plugin files. We then created an object detector script and updated our Unity scene to use it. Finally, we tested everything in the browser to ensure that our project worked as expected. With this tutorial, you should now be able to use TensorFlow.js for in-browser object detection in your Unity projects."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html",
    "href": "posts/transformers-book-notes/chapter-1/index.html",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "",
    "text": "Key Advancements\nRecurrent Architectures\nThe Encoder-Decoder Framework\nAttention Mechanisms\nTransfer Learning in NLP\nBridging the Gap With Hugging Face Transformers\nA Tour of Transformer Applications\nThe Hugging Face Ecosystem\nMain Challenges with Transformers\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#key-advancements",
    "href": "posts/transformers-book-notes/chapter-1/index.html#key-advancements",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Key Advancements",
    "text": "Key Advancements\n\nAttention Is All You Need\n\npublished in June 2017 by researchers at Google\nintroduced the Transformer architecture for sequence modeling\noutperformed Recurrent Neural Networks (RNNs) on machine translation tasks, both in terms of translation quality and training cost\n\nUniversal Language Model Fine-tuning for Text Classification\n\npublished in January 2018 by Jeremy Howard and Sebastian Ruder\nintroduced an effective training method called ULMFiT\nshowed that training Long Short-Term Memory Networks (LSTMs) on a very large and diverse corpus could produce state-of-the-art text classifiers with little labeled data\ninspired other research groups to combine transformers with unsupervised learning\n\nImproving Language Understanding with Unsupervised Learning\n\npublished by OpenAI in June 2018\nintroduced Generative Pretrained Transformer (GPT)\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\npublished by researchers at Google in October 2018\n\nCombining the Transformer architecture with unsupervised learning removed the need to train task-specific architectures from scratch.\nPretrained Transformers broke almost every benchmark in NLP by a significant margin."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#recurrent-architectures",
    "href": "posts/transformers-book-notes/chapter-1/index.html#recurrent-architectures",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Recurrent Architectures",
    "text": "Recurrent Architectures\n\nRecurrent architectures such as LSTMs were state of the art in Natural Language Processing (NLP) before Transformers.\nRecurrent architectures contain a feedback loop that allows information to propagate from one step to another.\n\nideal for sequential data like text.\n\nA Recurrent Neural Network receives an input token and feeds it through the network.\nThe network outputs a vector called a hidden state, and it passes some information back to itself through a feedback loop.\nThe information passed through the feedback loop allows an RNN to keep track of details from previous steps and use it to make predictions.\nMany still use recurrent architectures for NLP, speech processing, and time-series tasks.\nThe Unreasonable Effectiveness of Recurrent Neural Networks\n\nprovides an overview of RNNs and demonstrates how to train a language model on several datasets\n\nRNNs were critical in developing systems to translate a sequence of words from one language to another.\n\nknown as machine translation\n\nThe computations for recurrent models are inherently sequential and cannot parallelize across the input.\n\nThe inability to parallelize computations is a fundamental shortcoming of recurrent models."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#the-encoder-decoder-framework",
    "href": "posts/transformers-book-notes/chapter-1/index.html#the-encoder-decoder-framework",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "The Encoder-Decoder Framework",
    "text": "The Encoder-Decoder Framework\n\nSequence to Sequence Learning with Neural Networks\n\npublished in 2014 by researchers at Google\n\nAn encoder-decoder is also known as a sequence-to-sequence architecture.\nThis type of architecture is well-suited for situations where the input and output are both sequences of arbitrary length.\nAn encoder encodes information from an input sequence into a numerical representation.\n\nThis numerical representation is often called the last hidden state.\n\nThe decoder uses the numerical representation to generate the output sequence.\nThe encoder and decoder can use any neural network architecture that can model sequences.\nThe final hidden state of the encoder is the only information the decoder has access to when generating the output.\n\nIt has to represent the meaning of the whole input sequence.\nThis requirement creates an information bottleneck that can limit performance for longer sequences."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#attention-mechanisms",
    "href": "posts/transformers-book-notes/chapter-1/index.html#attention-mechanisms",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Attention Mechanisms",
    "text": "Attention Mechanisms\n\nAttention mechanisms allow the decoder to access all of the encoder’s hidden states, not just the last one.\nThe decoder assigns a different amount of weight, called attention, to each state at every decoding time-step.\nThe attention values allow the decoder to prioritize which encoder state to use.\nAttention-based models focus on which input tokens are most relevant at each time step.\n\nThey can learn nontrivial alignments between the words in a generated translation and those in a source sentence.\n\nExample: An attention-based decoder can align the words “zone” and “Area” even when ordered differently in the source sentence and the translation.\n\n\nTransformers use a special kind of attention called self-attention and do not use any form of recurrence.\n\nSelf-attention operates on all states in the same layer of a neural network.\nThe outputs of the self-attention mechanisms serve as input to feed-forward networks.\nThis architecture trains much faster than recurrent models."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#transfer-learning-in-nlp",
    "href": "posts/transformers-book-notes/chapter-1/index.html#transfer-learning-in-nlp",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Transfer Learning in NLP",
    "text": "Transfer Learning in NLP\n\nTransfer learning involves using the knowledge a model learned from a previous task on a new one.\n\nComputer vision models first train on large-scale datasets such as ImageNet to learn the basic features of images before being fine-tuned on a downstream task.\nIt was not initially clear how to perform transfer learning for NLP tasks.\n\nFine-tuned models are typically more accurate than supervised models trained from scratch on the same amount of labeled data.\nWe adapt a pretrained model to a new task by splitting the model into a body and a head.\nThe head is the task-specific portion of the network.\nThe body contains broad features from the source domain learned during training.\nWe can use the body weights to initialize a new model head for a new task.\nTransfer learning typically produces high-quality models that we can efficiently train on many downstream tasks.\nThe ULMFit paper provided a general framework to perform transfer learning with NLP models.\n\nA model first trains to predict the next word based on those preceding it in a large-scale generic corpus to learn the basic features of the language.\n\nThis task is called language modeling.\n\nThe pretrained model then trains on the same task using an in-domain corpus.\nLastly, we fine-tune the model with a classification layer for the target task.\n\nThe ULMFit transfer learning framework provided the missing piece for transformers to take off.\nBoth GPT and BERT combine self-attention with transfer learning and set a new state of the art across many NLP benchmarks.\nGPT only uses the decoder part of the Transformer architecture and the language modeling approach as ULMFiT.\nBERT uses the encoder part of the Transformer architecture and a form of language modeling called masked language modeling.\n\nMasked language modeling requires the model to fill in randomly missing words in a text.\n\nGPT trained on the BookCorpus dataset while BERT trained on the BookCorpus dataset and English Wikipedia.\n\nThe BookCorpus dataset consists of thousands of unpublished books across many genres."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#bridging-the-gap-with-hugging-face-transformers",
    "href": "posts/transformers-book-notes/chapter-1/index.html#bridging-the-gap-with-hugging-face-transformers",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Bridging the Gap With Hugging Face Transformers",
    "text": "Bridging the Gap With Hugging Face Transformers\n\nApplying a novel machine learning architecture to a new application can be complicated and requires custom logic for each model and task.\n\nImplement the model architecture in code.\n\nPyTorch and TensorFlow are the most common frameworks for this.\n\nLoad pretrained weights if available.\nPreprocess the inputs, pass them through the model, and apply task-specific postprocessing.\nImplement data loaders and define loss functions and optimizers to train the model.\n\nCode released by research groups is rarely standardized and often requires days of engineering to adapt to new use cases.\n\nDifferent research labs often release their models in incompatible frameworks, making it difficult for practitioners to port these models to their applications.\n\nHugging Face Transformers provides a standardized interface to a wide range of transformer models, including code and tools to adapt these models to new applications.\n\nThe availability of a standardized interface catalyzed the explosion of research into transformers and made it easy for NLP practitioners to integrate these models into real-life applications.\n\nThe library supports the PyTorch, TensorFlow, and JAX deep learning frameworks and provides task-specific model heads to fine-tune transformers on downstream tasks."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#a-tour-of-transformer-applications",
    "href": "posts/transformers-book-notes/chapter-1/index.html#a-tour-of-transformer-applications",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "A Tour of Transformer Applications",
    "text": "A Tour of Transformer Applications\n\nHugging Face Transformers has a layered API that allows users to interact with the library at various levels of abstraction.\n\n\nPipelines\n\nPipelines abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model.\nHugging Face provides pipelines for several tasks.\nInstantiate a pipeline by calling the pipeline() function and providing the name of the desired task.\n\n'audio-classification'\n'automatic-speech-recognition'\n'feature-extraction'\n'text-classification'\n'token-classification'\n'question-answering'\n'table-question-answering'\n'fill-mask'\n'summarization'\n'translation'\n'text2text-generation'\n'text-generation'\n'zero-shot-classification'\n'conversational'\n'image-classification'\n'object-detection'\n\nThe names for the supported tasks are available in the transformers.pipelines.SUPPORTED_TASKS dictionary.\nThe pipeline automatically downloads the model weights for the selected task and caches them for future use.\nEach pipeline takes a string of text or a list of strings as input and returns a list of predictions.\n\nEach prediction is in a Python dictionary along with the corresponding confidence score.\n\n\nimport transformers\nimport datasets\nimport pandas as pd\nfrom transformers import pipeline\n\ntransformers.logging.set_verbosity_error()\ndatasets.logging.set_verbosity_error()\npipeline\n&lt;function transformers.pipelines.pipeline(task: str, model: Optional = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, use_auth_token: Union[bool, str, NoneType] = None, model_kwargs: Dict[str, Any] = {}, **kwargs) -&gt; transformers.pipelines.base.Pipeline&gt;\nlist(transformers.pipelines.SUPPORTED_TASKS.keys())\n['audio-classification',\n 'automatic-speech-recognition',\n 'feature-extraction',\n 'text-classification',\n 'token-classification',\n 'question-answering',\n 'table-question-answering',\n 'fill-mask',\n 'summarization',\n 'translation',\n 'text2text-generation',\n 'text-generation',\n 'zero-shot-classification',\n 'conversational',\n 'image-classification',\n 'object-detection']\ntransformers.pipelines.TASK_ALIASES\n{'sentiment-analysis': 'text-classification', 'ner': 'token-classification'}\n\nSample Text: Customer Review\ntext = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\nfrom your online store in Germany. Unfortunately, when I opened the package, \\\nI discovered to my horror that I had been sent an action figure of Megatron \\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the \\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning \\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n\n\nText Classification Pipeline\n\nDocumentation\nThe text-classification pipeline supports sentiment analysis, multiclass, and multilabel classification and performs sentiment analysis by default.\n\nclassifier = pipeline(\"text-classification\")\n# Classify the customer review as positive or negative\noutputs = classifier(text)\npd.DataFrame(outputs)    \n\n\n\n\n\n\n\n\nlabel\n\n\nscore\n\n\n\n\n\n\n0\n\n\nNEGATIVE\n\n\n0.901546\n\n\n\n\n\n\n\n\nNamed Entity Recognition Pipeline\n\nDocumentation\nNamed entity recognition (NER) involves extracting real-world objects like products, places, and people from a piece of text.\nDefault Entity Labels\n\nMISC: Miscellaneous\nPER: Person\nORG: Organization\nLOC: Location\n\n\n# Create a named entity recognizer that groups words according to the model's predictions\nner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\nNote: The simple aggregation strategy might end up splitting words undesirably.\nner_tagger.model.config.id2label\n{0: 'O',\n 1: 'B-MISC',\n 2: 'I-MISC',\n 3: 'B-PER',\n 4: 'I-PER',\n 5: 'B-ORG',\n 6: 'I-ORG',\n 7: 'B-LOC',\n 8: 'I-LOC'}\noutputs = ner_tagger(text)\npd.DataFrame(outputs)    \n\n\n\n\n\n\n\n\nentity_group\n\n\nscore\n\n\nword\n\n\nstart\n\n\nend\n\n\n\n\n\n\n0\n\n\nORG\n\n\n0.879010\n\n\nAmazon\n\n\n5\n\n\n11\n\n\n\n\n1\n\n\nMISC\n\n\n0.990859\n\n\nOptimus Prime\n\n\n36\n\n\n49\n\n\n\n\n2\n\n\nLOC\n\n\n0.999755\n\n\nGermany\n\n\n90\n\n\n97\n\n\n\n\n3\n\n\nMISC\n\n\n0.556570\n\n\nMega\n\n\n208\n\n\n212\n\n\n\n\n4\n\n\nPER\n\n\n0.590256\n\n\n##tron\n\n\n212\n\n\n216\n\n\n\n\n5\n\n\nORG\n\n\n0.669693\n\n\nDecept\n\n\n253\n\n\n259\n\n\n\n\n6\n\n\nMISC\n\n\n0.498349\n\n\n##icons\n\n\n259\n\n\n264\n\n\n\n\n7\n\n\nMISC\n\n\n0.775362\n\n\nMegatron\n\n\n350\n\n\n358\n\n\n\n\n8\n\n\nMISC\n\n\n0.987854\n\n\nOptimus Prime\n\n\n367\n\n\n380\n\n\n\n\n9\n\n\nPER\n\n\n0.812096\n\n\nBumblebee\n\n\n502\n\n\n511\n\n\n\n\n\n\nNote: The words Megatron, and Decepticons were split into separate words.\nNote: The ## symbols are produced by the model’s tokenizer.\nner_tagger.tokenizer.vocab_size\n28996\npd.DataFrame(ner_tagger.tokenizer.vocab, index=[0]).T.head(10)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nRees\n\n\n24646\n\n\n\n\nseeded\n\n\n14937\n\n\n\n\nRuby\n\n\n11374\n\n\n\n\nLibraries\n\n\n27927\n\n\n\n\nfoil\n\n\n20235\n\n\n\n\ncollapsed\n\n\n7322\n\n\n\n\nmembership\n\n\n5467\n\n\n\n\nBirth\n\n\n20729\n\n\n\n\nTexans\n\n\n25904\n\n\n\n\nSaul\n\n\n18600\n\n\n\n\n\n\n\n\nQuestion Answering Pipeline\n\nDocumentation\nQuestion answering involves having a model find the answer to a specified question using a given passage of text.\n\nreader = pipeline(\"question-answering\")\nquestion = \"What does the customer want?\"\noutputs = reader(question=question, context=text)\npd.DataFrame([outputs])    \n\n\n\n\n\n\n\n\nscore\n\n\nstart\n\n\nend\n\n\nanswer\n\n\n\n\n\n\n0\n\n\n0.631291\n\n\n335\n\n\n358\n\n\nan exchange of Megatron\n\n\n\n\n\n\nNote: This particular kind of question answering is called extractive question answering. The answer is extracted directly from the text.\n\n\nSummarization Pipeline\n\nDocumentation\nText summarization involves generating a short version of a long passage of text while retaining all the relevant facts.\nTasks requiring a model to generate new text are more challenging than extractive ones.\n\nsummarizer = pipeline(\"summarization\")\n# Limit the generated summary to 45 words\noutputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\nprint(outputs[0]['summary_text'])\n Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\nNote: The model captured the essence of the customer message but directly copied some of the original text.\n\n\nTranslation Pipeline\n\nDocumentation\nThe model generates a translation of a piece of text in the target language.\n\n# Create a translator that translates English to German\n# Override the default model selection\ntranslator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n# Require the model to generate a translation at least 100 words long\noutputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\nprint(outputs[0]['translation_text'])\nSehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt. Anbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, bald von Ihnen zu hören. Aufrichtig, Bumblebee.\nNote: The model supposedly did a good job translating the text. (I don’t speak German.)\n\n\nText Generation Pipeline\n\nDocumentation\nThe model generates new text to complete a provided text prompt.\n\nfrom transformers import set_seed\n# Set the random seed to get reproducible results\nset_seed(42)\ngenerator = pipeline(\"text-generation\")\nresponse = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt = text + \"\\n\\nCustomer service response:\\n\" + response\noutputs = generator(prompt, max_length=200)\nprint(outputs[0]['generated_text'])\nDear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n\nCustomer service response:\nDear Bumblebee, I am sorry to hear that your order was mixed up. The order was completely mislabeled, which is very common in our online store, but I can appreciate it because it was my understanding from this site and our customer service of the previous day that your order was not made correct in our mind and that we are in a process of resolving this matter. We can assure you that your order"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#the-hugging-face-ecosystem",
    "href": "posts/transformers-book-notes/chapter-1/index.html#the-hugging-face-ecosystem",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "The Hugging Face Ecosystem",
    "text": "The Hugging Face Ecosystem\n\nHugging Face Transformers is surrounded by an ecosystem of helpful tools that support the modern machine learning workflow.\nThis ecosystem consists of a family of code libraries and a hub of pretrained model weights, datasets, scripts for evaluation, other resources.\n\n\nThe Hugging Face Hub\n\nThe Hub hosts over 20,000 freely available models plus datasets and scripts for computing metrics.\nModel and dataset cards document the contents of the models and datasets.\nFilters are available for tasks, frameworks, datasets, and more designed to help quickly navigate the Hub.\nUsers can directly try out any model through task-specific widgets.\n\n\n\nHugging Face Tokenizers\n\nDocumentation\nTokenizers split the raw input text into smaller pieces called tokens.\nTokens can be words, parts of words, or single characters.\nHugging Face Tokenizers takes care of all the preprocessing and postprocessing steps, such as normalizing the inputs and transforming the model outputs to the required format.\nThe Tokenizers library uses a Rust backend for fast tokenization.\n\n\n\nHugging Face Datasets\n\nDocumentation\nThe Datasets library provides a standard interface for thousands of datasets to simplify loading, processing, and storing datasets.\nSmart caching removes the need to perform preprocessing steps each time your run your code.\nMemory mapping helps avoid RAM limitations by storing the contents of a file in virtual memory and enables multiple processes to modify the file more efficiently.\nThe library is interoperable with frameworks like Pandas and NumPy.\nScripts are available for many metrics to help make experiments more reproducible and trustworthy.\n\n\n\nHugging Face Accelerate\n\nDocumentation\nThe Accelerate library adds a layer of abstraction to training loops, which takes care of all the custom logic necessary for the training infrastructure."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#main-challenges-with-transformers",
    "href": "posts/transformers-book-notes/chapter-1/index.html#main-challenges-with-transformers",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "Main Challenges with Transformers",
    "text": "Main Challenges with Transformers\n\nLanguage\n\nIt is hard to find pretrained models for languages other than English.\n\n\n\nData Availability\n\nEven with transfer learning, transformers still need a lot of data compared to humans to perform a task.\n\n\n\nWorking With Long Documents\n\nSelf-attention becomes computationally expensive when working on full-length documents.\n\n\n\nOpacity\n\nIt is hard or impossible to determine precisely why a model made a given prediction.\n\n\n\nBias\n\nBiases present in the training data imprint into the model."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-1/index.html#references",
    "href": "posts/transformers-book-notes/chapter-1/index.html#references",
    "title": "Notes on Transformers Book Ch. 1",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nNext: Notes on Transformers Book Ch. 2"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html",
    "href": "posts/transformers-book-notes/chapter-10/index.html",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "",
    "text": "Training Transformers from Scratch\nProject: Python Source Code Generator\nLarge Datasets and Where to Find Them\nBuilding a Tokenizer\nTraining a Model from Scratch\nResults and Analysis\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#training-transformers-from-scratch",
    "href": "posts/transformers-book-notes/chapter-10/index.html#training-transformers-from-scratch",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Training Transformers from Scratch",
    "text": "Training Transformers from Scratch\n\nEfficiently training large models from scratch requires special tools for distributed training."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#project-python-source-code-generator",
    "href": "posts/transformers-book-notes/chapter-10/index.html#project-python-source-code-generator",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Project: Python Source Code Generator",
    "text": "Project: Python Source Code Generator\n\nThe goal is to train a GPT-like model to generate Python source code.\n\n\nExisting AI Code Completion Products\n\nGitHub Copilot\nTabNine\nKite\n\n\n\nCodeParrot\n\nGitHub Repository\nCodeParrot is a GPT-2 model trained from scratch on Python code."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#large-datasets-and-where-to-find-them",
    "href": "posts/transformers-book-notes/chapter-10/index.html#large-datasets-and-where-to-find-them",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Large Datasets and Where to Find Them",
    "text": "Large Datasets and Where to Find Them\n\nMany domains often have large amounts of data available such as legal documents, biomedical databases, and programming codebases.\nLarge datasets can usually only be labeled using heuristics or accompanying metadata.\nWe can still use large unlabeled datasets to fine-tune language models for domain adaptation.\nUsing a pretrained model forces you to use the model’s corresponding tokenizer.\nUsing a tokenizer trained on a corpus from a different domain is typically suboptimal.\n\n\nChallenges of Building a Large-Scale Corpus\n\nThe model will inherit any defects in the pretraining corpus.\nIt becomes more difficult to control or fully understand the contents of a dataset the larger it gets.\nMost exceedingly large datasets are not handcrafted.\nCreating large-scale datasets typically requires using data generated as a side effect of other activities.\nThe high degree of automation used to create large-scale datasets means there is limited control over the content and the method to create them.\nThere is an increased risk of training a model on lower-quality and biased data.\nA significant portion of the C4 corpus used to train T5 is machine-translated rather than human-translated.\nThe stopword filtering in C4 disproportionately removed African-American English from the corpus.\nIt is challenging to find a middle ground between including too much explicit content and erasing all mention of sexuality or gender.\nCommon words like “sex” are absent from C4.\nThere are many copyright violations in the Bookcorpus dataset used to train BERT.\nBookcorpus also contains genre-skew toward “romance” novels.\nAligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\nAddressing “Documentation Debt” in Machine Learning Research: A Retrospective Datasheet for BookCorpus\n\n\n\nCompare text generations from GPT and GPT-2\n\nThe original GPT model trained predominately on BookCorpus.\nGPT-2 trained on web pages, blogs, and news articles linked from Reddit.\n\n\nfrom transformers import pipeline, set_seed\n\nInitialze text generation pipelines with the original GPT and GPT-2\ngeneration_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\ngeneration_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")\nNote: The main difference between the two models is the pretraining dataset.\n\nCompare the model sizes\ndef model_size(model):\n    return sum(t.numel() for t in model.parameters())\n\nprint(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\nprint(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")\n    GPT  size: 116.5M parameters\n    GPT2 size: 124.4M parameters\nNote: The original GPT model is approximately the same size as the smallest GPT-2 variant.\n\nReset random seed\nset_seed(1)\n\nDefine a function to generate text using a prompt\ndef enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n    out = pipe(prompt, num_return_sequences=num_return_sequences,\n               clean_up_tokenization_spaces=True)\n    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n\nCompare the output of the two models\nprompt = \"\\nWhen they came back\"\nprint(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\nprint(\"\")\nprint(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))\n    GPT completions:\n    1.\n    When they came back. \n     \" we need all we can get, \" jason said once they had settled into the back of the truck without anyone stopping them. \" after getting out here, it 'll be up to us what to find. for now\n    2.\n    When they came back. \n     his gaze swept over her body. he 'd dressed her, too, in the borrowed clothes that she 'd worn for the journey. \n     \" i thought it would be easier to just leave you there. \" a woman like\n    3.\n    When they came back to the house and she was sitting there with the little boy. \n     \" don't be afraid, \" he told her. she nodded slowly, her eyes wide. she was so lost in whatever she discovered that tom knew her mistake\n    \n    GPT-2 completions:\n    1.\n    When they came back we had a big dinner and the other guys went to see what their opinion was on her. I did an hour and they were happy with it.\n    2.\n    When they came back to this island there had been another massacre, but he could not help but feel pity for the helpless victim who had been left to die, and that they had failed that day. And so was very, very grateful indeed.\n    3.\n    When they came back to our house after the morning, I asked if she was sure. She said, \"Nope.\" The two kids were gone that morning. I thought they were back to being a good friend.\n    \n    When Dost\nNote:\n\nThe text generated with the original GPT model has a distinctive romance skew.\nGPT-2 generates more neutral text containing blog-like or adventure-related elements.\nA model reflects the language bias and over or underrepresentation of populations of the dataset used to train it.\nWe need to consider the model’s biases concerning the target audience.\nTowards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure\n\n\n\n\nBuilding a Custom Code Dataset\n\nWe can obtain a pretraining corpus of Python code from GitHub repositories.\nWe can access GitHub repositories via the GitHub REST API or public dataset inventories like Google BigQuery.\nThe GitHub REST API is rate limited but provides access to additional attributes like star and downstream usage information.\nThe Libraries.io service monitors open source packages.\n\n\nbigquery-public-data.github_repos.contents table\n\nThe bigquery-public-data.github_repos.contents table contains copies of all ASCII files less than 10MB in size.\n\n\n\nCodeSearchNet corpus\n\nThe CodeSearchNet corpus contains 2 million comment-code pairs from open-source libraries hosted on GitHub.\nIt contains code and documentation for several programming languages.\nHugging Face Dataset Card\n\n\n\nCreating a dataset with Google BigQuery\n\nUnsupervised Translation of Programming Languages\n\nSteps to export Python files\n\nCreate a Google Cloud account.\nCreate a Google BigQuery project under your account.\nCreate a dataset inside the project.\nCreate a table in the dataset to store the results of the SQL request.\nPrepare the following SQL query and specify a destination table\n\nSELECT \n    f.repo_name, f.path, c.copies, c.size, c.content, l.license\nFROM\n    `bigquery-public-data.github_repos.files` AS f\nJOIN\n    `bigquery-public-data.github_repos.contents` AS c\nON\n    f.id = c.id\nJOIN\n    `bigquery-public-data.github_repos.licenses` as l\nON\n    f.repo_name = l.repo_name\nWHERE\n    NOT c.binary\n    AND ((F.path LIKE '%.py')\n        AND (c.size BETWEEN 1024 and 1048575))\n\nRun the query\n\nNote: Encoutered the following error when attempting to run the query\nQuota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas \n\nThe above command processes about 2.6TB of data to extract 26.8 million files.\nThe resulting dataset contains about 50 GB of compressed JSON files.\nThe dataset is about 200GB when uncompressed.\nEach JSON file contains source code from Python files.\nThe query filters empty files like __init__.py files and files larger than 1MB.\nThe query includes the licenses for the files so we can filter the training data later on.\n\nSteps to download results from Google Cloud\n\nExport results to Google Cloud\n\nCreate a bucket and a folder in Google Cloud Storage (GCS).\nExport your table to this bucket by selecting Export &gt; Export to GCS, with a JSON export format and gzip compression.\n\nDownload the bucket to your local machine using gsutil\n\n\nInstall gsutil with pip install gsutil.\n\nConfigure gsutil with your Google account: gsutil config.\nCopy your bucket on your machine:\n\ngsutil -m -o \"GSUtil:parallel_process_count=1\" cp -r gs://&lt;name_of_bucket&gt;\n\nAlternative: Download the dataset from Hugging Face Hub\ngit clone https://huggingface.co/datasets/transformersbook/codeparrot\n\n\n\nTo Filter the Noise or Not?\n\nData preparation is crucial, and we should clean the dataset as much as possible.\nThe quality of code in GitHub repositories varies greatly.\nHaving some noise in the training dataset makes our code generation system robust to noisy inputs at inference time but also makes predictions more random.\nThe intended use case and whole-system integration determine whether you want more or less noisy data and add pre and post-filtering operations.\n\n\nPotential steps to clean dataset\n\nFilter code based on stars or usage information.\nCode with more stars or higher usage is more likely to be higher quality.\nRemove duplicated code samples.\nConsider copyright information.\nInvestigate the language used in the documentation, comments, or docstrings.\nRemove personal identifying information such as passwords or keys.\n\n\n\n\nWorking with Large Datasets\n\nWorking with large datasets requires additional considerations regarding disk space and RAM usage.\nIt is common for datasets to be larger than the available RAM.\nThe Hugging Face Datasets library provides memory mapping and streaming functionality to address RAM and disk space limitations.\n\n\nMemory mapping\n\nHugging Face Datasets uses a mechanism for zero-copy and zero-overhead memory mapping.\nThe mechanism caches each dataset in a file that directly reflects the content in RAM.\nHugging Face Datasets opens a read-only pointer to this file and uses it as a substitute for RAM.\n\n\nfrom datasets import load_dataset, DownloadConfig\n\nDecompress and load the downloaded dataset from the local folder\n\nNote: The following code block assumes that you have downloaded the BigQuery dataset to a folder called codeparrot. We suggest skipping this step since it will unpack the compressed files and require ~180GB of disk space. This code is just for demonstration purposes and you can just continue below with the streamed dataset which will not consume that much disk space.\n\n\ndownload_config = DownloadConfig(delete_extracted=True, cache_dir=\"/mnt/980SSD/Datasets/codeparrot-cache\")\ndataset = load_dataset(\"/mnt/980SSD/Datasets/codeparrot\", cache_dir=\"/mnt/980SSD/Datasets/codeparrot-cache\", split=\"train\",\n                       download_config=download_config)\n    Dataset json downloaded and prepared to /mnt/980SSD/Datasets/codeparrot-cache/json/codeparrot-43fc192cc9f62326/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nNote:\n\nThe delete_extracted=True argument deletes the extracted files to free up disk space.\nThe Hugging Face Datasets library extracted and read the compressed JSON files by loading them in a single optimized cache file.\n\n\nimport psutil, os\n\nCheck the size of the cached dataset\nprint(f\"Number of python files code in dataset : {len(dataset)}\")\nds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n# os.stat.st_size is expressed in bytes, so we convert to GB\nprint(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n# Process.memory_info is expressed in bytes, so we convert to MB\nprint(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss &gt;&gt; 20} MB\")\n    Number of python files code in dataset : 18695559\n    Dataset size (cache file) : 183.68 GB\n    RAM used: 4359 MB\nNote:\n\nThe dataset is much larger than the available RAM, but we can still load and access it.\nNLP data is typically lightweight to load compared to the model processing computations.\nThe zero-copy/zero-overhead format uses Apache Arrow under the hood for efficiency.\n\n\n\n\nStreaming\n\nSome datasets are too large to fit in most hard drives.\nThe Hugging Face Datasets library supports streaming many compressed and uncompressed file formats that we can read line-by-line.\nHugging Face Datasets opens and reads compressed JSON files on the fly in streaming mode.\nStreamed datasets are of the type IterableDataset.\nWe cannot access random elements and need to read them in order.\nMethods like shuffle() operate by fetching a buffer of examples and shuffling within this buffer.\nThe samples of a streamed dataset are identical to those of a nonstreamed dataset.\nStreamed datasets do not generate a cache file on the drive or require significant RAM.\nIndividual batches load into memory as requested, reducing the memory footprint.\nWe can also stream remote datasets from the Hugging Face Hub, allowing us to use arbitrarily large datasets on small servers.\n\n\nstreamed_dataset = load_dataset(\"/mnt/980SSD/Datasets/codeparrot\", split=\"train\", streaming=True)\n    AttributeError: '_io.BufferedReader' object has no attribute 'loc'\n\nIterate through the streamed dataset\niterator = iter(streamed_dataset)\n\nprint(dataset[0] == next(iterator))\nprint(dataset[1] == next(iterator))\n\nStream a remote dataset\nremote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n                              streaming=True)\n\n\n\n\nAdding Datasets to the Hugging Face Hub\n\nPushing our dataset to the Hugging Face Hub allows us to access it from a training server and share it with the community.\n\n\nCommand Line Steps\n\nLog into Hugging Face account\n\nhuggingface-cli login\n\nCreate a new dataset repository on the Hub for the training split\n\nhuggingface-cli repo create --type dataset codeparrot-train\n\nCreate a new dataset repository on the Hub for the validation split\n\nhuggingface-cli repo create --type dataset codeparrot-valid\n\nClone the training repository\n\nhuggingface-cli repo create --type dataset codeparrot-train\n\nClone the validation repository\n\nhuggingface-cli repo create --type dataset codeparrot-valid\n\nCopy all but the last GitHub file to the as the training set\n\ncd codeparrot-train\ncp ../codeparrot/*.json.gz .\nrm ./file-000000000183.json.gz\n\nCommit the files and push them to the Hub\n\ngit add .\ngit commit -m \"Adding dataset files\"\ngit push\n\nRepeat the process for the validation set\n\ncd ../codeparrot-valid\ncp ../codeparrot/file-000000000183.json.gz\nmv ./file-000000000183.json.gz ./file-000000000183_validation.json.gz\ngit add .\ngit commit -m \"Adding dataset files\"\ngit push\n\nIt is good practice to add README cards that explain how the datasets were created and provide as much helpful information as possible.\nA well-documented dataset is more likely to be valuable to other people, including the future you.\nHugging Face Dataset Card Creation Guide"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#building-a-tokenizer",
    "href": "posts/transformers-book-notes/chapter-10/index.html#building-a-tokenizer",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Building a Tokenizer",
    "text": "Building a Tokenizer\n\nIt is crucial to stick with the same preprocessing design choices used during the pretraining process when using a pretrained model.\nUsing a tokenizer prepared for another dataset when training a new model can be suboptimal.\n\nThe T5 tokenizer uses extensive stopword filtering and is unaware of some common English words like “sex.”\nThe CamemBERT tokenizer is only trained on French text and is unaware of common English words such as “being.”\n\n\n\nfrom transformers import AutoTokenizer\n\ndef tok_list(tokenizer, string):\n    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n    return [tokenizer.decode(tok) for tok in input_ids]\nInitialize tokenizers using the pretrained T5 and CamemBERT model vocabularies\ntokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\ntokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n\nTest the limitations of the T5 and CamemBERT tokenizers\nprint(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\nprint(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')\n    T5 tokens for \"sex\": ['', 's', 'ex']\n    CamemBERT tokens for \"being\": ['be', 'ing']\nNote:\n\nSplitting such short and common words into subparts is often inefficient as it increases the sequence length of the model.\nIt is essential to consider the domain and the preprocessing of the dataset used to train a tokenizer.\nThe tokenizer and model can encode bias from the dataset that impacts the downstream behavior of the model.\n\n\n\nThe Tokenizer Model\n\nTraining a tokenizer is a way to create an optimal mapping from a string of text to a list of integers that the model can ingest.\nThe optimal string-to-integer conversion involves a vocabulary consisting of a list of atomic strings and an associated method to convert, normalize, cut, or map a text string into a list of indices with this vocabulary.\nThe list of indices is the input for the neural network.\nThe tokenizer processing pipeline involves normalization, pre-tokenization, the tokenizer model, and postprocessing.\nThe tokenizer model trains on a corpus.\nSeveral subword tokenization algorithms are available, such as BPE, WordPiece, and Unigram.\nBPE starts from a list of single characters and creates a vocabulary by progressively creating new tokens formed by merging the most frequently co-occurring basic units and adding them to the list.\nThis process continues until we reach the predefined vocabulary size.\nUnigram initializes its base vocabulary with all the words in the corpus and potential subwords and progressively removes or splits the less helpful tokens until it reaches the target vocab size.\nThe impact of the chosen tokenization algorithm on downstream performance varies based on the task.\nIt is difficult to identify if one algorithm is better than the others.\nBoth BPE and Unigram perform reasonably well in most cases.\n\n\n\nMeasuring Tokenizer Performance\n\nIt is challenging to measure a tokenizer’s optimality and performance in practice.\nSubword fertility calculates the average number of subwords produced per tokenized word.\nThe proportion of continued words refers to the amount of tokenized words in a corpus split into at least two subtokens.\nCoverage metrics track information like the proportion of unknown words or rarely used tokens in a tokenized corpus.\nWe often estimate the robustness to misspelling or noise and model performance on such out-of-domain examples.\nThese measures provide different views on tokenizer performance.\nHowever, they tend to ignore the interaction of the tokenizer with the model.\nThe best way to evaluate tokenizers is using the downstream performance of the model.\n\n\n\nA Tokenizer for Python\n\nUsing a natural language pre-tokenizer for Python code might be suboptimal.\nIndentation has semantic meaning in Python code.\nSplitting on all whitespaces and removing them would remove valuable indentation information.\nLine breaks are not meaningful in Python code, and we can remove them without issue.\nUnderscores can be part of single variable names and would not to use for splitting text.\nByte-level tokenizers preserve spaces and might be a good candidate for tokenizing code.\nPython has a built-in tokenize module that splits Python code strings into meaningful units.\n\nThis approach is slow since it is Python-based and limited by the Python global interpreter lock (GIL).\n\nMost tokenizers provided by the Hugging Face Tokenizers library are in Rust and many orders of magnitude faster to train and use.\n\n\nfrom transformers import AutoTokenizer\n\nTest the byte-level GPT-2 tokenizer on Python code\npython_code = r\"\"\"def say_hello():\n    print(\"Hello, World!\")\n# Print it\nsay_hello()\n\"\"\"\npython_code\n    'def say_hello():\\n    print(\"Hello, World!\")\\n# Print it\\nsay_hello()\\n'\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\npd.DataFrame(tokenizer(python_code).tokens()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n\n\n\n\n0\n\n\ndef\n\n\nĠsay\n\n\n_\n\n\nhello\n\n\n():\n\n\nĊ\n\n\nĠ\n\n\nĠ\n\n\nĠ\n\n\nĠprint\n\n\n(“\n\n\nHello\n\n\n,\n\n\nĠWorld\n\n\n!“\n\n\n)\n\n\nĊ\n\n\n#\n\n\nĠPrint\n\n\nĠit\n\n\nĊ\n\n\nsay\n\n\n_\n\n\nhello\n\n\n()\n\n\nĊ\n\n\n\n\n\n\n\nInspect the normalization step\nprint(tokenizer.backend_tokenizer.normalizer)\n    None\nNote: The GPT-2 tokenizer does not use normalization and works directly on raw Unicode inputs.\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nInspect the pre-tokenization step\npd.DataFrame(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\ndef\n\n\n(0, 3)\n\n\n\n\n1\n\n\nĠsay\n\n\n(3, 7)\n\n\n\n\n2\n\n\n_\n\n\n(7, 8)\n\n\n\n\n3\n\n\nhello\n\n\n(8, 13)\n\n\n\n\n4\n\n\n():\n\n\n(13, 16)\n\n\n\n\n5\n\n\nĊĠĠĠ\n\n\n(16, 20)\n\n\n\n\n6\n\n\nĠprint\n\n\n(20, 26)\n\n\n\n\n7\n\n\n(“\n\n\n(26, 28)\n\n\n\n\n8\n\n\nHello\n\n\n(28, 33)\n\n\n\n\n9\n\n\n,\n\n\n(33, 34)\n\n\n\n\n10\n\n\nĠWorld\n\n\n(34, 40)\n\n\n\n\n11\n\n\n!“)\n\n\n(40, 43)\n\n\n\n\n12\n\n\nĊ\n\n\n(43, 44)\n\n\n\n\n13\n\n\n#\n\n\n(44, 45)\n\n\n\n\n14\n\n\nĠPrint\n\n\n(45, 51)\n\n\n\n\n15\n\n\nĠit\n\n\n(51, 54)\n\n\n\n\n16\n\n\nĊ\n\n\n(54, 55)\n\n\n\n\n17\n\n\nsay\n\n\n(55, 58)\n\n\n\n\n18\n\n\n_\n\n\n(58, 59)\n\n\n\n\n19\n\n\nhello\n\n\n(59, 64)\n\n\n\n\n20\n\n\n()\n\n\n(64, 66)\n\n\n\n\n21\n\n\nĊ\n\n\n(66, 67)\n\n\n\n\n\n\nNote:\n\nHugging Face Tokenizers provides an offset tracking feature for switching between strings and tokens.\nHugging Face Tokenizers tracks all operations on the input string so that it is possible to know what part of the input string corresponds to a token after tokenization.\nThe numbers in the above output indicate where each token originates in the original string.\nThe word “hello” corresponds to the characters 8 to 13 in the original string.\nEach Unicode character is composed of between 1 and 4 bytes.\nThere are 143,859 Unicode characters and 256 elements in the byte alphabet.\nWe can express each Unicode character as a sequence of bytes.\nWe can have a model using an alphabet of only 256 words and process any Unicode string.\n\n\nCheck the representations of some Unicode characters\na, e = u\"a\", u\"€\"\nbyte = ord(a.encode(\"utf-8\"))\nprint(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\nbyte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\nprint(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')\n    `a` is encoded as `b'a'` with a single byte: 97\n    `€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\nNote:\n\nBuilding our vocabulary from the 143,859 Unicode characters would make the model’s embedding layer extremely large.\nUsing only the 256 byte-values as the vocabulary would result in longer input sequences.\n\nByT5: Towards a token-free future with pre-trained byte-to-byte models\n\nThe ByT5 paper provides a details study of the overhead from using byte values for our vocabulary.\n\n\nThe BPE algorithm constructs a medium-sized vocabulary by extending the 256 byte-values with the most common combinations of bytes.\nThe name, Byte-Pair Encoding, comes from a data compression technique proposed by Philip Gage in 1994, which operated on bytes.\n\nA New Algorithm for Data Compression Optimization\n\nStandard BPE algorithms in NLP typically operate on Unicode strings rather than bytes.\n\nA recent type of BPE that works specifically on bytes is called byte-level BPE.\n\nThe BPE algorithms are designed to work with clean Unicode strings as inputs, not bytes, and expect regular ASCII characters in the inputs without spaces or control characters.\nMany Unicode control characters correspond to the 256 first bytes.\nThe GPT-2 tokenizer maps all the 256 input bytes to printable Unicode characters, which the BPE algorithms can digest.\n\n\nfrom transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n\nInspect the GPT-2 mapping of bytes to Unicode characters\nbyte_to_unicode_map = bytes_to_unicode()\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\nbase_vocab = list(unicode_to_byte_map.keys())\n\nprint(f'Size of our base vocabulary: {len(base_vocab)}')\nprint(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')\nSize of our base vocabulary: 256\nFirst element: `!`, last element: `Ń`\n\nExamples of character mappings in BPE\nbyte_to_unicode_map = bytes_to_unicode()\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\nbase_vocab = list(unicode_to_byte_map.keys())\n\nexamples = [\n    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n]\n\npd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])\n\n\n\n\n\n\n\n\nDescription\n\n\nCharacter\n\n\nBytes\n\n\nMapped bytes\n\n\n\n\n\n\n0\n\n\nRegular characters\n\n\na and ?\n\n\n97 and 63\n\n\na and ?\n\n\n\n\n1\n\n\nNonprintable control character (carriage return)\n\n\nU+000D\n\n\n13\n\n\nč\n\n\n\n\n2\n\n\nA space\n\n\n\n\n\n32\n\n\nĠ\n\n\n\n\n3\n\n\nA nonbreakable space\n\n\n\\xa0\n\n\n160\n\n\nł\n\n\n\n\n4\n\n\nA newline character\n\n\n\\n\n\n\n10\n\n\nĊ\n\n\n\n\n\n\n\nInspect the pre-tokenization step again\npd.DataFrame(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\ndef\n\n\n(0, 3)\n\n\n\n\n1\n\n\nĠsay\n\n\n(3, 7)\n\n\n\n\n2\n\n\n_\n\n\n(7, 8)\n\n\n\n\n3\n\n\nhello\n\n\n(8, 13)\n\n\n\n\n4\n\n\n():\n\n\n(13, 16)\n\n\n\n\n5\n\n\nĊĠĠĠ\n\n\n(16, 20)\n\n\n\n\n6\n\n\nĠprint\n\n\n(20, 26)\n\n\n\n\n7\n\n\n(“\n\n\n(26, 28)\n\n\n\n\n8\n\n\nHello\n\n\n(28, 33)\n\n\n\n\n9\n\n\n,\n\n\n(33, 34)\n\n\n\n\n10\n\n\nĠWorld\n\n\n(34, 40)\n\n\n\n\n11\n\n\n!“)\n\n\n(40, 43)\n\n\n\n\n12\n\n\nĊ\n\n\n(43, 44)\n\n\n\n\n13\n\n\n#\n\n\n(44, 45)\n\n\n\n\n14\n\n\nĠPrint\n\n\n(45, 51)\n\n\n\n\n15\n\n\nĠit\n\n\n(51, 54)\n\n\n\n\n16\n\n\nĊ\n\n\n(54, 55)\n\n\n\n\n17\n\n\nsay\n\n\n(55, 58)\n\n\n\n\n18\n\n\n_\n\n\n(58, 59)\n\n\n\n\n19\n\n\nhello\n\n\n(59, 64)\n\n\n\n\n20\n\n\n()\n\n\n(64, 66)\n\n\n\n\n21\n\n\nĊ\n\n\n(66, 67)\n\n\n\n\n\n\nNote:\n\nConsecutive spaces count as a single word.\nEach space preceding a word is attached to and considered part of the following word.\n\n\nCheck the size of the GPT-2 vocabulary\nprint(f\"Size of the vocabulary: {len(tokenizer)}\")\n    Size of the vocabulary: 50257\nNote: The GPT-2 vocabulary consists of the base vocabulary with the 256 values of the bytes, 50,000 additional tokens created by repeatedly merging the most commonly occurring tokens, and a special character to represent document boundaries.\n\nRun the GPT-2 tokenizer pipeline again\npd.DataFrame(tokenizer(python_code).tokens()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n\n\n\n\n0\n\n\ndef\n\n\nĠsay\n\n\n_\n\n\nhello\n\n\n():\n\n\nĊ\n\n\nĠ\n\n\nĠ\n\n\nĠ\n\n\nĠprint\n\n\n(“\n\n\nHello\n\n\n,\n\n\nĠWorld\n\n\n!“\n\n\n)\n\n\nĊ\n\n\n#\n\n\nĠPrint\n\n\nĠit\n\n\nĊ\n\n\nsay\n\n\n_\n\n\nhello\n\n\n()\n\n\nĊ\n\n\n\n\n\n\nNote:\n\nThe tokenizer keeps most of the words but splits indentations into several consecutive spaces.\nThe training corpus for the tokenizer mostly contained text where consecutive spaces are rare.\nThe BPE model does not include a specific token for indentation, meaning it is not well suited for Python code.\n\n\n\n\nTraining a Tokenizer\n\nA tokenizer learns which letter combinations are the most frequent in a target corpus.\nThe corpus does not need to be very large, just representative of the target domain.\nWe can train a tokenizer on a target corpus using the tokenizer.train_new_from_iterator() method.\nWe need to specify a target vocab size and prepare an iterator to supply lists of input strings.\nThe tokenizer might store unusual character sequences depending on the vocab size and the exact texts in the corpus.\n\nCheck the longest words in the GPT-2 tokenizer vocabulary\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\npd.DataFrame([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]).style.hide(axis='columns')\n\n\n\n\n\n\n\n\n0\n\n\nÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\n\n\n\n\n1\n\n\n=================================================================\n\n\n\n\n2\n\n\n—————————————————————-\n\n\n\n\n3\n\n\n================================================================\n\n\n\n\n4\n\n\n________________________________________________________________\n\n\n\n\n5\n\n\n—————————————————————-\n\n\n\n\n6\n\n\nÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\n\n\n\n\n7\n\n\n……………………………………………………….\n\n\n\n\n\n\nNote: These tokens look like separator lines used on forums.\n\nCheck the least frequent words\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\npd.DataFrame([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n&lt;|endoftext|&gt;\n\n\n\n\n1\n\n\ngazed\n\n\n\n\n2\n\n\ninformants\n\n\n\n\n3\n\n\nCollider\n\n\n\n\n4\n\n\nregress\n\n\n\n\n5\n\n\nominated\n\n\n\n\n6\n\n\namplification\n\n\n\n\n7\n\n\nCompar\n\n\n\n\n8\n\n\n….”\n\n\n\n\n9\n\n\n(/\n\n\n\n\n10\n\n\nCommission\n\n\n\n\n11\n\n\nHitman\n\n\n\n\n\n\nNote:\n\nThe &lt;|endoftext|&gt; token specifies the end of a text sequence and is not from the training corpus.\nThe model has to learn an associated word embedding for each token.\nThis tokenizer embeds some highly time and space-specific knowledge of the world by granting these words separate tokens.\nOverly specific tokens can indicate the target vocab size is too large or that the corpus contains peculiar tokens.\nWe don’t want the embedding matrix to contain too many noisy words.\n\n\nfrom tqdm.auto import tqdm\n\nTrain a fresh tokenizer on 100,000 documents\nlength = 100000\ndataset_name = 'transformersbook/codeparrot-train'\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\niter_dataset = iter(dataset)\n\ndef batch_iterator(batch_size=10):\n    for _ in tqdm(range(0, length, batch_size)):\n        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n\nnew_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n                                                  vocab_size=12500,\n                                                  initial_alphabet=base_vocab)\n\nExamine the first tokens added by the BPE algorithm\ntokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\npd.DataFrame([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\nse\n\n\nin\n\n\n\n\nre\n\n\non\n\n\nte\n\n\n\n\n\n\n\n\nor\n\n\nst\n\n\nde\n\n\n\n\n\nth\n\n\nle\n\n\n=\n\n\nlf\n\n\nself\n\n\nme\n\n\nal\n\n\n\n\n\n\nNote:\n\nThere are various standard levels of indentation and whitespace tokens and short common Python keywords.\nThe BPE algorithm is working as intended.\n\n\nExamine the last tokens added by the BPE algorithm\npd.DataFrame([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n\n\n\n\n0\n\n\ncapt\n\n\nembedded\n\n\nregarding\n\n\nBundle\n\n\n355\n\n\nrecv\n\n\ndmp\n\n\nvault\n\n\nMongo\n\n\npossibly\n\n\nimplementation\n\n\nMatches\n\n\n\n\n\n\nNote:\n\nThere are still some relatively common words like the recv method.\nThere are also some more noisy words potentially from comments.\n\n\nTest the custom tokenizer on the sample code\npd.DataFrame(new_tokenizer(python_code).tokens()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n\n\n\n\n0\n\n\ndef\n\n\nĠs\n\n\nay\n\n\n_\n\n\nhello\n\n\n():\n\n\nĊĠĠĠ\n\n\nĠprint\n\n\n(“\n\n\nHello\n\n\n,\n\n\nĠWor\n\n\nld\n\n\n!“)\n\n\nĊ\n\n\n#\n\n\nĠPrint\n\n\nĠit\n\n\nĊ\n\n\ns\n\n\nay\n\n\n_\n\n\nhello\n\n\n()\n\n\nĊ\n\n\n\n\n\n\nNote: The tokenize splits common English words like “World” and “say.”\n\nimport keyword\n\n\nkeyword\n\nDocumentation\nDetermine if a string is a keyword or soft keyword.\n\nCheck if all the Python reserved words are in the vocabulary\nprint(f'There are in total {len(keyword.kwlist)} Python keywords.')\nfor keyw in keyword.kwlist:\n    if keyw not in new_tokenizer.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')\n    There are in total 36 Python keywords.\n    No, keyword `__peg_parser__` is not in the vocabulary\n    No, keyword `await` is not in the vocabulary\n    No, keyword `finally` is not in the vocabulary\n    No, keyword `nonlocal` is not in the vocabulary\nNote: Several frequent keywords like “finally” are not in the vocabulary.\n\nReset random seed\nset_seed(1)\n\nTrain a tokenizer using a larger target vocab size and dataset sample\nlength = 200000\nnew_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n    vocab_size=32768, initial_alphabet=base_vocab)\n\nCheck the last tokens added\ntokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n                reverse=False)\npd.DataFrame([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n\n\n\n\n0\n\n\n组\n\n\ntypically\n\n\nARGIN\n\n\nTermination\n\n\nStaticText\n\n\ninteresting\n\n\nCircular\n\n\ncombinatorics\n\n\n)([\n\n\n969\n\n\nEAR\n\n\nGap\n\n\n\n\n\n\nNote: The group of least-frequent tokens does not contain any Python keywords.\n\nTest the new tokenizer on the sample code\npd.DataFrame(new_tokenizer_larger(python_code).tokens()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n\n\n\n\n0\n\n\ndef\n\n\nĠsay\n\n\n_\n\n\nhello\n\n\n():\n\n\nĊĠĠĠ\n\n\nĠprint\n\n\n(“\n\n\nHello\n\n\n,\n\n\nĠWorld\n\n\n!“)\n\n\nĊ\n\n\n#\n\n\nĠPrint\n\n\nĠit\n\n\nĊ\n\n\nsay\n\n\n_\n\n\nhello\n\n\n()\n\n\nĊ\n\n\n\n\n\n\nNote: The new tokenizer keeps the indents in the vocabulary and does not split common English words.\n\nfor keyw in keyword.kwlist:\n    if keyw not in new_tokenizer_larger.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')\n    No, keyword `__peg_parser__` is not in the vocabulary\n    No, keyword `nonlocal` is not in the vocabulary\nNote:\n\nThe new tokenizer vocabulary is still missing a couple of rare Python keywords, neither of which are relevant for most Python code.\nThe __peg_parser__ keyword is an easter egg for the new PEG parser and will not be in Python 3.10.\nThe nonlocal keyword causes listed identifiers to refer to previously bound variables in the nearest enclosing scope, excluding globals.\nThe new tokenizer is more efficient than the standard GPT-2 tokenizer as it uses fewer tokens to encode a given code sample.\n\n\nDisable Tokenizers Parallelism\n%env TOKENIZERS_PARALLELISM=false\n    env: TOKENIZERS_PARALLELISM=false\n\n\n\n\nSaving a Custom Tokenizer on the Hub\nLog into Hugging Face account\nfrom huggingface_hub import notebook_login\nnotebook_login()\n    Login successful\n    Your token has been saved to /home/innom-dt/.huggingface/token\n\nPush custom tokenizer to Hugging Face Hub\nmodel_ckpt = \"codeparrot\"\n# org = \"transformersbook\"\nnew_tokenizer_larger.push_to_hub(model_ckpt)\n    'https://huggingface.co/cj-mills/codeparrot/commit/97c7905ef55cb4139e88f9b9d17225c372fc8f55'\n\nLoad the custom tokenizer from the Hub repository\n# reloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\nreloaded_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\npd.DataFrame(reloaded_tokenizer(python_code).tokens()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n\n\n\n\n0\n\n\ndef\n\n\nĠsay\n\n\n_\n\n\nhello\n\n\n():\n\n\nĊĠĠĠ\n\n\nĠprint\n\n\n(“\n\n\nHello\n\n\n,\n\n\nĠWorld\n\n\n!“)\n\n\nĊ\n\n\n#\n\n\nĠPrint\n\n\nĠit\n\n\nĊ\n\n\nsay\n\n\n_\n\n\nhello\n\n\n()\n\n\nĊ\n\n\n\n\n\n\nPush the smaller tokenizer to Hugging Face Hub\nnew_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\")\n    'https://huggingface.co/cj-mills/codeparrot-small-vocabulary/commit/b4efe8c9692ce772175b97b01cffc9f1924ae706'"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#training-a-model-from-scratch",
    "href": "posts/transformers-book-notes/chapter-10/index.html#training-a-model-from-scratch",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Training a Model from Scratch",
    "text": "Training a Model from Scratch\n\nCodeParrot Trainng Script and Instructions\n\n\nA Tale of Pretraining Objectives\n\nThe large-scale pretraining corpus allows us to tackle several downstream tasks.\nThe selected task will influence which pretraining objective we choose.\n\n\nCausal language modeling\n\nCausal language modeling is a self-supervised approach that does not require annotations.\nCode autocompletion is a directly related downstream task.\nWe can provide a model with the beginning of a code sample and have it generate possible completions.\nA decoder-only architecture like the GPT family is usually best suited for this task.\n\n\n\nMasked language modeling\n\nMasked language modeling (also called denoising) is a self-supervised training objective.\nWe can provide a model with a noisy code sample (e.g., by replacing a code instruction with a random or masked word) and have it reconstruct the original clean sequence.\nMasked language modeling is not directly related to a downstream task like autocompletion, but it is a practical pretraining objective for learning general representations.\nWe can combine masked language modeling with fine-tuning the model on a downstream task.\nEncoder architectures are best suited to this pretraining objective.\n\n\n\nSequence-to-sequence training\n\nSequence-to-sequence training is a supervised learning objective where one category serves as input while another serves as labels.\nWe can use a heuristic like regular expressions to separate comments or docstrings from code and build a large-scale annotated dataset of code-comment pairs.\nWe can then use this dataset to train a model to transcript comments in code or vice versa.\nDocument generation from code and code generation from comments are directly-related downstream tasks.\nEncoder decoder architectures are best suited to sequence-to-sequence objectives.\n\n\n\n\nInitializing the Model\n\nNOTE: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you use the small checkpoint by replacing the configuration with config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer)).\n\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\nInstantiate a tokenizer using the custom checkpoint\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nStart with the hyperparameters for training the 1.5 billion-parameter GPT-2 variant\nconfig = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\nconfig\n    GPT2Config {\n      \"_name_or_path\": \"gpt2-xl\",\n      \"activation_function\": \"gelu_new\",\n      \"architectures\": [\n        \"GPT2LMHeadModel\"\n      ],\n      \"attn_pdrop\": 0.1,\n      \"bos_token_id\": 50256,\n      \"embd_pdrop\": 0.1,\n      \"eos_token_id\": 50256,\n      \"initializer_range\": 0.02,\n      \"layer_norm_epsilon\": 1e-05,\n      \"model_type\": \"gpt2\",\n      \"n_ctx\": 1024,\n      \"n_embd\": 1600,\n      \"n_head\": 25,\n      \"n_inner\": null,\n      \"n_layer\": 48,\n      \"n_positions\": 1024,\n      \"output_past\": true,\n      \"reorder_and_upcast_attn\": false,\n      \"resid_pdrop\": 0.1,\n      \"scale_attn_by_inverse_layer_idx\": false,\n      \"scale_attn_weights\": true,\n      \"summary_activation\": null,\n      \"summary_first_dropout\": 0.1,\n      \"summary_proj_to_labels\": true,\n      \"summary_type\": \"cls_index\",\n      \"summary_use_proj\": true,\n      \"task_specific_params\": {\n        \"text-generation\": {\n          \"do_sample\": true,\n          \"max_length\": 50\n        }\n      },\n      \"transformers_version\": \"4.18.0\",\n      \"use_cache\": true,\n      \"vocab_size\": 32768\n    }\n\nFree unoccupied cached memory\nimport torch\ntorch.cuda.empty_cache()\n\nInitialize a GPT-2 XL model using the custom tokenizer\nmodel = AutoModelForCausalLM.from_config(config)\n\nCheck the model size\nprint(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')\n    GPT-2 (xl) size: 1529.6M parameters\nNote: Large models are generally more efficient to train as long as the dataset is reasonably large.\n\n!git lfs install\n    Updated Git hooks.\n    Git LFS initialized.\n\nSave the newly initialized model to the Hub\nmodel.save_pretrained(\"models/\" + model_ckpt+\"-large\", push_to_hub=True)\n    OSError: EOF\n    error: failed to push some refs to 'https://user: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@huggingface.co/cj-mills/codeparrot-large'\n\nInitialize a smaller GPT-2 variant using the custom tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nconfig_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\nmodel_small = AutoModelForCausalLM.from_config(config_small)\n\nCheck smaller model size\nprint(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')\nGPT-2 size: 111.0M parameters\n\nPush the smaller model to the Hub\nmodel_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True)\n\n\n\nImplementing the Dataloader\n\nWe want to supply our model with sequences that fill its context length for maximal efficiency.\nSome code examples might be shorter or longer than the 1,024 token context length.\nWe can concatenate several examples to create a long sequence using the EOS token as a separator.\nWe then split this sequence into equally sized chunks that fill the context length.\n\ninput_characters = number_of_sequences * sequence_length * characters_per_token\n\ninput_characters: the number of characters in the string input to the tokenizer\nnumber_of_seqeunces: the number of (truncated) sequences returned by the tokenizer\nsequence_length: the number of tokens per sequence returned by the tokenizer\ncharacters_per_token: the average number of characters per output token that we first need to estimate\n\nEstimate the average character length per token\nexamples, total_characters, total_tokens = 500, 0, 0\ndataset = load_dataset('transformersbook/codeparrot-train', split='train',\n                       streaming=True)\n\nfor _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n    total_characters += len(example['content'])\n    total_tokens += len(tokenizer(example['content']).tokens())\n\ncharacters_per_token = total_characters / total_tokens\n\nprint(characters_per_token)\n3.621530410894045\nNote: We’ll round this to \\(3.6\\).\n\nimport torch\nfrom torch.utils.data import IterableDataset\n\nDefine an IterableDataset class for preparing constant-length inputs\nclass ConstantLengthDataset(IterableDataset):\n    \n    def __init__(self, tokenizer, dataset, seq_length=1024,\n                 num_of_sequences=1024, chars_per_token=3.6):\n        self.tokenizer = tokenizer\n        self.concat_token_id = tokenizer.eos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.input_characters = num_of_sequences * seq_length * chars_per_token\n    \n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                # Check if the buffer is full\n                if buffer_len &gt;= self.input_characters:\n                    m=f\"Buffer full: {buffer_len}&gt;={self.input_characters:.0f}\"\n                    print(m)\n                    break\n                # Try to add the next code sample to the buffer\n                try:\n                    m=f\"Fill buffer: {buffer_len}&lt;{self.input_characters:.0f}\"\n                    print(m)\n                    buffer.append(next(iterator)[\"content\"])\n                    buffer_len += len(buffer[-1])\n                # Reset iterator\n                except StopIteration:\n                    iterator = iter(self.dataset)\n            \n            all_token_ids = []\n            # Tokenize the code samples in the buffer\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n            # Concatenate the tokenized code samples\n            for tokenized_input in tokenized_inputs['input_ids']:\n                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n            # Split the sequence into equally sized chunks\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    yield torch.tensor(input_ids)\nNote: We don’t need attention masks here since all sequences precisely fill the context length of 1024 tokens.\n\nPrepare the constant-length dataset\nshuffled_dataset = dataset.shuffle(buffer_size=100)\nconstant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n                                                num_of_sequences=10)\nNote: We can’t shuffle iterable datasets as a whole, so we need to use a buffer instead.\n\nVerify the dataset yields equal length chunks\ndataset_iterator = iter(constant_length_dataset)\n\nlengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\nprint(f\"Lengths of the sequences: {lengths}\")\n    Fill buffer: 0&lt;36864\n    Fill buffer: 4344&lt;36864\n    Fill buffer: 5460&lt;36864\n    Fill buffer: 7467&lt;36864\n    Fill buffer: 13812&lt;36864\n    Fill buffer: 16142&lt;36864\n    Fill buffer: 17571&lt;36864\n    Fill buffer: 25693&lt;36864\n    Fill buffer: 27359&lt;36864\n    Fill buffer: 28903&lt;36864\n    Fill buffer: 32076&lt;36864\n    Buffer full: 49996&gt;=36864\n    Lengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n\n\n\nDefining the Training Loop\n\nEven modern GPUs can’t train a model at GPT-2 scale in a reasonable time.\nWe need to use data parallelism to utilize several GPUs for training.\nThe Hugging Face Accelerate library makes distributed training and changing the underlying hardware for training easier.\nHugging Face Accelerate provides an API to make training scripts run with mixed precision and in any distributed setting.\nThe same code can run seamlessly on your local machine for debugging and a beefy training cluster for a final training run.\n\n\nfrom argparse import Namespace\n\nDefine the hyperparameters\n# Commented parameters correspond to the small model\nconfig = {\"train_batch_size\": 2, # 12\n          \"valid_batch_size\": 2, # 12\n          \"weight_decay\": 0.1,\n          \"shuffle_buffer\": 1000,\n          \"learning_rate\": 2e-4, # 5e-4\n          \"lr_scheduler_type\": \"cosine\",\n          \"num_warmup_steps\": 750, # 2000\n          \"gradient_accumulation_steps\": 16, # 1\n          \"max_train_steps\": 50000, # 150000\n          \"max_eval_steps\": -1,\n          \"seq_length\": 1024,\n          \"seed\": 1,\n          \"save_checkpoint_steps\": 50000} # 15000\n\nargs = Namespace(**config)\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport logging\nimport wandb\n\n\nlogging.getLogger()\n\nDocumentation\nCreate a Logger object.\n\n\n\ntorch.utils.tensorboard.writer.SummaryWriter\n\nDocumentation\nWrite entries directly to event files for TensorBoard\n\n\n\nwandb\n\nGitHub Repository\nDocumentation\nA tool for visualizing and tracking machine learning experiements.\n\n\nDefine a method to initialize the loggers for the training process\ndef setup_logging(project_name):\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n        logging.StreamHandler()])\n    if accelerator.is_main_process: # We only want to set up logging once\n        wandb.init(project=project_name, config=args)\n        run_name = wandb.run.name\n        tb_writer = SummaryWriter()\n        tb_writer.add_hparams(vars(args), {'0': 0})\n        logger.setLevel(logging.INFO)\n        datasets.utils.logging.set_verbosity_debug()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        tb_writer = None\n        run_name = ''\n        logger.setLevel(logging.ERROR)\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    return logger, tb_writer, run_name\nNote:\n\nEach worker gets a unique accelerator.process_index, which we use with the FileHandler to write the logs of each worker to an individual file.\nWe’ll use the unique run_name to name our experiment branch on the Hub.\n\n\nDefine function to log metrics with TensorBoard and Weights and Biases\ndef log_metrics(step, metrics):\n    logger.info(f\"Step {step}: {metrics}\")\n    if accelerator.is_main_process:\n        wandb.log(metrics)\n        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]\n\nfrom torch.utils.data.dataloader import DataLoader\n\nDefine a function to create dataloaders for the training and validation sets\ndef create_dataloaders(dataset_name):\n    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n                              streaming=True)\n    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n                                    seed=args.seed)\n    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n                              streaming=True)\n    \n    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n                                          seq_length=args.seq_length)\n    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n                                          seq_length=args.seq_length)\n    \n    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n    return train_dataloader, eval_dataloader\nNote: Hugging Face Accelerate takes care of distributing batches to each worker.\n\nDefine a helper function to differentiate the parameters that should receive weight decay\n\nBiases and LayerNorm weights are generally not subject to weight decay.\n\ndef get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n    params_with_wd, params_without_wd = [], []\n    for n, p in model.named_parameters():\n        if any(nd in n for nd in no_decay):\n            params_without_wd.append(p)\n        else:\n            params_with_wd.append(p)\n    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n            {'params': params_without_wd, 'weight_decay': 0.0}]\n\nDefine a function to evaluate the model on the validation set\ndef evaluate():\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(batch, labels=batch)\n        loss = outputs.loss.repeat(args.valid_batch_size)\n        losses.append(accelerator.gather(loss))\n        if args.max_eval_steps &gt; 0 and step &gt;= args.max_eval_steps: break\n    loss = torch.mean(torch.cat(losses))\n    try:\n        perplexity = torch.exp(loss)\n    except OverflowError:\n        perplexity = torch.tensor(float(\"inf\"))\n    return loss.item(), perplexity.item()\nNote:\n\nThe perplexity measures how well the model’s output probability distributions predict the targeted tokens.\nA lower perplexity corresponds to better performance.\nWe compute the perplexity by exponentiating the cross-entropy loss from the model’s output.\n\n\nTraining session\n# Reset random seed\nset_seed(args.seed)\n\n# Accelerator\naccelerator = Accelerator()\nsamples_per_step = accelerator.state.num_processes * args.train_batch_size\n\n# Logging\nlogger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\nlogger.info(accelerator.state)\n\n# Load model and tokenizer\nif accelerator.is_main_process:\n    # Check out a new branch for the current run\n    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\nmodel = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\ntokenizer = AutoTokenizer.from_pretrained(\"./\")\n\n# Load dataset and dataloader\ntrain_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n\n# Prepare the optimizer and learning rate scheduler\noptimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\nlr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n                             num_warmup_steps=args.num_warmup_steps,\n                             num_training_steps=args.max_train_steps,)\ndef get_lr():\n    return optimizer.param_groups[0]['lr']\n\n# Prepare everything with our `accelerator` (order of args is not important)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader)\n\n# Train model\nmodel.train()\ncompleted_steps = 0\nfor step, batch in enumerate(train_dataloader, start=1):\n    loss = model(batch, labels=batch).loss\n    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n                       'steps': completed_steps, 'loss/train': loss.item()})\n    loss = loss / args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    # Use gradient accumulation to imitate larger batch sizes\n    if step % args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        completed_steps += 1\n    if step % args.save_checkpoint_steps == 0:\n        logger.info('Evaluating and saving model checkpoint')\n        # Evaluate the model every time we save a new checkpoint\n        eval_loss, perplexity = evaluate()\n        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n        # Synchronize the model before storing the latest checkpoint\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        if accelerator.is_main_process:\n            # Save the latest checkpoint to disk\n            unwrapped_model.save_pretrained(\"./\")\n            # Push the latest checkpoint to the Hub\n            hf_repo.push_to_hub(commit_message=f'step {step}')\n        model.train()\n    if completed_steps &gt;= args.max_train_steps:\n        break\n\n# Evaluate and save the last checkpoint\nlogger.info('Evaluating and saving model after training')\neval_loss, perplexity = evaluate()\nlog_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nif accelerator.is_main_process:\n    unwrapped_model.save_pretrained(\"./\")\n    hf_repo.push_to_hub(commit_message=f'final model')\nNote:\n\nhere are several approaches to distributed training depending on the model size and volume of data.\nHugging Face Accelerate uses DataDistributedParalellism (DDP).\nDDP allows you to train models faster with larger batch sizes that wouldn’t fit into any single GPU.\nHugging Face Accelerate prepares batches of data and sends them to the workers.\nEach worker consists of a GPU and calculates the loss and their respective accumulated gradients from forward and backward passes with a local copy of the model.\nWe average the gradients from each node with a reduce pattern and send the average back to each worker.\nWe apply the gradients using the optimizer on each node to avoid transferring copies of the large models between nodes.\nWe repeat the process after updating the models for each worker.\nDDP requires that the model fits on a single GPU.\nFitting larger networks into memory.\nModel Paralellism\n\n\n\n\n\nThe Training Run\n\nWe can save the training steps to a script and push them to a repository on the Hub.\nWe can then execute the training script on a training server using the accelerate launch command.\n\ngit clone https://huggingface.co/transformerbook/codeparrot\ncd codeparrot\npip install -r requirements.txt\nwandb login\naccelerate config\naccelerate launch codparrot_training.py\n\nThe accelerate config command guides you through setting up the infrastructure.\nHugging Face uses a2-megagpu-16g instances on Google Cloud for experiments (pricing).\nReducing 90% in costs with Spot VMs for Machine Learning on Google Kubernetes Engine in GCP\n\nConfiguration used to train CodeParrot models\n\n\n\nSetting\nValue\n\n\n\n\nCompute Environment?\nmulti-gpu\n\n\nHow many machines?\n1\n\n\nDeepSpeed?\nNo\n\n\nHow many processes?\n16\n\n\nUse FP16?\nYes\n\n\n\n\nRunning the training script with the above settings takes about 24 hours for the small model and seven days for the large model.\nTest the code on smaller infrastructure before using expensive cloud instances.\nWe can merge the experiment branch back into the main one after training completes.\n\ngit checkout main\ngit merge &lt;RUN_NAME&gt;\ngit push"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#results-and-analysis",
    "href": "posts/transformers-book-notes/chapter-10/index.html#results-and-analysis",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "Results and Analysis",
    "text": "Results and Analysis\n\nThe training loss and validation perplexity should go down continuously during training.\nThe large model converges with fewer processed tokens, but training takes longer overall.\nQualitative analysis involves looking at concrete examples and trying to better understand in which cases the model succeeds and fails.\nQuantitative analysis involves evaluating model performance statistically on a large set of test cases.\n\n\nfrom transformers import pipeline, set_seed\n\nWrap the small model in a text generation pipeline\nmodel_ckpt = 'transformersbook/codeparrot-small'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)\n\nimport re\nfrom transformers import set_seed \n\nDefine a function to extract the first code block from the model output\ndef first_block(string):\n    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n\nDefine a function to print out generated code completions\ndef complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n    set_seed(seed)\n    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n                  \"do_sample\":True,}\n    code_gens = generation(prompt, num_return_sequences=num_completions, \n                            max_length=max_length, **gen_kwargs)\n    code_strings = []\n    for code_gen in code_gens:\n        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n        code_strings.append(generated_code)\n    print(('\\n'+'='*80 + '\\n').join(code_strings))\n\nTest the model on a simple task\nprompt = '''def area_of_rectangle(a: float, b: float):\n    \"\"\"Return the area of the rectangle.\"\"\"'''\ncomplete_code(generation, prompt)\n\n        return math.sqrt(a * b)\n    ================================================================================\n    \n        return a * b / 2.0\n    ================================================================================\n    \n        return a * b\n    ================================================================================\n    \n        return a * b / 2.0\nNote: The generated outputs look convincing, but not all of them are correct.\n\nTest the model on a more complex task\nprompt = '''def get_urls_from_html(html):\n    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\ncomplete_code(generation, prompt)\n\n        if not html:\n            return []\n        return [url for url in re.findall(r'&lt;a href=\"(/[^/]+/[^\"]+?)\"&gt;', html)]\n    ================================================================================\n    \n        return [url for url in re.findall(r'&lt;a href=\"(.*?)\"', html)\n                if url]\n    ================================================================================\n    \n        return [url for url in re.findall(r'&lt;a href=\"(.*?)\"', html)]\n    ================================================================================\n    \n        return re.findall(r'&lt;a href=\"([^\"]+)\"&gt;', html)\nNote: The second attempt is not quite right, but the other three generations are correct.\n\nTest the generated code\nimport requests\n\ndef get_urls_from_html(html):\n    return [url for url in re.findall(r'&lt;a href=\"(.*?)\"', html) if url]\n\npd.DataFrame(get_urls_from_html(requests.get('https://hf.co/').text))\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nhttps://huggingface.co/bigscience/tr11-176B-ml-logs\n\n\n\n\n1\n\n\nhttps://github.com/huggingface/transformers\n\n\n\n\n2\n\n\n/join\n\n\n\n\n3\n\n\n/tasks\n\n\n\n\n4\n\n\nhttps://huggingface.co/transformers\n\n\n\n\n5\n\n\n/inference-api\n\n\n\n\n6\n\n\n/distilbert-base-uncased\n\n\n\n\n7\n\n\n/dbmdz/bert-large-cased-finetuned-conll03-english\n\n\n\n\n8\n\n\nhttps://bigscience.huggingface.co/\n\n\n\n\n9\n\n\nhttps://bigscience.huggingface.co/blog/t0\n\n\n\n\n10\n\n\nhttps://medium.com/huggingface/distilbert-8cf3380435b5\n\n\n\n\n11\n\n\nhttps://arxiv.org/abs/1811.06031\n\n\n\n\n12\n\n\nhttps://arxiv.org/abs/1803.10631\n\n\n\n\n13\n\n\n/coref\n\n\n\n\n14\n\n\nhttps://transformer.huggingface.co/\n\n\n\n\n\n\nNote: The URLs starting with https are external pages, while the others are subpages of the main website.\n\nWrap the large model in a text generation pipeline\nmodel_ckpt = 'transformersbook/codeparrot'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)\n\nTry to translate a function from pure Python to NumPy using the large model\nprompt = '''# a function in native python:\ndef mean(a):\n    return sum(a)/len(a)\n\n# the same function using numpy:\nimport numpy as np\ndef mean(a):'''\ncomplete_code(generation, prompt, max_length=64)\n\n        return np.mean(a)\n    ================================================================================\n    \n        return sum(a)/len(a)\n    ================================================================================\n    \n        return np.mean(a)\n    ================================================================================\n    \n        return sum(a)/len(a)\nNote: It worked.\n\nTry building a Scilit-learn model\nprompt = '''X = np.random.randn(100, 100)\ny = np.random.randint(0, 1, 100)\n\n# fit random forest classifier with 20 estimators'''\ncomplete_code(generation, prompt, max_length=96)\n\n    reg = DummyRegressor()\n    \n    forest = RandomForestClassifier(n_estimators=20)\n    \n    forest.fit(X, y)\n    ================================================================================\n    \n    clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\n    clf.fit(X, y)\n    ================================================================================\n    \n    clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\n    clf.fit(X, y)\n    ================================================================================\n    \n    clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\n    clf.fit(X, y)\nNote:\n\nThe second attempt used an extra-trees classifier, but the other three generated what we asked.\nThe BLEU score is not well suited for measuring the quality of generated code as it would punish a generation that deviates from the reference naming.\nThe success of a program does not depend on the naming scheme as long as it is consistent.\nWe can use traditional software development methods like unit tests to measure the quality of generated code.\nOpenAI evaluated Codex models by running several code generations for coding tasks through some unit tests and calculating the fraction that passes the tests.\nEvaluating Large Language Models Trained on Code"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-10/index.html#references",
    "href": "posts/transformers-book-notes/chapter-10/index.html#references",
    "title": "Notes on Transformers Book Ch. 10",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 9\nNext: Notes on Transformers Book Ch. 11"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-11/index.html",
    "href": "posts/transformers-book-notes/chapter-11/index.html",
    "title": "Notes on Transformers Book Ch. 11",
    "section": "",
    "text": "Scaling Transformers\nGoing Beyond Text\nMultimodal Transformers\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-11/index.html#scaling-transformers",
    "href": "posts/transformers-book-notes/chapter-11/index.html#scaling-transformers",
    "title": "Notes on Transformers Book Ch. 11",
    "section": "Scaling Transformers",
    "text": "Scaling Transformers\n\nThe Bitter Lesson\n\nRichard Sutton argued that general methods that leverage computation are far more effective in AI than methods that leverage domain knowledge.\nThe human knowledge approach tends to complicate things, making them less suited to taking advantage of general methods leveraging computation.\nSearch methods and learning methods seem to scale arbitrarily with computation power.\n\nLarge language models perform better on downstream tasks.\nInteresting capabilities like zero-shot and few-shot learning emerge in the 10 to 100-billion parameter range.\nComputing power and training data must also scale with parameter count.\nLarge language models like GPT-3 are estimated to cost $4.6 million to train.\nThe high cost of training large models means we need a way to estimate the model’s performance in advance.\nScaling Laws for Neural Language Models\n\nThe performance of language models appears to obey a power-law relationship with model size and other factors.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nPlot parameter counts over time for prominent Transformer architectures\nmodel_data = [\n    {'date': '12-06-2017', 'name': 'Transformer', 'size': 213*1e6},\n    {'date': '11-06-2018', 'name': 'GPT', 'size': 110*1e6},\n    {'date': '11-10-2018', 'name': 'BERT', 'size': 340*1e6},\n    {'date': '14-02-2019', 'name': 'GPT-2', 'size': 1.5*1e9},\n    {'date': '23-10-2019', 'name': 'T5', 'size': 11*1e9},\n    {'date': '17-09-2019', 'name': 'Megatron', 'size': 8.3*1e9},\n    {'date': '13-02-2020', 'name': 'Turing-NLG', 'size': 17*1e9},\n    {'date': '30-06-2020', 'name': 'GShard', 'size': 600*1e9},\n    {'date': '28-05-2020', 'name': 'GPT-3', 'size': 175*1e9},\n    {'date': '11-01-2021', 'name': 'Switch-C', 'size': 1.571*10e12},\n]\n\ndef label_point(x, y, val, ax):\n    a = pd.concat({\"x\": x, \"y\": y, \"val\": val}, axis=1)\n    for i, point in a.iterrows():\n        ax.text(\n            point[\"x\"],\n            point[\"y\"],\n            str(point[\"val\"]),\n            horizontalalignment=\"center\",\n            verticalalignment=\"bottom\",\n        )\n\ndf_lm = pd.DataFrame.from_records(model_data)\ndf_lm[\"date\"] = pd.to_datetime(df_lm[\"date\"], dayfirst=True)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 4))\ndf_lm.plot(x=\"date\", y=\"size\", kind=\"scatter\", s=15, ax=ax)\nax.set_yscale(\"log\")\nlabel_point(df_lm[\"date\"], df_lm[\"size\"], df_lm[\"name\"], ax)\nax.set_xlabel(\"Release date\")\nax.set_ylabel(\"Number of parameters\")\nax.grid(True)\nplt.subplots_adjust(top=1.2)\nplt.show()\n\n\n\n\n\n\n\nScaling Laws\n\nScaling laws allow us to empirically quantify the “bigger is better” paradigm for language models by studying their behavior with varying compute budgets \\(C\\), dataset sizes \\(D\\), and model sizes \\(N\\).\nWe measure dataset size in the number of tokens.\nThe model size excludes parameters from the embedding layers.\nWe chart the dependence of the cross-entropy loss on these three factors to determine if a relationship emerges.\nScaling laws imply that increasing compute budget, dataset size, and model size in tandem is more productive than architectural tweaks or hyperparameter optimization to improve performance.\nThe test loss has a power-law relationship with computation budget, dataset size, and model size across several orders of magnitude.\nWe can express \\(L\\left( X \\right) \\sim 1/X^{\\alpha}\\) for \\(X = N, C, D\\) where \\(\\alpha\\) is a scaling exponent determined by a fit to the loss curve.\n\nTypical values for \\(\\alpha\\) lie in the range \\(\\left[0.05,0.095 \\right]\\).\nScaling Laws for Autoregressive Generative Modeling\n\nThese power laws mean we can extrapolate the early part of a loss curve to predict the approximate loss from training longer.\nLarger models can achieve the same performance as smaller models with fewer training steps.\nScaling laws are also present for other modalities like images, videos, and mathematical problem-solving.\n\n\n\nChallenges with Scaling\n\nProvisioning and managing hundreds or thousands of GPU nodes typically requires specialized engineers familiar with running large-scale, distributed experiments.\nMost companies cannot afford the teams and resources to train models at the largest scales.\n\nA recently proposed distributed deep learning framework enables smaller groups to pool their computational resources and pre-train models.\n\nDistributed Deep Learning in Open Collaborations\n\nLarge models require large, high-quality datasets.\n\nIt is hard to curate only high-quality training examples when the dataset contains terabytes of text.\nWe need a way to control common biases in the dataset to prevent the model from inheriting them.\nThere are potential licensing issues when using large-scale web-text corpora.\nLarge-scale text datasets might contain personal information.\n\nEvaluating trained models on downstream tasks requires additional time and resources.\n\nWe need to probe the model for biased and toxic output, even when using a cleaned dataset.\n\nOptimization approaches like distillation, pruning, and quantization might not be enough when starting with a model that is hundreds of gigabytes in size.\nOpenAI API\nHugging Face Accelerated Inference API\nBigScience is a one-year-long research workshop meant to foster discussions and reflections on the research questions surrounding large language models, the challenges of creating and sharing them, and datasets used for research.\n\nThe collaborative tasks involve creating, sharing, and evaluating a massive multilingual dataset and language model.\n\nEleutherAI is a decentralized collective of volunteers focused on AI alignment, scaling, and open-source AI research.\n\nEleutherAI wants to train and open-source a GPT-3-sized model.\nGPT-Neo 2.7B\nGPT-J 6B\n\n\n\n\nAttention Please!\n\nSelf-attention involves performing pairwise comparisons of all the tokens in a sequence, which becomes a computational bottleneck.\nThe self-attention layer of the Transformer architecture naively scales like \\(O(n^{2})\\), where n is the length of the sequence.\nA recent paper from Google shows we can reduce the memory complexity to \\(O \\left( \\log{n} \\right)\\) via a simple reordering of the operations.\n\nSelf-attention Does Not Need \\(O(n^{2})\\) Memory\n\nMuch of the recent research on transformers focuses on making self-attention more efficient.\n\nEfficient Transformers: A Survey\n\nCommon approaches to making attention more efficient involve introducing sparsity into the attention mechanism or applying kernels to the attention matrix.\n\n\n\nSparse Attention\n\nWe can reduce the number of computations performed in the self-attention layer by limiting the number of query-key pairs it generates according to a predefined pattern.\nThere are a handful of popular “atomic” sparsity patterns.\n\nA Survey of Transformers\n\nGlobal attention defines a few tokens in the sequence that are allowed to attend to all others.\nBand attention computes attention over a diagonal band.\nDilated attention skips some query-key pairs using a dilated window with gaps.\nRandom attention samples a few keys for each query to compute attention scores.\nBlock local attention divides the sequence into blocks and restricts attention to within these blocks.\nMost transformer models with sparse attention use a mix of atomic sparsity patterns to generate the final attention matrix.\nModels like Longformer use a mix of global and band attention, while Bigbird adds random attention.\nIntroducing sparsity into the attention matrix enables models to process much longer sequences.\nIt is also possible to learn the sparsity pattern by clustering the tokens into chunks.\n\nReformer uses a hash function to cluster similar tokens.\n\n\n\n\nLinearized Attention\n\nLinearized attention involves changing the order of operations for computing attention scores.\nWe compute the self-attention score of the queries and keys using a similarity function like the dot product.\nFor a general similarity function \\(sim \\left( q_{i},k_{j} \\right)\\), we can express the attention outputs as the following equation:\n\\[y_{i} = \\sum_{j}{\\frac{sim \\left( Q_{i}, K_{j} \\right)}{\\sum_{k}{sim\\left( Q_{i}, K_{k} \\right)}}V_{j}}\\]\nThe trick behind linearized attention mechanisms is to express the similarity function as a kernel function that decomposes the operation into two pieces:\n\\[sim \\left( Q_{j}, K_{j} \\right) = \\phi \\left(Q_{i} \\right)^{T} \\phi \\left( K_{j} \\right)\\]\nwhere \\(\\phi\\) is typically a high-dimensional feature map.\n\\(\\phi \\left( Q_{i} \\right)\\) is independent of \\(j\\) and \\(k\\), so we can pull it under the sums to write the attention output as follows:\n\\[y_{i} = \\frac{\\phi \\left(Q_{i} \\right)^{T} \\sum_{j}{\\phi \\left( K_{j} \\right)} V_{j}^{T}}{\\phi \\left(Q_{i} \\right)^{T} \\sum_{k}{\\phi \\left( K_{k} \\right)}}\\]\nBy first computing \\(\\sum_{j}{\\phi \\left( K_{j} \\right)} V_{j}^{T}\\) and \\(\\sum_{k}{\\phi \\left( K_{k} \\right)}\\), we can effectively linearize the space and time complexity of self-attention.\nPopular methods that implement linearized self-attention include Linear Transformer and Performer.\n\nTransformers are RNNs: Fast Autoregressive Transformers with Linear Attention\nRethinking Attention with Performers"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-11/index.html#going-beyond-text",
    "href": "posts/transformers-book-notes/chapter-11/index.html#going-beyond-text",
    "title": "Notes on Transformers Book Ch. 11",
    "section": "Going Beyond Text",
    "text": "Going Beyond Text\n\nDeveloping effective strategies for common textual tasks like classification and question answering allows us to address many types of real-world problems.\n\n\nLimitations to using text\n\nHuman reporting bias\n\nThe frequencies of events in the training text my not represent their actual frequencies.\n\nReporting Bias and Knowledge Acquisition\n\nA model trained exclusively on text from the internet might have a distorted image of the world.\n\n\n\nCommon Sense\n\nMost do not document their reasoning based on common sense.\nLanguage models trained on text might know many facts about the world but lack basic common-sense reasoning.\n\n\n\nFacts\n\nA probabilistic language model cannot reliably store facts and can produce factually incorrect text.\nSuch models can detect named entities but have no direct way to access information about them.\n\n\n\nModality\n\nLanguage models can’t connect to other modalities, such as audio, visual signals, or tabular data, that might address some of these limitations.\n\n\n\n\nVision\n\nTransformers are now achieving efficiency similar to or better than Convolutional Neural Networks (CNNs).\n\n\niGPT\n\niGPT (short for image GPT) uses the GPT architecture and autoregressive pretraining objective to predict future pixel values by viewing images as sequences of pixels.\nGenerative Pretraining From Pixels\nPretraining on large image datasets enables iGPT to “autocomplete” partial images.\niGPT achieves performant results on classification tasks when using a classification head.\n\n\n\nViT\n\nVision Transformer (Vit) is a BERT-style take on transformers for vision.\nWe split the image into smaller patches and then embed each of these patches with a linear projection.\nWe combine the patch embeddings with position embeddings and feed them through an ordinary transformer encoder.\nWe mask or distort some of the patches during training, and the objective is to predict the average color of the masked patch.\nThis approach did not produce better results when pretrained on the standard ImageNet dataset, but it scaled significantly better than Convolutional Neural Networks on larger datasets.\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nThe Hugging Face Transformers library includes Vision Transformer.\n\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nLoad an image of a dog\nimage = Image.open(\"dog.jpg\")\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nfrom transformers import pipeline\n\n\n\nImageClassificationPipeline\n\nDocumentation\nCreate an image classification pipeline\n\nCreate an image classification pipeline\nimage_classifier = pipeline(\"image-classification\")\n\nGet the model architecture\nimage_classifier.model.config.architectures\n    ['ViTForImageClassification']\n\n\n\nViTForImageClassification\n\nDocumentation\nCreate a ViT Model transformer with an image classification head for ImageNet.\n\nGet the link to the Hugging Face model card\nprint(f\"https://huggingface.co/{image_classifier.model.config._name_or_path}\")\nhttps://huggingface.co/google/vit-base-patch16-224\n\nView potential Image classes\npd.DataFrame(list(image_classifier.model.config.id2label.values())).T\n\nPerform image classification\npreds = image_classifier(image)\npreds_df = pd.DataFrame(preds)\npreds_df\n\n\n\n\n\n\n\n\nscore\n\n\nlabel\n\n\n\n\n\n\n0\n\n\n0.989680\n\n\ngolden retriever\n\n\n\n\n1\n\n\n0.002968\n\n\nLabrador retriever\n\n\n\n\n2\n\n\n0.000502\n\n\nkuvasz\n\n\n\n\n3\n\n\n0.000402\n\n\nIrish setter, red setter\n\n\n\n\n4\n\n\n0.000345\n\n\ntennis ball\n\n\n\n\n\n\nNote:\n\nThe model correctly classifies the dog as a Golden Retriever.\nVideo models are a natural extension of image models and add a temporal dimension on top of the spatial dimension.\nVideo tasks are more challenging as the volume of data gets much larger, and we need to deal with an extra dimension.\nModels such as TimeSformer introduce a spatial and temporal attention mechanism.\n\nIs Space-Time Attention All You Need for Video Understanding?\nSuch models can help build tools for many tasks such as video classification or annotation.\n\n\n\n\n\n\nTables\n\nLots of data is in structured databases instead of raw text.\nTable Parser (TAPAS) applies the Transformer architecture to tables by combining the tabular information with the query.\nTAPAS: Weakly Supervised Table Parsing via Pre-training\n\n\n\n\n\n\nCreate some sample table data\nbook_data = [\n    {\"chapter\": 0, \"name\": \"Introduction\", \"start_page\": 1, \"end_page\": 11},\n    {\"chapter\": 1, \"name\": \"Text classification\", \"start_page\": 12, \n     \"end_page\": 48},\n    {\"chapter\": 2, \"name\": \"Named Entity Recognition\", \"start_page\": 49,\n     \"end_page\": 73},\n    {\"chapter\": 3, \"name\": \"Question Answering\", \"start_page\": 74, \n     \"end_page\": 120},\n    {\"chapter\": 4, \"name\": \"Summarization\", \"start_page\": 121, \n     \"end_page\": 140},\n    {\"chapter\": 5, \"name\": \"Conclusion\", \"start_page\": 141, \n     \"end_page\": 144}\n]\n\ntable = pd.DataFrame(book_data)\ntable['number_of_pages'] = table['end_page']-table['start_page']\nNote: We need to make all columns of type str to play nicely with TAPAS.\n\ntable = table.astype(str)\ntable\n\n\n\n\n\n\n\n\nchapter\n\n\nname\n\n\nstart_page\n\n\nend_page\n\n\nnumber_of_pages\n\n\n\n\n\n\n0\n\n\n0\n\n\nIntroduction\n\n\n1\n\n\n11\n\n\n10\n\n\n\n\n1\n\n\n1\n\n\nText classification\n\n\n12\n\n\n48\n\n\n36\n\n\n\n\n2\n\n\n2\n\n\nNamed Entity Recognition\n\n\n49\n\n\n73\n\n\n24\n\n\n\n\n3\n\n\n3\n\n\nQuestion Answering\n\n\n74\n\n\n120\n\n\n46\n\n\n\n\n4\n\n\n4\n\n\nSummarization\n\n\n121\n\n\n140\n\n\n19\n\n\n\n\n5\n\n\n5\n\n\nConclusion\n\n\n141\n\n\n144\n\n\n3\n\n\n\n\n\n\n\nCreate a table question answering pipeline\ntable_qa = pipeline(\"table-question-answering\")\ntable_qa.model.config\n    TapasConfig {\n      \"_name_or_path\": \"google/tapas-base-finetuned-wtq\",\n      \"aggregation_labels\": {\n        \"0\": \"NONE\",\n        \"1\": \"SUM\",\n        \"2\": \"AVERAGE\",\n        \"3\": \"COUNT\"\n      },\n      \"aggregation_loss_weight\": 1.0,\n      \"aggregation_temperature\": 1.0,\n      \"allow_empty_column_selection\": false,\n      \"answer_loss_cutoff\": 0.664694,\n      \"answer_loss_importance\": 1.0,\n      \"architectures\": [\n        \"TapasForQuestionAnswering\"\n      ],\n      \"attention_probs_dropout_prob\": 0.1,\n      \"average_approximation_function\": \"ratio\",\n      \"average_logits_per_cell\": false,\n      \"cell_selection_preference\": 0.207951,\n      \"disable_per_token_loss\": false,\n      \"gradient_checkpointing\": false,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"huber_loss_delta\": 0.121194,\n      \"init_cell_selection_weights_to_zero\": true,\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"layer_norm_eps\": 1e-12,\n      \"max_num_columns\": 32,\n      \"max_num_rows\": 64,\n      \"max_position_embeddings\": 1024,\n      \"model_type\": \"tapas\",\n      \"no_aggregation_label_index\": 0,\n      \"num_aggregation_labels\": 4,\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"pad_token_id\": 0,\n      \"positive_label_weight\": 10.0,\n      \"reset_position_index_per_cell\": true,\n      \"select_one_column\": true,\n      \"softmax_temperature\": 1.0,\n      \"temperature\": 0.0352513,\n      \"transformers_version\": \"4.18.0\",\n      \"type_vocab_size\": [\n        3,\n        256,\n        256,\n        2,\n        256,\n        256,\n        10\n      ],\n      \"type_vocab_sizes\": [\n        3,\n        256,\n        256,\n        2,\n        256,\n        256,\n        10\n      ],\n      \"use_answer_as_supervision\": true,\n      \"use_gumbel_for_aggregation\": false,\n      \"use_gumbel_for_cells\": false,\n      \"use_normalized_answer_loss\": false,\n      \"vocab_size\": 30522\n    }\n\nGet the link to the Hugging Face model card\nprint(f\"https://huggingface.co/{table_qa.model.config._name_or_path}\")\n    https://huggingface.co/google/tapas-base-finetuned-wtq\n\npd.DataFrame(table_qa.tokenizer.vocab.keys()).head(1500).T\n\n\nTapasForQuestionAnswering\n\nDocumentation\nCreate a Tapas Model with a cell selection head and optional aggregation head for question answering tasks.\n\nPass some queries to the model\nqueries = [\"What's the topic in chapter 4?\",\n           \"What is the total number of pages?\",\n           \"On which page does the chapter about question-answering start?\",\n           \"How many chapters have more than 20 pages?\"]\npreds = table_qa(table, queries)\nfor query, pred in zip(queries, preds):\n    print(query)\n    if pred[\"aggregator\"] == \"NONE\": \n        print(\"Predicted answer: \" + pred[\"answer\"])\n    else: \n        print(\"Predicted answer: \" + pred[\"answer\"])\n    print('='*50)\n    What's the topic in chapter 4?\n    Predicted answer: Summarization\n    ==================================================\n    What is the total number of pages?\n    Predicted answer: SUM &gt; 10, 36, 24, 46, 19, 3\n    ==================================================\n    On which page does the chapter about question-answering start?\n    Predicted answer: AVERAGE &gt; 74\n    ==================================================\n    How many chapters have more than 20 pages?\n    Predicted answer: COUNT &gt; 1, 2, 3\n    ==================================================\nNote:\n\nThe model predicted exactly one cell with no aggregation for the first query, and the answer is correct.\nFor the second query, the model correctly predicted that we need to sum the individual page counts for each chapter to determine the total number of pages.\nThe model correctly answered question three but included an unnecessary average aggregation.\nThe model correctly determined that chapters 1, 2, and 3 have more than 20 pages.\nThe ability to ask questions in natural language instead of Python code allows a much wider audience to query the data to answer specific questions."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-11/index.html#multimodal-transformers",
    "href": "posts/transformers-book-notes/chapter-11/index.html#multimodal-transformers",
    "title": "Notes on Transformers Book Ch. 11",
    "section": "Multimodal Transformers",
    "text": "Multimodal Transformers\n\nSpeech-to-Text\n\nSpeaking is more convenient than reading and writing for a significant portion of the population.\nAutomatic speech recognition (ASR) involves converting spoken words to text and enables voice technologies like Siri to answer questions like “What is the weather like today?”.\nThe wave2vec 2.0 family of models is one of the most recent developments in ASR and uses a transformer layer in combination with a CNN.\n\nwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\n\nThese models leverage unlabeled data to achieve competitive results with only a few minutes of labeled data.\nThe Hugging Face Transformers library includes wave2vec 2.0 models.\n\nCreate an automatic speech recognition pipeline\nasr = pipeline(\"automatic-speech-recognition\")\nasr.model.config\n    Wav2Vec2Config {\n      \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n      \"activation_dropout\": 0.1,\n      \"adapter_kernel_size\": 3,\n      \"adapter_stride\": 2,\n      \"add_adapter\": false,\n      \"apply_spec_augment\": true,\n      \"architectures\": [\n        \"Wav2Vec2ForCTC\"\n      ],\n      \"attention_dropout\": 0.1,\n      \"bos_token_id\": 1,\n      \"classifier_proj_size\": 256,\n      \"codevector_dim\": 256,\n      \"contrastive_logits_temperature\": 0.1,\n      \"conv_bias\": false,\n      \"conv_dim\": [\n        512,\n        512,\n        512,\n        512,\n        512,\n        512,\n        512\n      ],\n      \"conv_kernel\": [\n        10,\n        3,\n        3,\n        3,\n        3,\n        2,\n        2\n      ],\n      \"conv_stride\": [\n        5,\n        2,\n        2,\n        2,\n        2,\n        2,\n        2\n      ],\n      \"ctc_loss_reduction\": \"sum\",\n      \"ctc_zero_infinity\": false,\n      \"diversity_loss_weight\": 0.1,\n      \"do_stable_layer_norm\": false,\n      \"eos_token_id\": 2,\n      \"feat_extract_activation\": \"gelu\",\n      \"feat_extract_dropout\": 0.0,\n      \"feat_extract_norm\": \"group\",\n      \"feat_proj_dropout\": 0.1,\n      \"feat_quantizer_dropout\": 0.0,\n      \"final_dropout\": 0.1,\n      \"gradient_checkpointing\": false,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout\": 0.1,\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"layer_norm_eps\": 1e-05,\n      \"layerdrop\": 0.1,\n      \"mask_feature_length\": 10,\n      \"mask_feature_min_masks\": 0,\n      \"mask_feature_prob\": 0.0,\n      \"mask_time_length\": 10,\n      \"mask_time_min_masks\": 2,\n      \"mask_time_prob\": 0.05,\n      \"model_type\": \"wav2vec2\",\n      \"num_adapter_layers\": 3,\n      \"num_attention_heads\": 12,\n      \"num_codevector_groups\": 2,\n      \"num_codevectors_per_group\": 320,\n      \"num_conv_pos_embedding_groups\": 16,\n      \"num_conv_pos_embeddings\": 128,\n      \"num_feat_extract_layers\": 7,\n      \"num_hidden_layers\": 12,\n      \"num_negatives\": 100,\n      \"output_hidden_size\": 768,\n      \"pad_token_id\": 0,\n      \"proj_codevector_dim\": 256,\n      \"tdnn_dilation\": [\n        1,\n        2,\n        3,\n        1,\n        1\n      ],\n      \"tdnn_dim\": [\n        512,\n        512,\n        512,\n        512,\n        1500\n      ],\n      \"tdnn_kernel\": [\n        5,\n        3,\n        3,\n        1,\n        1\n      ],\n      \"transformers_version\": \"4.18.0\",\n      \"use_weighted_layer_sum\": false,\n      \"vocab_size\": 32,\n      \"xvector_output_dim\": 512\n    }\n\nGet the link to the Hugging Face model card\nprint(f\"https://huggingface.co/{asr.model.config._name_or_path}\")\n    https://huggingface.co/facebook/wav2vec2-base-960h\nNote: The model trained on 960 hours of speech audio.\n\n\nWav2Vec2ForCTC\n\nDocumentation\nCreate a Wav2Vec2 model with a language modeling head for Connectionist Temporal Classification (CTC).\n\n\nfrom datasets import load_dataset\n\n\n\nThe SUPERB Dataset\n\nHugging Face Dataset Card\nSUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data.\n\nLoad the ASR subset of the SUPERB dataset\nds = load_dataset(\"superb\", \"asr\", split=\"validation[:1]\")\npd.DataFrame(ds[0])\n\n\n\n\n\n\n\n\nfile\n\n\naudio\n\n\ntext\n\n\nspeaker_id\n\n\nchapter_id\n\n\nid\n\n\n\n\n\n\narray\n\n\n/home/innom-dt/.cache/huggingface/datasets/downloads/extracted/aa91addd71e85ab524e5b5b56fa3d0de777838850cb76ec55ad066e969fd5144/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\n\n\n[0.002380371, 0.0020751953, 0.0019836426, 0.002105713, 0.0016174316, 0.00030517578, 9.1552734e-05, 0.00033569336, 0.0009765625, 0.0018310547, 0.0020141602, 0.002105713, 0.001739502, 0.00045776367, -0.00039672852, 0.00045776367, 0.0010070801, 9.1552734e-05, 0.00048828125, 0.001159668, 0.0007324219, 0.0009460449, 0.0018005371, 0.0018310547, 0.00088500977, 0.0004272461, 0.00048828125, 0.0007324219, 0.0010986328, 0.002105713, 0.0025634766, 0.002532959, 0.0025634766, 0.0022888184, 0.0018005371, 0.0010681152, 0.00064086914, 0.00012207031, 0.0002746582, 0.001159668, 0.0015258789, 0.0015563965, 0.0019226074, 0.0012207031, -3.0517578e-05, -0.00036621094, -0.00039672852, -0.00039672852, -0.00015258789, 0.0006713867, 0.0012817383, 0.0018615723, 0.0015869141, 0.0012817383, 0.0007324219, 9.1552734e-05, -0.000579834, -0.00045776367, 9.1552734e-05, 0.00033569336, 0.00024414062, 0.0011291504, 0.001373291, 0.0012817383, 0.00088500977, 0.00030517578, -0.00088500977, -0.0014648438, -0.0008239746, 0.00012207031, 0.0011901855, 0.0019226074, 0.0016479492, 0.00088500977, 0.00076293945, 0.0004272461, -0.0005187988, -0.0005493164, -0.00036621094, -0.0004272461, -0.00018310547, 0.000579834, 0.0009460449, 0.0007324219, 0.0010070801, 0.0007019043, 0.00024414062, -0.00018310547, -0.00064086914, -0.00088500977, -0.00048828125, 0.0002746582, 0.0007324219, 0.0018310547, 0.0018005371, 0.0012512207, 0.00061035156, -0.00036621094, -0.0012817383, -0.00091552734, …]\n\n\nMISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n\n\n1272\n\n\n128104\n\n\n1272-128104-0000\n\n\n\n\npath\n\n\n/home/innom-dt/.cache/huggingface/datasets/downloads/extracted/aa91addd71e85ab524e5b5b56fa3d0de777838850cb76ec55ad066e969fd5144/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\n\n\n/home/innom-dt/.cache/huggingface/datasets/downloads/extracted/aa91addd71e85ab524e5b5b56fa3d0de777838850cb76ec55ad066e969fd5144/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\n\n\nMISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n\n\n1272\n\n\n128104\n\n\n1272-128104-0000\n\n\n\n\nsampling_rate\n\n\n/home/innom-dt/.cache/huggingface/datasets/downloads/extracted/aa91addd71e85ab524e5b5b56fa3d0de777838850cb76ec55ad066e969fd5144/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\n\n\n16000\n\n\nMISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n\n\n1272\n\n\n128104\n\n\n1272-128104-0000\n\n\n\n\n\n\nNote:\n\nThe file column contains the path to the audio sample, and the text column contains the expected transcription.\nWe can use the SoundFile library to read each audio file and convert the audio to an array of floats.\n\n\nimport soundfile as sf\n\nAdd a new column storing each audio sample as an array of floats\ndef map_to_array(batch):\n    speech, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech\n    return batch\n\nds = ds.map(map_to_array)\n\nPlay a sample from the dataset\nfrom IPython.display import Audio\n\ndisplay(Audio(ds[0]['speech'], rate=16000))\n\nds.set_format(\"numpy\")\n\nPass the audio sample the pipeline\npred = asr(ds[0][\"speech\"])\nprint(pred)\n    {'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'}\nNote:\n\nThe words in the transcription are correct, but the punctuation is missing.\nIt is hard to infer punctuation from audio alone, and we could add it in a post-processing step.\nBuilding a model for a new language still requires a minimum amount of labeled data, which can be challenging to obtain.\nA new method named wav2vec-U combines clever clustering and GAN training to build a speech-to-text model using only independent unlabeled speech and unlabeled text data.\n\nThis method requires not aligned speech and text data, enabling the training of highly performant speech-to-text models for a much larger spectrum of languages.\nUnsupervised Speech Recognition\n\n\n\n\n\n\n\n\n\n\n\nVision and Text\n\nThere have been several developments in combining visual and textual information.\n\n\nVQA\n\nMaking the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\nModels such as LXMERT and VisualBERT use vision models like ResNets to extract features from images and then use transformer encoders to combine them with the natural questions and predict and answer.\n\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers\nVisualBERT: A Simple and Performant Baseline for Vision and Language\n\n\n\n\nLayoutLM\n\nThe LayoutLM family of models uses an enhanced Transformer architecture that receives a text sequence, an image, and a layout as input.\nThere are embedding layers associated with each modality, a spatially-aware self-attention mechanism, and a mix of image and text/image pretraining objectives to align the different modalities.\nLayoutLM models pre-train on millions of scanned documents and can transfer to various downstream tasks, similar to BERT for NLP.\nLayoutLM models are the current state of the art for analyzing scanned business documents like receipts, invoices, or reports.\n\n\n\n\n\n\n\n\nDALL·E\n\nDALLE uses the GPT architecture and autoregressive modeling to generate images from text.\nIt regards the words and pixels as one sequence of tokens and can, therefore, continue generating an image from a text prompt.\nZero-Shot Text-to-Image Generation\n\n\n\nCLIP\n\nLearning Transferable Visual Models From Natural Language Supervision\nWe can use the pretrained model for classification by embedding the possible classes with the text encoder and comparing the class embeddings to the image embedding that we want to classify.\nWe select the class with the highest similarity.\nCLIP has remarkable zero-shot image classification performance and is competitive with fully supervised-trained vision models while being more flexible.\nWe need to instantiate a processor that contains a feature extractor and a tokenizer for image-to-text tasks.\nThe feature extractor converts the image into a form suitable for the model, while the tokenizer decodes the model predictions into text.\n\n\n\n\n\n\n\nfrom transformers import CLIPProcessor, CLIPModel\n\n\n\nCLIPProcessor\n\nDocumentation\nCreate a CLIP processor which wraps a CLIP feaure extractor and a CLIP tokenizer into a single processor.\n\n\n\nCLIPModel\n\nDocumentation\n\nInstantiate a CLIPModel and processor\nclip_ckpt = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(clip_ckpt)\nprocessor = CLIPProcessor.from_pretrained(clip_ckpt)\n\nprint(f\"https://huggingface.co/{clip_ckpt}\")\n    https://huggingface.co/openai/clip-vit-base-patch32\n\nprocessor\n    CLIPProcessor:\n    - feature_extractor: CLIPFeatureExtractor {\n      \"crop_size\": 224,\n      \"do_center_crop\": true,\n      \"do_normalize\": true,\n      \"do_resize\": true,\n      \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n      \"image_mean\": [\n        0.48145466,\n        0.4578275,\n        0.40821073\n      ],\n      \"image_std\": [\n        0.26862954,\n        0.26130258,\n        0.27577711\n      ],\n      \"resample\": 3,\n      \"size\": 224\n    }\n    \n    - tokenizer: PreTrainedTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_len=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"&lt;|startoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '&lt;|endoftext|&gt;'})\n\nLoad a test image\nimage = Image.open(\"dog.jpg\")\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\nimport torch\n\nCreate some sample image captions\ntexts = [\"a photo of a golden retriever\", \"a photo of a dog\", \"a photo of agi\"]\n\nCompare the image to the captions\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\npd.DataFrame(zip(texts, probs[0].numpy()), columns=['Text', \"Probability\"])\n\n\n\n\n\n\n\n\nText\n\n\nProbability\n\n\n\n\n\n\n0\n\n\na photo of a golden retriever\n\n\n0.868025\n\n\n\n\n1\n\n\na photo of a dog\n\n\n0.131801\n\n\n\n\n2\n\n\na photo of agi\n\n\n0.000174"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-11/index.html#references",
    "href": "posts/transformers-book-notes/chapter-11/index.html#references",
    "title": "Notes on Transformers Book Ch. 11",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 10"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html",
    "href": "posts/transformers-book-notes/chapter-2/index.html",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "",
    "text": "Project: Analyze Product Sentiment on Twitter\nThe Dataset\nFrom Text to Tokens\nTraining a Text Classifier\nConclusion\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#project-analyze-product-sentiment-on-twitter",
    "href": "posts/transformers-book-notes/chapter-2/index.html#project-analyze-product-sentiment-on-twitter",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "Project: Analyze Product Sentiment on Twitter",
    "text": "Project: Analyze Product Sentiment on Twitter\n\nSentiment analysis involves classifying the feelings or opinions expressed in a given text.\nThe goal is to build a system that automatically classifies emotions expressed in Twitter messages about a product.\nA model will take a single tweet as input and assign one of the possible labels.\nPossible labels include anger, fear, joy, love, sadness, and surprise.\nThe project will use a variant of BERT called DistilBERT.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n\nDistilBERT achieves comparable accuracy to BERT while being significantly more efficient in size and speed.\nDistilBERT was created in 2019 by researchers at Hugging Face.\n\n\n\nHugging Face Project Pipeline:\n\nLoad and process datasets using the Datasets library.\nTokenize input texts using the Tokenizers library.\nLoad, train, and run models using the Transformers library.\nLoad metrics and evaluate models using the Datasets library."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#the-dataset",
    "href": "posts/transformers-book-notes/chapter-2/index.html#the-dataset",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "The Dataset",
    "text": "The Dataset\n\nCARER: Contextualized Affect Representations for Emotion Recognition\n\nThe authors of the paper created an emotion dataset of English twitter messages.\nThe emotion dataset contains messages that express anger, fear, joy, love, sadness, or surprise.\nEmoticons present in the tweets determine the initial labels.\nA graph-based algorithm then uses these initial labels to construct contextualized, pattern-based emotion features.\nWord embeddings help further enrich these features.\n\nGitHub Repository\nHugging Face Dataset Card\n\n\nA First Look at Hugging Face Datasets\n\nGitHub Repository\nDocumentation\nHugging Face Datasets is based on Apache Arrow.\n\nApache Arrow defines a typed columnar format that is more memory efficient than native Python.\n\n\nfrom datasets import list_datasets\n\nprint_source(list_datasets, exclude_doc=True)\n    def list_datasets(with_community_datasets=True, with_details=False):\n        datasets = huggingface_hub.list_datasets(full=with_details)\n        if not with_community_datasets:\n            datasets = [dataset for dataset in datasets if '/' not in dataset.id]\n        if not with_details:\n            datasets = [dataset.id for dataset in datasets]\n        return datasets\n\n# Get a list of all the datasets scripts available on the Hugging Face Hub\nall_datasets = list_datasets()\nprint(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\nprint(f\"The first 10 are: {all_datasets[:10]}\")\n    There are 3896 datasets currently available on the Hub\n    The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n\nfrom datasets import load_dataset\n\nload_dataset\n\nDocumentation\nThis method downloads and imports the loading script for the specified dataset.\nThe script defines the citation, info, and format of the dataset, the URL to the original data files, and the code to load examples from the original files.\nThe script downloads the dataset files and caches them in typed Apache Arrow tables.\nSeveral loading scripts are available to handle local and remote datasets.\n\n\n\nMethods to Load Common Data Formats\n\n\n\n\n\n\n\n\nData format\nLoading script\nExample\n\n\n\n\nCSV\ncsv\nload_dataset(\"csv\", data_files=\"my_file.csv\")\n\n\nText\ntext\nload_dataset(\"text\", data_files=\"my_file.txt\")\n\n\nJSON\njson\nload_dataset(\"json\", data_files=\"my_file.jsonl\")\n\n\n\n\nprint_source(load_dataset)\n    def load_dataset(path: str, name: Optional[str]=None, data_dir: Optional[\n        str]=None, data_files: Optional[Union[str, Sequence[str], Mapping[str,\n        Union[str, Sequence[str]]]]]=None, split: Optional[Union[str, Split]]=\n        None, cache_dir: Optional[str]=None, features: Optional[Features]=None,\n        download_config: Optional[DownloadConfig]=None, download_mode: Optional\n        [GenerateMode]=None, ignore_verifications: bool=False, keep_in_memory:\n        Optional[bool]=None, save_infos: bool=False, revision: Optional[Union[\n        str, Version]]=None, use_auth_token: Optional[Union[bool, str]]=None,\n        task: Optional[Union[str, TaskTemplate]]=None, streaming: bool=False,\n        script_version='deprecated', **config_kwargs) -&gt;Union[DatasetDict,\n        Dataset, IterableDatasetDict, IterableDataset]:\n        if script_version != 'deprecated':\n            warnings.warn(\n                \"'script_version' was renamed to 'revision' in version 1.13 and will be removed in 1.15.\"\n                , FutureWarning)\n            revision = script_version\n        ignore_verifications = ignore_verifications or save_infos\n        builder_instance = load_dataset_builder(path=path, name=name, data_dir=\n            data_dir, data_files=data_files, cache_dir=cache_dir, features=\n            features, download_config=download_config, download_mode=\n            download_mode, revision=revision, use_auth_token=use_auth_token, **\n            config_kwargs)\n        if streaming:\n            extend_module_for_streaming(builder_instance.__module__,\n                use_auth_token=use_auth_token)\n            if not builder_instance.__module__.startswith('datasets.'):\n                for imports in get_imports(inspect.getfile(builder_instance.\n                    __class__)):\n                    if imports[0] == 'internal':\n                        internal_import_name = imports[1]\n                        internal_module_name = '.'.join(builder_instance.\n                            __module__.split('.')[:-1] + [internal_import_name])\n                        extend_module_for_streaming(internal_module_name,\n                            use_auth_token=use_auth_token)\n            return builder_instance.as_streaming_dataset(split=split,\n                use_auth_token=use_auth_token)\n        try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\n        builder_instance.download_and_prepare(download_config=download_config,\n            download_mode=download_mode, ignore_verifications=\n            ignore_verifications, try_from_hf_gcs=try_from_hf_gcs,\n            use_auth_token=use_auth_token)\n        keep_in_memory = (keep_in_memory if keep_in_memory is not None else\n            is_small_dataset(builder_instance.info.dataset_size))\n        ds = builder_instance.as_dataset(split=split, ignore_verifications=\n            ignore_verifications, in_memory=keep_in_memory)\n        if task is not None:\n            ds = ds.prepare_for_task(task)\n        if save_infos:\n            builder_instance._save_infos()\n        return ds\n\n\nAutomated Process\n# Download dataset from Hub\nemotions = load_dataset(\"emotion\")\n\npd.DataFrame(list(emotions.cache_files.items()))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\ntrain\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-train.arrow’}]\n\n\n\n\n1\n\n\nvalidation\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-validation.arrow’}]\n\n\n\n\n2\n\n\ntest\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-test.arrow’}]\n\n\n\n\n\n\n\n\nManual Process - Local\n# Get the download URLs\nurls = list(emotions['train'].info.download_checksums.keys())\nurls\n    ['https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1',\n     'https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1',\n     'https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1']\n\n# Download each dataset to current directory\nfor url in urls:\n    # remove url parameters\n    url = url.split('?')[0]\n    # run the wget shell command in the jupyter notebook\n    !wget $url\n    --2022-04-01 11:59:26--  https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212\n    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n    HTTP request sent, awaiting response... 301 Moved Permanently\n    Location: /s/raw/1pzkadrvffbqw6o/train.txt [following]\n    --2022-04-01 11:59:26--  https://www.dropbox.com/s/raw/1pzkadrvffbqw6o/train.txt\n    Reusing existing connection to www.dropbox.com:443.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com/cd/0/inline/BimHlqS8EcLnVO8-ErygeREvWupg-stxp_BKxrhhRBD8zXEOdQ5P-ssnFHFhv63Jx0wos3YwmuzmYs4Ex3iGW6lF430Y2yc4Y-ro00V20otuMPHh1I7x6YnZWmMe_xQOeM_-RNv_CbVeXC2wxDFZxE-TWzFuwjHo-RUy7RcwlYWMng/file# [following]\n    --2022-04-01 11:59:26--  https://ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com/cd/0/inline/BimHlqS8EcLnVO8-ErygeREvWupg-stxp_BKxrhhRBD8zXEOdQ5P-ssnFHFhv63Jx0wos3YwmuzmYs4Ex3iGW6lF430Y2yc4Y-ro00V20otuMPHh1I7x6YnZWmMe_xQOeM_-RNv_CbVeXC2wxDFZxE-TWzFuwjHo-RUy7RcwlYWMng/file\n    Resolving ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com (ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f\n    Connecting to ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com (ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 1658616 (1.6M) [text/plain]\n    Saving to: ‘train.txt.8’\n    \n    train.txt.8         100%[===================&gt;]   1.58M  --.-KB/s    in 0.1s    \n    \n    2022-04-01 11:59:27 (12.6 MB/s) - ‘train.txt.8’ saved [1658616/1658616]\n    \n    --2022-04-01 11:59:27--  https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt\n    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212\n    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n    HTTP request sent, awaiting response... 301 Moved Permanently\n    Location: /s/raw/2mzialpsgf9k5l3/val.txt [following]\n    --2022-04-01 11:59:27--  https://www.dropbox.com/s/raw/2mzialpsgf9k5l3/val.txt\n    Reusing existing connection to www.dropbox.com:443.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com/cd/0/inline/BinpUxjuQUPZKSAw9nVygw-6QF-JqzCuvRo2N8QqZPM8-Aqp5PxM0tHDJ3zclYqIKMhc_9_ORaLBDtdxeknAqfm_e3E0QJIYPA4tUpTQ7h31LAD_sc__6kyvioIZzjK61S5MlbTyM3YUMq3gPYMRH9_XE5gYrjnC1pddo3lRgrcUrg/file# [following]\n    --2022-04-01 11:59:27--  https://ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com/cd/0/inline/BinpUxjuQUPZKSAw9nVygw-6QF-JqzCuvRo2N8QqZPM8-Aqp5PxM0tHDJ3zclYqIKMhc_9_ORaLBDtdxeknAqfm_e3E0QJIYPA4tUpTQ7h31LAD_sc__6kyvioIZzjK61S5MlbTyM3YUMq3gPYMRH9_XE5gYrjnC1pddo3lRgrcUrg/file\n    Resolving ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com (ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f\n    Connecting to ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com (ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 204240 (199K) [text/plain]\n    Saving to: ‘val.txt.8’\n    \n    val.txt.8           100%[===================&gt;] 199.45K  --.-KB/s    in 0.09s   \n    \n    2022-04-01 11:59:28 (2.23 MB/s) - ‘val.txt.8’ saved [204240/204240]\n    \n    --2022-04-01 11:59:28--  https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212\n    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n    HTTP request sent, awaiting response... 301 Moved Permanently\n    Location: /s/raw/ikkqxfdbdec3fuj/test.txt [following]\n    --2022-04-01 11:59:28--  https://www.dropbox.com/s/raw/ikkqxfdbdec3fuj/test.txt\n    Reusing existing connection to www.dropbox.com:443.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com/cd/0/inline/BileG7vM49CD4NPqfWBG0td8OcodftXS6fihHcq6NCZrPE8Xn9puhgIP1mCk-KXlQnwxW_3WTCdvFmmavZXbvU5qj_mu4PoCB4quNit8j4vVynpa3QWMxcPTiHfQB8UgZaKz319rr67HSjySTKFR1xvmTxTwZIsB0Ixss_Bem8ixQg/file# [following]\n    --2022-04-01 11:59:28--  https://uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com/cd/0/inline/BileG7vM49CD4NPqfWBG0td8OcodftXS6fihHcq6NCZrPE8Xn9puhgIP1mCk-KXlQnwxW_3WTCdvFmmavZXbvU5qj_mu4PoCB4quNit8j4vVynpa3QWMxcPTiHfQB8UgZaKz319rr67HSjySTKFR1xvmTxTwZIsB0Ixss_Bem8ixQg/file\n    Resolving uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com (uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f\n    Connecting to uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com (uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 206760 (202K) [text/plain]\n    Saving to: ‘test.txt.8’\n    \n    test.txt.8          100%[===================&gt;] 201.91K  --.-KB/s    in 0.07s   \n    \n    2022-04-01 11:59:29 (2.95 MB/s) - ‘test.txt.8’ saved [206760/206760]\n\n!head -5 train.txt\n    i didnt feel humiliated;sadness\n    i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness\n    im grabbing a minute to post i feel greedy wrong;anger\n    i am ever feeling nostalgic about the fireplace i will know that it is still on the property;love\n    i am feeling grouchy;anger\n\ndataset_names = ['train', 'validation', 'test']\nfile_names = [url.split('?')[0].split('/')[-1] for url in urls]\ndata_files={name:file for name,file in zip(dataset_names, file_names)}\ndata_files\n    {'train': 'train.txt', 'validation': 'val.txt', 'test': 'test.txt'}\n\nemotions_local = load_dataset(\"csv\", data_files=data_files, sep=\";\", names=[\"text\", \"label\"])\n\nemotions_local\n    DatasetDict({\n        train: Dataset({\n            features: ['text', 'label'],\n            num_rows: 16000\n        })\n        validation: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n        test: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n    })\n\npd.DataFrame(list(emotions_local.cache_files.items()))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\ntrain\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-train.arrow’}]\n\n\n\n\n1\n\n\nvalidation\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-validation.arrow’}]\n\n\n\n\n2\n\n\ntest\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-test.arrow’}]\n\n\n\n\n\n\n\n\nManual Process - Remote\ndata_files = {name:url for name,url in zip(dataset_names,urls)}\ndata_files\n    {'train': 'https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1',\n     'validation': 'https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1',\n     'test': 'https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1'}\n\nemotions_remote = load_dataset(\"csv\", data_files=data_files, sep=\";\", names=[\"text\", \"label\"])\n\nemotions_remote\n    DatasetDict({\n        train: Dataset({\n            features: ['text', 'label'],\n            num_rows: 16000\n        })\n        validation: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n        test: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n    })\n\npd.DataFrame(list(emotions_remote.cache_files.items()))\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\ntrain\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-train.arrow’}]\n\n\n\n\n1\n\n\nvalidation\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-validation.arrow’}]\n\n\n\n\n2\n\n\ntest\n\n\n[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-test.arrow’}]\n\n\n\n\n\n\n\n\nDatasetDict\n\nDocumentation\nA dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n\n\nemotions\n    DatasetDict({\n        train: Dataset({\n            features: ['text', 'label'],\n            num_rows: 16000\n        })\n        validation: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n        test: Dataset({\n            features: ['text', 'label'],\n            num_rows: 2000\n        })\n    })\nNote: The data is already split into training, validation, and test sets.\n\n\nDataset\n\nDocumentation\nThe base class datasets.Dataset implements a Dataset backed by an Apache Arrow table.\nBehaves like an ordinary Python array or list.\n\n\ntrain_ds = emotions[\"train\"]\ntrain_ds\n    Dataset({\n        features: ['text', 'label'],\n        num_rows: 16000\n    })\n\nlen(train_ds)\n    16000\n\ntrain_ds[0]\n    {'text': 'i didnt feel humiliated', 'label': 0}\n\ntrain_ds.column_names\n    ['text', 'label']\n\n# Check the data types used for text and labels.\nprint(train_ds.features)\n    {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}\n\n\n\nClassLabel\n\nDocumentation\nFeature type for integer class labels.\nThis class provides methods to convert integer labels to strings and strings to integer labels.\n\n\ndatasets.ClassLabel\n    datasets.features.features.ClassLabel\n\n\n\nValue\n\nDocumentation\n\n\ndatasets.Value\n    datasets.features.features.Value\n\nprint_source(datasets.Value, False)\n    @dataclass\n    class Value:\n        \"\"\"\n        The Value dtypes are as follows:\n    \n        null\n        bool\n        int8\n        int16\n        int32\n        int64\n        uint8\n        uint16\n        uint32\n        uint64\n        float16\n        float32 (alias float)\n        float64 (alias double)\n        timestamp[(s|ms|us|ns)]\n        timestamp[(s|ms|us|ns), tz=(tzstring)]\n        binary\n        large_binary\n        string\n        large_string\n        \"\"\"\n        dtype: str\n        id: Optional[str] = None\n        pa_type: ClassVar[Any] = None\n        _type: str = field(default='Value', init=False, repr=False)\n    \n        def __post_init__(self):\n            if self.dtype == 'double':\n                self.dtype = 'float64'\n            if self.dtype == 'float':\n                self.dtype = 'float32'\n            self.pa_type = string_to_arrow(self.dtype)\n    \n        def __call__(self):\n            return self.pa_type\n    \n        def encode_example(self, value):\n            if pa.types.is_boolean(self.pa_type):\n                return bool(value)\n            elif pa.types.is_integer(self.pa_type):\n                return int(value)\n            elif pa.types.is_floating(self.pa_type):\n                return float(value)\n            elif pa.types.is_string(self.pa_type):\n                return str(value)\n            else:\n                return value\n\n\n\nFrom Datasets to DataFrames\n\nHugging Face Datasets provides a set_format method to convert Datasets objects to Pandas DataFrames.\nThe underlying data format is still an Arrow table.\n\n\nDatasetDict.set_format\n\nDocumentation\nSet the format for every Dataset object in the dictionary.\n\n\nprint_source(datasets.DatasetDict.set_format, exclude_doc=True)\n    def set_format(self, type: Optional[str]=None, columns: Optional[List]=None,\n        output_all_columns: bool=False, **format_kwargs):\n        self._check_values_type()\n        for dataset in self.values():\n            dataset.set_format(type=type, columns=columns, output_all_columns=\n                output_all_columns, **format_kwargs)\n\n\nDataset.set_format\n\nDocumentation\nSet the __getitem__ return format.\n\nNone (Python object), numpy, torch, tensorflow, pandas, arrow\n\n\n\nprint_source(datasets.Dataset.set_format, exclude_doc=True)\n    @fingerprint_transform(inplace=True)\n    def set_format(self, type: Optional[str]=None, columns: Optional[List]=None,\n        output_all_columns: bool=False, **format_kwargs):\n        format_kwargs.update(format_kwargs.pop('format_kwargs', {}))\n        type = get_format_type_from_alias(type)\n        _ = get_formatter(type, features=self.features, **format_kwargs)\n        if isinstance(columns, str):\n            columns = [columns]\n        if isinstance(columns, tuple):\n            columns = list(columns)\n        if columns is not None and any(col not in self._data.column_names for\n            col in columns):\n            raise ValueError(\n                f'Columns {list(filter(lambda col: col not in self._data.column_names, columns))} not in the dataset. Current columns in the dataset: {self._data.column_names}'\n                )\n        if columns is not None:\n            columns = columns.copy()\n        self._format_type = type\n        self._format_kwargs = format_kwargs\n        self._format_columns = columns\n        self._output_all_columns = output_all_columns\n        logger.debug(\n            'Set __getitem__(key) output type to %s for %s columns  (when key is int or slice) and %s output other (un-formatted) columns.'\n            , 'python objects' if type is None else type, 'no' if columns is\n            None else str(columns), 'do' if output_all_columns else \"don't\")\n\nemotions.set_format(type=\"pandas\")\ndf = emotions[\"train\"][:]\ndf.head()\n\n\n\n\n\n\n\n\ntext\n\n\nlabel\n\n\n\n\n\n\n0\n\n\ni didnt feel humiliated\n\n\n0\n\n\n\n\n1\n\n\ni can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n\n\n0\n\n\n\n\n2\n\n\nim grabbing a minute to post i feel greedy wrong\n\n\n3\n\n\n\n\n3\n\n\ni am ever feeling nostalgic about the fireplace i will know that it is still on the property\n\n\n2\n\n\n\n\n4\n\n\ni am feeling grouchy\n\n\n3\n\n\n\n\n\n\n\n\nClassLabel.int2str\n\nDocumentation\nConvert an integer label to the corresponding class name string.\n\n\nprint_source(datasets.ClassLabel.int2str)\n    def int2str(self, values: Union[int, Iterable]):\n        assert isinstance(values, int) or isinstance(values, Iterable\n            ), f'Values {values} should be an integer or an Iterable (list, numpy array, pytorch, tensorflow tensors)'\n        return_list = True\n        if isinstance(values, int):\n            values = [values]\n            return_list = False\n        for v in values:\n            if not 0 &lt;= v &lt; self.num_classes:\n                raise ValueError(f'Invalid integer class label {v:d}')\n        if self._int2str:\n            output = [self._int2str[int(v)] for v in values]\n        else:\n            output = [str(v) for v in values]\n        return output if return_list else output[0]\n\n# Get the corresponding emotion name\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n# Add a new column with the corresponding emotion name\ndf[\"emotion\"] = df[\"label\"].apply(label_int2str)\ndf.head()\n\n\n\n\n\n\n\n\ntext\n\n\nlabel\n\n\nemotion\n\n\n\n\n\n\n0\n\n\ni didnt feel humiliated\n\n\n0\n\n\nsadness\n\n\n\n\n1\n\n\ni can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n\n\n0\n\n\nsadness\n\n\n\n\n2\n\n\nim grabbing a minute to post i feel greedy wrong\n\n\n3\n\n\nanger\n\n\n\n\n3\n\n\ni am ever feeling nostalgic about the fireplace i will know that it is still on the property\n\n\n2\n\n\nlove\n\n\n\n\n4\n\n\ni am feeling grouchy\n\n\n3\n\n\nanger\n\n\n\n\n\n\n\n\n\nLooking at the Class Distribution\n\nA Recipe for Training Neural Networks\n\nThe first step to training a neural network involves thoroughly inspecting the data.\nUnderstand the distribution of the training examples and look for patterns.\n\nDatasets with skewed class distribution might require a different treatment regarding the training loss and evaluation metrics.\n\n\nimport matplotlib.pyplot as plt\n# Increase the figure size\nplt.rcParams[\"figure.figsize\"] = (10,6)\n# Create a horizontal bar chart\ndf[\"emotion\"].value_counts(ascending=True).plot.barh()\nplt.title(\"Frequency of Classes\")\nplt.show()\n# Reset the figure size\nplt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n\n\n\n\n\nNote: Messages expressing joy and sadness are about 5-10 times more common than messages expressing love and surprise.\n\nMethods to Deal with Imbalanced Data\n\nRandomly oversample the minority class.\nRandomly undersample the majority class.\nGather more labeled data from the underrepresented classes.\n\n\n\nimbalanced-learn\n\nDocumentation\nThis library extends scikit-learn and provides tools for dealing with imbalanced classes.\n\n\n\n\nHow Long Are Our Tweets?\n\nTransformer models have a maximum input sequence length called the maximum context size.\nThe maximum context size for DistilBERT is 512 tokens, which is roughly equivalent to a few paragraphs of text.\nWe need to truncate pieces of text that do not fit in a model’s context size, which might remove crucial information.\nWe can approximate the number of tokens per twee for each emotion by looking at the distribution of words per tweet.\n\n\n# Increase the figure size\nplt.rcParams[\"figure.figsize\"] = (10,7)\n# Create a new column containing the number of words for each tweet\ndf[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n# Create a box plot\ndf.boxplot(\"Words Per Tweet\", by=\"emotion\", grid=False, showfliers=False, color=\"black\")\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()\n# Reset the figure size\nplt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n\n\n\n\n\nNote: Most tweets are between 15 and 20 words long, with a max length of around 50 words.\n\nDatasetDict.reset_format()\n\nDocumentation\nreturn format to python objects for all datasets in the dictionary\ncalls set_format with the default arguments\n\n\ndatasets.DatasetDict.reset_format\n    &lt;function datasets.dataset_dict.DatasetDict.reset_format(self)&gt;\n\nprint_source(datasets.DatasetDict.reset_format)\n    def reset_format(self):\n        self._check_values_type()\n        for dataset in self.values():\n            dataset.set_format()\n\nprint_source(datasets.Dataset.reset_format)\n    def reset_format(self):\n        self.set_format()\n\nemotions.reset_format()"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#from-text-to-tokens",
    "href": "posts/transformers-book-notes/chapter-2/index.html#from-text-to-tokens",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "From Text to Tokens",
    "text": "From Text to Tokens\n\nTransformer models cannot receive raw strings as input.\nText first needs to be tokenized and encoded as numerical vectors.\nThe three main tokenization strategies are character tokenization, word tokenization, and subword tokenization.\n\nIMPORTANT: Use the same tokenizer when training, fine-tuning, and performing inference with a given model.\n\nCharacter Tokenization\n\nCharacter-based tokenizers split the text into single characters.\nCharacter tokenization results in a smaller vocabulary and much fewer out-of-vocabulary tokens.\nIt also results in a much higher number of tokens for a given input sequence.\nA character-based representation is less meaningful compared to using words.\nThe model needs to learn linguistic structures like words from the data, making training more expensive.\nMost projects do not use character tokenization.\n\n\ntext = \"Tokenizing text is a core task of NLP.\"\ntokenized_text = list(text)\nprint(tokenized_text)\n    ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n\n\nNumericalization\n\nModels can only process numbers, so we need to encode tokens as numerical data.\nA simple encoding method is to convert each unique token to a unique integer.\n\n\n# Map each unique token to a unique integer\ntoken2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\nprint(token2idx)\n    {' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n\n# Encode the tokenized text\ninput_ids = [token2idx[token] for token in tokenized_text]\nprint(input_ids)\n    [5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n\n\nOne-hot Encoding\n\nIt is common to encode categorical variables as one-hot vectors, where a single entry has the value 1, and every other entry has the value 0.\nOne-hot encoding can help prevent the model from learning undesired relationships like fictitious ordering between names.\n\n\n# Sample categorical data\ncategorical_df = pd.DataFrame(\n    {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\ncategorical_df\n\n\n\n\n\n\n\n\nName\n\n\nLabel ID\n\n\n\n\n\n\n0\n\n\nBumblebee\n\n\n0\n\n\n\n\n1\n\n\nOptimus Prime\n\n\n1\n\n\n\n\n2\n\n\nMegatron\n\n\n2\n\n\n\n\n\n\nNote: A model might interpret the order of values in the Label ID column as significant.\n# Create one-hot vectors for each unique value in the Name column\npd.get_dummies(categorical_df[\"Name\"])\n\n\n\n\n\n\n\n\nBumblebee\n\n\nMegatron\n\n\nOptimus Prime\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\nNote: We can use this approach to prevent the model from learning similar undesired relationships in the input_ids list.\nimport torch\nimport torch.nn.functional as F\n\nPyTorch one_hot:\n\nDocumentation\nGenerate one-hot encodings for a tensor with a specified number of classes\n\n\nlen(input_ids), len(token2idx)\n    (38, 20)\n\n# Convert input_ids list to a tensor\ninput_ids = torch.tensor(input_ids)\n# Generate one-hot encodings\none_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\none_hot_encodings.shape\n    torch.Size([38, 20])\nNote: Make sure to set num_classes to the vocabulary size.\nprint(f\"Token: {tokenized_text[0]}\")\nprint(f\"Tensor index: {input_ids[0]}\")\nprint(f\"One-hot: {one_hot_encodings[0]}\")\n    Token: T\n    Tensor index: 5\n    One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nWord Tokenization\n\nWord-based tokenizers split the text into words and map each word to an integer.\nWord tokenization creates less work for the model as it does not need to learn such linguistic structures from the data.\nThere is a loss of meaning across very similar words.\nUser-defined rules tell the tokenizer how to split the raw text into words.\nA simple tokenization method is to split text using whitespace.\nMore sophisticated word tokenizers have additional rules to handle punctuation.\nOther methods involve stemming or lemmatization to normalize words to their stem.\n\nExample: “great”, “greater”, and “greatest” all become “great”\nThese methods come at the expense of losing some information in the text.\n\nWord tokenization results in a bigger vocabulary size, which requires more model parameters.\n\nThe first layer for a model that compressed the input vectors for a vocabulary with one million unique words to one thousand dimensional vectors would contain one billion weights.\n\nA popular method to limit the vocabulary size involves only adding the 100,000 most common words in the corpus.\n\nWords that are not part of the vocabulary are classified as unknown and mapped to a shared token.\nThis approach risks losing potentially important information related to rare words.\n\n\n\ntokenized_text = text.split()\nprint(tokenized_text)\n    ['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n\n\nSubword Tokenization\n\nSubword tokenization algorithms decompose rare words into meaningful subwords while keeping the most frequently used words as unique entities.\nSubword tokenization algorithms can identify start-of-word tokens.\nMost state-of-the-art English models use subword-tokenization.\nSubword tokenization helps keep vocabulary size and input length manageable by sharing information across different words.\nThere are several tokenization methods used in NLP.\n\nThe tokenizers for both the BERT and DistilBERT models use WordPiece.\nGPT-2 uses Byte Pair Encoding.\nSeveral multilingual models use SentencePiece or Unigram Segmentation.\n\n\n\nfrom transformers import AutoTokenizer\n\nAutoTokenizer\n\nDocumentation\nQuickly load the tokenizer associated with a pretrained model.\nAutoTokenizer belongs to a set of auto classes that automatically retrieve the model’s configuration, pretrained weights, or vocabulary from the name of a checkpoint.\n\n\nAutoTokenizer\n    transformers.models.auto.tokenization_auto.AutoTokenizer\n\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nNote: Hugging Face automatically caches the parameters of the pretrained tokenizer after the first download.\n\ntype(tokenizer)\ntransformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast\n\n\nDistilBertTokenizerFast\n\nDocumentation\nConstruct a “fast” DistilBERT tokenizer that runs end-to-end tokenization, including punctuation and WordPiece.\n\n\ntokenizer\n    PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n\ntokenizer.init_kwargs\n    {'do_lower_case': True,\n     'unk_token': '[UNK]',\n     'sep_token': '[SEP]',\n     'pad_token': '[PAD]',\n     'cls_token': '[CLS]',\n     'mask_token': '[MASK]',\n     'tokenize_chinese_chars': True,\n     'strip_accents': None,\n     'model_max_length': 512,\n     'special_tokens_map_file': None,\n     'name_or_path': 'distilbert-base-uncased'}\n\nprint(text)\n    Tokenizing text is a core task of NLP.\n\n# Map the raw text content to unique integers\nencoded_text = tokenizer(text)\nprint(encoded_text)\n    {'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nconvert_ids_to_tokens\n\nDocumentation\nConvert a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n\n\nprint_source(tokenizer.convert_ids_to_tokens)\n    def convert_ids_to_tokens(self, ids: Union[int, List[int]],\n        skip_special_tokens: bool=False) -&gt;Union[str, List[str]]:\n        if isinstance(ids, int):\n            return self._tokenizer.id_to_token(ids)\n        tokens = []\n        for index in ids:\n            index = int(index)\n            if skip_special_tokens and index in self.all_special_ids:\n                continue\n            tokens.append(self._tokenizer.id_to_token(index))\n        return tokens\n\n# Convert integer ids to tokens\ntokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\nprint(tokens)\n    ['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\nNote: 1. The [CLS] and [SEP] tokens indicate the start and end of a sequence respectively. 2. All the tokens are lower case. 3. The words “tokenizing” and “NLP” have been decomposed into subwords since they are rare. 4. The ## prefix indicates the preceding string was not whitespace.\n\n# Convert tokens to plain text\nprint(tokenizer.convert_tokens_to_string(tokens))\n    [CLS] tokenizing text is a core task of nlp. [SEP]\n\nprint_source(tokenizer.convert_tokens_to_string)\n    def convert_tokens_to_string(self, tokens: List[str]) -&gt;str:\n        return self.backend_tokenizer.decoder.decode(tokens)\n\ntokenizer.vocab_size\n    30522\n\ntokenizer.model_max_length\n    512\n\ntokenizer.model_input_names\n    ['input_ids', 'attention_mask']\n\n\n\nTokenizing the Whole Dataset\n\nWe need to define a processing function to tokenize training examples.\n\n\ndef tokenize(batch):\n    # Apply the tokenizer to a batch of examples\n    return tokenizer(batch[\"text\"],\n                     # Pad examples with zeros to the longest one in the batch\n                     padding=True, \n                     # Truncate the examples to the model's maximum context size\n                     truncation=True)\nprint(tokenize(emotions[\"train\"][:2]))\n    {'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\nNote: 1. The zeros have a corresponding [PAD] token in the vocabulary. 2. The attention mask allows the model to ignore padded parts of the input.\n\ntokens2ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\ndata = sorted(tokens2ids, key=lambda x : x[-1])\ndf = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\ndf.T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nSpecial Token\n\n\n[PAD]\n\n\n[UNK]\n\n\n[CLS]\n\n\n[SEP]\n\n\n[MASK]\n\n\n\n\nSpecial Token ID\n\n\n0\n\n\n100\n\n\n101\n\n\n102\n\n\n103\n\n\n\n\n\n\n\nDatasetDict.map\n\nDocumentation\nApply a function to all the elements in the tables for all datasets in the dictionary.\n\n\nprint_source(datasets.DatasetDict.map)\n    def map(self, function, with_indices: bool=False, input_columns: Optional[\n        Union[str, List[str]]]=None, batched: bool=False, batch_size: Optional[\n        int]=1000, remove_columns: Optional[Union[str, List[str]]]=None,\n        keep_in_memory: bool=False, load_from_cache_file: bool=True,\n        cache_file_names: Optional[Dict[str, Optional[str]]]=None,\n        writer_batch_size: Optional[int]=1000, features: Optional[Features]=\n        None, disable_nullable: bool=False, fn_kwargs: Optional[dict]=None,\n        num_proc: Optional[int]=None, desc: Optional[str]=None) -&gt;'DatasetDict':\n        self._check_values_type()\n        if cache_file_names is None:\n            cache_file_names = {k: None for k in self}\n        return DatasetDict({k: dataset.map(function=function, with_indices=\n            with_indices, input_columns=input_columns, batched=batched,\n            batch_size=batch_size, remove_columns=remove_columns,\n            keep_in_memory=keep_in_memory, load_from_cache_file=\n            load_from_cache_file, cache_file_name=cache_file_names[k],\n            writer_batch_size=writer_batch_size, features=features,\n            disable_nullable=disable_nullable, fn_kwargs=fn_kwargs, num_proc=\n            num_proc, desc=desc) for k, dataset in self.items()})\n\n\nDataset.map\n\nDocumentation\nApply a function to all the examples in the table and update the table.\n\n\n# Apply the processing function across all splits in the corpus in batches\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\nNote: * The map function applies the processing function to the entire dataset as a single batch when the batch size is None. * This approach ensures the input tensors and attention masks have the same shape globally.\nprint(emotions_encoded[\"train\"].column_names)\n    ['attention_mask', 'input_ids', 'label', 'text']"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#training-a-text-classifier",
    "href": "posts/transformers-book-notes/chapter-2/index.html#training-a-text-classifier",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "Training a Text Classifier",
    "text": "Training a Text Classifier\n\nModels like DistilBERT are pretrained to predict masked words in a sequence, and we need to modify them for text classification.\nWe can combine the body of the pretrained model with a custom classification head.\n\n\nArchitecture of an Encoder-Based Classifier\n\nTokenize the text and represent it as one-hot vectors called token encodings.\n\nThe size of the tokenized vocabulary determines the dimensions of the token encodings and usually consists of 20 thousand to 200 thousand unique tokens.\n\nConvert the token encodings to token embeddings, which are vectors living in the lowe-dimensional space.\nPass the token embeddings through the encoder block layers, which yield a hidden state for each input token.\nReplace the pretrained language modeling layer with a classification layer.\n\nNote: PyTorch skips the step of creating one-hot vectors because multiplying a matrix with a one-hot vector is the same as selecting a column with the token ID from the matrix.\n\n\nMethods to Train a Text Classifier\n\nFeature Extraction\n\nUse the hidden states as features and train the classifier on them without modifying the pretrained model.\n\n\n\nFine-tuning\n\nTrain the whole model end-to-end, which also updates the parameters of the pretrained model.\n\n\n\n\nTransformers as Feature Extractors\n\nThis method is well-suited for quickly training a small or shallow model.\nThe model could be a neural classification layer or a method that does not rely on gradients like random forests.\nUsing the transformer as a feature extractor is especially useful when GPUs are unavailable since the hidden states only need to be precomputed once.\n\n\nUsing pretrained models\n\nfrom transformers import AutoModel\n\n\nAutoModel.from_pretrained\n\nDocumentation\nInstantiate one of the base model classes of the library from a pretrained model.\n\n\nprint_source(AutoModel.from_pretrained)\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        trust_remote_code = kwargs.pop('trust_remote_code', False)\n        kwargs['_from_auto'] = True\n        if not isinstance(config, PretrainedConfig):\n            config, kwargs = AutoConfig.from_pretrained(\n                pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs)\n        if hasattr(config, 'auto_map') and cls.__name__ in config.auto_map:\n            if not trust_remote_code:\n                raise ValueError(\n                    f'Loading {pretrained_model_name_or_path} requires you to execute the modeling file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.'\n                    )\n            if kwargs.get('revision', None) is None:\n                logger.warn(\n                    'Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.'\n                    )\n            class_ref = config.auto_map[cls.__name__]\n            module_file, class_name = class_ref.split('.')\n            model_class = get_class_from_dynamic_module(\n                pretrained_model_name_or_path, module_file + '.py', class_name,\n                **kwargs)\n            return model_class.from_pretrained(pretrained_model_name_or_path, *\n                model_args, config=config, **kwargs)\n        elif type(config) in cls._model_mapping.keys():\n            model_class = _get_model_class(config, cls._model_mapping)\n            return model_class.from_pretrained(pretrained_model_name_or_path, *\n                model_args, config=config, **kwargs)\n        raise ValueError(\n            f\"\"\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n    Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\"\"\n            )\n\nmodel_ckpt = \"distilbert-base-uncased\"\n# Use a CUDA GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Instantiate a pretrained DistilBertModel\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)\n\nmodel\n    DistilBertModel(\n      (embeddings): Embeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): Transformer(\n        (layer): ModuleList(\n          (0): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n          (1): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n          (2): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n          (3): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n          (4): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n          (5): TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n    )\n\n\nExtracting the last hidden states\n\n# Encode some sample text\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nprint(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n    Input tensor shape: torch.Size([1, 6])\n\n# Move the input tensors to the same device as the model\ninputs = {k:v.to(device) for k,v in inputs.items()}\n# Get the last hidden states for the sample text\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)\n    BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n             [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n             [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n             [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n             [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n             [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n           device='cuda:0'), hidden_states=None, attentions=None)\nNote: The hidden state tensor has the shape [batch_size, n_tokens, hidden_dim].\n\noutputs.last_hidden_state.size()\n    torch.Size([1, 6, 768])\n\noutputs.last_hidden_state[:,0].size()\n    torch.Size([1, 768])\nNote: * For classification tasks, it is common to use the hidden state associated with the [CLS] token as the input feature. * Since the [CLS] token appears at the start of each sequence, we can extract it by accessing the associated index of the hidden state tensor.\n\ndef extract_hidden_states(batch):\n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n    # Disable automatic calculation of the gradient\n    with torch.no_grad():\n        # Extract last hidden states\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\nNote: The map() method requires the processing function to return Python or NumPy objects.\n\nemotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# Extract the hidden states for every token in the dataset\nemotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)\n\nemotions_hidden[\"train\"].column_names\n    ['attention_mask', 'hidden_state', 'input_ids', 'label', 'text']\n\n\nCreating a feature matrix\n\nWe can use the hidden states as input features and the labels as targets.\n\n\nimport numpy as np\n# Get the input and target data for the training and validation sets\nX_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\nX_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\ny_train = np.array(emotions_hidden[\"train\"][\"label\"])\ny_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\nX_train.shape, X_valid.shape\n    ((16000, 768), (2000, 768))\nX_train[0].size, y_train[0].size\n    (768, 1)\n\n\nVisualizing the training set\n\nfrom umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nUMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\n\nDocumentation\nUMAP is a dimension reduction technique that can be useful for visualization as a drop-in replacement for t-SNE.\nWe can use the UMAP algorithm to scale the 768-dimensional vectors down to a 2-dimensional representation.\nUMAP works best with feature values scaled to [0,1].\n\n\n\nScit-Kit Learn MinMaxScaler\n\nDocumentation\nTransform features by scaling each to a given range.\n\n\n# Scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n# Initialize and fit UMAP\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n# Create a DataFrame of 2D embeddings\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\ndf_emb[\"label\"] = y_train\ndf_emb.head()\n\n\n\n\n\n\n\n\nX\n\n\nY\n\n\nlabel\n\n\n\n\n\n\n0\n\n\n4.345317\n\n\n6.545871\n\n\n0\n\n\n\n\n1\n\n\n-2.770711\n\n\n5.816418\n\n\n0\n\n\n\n\n2\n\n\n5.433491\n\n\n3.048345\n\n\n3\n\n\n\n\n3\n\n\n-2.210309\n\n\n3.550199\n\n\n2\n\n\n\n\n4\n\n\n-3.055895\n\n\n3.723285\n\n\n3\n\n\n\n\n\n\nNote: The UMAP algorithm has compressed the hidden state vectors from 768 dimensions to 2 dimensions.\n\n\nmatplotlib.pyplot.hexbin\n\nDocumentation\nMake a 2D hexagonal binning plot of points x, y.\n\n\nfig, axes = plt.subplots(2, 3, figsize=(14,10))\n# Collapse the array into one dimension\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\nlabels = emotions[\"train\"].features[\"label\"].names\n\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {i}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap, gridsize=20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n\n# Adjust the padding between and around subplots.\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nNote: * The negative feelings such as sadness, anger, and fear occupy similar regions in the hidden state with slightly varying distributions. * The positive emotions, joy, and love, are well separated from the negative emotions and share a similar space. * Surprise is scattered all over the place. * The model did not train to know the difference between these emotions. It learned them implicitly by guessing masked words in the training corpus. * Just because some categories overlap when projected onto a lower-dimensional space does not mean they are not separable in the original space.\n\n\nTraining a simple classifier\n\nWe can use the hidden states to train a simple logistic regression model.\nThis type of model is trains quickly and does not require a GPU.\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nsklearn.linear_model.LogisticRegression\n\nDocumentation\nLogistic Regression classifier\n\n\n# Increase `max_iter` to guarantee convergence \nlr_clf = LogisticRegression(max_iter=3000)\nlr_clf.fit(X_train, y_train)\n    LogisticRegression(max_iter=3000)\n\nlr_clf.score(X_valid, y_valid)\n    0.6335\nNote: The model performs well, considering the training data is unbalanced.\nfrom sklearn.dummy import DummyClassifier\n\n\nsklearn.dummy.DummyClassifier\n\nDocumentation\nA DummyClassifier makes predictions using a predefined strategy and ignores the input features.\nThe classifier serves as a simple baseline to compare against other more complex classifiers.\n\n\n# Set the DummyClassifier to always select the most frequent class\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_clf.score(X_valid, y_valid)\n    0.352\nNote: The simple logistic regression classifier performs significantly better than a model that always selects the most frequent class.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\n\nsklearn.metrics.ConfusionMatrixDisplay\n\nDocumentation\nCreate a confusion matrix visualization\n\n\n\nsklearn.metrics.confusion_matrix\n\nDocumentation\nCompute confusion matrix to evaluate the accuracy of a classification.\n\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n    \ny_preds = lr_clf.predict(X_valid)\nplot_confusion_matrix(y_preds, y_valid, labels)\n\n\n\n\n\nNote: * Anger and fear are most often confused with sadness. * Love and surprise are frequently mistaken for joy.\n\n\n\nFine-Tuning Transformers\n\nFine-tuning results in superior performance than feature extraction but requires more computational resources such as GPUs.\nFine-tuning involves training the hidden states, so the classification head needs to be differentiable.\nTraining the hidden states that serve as input to the classifier helps avoid the problem of working with data that may not be well suited for the classification task.\n\n\nLoading a pretrained model\n\nfrom transformers import AutoModelForSequenceClassification\n\n\nAutoModelForSequenceClassification.from_pretrained\n\nDocumentation\nInstantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.\n\n\n# Specify the number of labels (i.e. the number of emotions)\nnum_labels = 6\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\nNote: The classifier head is randomly initialized.\n\ntype(model)\ntransformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification\n\n\nDistilBertForSequenceClassification\n\nDocumentation\nDistilBert Model transformer with a sequence classification head on top\n\n\nfor child in model.named_children(): print(child[0])\n    distilbert\n    pre_classifier\n    classifier\n    dropout\n\nlist(model.named_children())[-3:]\n    [('pre_classifier', Linear(in_features=768, out_features=768, bias=True)),\n     ('classifier', Linear(in_features=768, out_features=6, bias=True)),\n     ('dropout', Dropout(p=0.2, inplace=False))]\n\n\nDefining the performance metrics\n\nWe need to define a function to compute metrics for the trainer so we can monitor performance during training.\nThe function receives an EvalPrediction object containing predictions and label_ids attributes and returns a dictionary that maps each metric’s name to its value.\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\nsklearn.metrics.f1_score\n\nDocumentation\nCompute the \\(F_{1}\\)-score\n\n\n\nsklearn.metrics.accuracy_score\n\nDocumentation\nCompute the classification accuracy score.\n\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\n\nTraining the model\n\nWe can use the Hugging Face Hub API to push our fine-tuned model to our account on the Hub and share it with the community.\n\n\nfrom huggingface_hub import notebook_login\n\ninspect.getdoc(notebook_login)\n    'Displays a widget to login to the HF website and store the token.'\n\nprint_source(notebook_login)\n    def notebook_login():\n        try:\n            import ipywidgets.widgets as widgets\n            from IPython.display import clear_output, display\n        except ImportError:\n            raise ImportError(\n                'The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywdidgets` module: `pip install ipywidgets`.'\n                )\n        box_layout = widgets.Layout(display='flex', flex_flow='column',\n            align_items='center', width='50%')\n        token_widget = widgets.Password(description='Token:')\n        token_finish_button = widgets.Button(description='Login')\n        switch_button = widgets.Button(description='Use password')\n        login_token_widget = widgets.VBox([widgets.HTML(\n            NOTEBOOK_LOGIN_TOKEN_HTML_START), token_widget, token_finish_button,\n            widgets.HTML(NOTEBOOK_LOGIN_TOKEN_HTML_END), switch_button], layout\n            =box_layout)\n        display(login_token_widget)\n        input_widget = widgets.Text(description='Username:')\n        password_widget = widgets.Password(description='Password:')\n        password_finish_button = widgets.Button(description='Login')\n        login_password_widget = widgets.VBox([widgets.HTML(value=\n            NOTEBOOK_LOGIN_PASSWORD_HTML), widgets.HBox([input_widget,\n            password_widget]), password_finish_button], layout=box_layout)\n    \n        def login_token_event(t):\n            token_widget.value = ''\n            clear_output()\n            _login(HfApi(), token=token)\n        token_finish_button.on_click(login_token_event)\n    \n        def login_password_event(t):\n            password = password_widget.value\n            password_widget.value = ''\n            clear_output()\n            _login(HfApi(), username=username, password=password)\n        password_finish_button.on_click(login_password_event)\n    \n        def switch_event(t):\n            display(login_password_widget)\n        switch_button.on_click(switch_event)\n\nnotebook_login()\n!git config --global credential.helper store\nNote: The equivalent terminal command is huggingface-cli login.\n\nfrom transformers import TrainingArguments\n\n\nTrainingArguments\n\nDocumentation\nThe TrainingArguments class provides fine-grained control over the arguments related to the training loop.\n\n\npd.DataFrame(inspect.signature(TrainingArguments).parameters).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n28\n\n\n29\n\n\n30\n\n\n31\n\n\n32\n\n\n33\n\n\n34\n\n\n35\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n53\n\n\n54\n\n\n55\n\n\n56\n\n\n57\n\n\n58\n\n\n59\n\n\n60\n\n\n61\n\n\n62\n\n\n63\n\n\n64\n\n\n65\n\n\n66\n\n\n67\n\n\n68\n\n\n69\n\n\n70\n\n\n71\n\n\n72\n\n\n73\n\n\n74\n\n\n75\n\n\n76\n\n\n77\n\n\n78\n\n\n79\n\n\n\n\n\n\n0\n\n\noutput_dir\n\n\noverwrite_output_dir\n\n\ndo_train\n\n\ndo_eval\n\n\ndo_predict\n\n\nevaluation_strategy\n\n\nprediction_loss_only\n\n\nper_device_train_batch_size\n\n\nper_device_eval_batch_size\n\n\nper_gpu_train_batch_size\n\n\nper_gpu_eval_batch_size\n\n\ngradient_accumulation_steps\n\n\neval_accumulation_steps\n\n\nlearning_rate\n\n\nweight_decay\n\n\nadam_beta1\n\n\nadam_beta2\n\n\nadam_epsilon\n\n\nmax_grad_norm\n\n\nnum_train_epochs\n\n\nmax_steps\n\n\nlr_scheduler_type\n\n\nwarmup_ratio\n\n\nwarmup_steps\n\n\nlog_level\n\n\nlog_level_replica\n\n\nlog_on_each_node\n\n\nlogging_dir\n\n\nlogging_strategy\n\n\nlogging_first_step\n\n\nlogging_steps\n\n\nlogging_nan_inf_filter\n\n\nsave_strategy\n\n\nsave_steps\n\n\nsave_total_limit\n\n\nsave_on_each_node\n\n\nno_cuda\n\n\nseed\n\n\nfp16\n\n\nfp16_opt_level\n\n\nfp16_backend\n\n\nfp16_full_eval\n\n\nlocal_rank\n\n\nxpu_backend\n\n\ntpu_num_cores\n\n\ntpu_metrics_debug\n\n\ndebug\n\n\ndataloader_drop_last\n\n\neval_steps\n\n\ndataloader_num_workers\n\n\npast_index\n\n\nrun_name\n\n\ndisable_tqdm\n\n\nremove_unused_columns\n\n\nlabel_names\n\n\nload_best_model_at_end\n\n\nmetric_for_best_model\n\n\ngreater_is_better\n\n\nignore_data_skip\n\n\nsharded_ddp\n\n\ndeepspeed\n\n\nlabel_smoothing_factor\n\n\nadafactor\n\n\ngroup_by_length\n\n\nlength_column_name\n\n\nreport_to\n\n\nddp_find_unused_parameters\n\n\ndataloader_pin_memory\n\n\nskip_memory_metrics\n\n\nuse_legacy_prediction_loop\n\n\npush_to_hub\n\n\nresume_from_checkpoint\n\n\nhub_model_id\n\n\nhub_strategy\n\n\nhub_token\n\n\ngradient_checkpointing\n\n\npush_to_hub_model_id\n\n\npush_to_hub_organization\n\n\npush_to_hub_token\n\n\nmp_parameters\n\n\n\n\n\n\n\nbatch_size = 64\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=True, \n                                  log_level=\"error\")\ntraining_args.output_dir\n    'distilbert-base-uncased-finetuned-emotion'\n\nfrom transformers import Trainer\n\n\nTrainer\n\nDocumentation\nThe Trainer class provides a simple, feature-complete training and eval loop for PyTorch, optimized for Hugging Face Transformers.\n\nNote: Install Git-LFS before running the following code cell.\n\ntrainer = Trainer(model=model, args=training_args, \n                  compute_metrics=compute_metrics,\n                  train_dataset=emotions_encoded[\"train\"],\n                  eval_dataset=emotions_encoded[\"validation\"],\n                  tokenizer=tokenizer)\nNote: Had to add the following workaround\n\nold_collator = trainer.data_collator\ntrainer.data_collator = lambda data: dict(old_collator(data))\n\ntrainer.train();\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n    &lt;thead&gt;\n        &lt;tr style=\"text-align: left;\"&gt;\n            &lt;th&gt;Epoch&lt;/th&gt;\n            &lt;th&gt;Training Loss&lt;/th&gt;\n            &lt;th&gt;Validation Loss&lt;/th&gt;\n            &lt;th&gt;Accuracy&lt;/th&gt;\n            &lt;th&gt;F1&lt;/th&gt;\n        &lt;/tr&gt;\n    &lt;/thead&gt;\n    &lt;tbody&gt;\n        &lt;tr&gt;\n            &lt;td&gt;1&lt;/td&gt;\n            &lt;td&gt;0.044200&lt;/td&gt;\n            &lt;td&gt;0.239172&lt;/td&gt;\n            &lt;td&gt;0.926000&lt;/td&gt;\n            &lt;td&gt;0.926452&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;2&lt;/td&gt;\n            &lt;td&gt;0.046300&lt;/td&gt;\n            &lt;td&gt;0.220463&lt;/td&gt;\n            &lt;td&gt;0.936000&lt;/td&gt;\n            &lt;td&gt;0.936133&lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n&lt;/table&gt;\n\n\npreds_output = trainer.predict(emotions_encoded[\"validation\"])\nNote: The predict method returns a PredictionOutput object, which contains arrays of predictions and label_ids, along with the user-defined metrics.\n\ntype(preds_output)\ntransformers.trainer_utils.PredictionOutput\n\npreds_output.metrics\n{'test_loss': 0.22046349942684174,\n 'test_accuracy': 0.936,\n 'test_f1': 0.9361334972007946,\n 'test_runtime': 1.522,\n 'test_samples_per_second': 1314.038,\n 'test_steps_per_second': 21.025}\nNote: The fine-tuned model performs significantly better than the feature-based logistic regression classifier.\n\n# Get the predicted labels\ny_preds = np.argmax(preds_output.predictions, axis=1)\nplot_confusion_matrix(y_preds, y_valid, labels)\n\n\n\n\n\nNote:\n\nThis one is much closer to the ideal diagonal confusion matrix than the confusion matrix for the logistic regression model.\nThe love category is still often confused with joy.\nSurprise is frequently confused for joy or fear.\n\n\n\nError analysis\n\nA simple error-analysis technique involves sorting the validation samples by the model loss.\n\nThis approach allows us to find and correct mislabeled data.\n\nInspecting the model’s weakest predictions can help identify quirks of the dataset.\n\nCleaning the data or injecting similar examples can make the model more robust.\n\nWe can significantly improve model performance by refining the dataset without obtaining more data or using a larger model.\n\n\nfrom torch.nn.functional import cross_entropy\ndef forward_pass_with_label(batch):\n    # Place all input tensors on the same device as the model\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad():\n        output = model(**inputs)\n        pred_label = torch.argmax(output.logits, axis=-1)\n        # Compute the cross entropy loss between the prediction and the target\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n                             reduction=\"none\")\n\n    # Place outputs on CPU for compatibility with other dataset columns   \n    return {\"loss\": loss.cpu().numpy(), \n            \"predicted_label\": pred_label.cpu().numpy()}\n\n# Convert our dataset back to PyTorch tensors\nemotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# Compute loss values\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)\n\n# Create a DataFrame with the texts, losses, and predicted/true labels\nemotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))\n\n# Sort the validation samples by the model loss\ndf_test.sort_values(\"loss\", ascending=False).head(10)\n\n\n\n\n\n\n\n\ntext\n\n\nlabel\n\n\npredicted_label\n\n\nloss\n\n\n\n\n\n\n318\n\n\ni felt ashamed of these feelings and was scared because i knew that something wrong with me and thought i might be gay\n\n\nfear\n\n\nsadness\n\n\n8.869118\n\n\n\n\n1509\n\n\ni guess this is a memoir so it feels like that should be fine too except i dont know something about such a deep amount of self absorption made me feel uncomfortable\n\n\njoy\n\n\nfear\n\n\n8.770837\n\n\n\n\n1950\n\n\ni as representative of everything thats wrong with corporate america and feel that sending him to washington is a ludicrous idea\n\n\nsurprise\n\n\nsadness\n\n\n8.217673\n\n\n\n\n882\n\n\ni feel badly about reneging on my commitment to bring donuts to the faithful at holy family catholic church in columbus ohio\n\n\nlove\n\n\nsadness\n\n\n8.134083\n\n\n\n\n1757\n\n\ni feel like there s a reason to buy another tom petty record\n\n\nanger\n\n\njoy\n\n\n7.790391\n\n\n\n\n1111\n\n\nim lazy my characters fall into categories of smug and or blas people and their foils people who feel inconvenienced by smug and or blas people\n\n\njoy\n\n\nfear\n\n\n7.778357\n\n\n\n\n1500\n\n\ni guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed\n\n\nanger\n\n\nsadness\n\n\n7.741042\n\n\n\n\n1919\n\n\ni should admit when consuming alcohol myself in small amounts i feel much less inhibited ideas come to me more easily and i can write with greater ease\n\n\nfear\n\n\nsadness\n\n\n7.342785\n\n\n\n\n415\n\n\nim kind of embarrassed about feeling that way though because my moms training was such a wonderfully defining part of my own life and i loved and still love\n\n\nlove\n\n\nsadness\n\n\n7.320217\n\n\n\n\n1801\n\n\ni feel that he was being overshadowed by the supporting characters\n\n\nlove\n\n\nsadness\n\n\n6.833299\n\n\n\n\n\n\nNote:\n\nThe model made some incorrect predictions.\nSome examples seem mislabeled or do not fit into one of the six emotion classes.\nJoy, in particular, seems to be mislabeled several times.\n\n\ndf_test.sort_values(\"loss\", ascending=True).head(10)\n\n\n\n\n\n\n\n\ntext\n\n\nlabel\n\n\npredicted_label\n\n\nloss\n\n\n\n\n\n\n702\n\n\ni only find out that they are looking and feeling complacent just before a match started and i have no other way to find out except through the assistant manager\n\n\njoy\n\n\njoy\n\n\n0.000212\n\n\n\n\n1205\n\n\ni log on feeling vaguely sociable and after a short amount of time im all socialised out\n\n\njoy\n\n\njoy\n\n\n0.000214\n\n\n\n\n1607\n\n\ni feel incredibly mellow and spacey\n\n\njoy\n\n\njoy\n\n\n0.000214\n\n\n\n\n452\n\n\ni manage to complete the lap not too far behind the front runners and am feeling pretty jubilant until i realise that this is just the warm up\n\n\njoy\n\n\njoy\n\n\n0.000215\n\n\n\n\n400\n\n\ni are just relaxing together and i feel ecstatic and blissfully happy because i know he loves me and i love him\n\n\njoy\n\n\njoy\n\n\n0.000217\n\n\n\n\n911\n\n\ni feel in love with a cute little maltese\n\n\njoy\n\n\njoy\n\n\n0.000218\n\n\n\n\n1567\n\n\ni feel wonderful shayla admitted\n\n\njoy\n\n\njoy\n\n\n0.000220\n\n\n\n\n1198\n\n\ni feel like i should also mention that there was some content that i wasnt thrilled with either\n\n\njoy\n\n\njoy\n\n\n0.000220\n\n\n\n\n1951\n\n\ni can finish even if i have to eat and feel satisfied bellmont cabinets before it leaves bellmont cabinets a wipe out on the spot it is not necessary to wipe out for when you o\n\n\njoy\n\n\njoy\n\n\n0.000221\n\n\n\n\n293\n\n\ni am sure she makes all waiting couples feel this way but we left feeling like she is pulling for us and she will be so thrilled when it all works out\n\n\njoy\n\n\njoy\n\n\n0.000222\n\n\n\n\n\n\n\n\nSaving and sharing the model\n\nEveryone can share and download pretrained and fine-tuned models via the Hugging Face Hub.\n\n\n\nTrainer.push_to_hub\n\nDocumentation\nUpload the trainer model and tokenizer to the Hugging Face Model Hub.\n\n\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n    'https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-emotion/commit/5ca5827ba0121e07c8056a8592398e73beca3f17'\n\n\nInference\n\nWe can now perform inference using the fine-tuned model from our Hub repository.\n\n\nfrom transformers import pipeline\n# Change `transformersbook` to your Hub username\nmodel_id = \"cj-mills/distilbert-base-uncased-finetuned-emotion\"\nclassifier = pipeline(\"text-classification\", model=model_id)\n\ncustom_tweet = \"I saw a movie today and it was really good.\"\npreds = classifier(custom_tweet, return_all_scores=True)\npreds_df = pd.DataFrame(preds[0])\nplt.bar(labels, 100 * preds_df[\"score\"], color='C0')\nplt.title(f'\"{custom_tweet}\"')\nplt.ylabel(\"Class probability (%)\")\nplt.show()\n\n\n\n\n\ncustom_tweet = \"I saw a movie today and it was garbage!.\"\npreds = classifier(custom_tweet, return_all_scores=True)\npreds_df = pd.DataFrame(preds[0])\nplt.bar(labels, 100 * preds_df[\"score\"], color='C0')\nplt.title(f'\"{custom_tweet}\"')\nplt.ylabel(\"Class probability (%)\")\nplt.show()\n\n\n\n\n\ncustom_tweet = \"I saw a movie today and it was really weird.\"\npreds = classifier(custom_tweet, return_all_scores=True)\npreds_df = pd.DataFrame(preds[0])\nplt.bar(labels, 100 * preds_df[\"score\"], color='C0')\nplt.title(f'\"{custom_tweet}\"')\nplt.ylabel(\"Class probability (%)\")\nplt.show()"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#conclusion",
    "href": "posts/transformers-book-notes/chapter-2/index.html#conclusion",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "Conclusion",
    "text": "Conclusion\n\nNLP Challenges\n\nMoving a model to production\n\nHugging Face creates an inference endpoint automatically when you push a model to the Hub.\nHugging Face Accelerated Inference API\n\n\n\nIncreasing Inference Speed\n\nThe process used to create the more efficient DistilBERT model is called knowledge distillation.\n\n\n\nApplying a Model to other tasks\n\nTransformers are exceedingly versatile.\n\n\n\nUsing Non-English Text\n\nMultilingual transformers are available.\n\n\n\nWorking with little labeled data\n\nFine-tuning might not be an option when little labeled training data is available."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-2/index.html#references",
    "href": "posts/transformers-book-notes/chapter-2/index.html#references",
    "title": "Notes on Transformers Book Ch. 2",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 1\nNext: Notes on Transformers Book Ch. 3"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html",
    "href": "posts/transformers-book-notes/chapter-3/index.html",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "",
    "text": "The Transformer Architecture\nThe Encoder\nThe Decoder\nMeet the Transformers\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html#the-transformer-architecture",
    "href": "posts/transformers-book-notes/chapter-3/index.html#the-transformer-architecture",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "The Transformer Architecture",
    "text": "The Transformer Architecture\n\nStandard Transformers use an encoder-decoder architecture.\n\nAn encoder converts an input sequence of tokens into embedding vectors called the hidden state or context.\nA decoder uses the encoder’s hidden state to iteratively generate an output of tokens, one token at a time.\n\nThe encoder-decoder architecture is well-suited to sequence-to-sequence tasks like machine translation.\n\n\nSteps:\n\nTokenize the input and convert it to token embeddings.\nCombine the token embeddings with positional embeddings, which contain the positional information for each token.\nFeed the updated embeddings to the encoder.\n\nThe encoder is composed of a sequence of embedding layers, similar to stacking convolutional layers in computer vision.\n\nFeed the encoder’s output to each decoder layer.\nThe decoder predicts the most probable next token in the sequence.\nThe decoder output serves as input for the decoder to generate the next token.\nThe process repeats until the decoder predicts the End- of-Sequence (EOS) token or we reach the maximum length.\n\n\n\nTypes of Transformer Models\n\nEncoder-Only\n\nConvert an input sequence into a rich numerical representation.\nThe numerical representation is dependent on both the contexts to the left and right of the token (i.e., bidirectional attention).\nThese models are well-suited for tasks like text classification or named entity recognition.\n\n\n\nDecoder-Only\n\nTake in a text prompt and autocomplete the sequence by iteratively predicting the most probable next word.\nThe representation for a given token depends only on the left context (i.e., causal or autoregressive attention).\n\n\n\nEncoder-Decoder\n\nModel complex mappings from one sequence to another.\nThese models are well-suited for machine translation or summarization tasks.\n\nNote: * The line between applications for decoder-only versus encoder-only architectures is blurry. * We can prime decoder-only models for tasks like machine translation. * We can apply encoder-only models to summarization tasks. * Text Summarization with Pretrained Encoders"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html#the-encoder",
    "href": "posts/transformers-book-notes/chapter-3/index.html#the-encoder",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "The Encoder",
    "text": "The Encoder\n\nThe encoder consists of many encoder layers stacked next to each other.\nThe encoder stack updates the input embeddings to produce representations that encode some contextual information in the sequence.\nEach encoder layer receives a sequence of embeddings and feeds them through a multi-head self-attention sublayer.\nThe output of the multi-head attention layer serves as input for a fully connected feed-forward sublayer.\nWe apply the fully connected feed-forward layer to each input embedding.\nEach sublayer uses skip connections and layer normalization.\nThe output embeddings of each encoder layer have the same size as the inputs.\n\n\nSelf-Attention\n\nAttention is a mechanism that allows neural networks to assign a different amount of weight to each element in a sequence.\nFor text sequences, the elements are token embeddings where each token maps to a vector of some fixed dimension.\n\nA BERT model represents each token as a 768-dimensional vector.\n\nThe attention mechanism associated with recurrent models involves computing the relevance of each hidden state of the encoder to the decoder’s hidden state at a given decoding timestep.\nSelf-attention involves computing weights for all hidden states in the same set.\n\nWe use the whole sequence to compute a weighted average of each embedding.\n\nGiven a sequence of token embeddings \\(x_{1},\\ldots,x_{n}\\), self-attention produces a sequence of new embeddings \\[x^{\\prime}_{1},\\ldots,x^{\\prime}_{n}\\] where each \\[x^{\\prime}_{i}\\] is a linear combination of all the \\(x_{j}\\): ### \\[x^{\\prime}_{i} = \\sum^{n}_{j=1}{w_{ij}x_{j}}\\]\nThe coefficients \\(w_{ij}\\) are called attention weights, and we normalize them to add up to one.\nAveraging the token embeddings allows us to incorporate context from the entire sequence.\n\n\nScaled dot-product attention\n\nScaled dot-product attention is the most common way to implement a self-attention layer.\n\n\nImplementation Steps\n\nProject each token into three vectors called “query,” “key,” and “value,” respectively.\n\nThe names query, key, and value originate from information retrieval systems.\n\nDetermine how much the query and key vectors relate to each other by computing the dot product of the two vectors.\n\nQueries and keys similar to each other will have a large dot product, while those that are not will have little to no overlap.\nThe dot products of the queries and keys are called the attention scores.\nFor a sequence with \\(n\\) input tokens, there is a corresponding \\(n \\times n\\) matrix of attention scores.\n\nMultiply the attention scores by a scaling factor to normalize their variance and then apply the softmax function to ensure all column values sum to one.\n\nThe resulting \\(n \\times n\\) matrix contains all the attention weights.\n\nMatrix multiply the attention weights by the value vector \\(v_{1},\\ldots,v_{n}\\) to obtain an updated representation for embedding \\[x^{\\prime}_{i} = \\sum{w_{ji}v_{j}}\\].\n\n\n\n\nBertViz\n\nGitHub Repository\nBertViz is an interactive tool to visualize attention in Transformer language models.\n\nNote: Need to add the D3.js and jQuery libraries to use bertviz in JupyterLab.\n\n%%javascript\nrequire.config({\n  paths: {\n      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n  }\n});\n\nfrom transformers import AutoTokenizer\nfrom bertviz.transformers_neuron_view import BertModel\nfrom bertviz.neuron_view import show\n\n\nneuron_view\n\nDocumentation\nTrace the computation of the weights to show how the query and key vectors combine to produce the final weight.\n\n\n# Select a BERT model checkpoint\nmodel_ckpt = \"bert-base-uncased\"\n# Instantiate a BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n# Instantiate a pretrained pytorch model from a pre-trained model configuration\nmodel = BertModel.from_pretrained(model_ckpt)\n\ntext = \"time flies like an arrow\"\nshow(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)\n\n\n\n\n\nNote: The query vector for “flies” has the most overlap with the key vector for “arrow.”\n\n\nneuron_view.show\n\nSource Code\n\n\nprint_source(show, False)\n    def show(model, model_type, tokenizer, sentence_a, sentence_b=None,\n        display_mode='dark', layer=None, head=None):\n        if sentence_b:\n            vis_html = \"\"\"\n            &lt;div id=\"bertviz\" style='padding:8px'&gt;\n              &lt;span style=\"user-select:none\"&gt;\n                &lt;span class=\"dropdown-label\"&gt;Layer: &lt;/span&gt;&lt;select id=\"layer\"&gt;&lt;/select&gt;\n                &lt;span class=\"dropdown-label\"&gt;Head: &lt;/span&gt;&lt;select id=\"att_head\"&gt;&lt;/select&gt;\n                &lt;span class=\"dropdown-label\"&gt;Attention: &lt;/span&gt;&lt;select id=\"filter\"&gt;\n                  &lt;option value=\"all\"&gt;All&lt;/option&gt;\n                  &lt;option value=\"aa\"&gt;Sentence A -&gt; Sentence A&lt;/option&gt;\n                  &lt;option value=\"ab\"&gt;Sentence A -&gt; Sentence B&lt;/option&gt;\n                  &lt;option value=\"ba\"&gt;Sentence B -&gt; Sentence A&lt;/option&gt;\n                  &lt;option value=\"bb\"&gt;Sentence B -&gt; Sentence B&lt;/option&gt;\n                &lt;/select&gt;\n              &lt;/span&gt;\n              &lt;div id='vis'&gt;&lt;/div&gt;\n            &lt;/div&gt;\n            \"\"\"\n        else:\n            vis_html = \"\"\"\n                &lt;div id=\"bertviz\" style='padding:8px'&gt;\n                  &lt;span style=\"user-select:none\"&gt;\n                    &lt;span class=\"dropdown-label\"&gt;Layer: &lt;/span&gt;&lt;select id=\"layer\"&gt;&lt;/select&gt;\n                    &lt;span class=\"dropdown-label\"&gt;Head: &lt;/span&gt; &lt;select id=\"att_head\"&gt;&lt;/select&gt;\n                  &lt;/span&gt;\n                  &lt;div id='vis'&gt;&lt;/div&gt;\n                &lt;/div&gt;\n             \"\"\"\n        display(HTML(\n            '&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"&gt;&lt;/script&gt;'\n            ))\n        display(HTML(vis_html))\n        __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.\n            dirname(__file__)))\n        attn_data = get_attention(model, model_type, tokenizer, sentence_a,\n            sentence_b, include_queries_and_keys=True)\n        if model_type == 'gpt2':\n            bidirectional = False\n        else:\n            bidirectional = True\n        params = {'attention': attn_data, 'default_filter': 'all',\n            'bidirectional': bidirectional, 'display_mode': display_mode,\n            'layer': layer, 'head': head}\n        vis_js = open(os.path.join(__location__, 'neuron_view.js')).read()\n        display(Javascript('window.bertviz_params = %s' % json.dumps(params)))\n        display(Javascript(vis_js))\n\n\nneuron_view.get_attention\n\nSource Code\nCompute representation of attention to pass to the d3 visualization\n\n\nfrom transformers import AutoTokenizer\nmodel_ckpt = \"bert-base-uncased\"\ntext = \"time flies like an arrow\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Get encoded inputs as PyTorch tensors and exclude special tokens\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\ninputs.input_ids\n    tensor([[ 2051, 10029,  2066,  2019,  8612]])\n\nfrom transformers import AutoConfig\nNote: * Every checkpoint in Hugging Face Transformers is assigned a configuration file, which specifies various hyperparameters. * The AutoConfig class also stores metadata such as label names.\n\n\nBertConfig\n\nDocumentation\nThis is the configuration class to store the configuration of a BERT model.\n\n\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig\n    BertConfig {\n      \"architectures\": [\n        \"BertForMaskedLM\"\n      ],\n      \"attention_probs_dropout_prob\": 0.1,\n      \"classifier_dropout\": null,\n      \"gradient_checkpointing\": false,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"layer_norm_eps\": 1e-12,\n      \"max_position_embeddings\": 512,\n      \"model_type\": \"bert\",\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"pad_token_id\": 0,\n      \"position_embedding_type\": \"absolute\",\n      \"transformers_version\": \"4.11.3\",\n      \"type_vocab_size\": 2,\n      \"use_cache\": true,\n      \"vocab_size\": 30522\n    }\n\n\nfrom torch import nn\n\n\nnn.Embedding\n\nDocumentation\nCreate a simple lookup table that stores embeddings of a fixed dictionary size.\n\n\n# Initialize dense embeddings where each entry contains a nonzero value\ntoken_emb = nn.Embedding(config.vocab_size, config.hidden_size)\ntoken_emb\n    Embedding(30522, 768)\n\nNote: * The embeddings at this point are independent of their context. * Words that are spelled the same but have different meanings have indistinguishable representations. * The subsequent attention layers provide the missing context.\n\n# Generate embeddings for the input sequence\ninputs_embeds = token_emb(inputs.input_ids)\ninputs_embeds.size()\n    torch.Size([1, 5, 768])\n\nimport torch\nfrom math import sqrt \n\n\ntorch.bmm\n\nDocumentation\nPerform a batched matrix multiplication.\n\n\n# Step 1: Initialize the query, key, and value vectors using the input embeddings\nquery = key = value = inputs_embeds\ndim_k = key.size(-1)\n# Step 2: Calculate the dot product of the query and key vectors\n# Step 3.1: and scale the result using the size of the embedding vectors\nscores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\nscores.size()\n    torch.Size([1, 5, 5])\nNote: * We now have a \\(5x5\\) matrix of attention scores per sample in the batch. * We ordinarily generate the query, key, and value vectors by applying independent weight matrices \\(W_{Q,K,V}\\) to the embeddings.\n\nimport torch.nn.functional as F\n\n\nfunctional.softmax\n\nDocumentation\nApply the softmax function to a tensor.\n\n\n# Step 3.2: Apply the softmax function to the scaled dot product\nweights = F.softmax(scores, dim=-1)\nweights.sum(dim=-1)\n    tensor([[1., 1., 1., 1., 1.]], grad_fn=&lt;SumBackward1&gt;)\n\nweights[0][0]\n    tensor([1.0000e+00, 5.1735e-12, 3.7513e-13, 2.1163e-12, 9.7180e-13],\n           grad_fn=&lt;SelectBackward0&gt;)\n\n# Step 4: Matrix multiply the attention weights by the value vector\nattn_outputs = torch.bmm(weights, value)\nattn_outputs.shape\n    torch.Size([1, 5, 768])\n\ndef scaled_dot_product_attention(query, key, value):\n    \"\"\"\n    Refactor all four steps to compute self-attention into a single function\n    \n    1. Compute the dot product of the query and key vectors.\n    2. Scale the dot product using the square root of the embedding size\n    3. Apply the softmax function.\n    4. Matrix multiply the attention weights by the value vector.\n    \"\"\"\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n    weights = F.softmax(scores, dim=-1)\n    return torch.bmm(weights, value)\nNote: * The attention mechanism will assign a high score to identical words when the “query” and “key” vectors are equal. * In practice, complementary words in the context better inform the meaning of a word than identical words. * The self-attention layer typically applies three independent linear transformations to each embedding to generate the query, key, and value vectors rather than using the same vector for each.\n\n\nMulti-headed attention\n\nThe softmax of one attention head tends to focus on one aspect of similarity.\nHaving several heads allows the model to focus on several aspects at once.\nThe model learns what aspects of similarity to focus on from the data, similar to the convolutional filters in computer vision models.\n\n\nclass AttentionHead(nn.Module):\n    \"\"\"\n    A single self-attention head that produces tensors of shape [batch_size, seq_len, head_dim]\n    \n    Args:\n        embed_dim: the number of embedding dimensions of the tokens\n        head_dim: the number of dimensions we are projecting into\n    \"\"\"\n    def __init__(self, embed_dim, head_dim):\n        super().__init__()\n        self.q = nn.Linear(embed_dim, head_dim)\n        self.k = nn.Linear(embed_dim, head_dim)\n        self.v = nn.Linear(embed_dim, head_dim)\n\n    def forward(self, hidden_state):\n        attn_outputs = scaled_dot_product_attention(\n            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n        return attn_outputs\nNote: It is common practice to use a multiple of embed_dim for head_dim so that the computation across each head is constant.\n\n\ntorch.nn.ModuleList\n\nDocumentation\nStore properly registered Modules in an indexable list.\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    A multi-head attention layer that concatenates the output of each attention head\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        num_heads = config.num_attention_heads\n        head_dim = embed_dim // num_heads\n        self.heads = nn.ModuleList(\n            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n        )\n        self.output_linear = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, hidden_state):\n        # Concatenate the output of each attention head\n        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n        # Pass the concatenated output through a linear layer to\n        # produce a tensor of shape [batch_size, seq_len, hidden_dim]\n        x = self.output_linear(x)\n        return x\n\n# Initialize a multi-head attention layer using the BertConfig\nmultihead_attn = MultiHeadAttention(config)\n# Pass the input embeddings through the attention layer\nattn_output = multihead_attn(inputs_embeds)    \nattn_output.size() \n    torch.Size([1, 5, 768])\n\nfrom bertviz import head_view\nfrom transformers import AutoModel\n\n\nhead_view\n\nDocumentation\n\n\n# Initialize a BERT model using the pretrained model checkpoint\nmodel = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n\nsentence_a = \"time flies like an arrow\"\nsentence_b = \"fruit flies like a banana\"\n# Tokenize the input sentences\nviz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n\nfor k,v in viz_inputs.items(): print(f\"{k}: {v}\")\n    input_ids: tensor([[  101,  2051, 10029,  2066,  2019,  8612,   102,  5909, 10029,  2066,\n              1037, 15212,   102]])\n    token_type_ids: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n    attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\nviz_inputs.input_ids.shape\n    torch.Size([1, 13])\n\nattention = model(**viz_inputs).attentions\nlen(attention), attention[0].shape\n    (12, torch.Size([1, 12, 13, 13]))\nNote: BERT has 12 attention heads.\nsentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\nsentence_b_start\n    tensor([7])\n\ntokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\ntokens\n    ['[CLS]',\n     'time',\n     'flies',\n     'like',\n     'an',\n     'arrow',\n     '[SEP]',\n     'fruit',\n     'flies',\n     'like',\n     'a',\n     'banana',\n     '[SEP]']\n\nhead_view(attention, tokens, sentence_b_start, heads=[8])\n\n\n\n\n\nNote:\n\nThe attention weights are highest between words in the same sentence.\nThe attention weights allow the model to distinguish the use of “flies” as a verb or a noun.\n\n\n\n\nThe Feed-Forward Layer\n\nThe feed-forward sublayer contains two linear layers and processes each embedding independently.\nA rule of thumb is to have the hidden size of the first layer be four times the size of the embeddings.\nThe feed-forward layer typically uses the Gaussian Error Linear Units (GELU) activation function.\n\nThe GELU function weights inputs by their value, whereas the ReLU function gates inputs by their sign.\n\nThe predominant theory is most of the capacity and memorization happens in the feed-forward layer.\n\nMost choose to scale the feed-forward layer when scaling a model.\n\n\n\nconfig.hidden_size, config.intermediate_size\n    (768, 3072)\n\nconfig.hidden_size*4\n    3072\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    A simple two-layer fully-connected network that processes each embedding independently\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        \n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.gelu(x)\n        x = self.linear_2(x)\n        x = self.dropout(x)\n        return x\n\nfeed_forward = FeedForward(config)\nff_outputs = feed_forward(attn_outputs)\nff_outputs.size()\n    torch.Size([1, 5, 768])\n\n\nAdding Layer Normalization\n\nLayer Normalization Paper\nLayer normalization normalizes each input in the batch to have zero mean and a variance of one.\nSkip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor.\nThe original Transformer paper uses post-layer normalization, which performs normalization between skip connections.\n\nPost-layer normalization typically requires gradually increasing the learning rate during training to prevent the gradients from diverging.\n\nThe most common approach is to use pre-layer normalization, which performs normalization within the span of skip connections.\n\nTraining is typically more stable with pre-layer normalization.\n\n\n\nnn.LayerNorm\n\nDocumentation\nApply Layer Normalization over a mini-batch of inputs\n\n\ninputs_embeds[0][0][0], inputs_embeds.shape\n    (tensor(0.0497, grad_fn=&lt;SelectBackward0&gt;), torch.Size([1, 5, 768]))\n\nlayer_norm = nn.LayerNorm(config.hidden_size)\n# Activate module\nlayer_norm_embeds = layer_norm(inputs_embeds)\nlayer_norm_embeds[0][0][0], layer_norm_embeds.shape\n    (tensor(0.0352, grad_fn=&lt;SelectBackward0&gt;), torch.Size([1, 5, 768]))\n\nclass TransformerEncoderLayer(nn.Module):\n    \"\"\"\n    A complete encoder layer that performs pre-layer normalization\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.attention = MultiHeadAttention(config)\n        self.feed_forward = FeedForward(config)\n\n    def forward(self, x):\n        # Apply layer normalization and then copy input into query, key, value\n        hidden_state = self.layer_norm_1(x)\n        # Apply attention with a skip connection\n        x = x + self.attention(hidden_state)\n        # Apply feed-forward layer with a skip connection\n        x = x + self.feed_forward(self.layer_norm_2(x))\n        return x\n\nencoder_layer = TransformerEncoderLayer(config)\ninputs_embeds.shape, encoder_layer(inputs_embeds).size()\n    (torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))\n\n\n\nPositional Embeddings\n\nAugment the token embeddings with a positional-dependent pattern of values arranged in a vector.\nThe attention heads and feed-forward layers in each stack learn to incorporate positional information when the pattern is characteristic for each position.\n\n\nLearnable Position Embeddings\n\nA popular approach involves learning a pattern during pretraining.\nThis approach works the same as the token embeddings but uses the position index as input.\nLearnable position embeddings work best when there is a large amount of pretraining data.\n\n\n\nAbsolute Position Representations\n\nUse static patterns consisting of modulated sine and cosine signals to encode the positions of tokens.\nThis approach works well when we don’t have access to a large dataset.\n\n\n\nRelative Positional Encoding\n\nEncode the relative positions between tokens.\nRelative Positional Encoding requires modifying the attention mechanism with additional terms that account for the relative position between tokens.\n\n\n\nRotary Position Embeddings\n\nCombine the idea of absolute and relative positional representations.\nRotary position embeddings achieve excellent results on many tasks.\n\n\nclass Embeddings(nn.Module):\n    \"\"\"\n    A custom embedding layer that combines a token embedding layer that projects the `input_ids` \n    to a dense hidden state with the positional embedding that does the same for `position_ids`\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(config.vocab_size, \n                                             config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n                                                config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout()\n\n    def forward(self, input_ids):\n        # Create position IDs for input sequence\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n        # Create token and position embeddings\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        # Combine token and position embeddings\n        embeddings = token_embeddings + position_embeddings\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\nNote: The embedding layer creates a single dense embedding for each token.\nconfig.max_position_embeddings\n    512\ninputs.input_ids.size()\n    torch.Size([1, 5])\ntorch.arange(5, dtype=torch.long).unsqueeze(0)\n    tensor([[0, 1, 2, 3, 4]])\n\nembedding_layer = Embeddings(config)\nembedding_layer(inputs.input_ids).size()\n    torch.Size([1, 5, 768])\n\nclass TransformerEncoder(nn.Module):\n    \"\"\"\n    A full encoder\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.layers = nn.ModuleList([TransformerEncoderLayer(config) \n                                     for _ in range(config.num_hidden_layers)])\n\n    def forward(self, x):\n        # Incorporate positional information\n        x = self.embeddings(x)\n        for layer in self.layers: x = layer(x)\n        return x\n\nencoder = TransformerEncoder(config)\nencoder(inputs.input_ids).size()\n    torch.Size([1, 5, 768])\n\n\n\nAdding a Classification Head\n\nTransformer models typically consist of a task-independent body and a task-dependent head.\nWe can attach a dropout and linear layer to the pretrained body to make a classification prediction.\n\n\nclass TransformerForSequenceClassification(nn.Module):\n    \"\"\"\n    A classification head that extends and existing encoder for sequence classification\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = TransformerEncoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        \n    def forward(self, x):\n        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\nconfig.num_labels = 3\nencoder_classifier = TransformerForSequenceClassification(config)\nencoder_classifier(inputs.input_ids).size()\n    torch.Size([1, 3])\n\nencoder_classifier(inputs.input_ids)\n    tensor([[ 0.8427, -0.7799, -0.4482]], grad_fn=&lt;AddmmBackward0&gt;)\nNote: For each example in the batch, we get the unnormalized logits for each class in the output."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html#the-decoder",
    "href": "posts/transformers-book-notes/chapter-3/index.html#the-decoder",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "The Decoder",
    "text": "The Decoder\n\nThe main difference between the encoder and decoder is that the decoder has two attention sublayers.\n\n\nminGPT\n\nGitHub Repository\nA minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training\n\n\n\nMasked Multi-Head Self-Attention Layer\n\nThe Masked Multi-Head Attention Layer ensures the tokens generated at each timestep depend only on the past outputs and the current token by masking the inputs.\nThe goal is to prevent the decoder from cheating during training by copying the target translation.\nUse a mask matrix with ones on the lower diagonal and zeros above.\n\n\n\nEncoder-Decoder Attention Layer\n\nThe Encoder-Decoder Attention Layer performs multi-head attention using the encoder stack’s output “key” and “value” vectors.\nThe intermediate representations from the decoder serve as the “query” vectors.\nThe encoder-decoder attention layer learns to relate tokens from two different sequences.\nThe decoder has access to the encoder keys and values in each block.\n\n\ntorch.tril\n\nDocumentation\nGet the lower triangular part of the matrix or batch of matrices\n\nseq_len = inputs.input_ids.size(-1)\nmask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\nmask[0]\n    tensor([[1., 0., 0., 0., 0.],\n            [1., 1., 0., 0., 0.],\n            [1., 1., 1., 0., 0.],\n            [1., 1., 1., 1., 0.],\n            [1., 1., 1., 1., 1.]])\n\n# Replace the values above the diagonal with negative infinity\nscores.masked_fill(mask == 0, -float(\"inf\"))\n    tensor([[[27.0041,    -inf,    -inf,    -inf,    -inf],\n             [ 1.0166, 25.6633,    -inf,    -inf,    -inf],\n             [-1.6074, -0.4446, 28.2530,    -inf,    -inf],\n             [ 0.1228, -1.1218, -1.4012, 26.3671,    -inf],\n             [-0.6555, -1.0425,  0.4620, -1.5529, 27.0796]]],\n           grad_fn=&lt;MaskedFillBackward0&gt;)\nNote: Setting the upper values to negative infinity guarantees the attention weights are all zero after applying softmax.\n\ndef scaled_dot_product_attention(query, key, value, mask=None):\n    \"\"\"\n    Compute self-attention and apply a mask to the atttention scores\n    \"\"\"\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n    weights = F.softmax(scores, dim=-1)\n    return weights.bmm(value)"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html#meet-the-transformers",
    "href": "posts/transformers-book-notes/chapter-3/index.html#meet-the-transformers",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "Meet the Transformers",
    "text": "Meet the Transformers\n\nThe Encoder Branch\n\nEncoder-only models still dominate research and industry on NLU tasks such as text classification, named entity recognition, and question answering.\n\n\nBERT\n\nBERT was the first encoder-only Transformer model and outperformed all state-of-the-art models on the GLUE benchmark.\n\nThe GLUE Benchmark measures natural language understanding (NLU) across several tasks of varying difficulty.\n\nBERT is pretrained to predict masked tokens in a piece of text (Masked Language Modeling) and determine if one text passage likely follows another (Next Sentence Prediction).\n\n\n\nDistilBERT\n\nDistilBERT uses knowledge distillation during pretraining to achieve 97% of BERT’s performance while using 40% less memory and is 60% faster.\n\n\n\nRoBERTa\n\nRoBERTa trains for longer on larger batches with more training data than BERT and drops the Next Sentence Prediction task.\nRoBERTa achieves significantly higher performance compared to BERT.\n\n\n\nXLM: Cross-lingual Language Model\n\nThe XLM paper explores the effectiveness of cross-lingual model pretraining.\nThe authors introduced Translation Language Modeling (TLM), which extends Masked Language Modeling to multiple languages.\nThe authors achieved state-of-the-art results on several multilingual NLU benchmarks and translation tasks.\n\n\n\nXLM-RoBERTa\n\nXLM-Roberta massively upscales the amount of training data used for multilingual pretraining.\nThe developers created a dataset with 2.5 terabytes of text and pretrained an encoder with Masked Language Modeling.\n\n\n\nALBERT\n\nAlberta decouples the token embedding and hidden dimensions to decrease parameter count.\nAll layers share the same parameters, decreasing the “effective” number of parameters even further.\nA Sentence Ordering Prediction (SOP) objective replaces the Next Sentence Prediction objective.\nWith Sentence Ordering Prediction, the model predicts whether two consecutive sentences are out of order.\nThese changes make it possible to train larger models with fewer parameters and achieve superior performance on NLU tasks.\n\n\n\nELECTRA\n\nStandard MLM pretraining only updates the representations of the masked tokens.\nELECTRA uses a two-model approach to address this limitation.\n\nThe first model predicts masked tokens.\nThe second model, called the discriminator, distinguishes between original tokens and predicted tokens in the first model’s output.\n\nThe discriminator makes a binary classification for every token, making training 30 times more efficient.\nWe fine-tune the discriminator for downstream tasks.\n\n\n\n\n\nDeBERTa\n\nDeBERTa represents each token using a vector for the content and a vector for relative position.\n\nDisentangling the tokens’ content from their relative positions helps the self-attention layers better model the dependency of nearby token pairs.\n\nDeBERTa adds an absolute position embedding just before the softmax layer of the token decoding head.\nDeBERTa was the first model (as an ensemble) to beat the human baseline on the SuperGLUE benchmark.\n\n\n\n\nThe Decoder Branch\n\nDecoder models are exceptionally good at predicting the next word in a sequence.\nMost of the progress for decoder models comes from training with larger datasets and scaling the language models to larger sizes.\n\n\nGPT: Generative Pretrained Transformer\n\nGPT combined the transformer decoder architecture and transfer learning.\nThe training process involves predicting the next word based on the previous ones.\nThe model also achieved impressive results on downstream tasks such as classification.\n\n\n\nGPT-2\n\nGPT-2 upscales the original GPT model and training set.\nThe model can produce longer sequences of coherent text.\n\n\n\nCTRL: Conditional Transformer Language\n\nCTRL adds “control tokens” at the beginning of a sequence to control the style of the generated text.\n\n\n\nGPT-3\n\nScaling Laws for Neural Language Models\n\nThere are simple power laws that govern the relationship between compute, dataset size, model size, and the performance of a language model.\n\nGPT-3 upscales GPT-2 to 175 billion parameters.\nThe model can generate impressively realistic text passages and exhibits few-shot learning capabilities.\n\n\n\nGPT-Neo/GPT-J-6B\n\nThese are GPT-like models trained by EleutherAI, a collective of researchers who aim to recreate and release GPT-3 scale models.\nThe current models come in 1.3, 2.7, and 6 billion variants and are competitive with the smaller GPT-3 models offered by OpenAI.\n\n\n\n\nThe Encoder-Decoder Branch\n\nT5\n\nThe T5 model frames all Natural Language Understanding and Generation tasks as sequence-to-sequence tasks.\nThe model handles text classification problems by feeding the text to the encoder and generating a label as plain text instead of a class id.\nThe T5 architecture uses the original Transformer architecture.\nThe model trains on the C4 dataset using masked language modeling and the SuperGLUE tasks.\n\n\n\nBART\n\nBART combines the pretraining procedures of BERT and GPT within the encoder-decoder architecture.\nThe input sequences undergo one of several possible transformations, including simple masking, sentence permutation, token deletion, and document rotation.\nThe modified inputs pass through the encoder, and the decoder reconstructs the original texts.\n\n\n\nM2M-100\n\nM2M-100 is the first model able to translate between 100 languages.\n\nThis capability enables high-quality translation between rare and underrepresented languages.\n\nThe model uses prefix tokens to indicate the source and target language.\n\n\n\nBigBird\n\nBigBird uses a sparse form of attention that scales linearly to avoid the quadratic memory requirements of standard attention mechanisms.\nBigBird has a maximum context size of 4092 compared to the context size of 512 in most BERT models."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-3/index.html#references",
    "href": "posts/transformers-book-notes/chapter-3/index.html#references",
    "title": "Notes on Transformers Book Ch. 3",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 2\nNext: Notes on Transformers Book Ch. 4"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html",
    "href": "posts/transformers-book-notes/chapter-4/index.html",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "",
    "text": "Introduction\nProject: Multilingual Named Entity Recognition\nThe Dataset\nMultilingual Transformers\nA Closer Look at Tokenization\nTransformers for Named Entity Recognition\nThe Anatomy of the Transformers Model Class\nTokenizing Texts for NER\nPerformance Measures\nFine-Tuning XLM-RoBERTa\nError Analysis\nCross-Lingual Transfer\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#introduction",
    "href": "posts/transformers-book-notes/chapter-4/index.html#introduction",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Introduction",
    "text": "Introduction\n\nNon-English pretrained models typically exist only for languages like German, Russian, or Mandarin, where plenty of web text is available for pretraining.\nAvoid maintaining multiple monolingual models in production when possible.\nTransformer models pretrained on large corpora across many languages can perform zero-shot cross-lingual transfer.\n\nWe can fine-tune a model using one language and apply it to others without further training.\n\nMultilingual transformers are well-suited for situations where a speaker alternates between two or more languages in the context of a single conversation."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#project-multilingual-named-entity-recognition",
    "href": "posts/transformers-book-notes/chapter-4/index.html#project-multilingual-named-entity-recognition",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Project: Multilingual Named Entity Recognition",
    "text": "Project: Multilingual Named Entity Recognition\n\nThe goal is to fine-tine the transformer model XLM-RoBERTa to perform named entity recognition for a customer in Switzerland, where there are four national languages.\n\nWe will use German, French, Italian, and English as the four languages.\n\nNamed entity recognition involves extracting real-world objects like products, places, and people from a piece of text.\n\nSome potential NER applications include gaining insights from company documents, augmenting the quality of search engines, and building a structured database from a corpus."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#the-dataset",
    "href": "posts/transformers-book-notes/chapter-4/index.html#the-dataset",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "The Dataset",
    "text": "The Dataset\n\nWikiAnn (a.k.a PAN-X)\n\nWikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages.\nEach article has annotations for location, person, and organization tags in the IOB2 format.\n\nThe IOB2 format indicates the beginning of an entity with a B- prefix, consecutive tags belonging to the same entity with an I- prefix, and tokens that do not belong to any entity with an O tag.\n\nWikiANN is a subset of the XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization benchmark.\nCross-lingual Name Tagging and Linking for 282 Languages\nHugging Face Dataset Card\n\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# An example of a sequence annotated with named entities in IOB2 format\ntoks = \"Jeff Dean is a computer scientist at Google in California\".split()\nlbls = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\npd.DataFrame(data=[toks, lbls], index=['Tokens', 'Tags'])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nTokens\n\n\nJeff\n\n\nDean\n\n\nis\n\n\na\n\n\ncomputer\n\n\nscientist\n\n\nat\n\n\nGoogle\n\n\nin\n\n\nCalifornia\n\n\n\n\nTags\n\n\nB-PER\n\n\nI-PER\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nB-ORG\n\n\nO\n\n\nB-LOC\n\n\n\n\n\n\n\nfrom datasets import get_dataset_config_names\n\nget_dataset_config_names\n\nDocumentation\nGet the list of available configuration names for a particular dataset.\n\n\nprint_source(get_dataset_config_names)\n    def get_dataset_config_names(path: str, revision: Optional[Union[str,\n        Version]]=None, download_config: Optional[DownloadConfig]=None,\n        download_mode: Optional[GenerateMode]=None, force_local_path: Optional[\n        str]=None, dynamic_modules_path: Optional[str]=None, data_files:\n        Optional[Union[Dict, List, str]]=None, **download_kwargs):\n        dataset_module = dataset_module_factory(path, revision=revision,\n            download_config=download_config, download_mode=download_mode,\n            force_local_path=force_local_path, dynamic_modules_path=\n            dynamic_modules_path, data_files=data_files, **download_kwargs)\n        builder_cls = import_main_class(dataset_module.module_path)\n        return list(builder_cls.builder_configs.keys()) or [dataset_module.\n            builder_kwargs.get('name', 'default')]\n\n\n\nxtreme Hugging Face Dataset Card\n# Get the names of the subsets for the XTREME dataset\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")\n    XTREME has 183 configurations\npd.DataFrame(xtreme_subsets).T.style.hide(axis='columns').hide(axis='index')\n\n\n\n\n\n\n\n\nXNLI\n\n\ntydiqa\n\n\nSQuAD\n\n\nPAN-X.af\n\n\nPAN-X.ar\n\n\nPAN-X.bg\n\n\nPAN-X.bn\n\n\nPAN-X.de\n\n\nPAN-X.el\n\n\nPAN-X.en\n\n\nPAN-X.es\n\n\nPAN-X.et\n\n\nPAN-X.eu\n\n\nPAN-X.fa\n\n\nPAN-X.fi\n\n\nPAN-X.fr\n\n\nPAN-X.he\n\n\nPAN-X.hi\n\n\nPAN-X.hu\n\n\nPAN-X.id\n\n\nPAN-X.it\n\n\nPAN-X.ja\n\n\nPAN-X.jv\n\n\nPAN-X.ka\n\n\nPAN-X.kk\n\n\nPAN-X.ko\n\n\nPAN-X.ml\n\n\nPAN-X.mr\n\n\nPAN-X.ms\n\n\nPAN-X.my\n\n\nPAN-X.nl\n\n\nPAN-X.pt\n\n\nPAN-X.ru\n\n\nPAN-X.sw\n\n\nPAN-X.ta\n\n\nPAN-X.te\n\n\nPAN-X.th\n\n\nPAN-X.tl\n\n\nPAN-X.tr\n\n\nPAN-X.ur\n\n\nPAN-X.vi\n\n\nPAN-X.yo\n\n\nPAN-X.zh\n\n\nMLQA.ar.ar\n\n\nMLQA.ar.de\n\n\nMLQA.ar.vi\n\n\nMLQA.ar.zh\n\n\nMLQA.ar.en\n\n\nMLQA.ar.es\n\n\nMLQA.ar.hi\n\n\nMLQA.de.ar\n\n\nMLQA.de.de\n\n\nMLQA.de.vi\n\n\nMLQA.de.zh\n\n\nMLQA.de.en\n\n\nMLQA.de.es\n\n\nMLQA.de.hi\n\n\nMLQA.vi.ar\n\n\nMLQA.vi.de\n\n\nMLQA.vi.vi\n\n\nMLQA.vi.zh\n\n\nMLQA.vi.en\n\n\nMLQA.vi.es\n\n\nMLQA.vi.hi\n\n\nMLQA.zh.ar\n\n\nMLQA.zh.de\n\n\nMLQA.zh.vi\n\n\nMLQA.zh.zh\n\n\nMLQA.zh.en\n\n\nMLQA.zh.es\n\n\nMLQA.zh.hi\n\n\nMLQA.en.ar\n\n\nMLQA.en.de\n\n\nMLQA.en.vi\n\n\nMLQA.en.zh\n\n\nMLQA.en.en\n\n\nMLQA.en.es\n\n\nMLQA.en.hi\n\n\nMLQA.es.ar\n\n\nMLQA.es.de\n\n\nMLQA.es.vi\n\n\nMLQA.es.zh\n\n\nMLQA.es.en\n\n\nMLQA.es.es\n\n\nMLQA.es.hi\n\n\nMLQA.hi.ar\n\n\nMLQA.hi.de\n\n\nMLQA.hi.vi\n\n\nMLQA.hi.zh\n\n\nMLQA.hi.en\n\n\nMLQA.hi.es\n\n\nMLQA.hi.hi\n\n\nXQuAD.ar\n\n\nXQuAD.de\n\n\nXQuAD.vi\n\n\nXQuAD.zh\n\n\nXQuAD.en\n\n\nXQuAD.es\n\n\nXQuAD.hi\n\n\nXQuAD.el\n\n\nXQuAD.ru\n\n\nXQuAD.th\n\n\nXQuAD.tr\n\n\nbucc18.de\n\n\nbucc18.fr\n\n\nbucc18.zh\n\n\nbucc18.ru\n\n\nPAWS-X.de\n\n\nPAWS-X.en\n\n\nPAWS-X.es\n\n\nPAWS-X.fr\n\n\nPAWS-X.ja\n\n\nPAWS-X.ko\n\n\nPAWS-X.zh\n\n\ntatoeba.afr\n\n\ntatoeba.ara\n\n\ntatoeba.ben\n\n\ntatoeba.bul\n\n\ntatoeba.deu\n\n\ntatoeba.cmn\n\n\ntatoeba.ell\n\n\ntatoeba.est\n\n\ntatoeba.eus\n\n\ntatoeba.fin\n\n\ntatoeba.fra\n\n\ntatoeba.heb\n\n\ntatoeba.hin\n\n\ntatoeba.hun\n\n\ntatoeba.ind\n\n\ntatoeba.ita\n\n\ntatoeba.jav\n\n\ntatoeba.jpn\n\n\ntatoeba.kat\n\n\ntatoeba.kaz\n\n\ntatoeba.kor\n\n\ntatoeba.mal\n\n\ntatoeba.mar\n\n\ntatoeba.nld\n\n\ntatoeba.pes\n\n\ntatoeba.por\n\n\ntatoeba.rus\n\n\ntatoeba.spa\n\n\ntatoeba.swh\n\n\ntatoeba.tam\n\n\ntatoeba.tel\n\n\ntatoeba.tgl\n\n\ntatoeba.tha\n\n\ntatoeba.tur\n\n\ntatoeba.urd\n\n\ntatoeba.vie\n\n\nudpos.Afrikaans\n\n\nudpos.Arabic\n\n\nudpos.Basque\n\n\nudpos.Bulgarian\n\n\nudpos.Dutch\n\n\nudpos.English\n\n\nudpos.Estonian\n\n\nudpos.Finnish\n\n\nudpos.French\n\n\nudpos.German\n\n\nudpos.Greek\n\n\nudpos.Hebrew\n\n\nudpos.Hindi\n\n\nudpos.Hungarian\n\n\nudpos.Indonesian\n\n\nudpos.Italian\n\n\nudpos.Japanese\n\n\nudpos.Kazakh\n\n\nudpos.Korean\n\n\nudpos.Chinese\n\n\nudpos.Marathi\n\n\nudpos.Persian\n\n\nudpos.Portuguese\n\n\nudpos.Russian\n\n\nudpos.Spanish\n\n\nudpos.Tagalog\n\n\nudpos.Tamil\n\n\nudpos.Telugu\n\n\nudpos.Thai\n\n\nudpos.Turkish\n\n\nudpos.Urdu\n\n\nudpos.Vietnamese\n\n\nudpos.Yoruba\n\n\n\n\n\n\nNote: We are only interested in the PAN-X subsets for this project.\n\n# Look for configuration names containing 'PAN'\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\nlen(panx_subsets), panx_subsets[:3]\n    (40, ['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg'])\nNote: * There are 40 PAN-X subsets. * Each subset has a two-letter suffix indicating the ISO 639-1 language code. * German (de) * French (fr) * Italian (it) * English (en)\n\nfrom datasets import load_dataset\nfrom collections import defaultdict\nfrom datasets import DatasetDict\n# Specify the desired language codes\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\n# Specify the percentage each language should contribute to the total dataset\nfracs = [0.629, 0.229, 0.084, 0.059]\nNote: * These percentages represent the spoken proportions for each language in Switzerland. * This language imbalance simulates the common situation where acquiring labeled examples in a minority language is cost-prohibitive.\n\n\n\nDataset.shuffle\n\nDocumentation\nCreate a new dataset with shuffled rows.\n\n\n\nDataset.select\n\nDocumentation\nCreate a new dataset with rows selected following the list/array of indices.\n\n\n\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # Shuffle and downsample each split according to spoken proportion\n    for split in ds:\n        panx_ch[lang][split] = (\n            ds[split]\n            # Shuffle the dataset split rows\n            .shuffle(seed=0)\n            # Select subset of dataset split\n            .select(range(int(frac * ds[split].num_rows))))\n\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])\n\n\n\n\n\n\n\n\nde\n\n\nfr\n\n\nit\n\n\nen\n\n\n\n\n\n\nNumber of training examples\n\n\n12580\n\n\n4580\n\n\n1680\n\n\n1180\n\n\n\n\n\n\n\ntrain_size = sum([panx_ch[lang]['train'].num_rows for lang in langs])\ntrain_size\n    20020\n\npd.DataFrame(\n    {lang: [panx_ch[lang][\"train\"].num_rows, \n            f'{panx_ch[lang][\"train\"].num_rows/train_size*100:.2f}%'] for lang in langs\n    }, index=[\"Number of training examples\", \"Proportion of Dataset\"])\n\n\n\n\n\n\n\n\nde\n\n\nfr\n\n\nit\n\n\nen\n\n\n\n\n\n\nNumber of training examples\n\n\n12580\n\n\n4580\n\n\n1680\n\n\n1180\n\n\n\n\nProportion of Dataset\n\n\n62.84%\n\n\n22.88%\n\n\n8.39%\n\n\n5.89%\n\n\n\n\n\n\n\nfor lang in langs: print(panx_ch[lang][\"train\"])\n    Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 12580\n    })\n    Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 4580\n    })\n    Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 1680\n    })\n    Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 1180\n    })\n\nelement = panx_ch[\"de\"][\"train\"][0]\npd.DataFrame(element).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n\n\n\n\ntokens\n\n\n2.000\n\n\nEinwohnern\n\n\nan\n\n\nder\n\n\nDanziger\n\n\nBucht\n\n\nin\n\n\nder\n\n\npolnischen\n\n\nWoiwodschaft\n\n\nPommern\n\n\n.\n\n\n\n\nner_tags\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n5\n\n\n6\n\n\n0\n\n\n0\n\n\n5\n\n\n5\n\n\n6\n\n\n0\n\n\n\n\nlangs\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\n\n\n\n\nNote:\n\nThe German text translates to “2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania.”\n\nGdansk Bay is a bay in the Baltic Sea.\nThe word “voivodeship” corresponds to a state in Poland.\n\nThe ner_tags column corresponds to the mapping of each entity to a class ID.\nThe Dataset object has a “features” attribute that specifies the underlying data types associated with each column.\n\n\ntags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\ntags\n    ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None)\n\ntags.names\n    ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n\nMap the class IDs to the corresponding tag names\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)\npd.DataFrame(panx_de[\"train\"][0]).reindex(columns=[\"tokens\", \"ner_tags_str\",\"ner_tags\",\"langs\"]).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n\n\n\n\ntokens\n\n\n2.000\n\n\nEinwohnern\n\n\nan\n\n\nder\n\n\nDanziger\n\n\nBucht\n\n\nin\n\n\nder\n\n\npolnischen\n\n\nWoiwodschaft\n\n\nPommern\n\n\n.\n\n\n\n\nner_tags_str\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nB-LOC\n\n\nI-LOC\n\n\nO\n\n\nO\n\n\nB-LOC\n\n\nB-LOC\n\n\nI-LOC\n\n\nO\n\n\n\n\nner_tags\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n5\n\n\n6\n\n\n0\n\n\n0\n\n\n5\n\n\n5\n\n\n6\n\n\n0\n\n\n\n\nlangs\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\nde\n\n\n\n\n\n\n\nfrom collections import Counter\nCalculate the frequencies of each entity across each split\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\npd.DataFrame.from_dict(split2freqs, orient=\"index\")\n\n\n\n\n\n\n\n\nORG\n\n\nLOC\n\n\nPER\n\n\n\n\n\n\nvalidation\n\n\n2683\n\n\n3172\n\n\n2893\n\n\n\n\ntest\n\n\n2573\n\n\n3180\n\n\n3071\n\n\n\n\ntrain\n\n\n5366\n\n\n6186\n\n\n5810\n\n\n\n\n\n\nNote: The distributions of the entity frequencies are roughly the same for each split."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#multilingual-transformers",
    "href": "posts/transformers-book-notes/chapter-4/index.html#multilingual-transformers",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Multilingual Transformers",
    "text": "Multilingual Transformers\n\nMultilingual transformers use a corpus consisting of documents in many languages for pretraining.\n\nThe models do not receive any explicit information to differentiate among languages.\n\nThe resulting linguistic representations generalize well across languages for many downstream tasks.\nMany use the CoNLL-2002 and CoNLL-2003 datasets as benchmarks to measure the progress of cross-lingual transfer for named entity recognition for English, Dutch, Spanish, and German.\n\n\nEvaluation Methods\n\nen: Fine-tune using the English training data and then evaluate the model on each language’s test set.\neach: Fine-tune and evaluate using monolingual test data to measure per-language performance.\nall: Fine-tune using all the training data to evaluate the model on each language’s test set."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#a-closer-look-at-tokenization",
    "href": "posts/transformers-book-notes/chapter-4/index.html#a-closer-look-at-tokenization",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "A Closer Look at Tokenization",
    "text": "A Closer Look at Tokenization\n\nXLM-RoBERTa uses the SentencePiece subword tokenizer instead of the WordPiece tokenizer.\n\n\nfrom transformers import AutoTokenizer\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\nCompare the WordPiece and SentencePiece tokenizers\ntext = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()\npd.DataFrame([bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nBERT\n\n\n[CLS]\n\n\nJack\n\n\nSpa\n\n\n##rrow\n\n\nloves\n\n\nNew\n\n\nYork\n\n\n!\n\n\n[SEP]\n\n\nNone\n\n\n\n\nXLM-R\n\n\n&lt;s&gt;\n\n\n_Jack\n\n\n_Spar\n\n\nrow\n\n\n_love\n\n\ns\n\n\n_New\n\n\n_York\n\n\n!\n\n\n&lt;/s&gt;\n\n\n\n\n\n\nNote: SentencePiece uses &lt;s&gt; and &lt;/s&gt; to indicate the start and end sequences.\n\n\nThe Tokenizer Pipeline\n\n1. Normalization\n\nThe normalization step includes the operations to clean up raw text, such as stripping whitespace and removing accented characters.\nUnicode normalization schemes replace the various ways to write the same character with standard forms.\n\nUnicode normalization is particularly effective when working with multilingual corpora.\n\nLowercasing can help reduce the vocabulary size when the model only accepts and uses lowercase characters.\n\n\n\n2. Pretokenization\n\nThe pre-tokenization step splits a text into smaller objects, and the final tokens will be subunits of these objects.\nSome languages might require language-specific pre-tokenization methods.\n\n\n\n3. Tokenizer Model\n\nThe tokenizer model analyzes the training corpus to find the most commonly occurring groups of characters, which become the vocab.\n\n\n\n4. Postprocessing\n\nThe postprocessing step applies some additional transformations, such as adding special characters to the start or end of an input sequence.\n\n\n\n\nThe SentencePiece Tokenizer\n\nThe SentencePiece tokenizer builds on the Unigram subword segmentation algorithm and encodes each input text as a sequence of Unicode characters.\nSentencePiece supports the byte-pair-encoding (BPE) algorithm and the unigram language model.\nSentencePiece replaces whitespace with the Unicode symbol U+2581 for _.\n\n\n\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")\n    '&lt;s&gt; Jack Sparrow loves New York!&lt;/s&gt;'"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#transformers-for-named-entity-recognition",
    "href": "posts/transformers-book-notes/chapter-4/index.html#transformers-for-named-entity-recognition",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Transformers for Named Entity Recognition",
    "text": "Transformers for Named Entity Recognition\n\nFor text classification, BERT uses the [CLS] token to represent an entire sequence of text.\nFor named entity recognition, BERT feeds the representation of each input token through the same fully connected layer to output the entity of each one.\n\nWe can assign the entity label to the first subword of a word and ignore the rest."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#the-anatomy-of-the-transformers-model-class",
    "href": "posts/transformers-book-notes/chapter-4/index.html#the-anatomy-of-the-transformers-model-class",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "The Anatomy of the Transformers Model Class",
    "text": "The Anatomy of the Transformers Model Class\n\nThe Hugging Face Transformers library has dedicated classes for each architecture and task.\nWe can extend existing models for specific use cases with little overhead.\n\n\nBodies and Heads\n\nHugging Face Transformers splits model architectures into a body and head\nThe body is task-agnostic, and the model head is unique to a specific downstream task.\n\n\n\nCreating a Custom Model for Token Classification\n\nimport torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\n\nTokenClassifierOutput\n\nDocumentation\nA base class for outputs of token classification models.\n\nprint_source(TokenClassifierOutput)\n    @dataclass\n    class TokenClassifierOutput(ModelOutput):\n        loss: Optional[torch.FloatTensor] = None\n        logits: torch.FloatTensor = None\n        hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n        attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n\n\nModelOutput\n\nDocumentation\nThe base class for all model outputs.\n\n\n\nRobertaModel\n\nDocumentation\nA bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n\n\n\nRobertaPreTrainedModel\n\nSource Code\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\n\nXLMRobertaConfig()\n    XLMRobertaConfig {\n      \"attention_probs_dropout_prob\": 0.1,\n      \"bos_token_id\": 0,\n      \"classifier_dropout\": null,\n      \"eos_token_id\": 2,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"layer_norm_eps\": 1e-12,\n      \"max_position_embeddings\": 512,\n      \"model_type\": \"xlm-roberta\",\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"pad_token_id\": 1,\n      \"position_embedding_type\": \"absolute\",\n      \"transformers_version\": \"4.11.3\",\n      \"type_vocab_size\": 2,\n      \"use_cache\": true,\n      \"vocab_size\": 30522\n    }\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    # Use the standard XLM-RoBERTa settings.\n    config_class = XLMRobertaConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body\n        # Set add_pooling_layer to False to get all hidden states in the output\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                labels=None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits, \n                                     hidden_states=outputs.hidden_states, \n                                     attentions=outputs.attentions)\n\n\n\n\nLoading a Custom Model\n\nWe need to provide the tags for labeling each entity and mappings to convert between tags and IDs\n\nDefine the mappings to convert between tags and index IDs\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\nindex2tag, tag2index\n    ({0: 'O',\n      1: 'B-PER',\n      2: 'I-PER',\n      3: 'B-ORG',\n      4: 'I-ORG',\n      5: 'B-LOC',\n      6: 'I-LOC'},\n     {'O': 0,\n      'B-PER': 1,\n      'I-PER': 2,\n      'B-ORG': 3,\n      'I-ORG': 4,\n      'B-LOC': 5,\n      'I-LOC': 6})\n\nfrom transformers import AutoConfig\nOverride the default parameters stored in XLMRobertaConfig\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n                                         num_labels=tags.num_classes,\n                                         id2label=index2tag, label2id=tag2index)\nxlmr_config\n    XLMRobertaConfig {\n      \"architectures\": [\n        \"XLMRobertaForMaskedLM\"\n      ],\n      \"attention_probs_dropout_prob\": 0.1,\n      \"bos_token_id\": 0,\n      \"classifier_dropout\": null,\n      \"eos_token_id\": 2,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"id2label\": {\n        \"0\": \"O\",\n        \"1\": \"B-PER\",\n        \"2\": \"I-PER\",\n        \"3\": \"B-ORG\",\n        \"4\": \"I-ORG\",\n        \"5\": \"B-LOC\",\n        \"6\": \"I-LOC\"\n      },\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"label2id\": {\n        \"B-LOC\": 5,\n        \"B-ORG\": 3,\n        \"B-PER\": 1,\n        \"I-LOC\": 6,\n        \"I-ORG\": 4,\n        \"I-PER\": 2,\n        \"O\": 0\n      },\n      \"layer_norm_eps\": 1e-05,\n      \"max_position_embeddings\": 514,\n      \"model_type\": \"xlm-roberta\",\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"output_past\": true,\n      \"pad_token_id\": 1,\n      \"position_embedding_type\": \"absolute\",\n      \"transformers_version\": \"4.11.3\",\n      \"type_vocab_size\": 1,\n      \"use_cache\": true,\n      \"vocab_size\": 250002\n    }\n\nimport torch\nLoad a pretrained XLM-RoBERTa model with the custom classification head and configuration parameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))\n\nEncode some sample text\ntext\n    'Jack Sparrow loves New York!'\n\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_Jack\n\n\n_Spar\n\n\nrow\n\n\n_love\n\n\ns\n\n\n_New\n\n\n_York\n\n\n!\n\n\n&lt;/s&gt;\n\n\n\n\nInput IDs\n\n\n0\n\n\n21763\n\n\n37456\n\n\n15555\n\n\n5161\n\n\n7\n\n\n2356\n\n\n5753\n\n\n38\n\n\n2\n\n\n\n\n\n\n\nTest model predictions with the untrained classifier\noutputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")\n    Number of tokens in sequence: 10\n    Shape of outputs: torch.Size([1, 10, 7])\nNote: The logits have the shape [batch_size, num_tokens, num_tags].\n\npreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_Jack\n\n\n_Spar\n\n\nrow\n\n\n_love\n\n\ns\n\n\n_New\n\n\n_York\n\n\n!\n\n\n&lt;/s&gt;\n\n\n\n\nTags\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\n\n\n\n\nNote: The output is useless as the weights are still randomly initialized.\n\nWrap the prediction steps in a helper function\ndef tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(input_ids)[0]\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#tokenizing-texts-for-ner",
    "href": "posts/transformers-book-notes/chapter-4/index.html#tokenizing-texts-for-ner",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Tokenizing Texts for NER",
    "text": "Tokenizing Texts for NER\nCollect the words and tags as ordinary lists\nde_example = panx_de['train'][0]\nwords, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\npd.DataFrame([words,labels], index=[\"words\", \"labels\"])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n\n\n\n\nwords\n\n\n2.000\n\n\nEinwohnern\n\n\nan\n\n\nder\n\n\nDanziger\n\n\nBucht\n\n\nin\n\n\nder\n\n\npolnischen\n\n\nWoiwodschaft\n\n\nPommern\n\n\n.\n\n\n\n\nlabels\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n5\n\n\n6\n\n\n0\n\n\n0\n\n\n5\n\n\n5\n\n\n6\n\n\n0\n\n\n\n\n\n\n\nTokenize each word\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\npd.DataFrame(tokenized_input.values(), index=tokenized_input.keys())\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n\n\n\n\ninput_ids\n\n\n0\n\n\n70101\n\n\n176581\n\n\n19\n\n\n142\n\n\n122\n\n\n2290\n\n\n708\n\n\n1505\n\n\n18363\n\n\n18\n\n\n23\n\n\n122\n\n\n127474\n\n\n15439\n\n\n13787\n\n\n14\n\n\n15263\n\n\n18917\n\n\n663\n\n\n6947\n\n\n19\n\n\n6\n\n\n5\n\n\n2\n\n\n\n\nattention_mask\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\nNote: The is_split_into_words argument tells the tokenizer the input sequence is a list of separated words.\n\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\npd.DataFrame(tokens, columns=[\"tokens\"]).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n\n\n\n\ntokens\n\n\n&lt;s&gt;\n\n\n_2.000\n\n\n_Einwohner\n\n\nn\n\n\n_an\n\n\n_der\n\n\n_Dan\n\n\nzi\n\n\nger\n\n\n_Buch\n\n\nt\n\n\n_in\n\n\n_der\n\n\n_polni\n\n\nschen\n\n\n_Wo\n\n\ni\n\n\nwod\n\n\nschaft\n\n\n_Po\n\n\nmmer\n\n\nn\n\n\n_\n\n\n.\n\n\n&lt;/s&gt;\n\n\n\n\n\n\nNote: We can use the word_ids() function to mask the subword representations after the first subword.\n\n\nBatchEncoding.word_ids.word_ids\n\nDocumentation\nGet a list indicating the word corresponding to each token.\n\nword_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_2.000\n\n\n_Einwohner\n\n\nn\n\n\n_an\n\n\n_der\n\n\n_Dan\n\n\nzi\n\n\nger\n\n\n_Buch\n\n\nt\n\n\n_in\n\n\n_der\n\n\n_polni\n\n\nschen\n\n\n_Wo\n\n\ni\n\n\nwod\n\n\nschaft\n\n\n_Po\n\n\nmmer\n\n\nn\n\n\n_\n\n\n.\n\n\n&lt;/s&gt;\n\n\n\n\nWord IDs\n\n\nNone\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n8\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n10\n\n\n10\n\n\n10\n\n\n11\n\n\n11\n\n\nNone\n\n\n\n\n\n\nNote: The &lt;s&gt; and &lt;/s&gt; tokens map to None as they are not words from the original text.\n\nSet -100 as the label for the start and end tokens and masked subwords\n\nThe PyTorch cross-entropy loss class has an attribute called ignore_index whose value is -100.\n\nprevious_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n    \nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_2.000\n\n\n_Einwohner\n\n\nn\n\n\n_an\n\n\n_der\n\n\n_Dan\n\n\nzi\n\n\nger\n\n\n_Buch\n\n\nt\n\n\n_in\n\n\n_der\n\n\n_polni\n\n\nschen\n\n\n_Wo\n\n\ni\n\n\nwod\n\n\nschaft\n\n\n_Po\n\n\nmmer\n\n\nn\n\n\n_\n\n\n.\n\n\n&lt;/s&gt;\n\n\n\n\nWord IDs\n\n\nNone\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n8\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n10\n\n\n10\n\n\n10\n\n\n11\n\n\n11\n\n\nNone\n\n\n\n\nLabel IDs\n\n\n-100\n\n\n0\n\n\n0\n\n\n-100\n\n\n0\n\n\n0\n\n\n5\n\n\n-100\n\n\n-100\n\n\n6\n\n\n-100\n\n\n0\n\n\n0\n\n\n5\n\n\n-100\n\n\n5\n\n\n-100\n\n\n-100\n\n\n-100\n\n\n6\n\n\n-100\n\n\n-100\n\n\n0\n\n\n-100\n\n\n-100\n\n\n\n\nLabels\n\n\nIGN\n\n\nO\n\n\nO\n\n\nIGN\n\n\nO\n\n\nO\n\n\nB-LOC\n\n\nIGN\n\n\nIGN\n\n\nI-LOC\n\n\nIGN\n\n\nO\n\n\nO\n\n\nB-LOC\n\n\nIGN\n\n\nB-LOC\n\n\nIGN\n\n\nIGN\n\n\nIGN\n\n\nI-LOC\n\n\nIGN\n\n\nIGN\n\n\nO\n\n\nIGN\n\n\nIGN\n\n\n\n\n\n\n\nWrap the tokenization and label alignment steps into a single function\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\nDefine a mapping function to encode the dataset\ndef encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True, \n                      remove_columns=['langs', 'ner_tags', 'tokens'])\n\npanx_ch[\"de\"]\n    DatasetDict({\n        validation: Dataset({\n            features: ['tokens', 'ner_tags', 'langs'],\n            num_rows: 6290\n        })\n        test: Dataset({\n            features: ['tokens', 'ner_tags', 'langs'],\n            num_rows: 6290\n        })\n        train: Dataset({\n            features: ['tokens', 'ner_tags', 'langs'],\n            num_rows: 12580\n        })\n    })\n\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\npanx_de_encoded\n    DatasetDict({\n        validation: Dataset({\n            features: ['attention_mask', 'input_ids', 'labels'],\n            num_rows: 6290\n        })\n        test: Dataset({\n            features: ['attention_mask', 'input_ids', 'labels'],\n            num_rows: 6290\n        })\n        train: Dataset({\n            features: ['attention_mask', 'input_ids', 'labels'],\n            num_rows: 12580\n        })\n    })"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#performance-measures",
    "href": "posts/transformers-book-notes/chapter-4/index.html#performance-measures",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Performance Measures",
    "text": "Performance Measures\n\nStandard performance measures for NER tasks include precision, recall, and F1-score.\nThe model needs to correctly predict all words of an entity for a prediction to count as correct.\n\n\nseqval\n\nGitHub Repository\nA Python framework for sequence labeling evaluation\n\n\nfrom seqeval.metrics import classification_report\n\n\nclassification_report\n\nSource Code\nBuild a text report showing the main classification metrics for a sequence of targets and predictions.\nThe function expects targets and predictions as lists of lists.\n\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],[\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],[\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))\n                  precision    recall  f1-score   support\n    \n            MISC       0.00      0.00      0.00         1\n             PER       1.00      1.00      1.00         1\n    \n       micro avg       0.50      0.50      0.50         2\n       macro avg       0.50      0.50      0.50         2\n    weighted avg       0.50      0.50      0.50         2\n\nimport numpy as np\nFormat predictions and target labels for seqval\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#fine-tuning-xlm-roberta",
    "href": "posts/transformers-book-notes/chapter-4/index.html#fine-tuning-xlm-roberta",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Fine-Tuning XLM-RoBERTa",
    "text": "Fine-Tuning XLM-RoBERTa\nDefine training attributes\nfrom transformers import TrainingArguments\nnum_epochs = 3\nbatch_size = 64\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n    logging_steps=logging_steps, push_to_hub=True, fp16=True)\nNote: Set save_steps to a large number to disable checkpointing.\n\nLog into Hugging Face account\nfrom huggingface_hub import notebook_login\nnotebook_login()\n    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n    To disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n    Login successful\n    Your token has been saved to /home/innom-dt/.huggingface/token\n    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n    To disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nCompute the \\(f_{1}\\)-score on the validation set\nfrom seqeval.metrics import f1_score\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, \n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}\n\nDefine a collator to pad each input sequence to the highest sequence length in a batch\nfrom transformers import DataCollatorForTokenClassification\n\n\nDataCollatorForTokenClassification\n\nDocumentation\nCreate a data collator that will dynamically pad inputs and labels.\n\n\nprint_source(DataCollatorForTokenClassification.torch_call)\n    def torch_call(self, features):\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        labels = [feature[label_name] for feature in features\n            ] if label_name in features[0].keys() else None\n        batch = self.tokenizer.pad(features, padding=self.padding, max_length=\n            self.max_length, pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt' if labels is None else None)\n        if labels is None:\n            return batch\n        sequence_length = torch.tensor(batch['input_ids']).shape[1]\n        padding_side = self.tokenizer.padding_side\n        if padding_side == 'right':\n            batch[label_name] = [(list(label) + [self.label_pad_token_id] * (\n                sequence_length - len(label))) for label in labels]\n        else:\n            batch[label_name] = [([self.label_pad_token_id] * (sequence_length -\n                len(label)) + list(label)) for label in labels]\n        batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n        return batch\n\nDataCollatorForTokenClassification.label_pad_token_id\n    -100\nNote: * We need to pad the labels as they are also sequences. * The collator pads label sequences with the value -100, so the PyTorch loss function ignores them.\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n\nCreate a helper function to initialize a new model for a training session\ndef model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))\n\nDisable Tokenizers Parallelism\n%env TOKENIZERS_PARALLELISM=false\n    env: TOKENIZERS_PARALLELISM=false\n\nInitialize the Trainer object\nfrom transformers import Trainer\ntrainer = Trainer(model_init=model_init, args=training_args, \n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"], \n                  tokenizer=xlmr_tokenizer)\n\nRun the training loop and push the final model to the Hugging Face Hub\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n0.326400\n\n\n0.162317\n\n\n0.813909\n\n\n\n\n2\n\n\n0.136000\n\n\n0.133068\n\n\n0.845137\n\n\n\n\n3\n\n\n0.096000\n\n\n0.131872\n\n\n0.857581\n\n\n\n\n\n\n    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-de/commit/1ebdc3c9051a980588be5a495ad96896f330932c'\n\nHow to manually display the training log\ntrainer.state.log_history\n    [{'loss': 0.3264,\n      'learning_rate': 3.3671742808798645e-05,\n      'epoch': 0.99,\n      'step': 196},\n     {'eval_loss': 0.1623172014951706,\n      'eval_f1': 0.8139089269612262,\n      'eval_runtime': 7.0145,\n      'eval_samples_per_second': 896.714,\n      'eval_steps_per_second': 14.114,\n      'epoch': 1.0,\n      'step': 197},\n     {'loss': 0.136,\n      'learning_rate': 1.7174280879864637e-05,\n      'epoch': 1.99,\n      'step': 392},\n     {'eval_loss': 0.1330675333738327,\n      'eval_f1': 0.8451372416130125,\n      'eval_runtime': 6.8702,\n      'eval_samples_per_second': 915.543,\n      'eval_steps_per_second': 14.41,\n      'epoch': 2.0,\n      'step': 394},\n     {'loss': 0.096,\n      'learning_rate': 6.76818950930626e-07,\n      'epoch': 2.98,\n      'step': 588},\n     {'eval_loss': 0.13187244534492493,\n      'eval_f1': 0.8575809199318569,\n      'eval_runtime': 6.8965,\n      'eval_samples_per_second': 912.061,\n      'eval_steps_per_second': 14.355,\n      'epoch': 3.0,\n      'step': 591},\n     {'train_runtime': 95.0424,\n      'train_samples_per_second': 397.086,\n      'train_steps_per_second': 6.218,\n      'total_flos': 1039360955930616.0,\n      'train_loss': 0.18559023183211054,\n      'epoch': 3.0,\n      'step': 591}]\n\ndf = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss', 'eval_f1']]\ndf = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", \"eval_f1\":\"F1\"})\ndf['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\ndf['Training Loss'] = df[\"Training Loss\"].ffill()\ndf[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\ndf.drop_duplicates()\n\n\n\n\n\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n0\n\n\n1\n\n\n0.3264\n\n\n0.162317\n\n\n0.813909\n\n\n\n\n2\n\n\n2\n\n\n0.1360\n\n\n0.133068\n\n\n0.845137\n\n\n\n\n4\n\n\n3\n\n\n0.0960\n\n\n0.131872\n\n\n0.857581\n\n\n\n\n\n\n\nTest the model on some sample text\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_Jeff\n\n\n_De\n\n\nan\n\n\n_ist\n\n\n_ein\n\n\n_Informati\n\n\nker\n\n\n_bei\n\n\n_Google\n\n\n_in\n\n\n_Kaliforni\n\n\nen\n\n\n&lt;/s&gt;\n\n\n\n\nTags\n\n\nO\n\n\nB-PER\n\n\nI-PER\n\n\nI-PER\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nB-ORG\n\n\nO\n\n\nB-LOC\n\n\nI-LOC\n\n\nO\n\n\n\n\n\n\nNote: The fine-tuned model correctly identifies the entities in the sample text."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#error-analysis",
    "href": "posts/transformers-book-notes/chapter-4/index.html#error-analysis",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Error Analysis",
    "text": "Error Analysis\n\nError analysis is an effective tool to understand a model’s strengths and weaknesses.\nLooking at the errors can yield helpful insights and reveal bugs that would be hard to spot by only looking at the code.\nThere are several failure modes where a model might appear to perform well but have serious flaws.\n\n\nFailure Modes\n\nWe might accidentally mask too many tokens and some labels, resulting in a promising loss drop.\nThe metrics function might have a bug.\nWe might include the zero class, skewing the accuracy and \\(F_{1}\\)-score.\n\n\nDefine a function that returns the loss and predicted labels for a single batch\nfrom torch.nn.functional import cross_entropy\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model  \n        output = trainer.model(input_ids, attention_mask)\n        # Logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7), \n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}\n\nGet the loss and predictions for the validation set\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()\n\nindex2tag[-100] = \"IGN\"\n# Map IDs to tokens\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n# Map predicted label index to tag\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(lambda x: [index2tag[i] for i in x])\n# Map target label index to tag\ndf[\"labels\"] = df[\"labels\"].apply(lambda x: [index2tag[i] for i in x])\n# Remove padding for the loss field\ndf['loss'] = df.apply(lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n# Remove padding for the predicted label field\ndf['predicted_label'] = df.apply(lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1).T\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\nattention_mask\n\n\n[1, 1, 1, 1, 1, 1, 1]\n\n\n\n\ninput_ids\n\n\n[0, 10699, 11, 15, 16104, 1388, 2]\n\n\n\n\nlabels\n\n\n[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]\n\n\n\n\nloss\n\n\n[0.0, 0.03210718, 0.0, 0.05737416, 0.0494957, 0.062034503, 0.0]\n\n\n\n\npredicted_label\n\n\n[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]\n\n\n\n\ninput_tokens\n\n\n[&lt;s&gt;, Ham, a, (, Unternehmen, ), &lt;/s&gt;]\n\n\n\n\n\n\n\n# Transform each element of a list-like to a row\ndf_tokens = df.apply(pd.Series.explode)\n# Remove the tokens labeled with 'IGN'\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\n# Round loss values to two decimal places\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7).T.style.hide(axis='columns')\n\n\n\n\n\n\n\n\nattention_mask\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\ninput_ids\n\n\n10699\n\n\n15\n\n\n16104\n\n\n1388\n\n\n56530\n\n\n83982\n\n\n10\n\n\n\n\nlabels\n\n\nB-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nO\n\n\nB-ORG\n\n\nI-ORG\n\n\n\n\nloss\n\n\n0.030000\n\n\n0.060000\n\n\n0.050000\n\n\n0.060000\n\n\n0.000000\n\n\n0.600000\n\n\n0.380000\n\n\n\n\npredicted_label\n\n\nB-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nO\n\n\nB-ORG\n\n\nI-ORG\n\n\n\n\ninput_tokens\n\n\n_Ham\n\n\n_(\n\n\n_Unternehmen\n\n\n_)\n\n\n_WE\n\n\n_Luz\n\n\n_a\n\n\n\n\n\n\n\n(\n    # Group data by the input tokens\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    # Aggregate the losses for each token\n    .agg([\"count\", \"mean\", \"sum\"])\n    # Get rid of multi-level columns\n    .droplevel(level=0, axis=1)\n    # Sort values with the highest losses first\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\ninput_tokens\n\n\n_\n\n\n_in\n\n\n_von\n\n\n_der\n\n\n_/\n\n\n_und\n\n\n_(\n\n\n_)\n\n\n_’’\n\n\n_A\n\n\n\n\ncount\n\n\n6066\n\n\n989\n\n\n808\n\n\n1388\n\n\n163\n\n\n1171\n\n\n246\n\n\n246\n\n\n2898\n\n\n125\n\n\n\n\nmean\n\n\n0.03\n\n\n0.11\n\n\n0.14\n\n\n0.07\n\n\n0.51\n\n\n0.07\n\n\n0.28\n\n\n0.27\n\n\n0.02\n\n\n0.47\n\n\n\n\nsum\n\n\n187.46\n\n\n110.59\n\n\n110.46\n\n\n100.7\n\n\n83.81\n\n\n83.13\n\n\n69.48\n\n\n67.49\n\n\n59.03\n\n\n58.63\n\n\n\n\n\n\nNote:\n\nThe whitespace token has the highest total loss since it is the most common token.\nThe whitespace token has a low mean loss, indicating the model does not struggle to classify it.\nWords like “in,” “von,” “der,” and “und” often appear together with named entities and are sometimes part of them.\nIt is rare to have parentheses, slashes, and capital letters at the beginning of words, but those have a relatively high average loss.\n\n\n(\n    # Group data by the label IDs\n    df_tokens.groupby(\"labels\")[[\"loss\"]] \n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nlabels\n\n\nB-ORG\n\n\nI-LOC\n\n\nI-ORG\n\n\nB-LOC\n\n\nB-PER\n\n\nI-PER\n\n\nO\n\n\n\n\ncount\n\n\n2683\n\n\n1462\n\n\n3820\n\n\n3172\n\n\n2893\n\n\n4139\n\n\n43648\n\n\n\n\nmean\n\n\n0.59\n\n\n0.59\n\n\n0.42\n\n\n0.34\n\n\n0.3\n\n\n0.18\n\n\n0.03\n\n\n\n\nsum\n\n\n1582.79\n\n\n857.5\n\n\n1598.29\n\n\n1073.82\n\n\n861.09\n\n\n727.88\n\n\n1419.61\n\n\n\n\n\n\nNote: B-ORG has the highest average loss, meaning the model struggles to find the beginning of organization entities.\n\nPlot a confusion matrix of the token classification\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\nplot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)\n\n\n\n\n\nNote: The model often confuses the beginning subword (B-ORG) of an organizational entity with the subsequent subwords (I-ORG).\n\nExamine token sequences with high losses\ndef get_samples(df):\n    # Iterate over DataFrame rows\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n\n\n\n\ntokens\n\n\n_’\n\n\n_’’\n\n\n_Τ\n\n\nΚ\n\n\n_’’\n\n\n_’\n\n\n_’\n\n\n_’’\n\n\n_T\n\n\n_’’\n\n\n_’\n\n\nri\n\n\n_’’\n\n\n_’\n\n\nk\n\n\n_’’\n\n\n_’\n\n\nala\n\n\n&lt;/s&gt;\n\n\n\n\nlabels\n\n\nO\n\n\nO\n\n\nO\n\n\nIGN\n\n\nO\n\n\nO\n\n\nB-LOC\n\n\nI-LOC\n\n\nI-LOC\n\n\nI-LOC\n\n\nI-LOC\n\n\nIGN\n\n\nI-LOC\n\n\nI-LOC\n\n\nIGN\n\n\nI-LOC\n\n\nI-LOC\n\n\nIGN\n\n\nIGN\n\n\n\n\npreds\n\n\nO\n\n\nO\n\n\nB-ORG\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\n\n\nlosses\n\n\n0.00\n\n\n0.00\n\n\n2.42\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n9.83\n\n\n9.15\n\n\n7.60\n\n\n6.55\n\n\n6.66\n\n\n0.00\n\n\n5.83\n\n\n6.83\n\n\n0.00\n\n\n7.26\n\n\n7.44\n\n\n0.00\n\n\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n\n\n\n\ntokens\n\n\n_’’\n\n\n8\n\n\n.\n\n\n_Juli\n\n\n_’’\n\n\n_:\n\n\n_Protest\n\n\ncamp\n\n\n_auf\n\n\n_dem\n\n\n_Gelände\n\n\n_der\n\n\n_Republika\n\n\nn\n\n\nischen\n\n\n_Gar\n\n\nde\n\n\n&lt;/s&gt;\n\n\n\n\nlabels\n\n\nB-ORG\n\n\nIGN\n\n\nIGN\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nIGN\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nIGN\n\n\nIGN\n\n\nI-ORG\n\n\nIGN\n\n\nIGN\n\n\n\n\npreds\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nB-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nO\n\n\n\n\nlosses\n\n\n8.37\n\n\n0.00\n\n\n0.00\n\n\n4.67\n\n\n9.00\n\n\n8.87\n\n\n6.17\n\n\n0.00\n\n\n7.98\n\n\n8.33\n\n\n7.00\n\n\n4.32\n\n\n2.61\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.00\n\n\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n\n\n\n\ntokens\n\n\n_United\n\n\n_Nations\n\n\n_Multi\n\n\ndimensional\n\n\n_Integra\n\n\nted\n\n\n_Stabil\n\n\nization\n\n\n_Mission\n\n\n_in\n\n\n_the\n\n\n_Central\n\n\n_African\n\n\n_Republic\n\n\n&lt;/s&gt;\n\n\n\n\nlabels\n\n\nB-PER\n\n\nI-PER\n\n\nI-PER\n\n\nIGN\n\n\nI-PER\n\n\nIGN\n\n\nI-PER\n\n\nIGN\n\n\nI-PER\n\n\nI-PER\n\n\nI-PER\n\n\nI-PER\n\n\nI-PER\n\n\nI-PER\n\n\nIGN\n\n\n\n\npreds\n\n\nB-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\nI-ORG\n\n\n\n\nlosses\n\n\n5.46\n\n\n5.36\n\n\n5.51\n\n\n0.00\n\n\n5.53\n\n\n0.00\n\n\n5.46\n\n\n0.00\n\n\n5.06\n\n\n5.22\n\n\n5.62\n\n\n5.71\n\n\n5.36\n\n\n5.09\n\n\n0.00\n\n\n\n\n\n\nNote:\n\nThe PAN-X dataset used an imperfect automated process to apply annotations, resulting in some labeling issues.\n\nThe United Nations and the Central African Republic are organizations, not people.\nThe date “8. Juli” (July 8th) also has an incorrect label.\n\n\n\nExamine sequences with an opening parenthesis\nu\"\\u2581(\"\n    '_('\n\n\n\n\n\n\n```python df_tmp = df.loc[df[“input_tokens”].apply(lambda x: u”581(” in x)].head(2) for sample in get_samples(df_tmp): display(sample) ````\n\n\n* We generally don’t include the parentheses and their contents as part of the named entity, but the automated annotation process does. * Some parentheses contain a geographic specification. * We might want to disconnect this information from the original location in the annotations. * The dataset consists of Wikipedia articles in different languages, and the article titles often contain an explanation in parentheses. * We need to know about such characteristics in our datasets when rolling out models to production. * We can use these insights to clean up the dataset and retrain the model."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#cross-lingual-transfer",
    "href": "posts/transformers-book-notes/chapter-4/index.html#cross-lingual-transfer",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "Cross-Lingual Transfer",
    "text": "Cross-Lingual Transfer\nCreate a helper function to evaluate the model on different datasets\ndef get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]\n\nExamine the German model’s performance on the German test set\nf1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")\n    F1-score of [de] model on [de] dataset: 0.859\nTest the German model’s performance on French text\ntext_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n\n\n\n\nTokens\n\n\n&lt;s&gt;\n\n\n_Jeff\n\n\n_De\n\n\nan\n\n\n_est\n\n\n_informatic\n\n\nien\n\n\n_chez\n\n\n_Google\n\n\n_en\n\n\n_Cali\n\n\nfor\n\n\nnie\n\n\n&lt;/s&gt;\n\n\n\n\nTags\n\n\nO\n\n\nB-PER\n\n\nI-PER\n\n\nI-PER\n\n\nO\n\n\nO\n\n\nO\n\n\nO\n\n\nB-ORG\n\n\nO\n\n\nB-LOC\n\n\nB-LOC\n\n\nI-LOC\n\n\nO\n\n\n\n\n\n\nNote: The model correctly labeled the French translation of “Kalifornien” as a location.\n\nDefine a helper function to encode a dataset and generate a classification report\ndef evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])\n\nEvaluate the German model’s performance on the French test set\nf1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")\n    F1-score of [de] model on [fr] dataset: 0.708\nNote: The German model still performs relatively well despite not training on a single labeled French example.\n\nEvaluate the German model’s performance on the Italian test set\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")\n    F1-score of [de] model on [it] dataset: 0.691\n\nEvaluate the German model’s performance on the English test set\n#hide_output\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")\n    F1-score of [de] model on [en] dataset: 0.596\nNote: The model performs worse on the English dataset despite being closer to German than French.\n\n\nWhen Does Zero-Shot Transfer Make Sense?\n\nWe can determine at which point zero-shot cross-lingual transfer is superior to fine-tuning on a monolingual corpus by fine-tuning the model on training sets of increasing size.\n\nDefine a function to train a model on a downsampled dataset\ndef train_on_subset(dataset, num_samples):\n    # Downsample the training set to the target number of samples\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    training_args.logging_steps = len(train_ds) // batch_size\n    # Traing the model on the downsampled dataset\n    trainer = Trainer(model_init=model_init, args=training_args,\n        data_collator=data_collator, compute_metrics=compute_metrics,\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n    # Return the performance metrics\n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})\n\nEncode the French Dataset\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])\n\nTrain the model on 250 French samples\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n2.360800\n\n\n2.210924\n\n\n0.109819\n\n\n\n\n2\n\n\n2.192800\n\n\n1.484458\n\n\n0.032251\n\n\n\n\n3\n\n\n1.480200\n\n\n1.368229\n\n\n0.008093\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnum_samples\n\n\nf1_score\n\n\n\n\n\n\n0\n\n\n250\n\n\n0.007832\n\n\n\n\n\n\nNote: The French model significantly underperforms the German model when using only 250 examples.\n\nTrain the model on an increasing number of French samples\nfor num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n2.204900\n\n\n1.488627\n\n\n0.023411\n\n\n\n\n2\n\n\n1.465700\n\n\n1.249257\n\n\n0.144914\n\n\n\n\n3\n\n\n1.217400\n\n\n1.093112\n\n\n0.161066\n\n\n\n\n\n\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n1.800700\n\n\n1.176858\n\n\n0.175238\n\n\n\n\n2\n\n\n1.016500\n\n\n0.698511\n\n\n0.578441\n\n\n\n\n3\n\n\n0.669500\n\n\n0.540801\n\n\n0.639231\n\n\n\n\n\n\n\n\n\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n1.413700\n\n\n0.686913\n\n\n0.531559\n\n\n\n\n2\n\n\n0.526900\n\n\n0.386696\n\n\n0.741683\n\n\n\n\n3\n\n\n0.318900\n\n\n0.352989\n\n\n0.771843\n\n\n\n\n\n\n\n\n\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n0.895500\n\n\n0.371288\n\n\n0.757611\n\n\n\n\n2\n\n\n0.324200\n\n\n0.327193\n\n\n0.777248\n\n\n\n\n3\n\n\n0.243800\n\n\n0.284226\n\n\n0.822527\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()\n\n\n\n\n\nNote: * The zero-shot transfer model remains competitive until about 1500 training examples. * Getting domain experts to label hundreds (let alone thousands) of documents can be costly, especially for NER.\n\n\n\nFine-Tuning on Multiple Languages at Once\n\nWe can mitigate the performance drop from zero-shot cross-lingual transfer by fine-tuning with multiple languages at once.\n\n\nfrom datasets import concatenate_datasets\n\n\nconcatenate_datasets\n\nDocumentation\nConvert a list of Dataset objects with the same schema into a single Dataset.\n\n\nDefine a function to combine a list of datasets using based on split names\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus\n\nCombine the German and French datasets\npanx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])\n\nUpdate training attributes\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\nTrain the model on the combined dataset\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\n\n\n\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n0.371800\n\n\n0.176133\n\n\n0.822272\n\n\n\n\n2\n\n\n0.153500\n\n\n0.160763\n\n\n0.840360\n\n\n\n\n3\n\n\n0.107400\n\n\n0.157969\n\n\n0.854692\n\n\n\n\n\n\nTrainOutput(global_step=807, training_loss=0.2103209033368393, metrics={'train_runtime': 129.7072, 'train_samples_per_second': 396.894, 'train_steps_per_second': 6.222, 'total_flos': 1399867154966784.0, 'train_loss': 0.2103209033368393, 'epoch': 3.0})\n\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-de-fr/commit/e93b59a0d16dc03a657342fd9bf31413af9aebc1'\n\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")\n    F1-score of [de-fr] model on [de] dataset: 0.862\n    F1-score of [de-fr] model on [fr] dataset: 0.848\n    F1-score of [de-fr] model on [it] dataset: 0.793\n    F1-score of [de-fr] model on [en] dataset: 0.688\nNote: The model now performs much better on the French split, and it even improved on the Italian and English sets.\n\nTest the performance from fine-tuning on each language separately\ncorpora = [panx_de_encoded]\n\n# Exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # Fine-tune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # Collect F1-scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # Add monolingual corpus to list of corpora to concatenate\n    corpora.append(ds_encoded)\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n0.854100\n\n\n0.352915\n\n\n0.782609\n\n\n\n\n2\n\n\n0.306900\n\n\n0.280733\n\n\n0.815359\n\n\n\n\n3\n\n\n0.226200\n\n\n0.271911\n\n\n0.829342\n\n\n\n\n\n\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n1.454800\n\n\n0.652213\n\n\n0.545667\n\n\n\n\n2\n\n\n0.521400\n\n\n0.347615\n\n\n0.740443\n\n\n\n\n3\n\n\n0.318600\n\n\n0.292827\n\n\n0.773021\n\n\n\n\n\n\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n1.711900\n\n\n1.000937\n\n\n0.226577\n\n\n\n\n2\n\n\n0.891000\n\n\n0.640531\n\n\n0.528084\n\n\n\n\n3\n\n\n0.602300\n\n\n0.508421\n\n\n0.579369\n\n\n\n\n\n\n\nTest the performance from multilingual learning on all the corpora\ncorpora_encoded = concatenate_splits(corpora)\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n    eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")\nCloning https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-all into local empty directory.\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nF1\n\n\n\n\n\n\n1\n\n\n0.370100\n\n\n0.200005\n\n\n0.805385\n\n\n\n\n2\n\n\n0.162900\n\n\n0.168012\n\n\n0.837781\n\n\n\n\n3\n\n\n0.115600\n\n\n0.167403\n\n\n0.847745\n\n\n\n\n\n\n    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-all/commit/f01950fd63b31959f5c3d520125366485fb375b6'\n\nGenerate predictions on the test set for each language\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n\nscores_data = {\"de\": f1_scores[\"de\"],\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n               \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n                         inplace=True)\nf1_scores_df\n\n\n\n\n\n\nEvaluated on\n\n\nde\n\n\nfr\n\n\nit\n\n\nen\n\n\n\n\nFine-tune on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nde\n\n\n0.8590\n\n\n0.7079\n\n\n0.6910\n\n\n0.5962\n\n\n\n\neach\n\n\n0.8590\n\n\n0.8321\n\n\n0.7696\n\n\n0.5962\n\n\n\n\nall\n\n\n0.8592\n\n\n0.8568\n\n\n0.8646\n\n\n0.7678\n\n\n\n\n\n\nNote:\n\nMultilingual learning can provide significant performance gains.\nYou should generally focus your attention on cross-lingual transfer within language families."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-4/index.html#references",
    "href": "posts/transformers-book-notes/chapter-4/index.html#references",
    "title": "Notes on Transformers Book Ch. 4",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 3\nNext: Notes on Transformers Book Ch. 5"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html",
    "href": "posts/transformers-book-notes/chapter-5/index.html",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "",
    "text": "Introduction\nThe Challenge with Generating Coherent Text\nGreedy Search Decoding\nBeam Search Decoding\nSampling Methods\nTop-k and Nucleus Sampling\nWhich Decoding Method Is Best?\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#introduction",
    "href": "posts/transformers-book-notes/chapter-5/index.html#introduction",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Introduction",
    "text": "Introduction\n\nTransformer-based language models like GPT-2 and GPT-3 can generate text almost indistinguishable from text written by humans.\nSuch models acquire a broad set of skills and pattern recognition abilities by learning to predict the next word in the text of millions of web pages.\nWe can activate these skills with different kinds of input prompts.\nLanguage models are exposed to sequences of tasks during pretraining that we can adapt during inference."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#the-challenge-with-generating-coherent-text",
    "href": "posts/transformers-book-notes/chapter-5/index.html#the-challenge-with-generating-coherent-text",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "The Challenge with Generating Coherent Text",
    "text": "The Challenge with Generating Coherent Text\n\nConverting a model’s probabilistic output to text requires a decoding method.\nThe decoding process is iterative and involves significantly more computing than passing inputs once through the forward pass of a model.\nThe quality and diversity of the generated text depend on the choice of decoding method and associated hyperparameters.\n\n\nGPT-2 Pretraining Process\n\nGPT-2 is pretrained to estimate the probability \\(P\\left(y \\vert x \\right)\\) of a sequence of tokens \\(y = y_{1},y_{2},\\ldots,y_{t}\\) occurring in the text \\(x = x_{1},x_{2},\\ldots,x_{k}\\), given some initial prompt or context sequence.\nIt is common to use the chain rule of probability to factorize it as a product of conditional probabilities.\n\n\n\n\\[P\\left(y_{1},\\ldots,y_{t} \\vert x \\right) = \\prod^{N}_{t=1}{P\\left(y_{t} \\vert y_{ \\ &lt; \\ t},x \\right)}\\]\n\n\nwhere \\(y_{ \\ &lt; \\ t}\\) is the shorthand notation for the sequence \\(y_{1},\\ldots,y_{t-1}\\)\n\n\n\nWe can adapt this token prediction task to generate sequences of arbitrary length by feeding the model a prompt.\nWe then iteratively add the next predicted token to the prompt and feed the new prompt to the model.\nSome call this type of text generation conditional text generation since the output sequence depends on the choice of input prompt.\n\n\n\nDecoding\n\nA decoding method determines which token to select at each timestep.\nThe language model produces a logit \\(z_{t,i}\\) per token in the vocabulary at each time step.\nWe can get the probability distribution over the next possible token \\(w_{i}\\) by taking the softmax.\n\n\n\n\\[P\\left(y_{t} = w_{i} \\vert y_{ \\ &lt; \\ t},x \\right) = softmax \\left( z_{t,i} \\right)\\]\n\nMost decoder methods search for the most likely overall sequence by picking a \\(\\hat{y}\\) such that\n\n\n\n\\[\\hat{y} = \\underset{y}{argmax} P\\left(y \\vert x \\right)\\]\n\nWe use approximations for \\(\\hat{y}\\) instead of finding it directly."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#greedy-search-decoding",
    "href": "posts/transformers-book-notes/chapter-5/index.html#greedy-search-decoding",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Greedy Search Decoding",
    "text": "Greedy Search Decoding\n\nThe simplest decoding method is to greedily select the token with the highest probability at each timestep.\n\n\n\\[\\hat{y}_{t} = \\underset{y}{argmax} {P\\left(y_{t} \\vert y_{ \\ &lt; \\ t},x \\right)}\\]\n\nGreedy search decoding tends to produce repetitive output sequences.\nGreedy search can miss sequences whose overall probability is higher when low probability words precede high-probability words.\nGreedy search is not suitable for text generation tasks that require diversity.\nGreedy search is better suited for producing short sequences like arithmetic that require deterministic and factually correct output.\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nLoad the 1.5-billion-parameter version of GPT-2 with a language modeling head\nNote: The model takes up around 8GB of VRAM.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nPrepare Input\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\ntokenizer.convert_ids_to_tokens(input_ids[0])\n    ['Transform', 'ers', 'Ġare', 'Ġthe']\n\nPerform Greedy Search Decoding\niterations = []\nn_steps = 8\nchoices_per_step = 5\n\nwith torch.no_grad():\n    for _ in range(n_steps):\n        iteration = dict()\n        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n        output = model(input_ids=input_ids)\n        # Select logits of the first batch and the last token and apply softmax\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        # Store tokens with highest probabilities\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        # Append predicted next token to input\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        iterations.append(iteration)\n        \npd.DataFrame(iterations)\n\n\n\n\n\n\n\n\n\nInput\n\n\nChoice 1\n\n\nChoice 2\n\n\nChoice 3\n\n\nChoice 4\n\n\nChoice 5\n\n\n\n\n\n\n0\n\n\nTransformers are the\n\n\nmost (8.53%)\n\n\nonly (4.96%)\n\n\nbest (4.65%)\n\n\nTransformers (4.37%)\n\n\nultimate (2.16%)\n\n\n\n\n1\n\n\nTransformers are the most\n\n\npopular (16.78%)\n\n\npowerful (5.37%)\n\n\ncommon (4.96%)\n\n\nfamous (3.72%)\n\n\nsuccessful (3.20%)\n\n\n\n\n2\n\n\nTransformers are the most popular\n\n\ntoy (10.63%)\n\n\ntoys (7.23%)\n\n\nTransformers (6.60%)\n\n\nof (5.46%)\n\n\nand (3.76%)\n\n\n\n\n3\n\n\nTransformers are the most popular toy\n\n\nline (34.38%)\n\n\nin (18.20%)\n\n\nof (11.71%)\n\n\nbrand (6.10%)\n\n\nline (2.69%)\n\n\n\n\n4\n\n\nTransformers are the most popular toy line\n\n\nin (46.28%)\n\n\nof (15.09%)\n\n\n, (4.94%)\n\n\non (4.40%)\n\n\never (2.72%)\n\n\n\n\n5\n\n\nTransformers are the most popular toy line in\n\n\nthe (65.99%)\n\n\nhistory (12.42%)\n\n\nAmerica (6.91%)\n\n\nJapan (2.44%)\n\n\nNorth (1.40%)\n\n\n\n\n6\n\n\nTransformers are the most popular toy line in the\n\n\nworld (69.26%)\n\n\nUnited (4.55%)\n\n\nhistory (4.29%)\n\n\nUS (4.23%)\n\n\nU (2.30%)\n\n\n\n\n7\n\n\nTransformers are the most popular toy line in the world\n\n\n, (39.73%)\n\n\n. (30.64%)\n\n\nand (9.87%)\n\n\nwith (2.32%)\n\n\ntoday (1.74%)\n\n\n\n\n\n\nNote: The generated sentence indicates that GPT-2 internalized some knowledge about the Transformers media franchise during pretraining.\n\n\nGenerationMixin.generate\n\nDocumentation\nGenerates sequences of token ids for models with a language modeling head.\ngreedy decoding:\n\nnum_beams=1 and do_sample=False\n\nmultinomial decoding:\n\nnum_beams=1 and do_sample=True\n\nbeam-search decoding:\n\nnum_beams&gt;1 and do_sample=False\n\nbeam-search multinomial sampling:\n\nnum_beams&gt;1 and do_sample=True\n\ndiverse beam-search decoding:\n\nnum_beams&gt;1 and `num_beam_groups&gt;1\n\nconstrained beam-search decoding:\n\nconstraints!=None or force_words_ids!=None\n\n\n\nPerform Greedy Search Decoding with the generate() function\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))\n    Transformers are the most popular toy line in the world,\n\nTry to perform arithmetic with Greedy Search Decoding\nmax_length = 20\ninput_txt = \"\"\"5 + 8 =&gt; 13 \\n 7 + 2 =&gt; 9 \\n 1 + 0 =&gt;\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length, \n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n    5 + 8 =&gt; 13 \n     7 + 2 =&gt; 9 \n     1 + 0 =&gt; 1 \n\nmax_length = 20\ninput_txt = \"\"\"5 + 8 =&gt; 13 \\n 7 + 2 =&gt; 9 \\n 2 * 10 =&gt;\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length, \n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n    5 + 8 =&gt; 13 \n     7 + 2 =&gt; 9 \n     2 * 10 =&gt; 20 \n\nmax_length = 20\ninput_txt = \"\"\"5 + 8 =&gt; 13 \\n 7 + 2 =&gt; 9 \\n 2 * 13 =&gt;\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length, \n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n    5 + 8 =&gt; 13 \n     7 + 2 =&gt; 9 \n     2 * 13 =&gt; 13 \nNote: Not perfect.\n\nTry to replicate the OpenAI Unicorn story with Greedy Search Decoding\nmax_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length, \n                               do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\n​    \n​    The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\n​    \n​    The researchers were surprised to find that the unicorns were able\nNote: The results demonstrate the repetitive output that is characteristic of greedy search decoding."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#beam-search-decoding",
    "href": "posts/transformers-book-notes/chapter-5/index.html#beam-search-decoding",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Beam Search Decoding",
    "text": "Beam Search Decoding\n\nBeam search keeps track of the \\(top-b\\) most probable next tokens, where \\(b\\) is the number of beams or partial hypotheses.\nThere is a tradeoff between output quality and speed when choosing the number of beams.\nWe choose the next set of beams by considering all possible next-token extensions of the existing ones and selecting the \\(b\\) most likely extensions.\nWe repeat this process until we reach the maximum length or an EOS token.\nWe select the most likely sequence by ranking the \\(b\\) beams according to their log probabilities.\n\n\n\\[\\log{P\\left(y_{1},\\ldots,y_{t} \\vert x \\right)} = \\sum^{N}_{t=1}{\\log{P\\left(y_{t} \\vert y_{ \\ &lt; \\ t},x \\right)}}\\]\n\nNote: We use the log probabilities to avoid numerical instability due to floating-point precision.\n0.5 ** 1024\n    5.562684646268003e-309\n\nimport numpy as np\n\nsum([np.log(0.5)] * 1024)\n    -709.7827128933695\n\nimport torch.nn.functional as F\n\n\nlog_softmax\n\nDocumentation\nMathematically equivalent to log(softmax(x))\n\n\n\n\n\\[\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i)}{ \\sum_j \\exp(x_j)} \\right)\\]\n\nDefine a function to calculate the log probability of a single token\ndef log_probs_from_logits(logits, labels):\n    # Normalize the logits with softmax before taking the log\n    logp = F.log_softmax(logits, dim=-1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    return logp_label\n\nDefine a function to calculate the log probabilities of a sequence\ndef sequence_logprob(model, labels, input_len=0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(\n            # We dont need the last logit since we don't have a ground truth token for it\n            # We don't have a logit for the first token\n            output.logits[:, :-1, :], labels[:, 1:])\n        # Sum the log probabilities for each token\n        # Ignore the log probabilities of the input sequence\n        seq_log_prob = torch.sum(log_probs[:, input_len:])\n    return seq_log_prob.cpu().numpy()\n\nCompare the log probabilities texts generated by greedy and beam search\nlogp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_greedy[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\n​    \n​    The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\n​    \n​    The researchers were surprised to find that the unicorns were able\n​    \n    log-prob: -87.43\n\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n                             do_sample=False)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n\n\n​    \n​    The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n​    \n    log-prob: -55.23\nNote: * A higher log probability is better. * Beam search still suffers from repetitive text. * We can impose an n-gram penalty that tracks which n-grams are already present in the output.\n\nTest beam search with an n-gram penalty\noutput_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n                             do_sample=False, no_repeat_ngram_size=2)\nlogp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\nprint(tokenizer.decode(output_beam[0]))\nprint(f\"\\nlog-prob: {logp:.2f}\")\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n​    \n    According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n    \n    log-prob: -93.12\nNote: * There are no repetitions, and the text remains coherent despite a lower log probability. * We can use beam search with an n-gram penalty to find a tradeoff between focusing on high-probability tokens while reducing repetitions."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#sampling-methods",
    "href": "posts/transformers-book-notes/chapter-5/index.html#sampling-methods",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Sampling Methods",
    "text": "Sampling Methods\n\nThe simplest sampling method is to randomly sample from the probability distribution of the model’s outputs over the entire vocabulary at each timestep.\n\n\n\\[P\\left(y_{t} = w_{i} \\vert y_{ \\ &lt; \\ t},x \\right) = \\text{softmax} \\left( z_{t,i} \\right) = \\frac{\\exp(z_{t,i})}{ \\sum^{|V|}_{j=1} \\exp(z_{t,j})}\\]\n\nwhere \\(\\vert V \\vert\\) denotes the cardinality of the vocabulary\nWe can control the diversity of the output by adding a temperature parameter \\(T\\) that rescales the logits before taking the softmax.\n\n\n\n\\[\\left(y_{t} = w_{i} \\vert y_{ \\ &lt; \\ t},x \\right) = \\text{softmax} \\left( z_{t,i} \\right) = \\frac{\\frac{\\exp(z_{t,i})}{T}}{ \\sum^{|V|}_{j=1} \\frac{\\exp(z_{t,j}}{T})}\\]\n\nWe can tune the temperature parameter to control the shape of the probability distribution.\nA \\(T\\) value much less than \\(1\\) suppresses the rare tokens.\nA \\(T\\) value much greater than \\(1\\) causes each token to become equally likely.\n\n\nimport matplotlib.pyplot as plt\n\nDefine a softmax function with a temperature parameter\ndef softmax(logits, T=1):\n    e_x = np.exp(logits / T)\n    return e_x / e_x.sum()\n\nPlot a distribution of randomly generated token probabilities for three selected temperatures\nlogits = np.exp(np.random.random(1000))\nsorted_logits = np.sort(logits)[::-1]\nx = np.arange(1000)\n\nfor T in [0.5, 1.0, 2.0]:\n    plt.step(x, softmax(sorted_logits, T), label=f\"T={T}\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Sorted token probabilities\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\nReset random seed\ntorch.manual_seed(42);\n\nSample generated text with a high temperature\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n                             temperature=2.0, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    While the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter\nNote: Sampling with a high temperature produces gibberish.\n\nReset random seed\ntorch.manual_seed(42);\n\nSample generated text with a low temperature\noutput_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n                             temperature=0.5, top_k=0)\nprint(tokenizer.decode(output_temp[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.\n\n\n​    \n​    The unicorns were living in a remote valley in the Andes mountains\n​    \n    'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.\n\n\n​    \n​    'But when\nNote: Sampling with a low temperature produces a much more coherent output."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#top-k-and-nucleus-sampling",
    "href": "posts/transformers-book-notes/chapter-5/index.html#top-k-and-nucleus-sampling",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Top-k and Nucleus Sampling",
    "text": "Top-k and Nucleus Sampling\nReset random seed\ntorch.manual_seed(42);\n\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\nCalculate the probability distribution of the model’s outputs at \\(T=1\\)\nwith torch.no_grad():\n    output = model(input_ids=input_ids)\n    next_token_logits = output.logits[:, -1, :]\n    probs = F.softmax(next_token_logits, dim=-1).detach().cpu().numpy()\n\nPlot the cumulative probability distribution of the model’s outputs at \\(T=1\\)\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\naxes[0].hist(probs[0], bins=np.logspace(-10, -1, 100), color=\"C0\", edgecolor=\"C0\")\naxes[0].set_xscale(\"log\")\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Probability distribution\")\naxes[0].set_xlabel(\"Probability\")\naxes[0].set_ylabel(\"Count\")\n\naxes[1].plot(np.cumsum(np.sort(probs[0])[::-1]), color=\"black\")\naxes[1].set_xlim([0, 10000])\naxes[1].set_ylim([0.75, 1.01])\naxes[1].set_title(\"Cumulative probability\")\naxes[1].set_ylabel(\"Probability\")\naxes[1].set_xlabel(\"Token (descending probability)\")\naxes[1].minorticks_on()\n\ntop_k_label = 'top-k threshold (k=2000)'\ntop_p_label = 'nucleus threshold (p=0.95)'\naxes[1].vlines(x=2000, ymin=0, ymax=2, color='C0', label=top_k_label)\naxes[1].hlines(y=0.95, xmin=0, xmax=10000, color='C1', label=top_p_label, linestyle='--')\naxes[1].legend(loc='lower right')\nplt.tight_layout()\n\n\n\n\n\nNote: * The histogram has a peak around 10^-8 and a second smaller peak around 10^-4, followed by a sharp drop. * The probability of picking the token with the highest likelihood is 1 in 10. * There are 50,257 tokens in GPT-2’s vocabulary. * The curved line in the Cumulative probability chart represents the probability of picking any of the preceding tokens. * There is a 1 in 100 chance of not picking any of the tokens that are not n the top 2000. * There is a significant chance of picking an unlikely token when sampling hundreds of times. * Picking such tokens can negatively impact the quality of the generated text.\n\n\nTop-k Sampling\n\nThe idea behind top-k sampling is to avoid low-probability choices by only choosing from the k tokens with the highest probability.\nWe can find a good value for k by looking at some text quality metrics.\n\n\nReset random seed\ntorch.manual_seed(42);\n\nGenerate text using the 50 tokens with the highest probability\noutput_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n                             top_k=50)\nprint(tokenizer.decode(output_topk[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The wild unicorns roam the Andes Mountains in the region of Cajamarca, on the border with Argentina (Picture: Alamy/Ecole Nationale Supérieure d'Histoire Naturelle)\n​    \n    The researchers came across about 50 of the animals in the valley. They had lived in such a remote and isolated area at that location for nearly a thousand years that\n\n\n### Nucleus (top-p) Sampling\n* The idea behind nucleus sampling is to cut off the long tail of the distribution after reaching a certain probability mass in the selection.\n* We order all tokens in descending order by probability and add one token after another from the top of the list until the sum of the probabilities of the selected tokens reaches the target mass.\n\nReset random seed\ntorch.manual_seed(42);\n\nGenerate text using top-p sampling\noutput_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n                             top_p=0.90)\nprint(tokenizer.decode(output_topp[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.\n\n\n​    \n​    According to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.\n\n\n​    \n​    Since their migration, the animals have been adapting to\nNote: Top-p sampling also produces a coherent story.\n\nReset random seed\ntorch.manual_seed(42);\n\nGenerate text using top-k and top-p sampling\noutput_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n                             top_k=50, top_p=0.90)\nprint(tokenizer.decode(output_topp[0]))\n    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n​    \n​    The scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.\n\n\n​    \n​    According to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.\n\n\n​    \n​    Since their migration, the animals have been adapting to"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#which-decoding-method-is-best",
    "href": "posts/transformers-book-notes/chapter-5/index.html#which-decoding-method-is-best",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "Which Decoding Method Is Best?",
    "text": "Which Decoding Method Is Best?\n\nThe best approach depends on the nature of the task.\nLower the temperature or use deterministic methods to perform a precise task like arithmetic or providing an answer to a specific question.\nSwitch to sampling methods and increase the temperature when you want the model to generate longer text and be more creative."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-5/index.html#references",
    "href": "posts/transformers-book-notes/chapter-5/index.html#references",
    "title": "Notes on Transformers Book Ch. 5",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 4\nNext: Notes on Transformers Book Ch. 6"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html",
    "href": "posts/transformers-book-notes/chapter-6/index.html",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "",
    "text": "Introduction\nProject: Summarize Dialogues Between Several People\nThe CNN/DailyMail Dataset\nText Summarization Pipelines\nComparing Different Summaries\nMeasuring the Quality of Generated Text\nEvaluating PEGASUS on the CNN/DailyMail Dataset\nTraining a Summarization Model\nConclusion\nReferences\nNote: Had to update the datasets package to version 2.0.0."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#introduction",
    "href": "posts/transformers-book-notes/chapter-6/index.html#introduction",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Introduction",
    "text": "Introduction\n\nText summarization requires the model to understand long passages, reason about the contents, and produce fluent text that incorporates the main topics from the original document."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#project-summarize-dialogues-between-several-people",
    "href": "posts/transformers-book-notes/chapter-6/index.html#project-summarize-dialogues-between-several-people",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Project: Summarize Dialogues Between Several People",
    "text": "Project: Summarize Dialogues Between Several People\n\nText summarization requires the model to understand long passages, reason about the contents, and produce fluent text that incorporates the main topics from the original document.\nText summarization is a classic sequence-to-sequence task with an input text and a target text.\nThe goal is to build an encoder-decoder model to condense dialogues between several people into a crisp summary."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#the-cnndailymail-dataset",
    "href": "posts/transformers-book-notes/chapter-6/index.html#the-cnndailymail-dataset",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "The CNN/DailyMail Dataset",
    "text": "The CNN/DailyMail Dataset\n\nThe CNN/DailyMail dataset contains over 300,000 pairs of news articles and their corresponding summaries.\nThe summaries are composed of the bullet points that CNN and The Daily Mail attach to their articles.\nThe summaries are abstractive, consisting of new sentences instead of excerpts.\nGitHub Repository\nHugging Face Dataset Card\n\n\nfrom datasets import load_dataset\n\nLoad Version 3.0.0 of the CNN/DailMail dataset\ndataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\nprint(f\"Features: {dataset['train'].column_names}\")\n    Features: ['article', 'highlights', 'id']\nNote:\n\nThe dataset has three columns.\n\nThe article column contains the news articles.\nThe highlights column contains the summaries.\nThe ids column uniquely identifies each article.\n\n\n\nView an excerpt from an article\nsample = dataset[\"train\"][1]\narticle_len = len(sample[\"article\"])\nhighlights_len = len(sample[\"highlights\"])\nprint(f\"\"\"\nArticle (excerpt of 500 characters, total length: {article_len}):\n\"\"\")\nprint(sample[\"article\"][:500])\nprint(f'\\nSummary (length: {highlights_len}):')\nprint(sample[\"highlights\"])\nprint(f'\\nThe article is {(article_len/highlights_len):.2f} times longer than the summary.')\n\n    Article (excerpt of 500 characters, total length: 3192):\n    \n    (CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n    \n    Summary (length: 180):\n    Usain Bolt wins third gold of world championship .\n    Anchors Jamaica to 4x100m relay victory .\n    Eighth gold at the championships for Bolt .\n    Jamaica double up in women's 4x100m relay .\n    \n    The article is 17.73 times longer than the summary.\nNote: * Articles can be very long compared to the target summary. * Long articles are challenging for most transformer models as their context size usually maxes out around 1,000 tokens. * A thousand tokens can contain a few paragraphs of text. * The standard workaround to this limitation is to truncate the text to the model’s context size. * This approach risks losing important information in the excluded portion of the text."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#text-summarization-pipelines",
    "href": "posts/transformers-book-notes/chapter-6/index.html#text-summarization-pipelines",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Text Summarization Pipelines",
    "text": "Text Summarization Pipelines\nGet a 2,000 character excerpt from an article\nsample_text = dataset[\"train\"][1][\"article\"][:2000]\n# We'll collect the generated summaries of each model in a dictionary\nsummaries = {}\nNote: It is a convention in summarization to separate sentences by a newline.\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\n\nThe Natural Language Tookit (NLTK)\n\nHomepage\nThe toolkit provides easy-to-use interfaces and a suite of text processing libraries.\n\n\n\nsent_tokenize\n\nDocumentation\nGet a sentence-tokenized copy of a text.\n\n\n\nnltk.tokenize.punkt\n\nDocumentation\nThe Punkt sentence tokenizer divides a text into a list of sentences.\nAn unsupervised algorithm builds a model of abbreviation words, collocations, and words that start sentences.\n\nDownload the Punkt package\nnltk.download(\"punkt\")\n    [nltk_data] Downloading package punkt to /home/innom-dt/nltk_data...\n    [nltk_data]   Package punkt is already up-to-date!\n\n    True\n\nTest the sentence tokenizer\nstring = \"The U.S. are a country. The U.N. is an organization.\"\nsent_tokenize(string)\n    ['The U.S. are a country.', 'The U.N. is an organization.']\n\n\n\nSummarization Baseline\n\nA common baseline for summarizing news articles is to take the first three sentences of the article.\n\nDefine a function to extract the first three sentences from a text\ndef three_sentence_summary(text):\n    return \"\\n\".join(sent_tokenize(text)[:3])\n\nCreate a baseline summary\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)\n\nSet device number for Hugging Face Pipelines\nimport torch\n# 0 for the first CUDA GPU\n# -1 for CPU\ndevice_num = 0 if torch.cuda.is_available() else -1\n\n\n\nGPT-2\n\nWe can use the GPT-2 model to generate summaries by appending “TL;DR” to the end of the input text.\nTL;DR indicates a short version of a long post.\n\n\nfrom transformers import pipeline, set_seed\n\n\nTextGenerationPipeline\n\nDocumentation\nCreate a language generation pipeline that predicts the words that will follow a specified text prompt.\n\nReset random seed\nset_seed(42)\n\nCreate a language generation pipeline using GPT-2\npipe = pipeline(\"text-generation\", model=\"gpt2-xl\", device=device_num)\ntype(pipe)\n    transformers.pipelines.text_generation.TextGenerationPipeline\n\nCreate a summary with the GPT-2 model\ngpt2_query = sample_text + \"\\nTL;DR:\\n\"\npipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\nsummaries[\"gpt2\"] = \"\\n\".join(\n    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\npipe_out[0]\n    {'generated_text': '(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men\\'s 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles. The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital. \"I\\'m proud of myself and I\\'ll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems. Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt. Earlier, Jamaica\\'s women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds. Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover. The British quartet, who were initially fourth, were promoted to the bronze which eluded their men\\'s team. Fraser-Pryce, like Bolt ag\\nTL;DR:\\nThe World Championships have come to a close and Usain Bolt has been crowned world champion. The Jamaica sprinter ran a lap of the track at 20.52 seconds, faster than even the world\\'s best sprinter from last year -- South Korea\\'s Yuna Kim, whom Bolt outscored by 0.26 seconds. It\\'s his third medal in succession at the championships: 2011, 2012 and'}\n\nsummaries[\"gpt2\"]\n    \"The World Championships have come to a close and Usain Bolt has been crowned world champion.\\nThe Jamaica sprinter ran a lap of the track at 20.52 seconds, faster than even the world's best sprinter from last year -- South Korea's Yuna Kim, whom Bolt outscored by 0.26 seconds.\\nIt's his third medal in succession at the championships: 2011, 2012 and\"\n\n\n\n\nT5\n\nWe can perform several tasks using the text prompts from the training process.\n\n\nText-to-Text Prompts:\n\nTranslation: “translate {source-language} to {target-languge}: {text}”\nLinguistic Acceptability: “cola sentence: {text}”\nSemantic Similarity: “stsb sentence 1: {text1} sentence 2: {text2}”\nSummarization: “summarize: {text}”\n\nReset random seed\nset_seed(42)\n\nCreate a text generation pipeline with the T5 model\npipe = pipeline(\"summarization\", model=\"t5-large\", device=device_num)\n\nCreate a summary with the T5 model\npipe_out = pipe(sample_text)\nsummaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n\nsummaries[\"t5\"]\n    \"usain bolt wins his third gold medal of the world championships in the men's 4x100m relay .\\nthe 26-year-old anchored Jamaica to victory in the event in the Russian capital .\\nhe has now collected eight gold medals at the championships, equaling the record .\"\n\n\n\n\nBART\n\nThe “facebook/bart-large-cnn” model checkpoint is fine-tuned specifically on the CNN/DailyMail dataset.\n\nReset random seed\nset_seed(42)\n\nCreate a text generation pipeline with the BART model\npipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device_num)\n\nCreate a summary with the BART model\npipe_out = pipe(sample_text)\nsummaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n\nsummaries[\"bart\"]\n    \"Usain Bolt wins his third gold of the world championships in Moscow.\\nBolt anchors Jamaica to victory in the men's 4x100m relay.\\nThe 26-year-old has now won eight gold medals at world championships.\\nJamaica's women also win gold in the relay, beating France in the process.\"\n\n\n\nPEGASUS\n\nPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\nPEGASUS is an encoder-decoder transformer trained to predict masked sentences in multisentence tests.\nThe authors argue that the pretraining objective is more effective the closer is to the downstream task.\nThe authors trained PEGASUS to reconstruct sentences containing most of the content of their surrounding paragraphs in a large corpus.\nThe model has dedicated tokens for newlines, so we do not need the sent_tokenize function.\n\nReset random seed\nset_seed(42)\n\nCreate a text generation pipeline with the PEGASUS model\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", device=device_num)\nCreate a summary with the PEGASUS model\npipe_out = pipe(sample_text)\nsummaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .&lt;n&gt;\", \".\\n\")\n\nsummaries[\"pegasus\"]\n    \"Usain Bolt wins third gold of world championships.\\nAnchors Jamaica to victory in men's 4x100m relay.\\nEighth gold at the championships for Bolt.\\nJamaica also win women's 4x100m relay .\""
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#comparing-different-summaries",
    "href": "posts/transformers-book-notes/chapter-6/index.html#comparing-different-summaries",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Comparing Different Summaries",
    "text": "Comparing Different Summaries\nprint(\"GROUND TRUTH\")\nprint(dataset[\"train\"][1][\"highlights\"])\nprint(\"\")\n\nfor model_name in summaries:\n    print(model_name.upper())\n    print(summaries[model_name])\n    print(\"\")\n    GROUND TRUTH\n    Usain Bolt wins third gold of world championship .\n    Anchors Jamaica to 4x100m relay victory .\n    Eighth gold at the championships for Bolt .\n    Jamaica double up in women's 4x100m relay .\n    \n    BASELINE\n    (CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n    The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n    The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n    \n    GPT2\n    The World Championships have come to a close and Usain Bolt has been crowned world champion.\n    The Jamaica sprinter ran a lap of the track at 20.52 seconds, faster than even the world's best sprinter from last year -- South Korea's Yuna Kim, whom Bolt outscored by 0.26 seconds.\n    It's his third medal in succession at the championships: 2011, 2012 and\n    \n    T5\n    usain bolt wins his third gold medal of the world championships in the men's 4x100m relay .\n    the 26-year-old anchored Jamaica to victory in the event in the Russian capital .\n    he has now collected eight gold medals at the championships, equaling the record .\n    \n    BART\n    Usain Bolt wins his third gold of the world championships in Moscow.\n    Bolt anchors Jamaica to victory in the men's 4x100m relay.\n    The 26-year-old has now won eight gold medals at world championships.\n    Jamaica's women also win gold in the relay, beating France in the process.\n    \n    PEGASUS\n    Usain Bolt wins third gold of world championships.\n    Anchors Jamaica to victory in men's 4x100m relay.\n    Eighth gold at the championships for Bolt.\n    Jamaica also win women's 4x100m relay .\nNote: * The summary generated by GPT-2 is quite different than the others. * GPT-2 summarizes the characters instead of the text. * The GPT-2 model often invents facts since it did not train to generate truthful summaries. * There are significant similarities between the summaries generated by the T5, BART, and PEGASUS models. * PEGASUS gets incredibly close to the ground truth summary."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#measuring-the-quality-of-generated-text",
    "href": "posts/transformers-book-notes/chapter-6/index.html#measuring-the-quality-of-generated-text",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Measuring the Quality of Generated Text",
    "text": "Measuring the Quality of Generated Text\n\nConventional metrics like accuracy do not reflect the quality of the generated text.\nThe field of text-generation is still looking for better evaluation metrics.\nThe two most common metrics used to evaluate generated text are BLEU and ROUGE.\nHuman judgment is still the best measure.\n\n\nBilingual Evaluation Understudy (BLEU)\n\nBLEU: a method for automatic evaluation of machine translation\nBLEU is a precision-based metric where we count the number of words or n-grams in the generated text that occur in the reference text and divide it by the length of the reference.\nWe only count a word as many times as it occurs in the reference text.\nBLEU is a popular metric for tasks like machine translation where precision takes priority.\nGiven one generated sentence, \\(snt\\), that we to compare against a reference sentence, \\(snt^{\\prime}\\), we extract all possible n-grams of degree \\(n\\) and do the accounting to get the precision \\(P_{n}\\).\n\n\n\n\\[P_{n} = \\frac{\\sum_{n-gram \\ \\in \\ snt}{Count_{clip}(n-gram)}}{\\sum_{n-gram \\ \\in \\ snt^{\\prime}}{Count(n-gram)}}\\]\n\nWe clip the occurrence count of an n-gram at how many times it appears in the reference sentence to avoid repetitive generations.\nWe sum over all the examples in the corpus \\(C\\).\n\n\n\n\\[P_{n} = \\frac{\\sum_{snt \\ \\in \\ C}\\sum_{n-gram \\ \\in \\ snt}{Count_{clip}(n-gram)}}{\\sum_{snt^{\\prime} \\ \\in \\ C}\\sum_{n-gram \\ \\in \\ snt^{\\prime}}{Count(n-gram)}}\\]\n\nThe precision score favors short sentences.\nThe authors of BLEU introduce a brevity penalty to account for this.\n\n\n\n\\[BR = min \\left(1,e^{\\frac{1 - \\ell_{ref}}{\\ell_{gen}}} \\right)\\]\n\nBy taking the minimum, we ensure that this penalty never exceeds \\(1\\), and the exponential term becomes exponentially small when the length of the generated text is smaller than the reference text.\nWe don’t use recall because it would incentivize translations that used all the words from all reference texts.\nThe equation for the BLEU score:\n\n\n\n\\[\\text{BLEU-N} = BR \\times \\left( \\prod^{N}_{n=1}{P_{n}} \\right)^{\\frac{1}{N}}\\]\n\nThe last term is the geometric mean of the modified precision up to n-gram \\(N\\).\nThe BLEU score does not account for synonyms and uses fragile heuristics.\nEvaluating Text Output in NLP: BLEU at your own risk\n\nThis post provides an exposition of BLEU’s flaws.\n\nThe default BLEU metric expects tokenized text, leading to varying results for different tokenization methods.\nThe SacreBLEU metric internalizes the tokenization step and is the preferred benchmarking metric.\n\n\nfrom datasets import load_metric\n\n\nload_metric\n\nDocumentation\nLoad a datasets.Metric\n\nLoad the SacreBLEU metric\nbleu_metric = load_metric(\"sacrebleu\")\n\nbleu_metric.codebase_urls\n    ['https://github.com/mjpost/sacreBLEU']\n\nbleu_metric\n    Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n    Produces BLEU scores along with its sufficient statistics\n    from a source against one or more references.\n    \n    Args:\n        predictions: The system stream (a sequence of segments).\n        references: A list of one or more reference streams (each a sequence of segments).\n        smooth_method: The smoothing method to use. (Default: 'exp').\n        smooth_value: The smoothing value. Only valid for 'floor' and 'add-k'. (Defaults: floor: 0.1, add-k: 1).\n        tokenize: Tokenization method to use for BLEU. If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for\n            Japanese and '13a' (mteval) otherwise.\n        lowercase: Lowercase the data. If True, enables case-insensitivity. (Default: False).\n        force: Insist that your tokenized input is actually detokenized.\n    \n    Returns:\n        'score': BLEU score,\n        'counts': Counts,\n        'totals': Totals,\n        'precisions': Precisions,\n        'bp': Brevity penalty,\n        'sys_len': predictions length,\n        'ref_len': reference length,\n    \n    Examples:\n    \n        &gt;&gt;&gt; predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n        &gt;&gt;&gt; references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n        &gt;&gt;&gt; sacrebleu = datasets.load_metric(\"sacrebleu\")\n        &gt;&gt;&gt; results = sacrebleu.compute(predictions=predictions, references=references)\n        &gt;&gt;&gt; print(list(results.keys()))\n        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n        &gt;&gt;&gt; print(round(results[\"score\"], 1))\n        100.0\n    \"\"\", stored examples: 0)\nNote:\n\nThe bleu_metric object is an instance of the Metric class and works as an aggregator.\nAdd single instances with the add() method or whole batches via add_batch().\nCall the compute() method to calculate the metric.\n\n\nimport pandas as pd\nimport numpy as np\n\nAdd a single prediction and refernce to the metric’s stack\nbleu_metric.add(prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n\nCompute the metrics with very differnt sentences\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\n\n\n\n\n\n\n\n\nValue\n\n\n\n\n\n\nscore\n\n\n0.0\n\n\n\n\ncounts\n\n\n[2, 0, 0, 0]\n\n\n\n\ntotals\n\n\n[6, 5, 4, 3]\n\n\n\n\nprecisions\n\n\n[33.33, 0.0, 0.0, 0.0]\n\n\n\n\nbp\n\n\n1.0\n\n\n\n\nsys_len\n\n\n6\n\n\n\n\nref_len\n\n\n6\n\n\n\n\n\n\nNote:\n\nThe precision of the 1-gram is 2/6 since the word “the” appears twice in the reference text.\nThe BLEU score also works if there are multiple reference translations.\nBLEU integrates methods to modify the precision calculation to make the metric smoother for zero counts in the n-grams.\n\n\nCompute the metrics with very similar sentences\nbleu_metric.add(prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\n\n\n\n\n\n\n\n\nValue\n\n\n\n\n\n\nscore\n\n\n57.893007\n\n\n\n\ncounts\n\n\n[5, 3, 2, 1]\n\n\n\n\ntotals\n\n\n[5, 4, 3, 2]\n\n\n\n\nprecisions\n\n\n[100.0, 75.0, 66.67, 50.0]\n\n\n\n\nbp\n\n\n0.818731\n\n\n\n\nsys_len\n\n\n5\n\n\n\n\nref_len\n\n\n6\n\n\n\n\n\n\nNote:\n\nThe precision scores are much better.\nThe 1-grams in the prediction all match.\n\n\n\n\n\nROUGE\n\nROUGE: A Package for Automatic Evaluation of Summaries\nThe ROUGE score targets applications like summarization, where high recall is more important than precision alone.\nWe check how many n-grams in the reference text also occur in the generated text.\n\n\n\n\\[\\text{ROUGE-N} = \\frac{\\sum_{snt^{\\prime} \\ \\in \\ C}\\sum_{n-gram \\ \\in \\ snt^{\\prime}}{Count_{match}(n-gram)}}{\\sum_{snt^{\\prime} \\ \\in \\ C}\\sum_{n-gram \\ \\in \\ snt^{\\prime}}{Count(n-gram)}}\\]\n\nThere is a separate score to measure the longest common substring (LCS) called ROUGE-L.\nWe can calculate the LCS for any pair of strings.\nWe need to normalize the LCS value when comparing two samples of different lengths.\n\n\n\n\\[F_{LCS} = \\frac{\\left( 1 + \\beta^{2} \\right)R_{LCS}P_{LCS}}{R_{LCS} + \\beta P_{LCS}} \\text{, where } \\beta = \\frac{P_{LCS}}{R_{LCS}}\\]\n\nThe Hugging Face Datasets implementation calculates two variants of ROUGE.\nROUGE-L calculates the score per sentence and averages it for the summaries.\nROUGE-Lsum calculates the score per sentence directly over the whole summary.\n\n\nrouge_metric = load_metric(\"rouge\")\n\nrouge_metric.codebase_urls\n    ['https://github.com/google-research/google-research/tree/master/rouge']\n\nrouge_metric\n    Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n    Calculates average rouge scores for a list of hypotheses and references\n    Args:\n        predictions: list of predictions to score. Each predictions\n            should be a string with tokens separated by spaces.\n        references: list of reference for each prediction. Each\n            reference should be a string with tokens separated by spaces.\n        rouge_types: A list of rouge types to calculate.\n            Valid names:\n            `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n            `\"rougeL\"`: Longest common subsequence based scoring.\n            `\"rougeLSum\"`: rougeLsum splits text using `\"\n    \"`.\n            See details in https://github.com/huggingface/datasets/issues/617\n        use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n        use_agregator: Return aggregates if this is set to True\n    Returns:\n        rouge1: rouge_1 (precision, recall, f1),\n        rouge2: rouge_2 (precision, recall, f1),\n        rougeL: rouge_l (precision, recall, f1),\n        rougeLsum: rouge_lsum (precision, recall, f1)\n    Examples:\n    \n        &gt;&gt;&gt; rouge = datasets.load_metric('rouge')\n        &gt;&gt;&gt; predictions = [\"hello there\", \"general kenobi\"]\n        &gt;&gt;&gt; references = [\"hello there\", \"general kenobi\"]\n        &gt;&gt;&gt; results = rouge.compute(predictions=predictions, references=references)\n        &gt;&gt;&gt; print(list(results.keys()))\n        ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n        &gt;&gt;&gt; print(results[\"rouge1\"])\n        AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n        &gt;&gt;&gt; print(results[\"rouge1\"].mid.fmeasure)\n        1.0\n    \"\"\", stored examples: 0)\n\nApply the ROUGE score to the generated summaries\nreference = dataset[\"train\"][1][\"highlights\"]\nrecords = []\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\nfor model_name in summaries:\n    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n    score = rouge_metric.compute()\n    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n    records.append(rouge_dict)\npd.DataFrame.from_records(records, index=summaries.keys())\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\nbaseline\n\n\n0.303571\n\n\n0.090909\n\n\n0.214286\n\n\n0.232143\n\n\n\n\ngpt2\n\n\n0.276596\n\n\n0.065217\n\n\n0.170213\n\n\n0.276596\n\n\n\n\nt5\n\n\n0.486486\n\n\n0.222222\n\n\n0.378378\n\n\n0.486486\n\n\n\n\nbart\n\n\n0.582278\n\n\n0.207792\n\n\n0.455696\n\n\n0.506329\n\n\n\n\npegasus\n\n\n0.866667\n\n\n0.655172\n\n\n0.800000\n\n\n0.833333\n\n\n\n\n\n\nNote:\n\nThe ROUGE metric in the Hugging Face Datasets library also calculates confidence intervals.\n\nWe can access the average value in the mid attribute and the interval with the low and high.\n\nGPT-2 is the only model not explicitly trained to summarize, and it performs the worst.\nThe first-three-sentence baseline performs better than the GPT-2 model.\nPEGASUS performs the best by a wide margin."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#evaluating-pegasus-on-the-cnndailymail-dataset",
    "href": "posts/transformers-book-notes/chapter-6/index.html#evaluating-pegasus-on-the-cnndailymail-dataset",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Evaluating PEGASUS on the CNN/DailyMail Dataset",
    "text": "Evaluating PEGASUS on the CNN/DailyMail Dataset\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\ndataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\nrouge_metric = load_metric(\"rouge\", cache_dir=None)\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\nDefine a function to evaluate the three-sentence baseline\ndef evaluate_summaries_baseline(dataset, metric,\n                                column_text=\"article\", \n                                column_summary=\"highlights\"):\n    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n    metric.add_batch(predictions=summaries,\n                     references=dataset[column_summary])    \n    score = metric.compute()\n    return score\n\nEvaluate the baseline summary on the test set\n# Only use 1,000 samples\ntest_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n\nscore = evaluate_summaries_baseline(test_sampled, rouge_metric)\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\npd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\nbaseline\n\n\n0.388071\n\n\n0.170554\n\n\n0.247146\n\n\n0.354972\n\n\n\n\n\n\n\nfrom tqdm import tqdm\nimport torch\n\nSet compute device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nDefine a function to split the dataset into smaller batches\ndef chunks(list_of_elements, batch_size):\n    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\nDefine a function to evalue the PEGASUS model\ndef evaluate_summaries_pegasus(dataset, metric, model, tokenizer, \n                               batch_size=16, device=device, \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n    article_batches = list(chunks(dataset[column_text], batch_size))\n    target_batches = list(chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                        padding=\"max_length\", return_tensors=\"pt\")\n        \n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                         attention_mask=inputs[\"attention_mask\"].to(device), \n                         length_penalty=0.8, num_beams=8, max_length=128)\n        \n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                clean_up_tokenization_spaces=True) \n               for s in summaries]\n        decoded_summaries = [d.replace(\"&lt;n&gt;\", \" \") for d in decoded_summaries]\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n        \n    score = metric.compute()\n    return score\n\nFree unoccupied cached memory\npipe = None\ntorch.cuda.empty_cache()\n\nLoad a pretrained PEGASUS model for sequence-to-sequence tasks\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\nEvaluate the PEGASUS model on the test set\nscore = evaluate_summaries_pegasus(test_sampled, rouge_metric, \n                                   model, tokenizer, batch_size=8)\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\npd.DataFrame(rouge_dict, index=[\"pegasus\"])\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\npegasus\n\n\n0.427195\n\n\n0.207378\n\n\n0.305054\n\n\n0.36919\n\n\n\n\n\n\nNote:\n\nThe loss is independent of the decoding strategy.\nThe ROUGE score is highly dependent on the decoding strategy.\nROUGE and BLEU correlate better with human judgment than loss or accuracy."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#training-a-summarization-model",
    "href": "posts/transformers-book-notes/chapter-6/index.html#training-a-summarization-model",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Training a Summarization Model",
    "text": "Training a Summarization Model\n\nThe SAMSum Dataset\n\nHugging Face Dataset Card\nThe SAMSum dataset contains about 16k messenger-like conversations with summaries.\nThese dialogues could represent the interactions between a customer and the support center in an enterprise setting.\n\nGenerating accurate summaries can help improve customer service and detect common patterns among customer requests.\n\n\nLoad the SAMSum dataset\ndataset_samsum = load_dataset(\"samsum\")\nsplit_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n\nView sample from dataset\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(\"\\nDialogue:\")\nprint(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(\"\\nSummary:\")\nprint(dataset_samsum[\"test\"][0][\"summary\"])\n    Split lengths: [14732, 819, 818]\n    Features: ['id', 'dialogue', 'summary']\n    \n    Dialogue:\n    Hannah: Hey, do you have Betty's number?\n    Amanda: Lemme check\n    Hannah: &lt;file_gif&gt;\n    Amanda: Sorry, can't find it.\n    Amanda: Ask Larry\n    Amanda: He called her last time we were at the park together\n    Hannah: I don't know him well\n    Hannah: &lt;file_gif&gt;\n    Amanda: Don't be shy, he's very nice\n    Hannah: If you say so..\n    Hannah: I'd rather you texted him\n    Amanda: Just text him 🙂\n    Hannah: Urgh.. Alright\n    Hannah: Bye\n    Amanda: Bye bye\n    \n    Summary:\n    Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\nNote: The dialogues resemble a typical chat application, including emojis and placeholders for GIFs.\n\n\n\nEvaluating PEGASUS on SAMSum\nFree unoccupied cached memory\nmodel = None\ntorch.cuda.empty_cache()\n\nReset random seed\nset_seed(42)\n\nCreate a text generation pipeline with the PEGASUS model\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", device=device_num)\n\npipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(\"Summary:\")\nprint(pipe_out[0][\"summary_text\"].replace(\" .&lt;n&gt;\", \".\\n\"))\n    Summary:\n    Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n    Hannah: I'd rather you texted him.\n    Amanda: Just text him .\nNote:\n\nThe model tries to summarize by extracting the key sentences from the dialogue.\nThe summaries in SAMSum are more abstract.\n\n\nReset random seed\nset_seed(42)\n\nFree unoccupied cached memory\npipe = None\ntorch.cuda.empty_cache()\n\nLoad a pretrained PEGASUS model for sequence-to-sequence tasks\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\nRun the full ROUGE evaluation on the test set\nscore = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n                                   tokenizer, column_text=\"dialogue\",\n                                   column_summary=\"summary\", batch_size=8)\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\npd.DataFrame(rouge_dict, index=[\"pegasus\"])\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\npegasus\n\n\n0.296091\n\n\n0.087493\n\n\n0.229237\n\n\n0.229642\n\n\n\n\n\n\nNote: The results are not great since the SAMSum dataset is quite different from the CNN/DailyMail dataset.\n\n\n\nFine-Tuning PEGASUS\nExamine the length distribution of the input and output\nd_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\ns_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\naxes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[0].set_title(\"Dialogue Token Length\")\naxes[0].set_xlabel(\"Length\")\naxes[0].set_ylabel(\"Count\")\naxes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[1].set_title(\"Summary Token Length\")\naxes[1].set_xlabel(\"Length\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nNote:\n\nMost dialogues are shorter than the CNN/DailyMail articles, with 100-200 tokens per dialogue.\nThe summaries are also much short, with around 20-40 tokens.\n\n\nDefine a function to tokenize the SAMSum dataset\ndef convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\n                                truncation=True)\n    # Temporarily set the tokenizer for the decoder\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n                                     truncation=True)\n    \n    return {\"input_ids\": input_encodings[\"input_ids\"],\n            \"attention_mask\": input_encodings[\"attention_mask\"],\n            \"labels\": target_encodings[\"input_ids\"]}\nNote: Some models require special tokens in the decoder inputs.\n\nTokenize the dataset\ndataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, \n                                       batched=True)\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n\nCreate a data collator\n\nThe Trainer object calls a data collator before feeding the batch through the model.\nThe data collator for summarization tasks needs to stack the inputs and prepare the targets for the decoder.\nIt is common to apply teacher forcing in the decoder in a sequence-to-sequence setup.\n\nTeacher forcing involves feeding the decoder input tokens that consist of the labels shifted by one in addition to * the encoder output.\nThe decoder gets the ground truth shifted by one as input when making predictions.\nWe shift by one, so the decoder only sees the previous ground truth labels and not the current or future ones.\nThe decoder already has masked self-attention that masks all inputs at present and in the future.\n\n\nVisualize shifting the decoder input by one\ntext = ['PAD','Transformers', 'are', 'awesome', 'for', 'text', 'summarization']\nrows = []\nfor i in range(len(text)-1):\n    rows.append({'step': i+1, 'decoder_input': text[:i+1], 'label': text[i+1]})\npd.DataFrame(rows).set_index('step')\n\n\n\n\n\n\n\n\ndecoder_input\n\n\nlabel\n\n\n\n\nstep\n\n\n\n\n\n\n\n\n\n\n1\n\n\n[PAD]\n\n\nTransformers\n\n\n\n\n2\n\n\n[PAD, Transformers]\n\n\nare\n\n\n\n\n3\n\n\n[PAD, Transformers, are]\n\n\nawesome\n\n\n\n\n4\n\n\n[PAD, Transformers, are, awesome]\n\n\nfor\n\n\n\n\n5\n\n\n[PAD, Transformers, are, awesome, for]\n\n\ntext\n\n\n\n\n6\n\n\n[PAD, Transformers, are, awesome, for, text]\n\n\nsummarization\n\n\n\n\n\n\n\nfrom transformers import DataCollatorForSeq2Seq\n\n\nDataCollatorForSeq2Seq\n\nDocumentation\nCreate a data collator for language modeling.\nThe data collator dynamically pads inputs to the maximum length of a batch.\n\n\nprint_source(DataCollatorForSeq2Seq)\n    @dataclass\n    class DataCollatorForSeq2Seq:\n        tokenizer: PreTrainedTokenizerBase\n        model: Optional[Any] = None\n        padding: Union[bool, str, PaddingStrategy] = True\n        max_length: Optional[int] = None\n        pad_to_multiple_of: Optional[int] = None\n        label_pad_token_id: int = -100\n        return_tensors: str = 'pt'\n    \n        def __call__(self, features, return_tensors=None):\n            if return_tensors is None:\n                return_tensors = self.return_tensors\n            labels = [feature['labels'] for feature in features\n                ] if 'labels' in features[0].keys() else None\n            if labels is not None:\n                max_label_length = max(len(l) for l in labels)\n                padding_side = self.tokenizer.padding_side\n                for feature in features:\n                    remainder = [self.label_pad_token_id] * (max_label_length -\n                        len(feature['labels']))\n                    if isinstance(feature['labels'], list):\n                        feature['labels'] = (feature['labels'] + remainder if \n                            padding_side == 'right' else remainder + feature[\n                            'labels'])\n                    elif padding_side == 'right':\n                        feature['labels'] = np.concatenate([feature['labels'],\n                            remainder]).astype(np.int64)\n                    else:\n                        feature['labels'] = np.concatenate([remainder, feature[\n                            'labels']]).astype(np.int64)\n            features = self.tokenizer.pad(features, padding=self.padding,\n                max_length=self.max_length, pad_to_multiple_of=self.\n                pad_to_multiple_of, return_tensors=return_tensors)\n            if self.model is not None and hasattr(self.model,\n                'prepare_decoder_input_ids_from_labels'):\n                decoder_input_ids = (self.model.\n                    prepare_decoder_input_ids_from_labels(labels=features[\n                    'labels']))\n                features['decoder_input_ids'] = decoder_input_ids\n            return features\n\nInitialize the data collator\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\nfrom transformers import TrainingArguments, Trainer\n\nPrepare training arguments\ntraining_args = TrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10, push_to_hub=True,\n    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16, fp16=True)\n    PyTorch: setting up devices\n    The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n\nDisable Tokenizers Parallelism\n%env TOKENIZERS_PARALLELISM=false\n    env: TOKENIZERS_PARALLELISM=false\nfrom huggingface_hub import notebook_login\n\nLog into Hugging Face account\nnotebook_login()\n    Login successful\n    Your token has been saved to /home/innom-dt/.huggingface/token\n\nFree unoccupied cached memory\npipe = None\ntorch.cuda.empty_cache()\n\nInitialize the Trainer object\ntrainer = Trainer(model=model, args=training_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\n\nNote: Had to add the following workaround.\nold_collator = trainer.data_collator\ntrainer.data_collator = lambda data: dict(old_collator(data))\n\nRun the training loop and evaluate the trained model\ntrainer.train()\nscore = evaluate_summaries_pegasus(\n    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n    The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: summary, dialogue, id.\n    ***** Running training *****\n      Num examples = 14732\n      Num Epochs = 1\n      Instantaneous batch size per device = 1\n      Total train batch size (w. parallel, distributed & accumulation) = 16\n      Gradient Accumulation steps = 16\n      Total optimization steps = 920\n100%|███████████████████████████████████████████████████████████████████████████████| 410/410 [05:03&lt;00:00,  1.35it/s]\n\npd.DataFrame(rouge_dict, index=[f\"pegasus\"])\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\npegasus\n\n\n0.424058\n\n\n0.191855\n\n\n0.333271\n\n\n0.333591\n\n\n\n\n\n\nNote: The ROUGE scores are significantly better than the model without fine-tuning.\n\nPush the final model to the Hugging Face Hub\ntrainer.push_to_hub(\"Training complete!\")\n    Saving model checkpoint to pegasus-samsum\n    Configuration saved in pegasus-samsum/config.json\n    Model weights saved in pegasus-samsum/pytorch_model.bin\n    tokenizer config file saved in pegasus-samsum/tokenizer_config.json\n    Special tokens file saved in pegasus-samsum/special_tokens_map.json\n\n\n    'https://huggingface.co/cj-mills/pegasus-samsum/commit/696c3fca143486ed2288d90c06dd93257cb42c0f'\n\n\n\n\nEvaluate Generations as Part of the Training Loop\n\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nseq2seq_training_args = Seq2SeqTrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10, push_to_hub=True,\n    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16, fp16=True, predict_with_generate=True)\n    PyTorch: setting up devices\n    The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n\nseq2seq_trainer = Seq2SeqTrainer(model=model, args=seq2seq_training_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\n    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/pegasus-samsum is already a clone of https://huggingface.co/cj-mills/pegasus-samsum. Make sure you pull the latest changes with `repo.git_pull()`.\n    Using amp fp16 backend\n\nNote: Had to add the following workaround.\nold_collator = seq2seq_trainer.data_collator\nseq2seq_trainer.data_collator = lambda data: dict(old_collator(data))\n\nseq2seq_trainer.train()\n    The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: summary, dialogue, id.\n    ***** Running training *****\n      Num examples = 14732\n      Num Epochs = 1\n      Instantaneous batch size per device = 1\n      Total train batch size (w. parallel, distributed & accumulation) = 16\n      Gradient Accumulation steps = 16\n      Total optimization steps = 920\n\nseq2seq_score = evaluate_summaries_pegasus(\n    dataset_samsum[\"test\"], rouge_metric, seq2seq_trainer.model, tokenizer,\n    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n\nseq2seq_rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n\npd.DataFrame(seq2seq_rouge_dict, index=[f\"pegasus\"])\n\n\n\n\n\n\n\n\nrouge1\n\n\nrouge2\n\n\nrougeL\n\n\nrougeLsum\n\n\n\n\n\n\npegasus\n\n\n0.424058\n\n\n0.191855\n\n\n0.333271\n\n\n0.333591\n\n\n\n\n\n\n\n\n\nGenerating Dialogue Summaries\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n# Set up a pipeline with the uploaded model\npipe = pipeline(\"summarization\", model=\"cj-mills/pegasus-samsum\")\n\nprint(\"Dialogue:\")\nprint(sample_text)\nprint(\"\\nReference Summary:\")\nprint(reference)\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\n    Dialogue:\n    Hannah: Hey, do you have Betty's number?\n    Amanda: Lemme check\n    Hannah: &lt;file_gif&gt;\n    Amanda: Sorry, can't find it.\n    Amanda: Ask Larry\n    Amanda: He called her last time we were at the park together\n    Hannah: I don't know him well\n    Hannah: &lt;file_gif&gt;\n    Amanda: Don't be shy, he's very nice\n    Hannah: If you say so..\n    Hannah: I'd rather you texted him\n    Amanda: Just text him 🙂\n    Hannah: Urgh.. Alright\n    Hannah: Bye\n    Amanda: Bye bye\n    \n    Reference Summary:\n    Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n    \n    Model Summary:\n    Amanda can't find Betty's number. Larry called Betty the last time they were at the park together. Hannah wants Amanda to text him instead.\nNote: The model learned to synthesize the dialogue into a summary without extracting existing passages.\n\nTest the fine-tuned model on custom input\ncustom_dialogue = \"\"\"\\\nThom: Hi guys, have you heard of transformers?\nLewis: Yes, I used them recently!\nLeandro: Indeed, there is a great library by Hugging Face.\nThom: I know, I helped build it ;)\nLewis: Cool, maybe we should write a book about it. What do you think?\nLeandro: Great idea, how hard can it be?!\nThom: I am in!\nLewis: Awesome, let's do it together!\n\"\"\"\nprint(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])\n    Thom, Lewis and Leandro are going to write a book about transformers. Thom helped build a library by Hugging Face. They are going to do it together.\nNote: The model generated a coherent summary and synthesized the third and fourth lines into a logical combination."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#conclusion",
    "href": "posts/transformers-book-notes/chapter-6/index.html#conclusion",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "Conclusion",
    "text": "Conclusion\n\nIt is still an open question regarding the best way to summarize texts longer than the model’s content size.\nRecursively Summarizing Books with Human Feedback\n\nOpenAI scaled summarization by applying the model recursively to long documents and using human feedback."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-6/index.html#references",
    "href": "posts/transformers-book-notes/chapter-6/index.html#references",
    "title": "Notes on Transformers Book Ch. 6",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 5\nNext: Notes on Transformers Book Ch. 7"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html",
    "href": "posts/transformers-book-notes/chapter-7/index.html",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "",
    "text": "Introduction\nProject: Build a Review-Based QA System\nImproving Our QA Pipeline\nGoing Beyond Extractive QA\nConclusion\nReferences\nDisable Tokenizers Parallelism\nSuppress Haystack logging"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#introduction",
    "href": "posts/transformers-book-notes/chapter-7/index.html#introduction",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "Introduction",
    "text": "Introduction\n\nExtractive question answering is the most common form and requires the answer to be available as a span of text in a document.\nCommunity question answering involves gathering question-answer pairs generated by users. We then use semantic similarity search to find the closest matching answer to a new question.\nLong-form question answering aims to generate complex paragraph-length answers to open-ended questions.\nIt is also possible to perform question answering over tables.\n\nTransformer models like TAPAS can perform aggregations to produce the final answer.\n\nThe domain of data a QA system has access to usually determines how it is categorized.\n\nClosed-domain QA deals with questions about a narrow topic like a single product category.\nOpen-domain QA deals with questions about almost anything."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#project-build-a-review-based-qa-system",
    "href": "posts/transformers-book-notes/chapter-7/index.html#project-build-a-review-based-qa-system",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "Project: Build a Review-Based QA System",
    "text": "Project: Build a Review-Based QA System\n\nThe goal is to build a question-answering model that finds answers to questions in customer reviews.\n\n\nThe Dataset\n\nSubjQA: A Dataset for Subjectivity and Review Comprehension\nGitHub Repository\nHugging Face Dataset Card\nSubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers.\nThe dataset contains approximately 10,000 questions over reviews from six domains, including books, movies, grocery, electronics, TripAdvisor, and restaurants.\nEach question is associated with a review that contains the answer.\nThe dataset includes unanswerable questions designed to produce more robust models.\nThe subjectivity of the questions and answers makes the task potentially more challenging than if they were factual.\n\n\nfrom datasets import get_dataset_config_names\n\nGet the names of the domains in the SubjQA dataset\ndomains = get_dataset_config_names(\"subjqa\")\ndomains\n    ['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']\n\nfrom datasets import load_dataset\n\nLoad the electronics subset of the SubjQA dataset\nsubjqa = load_dataset(\"subjqa\", name=\"electronics\")\nsubjqa\n    DatasetDict({\n        train: Dataset({\n            features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n            num_rows: 1295\n        })\n        test: Dataset({\n            features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n            num_rows: 358\n        })\n        validation: Dataset({\n            features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n            num_rows: 255\n        })\n    })\nNote: SubjQA stores the answers to each question as a nested dictionary.\n\nInspect an answer from the dataset\npd.DataFrame(subjqa[\"train\"][\"answers\"][1])\n\n\n\n\n\n\n\n\ntext\n\n\nanswer_start\n\n\nanswer_subj_level\n\n\nans_subj_score\n\n\nis_ans_subjective\n\n\n\n\n\n\n0\n\n\nBass is weak as expected\n\n\n1302\n\n\n1\n\n\n0.508333\n\n\nTrue\n\n\n\n\n1\n\n\nBass is weak as expected, even with EQ adjusted up\n\n\n1302\n\n\n1\n\n\n0.508333\n\n\nTrue\n\n\n\n\n\n\n\nConvert each split to a Pandas DataFrame\ndfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n\nfor split, df in dfs.items():\n    print(f\"Number of questions in {split}: {df['id'].nunique()}\")\n    Number of questions in train: 1295\n    Number of questions in test: 358\n    Number of questions in validation: 255\nNote: * The dataset is relatively small, with 1,908 total examples. * Real-world QA datasets are likely to be small as it is expensive and labor-intensive to have domain experts label extractive QA datasets. * The CUAD dataset for extractive QA on legal contracts has an estimated value of two million dollars to account for the legal expertise needed to annotate its 13,000 examples.\n\n\nColumns of Interest\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ntitle\nThe Amazon Standard Identification Number (ASIN) associated with each product\n\n\nquestion\nThe question\n\n\nanswers.answer_text\nThe span of text in the reivew labeled by the annotator\n\n\nanswers.answer_start\nThe start character index of the answer span\n\n\ncontext\nThe customer review\n\n\n\n\nInspect some random training examples\nqa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\nsample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\nsample_df\n\n\n\n\n\n\n\n\ntitle\n\n\nquestion\n\n\nanswers.text\n\n\nanswers.answer_start\n\n\ncontext\n\n\n\n\n\n\n791\n\n\nB005DKZTMG\n\n\nDoes the keyboard lightweight?\n\n\n[this keyboard is compact]\n\n\n[215]\n\n\nI really like this keyboard. I give it 4 stars because it doesn’t have a CAPS LOCK key so I never know if my caps are on. But for the price, it really suffices as a wireless keyboard. I have very large hands and this keyboard is compact, but I have no complaints.\n\n\n\n\n1159\n\n\nB00AAIPT76\n\n\nHow is the battery?\n\n\n[]\n\n\n[]\n\n\nI bought this after the first spare gopro battery I bought wouldn’t hold a charge. I have very realistic expectations of this sort of product, I am skeptical of amazing stories of charge time and battery life but I do expect the batteries to hold a charge for a couple of weeks at least and for the charger to work like a charger. In this I was not disappointed. I am a river rafter and found that the gopro burns through power in a hurry so this purchase solved that issue. the batteries held a charge, on shorter trips the extra two batteries were enough and on longer trips I could use my friends JOOS Orange to recharge them.I just bought a newtrent xtreme powerpak and expect to be able to charge these with that so I will not run out of power again.\n\n\n\n\n\n\nNote:\n\nThe questions are not grammatically correct.\nAn empty text entry denotes an “unanswerable” question whose answer is not present in the review.\nWe can use the start index and length of the answer span to slice out the span of text in the review that corresponds to the answer.\n\n\nSlice out a span of text that corresponds to the answer\nstart_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\nend_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\nsample_df[\"context\"].iloc[0][start_idx:end_idx]\n    'this keyboard is compact'\n\nimport matplotlib.pyplot as plt\n\nCount the questions that begin with common starting words\ncounts = {}\nquestion_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n\nfor q in question_types:\n    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n\npd.Series(counts).sort_values().plot.barh()\nplt.title(\"Frequency of Question Types\")\nplt.show()\n\n\n\n\n\nNote: Questions that begin with “How,” “What,” and “Is” are the most common.\n\nLook at examples with the most common starting words\nfor question_type in [\"How\", \"What\", \"Is\"]:\n    for question in (\n        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n        .sample(n=3, random_state=42)['question']):\n        print(question)\n    How is the camera?\n    How do you like the control?\n    How fast is the charger?\n    What is direction?\n    What is the quality of the construction of the bag?\n    What is your impression of the product?\n    Is this how zoom works?\n    Is sound clear?\n    Is it a wireless keyboard?\n\n\n\n\nThe Stanford Question Answering Dataset (SQuAD)\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nKnow What You Don’t Know: Unanswerable Questions for SQuAD\nThe Standford Question Answering Dataset is a famous dataset for testing the ability of machines to read a passage of text and answer questions about it.\nThe dataset pioneered the (question, review, [answer sentences]) format used in SubjQA.\nThe creators sampled several hundred English articles from Wikipedia and asked crowd workers to generate a set of questions and answers for each paragraph.\nMost models since 2019 surpass human performance on this dataset.\n\nThis superhuman performance does not reflect genuine reading comprehension since models can identify answers to the “unanswerable” questions through patterns in the passages like antonyms.\n\nGoogle released the Natural Questions (NQ) dataset, which involves fact-seeking questions obtained from Google Search users.\n\nThe answers in NQ are much longer than in SQuAD and present a more challenging benchmark.\n\n\n\n\nExtracting Answers from Text\n\nWe need a way to identify a potential answer as a span of text in a customer review.\n\nFrame the supervised learning problems.\nTokenize and encode text for QA tasks.\nDeal with long passages that exceed a model’s maximum context size.\n\n\n\nSpan classification\n\nThe model needs to predict the start and end tokens of an answer.\nThis approach is the most common way to extract answers from a text.\n\n\n\nTransfer Learning\n\nWe can compensate for a relatively small dataset by starting with a language model fine-tuned on a large-scale QA dataset like SQuAD.\nThese models typically have strong reading comprehension capabilities and serve as a good baseline.\nHugging Face Hub models trained on SQuAD\n\nBaseline transformer models fine-tuned on SQuAD 2.0\n\n\n\nTransformer\nDescription\nNumbers of parameters\n\\(F_{1}\\)-score on SQUAD 2.0\n\n\n\n\nMiniLM\nA distilled version of BERT-base that preserves 99% of the performance while being twice as fast.\n66M\n79.5\n\n\nRoBERTa-base\nRoBERTa models have better performance than their BERT counterparts and can fine-tune on most QA datasets using a single GPU.\n125M\n83.0\n\n\nALBERT-XXL\nState-of-the-art performance on SQuAD 2.0, but computationally intensive and difficult to deploy.\n235M\n88.1\n\n\nXLM-RoBERTa-large\nMultilingual model for 100 languages with strong zero-shot performance.\n570M\n83.8\n\n\n\n\n\nTokenizing text for QA\n\nfrom transformers import AutoTokenizer\n\nInstantiate a tokenizer for the MiniLM model\n\nMiniLM Hugging Face Model Card\nMiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\n\n\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nTokenize a question-context pair\nquestion = \"How much music can this hold?\"\ncontext = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\nfile size.\"\"\"\ninputs = tokenizer(question, context, return_tensors=\"pt\")\n\nInspect tokenized inputs\ninput_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\ninput_df\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n\n\n\n\ninput_ids\n\n\n101\n\n\n2129\n\n\n2172\n\n\n2189\n\n\n2064\n\n\n2023\n\n\n2907\n\n\n1029\n\n\n102\n\n\n2019\n\n\n23378\n\n\n2003\n\n\n2055\n\n\n1015\n\n\n16914\n\n\n1013\n\n\n3371\n\n\n1010\n\n\n2061\n\n\n2055\n\n\n25961\n\n\n2847\n\n\n5834\n\n\n2006\n\n\n5371\n\n\n2946\n\n\n1012\n\n\n102\n\n\n\n\ntoken_type_ids\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nattention_mask\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\nNote:\n\nThe token_type_ids tensor indicates which part of the inputs corresponds to the question and context.\n\nA 0 indicates a question token.\nA 1 indicates a context token.\n\nThe token_type_ids are not present in all transformer models.\nBERT-like models use token_type_ids during pretraining to incorporate the next sentence prediction task.\n\n\nDecode the input_ids tensor\nprint(tokenizer.decode(inputs[\"input_ids\"][0]))\n    [CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so about 6000 hours depending on file size. [SEP]\nNote: The token_type_ids tensor determines the location of the first [SEP] token.\n\nimport torch\nfrom transformers import AutoModelForQuestionAnswering\n\n\n\nAutoModelForQuestionAnswering\n\nDocumentation\n\n\nInstantiate the MiniLM model with a QA head\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\ntype(model)\n    transformers.models.bert.modeling_bert.BertForQuestionAnswering\n\n\n\nBertForQuestionAnswering\n\nDocumentation\nCreate a BERT Model with a span classification head on top for extractive question-answering tasks.\n\n\n[child[0] for child in list(model.named_children())]\n    ['bert', 'qa_outputs']\n\nlist(model.named_children())[1]\n    ('qa_outputs', Linear(in_features=384, out_features=2, bias=True))\n\nNote:\n\nWe treat QA as a form of token classification.\nThe QA head takes the hidden states from the encoder and computes the logits for the start and end spans.\n\n\nRun inputs through the forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)\n    QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n             -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n             -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n             -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n             -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n              4.8934,  0.3047, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n             -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None, attentions=None)\n\ntype(outputs)\n    transformers.modeling_outputs.QuestionAnsweringModelOutput\n\n\n\nQuestionAnsweringModelOutput\n\nDocumentation\n\n\nprint_source(transformers.modeling_outputs.QuestionAnsweringModelOutput)\n    @dataclass\n    class QuestionAnsweringModelOutput(ModelOutput):\n        loss: Optional[torch.FloatTensor] = None\n        start_logits: torch.FloatTensor = None\n        end_logits: torch.FloatTensor = None\n        hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n        attentions: Optional[Tuple[torch.FloatTensor]] = None\n\nGet the logits for the start and end tokens\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\ntorch.argmax(start_logits), start_logits\n    (tensor(20),\n     tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n              -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n              -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n              -2.3107, -3.5110, -3.5713, -0.9862]]))\n\ntorch.argmax(end_logits), end_logits\n    (tensor(21),\n     tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n              -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n               4.8934,  0.3047, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n              -3.2531, -0.0914,  1.6211, -0.9623]]))\nNote: The larger, positive values correspond to more likely candidates for the start and end tokens.\n\nCompare the logit shapes to the input IDs\nprint(f\"Input IDs shape: {inputs.input_ids.size()}\")\nprint(f\"Start logits shape: {start_logits.size()}\")\nprint(f\"End logits shape: {end_logits.size()}\")\n    Input IDs shape: torch.Size([1, 28])\n    Start logits shape: torch.Size([1, 28])\n    End logits shape: torch.Size([1, 28])\nNote: There is a start and end logit associated with each input token.\n\ntokenizer.decode(inputs.input_ids[0][torch.argmax(start_logits):torch.argmax(end_logits)+1])\n    '6000 hours'\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nVisualize the predicted logits for the start and end tokens\n\nQuestion Answering with a Fine-Tuned BERT\n\n\ns_scores = start_logits.detach().numpy().flatten()\ne_scores = end_logits.detach().numpy().flatten()\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\ncolors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\nax1.bar(x=tokens, height=s_scores, color=colors)\nax1.set_ylabel(\"Start Scores\")\ncolors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\nax2.bar(x=tokens, height=e_scores, color=colors)\nax2.set_ylabel(\"End Scores\")\nplt.xticks(rotation=\"vertical\")\nplt.show()\n\n\n\n\n\n\nExtract the answer using the most likely start and end candidates\nstart_idx = torch.argmax(start_logits)  \nend_idx = torch.argmax(end_logits) + 1  \nanswer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\nanswer = tokenizer.decode(answer_span)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\n    Question: How much music can this hold?\n    Answer: 6000 hours\nNote: * Using the indices with the max values of the start and end logits can produce out-of-scope answers by selecting tokens that belong to the question instead of the context. * A question answering pipeline computes the best combination of start and end indices using various constraints such as being in-scope, requiring the start indices to precede the end indices.\n\nfrom transformers import pipeline\n\nInstantiate a question answering pipeline with the MiniML model\npipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\ntype(pipe)\n    transformers.pipelines.question_answering.QuestionAnsweringPipeline\n\n\n\nQuestionAnsweringPipeline\n\nDocumentation\nCreate a question anwering pipeline\n\n\nGet the top 3 most likely answers to the question\npipe(question=question, context=context, topk=3)\n    [{'score': 0.2651616930961609, 'start': 38, 'end': 48, 'answer': '6000 hours'},\n     {'score': 0.2208297997713089,\n      'start': 16,\n      'end': 48,\n      'answer': '1 MB/minute, so about 6000 hours'},\n     {'score': 0.10253523290157318,\n      'start': 16,\n      'end': 27,\n      'answer': '1 MB/minute'}]\nNote: * The score field contains the model’s probability estimate for each answer. * The model assigns a high start and end score to the [CLS] token when there is no answer to a question, and the pipeline maps this output to an empty string.\n\nAsk an unanswerable question\npipe(question=\"Why is there no data?\", context=context, handle_impossible_answer=True)\n    {'score': 0.9068416357040405, 'start': 0, 'end': 0, 'answer': ''}\n\n\n\nDealing with long passages\n\nThe context often contains more tokens than the maximum sequence length of the model.\nA decent portion of the SubjQA training set contains question-context pairs that won’t fit within MiniLM’s context size of 512 tokens.\nTruncating long texts is problematic for QA tasks as the answer could be near the end of the context.\nThe standard way to deal with long texts for QA is to apply a sliding window across the inputs, where each window contains a passage of tokens that fit in the model’s context.\n\nDistribution of tokens for each question-context pair in the SubjQA training set\ndef compute_input_length(row):\n    inputs = tokenizer(row[\"question\"], row[\"context\"])\n    return len(inputs[\"input_ids\"])\n\ndfs[\"train\"][\"n_tokens\"] = dfs[\"train\"].apply(compute_input_length, axis=1)\n\nfig, ax = plt.subplots()\ndfs[\"train\"][\"n_tokens\"].hist(bins=100, grid=False, ec=\"C0\", ax=ax)\nplt.xlabel(\"Number of tokens in question-context pair\")\nax.axvline(x=512, ymin=0, ymax=1, linestyle=\"--\", color=\"C1\", \n           label=\"Maximum sequence length\")\nplt.legend()\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\nEnable the sliding window for the tokenizer\nexample = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\ntokenized_example = tokenizer(example[\"question\"], example[\"context\"], \n                              return_overflowing_tokens=True, max_length=100, \n                              stride=25)\nNote: We now get a list containing input_ids for each window.\n\nGet the number of tokens in each window\nfor idx, window in enumerate(tokenized_example[\"input_ids\"]):\n    print(f\"Window #{idx} has {len(window)} tokens\")\n    Window #0 has 100 tokens\n    Window #1 has 88 tokens\n\nView where the windows overlap\nfor window in tokenized_example[\"input_ids\"]:\n    print(f\"{tokenizer.decode(window)} \\n\")\n    [CLS] how is the bass? [SEP] i have had koss headphones in the past, pro 4aa and\n    qz - 99. the koss portapro is portable and has great bass response. the work\n    great with my android phone and can be \" rolled up \" to be carried in my\n    motorcycle jacket or computer bag without getting crunched. they are very light\n    and don't feel heavy or bear down on your ears even after listening to music\n    with them on all day. the sound is [SEP]\n    \n    [CLS] how is the bass? [SEP] and don't feel heavy or bear down on your ears even\n    after listening to music with them on all day. the sound is night and day better\n    than any ear - bud could be and are almost as good as the pro 4aa. they are \"\n    open air \" headphones so you cannot match the bass to the sealed types, but it\n    comes close. for $ 32, you cannot go wrong. [SEP]\n\n\n\n\nUsing Haystack to Build a QA Pipeline\n\nReal-world users will only provide a question about a product rather than a question-context pair.\nWe need a way of selecting relevant passages from among all the reviews in our corpus.\nWe could concatenate all the reviews of a given product and feed them to the model as a single context.\nThis approach can introduce unacceptable latency for our user’s queries when the context gets too long.\nModern QA systems typically use the retriever-reader architecture.\n\n\nRetriever-Reader Architecture\n\nThe retriever is responsible for retrieving relevant documents for a given query.\nRetrievers can be either sparse or dense.\n\nSparse retrievers use word frequencies to represent each document and query as a vector where most elements are zero.\n\nThe retriever computes the inner dot product of a query vector and a document vector to determine their relevance.\n\nDense retrievers use encoders to represent the query and document as contextualized embeddings.\n\nThese embeddings encode semantic meaning and allow dense retrievers to improve search accuracy by understanding the content of the query.\n\n\nThe reader is responsible for extracting an answer from the documents provided by the retriever.\nThe reader is usually a reading comprehension model.\nThere can also be other components that apply postprocessing to the documents fetched by the retriever or to the answers extracted by the reader.\nThe retrieved documents may need reranking to eliminate noisy or irrelevant ones that might confuse the reader.\nThe reader’s answers might require postprocessing when the correct answer comes from various passages in a lengthy document.\n\n\n\nHaystack\n\nHomepage\nHaystack is an open-source framework for building search systems that work over massive document collectors.\nHaystack integrates tightly with Hugging Face Transformers.\nHaystack builds on the retriever-reader architecture by adding a document store and a pipeline component.\nThe document store component is a document-oriented database that stores the documents and metadata provided to the retriever at query time.\nThe pipeline combines all the components of a QA system to enable custom query flows, merging documents from multiple retrievers, etc.\n\n\n\nInitializing a document store\n\nHaystack provides various document stores to choose from, and we can pair each one with a dedicated set of retrievers.\n\nCompatibility of Haystack retreivers and document stores\n\n\n\n\nIn memory\nElasticsearch\nFAISS\nMilvus\n\n\n\n\nTF-IDF\nYes\nYes\nNo\nNo\n\n\nBM25\nNo\nYes\nNo\nNo\n\n\nEmbedding\nYes\nYes\nYes\nYes\n\n\nDPR\nYes\nYes\nYes\nYes\n\n\n\n\n\n\nElasticsearch\n\nInstallation Guide\nElasticsearch is a search engine that can handle textual, numerical, geospatial, structured, and unstructured data.\nElasticsearch can store huge volumes of data and quickly filter it with full-text search features.\nIt is the industry standard for infrastructure analytics.\n\nDownload and unpack Elasticsearch\nurl = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\nelasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n!wget -nc -q {url}\n!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\n\n\n\nPopen\n\nDocumentation\nExecute a child program in a new process.\n\n\n\nPipe\n\nDocumentation\n\n\n\nSTDOUT\n\nDocumentation\n\nStart the Elasticsearch server\nes_dir = \"elasticsearch-7.9.2\"\n!echo `pwd`/$es_dir\n\n    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/elasticsearch-7.9.2\n# Run Elasticsearch as a background process\n!pkexec chown -R daemon:daemon `pwd`/$es_dir\n\nes_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n                  stdout=PIPE, stderr=STDOUT)\n# Wait until Elasticsearch has started\n!sleep 30\n\n!lsof -i :9200\n    COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n    python   5323 innom-dt   67u  IPv4 146772      0t0  TCP localhost:41918-&gt;localhost:9200 (ESTABLISHED)\n    java    11790 innom-dt  243u  IPv6 147757      0t0  TCP localhost:9200-&gt;localhost:41918 (ESTABLISHED)\n    java    11790 innom-dt  295u  IPv6 126562      0t0  TCP ip6-localhost:9200 (LISTEN)\n    java    11790 innom-dt  296u  IPv6 126563      0t0  TCP localhost:9200 (LISTEN)\n\n# Alternative if Docker is installed\n# from haystack.utils import launch_es\n\n# launch_es()\n\nTest the connection to the local Elasticsearch\n!curl -X GET \"localhost:9200/?pretty\"\n    {\n      \"name\" : \"innomdt\",\n      \"cluster_name\" : \"elasticsearch\",\n      \"cluster_uuid\" : \"qci_PXdYR4uva2XnE7o-4Q\",\n      \"version\" : {\n        \"number\" : \"7.9.2\",\n        \"build_flavor\" : \"default\",\n        \"build_type\" : \"tar\",\n        \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n        \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n        \"build_snapshot\" : false,\n        \"lucene_version\" : \"8.6.2\",\n        \"minimum_wire_compatibility_version\" : \"6.8.0\",\n        \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n      },\n      \"tagline\" : \"You Know, for Search\"\n    }\n\nfrom haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n\n\n\nElasticsearchDocumentStore\n\nDocumentation\nCreate a DocumentStore using Elasticsearch to store and query the documents for our search.\n\nInstantiate the document store\n# Return the document embedding for later use with dense retriever \ndocument_store = ElasticsearchDocumentStore(return_embedding=True)\nNote: ElasticsearchDocumentStore creates an index called document for storing documents and an index called label for storing the annotated answer spans.\n\nFlush Elasticsearch with each notebook restart\nif len(document_store.get_all_documents()) or len(document_store.get_all_labels()) &gt; 0:\n    document_store.delete_documents(\"document\")\n    document_store.delete_documents(\"label\")\n\nPopulate the document index with the SubjQA reviews\nfor split, df in dfs.items():\n    # Exclude duplicate reviews\n    docs = [{\"text\": row[\"context\"], \n             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"], \n                     \"split\": split}} \n        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n    document_store.write_documents(docs, index=\"document\")\n    \nprint(f\"Loaded {document_store.get_document_count()} documents\")\n    Loaded 1615 documents\n\n\n\nInitializing a retriever\nfrom haystack.retriever.sparse import ElasticsearchRetriever\n\n\n\nElasticsearchRetriever\n\nDocumentation\nThe ElasticsearchRetriever uses the BM25 retriever by default.\n\nInitialize a sparse retriever based on BM25 * BM25 is an improved version of the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm and represents the question and context as sparse vectors. * BM25 improves on TF-IDF by saturating TF (how many times words in the query occur in a document) values after a set number of occurrences of the given term. It also normalizes the document length to favor short documents over long ones. * TF-IDF and BM25 Explanation\n\nes_retriever = ElasticsearchRetriever(document_store=document_store)\n\nExamine a simple query for a single electronics product\n\nFor review-based QA systems, it is crucial to restrict the queries to a single item.\n\nOtherwise, the retriever would source reviews about products unrelated to the user’s query.\n\nWe can decipher ASIN values with online tools like amazon ASIN or by appending the value of item_id to the www.amazon.com/dp/ URL.\n\n\nitem_id = \"B0074BW614\"\nquery = \"Is it good for reading?\"\nretrieved_docs = es_retriever.retrieve(\n    query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})\nNote:\n\nThe top_k argument specifies how many documents to return.\nThe specified filters ensure we only receive documents from the training set about the desired product.\nEach element of retrieved_docs is a Haystack Document object used to represent documents and includes the retriever’s query score and other metadata.\n\n\nExamine one of the retrieved documents\nprint(retrieved_docs[0])\n    {'text': 'This is a gift to myself.  I have been a kindle user for 4 years and this is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my laptop, my phone and my iPod classic.  I love my iPod but watching movies on the plane with it can be challenging because it is so small. Laptops battery life is not as good as the Kindle.  So the Fire combines for me what I needed all three to do. So far so good.', 'score': 6.243799, 'probability': 0.6857824513476455, 'question': None, 'meta': {'item_id': 'B0074BW614', 'question_id': '868e311275e26dbafe5af70774a300f3', 'split': 'train'}, 'embedding': None, 'id': '252e83e25d52df7311d597dc89eef9f6'}\n\nsample_doc = retrieved_docs[0].to_dict()\nsample_doc.update(sample_doc['meta'])\ndel sample_doc['meta']\npd.DataFrame(sample_doc.items()).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\n0\n\n\ntext\n\n\nscore\n\n\nprobability\n\n\nquestion\n\n\nembedding\n\n\nid\n\n\nitem_id\n\n\nquestion_id\n\n\nsplit\n\n\n\n\n1\n\n\nThis is a gift to myself. I have been a kindle user for 4 years and this is my third one. I never thought I would want a fire for I mainly use it for book reading. I decided to try the fire for when I travel I take my laptop, my phone and my iPod classic. I love my iPod but watching movies on the plane with it can be challenging because it is so small. Laptops battery life is not as good as the Kindle. So the Fire combines for me what I needed all three to do. So far so good.\n\n\n6.243799\n\n\n0.685782\n\n\nNone\n\n\nNone\n\n\n252e83e25d52df7311d597dc89eef9f6\n\n\nB0074BW614\n\n\n868e311275e26dbafe5af70774a300f3\n\n\ntrain\n\n\n\n\n\n\nNote:\n\nA larger scorer implies a document is a better match for the query.\nElasticsearch relies on Apache Lucene for indexing and search.\nElasticsearch uses Lucene’s practical scoring function by default.\n\nThe scoring function first filters the candidate documents by applying a Boolean test (does the document match the query?).\nThe scoring function then applies a similarity metric by representing the document and the query as vectors.\n\n\n\n\n\nInitializing a reader\n\nHaystack provides two types of readers to extract answers from a given context.\nThe FARMReader reader builds on Deepset’s FARM framework for fine-tuning and deploying transformers.\nFARMreader is compatible with Hugging Face Transformers and can load models directly from the Hugging Face Hub.\nThe TransformersReader builds on the QA pipeline from Hugging Face Transformers and is only suitable for running inference.\nBoth readers handle a model’s weights the same way but convert predictions to answers differently.\nThe Hugging Face QA pipeline normalizes the starting and ending logits with a softmax in each passage, so it is only meaningful to compare answers scores from the same section.\nFARM does not normalize logits, meaning we can compare inter-passage answers.\nThe TransformersReader sometimes predicts the same answer twice with different scores when the answer lies across two overlapping windows.\nFARM removes the duplicate answers.\n\n\nfrom haystack.reader.farm import FARMReader\n\n\n\nFARMReader\n\nDocumentation\n\nInitialize a FARMReader with the MiniLM model\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\nmax_seq_length, doc_stride = 384, 128\nreader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\n                    max_seq_len=max_seq_length, doc_stride=doc_stride, \n                    return_no_answer=True)\n    04/11/2022 17:30:41 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 17:30:41 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 17:30:41 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 17:30:41 - INFO - farm.utils -   Automatic Mixed Precision: None\n    Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n    04/11/2022 17:30:49 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n    04/11/2022 17:30:49 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 17:30:49 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 17:30:49 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 17:30:49 - INFO - farm.utils -   Automatic Mixed Precision: None\n    04/11/2022 17:30:49 - INFO - farm.infer -   Got ya 15 parallel workers to do inference ...\n    04/11/2022 17:30:49 - INFO - farm.infer -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n    04/11/2022 17:30:49 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\\n    04/11/2022 17:30:49 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\  /'\\\n    04/11/2022 17:30:49 - INFO - farm.infer -                               \nNote: * We can also fine-tune a reading comprehension model directly with Hugging Face Transformers and load it in TransformersReader to run inference. * Tutorial Link\n\nTest the reader\nprint(reader.predict_on_texts(question=question, texts=[context], top_k=1))\n    {'query': 'How much music can this hold?', 'no_ans_gap': 12.648084878921509, 'answers': [{'answer': '6000 hours', 'score': 10.69961929321289, 'probability': 0.3988136053085327, 'context': 'An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.', 'offset_start': 38, 'offset_end': 48, 'offset_start_in_doc': 38, 'offset_end_in_doc': 48, 'document_id': 'e344757014e804eff50faa3ecf1c9c75'}]}\n\nresult = (reader.predict_on_texts(question=question, texts=[context], top_k=1))\nresult.update(result['answers'][0])\ndel result['answers']\npd.DataFrame(result.items())\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\nquery\n\n\nHow much music can this hold?\n\n\n\n\n1\n\n\nno_ans_gap\n\n\n12.648085\n\n\n\n\n2\n\n\nanswer\n\n\n6000 hours\n\n\n\n\n3\n\n\nscore\n\n\n10.699619\n\n\n\n\n4\n\n\nprobability\n\n\n0.398814\n\n\n\n\n5\n\n\ncontext\n\n\nAn MP3 is about 1 MB/minute, so about 6000 hours depending on file size.\n\n\n\n\n6\n\n\noffset_start\n\n\n38\n\n\n\n\n7\n\n\noffset_end\n\n\n48\n\n\n\n\n8\n\n\noffset_start_in_doc\n\n\n38\n\n\n\n\n9\n\n\noffset_end_in_doc\n\n\n48\n\n\n\n\n10\n\n\ndocument_id\n\n\ne344757014e804eff50faa3ecf1c9c75\n\n\n\n\n\n\n\n\n\nPutting it all together\n\nHaystack provides a Pipeline abstraction that allows us to combine retrievers, readers, and other components as a customizable graph.\nThere are predefined pipelines specialized for QA systems.\n\n\nfrom haystack.pipeline import ExtractiveQAPipeline\nNote: The ExtractiveQAPipeline takes a single retriever-reader pair as its arguments.\n\nCreate an extractive QA pipeline\npipe = ExtractiveQAPipeline(reader, es_retriever)\nNote: Each pipeline has a run method that specifies how the query flow will execute.\n\nRun a simple example\nn_answers = 3\npreds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers,\n                 filters={\"item_id\": [item_id], \"split\":[\"train\"]})\n\nprint(f\"Question: {preds['query']} \\n\")\nfor idx in range(n_answers):\n    print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")\n    print(f\"Review snippet: ...{preds['answers'][idx]['context']}...\")\n    print(\"\\n\\n\")\n    Question: Is it good for reading? \n    \n    Answer 1: I mainly use it for book reading\n    Review snippet: ... is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my la...\n\n\n​    \n​    \n    Answer 2: the larger screen compared to the Kindle makes for easier reading\n    Review snippet: ...ght enough that I can hold it to read, but the larger screen compared to the Kindle makes for easier reading. I love the color, something I never thou...\n\n\n​    \n​    \n    Answer 3: it is great for reading books when no light is available\n    Review snippet: ...ecoming addicted to hers! Our son LOVES it and it is great for reading books when no light is available. Amazing sound but I suggest good headphones t...\n\n\n​    \n​    \nNote: The second and third answers are closer to what the question is asking."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#improving-our-qa-pipeline",
    "href": "posts/transformers-book-notes/chapter-7/index.html#improving-our-qa-pipeline",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "Improving Our QA Pipeline",
    "text": "Improving Our QA Pipeline\n\nThe retriever sets an upper bound on the performance of the whole QA system.\n\n\nEvaluating the Retriever\n\nThe recall metric measures the fraction of all relevant documents retrieved and is a prevalent method for evaluating retrievers.\nA document is relevant if it contains the answer.\nWe can compute recall for a given set of questions by counting the number of times an answer appears in the top-k documents returned by the retriever.\nThere are two ways to evaluate retrievers in Haystack.\n\nWe can use the retriever’s built-in eval() method for open and closed domain QA, but not for datasets like SubjQA.\nWe can build a custom Pipeline that combines a retriever with the EvalRetriever class, enabling us to implement custom metrics and query flows.\n\nMean average precision (mAP) rewards retrievers that can place the correct answers higher up in the document ranking and is a complementary metric to recall.\n\n\nfrom haystack.pipeline import Pipeline\nfrom haystack.eval import EvalDocuments\n\nDefine a custom pipeline\n\nEach node in the Pipeline graph represents a class that takes some inputs and produces some outputs via a run() method.\nAn outgoing_edges attribute indicates the number of outputs from the node.\nThe EvalRetreiver class keeps track of which documents have answers that match the ground truth.\n\n\nclass EvalRetrieverPipeline:\n    def __init__(self, retriever):\n        self.retriever = retriever\n        self.eval_retriever = EvalDocuments()\n        pipe = Pipeline()\n        pipe.add_node(component=self.retriever, name=\"ESRetriever\", \n                      inputs=[\"Query\"])\n        pipe.add_node(component=self.eval_retriever, name=\"EvalRetriever\", \n                      inputs=[\"ESRetriever\"])\n        self.pipeline = pipe\n\n\npipe = EvalRetrieverPipeline(es_retriever)\nNote: Each node has a name and a list of inputs.\n\nfrom haystack import Label\nNote: Haystack provides a Label object that represents the answer spans and their metadata in a standardized fashion.\n\nCreate a list of Label objects\nlabels = []\nfor i, row in dfs[\"test\"].iterrows():\n    # Metadata used for filtering in the Retriever\n    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n    # Populate labels for questions with answers\n    if len(row[\"answers.text\"]):\n        for answer in row[\"answers.text\"]:\n            label = Label(\n                question=row[\"question\"], answer=answer, id=i, origin=row[\"id\"],\n                meta=meta, is_correct_answer=True, is_correct_document=True,\n                no_answer=False)\n            labels.append(label)\n    # Populate labels for questions without answers\n    else:\n        label = Label(\n            question=row[\"question\"], answer=\"\", id=i, origin=row[\"id\"],\n            meta=meta, is_correct_answer=True, is_correct_document=True,\n            no_answer=True)  \n        labels.append(label)\n\nprint(labels[0])\n    {'id': 'e28f5e62-85e8-41b2-8a34-fbff63b7a466', 'created_at': None, 'updated_at':\n    None, 'question': 'What is the tonal balance of these headphones?', 'answer': 'I\n    have been a headphone fanatic for thirty years', 'is_correct_answer': True,\n    'is_correct_document': True, 'origin': 'd0781d13200014aa25860e44da9d5ea7',\n    'document_id': None, 'offset_start_in_doc': None, 'no_answer': False,\n    'model_id': None, 'meta': {'item_id': 'B00001WRSJ', 'question_id':\n    'd0781d13200014aa25860e44da9d5ea7'}}\n\npd.DataFrame(labels[0].to_dict().items())\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\nid\n\n\n7c4e950b-2c60-4721-8068-823605241c34\n\n\n\n\n1\n\n\ncreated_at\n\n\nNone\n\n\n\n\n2\n\n\nupdated_at\n\n\nNone\n\n\n\n\n3\n\n\nquestion\n\n\nWhat is the tonal balance of these headphones?\n\n\n\n\n4\n\n\nanswer\n\n\nI have been a headphone fanatic for thirty years\n\n\n\n\n5\n\n\nis_correct_answer\n\n\nTrue\n\n\n\n\n6\n\n\nis_correct_document\n\n\nTrue\n\n\n\n\n7\n\n\norigin\n\n\nd0781d13200014aa25860e44da9d5ea7\n\n\n\n\n8\n\n\ndocument_id\n\n\nNone\n\n\n\n\n9\n\n\noffset_start_in_doc\n\n\nNone\n\n\n\n\n10\n\n\nno_answer\n\n\nFalse\n\n\n\n\n11\n\n\nmodel_id\n\n\nNone\n\n\n\n\n12\n\n\nmeta\n\n\n{‘item_id’: ‘B00001WRSJ’, ‘question_id’: ‘d0781d13200014aa25860e44da9d5ea7’}\n\n\n\n\n\n\nNote:\n\nThe origin field contains the unique question ID, meaning we can filter the document store per question.\nThe item_id meta subfield allows us to filter the labels by product.\n\n\nAdd answers to a dedicated label index\ndocument_store.write_labels(labels, index=\"label\")\nprint(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\nquestion-answer pairs\"\"\")\n    Loaded 358 question-answer pairs\n\nAggregate all question-answer pairs associated with a unique ID\nlabels_agg = document_store.get_all_labels_aggregated(\n    index=\"label\",\n    open_domain=True,\n    aggregate_by_meta=[\"item_id\"]\n)\nprint(len(labels_agg))\n    330\n\nprint(labels_agg[109])\n    {'question': 'How does the fan work?', 'multiple_answers': [\"the fan itself isn't super loud. There is an adjustable dial to change fan speed\", 'the fan is really really good'], 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'f20dae56410f31632d6a9f8f8284657a', 'multiple_document_ids': [None, None], 'multiple_offset_start_in_docs': [None, None], 'no_answer': False, 'model_id': None, 'meta': {'item_id': 'B002MU1ZRS'}}\n\npd.DataFrame(labels_agg[109].to_dict().items())\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\nquestion\n\n\nHow does the fan work?\n\n\n\n\n1\n\n\nmultiple_answers\n\n\n[the fan itself isn’t super loud. There is an adjustable dial to change fan speed, the fan is really really good]\n\n\n\n\n2\n\n\nis_correct_answer\n\n\nTrue\n\n\n\n\n3\n\n\nis_correct_document\n\n\nTrue\n\n\n\n\n4\n\n\norigin\n\n\nf20dae56410f31632d6a9f8f8284657a\n\n\n\n\n5\n\n\nmultiple_document_ids\n\n\n[None, None]\n\n\n\n\n6\n\n\nmultiple_offset_start_in_docs\n\n\n[None, None]\n\n\n\n\n7\n\n\nno_answer\n\n\nFalse\n\n\n\n\n8\n\n\nmodel_id\n\n\nNone\n\n\n\n\n9\n\n\nmeta\n\n\n{‘item_id’: ‘B002MU1ZRS’}\n\n\n\n\n\n\nNote: The multiple_answers field aggregates all the answers associated with a given question.\n\nDefine a function that feeds each question-answer pair associated with each product to the evaluation pipeline and tracks the correct retrievals\ndef run_pipeline(pipeline, top_k_retriever=10, top_k_reader=4):\n    for l in labels_agg:\n        _ = pipeline.pipeline.run(\n            query=l.question,\n            top_k_retriever=top_k_retriever,\n            top_k_reader=top_k_reader,\n            top_k_eval_documents=top_k_retriever,    \n            labels=l,\n            filters={\"item_id\": [l.meta[\"item_id\"]], \"split\": [\"test\"]})\nrun_pipeline(pipe, top_k_retriever=3)\nprint(f\"Recall@3: {pipe.eval_retriever.recall:.2f}\")\n    Recall@3: 0.95\nNote: Increasing the top_k_retriever value improves the recall at the expense of providing more documents to the reader and slowing down the end-to-end pipeline.\n\nCreate a function to evaluate several \\(k\\) values on the test set\ndef evaluate_retriever(retriever, topk_values = [1,3,5,10,20]):\n    topk_results = {}\n\n    for topk in topk_values:\n        # Create Pipeline\n        p = EvalRetrieverPipeline(retriever)\n        # Loop over each question-answers pair in test set\n        run_pipeline(p, top_k_retriever=topk)\n        # Get metrics\n        topk_results[topk] = {\"recall\": p.eval_retriever.recall}\n        \n    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n\n\nes_topk_df = evaluate_retriever(es_retriever)\n\nPlot the results\ndef plot_retriever_eval(dfs, retriever_names):\n    fig, ax = plt.subplots()\n    for df, retriever_name in zip(dfs, retriever_names):\n        df.plot(y=\"recall\", ax=ax, label=retriever_name)\n    plt.xticks(df.index)\n    plt.ylabel(\"Top-k Recall\")\n    plt.xlabel(\"k\")\n    plt.show()\n    \nplot_retriever_eval([es_topk_df], [\"BM25\"])\n\n\n\n\n\nNote:\n\nThere is an inflection point around k=5, and we get almost perfect recall from k=10 onwards.\nWe want to use smaller k values when possible to reduce overall latency.\n\n\n\nDense Passage Retrieval\n\nDense Passage Retrieval for Open-Domain Question Answering\nSparse retrievers might not capture the relevant documents if the query contains terms not found in the review.\nDense Pattern Retrieval (DPR) uses two BERT models as encoders for the question and the passage.\nThese encoders map the input text into a d-dimensional vector representation of the [CLS] token.\nWe then calculate the dot-product similarity for the two vectors.\nThe encoders train on a dataset of relevant and irrelevant passages to learn that relevant question-passage pairs are more similar.\n\n\nfrom haystack.retriever.dense import DensePassageRetriever\n\n\n\nDensePassageRetriever\n\nDocumentation\n\nInitialize a dense retriever using encoders trained on the NQ corpus\ndpr_retriever = DensePassageRetriever(document_store=document_store,\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n    # Don't include the title in the embedding\n    embed_title=False)\nIterate over the indexed documents and apply the encoders to update the embedding representation\ndocument_store.update_embeddings(retriever=dpr_retriever)\n    04/11/2022 20:56:41 - INFO - haystack.document_store.elasticsearch -   Updating embeddings for all 1615 docs ...\n\n\n\n    Updating embeddings:   0%|          | 0/1615 [00:00&lt;?, ? Docs/s]\n\n\n\n    Create embeddings:   0%|          | 0/1616 [00:00&lt;?, ? Docs/s]\n\nEvaluate the dense retriever\ndpr_topk_df = evaluate_retriever(dpr_retriever)\nplot_retriever_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])\n\n\n\n\n\nNote: * The DPR does not provide a boost in recall over BM25 and saturates around k=3. * We can speed up the search of the embeddings by using Facebook’s FAISS library as the document store. * Tutorial Link\n\n\n\n\nEvaluating the Reader\n\nThere are two primary metrics to evaluate readers in extractive QA.\nExact Match (EM) is a binary metric that gives EM = 1 if the characters in the predicted and ground-truth answers match exactly and EM = 0 otherwise.\n\\(F_{1}\\)-score measures the harmonic mean of the precision and recall.\n\n\nfrom farm.evaluation.squad_evaluation import compute_f1, compute_exact\n\nCompute Exact Match and \\(F_{1}\\)-score\npred = \"about 6000 hours\"\nlabel = \"6000 hours\"\nprint(f\"EM: {compute_exact(label, pred)}\")\nprint(f\"F1: {compute_f1(label, pred)}\")\n    EM: 0\n    F1: 0.8\nNote: * These functions first normalize the prediction and label by removing punctuation, fixing whitespace, and converting to lowercase. * They then tokenize the normalized strings as a bag of words before computing the metric at the token level. * The EM is a much stricter metric than the \\(F_{1}\\)-score, but the \\(F_{1}\\)-score does not always catch truly incorrect answers.\n\npred = \"about 6000 dollars\"\nprint(f\"EM: {compute_exact(label, pred)}\")\nprint(f\"F1: {compute_f1(label, pred)}\")\n    EM: 0\n    F1: 0.4\nNote: * Tracking both metrics is an effective strategy to balance the trade-off between underestimating and overestimating model performance. * There are often multiple valid answers per question. * We calculate the metrics for each question-answer pair in the evaluation set and select the best score over all possible answers. * We obtain the overall scores by averaging the individual scores of each question-answer pair.\n\nfrom haystack.eval import EvalAnswers\n\nCreate a new pipeline to evaluate the reader\ndef evaluate_reader(reader):\n    score_keys = ['top_1_em', 'top_1_f1']\n    eval_reader = EvalAnswers(skip_incorrect_retrieval=False)\n    pipe = Pipeline()\n    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"Query\"])\n    pipe.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\n\n    for l in labels_agg:\n        doc = document_store.query(l.question, \n                                   filters={\"question_id\":[l.origin]})\n        _ = pipe.run(query=l.question, documents=doc, labels=l)\n                \n    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\n\nreader_eval = {}\nreader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)\nNote: The skip_incorrect_retrieval=False argument ensures the retriever always passes the context to the reader.\n\nPlot the results\ndef plot_reader_eval(reader_eval):\n    fig, ax = plt.subplots()\n    df = pd.DataFrame.from_dict(reader_eval)\n    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n    ax.set_xticklabels([\"EM\", \"F1\"])\n    plt.legend(loc='upper left')\n    plt.show()\n\nplot_reader_eval(reader_eval)\n\n\n\n\n\nNote: * The fine-tuned model performs significantly worse on SubjQA than on SQuAD 2.0. * The MiniLM model achieves EM and F1 scores of 76.1 and 79.5, respectively, on SQuAD 2.0. * Customer reviews are quite different from the Wikipedia articles in the SQuAD 2.0 dataset. * The inherent subjectivity of the SubjQA dataset might also be affecting performance.\n\n\n\nDomain Adaptation\n\nLearning and Evaluating General Linguistic Intelligence\n\nTransformer models are particularly adept at overfitting to SQuAD\n\nThe most straightforward way to improve readers is by fine-tuning the MiniLM model further on the SubjQA training set.\n\nWe need to convert the data to SQuAD JSON format.\n\n\nDefine a function to create the paragraphs array associated with each product ID. * Each element in this array contains a single context and a qas array of question-answer pairs\n\ndef create_paragraphs(df):\n    paragraphs = []\n    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n    for review_id, review in id2context.items():\n        qas = []\n        # Filter for all question-answer pairs about a specific context\n        review_df = df.query(f\"review_id == '{review_id}'\")\n        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n        # Build up the qas array\n        for qid, question in id2question.items():\n            # Filter for a single question ID\n            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n            ans_text = question_df[\"answers.text\"][0].tolist()\n            # Fill answerable questions\n            if len(ans_start_idxs):\n                answers = [\n                    {\"text\": text, \"answer_start\": answer_start}\n                    for text, answer_start in zip(ans_text, ans_start_idxs)]\n                is_impossible = False\n            else:\n                answers = []\n                is_impossible = True\n            # Add question-answer pairs to qas\n            qas.append({\"question\": question, \"id\": qid, \n                        \"is_impossible\": is_impossible, \"answers\": answers})\n        # Add context and question-answer pairs to paragraphs\n        paragraphs.append({\"qas\": qas, \"context\": review})\n    return paragraphs\n\nTest on rows associated with a single product ID\nproduct = dfs[\"train\"].query(\"title == 'B00001P4ZH'\")\ncreate_paragraphs(product)\n    [{'qas': [{'question': 'How is the bass?',\n        'id': '2543d296da9766d8d17d040ecc781699',\n        'is_impossible': True,\n        'answers': []}],\n      'context': 'I have had Koss headphones in the past, Pro 4AA and QZ-99.  The Koss Portapro is portable AND has great bass response.  The work great with my Android phone and can be \"rolled up\" to be carried in my motorcycle jacket or computer bag without getting crunched.  They are very light and do not feel heavy or bear down on your ears even after listening to music with them on all day.  The sound is night and day better than any ear-bud could be and are almost as good as the Pro 4AA.  They are \"open air\" headphones so you cannot match the bass to the sealed types, but it comes close. For $32, you cannot go wrong.'},\n     {'qas': [{'question': 'Is this music song have a goo bass?',\n        'id': 'd476830bf9282e2b9033e2bb44bbb995',\n        'is_impossible': False,\n        'answers': [{'text': 'Bass is weak as expected', 'answer_start': 1302},\n         {'text': 'Bass is weak as expected, even with EQ adjusted up',\n          'answer_start': 1302}]}],\n      'context': 'To anyone who hasn\\'t tried all the various types of headphones, it is important to remember exactly what these are: cheap portable on-ear headphones. They give a totally different sound then in-ears or closed design phones, but for what they are I would say they\\'re good. I currently own six pairs of phones, from stock apple earbuds to Sennheiser HD 518s. Gave my Portapros a run on both my computer\\'s sound card and mp3 player, using 256 kbps mp3s or better. The clarity is good and they\\'re very lightweight. The folding design is simple but effective. The look is certainly retro and unique, although I didn\\'t find it as comfortable as many have claimed. Earpads are *very* thin and made my ears sore after 30 minutes of listening, although this can be remedied to a point by adjusting the \"comfort zone\" feature (tightening the temple pads while loosening the ear pads). The cord seems to be an average thickness, but I wouldn\\'t get too rough with these. The steel headband adjusts smoothly and easily, just watch out that the slider doesn\\'t catch your hair. Despite the sore ears, the phones are very lightweight overall.Back to the sound: as you would expect, it\\'s good for a portable phone, but hardly earth shattering. At flat EQ the clarity is good, although the highs can sometimes be harsh. Bass is weak as expected, even with EQ adjusted up. To be fair, a portable on-ear would have a tough time comparing to the bass of an in-ear with a good seal or a pair with larger drivers. No sound isolation offered if you\\'re into that sort of thing. Cool 80s phones, though I\\'ve certainly owned better portable on-ears (Sony makes excellent phones in this category). Soundstage is very narrow and lacks body. A good value if you can get them for under thirty, otherwise I\\'d rather invest in a nicer pair of phones. If we\\'re talking about value, they\\'re a good buy compared to new stock apple buds. If you\\'re trying to compare the sound quality of this product to serious headphones, there\\'s really no comparison at all.Update: After 100 hours of burn-in time the sound has not been affected in any appreciable way. Highs are still harsh, and bass is still underwhelming. I sometimes use these as a convenience but they have been largely replaced in my collection.'},\n     {'qas': [{'question': 'How is the bass?',\n        'id': '455575557886d6dfeea5aa19577e5de4',\n        'is_impossible': False,\n        'answers': [{'text': 'The only fault in the sound is the bass',\n          'answer_start': 650}]}],\n      'context': \"I have had many sub-$100 headphones from $5 Panasonic to $100 Sony, with Sennheiser HD 433, 202, PX100 II (I really wanted to like these PX100-II, they were so very well designed), and even a Grado SR60 for awhile.  And what it basically comes down to is value.  I have never heard sound as good as these headphones in the $35 range, easily the best under $75.  I can't believe they're over 25 years old.It's hard to describe how much detail these headphones bring out without making it too harsh or dull.  I listen to every type of music from classical to hip hop to electronic to country, and these headphones are suitable for all types of music.  The only fault in the sound is the bass.  It's just a *slight* bit boomy, but you get to like it after a while to be honest.The design is from the 80s as you all have probably figured out.  It could use a update but it seems like Koss has tried to perfect this formula and failed in the past.  I don't really care about the looks or the way it folds up or the fact that my hair gets caught up in it (I have very short hair, even for a male).But despite it's design flaws, it's the most comfortable headphones I have ever worn, and the best part is that it's also the best sounding pair of headphones I have ever heard under $75.If you can get over the design flaws or if sound is the most important feature of headphones for you, there is nothing even close to this at this price range.This one is an absolute GEM.  I loved these so much I ordered two of the 25th Anniversary ones for a bit more.Update: I read some reviews about the PX100-II being much improved and better sounding than the PortaPro.  Since the PX100-II is relatively new, I thought I'd give it another listen.  This time I noticed something different.  The sound is warm, mellow, and neutral, but it loses a lot of detail at the expense of these attributes.  I still prefer higher-detail Portapro, but some may prefer the more mellow sound of the PX100-II.Oh by the way the Portapro comes in the straight plug now, not the angled plug anymore.  It's supposed to be for better compatibility with the iPods and iPhones out there.\"}]\n\nimport json\n\nApply the create_paragraphs() function each product ID in the DataFrame of each split\ndef convert_to_squad(dfs):\n    for split, df in dfs.items():\n        subjqa_data = {}\n        # Create `paragraphs` for each product ID\n        groups = (df.groupby(\"title\").apply(create_paragraphs)\n            .to_frame(name=\"paragraphs\").reset_index())\n        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n        # Save the result to disk\n        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n            json.dump(subjqa_data, f)\n            \nconvert_to_squad(dfs)\n\nFine-tune the reader on the train and validation splits\ntrain_filename = \"electronics-train.json\"\ndev_filename = \"electronics-validation.json\"\n\nreader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n             train_filename=train_filename, dev_filename=dev_filename)\n    04/11/2022 22:17:08 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 22:17:08 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 22:17:08 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 22:17:08 - INFO - farm.utils -   Automatic Mixed Precision: None\n    Preprocessing Dataset electronics-train.json:   1%|                            | 17/1265 [00:00&lt;00:12, 96.90 Dicts/s]04/11/2022 22:17:08 - WARNING - farm.data_handler.processor -   Answer using start/end indices is '  Operation of the menus and contro' while gold label text is 'Operation of the menus and controls'.\n    Example will not be converted for training/evaluation.\n    04/11/2022 22:17:08 - WARNING - farm.data_handler.processor -   Answer using start/end indices is '  This camera performs like the pros.  Fast accurate and easy to operat' while gold label text is 'This camera performs like the pros.  Fast accurate and easy to operated'.\n    Example will not be converted for training/evaluation.\n    Preprocessing Dataset electronics-train.json: 100%|| 1265/1265 [00:00&lt;00:00, 1899.15 Dicts/s]\n    04/11/2022 22:17:09 - ERROR - farm.data_handler.processor -   Unable to convert 2 samples to features. Their ids are : 595-0-0, 572-0-0\n    Preprocessing Dataset electronics-validation.json: 100%|| 252/252 [00:00&lt;00:00, 529.53 Dicts/s]\n    04/11/2022 22:17:10 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n    04/11/2022 22:17:10 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n    04/11/2022 22:17:10 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 164, 'num_warmup_steps': 32}'\n    04/11/2022 22:17:38 - INFO - haystack.reader.farm -   Saving reader model to ../../saved_models/deepset/minilm-uncased-squad2\n\nCompare performance to baseline model\nreader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\nplot_reader_eval(reader_eval)\n\n\n\n\n\nNote: Domain adaptation increased the EM score by 6x and the F1 score over 2x.\n\nLoad the model with FARMReader\nminilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\nminilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n                           max_seq_len=max_seq_length, doc_stride=doc_stride,\n                           return_no_answer=True)\n    04/11/2022 22:25:29 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 22:25:29 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 22:25:29 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 22:25:29 - INFO - farm.utils -   Automatic Mixed Precision: None\n\n    04/11/2022 22:25:38 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n    04/11/2022 22:25:38 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 22:25:38 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 22:25:38 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 22:25:38 - INFO - farm.utils -   Automatic Mixed Precision: None\n    04/11/2022 22:25:38 - INFO - farm.infer -   Got ya 15 parallel workers to do inference ...\n    04/11/2022 22:25:38 - INFO - farm.infer -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n    04/11/2022 22:25:38 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\\n    04/11/2022 22:25:38 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\  /'\\\n    04/11/2022 22:25:38 - INFO - farm.infer -                               \n\nFine-tune directly on the SubjQA training set\nminilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n             train_filename=train_filename, dev_filename=dev_filename)\n    04/11/2022 22:25:38 - INFO - farm.utils -   Using device: CUDA \n    04/11/2022 22:25:38 - INFO - farm.utils -   Number of GPUs: 1\n    04/11/2022 22:25:38 - INFO - farm.utils -   Distributed Training: False\n    04/11/2022 22:25:38 - INFO - farm.utils -   Automatic Mixed Precision: None\n\nCompare the results\nreader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\nplot_reader_eval(reader_eval)\n\n\n\n\n\nNote:\n\nThe model fine-tuned directly on the SubjQA dataset results in significantly worse performance than the model fine-tuned on SQuAD and SubjQA.\nUse cross-validation when evaluating transformers with small datasets.\n\nExample\n\n\n\n\n\nEvaluating the Whole QA Pipeline\nAugment the retriever pipeline with nodes for the reader and its evaluation\n# Initialize retriever pipeline\npipe = EvalRetrieverPipeline(es_retriever)\n# Add nodes for reader\neval_reader = EvalAnswers()\npipe.pipeline.add_node(component=reader, name=\"QAReader\", \n              inputs=[\"EvalRetriever\"])\npipe.pipeline.add_node(component=eval_reader, name=\"EvalReader\", \n              inputs=[\"QAReader\"])\n# Evaluate!\nrun_pipeline(pipe)\n# Extract metrics from reader\nreader_eval[\"QA Pipeline (top-1)\"] = {\n    k:v for k,v in eval_reader.__dict__.items()\n    if k in [\"top_1_em\", \"top_1_f1\"]}\n\nCompare the EM and \\(F_{1}\\)-scores for the reader against the whole QA pipeline\nplot_reader_eval({\"Reader\": reader_eval[\"Fine-tune on SQuAD + SubjQA\"], \n                  \"QA pipeline (top-1)\": reader_eval[\"QA Pipeline (top-1)\"]})\n\n\n\n\n\nNote: * There is an overall degradation compared to matching the question-context pairs as is done in the SQuAD-style evaluation. * We can circumvent this by increasing the number of possible answers the reader is allowed to predict."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#going-beyond-extractive-qa",
    "href": "posts/transformers-book-notes/chapter-7/index.html#going-beyond-extractive-qa",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "Going Beyond Extractive QA",
    "text": "Going Beyond Extractive QA\n\nAbstractive/Generative QA generates answers with a pretrained model rather than extracting them as spans of text.\nGenerative QA can potentially produce better-phrased answers that synthesize evidence across multiple passages.\nGenerative QA is a less mature but fast-moving field of research.\n\n\nRetrieval-augmented generation (RAG)\n\nRetrieval-augmented generation (RAG) is the current state-of-the-art.\nRAG extends the classic retriever-reader architecture by swapping the reader for a generator and using DPR as the retriever.\nThe generator is a pretrained sequence-to-sequence transformer like T5 or BART that receives latent vectors of documents from DPR and then iteratively generates an answer based on the query and these documents.\nWe can fine-tune the whole process end-to-end.\nThere are two types of RAG models available.\n\nRAG-Sequence uses the same retrieved document to generate the complete answer.\nRAG-Token can use a different document to generate each token in the answer, allowing the generator to synthesize evidence from multiple sources.\nRAG-Token models tend to perform better than RAG-Sequence models.\n\n\n\nfrom haystack.generator.transformers import RAGenerator\n\nInstantiate the generator\ngenerator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\",\n                        embed_title=False, num_beams=5)\n\nfrom haystack.pipeline import GenerativeQAPipeline\n\nCreate a pipeline that ties together the generator and retriever\npipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)\n\nDefine a function that takes a query and prints the top answsers\ndef generate_answers(query, top_k_generator=3):\n    preds = pipe.run(query=query, top_k_generator=top_k_generator, \n                     top_k_retriever=5, filters={\"item_id\":[\"B0074BW614\"]})  \n    print(f\"Question: {preds['query']} \\n\")\n    for idx in range(top_k_generator):\n        print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")\ngenerate_answers(query)\n    /home/innom-dt/miniconda3/envs/transformer-book-chapter7/lib/python3.9/site-packages/transformers/models/rag/tokenization_rag.py:92: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of Hugging Face Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n      warnings.warn(\n    /home/innom-dt/miniconda3/envs/transformer-book-chapter7/lib/python3.9/site-packages/transformers/generation_utils.py:1712: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n      warnings.warn(\n\n\n    Question: Is it good for reading? \n    \n    Answer 1:  the screen is absolutely beautiful\n    Answer 2:  the Screen is absolutely beautiful\n    Answer 3:  Kindle fire\nNote: The results suggest that the subjective nature of the question is confusing the generator.\n\nTry a more factual question\ngenerate_answers(\"What is the main drawback?\")\n    Question: What is the main drawback? \n    \n    Answer 1:  the price\n    Answer 2:  no flash support\n    Answer 3:  the cost\nNote: * These results are more sensible. * We could get better results by fine-tuning RAG end-to-end on SubjQA. * Starter scripts"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#conclusion",
    "href": "posts/transformers-book-notes/chapter-7/index.html#conclusion",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe techniques in this chapter can generalize to open-domain QA.\n\nCloudera’s Fast Forward QA series\n\nDeploying QA systems in the wild can be tricky.\nA significant part of the value comes from providing end-users with helpful search capabilities, followed by an extractive component.\nWe can use the reader in novel ways beyond answering on-demand user queries.\n\nResearchers at Grid Dynamics used a reader to automatically extract a set of pros and cons for each product in a client’s catalog and to extract named entities in a zero-shot fashion by creating queries like “What kind of camera?”\nFinding a needle in a haystack: building a question answering system for online store\n\nGenerative QA is still in its infancy, so only explore after trying extractive and search methods.\nMultimodal QA involves question answering over multiple modalities like text, tables, and images.\n\nMultimodel QA systems could enable users to answer complex questions that integrate information across different modalities.\nMultiModalQA: Complex Question Answering over Text, Tables and Images\n\nAnother area with practical business applications is QA over a knowledge graph, where the graph nodes correspond to real-world entities.\n\nOne can use the graph to answer questions about a missing element by encoding factoids as (subject, predicate, * object) triples.\nQuestion Answering on a Knowledge Graph Tutorial\n\nAutomatic question generation is a way to do unsupervised/weakly supervised training using unlabeled data or data augmentation.\n\nPAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\nSynthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-7/index.html#references",
    "href": "posts/transformers-book-notes/chapter-7/index.html#references",
    "title": "Notes on Transformers Book Ch. 7",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 6\nNext: Notes on Transformers Book Ch. 8"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html",
    "href": "posts/transformers-book-notes/chapter-8/index.html",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "",
    "text": "Making Transformers Efficient in Production\nProject: Optimize an Intent Detection Model\nCreating a Performance Benchmark\nMaking Models Smaller via Knowledge Distillation\nA Primer on Floating-Point and Fixed-Point Numbers\nMaking Models Faster with Quantization\nOptimizing Inference with ONNX and the ONNX Runtime\nMaking Models Sparser with Weight Pruning\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#making-transformers-efficient-in-production",
    "href": "posts/transformers-book-notes/chapter-8/index.html#making-transformers-efficient-in-production",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Making Transformers Efficient in Production",
    "text": "Making Transformers Efficient in Production\n\nA state-of-the-art model is not very useful if it is too slow or too large to meet an application’s business requirements.\nStarting with a faster, more compact model often results in degraded performance.\nKnowledge distillation, quantization, pruning, and graph optimization are complementary techniques that can speed up predictions and reduce the memory footprint of models.\nWe can combine some of these techniques to produce significant performance gains.\nRoblox: How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs\n\nRoblox improved the latency and throughput of their BERT classifier by over 30x by combining knowledge distillation and quantization."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#project-optimize-an-intent-detection-model",
    "href": "posts/transformers-book-notes/chapter-8/index.html#project-optimize-an-intent-detection-model",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Project: Optimize an Intent Detection Model",
    "text": "Project: Optimize an Intent Detection Model\n\nThe goal is to create a text-based assistant for a call center so customers can request their account balance and make bookings.\nThe assistant must be able to classify a wide variety of natural language text into a set of predefined intents.\nThe classifier must also handle out-of-scope queries and yield fallback responses when they do not belong to any predefined intents.\n\n\nThe Model\n\nThe baseline model is a fine-tuned BERT-base model that achieves 94% accuracy on the CLINC150 dataset.\nHugging Face Dataset Card\n\n\n\nCLINC150 Dataset\n\nHomepage\nHuggingFace Dataset Card\nThe CLINC150 dataset includes 22,500 in-scope queries across 150 intents and ten domains.\nThe dataset contains 1,200 out-of-scope queries that belong to an oos intent class.\n\n\nfrom transformers import pipeline\n\nInstantiate a text classification pipeline with the baseline model\nbert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=bert_ckpt)\n\nClassify a sample query\nquery = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in \nParis and I need a 15 passenger van\"\"\"\npipe(query)\n    [{'label': 'car_rental', 'score': 0.549003541469574}]\nNote: The model correctly detects that the user wants to rent a vehicle."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#creating-a-performance-benchmark",
    "href": "posts/transformers-book-notes/chapter-8/index.html#creating-a-performance-benchmark",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Creating a Performance Benchmark",
    "text": "Creating a Performance Benchmark\n\nDeploying transformers in production involves a tradeoff between several constraints.\nBusiness and product metrics are the most important to consider.\nModel performance refers to how the model performs on a well-crafted test set representing production data.\nModel performance is especially crucial when the cost of making errors is high or when performing inference on millions of examples and minor improvements translate to significant gains.\nLatency refers to how fast the model delivers predictions. Latency is most important for real-time environments with lots of traffic.\n\nThe Unfriendly Robot: Automatically flagging unwelcoming comments\n\nMemory constraints play an important role in mobile and edge devices where we need to perform inference without access to a cloud server.\nFailing to address these constraints can negatively impact the user experience.\nRunning expensive cloud servers that may only need to handle a few requests can lead to ballooning costs.\n\nDefine a benchmark that measures model performance, latency, and memory usage\nclass PerformanceBenchmark:\n    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n        self.pipeline = pipeline\n        self.dataset = dataset\n        self.optim_type = optim_type\n        \n    def compute_accuracy(self):\n        # We'll define this later\n        pass    \n\n    def compute_size(self):\n        # We'll define this later\n        pass\n\n    def time_pipeline(self):\n        # We'll define this later\n        pass\n    \n    def run_benchmark(self):\n        metrics = {}\n        metrics[self.optim_type] = self.compute_size()\n        metrics[self.optim_type].update(self.time_pipeline())\n        metrics[self.optim_type].update(self.compute_accuracy())\n        return metrics\n\nfrom datasets import load_dataset\n\nLoad the CLINC150 Dataset\nclinc = load_dataset(\"clinc_oos\", \"plus\")\nclinc\n    DatasetDict({\n        train: Dataset({\n            features: ['text', 'intent'],\n            num_rows: 15250\n        })\n        validation: Dataset({\n            features: ['text', 'intent'],\n            num_rows: 3100\n        })\n        test: Dataset({\n            features: ['text', 'intent'],\n            num_rows: 5500\n        })\n    })\nNote: * The plus configuration refers to the subset that contains the out-of-scope training examples. * Each example consists of a query in the text column and its corresponding intent.\n\nView an example\nsample = clinc[\"test\"][42]\nsample\n    {'text': 'transfer $100 from my checking to saving account', 'intent': 133}\n\nMap intent ID to the corresponding string\nintents = clinc[\"test\"].features[\"intent\"]\nintents.int2str(sample[\"intent\"])\n    'transfer'\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\npd.DataFrame(intents._int2str)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\nrestaurant_reviews\n\n\n\n\n1\n\n\nnutrition_info\n\n\n\n\n2\n\n\naccount_blocked\n\n\n\n\n3\n\n\noil_change_how\n\n\n\n\n4\n\n\ntime\n\n\n\n\n5\n\n\nweather\n\n\n\n\n6\n\n\nredeem_rewards\n\n\n\n\n7\n\n\ninterest_rate\n\n\n\n\n8\n\n\ngas_type\n\n\n\n\n9\n\n\naccept_reservations\n\n\n\n\n10\n\n\nsmart_home\n\n\n\n\n11\n\n\nuser_name\n\n\n\n\n12\n\n\nreport_lost_card\n\n\n\n\n13\n\n\nrepeat\n\n\n\n\n14\n\n\nwhisper_mode\n\n\n\n\n15\n\n\nwhat_are_your_hobbies\n\n\n\n\n16\n\n\norder\n\n\n\n\n17\n\n\njump_start\n\n\n\n\n18\n\n\nschedule_meeting\n\n\n\n\n19\n\n\nmeeting_schedule\n\n\n\n\n20\n\n\nfreeze_account\n\n\n\n\n21\n\n\nwhat_song\n\n\n\n\n22\n\n\nmeaning_of_life\n\n\n\n\n23\n\n\nrestaurant_reservation\n\n\n\n\n24\n\n\ntraffic\n\n\n\n\n25\n\n\nmake_call\n\n\n\n\n26\n\n\ntext\n\n\n\n\n27\n\n\nbill_balance\n\n\n\n\n28\n\n\nimprove_credit_score\n\n\n\n\n29\n\n\nchange_language\n\n\n\n\n30\n\n\nno\n\n\n\n\n31\n\n\nmeasurement_conversion\n\n\n\n\n32\n\n\ntimer\n\n\n\n\n33\n\n\nflip_coin\n\n\n\n\n34\n\n\ndo_you_have_pets\n\n\n\n\n35\n\n\nbalance\n\n\n\n\n36\n\n\ntell_joke\n\n\n\n\n37\n\n\nlast_maintenance\n\n\n\n\n38\n\n\nexchange_rate\n\n\n\n\n39\n\n\nuber\n\n\n\n\n40\n\n\ncar_rental\n\n\n\n\n41\n\n\ncredit_limit\n\n\n\n\n42\n\n\noos\n\n\n\n\n43\n\n\nshopping_list\n\n\n\n\n44\n\n\nexpiration_date\n\n\n\n\n45\n\n\nrouting\n\n\n\n\n46\n\n\nmeal_suggestion\n\n\n\n\n47\n\n\ntire_change\n\n\n\n\n48\n\n\ntodo_list\n\n\n\n\n49\n\n\ncard_declined\n\n\n\n\n50\n\n\nrewards_balance\n\n\n\n\n51\n\n\nchange_accent\n\n\n\n\n52\n\n\nvaccines\n\n\n\n\n53\n\n\nreminder_update\n\n\n\n\n54\n\n\nfood_last\n\n\n\n\n55\n\n\nchange_ai_name\n\n\n\n\n56\n\n\nbill_due\n\n\n\n\n57\n\n\nwho_do_you_work_for\n\n\n\n\n58\n\n\nshare_location\n\n\n\n\n59\n\n\ninternational_visa\n\n\n\n\n60\n\n\ncalendar\n\n\n\n\n61\n\n\ntranslate\n\n\n\n\n62\n\n\ncarry_on\n\n\n\n\n63\n\n\nbook_flight\n\n\n\n\n64\n\n\ninsurance_change\n\n\n\n\n65\n\n\ntodo_list_update\n\n\n\n\n66\n\n\ntimezone\n\n\n\n\n67\n\n\ncancel_reservation\n\n\n\n\n68\n\n\ntransactions\n\n\n\n\n69\n\n\ncredit_score\n\n\n\n\n70\n\n\nreport_fraud\n\n\n\n\n71\n\n\nspending_history\n\n\n\n\n72\n\n\ndirections\n\n\n\n\n73\n\n\nspelling\n\n\n\n\n74\n\n\ninsurance\n\n\n\n\n75\n\n\nwhat_is_your_name\n\n\n\n\n76\n\n\nreminder\n\n\n\n\n77\n\n\nwhere_are_you_from\n\n\n\n\n78\n\n\ndistance\n\n\n\n\n79\n\n\npayday\n\n\n\n\n80\n\n\nflight_status\n\n\n\n\n81\n\n\nfind_phone\n\n\n\n\n82\n\n\ngreeting\n\n\n\n\n83\n\n\nalarm\n\n\n\n\n84\n\n\norder_status\n\n\n\n\n85\n\n\nconfirm_reservation\n\n\n\n\n86\n\n\ncook_time\n\n\n\n\n87\n\n\ndamaged_card\n\n\n\n\n88\n\n\nreset_settings\n\n\n\n\n89\n\n\npin_change\n\n\n\n\n90\n\n\nreplacement_card_duration\n\n\n\n\n91\n\n\nnew_card\n\n\n\n\n92\n\n\nroll_dice\n\n\n\n\n93\n\n\nincome\n\n\n\n\n94\n\n\ntaxes\n\n\n\n\n95\n\n\ndate\n\n\n\n\n96\n\n\nwho_made_you\n\n\n\n\n97\n\n\npto_request\n\n\n\n\n98\n\n\ntire_pressure\n\n\n\n\n99\n\n\nhow_old_are_you\n\n\n\n\n100\n\n\nrollover_401k\n\n\n\n\n101\n\n\npto_request_status\n\n\n\n\n102\n\n\nhow_busy\n\n\n\n\n103\n\n\napplication_status\n\n\n\n\n104\n\n\nrecipe\n\n\n\n\n105\n\n\ncalendar_update\n\n\n\n\n106\n\n\nplay_music\n\n\n\n\n107\n\n\nyes\n\n\n\n\n108\n\n\ndirect_deposit\n\n\n\n\n109\n\n\ncredit_limit_change\n\n\n\n\n110\n\n\ngas\n\n\n\n\n111\n\n\npay_bill\n\n\n\n\n112\n\n\ningredients_list\n\n\n\n\n113\n\n\nlost_luggage\n\n\n\n\n114\n\n\ngoodbye\n\n\n\n\n115\n\n\nwhat_can_i_ask_you\n\n\n\n\n116\n\n\nbook_hotel\n\n\n\n\n117\n\n\nare_you_a_bot\n\n\n\n\n118\n\n\nnext_song\n\n\n\n\n119\n\n\nchange_speed\n\n\n\n\n120\n\n\nplug_type\n\n\n\n\n121\n\n\nmaybe\n\n\n\n\n122\n\n\nw2\n\n\n\n\n123\n\n\noil_change_when\n\n\n\n\n124\n\n\nthank_you\n\n\n\n\n125\n\n\nshopping_list_update\n\n\n\n\n126\n\n\npto_balance\n\n\n\n\n127\n\n\norder_checks\n\n\n\n\n128\n\n\ntravel_alert\n\n\n\n\n129\n\n\nfun_fact\n\n\n\n\n130\n\n\nsync_device\n\n\n\n\n131\n\n\nschedule_maintenance\n\n\n\n\n132\n\n\napr\n\n\n\n\n133\n\n\ntransfer\n\n\n\n\n134\n\n\ningredient_substitution\n\n\n\n\n135\n\n\ncalories\n\n\n\n\n136\n\n\ncurrent_location\n\n\n\n\n137\n\n\ninternational_fees\n\n\n\n\n138\n\n\ncalculator\n\n\n\n\n139\n\n\ndefinition\n\n\n\n\n140\n\n\nnext_holiday\n\n\n\n\n141\n\n\nupdate_playlist\n\n\n\n\n142\n\n\nmpg\n\n\n\n\n143\n\n\nmin_payment\n\n\n\n\n144\n\n\nchange_user_name\n\n\n\n\n145\n\n\nrestaurant_suggestion\n\n\n\n\n146\n\n\ntravel_notification\n\n\n\n\n147\n\n\ncancel\n\n\n\n\n148\n\n\npto_used\n\n\n\n\n149\n\n\ntravel_suggestion\n\n\n\n\n150\n\n\nchange_volume\n\n\n\n\n\n\n\nfrom datasets import load_metric \n\nLoad the accuracy metric\naccuracy_score = load_metric(\"accuracy\")\naccuracy_score\n    Metric(name: \"accuracy\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n    Args:\n        predictions: Predicted labels, as returned by a model.\n        references: Ground truth labels.\n        normalize: If False, return the number of correctly classified samples.\n            Otherwise, return the fraction of correctly classified samples.\n        sample_weight: Sample weights.\n    Returns:\n        accuracy: Accuracy score.\n    Examples:\n    \n        &gt;&gt;&gt; accuracy_metric = datasets.load_metric(\"accuracy\")\n        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n        &gt;&gt;&gt; print(results)\n        {'accuracy': 1.0}\n    \"\"\", stored examples: 0)\nNote: The accuracy metric expects the predictions and ground truth labels to be integers.\n\nImplement the PerformanceBenchmark.compute_accuracy() method\ndef compute_accuracy(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n    preds, labels = [], []\n    # Collect all the predictions and labels into lists\n    for example in self.dataset:\n        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n        label = example[\"intent\"]\n        preds.append(intents.str2int(pred))\n        labels.append(label)\n    # Compute the accuracy for the predictions\n    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n    return accuracy\n\n# Override the PerformanceBenchmark.compute_accuracy() method\nPerformanceBenchmark.compute_accuracy = compute_accuracy\n\nCompute the model size\nNote: * We can compute the model size using the torch.save() function. * The save() function uses Python’s pickle module. * The recommended way to save a PyTorch model is by using its state_dict. * The state_dict is a Python dictionary that maps each layer in a model to its learnable parameters.\nInspect the state_dict for the baseline model\nlist(pipe.model.state_dict().items())[42]\n    ('bert.encoder.layer.2.attention.self.value.weight',\n     tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\n               4.6521e-03,  2.9844e-02],\n             [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\n              -2.6890e-02, -2.1943e-02],\n             [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\n               3.1152e-02, -9.7786e-03],\n             ...,\n             [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\n               1.1093e-02,  2.9703e-03],\n             [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\n               6.7487e-03,  1.0511e-03],\n             [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\n               2.3981e-02, -4.2880e-02]]))\nNote: Each key-value pair corresponds to a specific layer and tensor in BERT.\n\nimport torch\nfrom pathlib import Path\n\nImplement the PerformanceBenchmark.compute_size() method\ndef compute_size(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n    state_dict = self.pipeline.model.state_dict()\n    tmp_path = Path(\"model.pt\")\n    # Temporarily save the model to disk\n    torch.save(state_dict, tmp_path)\n    # Calculate size in megabytes\n    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n    # Delete temporary file\n    tmp_path.unlink()\n    print(f\"Model size (MB) - {size_mb:.2f}\")\n    return {\"size_mb\": size_mb}\n\n# Override the PerformanceBenchmark.compute_size() method\nPerformanceBenchmark.compute_size = compute_size\n\nCompute the model latency\n\nFor this application, latency refers to the time it takes to feed a text query to the pipeline and return the predicted intent from the model.\n\n\nfrom time import perf_counter\n\n\ntime.perf_counter\n\nDocumentation\nGet the value in fractional seconds of a clock with the highest available resolution to measure a short duration.\n\n\nhelp(perf_counter)\n    Help on built-in function perf_counter in module time:\n    \n    perf_counter(...)\n        perf_counter() -&gt; float\n        \n        Performance counter for benchmarking.\n\nTest the latency of the baseline model\nfor _ in range(3):\n    start_time = perf_counter()\n    _ = pipe(query)\n    latency = perf_counter() - start_time\n    print(f\"Latency (ms) - {1000 * latency:.3f}\")\n    Latency (ms) - 29.646\n    Latency (ms) - 28.035\n    Latency (ms) - 27.233\nNote:\n\nThere is a notable spread in the latencies, so we should collect the latencies over many runs to calculate the mean and standard deviation.\nThe latency depends on the query length, and it is good practice to benchmark using queries the models are likely to encounter in production.\n\n\nimport numpy as np\n\nImplement the PerformanceBenchmark.time_pipeline() method\ndef time_pipeline(self, query=\"What is the pin number for my account?\"):\n    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n    latencies = []\n    # Warmup\n    for _ in range(10):\n        _ = self.pipeline(query)\n    # Timed run\n    for _ in range(100):\n        start_time = perf_counter()\n        _ = self.pipeline(query)\n        latency = perf_counter() - start_time\n        latencies.append(latency)\n    # Compute run statistics\n    time_avg_ms = 1000 * np.mean(latencies)\n    time_std_ms = 1000 * np.std(latencies)\n    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n\n# Override the PerformanceBenchmark.time_pipeline() method\nPerformanceBenchmark.time_pipeline = time_pipeline\n\nBenchmark the baseline model\npb = PerformanceBenchmark(pipe, clinc[\"test\"])\nperf_metrics = pb.run_benchmark()\n    Model size (MB) - 418.16\n    Average latency (ms) - 24.46 +\\- 1.20\n    Accuracy on test set - 0.867"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#making-models-smaller-via-knowledge-distillation",
    "href": "posts/transformers-book-notes/chapter-8/index.html#making-models-smaller-via-knowledge-distillation",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Making Models Smaller via Knowledge Distillation",
    "text": "Making Models Smaller via Knowledge Distillation\n\nKnowledge distillation is a general-purpose method for training a smaller student model to mimic the behavior of a slower but better-performing teacher.\nModel compression\n\nThis paper introduced the concept of knowledge distillation in 2006 in the context of ensemble models.\n\nDistilling the Knowledge in a Neural Network\n\nThis paper generalized knowledge distillation to deep neural networks and applied it to image classification and automatic speech recognition.\n\nThe current trend is to pre-train language models with ever-increasing parameters counts.\n\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n\nKnowledge distillation is a popular strategy to compress huge pretrained models and make them more suitable for building practical applications.\n\n\nKnowledge Distillation for Fine-Tuning\n\nKnowledge distillation for supervised tasks like fine-tuning involves augmenting the ground truth labels with a distribution of “soft probabilities” from the teacher, providing complementary information for the student.\nIf the teacher assigns high probabilities to multiple intents, they might lie close to each other in the feature space.\nThe goal is to train the student to distill some of this “dark knowledge” learned by the teacher.\nThis “dark knowledge” is not available from the labels alone.\nWe feed an input sequence \\(x\\) to the teacher to generate a vector of logits \\(z(x) = \\left[ z_{1}(x),\\ldots,z_{N}(x) \\right]\\) and convert these logits into probabilities using the softmax function.\n\n\n\n\\[\\frac{exp \\left( z_{i}(x) \\right)}{\\sum_{j}{exp \\left( z_{i}(x) \\right)}}\\]\n\nThe teacher will often assign a high probability to one class, with all other class probabilities close to zero, providing little additional information beyond the ground truth labels.\nWe can “soften” the probabilities by scaling the logits with a temperature hyperparameter \\(T\\) before applying the softmax.\n\n\n\n\\[p_{i}(x) = \\frac{exp \\left( \\frac{ z_{i}(x) }{T} \\right)}{\\sum_{j}{exp \\left( \\frac{ z_{i}(x) }{T} \\right)}}\\]\n\nHigher temperature values produce a softer probability distribution over the classes and reveal much more information about the decision boundary learned by the teacher for each example.\n\nWhen T = 1, we get the original softmax distribution.\n\nWe can use the Kullback-Leibler divergence to measure the difference between the teacher’s probability distribution and the student’s probability distribution.\n\n\n\n\\[D_{KL}(p,q) = \\sum_{i}{p_{i}(x)\\log{\\frac{p_{i}(x)}{q_{i}(x)}}}\\]\n\nWith the KL divergence, we can calculate how much is lost when we approximate the probability distribution of the teacher with the student.\nKowledge Distillation Loss:\n\n\n\n\\[L_{KD} = T^{2}D_{KL}\\]\n\n\\(T_{2}\\) is the normalization factor to account for the magnitude of the gradients produced by the soft labels scaling as \\(1/T^{2}\\).\nFor classification tasks, the student loss is a weighted average of the distillation loss with the usual cross-entropy loss \\(L_{CE}\\) of the ground truth labels.\n\n\n\n\\[L_{student} = \\alpha L_{CE} \\ + \\left( 1 - \\alpha \\right)L_{KD}\\]\n\n\\(\\alpha\\) is a hyperparameter that controls the relative strength of each loss.\n\n\n\nKnowledge Distillation for Pretraining\n\nWe can use knowledge distillation during pretraining to create a general-purpose student that we subsequently fine-tune on downstream tasks.\nThe teacher is a pretrained language model like BERT, which transfers its knowledge about masked-language modeling to the student.\nFor DistilBERT, we augment the masked language modeling loss \\(L_{mlm}\\) with a term from knowledge distillation and a cosine embedding loss \\(L_{cos} = 1 \\ - \\ \\cos \\left( h_{s},h_{t} \\right)\\) to align the directions of the hidden state vectors between the teacher and student.\n\n\n\n\n\\[L_{DistilBERT} = \\alpha L_{mlm} \\ + \\ \\beta L_{KD} \\ + \\ y \\ Loss_{cos}\\]\n\n\nCreating a Knowledge Distillation Trainer\n\nWe can augment the cross-entropy loss with an \\(L_{KD}\\) term by creating a custom trainer.\n\n\nAdditions to the base Trainer Class:\n\nThe new hyperparameters \\(\\alpha\\) and \\(T\\).\nThe fine-tuned teacher model\nA new loss function that combines the the cross-entropy loss with the knowledge distillation loss\n\n\nfrom transformers import TrainingArguments\n\nCreate a new TrainingArguments subclass with the new hyperparameters\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.temperature = temperature\n\n\n\nnn.KLDivLoss\n\nDocumentation\nCompute the Kullback-Leibler divergence loss.\n\n\n\n\n\\[L(y_{\\text{pred}},\\ y_{\\text{true}}) = y_{\\text{true}} \\cdot \\log \\frac{y_{\\text{true}}}{y_{\\text{pred}}} = y_{\\text{true}} \\cdot (\\log y_{\\text{true}} - \\log y_{\\text{pred}})\\]\n\nwhere \\(y_{\\text{pred}}\\) is the input and \\(y_{\\text{true}}\\) is the target\nThe inputs need to be in the form of log probabilities.\nThe labels need to be in the form of normal probabilities.\n\nCreate a new Trainer subclass and override the compute_loss() method\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs_stu = model(**inputs)\n        # Extract cross-entropy loss and logits from student\n        loss_ce = outputs_stu.loss\n        logits_stu = outputs_stu.logits\n        # Extract logits from teacher\n        with torch.no_grad():\n            outputs_tea = self.teacher_model(**inputs)\n            logits_tea = outputs_tea.logits\n        # Soften probabilities and compute distillation loss\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n        loss_kd = self.args.temperature ** 2 * loss_fct(\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n            F.softmax(logits_tea / self.args.temperature, dim=-1))\n        # Return weighted student loss\n        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n        return (loss, outputs_stu) if return_outputs else loss\nNote: The reduction=\"batchmean\" argument in nn.KVDivLoss() specifies that we average the losses over the batch dimension.\n\n\n\nChoosing a Good Student Initialization\n\nThe student model should be smaller to reduce the latency and memory footprint.\nFastFormers: Highly Efficient Transformer Models for Natural Language Understanding\n\nKnowledge distillation tends to work best when the teacher and student are of the same model type.\nDifferent model types like BERT and RoBERTa can have incompatible output embedding spaces, hindering the student’s ability to mimic the teacher.\n\nDistilBERT is a compatible student model for the BERT baseline model.\n\n\nfrom transformers import AutoTokenizer\n\nLoad the tokenizer for the DistilBERT student model\nstudent_ckpt = \"distilbert-base-uncased\"\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n\nTokenize and encode the queries\ndef tokenize_text(batch):\n    return student_tokenizer(batch[\"text\"], truncation=True)\n\nclinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\nclinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")\nNote: * We no longer need the text column. * The trainer looks for a column called labels when fine-tuning for classification tasks. * We can override this default with the label_names argument of the TrainingArguments object.\n\nDisable Tokenizers Parallelism\n%env TOKENIZERS_PARALLELISM=false\n    env: TOKENIZERS_PARALLELISM=false\nfrom huggingface_hub import notebook_login\n\nLog into Hugging Face account\nnotebook_login()\n    Login successful\n    Your token has been saved to /home/innom-dt/.huggingface/token\n\nDefine the metrics to track during training\ndef compute_metrics(pred):\n    predictions, labels = pred\n    # Get the most confident class predictions\n    predictions = np.argmax(predictions, axis=1)\n    # Compare the predictions to the ground truth label\n    return accuracy_score.compute(predictions=predictions, references=labels)\n\nDefine the training arguments\nbatch_size = 48\n\nfinetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\nstudent_training_args = DistillationTrainingArguments(\n    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\", \n    num_train_epochs=5, learning_rate=2e-5, \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01, \n    push_to_hub=True, fp16=True)\nNote: Starting with \\(\\alpha=1\\) to see how well the student performs without any signal from the teacher.\n\nstudent_training_args.logging_steps = len(clinc_enc['train']) // batch_size\nstudent_training_args.disable_tqdm = False\nstudent_training_args.save_steps = 1e9\nstudent_training_args.log_level = 40\n\nProvide the student model with the mappings between each intent and label ID\nid2label = pipe.model.config.id2label\nlabel2id = pipe.model.config.label2id\nfrom transformers import AutoConfig\n\nCreate a custom model configuration from the student\nnum_labels = intents.num_classes\nstudent_config = (AutoConfig.from_pretrained(student_ckpt, num_labels=num_labels, \n                                             id2label=id2label, label2id=label2id))\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\nUse a CUDA GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nDefine a function to initialize the student model with a sequence classification head\ndef student_init():\n    return (AutoModelForSequenceClassification\n            .from_pretrained(student_ckpt, config=student_config).to(device))\n\nInitialize the teacher model with a sequence classification head\nteacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\nteacher_model = (AutoModelForSequenceClassification\n                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n                 .to(device))\n\nInitialize the custom trainer\ndistilbert_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/distilbert-base-uncased-finetuned-clinc is already a clone of https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n\nNote: Had to add the following workaround.\nold_collator = distilbert_trainer.data_collator\ndistilbert_trainer.data_collator = lambda data: dict(old_collator(data))\n\nTrain the model\ndistilbert_trainer.train()\n\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nAccuracy\n\n\n\n\n\n\n1\n\n\n4.293800\n\n\n3.290489\n\n\n0.740968\n\n\n\n\n2\n\n\n2.634600\n\n\n1.883282\n\n\n0.832581\n\n\n\n\n3\n\n\n1.555400\n\n\n1.165018\n\n\n0.892581\n\n\n\n\n4\n\n\n1.018900\n\n\n0.863598\n\n\n0.910968\n\n\n\n\n5\n\n\n0.802800\n\n\n0.779555\n\n\n0.916129\n\n\n\n\n\n\n    TrainOutput(global_step=1590, training_loss=2.0571008596780165, metrics={'train_runtime': 62.8736, 'train_samples_per_second': 1212.75, 'train_steps_per_second': 25.289, 'total_flos': 413896353421488.0, 'train_loss': 2.0571008596780165, 'epoch': 5.0})\nNote: The student achieves a validation accuracy of nearly 92% compared to the teacher’s 94% accuracy.\n\nPush the trained model to Hugging Face Hub\ndistilbert_trainer.push_to_hub(\"Training completed!\")\n    'https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-clinc/commit/028b8f56cb944e1c7e1b8f4f6265c5beeddef127'\n\nLoad the fine-tuned student model into a text classification pipeline\nfinetuned_ckpt = \"cj-mills/distilbert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=finetuned_ckpt)\n\nBenchmark the student model\noptim_type = \"DistilBERT\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n    Model size (MB) - 255.89\n    Average latency (ms) - 12.44 +\\- 0.43\n    Accuracy on test set - 0.857\n\nimport matplotlib.pyplot as plt\n\nCompare the student performance metrics to the baseline model\nu'\\u25CC'\n    '◌'\n\ndef plot_metrics(perf_metrics, current_optim_type):\n    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n\n    for idx in df.index:\n        df_opt = df.loc[idx]\n        # Add a dashed circle around the current optimization type\n        if idx == current_optim_type:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx, \n                        marker='$\\u25CC$')\n        else:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n            \n    legend = plt.legend(bbox_to_anchor=(1,1))\n    for handle in legend.legendHandles:\n        handle.set_sizes([20])\n\n    plt.ylim(80,90)\n    # Use the slowest model to define the x-axis range\n    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n    plt.xlim(1, xlim)\n    plt.ylabel(\"Accuracy (%)\")\n    plt.xlabel(\"Average latency (ms)\")\n    plt.show()\n    \nplot_metrics(perf_metrics, optim_type)\n\n\n\n\n\nNote: The student is twice as fast and nearly as accurate.\n\n\n\nFinding Good Hyperparameters with Optuna\n\nOptuna: A Next-generation Hyperparameter Optimization Framework\nOptuna formulates the hyperparameter search problem in terms of an objective function optimized through multiple trials.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nThe Rosenbrock “banana function” of two variables\n\nThe Rosenbrock function is a famous test case for optimization.\nFinding the valley is easy, but converging to the global minimum is not.\n\n\ndef f(x, y):\n    return (1-x)**2+100*(y-x**2)**2\n\nPlot the banana function\nX, Y = np.meshgrid(np.linspace(-2, 2, 250), np.linspace(-1, 3, 250))\nZ = f(X,Y)\n_, ax = plt.subplots()\nax.plot([1], [1], 'x', mew=3, markersize=10, color=\"red\")\nax.contourf(X, Y, Z, np.logspace(-1, 3, 30), cmap='viridis', extend=\"both\")\nax.set_xlim(-1.3, 1.3)\nax.set_ylim(-0.9, 1.7)\nplt.show()\n\n\n\n\n\nNote: In Optuna, we can find the minimum of the \\(f(x,y)\\) function by defining an objective() function that returns the value of the \\(f(x,y)\\).\n\nDefine an objective function for the Rosenbrock function\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -2, 2)\n    y = trial.suggest_float(\"y\", -2, 2)\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\nNote: * The trial.suggest_float object specifies the parameter ranges to sample from uniformly. * Optuna collects multiple trials as a Study.\n\nimport optuna \n\n\noptuna.study.Study\n\nDocumentation\nA study corresponds to a set of trials for an optimization task.\nA study object provides interfaces to run a new Trial.\n\n\n\noptuna.create_study()\n\nCreate a new Study object.\n\n\nFind the best hyperparameters for the Rosenbrock function\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=1000)\nstudy.best_params\n    {'x': 0.9569346059991378, 'y': 0.920346631232987}\n\nX, Y = np.meshgrid(np.linspace(-2, 2, 250), np.linspace(-1, 3, 250))\nZ = f(X,Y)\n_, ax = plt.subplots()\nax.plot([study.best_params['x']], study.best_params['y'], 'x', mew=3, markersize=10, color=\"red\")\nax.contourf(X, Y, Z, np.logspace(-1, 3, 30), cmap='viridis', extend=\"both\")\nax.set_xlim(-1.3, 1.3)\nax.set_ylim(-0.9, 1.7)\nplt.show()\n\n\n\n\n\nNote: Optuna managed to find values for x and y that are reasonably close to the global minimum.\n\nDefine the hyperparameter space for \\(\\alpha\\) and \\(T\\)\ndef hp_space(trial):\n    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}\n\n\n\nTrainer.hyperparameter_search\n\nDocumentation\nLaunch an hyperparameter search using optuna or Ray Tune or SigOpt.\n\nRun the hyperparameter search\nbest_run = distilbert_trainer.hyperparameter_search(\n    n_trials=20, direction=\"maximize\", hp_space=hp_space)\ntype(best_run)\n    transformers.trainer_utils.BestRun\n\n\n\nBestRun\n\nSource Code\nStores the best run found by a hyperparameter search\n\n\nbest_run.hyperparameters\n    {'num_train_epochs': 10, 'alpha': 0.9901751316785802, 'temperature': 5}\n\nUpdate the training arguments with the new hyperparameter values\nfor k,v in best_run.hyperparameters.items():\n    setattr(student_training_args, k, v)\n\nDefine a new repository to store our distilled model\ndistilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\nstudent_training_args.output_dir = distilled_ckpt\n\nCreate a new Trainer with optimal parameters\ndistil_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/distilbert-base-uncased-distilled-clinc is already a clone of https://huggingface.co/cj-mills/distilbert-base-uncased-distilled-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n\nNote: Had to add the following workaround.\nold_collator = distil_trainer.data_collator\ndistil_trainer.data_collator = lambda data: dict(old_collator(data))\n\nTrain the model\ndistil_trainer.train();\n\n\n\n\n\nEpoch\n\n\nTraining Loss\n\n\nValidation Loss\n\n\nAccuracy\n\n\n\n\n\n\n1\n\n\n4.224600\n\n\n3.158392\n\n\n0.754516\n\n\n\n\n2\n\n\n2.403300\n\n\n1.565648\n\n\n0.865161\n\n\n\n\n3\n\n\n1.168400\n\n\n0.779509\n\n\n0.916129\n\n\n\n\n4\n\n\n0.569300\n\n\n0.465274\n\n\n0.932903\n\n\n\n\n5\n\n\n0.304200\n\n\n0.341210\n\n\n0.940645\n\n\n\n\n6\n\n\n0.179400\n\n\n0.291207\n\n\n0.940323\n\n\n\n\n7\n\n\n0.118400\n\n\n0.265375\n\n\n0.946129\n\n\n\n\n8\n\n\n0.087300\n\n\n0.255724\n\n\n0.943871\n\n\n\n\n9\n\n\n0.071900\n\n\n0.254949\n\n\n0.946452\n\n\n\n\n10\n\n\n0.064600\n\n\n0.252466\n\n\n0.946774\n\n\n\n\n\n\nNote: The student achieved over 94% accuracy despite having almost half the number of parameters of the teacher model.\n\nPush the trained model to Hugging Face Hub\ndistil_trainer.push_to_hub(\"Training complete\")\n    'https://huggingface.co/cj-mills/distilbert-base-uncased-distilled-clinc/commit/e4cee3ec87d5415df7ca130dfe1e75446de03b26'\n\n\n\n\nBenchmarking Our Distilled Model\nCreate a new text classification pipeline using the latest student model\ndistilled_ckpt = \"cj-mills/distilbert-base-uncased-distilled-clinc\"\npipe = pipeline(\"text-classification\", model=distilled_ckpt)\n\nBenchmark the latest student\noptim_type = \"Distillation\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n    Model size (MB) - 255.89\n    Average latency (ms) - 12.37 +\\- 0.35\n    Accuracy on test set - 0.887\nplot_metrics(perf_metrics, optim_type)\n\n\n\n\n\nNote: * The distillation student exceeds the baseline model performance. * The teacher model was likely not fine-tuned as systematically as the student."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#a-primer-on-floating-point-and-fixed-point-numbers",
    "href": "posts/transformers-book-notes/chapter-8/index.html#a-primer-on-floating-point-and-fixed-point-numbers",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "A Primer on Floating-Point and Fixed-Point Numbers",
    "text": "A Primer on Floating-Point and Fixed-Point Numbers\n\nMost transformers pre-train and fine-tune using FP32 or a mix of FP16 and FP32.\nThese floating-point data types provide the precision needed to accommodate the very different ranges of weights, activations, and gradients.\nA floating-point number like FP32 represents a sequence of 32 bits grouped in terms of a sign, exponent, and significand.\nThe sign determines whether the number is positive or negative.\nThe significand corresponds to the number of significant digits, scaled using the exponent in some fixed base (usually 2 for binary or 10 for decimal).\nWe can represent a wide range of real numbers through the exponent.\nThe decimal or binary point can go anywhere relative to the significant digits (hence the name “floating-point”).\nWe can reduce the precision of the data types after training without impacting the accuracy too much.\nIt is common to use a fixed-point format for the low-precision data types that represent real numbers as B-bit integers scaled by a common factor for all variables of the same data type.\n\nWe can represent the floating-point number \\(137.035\\) as the integer \\(137,035\\) scaled by \\(1/1000\\).\nWe control the range and precision of a fixed-point number by adjusting the scaling factor."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#making-models-faster-with-quantization",
    "href": "posts/transformers-book-notes/chapter-8/index.html#making-models-faster-with-quantization",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Making Models Faster with Quantization",
    "text": "Making Models Faster with Quantization\n\nQuantization makes computation more efficient by representing the weights and activations with low-precision data types like an 8-bit integer (INT8) instead of the usual 32-bit floating-point (FP32).\nReducing the number of bits means the model requires less memory, and operations like matrix multiplication are much faster with integer arithmetic.\nWe can quantize models with little to no impact on accuracy.\nWe “discretize” the floating-point values \\(f\\) in each tensor by mapping their range \\(\\left[ f_{max}, f_{min} \\right]\\) into a smaller one \\(\\left[ q_{max}, q_{min} \\right]\\) of fixed-point numbers \\(q\\) and linearly distributing all tensor values in between.\n\n\n\\[f = \\left( \\frac{f_{max} - f_{min}}{q_{max} - q_{min}} \\right)(q-Z) = S(q-Z)\\]\n\nwhere \\(S\\) is a positive floatin-point number and the constant \\(Z\\) has the same type as \\(q\\) and is called the zero point becaue it corresponds to the quentized value of the floating-point value \\(f=0\\)\nThe map needs to be affine (\\(y=Ax+b\\)) to get back floating-point numbers when we dequantize the fixed-point ones.\nTransformers and other deep neural networks are prime candidates for quantization because the weights and activations tend to take values in relatively small ranges.\n\nPlot the frequency distribution of values for a single attention weight matrix\nstate_dict = pipe.model.state_dict()\nweights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\nplt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\nplt.show()\n\n\n\n\n\nNote: The weight values fall in the range \\(\\left[ -0.1, 0.1 \\right]\\) around zero.\n\nCalculate the fixed-point scaling value\nzero_point = 0\nscale = (weights.max() - weights.min()) / (127 - (-128))\nscale\n    tensor(0.0053)\nNote: * The range of possible values for the integers is \\(\\left[ q_{max}, q_{min} \\right] = \\left[ -128, 127 \\right]\\) * The zero point coincides with the zero of FP32.\n\nQuantize a single weight matrix\n(weights / scale + zero_point).clamp(-128, 127).round().char()\n    tensor([[ -5,  -7,   0,  ...,  -6,  -4,   8],\n            [  9,   2,   1,  ...,  -4,   7,   0],\n            [ -9,  -6,   5,  ...,   1,   5,  -4],\n            ...,\n            [  5,   0,  12,  ...,   0,   6,  -1],\n            [  0,  -2, -12,  ...,  11,  -7, -13],\n            [-13,  -1,  -9,  ...,   8,   2,  -2]], dtype=torch.int8)\n\nfrom torch import quantize_per_tensor\n\n\nquantize_per_tensor\n\nDocumentation\nConvert a float tensor to a quantized tensor with a given scale and zero point.\n\nQuantize a single weight matrix using PyTorch\ndtype = torch.qint8\nquantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\nquantized_weights.int_repr()\n    tensor([[ -5,  -7,   0,  ...,  -6,  -4,   8],\n            [  9,   2,   1,  ...,  -4,   7,   0],\n            [ -9,  -6,   5,  ...,   1,   5,  -4],\n            ...,\n            [  5,   0,  12,  ...,   0,   6,  -1],\n            [  0,  -2, -12,  ...,  11,  -7, -13],\n            [-13,  -1,  -9,  ...,   8,   2,  -2]], dtype=torch.int8)\n\n\nfrom mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n\nPlot the effect of quantization on a transformer’s weights\n# Create histogram\nfig, ax = plt.subplots()\nax.hist(quantized_weights.dequantize().flatten().numpy(), \n         bins=250, range=(-0.3,0.3), edgecolor=\"C0\");\n# Create zoom inset\naxins = zoomed_inset_axes(ax, 5, loc='upper right')\naxins.hist(quantized_weights.dequantize().flatten().numpy(), \n         bins=250, range=(-0.3,0.3));\nx1, x2, y1, y2 = 0.05, 0.1, 500, 2500\naxins.set_xlim(x1, x2)\naxins.set_ylim(y1, y2)\naxins.axes.xaxis.set_visible(False)\naxins.axes.yaxis.set_visible(False)\nmark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\nplt.show()\n\n\n\n\n\n\nTime how long matrix multiplication takes with FP32.\n%%timeit \nweights @ weights\n    1.03 ms ± 2.14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\nfrom torch.nn.quantized import QFunctional\n\n\n\nQFunctional\n\nDocumentation\nA wrapper class for quantized operations\n\n\nq_fn = QFunctional()\n\nTime how long matrix multiplication takes with INT8.\n%%timeit\nq_fn.mul(quantized_weights, quantized_weights)\n    23.5 µs ± 179 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nNote: Using the INT8 tensors is significantly faster.\n\nimport sys\n\nCalculate the difference in storage requirements\nsys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())\n    3.999755879241598\nNote: * Quantization reduces memory storage requirements by up to a factor of four (32/8=4). * The actual compression rate for an entire model depends on which layers are quantized.\n\n\n\n\nApproaches to Quantization\n\nChanging the precision for all computations in the model introduces tiny but compounding disturbances in the model’s computational graph, which affect the model’s performance.\nThere are three main approaches for quantizing deep neural networks.\n\n\nDynamic Quantization\n\nDynamic quantization converts the weights and activations to INT8 after training completes.\nDynamic quantization happens on the fly, and we still read and write to memory the activations in floating-point format.\nThe conversion between integer and floating-point can be a performance bottleneck.\n\n\n\nStatic Quantization\n\nStatic quantization precomputes the quantization scheme by observing the activation patterns on a representative sample of the data ahead of inference time.\nStatic quantization enables us to skip the conversion between INT8 and FP32 values and speeds up the computations.\nStatic quantization requires access to an adequate data sample and introduces an additional step in the pipeline.\nStatic quantization does not address the discrepancy between the precision during training and inference, leading to a performance drop in the model’s metrics.\n\n\n\nQuantization-aware training\n\nQuantization-aware training simulates quantization during training by “fake” quantization of FP32 values.\nWe round the FP32 values to mimic the effect of quantization during the forward and backward passes.\nQuantization-aware training improves performance in terms of model metrics over static and dynamic quantization.\n\n\n\nWhat to choose\n\nDynamic quantization is the best approach for transformers as the main bottleneck for running inference is the compute and memory bandwidth associated with the enormous numbers of weights.\nThe limiting factor for smaller compute vision models is the memory bandwidth of the activations, making static quantization or quantization-aware training the best approach.\n\n\nfrom torch.quantization import quantize_dynamic\n\n\n\nquantize_dynamic\n\nDocumentation\nQuantize the weights of a floating-point model.\n\nQuantize the distilled student model\nmodel_ckpt = \"cj-mills/distilbert-base-uncased-distilled-clinc\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt).to(\"cpu\"))\n\nmodel_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n\nBenchmark the quantized model\npipe = pipeline(\"text-classification\", model=model_quantized, \n                tokenizer=tokenizer)\noptim_type = \"Distillation + quantization\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n    Model size (MB) - 132.40\n    Average latency (ms) - 5.33 +\\- 0.14\n    Accuracy on test set - 0.892\nplot_metrics(perf_metrics, optim_type)\n\n\n\n\n\nNote: The quantized model is nearly half the size of the distilled model and gained a slight accuracy boost."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#optimizing-inference-with-onnx-and-the-onnx-runtime",
    "href": "posts/transformers-book-notes/chapter-8/index.html#optimizing-inference-with-onnx-and-the-onnx-runtime",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Optimizing Inference with ONNX and the ONNX Runtime",
    "text": "Optimizing Inference with ONNX and the ONNX Runtime\n\nONNX is an open standard that defines a common set of operators and a common file format to represent deep learning models in different frameworks.\nThese operators are the building blocks for constructing a computational graph (often called an intermediate representation) for exported models.\nA computational graph represents the flow of data through the neural network.\nThe standardized operators and data types make it easy to switch between frameworks.\nONNX Runtime provides tools to optimize the ONNX graph through techniques like operator fusion and constant folding and defines an interface to execution providers that allow you to run the model on different types of hardware.\nA fused operator involves merging one operator (usually an activation function) into another, so they execute as a single step.\nConstant folding refers to evaluating constant expressions at compile time instead of runtime.\n\n\nOther Accelerators\n\nNVIDIA TensorRT\nApache TVM\nIntel OpenVINO Toolkit\n\n\n\nConvert model to ONNX format\n\nimport os\nfrom psutil import cpu_count\n\n\nOpenMP\n\nHomepage\nThe OpenMP API supports multi-platform shared-memory parallel programming in C/C++ and Fortran.\n\nSet OpenMP environment variables\nos.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\nos.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\"\nNote: * The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel computations in the ONNX runtime. * OMP_WAIT_POLICY=ACTIVE specifies that waiting threads should be active.\n\nfrom transformers.convert_graph_to_onnx import convert\n\n\n\ntransformers.convert_graph_to_onnx.convert()\n\nSource Code\nConvert a pipeline object to the ONNX Intermediate Representation (IR) format\nThe Hugging Face Transformers library provides a function called convert_graph_to_onnx.convert() that simplifies the process by taking the following steps:\n\nInitialize the model as a Pipeline.\nRun placeholder inputs through the pipeline so that ONNX can record the computational graph.\nDefine dynamic axes to handle dynamic sequence lengths.\nSave the graph with network parameters.\n\n\nConvert the distilled model to ONNX format using a text classification pipeline\nmodel_ckpt = \"cj-mills/distilbert-base-uncased-distilled-clinc\"\nonnx_model_path = Path(\"onnx/model.onnx\")\nconvert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer, \n        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")\n    ONNX opset version set to: 12\n    Loading pipeline (model: cj-mills/distilbert-base-uncased-distilled-clinc, tokenizer: PreTrainedTokenizerFast(name_or_path='cj-mills/distilbert-base-uncased-distilled-clinc', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))\n\n\n    /home/innom-dt/miniconda3/envs/transformer-book/lib/python3.9/site-packages/transformers/convert_graph_to_onnx.py:378: FutureWarning: The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of Transformers\n      warnings.warn(\n\n\n    Creating folder onnx\n    Using framework PyTorch: 1.11.0\n    Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n    Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n    Found output output_0 with shape: {0: 'batch'}\n    Ensuring inputs are in correct order\n    head_mask is not present in the generated input list.\n    Generated inputs order: ['input_ids', 'attention_mask']\n\nfrom onnxruntime import (GraphOptimizationLevel, InferenceSession, \n                         SessionOptions)\n\nDefine a function to create an InferenceSession\ndef create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"): \n    options = SessionOptions()\n    options.intra_op_num_threads = 1\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n    session = InferenceSession(str(model_path), options, providers=[provider])\n    session.disable_fallback()\n    return session\n\nCreate an inference session using the exported model\nonnx_model = create_model_for_provider(onnx_model_path)\n# onnx_model = create_model_for_provider(onnx_model_path, provider=\"CUDAExecutionProvider\")\n\nGet the class logits from the ONNX model\ninputs = clinc_enc[\"test\"][:1]\ndel inputs[\"labels\"]\nlogits_onnx = onnx_model.run(None, inputs)[0]\nlogits_onnx.shape\n    (1, 151)\n\nGet the most confident prediction\nnp.argmax(logits_onnx)\n    61\n\nCompare prediction to ground truth label\nclinc_enc[\"test\"][0][\"labels\"]\n61\nNote: The model prediction matches the ground truth.\n\n\n\n\nCreate Custom Pipeline\n\nThe ONNX model is not compatible with the text classification pipeline so we need to mimic the pipeline’s core behavior.\n\n\nfrom scipy.special import softmax\n\nDefine a custom pipeline class\nclass OnnxPipeline:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        \n    def __call__(self, query):\n        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n        inputs_onnx = {k: v.cpu().detach().numpy() \n                       for k, v in model_inputs.items()}\n        logits = self.model.run(None, inputs_onnx)[0][0, :]\n        probs = softmax(logits)\n        pred_idx = np.argmax(probs).item()\n        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]\n\nTest the custom pipeline\npipe = OnnxPipeline(onnx_model, tokenizer)\npipe(query)\n    [{'label': 'car_rental', 'score': 0.9709836}]\n\nDefine a performance benchmark class for ONNX models\nclass OnnxPerformanceBenchmark(PerformanceBenchmark):\n    def __init__(self, *args, model_path, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_path = model_path\n        \n    # Override the PerformanceBenchmark.compute_size() method\n    def compute_size(self):\n        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n        print(f\"Model size (MB) - {size_mb:.2f}\")\n        return {\"size_mb\": size_mb}\n\nBenchmark the ONNX Model\noptim_type = \"Distillation + ORT\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n                              model_path=\"onnx/model.onnx\")\nperf_metrics.update(pb.run_benchmark())\n    Model size (MB) - 255.90\n    Average latency (ms) - 10.42 +\\- 0.29\n    Accuracy on test set - 0.887\nplot_metrics(perf_metrics, optim_type)\n\n\n\n\n\nNote: Converting the distilled model to ONNX format decreased latency.\n\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\nhelp(quantize_dynamic)\n    Help on function quantize_dynamic in module onnxruntime.quantization.quantize:\n    \n    quantize_dynamic(model_input: pathlib.Path, model_output: pathlib.Path, op_types_to_quantize=[], per_channel=False, reduce_range=False, weight_type=&lt;QuantType.QInt8: 0&gt;, nodes_to_quantize=[], nodes_to_exclude=[], optimize_model=True, use_external_data_format=False, extra_options={})\n            Given an onnx model, create a quantized onnx model and save it into a file\n        :param model_input: file path of model to quantize\n        :param model_output: file path of quantized model\n        :param op_types_to_quantize: specify the types of operators to quantize, like ['Conv'] to quantize Conv only. It quantizes all supported operators by default\n        :param per_channel: quantize weights per channel\n        :param reduce_range: quantize weights with 7-bits. It may improve the accuracy for some models running on non-VNNI machine, especially for per-channel mode\n        :param nbits: number of bits to represent quantized data. Currently only supporting 8-bit types\n        :param activation_type: quantization data type of activation. Please refer to https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection\n        :param weight_type: quantization data type of weight. Please refer to https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection\n        :param nodes_to_quantize:\n            List of nodes names to quantize. When this list is not None only the nodes in this list\n            are quantized.\n            example:\n            [\n                'Conv__224',\n                'Conv__252'\n            ]\n        :param nodes_to_exclude:\n            List of nodes names to exclude. The nodes in this list will be excluded from quantization\n            when it is not None.\n        :parma use_external_data_format: option used for large size (&gt;2GB) model. Set to False by default.\n            :param extra_options:\n            key value pair dictionary for various options in different case. Current used:\n                extra.Sigmoid.nnapi = True/False  (Default is False)\n                ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).\n                WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).\n                EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.\n                                              Dyanmic mode currently is supported. Will support more in future.\n                DisableShapeInference = True/False : in dynamic quantize mode, shape inference is not must have\n                                                     and if it cause some issue, you could disable it.\n                ForceQuantizeNoInputCheck = True/False : By default, some latent operators like maxpool, transpose, do not quantize\n                                                         if their input is not quantized already. Setting to True to force such operator\n                                                         always quantize input and so generate quantized output. Also the True behavior\n                                                         could be disabled per node using the nodes_to_exclude.\n                MatMulConstBOnly = True/False: Default is True for dynamic mode. If enabled, only MatMul with const B will be quantized.\n\nQuantize the ONNX model\nmodel_input = \"onnx/model.onnx\"\nmodel_output = \"onnx/model.quant.onnx\"\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)\n\nBenchmark Quantized ONNX Model\nonnx_quantized_model = create_model_for_provider(model_output)\npipe = OnnxPipeline(onnx_quantized_model, tokenizer)\noptim_type = \"Distillation + ORT (quantized)\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type, \n                              model_path=model_output)\nperf_metrics.update(pb.run_benchmark())\n    Model size (MB) - 64.22\n    Average latency (ms) - 3.39 +\\- 0.25\n    Accuracy on test set - 0.893\nplot_metrics(perf_metrics, optim_type)\n\n\n\n\n\nNote: * The quantized ONNX model further reduced latency and improved accuracy compared to the quantized PyTorch model. * PyTorch only optimizes the nn.Linear modules while ONNX also quantized the embedding layer."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#making-models-sparser-with-weight-pruning",
    "href": "posts/transformers-book-notes/chapter-8/index.html#making-models-sparser-with-weight-pruning",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "Making Models Sparser with Weight Pruning",
    "text": "Making Models Sparser with Weight Pruning\n\nNeural Networks Block Movement Pruning\n\nA Hugging Face library for pruning a model while finetuning or training.\n\nApplications that run on mobile and edge devices can have significant memory constraints.\nWeight pruning gradually removes weight connections (and potentially neurons) during training such that the model becomes progressively sparser.\nThe resulting pruned model has fewer nonzero parameters, which we can store in a compact sparse matrix format.\nWe can combine pruning with quantization to obtain further compression.\n\n\nSparsity in Deep Neural Networks\n\n\nWeight Pruning Methods\n\nMost weight pruning methods calculate a matrix \\(S\\) of importance scores and select the top \\(k\\) percent of weights by importance.\n\n\n\n\\[Top_{k}(S)_{ij} = 1 \\text{ if } S_{ij} \\text{ in top k percent else } 0\\]\n\n\\(k\\) acts as a new hyperparameter to control the amount of sparsity in the model.\nLower values of k correspond to sparser matrices.\nWe can use these scores to define a mask matrix \\(M\\) that masks weights \\(W_{ik}\\) during the forward pass with some input and effectively creates a sparse network of activations \\(a_{i}\\).\n\n\n\n\\[a_{i} = \\sum_{k}{W_{ik}M_{ik}x_{k}}\\]\n\nQuestions to consider\n\nSecond order derivatives for network pruning: Optimal Brain Surgeon\nWhich weights should be pruned?\nHow should the remaining weights be adjuststed for best performance?\nHow can such network pruning be done in a computationally efficient way?\n\n\n\nMagnitude pruning\n\nMagnitude pruning calculates the scores according to the magnitude of the weights \\[S = \\left( \\left \\vert W_{ij} \\right \\vert \\right)_{1 \\ \\le \\ j, j \\ \\le \\ n}\\] and then derives the masks \\[M = Top_{k}(S)\\].\nIt is common to apply magnitude iteratively by first training the model to learn which connections are important and pruning weights of least importance.\n\nLearning both Weights and Connections for Efficient Neural Networks\n\nIt is generally better to gradually increase the initial sparsity \\(s_{i}\\) to a final value \\(s_{f}\\) after \\(N\\) steps.\n\nTo prune, or not to prune: exploring the efficacy of pruning for model compression\n\n\n\n\n\n\\[s_{t} = s_{f} + \\left( s_{i} - s_{f} \\right) \\left( 1 - \\frac{t - t_{0}}{N\\Delta t} \\right)^{3} for t \\in \\left\\{ t_{0},t_{0} + \\Delta t, \\ldots, t_{0} + N\\Delta t \\right\\}\\]\n\nThe idea is to update the binary masks \\(M\\) every \\(\\Delta t\\) step to allow masked weights to reactivate during training and recover from any potential accuracy losses introduced by the pruning process.\nThe cubic factor implies the rate of pruning is highest in the early phases and gradually tapers off.\nMagnitude pruning works for purely supervised learning, where the importance of each weight directly relates to the task at hand.\nIn transfer learning, the pretraining phase determines the importance of the weights, and magnitude pruning can remove connections needed for the fine-tuning task.\n\nPlot the cubic sparsity scheduler used for pruning\ndef _sparsity(t, t_0=0, dt=1, s_i=0, s_f=0.9, N=100):\n    return s_f + (s_i - s_f) * (1 - (t - t_0) / (N * dt))**3\n\nsteps = np.linspace(0,100,100)\nvalues = [_sparsity(t) for t in steps]\n\nfig, ax = plt.subplots()\nax.plot(steps, values)\nax.set_ylim(0,1)\nax.set_xlim(0,100)\nax.set_xlabel(\"Pruning step\")\nax.set_ylabel(\"Sparsity\")\nplt.grid(linestyle=\"dashed\")\nplt.show()\n\n\n\n\n\n\n\nMovement pruning\n\nMovement Pruning: Adaptive Sparsity by Fine-Tuning\nMovement pruning gradually removes weights during fine-tuning such that the model becomes progressively sparser.\nWe derive both the weights and scores through gradient descent during fine-tuning, meaning we also track the loss \\(L\\) for the scores \\(S_{ij}\\) in the backward pass.\nWe can then use the learned scores to generate the binary mask.\n\n\n\n\n\\[M = Top_{k}(S)\\]\n\nThe weights moving the most from zero are the most important ones to keep.\nThere is also a soft version of movement pruning where we use a global threshold \\(\\tau\\) to define the binary mask: \\(M = \\left( S \\gt \\tau \\right)\\)."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-8/index.html#references",
    "href": "posts/transformers-book-notes/chapter-8/index.html#references",
    "title": "Notes on Transformers Book Ch. 8",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 7\nNext: Notes on Transformers Book Ch. 9"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html",
    "href": "posts/transformers-book-notes/chapter-9/index.html",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "",
    "text": "Dealing with Few to No Labels\nProject: Build a GitHub Issues Tagger\nImplementing a Naive Bayesline\nWorking with No Labeled Data\nWorking with a Few Labels\nLeveraging Unlabeled Data\nConclusion\nReferences"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#dealing-with-few-to-no-labels",
    "href": "posts/transformers-book-notes/chapter-9/index.html#dealing-with-few-to-no-labels",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Dealing with Few to No Labels",
    "text": "Dealing with Few to No Labels\n\nWe often have little to no labeled data when starting a new project.\nNon-pretrained models do not perform well with little data.\nAnnotating additional training examples is time-consuming and expensive.\nThere are several methods for dealing with few to no labels.\nZero-shot learning often sets a strong baseline when there is no labeled data.\nStandard fine-tuning works well when there is a lot of labeled data.\nWe can fine-tune a language model on a large corpus of unlabeled data before training a classifier on a small number of labeled examples.\nMore sophisticated methods for training with unlabeled data include Unsupervised Data Augmentation and Uncertainty-aware self-training.\n\nUncertainty-aware Self-training for Text Classification with Few Labels\n\nWe can use few-shot learning when we only have a small number of labeled examples and no unlabeled data.\nWe can also use the embeddings from a pretrained language model to perform lookups with a nearest-neighbor search."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#project-build-a-github-issues-tagger",
    "href": "posts/transformers-book-notes/chapter-9/index.html#project-build-a-github-issues-tagger",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Project: Build a GitHub Issues Tagger",
    "text": "Project: Build a GitHub Issues Tagger\n\nMany support teams use issue trackers like Jira or GitHub to assist users by tagging issues with metadata based on the issue’s description.\nTags can define the issue type, the product causing the problem, or which team is responsible for handling the reported issue.\nAutomating issue tagging can significantly improve productivity and enables the support teams to focus on helping users.\nThe goal is to train a model that automatically tags GitHub issues for the Hugging Face Transformers library.\nGitHub issues contain a title, a description, and a set of tags/labels that characterize them.\nThe model will take a title and description as input and predict one or more labels (i.e., multilabel classification).\n\n\nGetting the Data\n\nWe can use the GitHub REST API to poll the Issues endpoint.\nThe Issues endpoint returns a list of JSON objects.\nEach JSON object includes whether it is open or closed, who opened the issue, the title, the body, and the labels.\nThe GitHub REST API treats pull requests as issues.\n\n\nimport time\nimport math\nimport requests\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nDefine a function to download issues for a GitHub project to a .jsonl file\n\nWe need to download the issues in batches to avoid exceeding GitHub’s limit on the number of requests per hour.\n\n\ndef fetch_issues(owner=\"huggingface\", repo=\"transformers\", num_issues=10_000, \n                 rate_limit=5_000):    \n    batch = []\n    all_issues = []\n    # Max number of issues we can request per page\n    per_page = 100\n    # Number of requests we need to make\n    num_pages = math.ceil(num_issues / per_page)\n    base_url = \"https://api.github.com/repos\"\n    \n    for page in tqdm(range(num_pages)):\n        # Query with state=all to get both open and closed issues\n        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n        # Sample: https://api.github.com/repos/huggingface/transformers/issues?page=0&per_page=100&state=all\n        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\")\n        batch.extend(issues.json())\n\n        if len(batch) &gt; rate_limit and len(all_issues) &lt; num_issues:\n            all_issues.extend(batch)\n            batch = [] # Flush batch for next time period\n            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n            time.sleep(60 * 60 + 1)\n            \n    all_issues.extend(batch)\n    df = pd.DataFrame.from_records(all_issues)\n    df.to_json(f\"github-issues-{repo}.jsonl\", orient=\"records\", lines=True)\nNote: It takes a while to fetch all the issues.\n\nDownload the GitHub Issues\n# fetch_issues()\n\n\n\nPreparing the Data\n\nimport pandas as pd\npd.set_option('max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.__version__\n    '1.4.2'\n\nImport the dataset\ndataset_url = \"https://git.io/nlp-with-transformers\"\ndf_issues = pd.read_json(dataset_url, lines=True)\nprint(f\"DataFrame shape: {df_issues.shape}\")\nDataFrame shape: (9930, 26)\n\nInspect a single GitHub issue\n# Convert Series to DataFrame\ndf_issues.loc[2].to_frame()\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\nurl\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/11044\n\n\n\n\nrepository_url\n\n\nhttps://api.github.com/repos/huggingface/transformers\n\n\n\n\nlabels_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/11044/labels{/name}\n\n\n\n\ncomments_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/11044/comments\n\n\n\n\nevents_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/11044/events\n\n\n\n\nhtml_url\n\n\nhttps://github.com/huggingface/transformers/issues/11044\n\n\n\n\nid\n\n\n849529761\n\n\n\n\nnode_id\n\n\nMDU6SXNzdWU4NDk1Mjk3NjE=\n\n\n\n\nnumber\n\n\n11044\n\n\n\n\ntitle\n\n\n[DeepSpeed] ZeRO stage 3 integration: getting started and issues\n\n\n\n\nuser\n\n\n{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n\n\nlabels\n\n\n[{‘id’: 2659267025, ‘node_id’: ‘MDU6TGFiZWwyNjU5MjY3MDI1’, ‘url’: ‘https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed’, ‘name’: ‘DeepSpeed’, ‘color’: ‘4D34F7’, ‘default’: False, ‘description’: ’’}]\n\n\n\n\nstate\n\n\nopen\n\n\n\n\nlocked\n\n\nFalse\n\n\n\n\nassignee\n\n\n{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n\n\nassignees\n\n\n[{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]\n\n\n\n\nmilestone\n\n\nNaN\n\n\n\n\ncomments\n\n\n0\n\n\n\n\ncreated_at\n\n\n2021-04-02 23:40:42\n\n\n\n\nupdated_at\n\n\n2021-04-03 00:00:18\n\n\n\n\nclosed_at\n\n\nNaT\n\n\n\n\nauthor_association\n\n\nCOLLABORATOR\n\n\n\n\nactive_lock_reason\n\n\nNone\n\n\n\n\nbody\n\n\n[This is not yet alive, preparing for the release, so please ignore for now]DeepSpeed ZeRO-3 has been integrated into HF transformers. I tried to write tests for a wide range of situations I’m sure I’ve missed some scenarios so if you run into any problems please file a separate issue. I’m going to use this issue to track progress on individual ZeRO3 issues.# Why would you want ZeRO-3a few words, while ZeRO-2 was very limited scability-wise - if model.half() couldn’t fit onto a single gpu, adding more gpus won’t have helped so if you had a 24GB GPU you couldn’t train a model larger than about 5B params.with ZeRO-3 the model weights are partitioned across multiple GPUs plus offloaded to CPU, the upper limit on model size has increased by about 2 orders of magnitude. That is ZeRO-3 allows you to scale to huge models with Trillions of parameters assuming you have enough GPUs and general RAM to support this. ZeRO-3 can benefit a lot from general RAM if you have it. If not that’s OK too. ZeRO-3 combines all your GPUs memory and general RAM into a vast pool of memory.you don’t have many GPUs but just a single one but have a lot of general RAM ZeRO-3 will allow you to fit larger models.course, if you run in an environment like the free google colab, while you can use run Deepspeed there, you get so little general RAM it’s very hard to make something out of nothing. Some users (or some sessions) one gets 12GB of RAM which is impossible to work with - you want at least 24GB instances. Setting is up might be tricky too, please see this notebook for an example:://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb# Getting startedthe latest deepspeed version:\\r\\npip install deepspeed\\r\\nwill want to be on a transformers master branch, if you want to run a quick test:\\r\\ngit clone https://github.com/huggingface/transformers\\r\\ncd transformers\\r\\nBS=4; PYTHONPATH=src USE_TF=0 deepspeed examples/seq2seq/run_translation.py \\\\r\\n--model_name_or_path t5-small --output_dir /tmp/zero3 --overwrite_output_dir --max_train_samples 64 \\\\r\\n--max_val_samples 64 --max_source_length 128 --max_target_length 128 --val_max_target_length 128 \\\\r\\n--do_train --num_train_epochs 1 --per_device_train_batch_size $BS --per_device_eval_batch_size $BS \\\\r\\n--learning_rate 3e-3 --warmup_steps 500 --predict_with_generate --logging_steps 0 --save_steps 0 \\\\r\\n--eval_steps 1 --group_by_length  --adafactor --dataset_name wmt16 --dataset_config ro-en --source_lang en \\\\r\\n--target_lang ro --source_prefix \"translate English to Romanian: \" \\\\r\\n--deepspeed examples/tests/deepspeed/ds_config_zero3.json\\r\\nwill find a very detailed configuration here: https://huggingface.co/transformers/master/main_classes/trainer.html#deepspeednew config file will look like this:json\\r\\n{\\r\\n    \"fp16\": {\\r\\n        \"enabled\": true,\\r\\n        \"loss_scale\": 0,\\r\\n        \"loss_scale_window\": 1000,\\r\\n        \"initial_scale_power\": 16,\\r\\n        \"hysteresis\": 2,\\r\\n        \"min_loss_scale\": 1\\r\\n    },\\r\\n\\r\\n    \"zero_optimization\": {\\r\\n        \"stage\": 3,\\r\\n        \"cpu_offload\": true,\\r\\n        \"cpu_offload_params\": true,\\r\\n        \"cpu_offload_use_pin_memory\" : true,\\r\\n        \"overlap_comm\": true,\\r\\n        \"contiguous_gradients\": true,\\r\\n        \"stage3_max_live_parameters\": 1e9,\\r\\n        \"stage3_max_reuse_distance\": 1e9,\\r\\n        \"stage3_prefetch_bucket_size\": 0.94e6,\\r\\n        \"stage3_param_persistence_threshold\": 1e4,\\r\\n        \"reduce_bucket_size\": 1e6,\\r\\n        \"prefetch_bucket_size\": 3e6,\\r\\n        \"sub_group_size\": 1e14,\\r\\n        \"stage3_gather_fp16_weights_on_model_save\": true\\r\\n    },\\r\\n\\r\\n    \"optimizer\": {\\r\\n        \"type\": \"AdamW\",\\r\\n        \"params\": {\\r\\n            \"lr\": 3e-5,\\r\\n            \"betas\": [0.8, 0.999],\\r\\n            \"eps\": 1e-8,\\r\\n            \"weight_decay\": 3e-7\\r\\n        }\\r\\n    },\\r\\n\\r\\n    \"scheduler\": {\\r\\n        \"type\": \"WarmupLR\",\\r\\n        \"params\": {\\r\\n            \"warmup_min_lr\": 0,\\r\\n            \"warmup_max_lr\": 3e-5,\\r\\n            \"warmup_num_steps\": 500\\r\\n        }\\r\\n    },\\r\\n\\r\\n    \"steps_per_print\": 2000,\\r\\n    \"wall_clock_breakdown\": false\\r\\n}\\r\\n\\r\\nif you were already using ZeRO-2 it’s only the zero_optimization stage that has changed.of the biggest nuances of ZeRO-3 is that the model weights aren’t inside model.state_dict, as they are spread out through multiple gpus. The Trainer has been modified to support this but you will notice a slow model saving - as it has to consolidate weights from all the gpus. I’m planning to do more performance improvements in the future PRs, but for now let’s focus on making things work.# Issues / Questionsyou have any general questions or something is unclear/missing in the docs please don’t hesitate to ask in this thread. But for any bugs or problems please open a new Issue and tag me there. You don’t need to tag anybody else. Thank you!\n\n\n\n\nperformed_via_github_app\n\n\nNaN\n\n\n\n\npull_request\n\n\nNone\n\n\n\n\n\n\nNote: The labels column contains the tags.\n\nInspect the labels column\npd.DataFrame(df_issues.loc[2]['labels'])\n\n\n\n\n\n\n\n\nid\n\n\nnode_id\n\n\nurl\n\n\nname\n\n\ncolor\n\n\ndefault\n\n\ndescription\n\n\n\n\n\n\n0\n\n\n2659267025\n\n\nMDU6TGFiZWwyNjU5MjY3MDI1\n\n\nhttps://api.github.com/repos/huggingface/transformers/labels/DeepSpeed\n\n\nDeepSpeed\n\n\n4D34F7\n\n\nFalse\n\n\n\n\n\n\n\n\n\nExtract the tags names from the labels column\ndf_issues[\"labels\"] = (df_issues[\"labels\"].apply(lambda x: [meta[\"name\"] for meta in x]))\ndf_issues[[\"labels\"]].head()\n\n\n\n\n\n\n\n\nlabels\n\n\n\n\n\n\n0\n\n\n[]\n\n\n\n\n1\n\n\n[]\n\n\n\n\n2\n\n\n[DeepSpeed]\n\n\n\n\n3\n\n\n[]\n\n\n\n\n4\n\n\n[]\n\n\n\n\n\n\n\nGet the number of labels per issue\ndf_issues[\"labels\"].apply(lambda x : len(x)).value_counts().to_frame().T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nlabels\n\n\n6440\n\n\n3057\n\n\n305\n\n\n100\n\n\n25\n\n\n3\n\n\n\n\n\n\nNote: Most GitHub issues have zero or one label, and very few have more than one label.\n\nView the only three issues with five tags\ndf_issues[df_issues['labels'].apply(lambda x: len(x) == 5)].T\n\n\n\n\n\n\n\n\n6005\n\n\n7541\n\n\n8266\n\n\n\n\n\n\nurl\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/5057\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/3513\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/2787\n\n\n\n\nrepository_url\n\n\nhttps://api.github.com/repos/huggingface/transformers\n\n\nhttps://api.github.com/repos/huggingface/transformers\n\n\nhttps://api.github.com/repos/huggingface/transformers\n\n\n\n\nlabels_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/5057/labels{/name}\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/3513/labels{/name}\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/2787/labels{/name}\n\n\n\n\ncomments_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/5057/comments\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/3513/comments\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/2787/comments\n\n\n\n\nevents_url\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/5057/events\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/3513/events\n\n\nhttps://api.github.com/repos/huggingface/transformers/issues/2787/events\n\n\n\n\nhtml_url\n\n\nhttps://github.com/huggingface/transformers/issues/5057\n\n\nhttps://github.com/huggingface/transformers/issues/3513\n\n\nhttps://github.com/huggingface/transformers/issues/2787\n\n\n\n\nid\n\n\n639635502\n\n\n589781536\n\n\n562124488\n\n\n\n\nnode_id\n\n\nMDU6SXNzdWU2Mzk2MzU1MDI=\n\n\nMDU6SXNzdWU1ODk3ODE1MzY=\n\n\nMDU6SXNzdWU1NjIxMjQ0ODg=\n\n\n\n\nnumber\n\n\n5057\n\n\n3513\n\n\n2787\n\n\n\n\ntitle\n\n\nExamples tests improvements\n\n\nAdding mbart-large-cc25\n\n\nDistillation code loss functions\n\n\n\n\nuser\n\n\n{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n{‘login’: ‘maksym-del’, ‘id’: 8141935, ‘node_id’: ‘MDQ6VXNlcjgxNDE5MzU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/8141935?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/maksym-del’, ‘html_url’: ‘https://github.com/maksym-del’, ‘followers_url’: ‘https://api.github.com/users/maksym-del/followers’, ‘following_url’: ‘https://api.github.com/users/maksym-del/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/maksym-del/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/maksym-del/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/maksym-del/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/maksym-del/orgs’, ‘repos_url’: ‘https://api.github.com/users/maksym-del/repos’, ‘events_url’: ‘https://api.github.com/users/maksym-del/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/maksym-del/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n{‘login’: ‘snaik2016’, ‘id’: 18183245, ‘node_id’: ‘MDQ6VXNlcjE4MTgzMjQ1’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/18183245?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/snaik2016’, ‘html_url’: ‘https://github.com/snaik2016’, ‘followers_url’: ‘https://api.github.com/users/snaik2016/followers’, ‘following_url’: ‘https://api.github.com/users/snaik2016/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/snaik2016/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/snaik2016/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/snaik2016/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/snaik2016/orgs’, ‘repos_url’: ‘https://api.github.com/users/snaik2016/repos’, ‘events_url’: ‘https://api.github.com/users/snaik2016/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/snaik2016/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n\n\nlabels\n\n\n[Examples, Good First Issue, Help wanted, cleanup, wontfix]\n\n\n[Documentation, Help wanted, New model, seq2seq, translation]\n\n\n[Core: Modeling, Distillation, PyTorch, Usage, wontfix]\n\n\n\n\nstate\n\n\nclosed\n\n\nclosed\n\n\nclosed\n\n\n\n\nlocked\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\nassignee\n\n\n{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\n{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}\n\n\nNone\n\n\n\n\nassignees\n\n\n[{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]\n\n\n[{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]\n\n\n[]\n\n\n\n\nmilestone\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\ncomments\n\n\n12\n\n\n8\n\n\n4\n\n\n\n\ncreated_at\n\n\n2020-06-16 12:45:32\n\n\n2020-03-29 12:32:30\n\n\n2020-02-09 05:21:33\n\n\n\n\nupdated_at\n\n\n2020-10-04 01:14:08\n\n\n2020-07-07 17:23:01\n\n\n2020-04-19 22:29:10\n\n\n\n\nclosed_at\n\n\n2020-10-04 01:14:08\n\n\n2020-07-07 17:23:01\n\n\n2020-04-19 22:29:10\n\n\n\n\nauthor_association\n\n\nMEMBER\n\n\nCONTRIBUTOR\n\n\nNONE\n\n\n\n\nactive_lock_reason\n\n\nNone\n\n\nNone\n\n\nNone\n\n\n\n\nbody\n\n\nThere are a few things about the examples/ tests that are suboptimal:. They never use cuda or fp16, even if they are available.. The @slow decorator used in the main tests is not importable, so there are no @slow tests.. test_run_glue uses distilbert-case-cased. It should use a smaller model, one of the tiny family here or a new tiny model.. There is no test coverage for TPU.help on any of these fronts would be much appreciated!\n\n\n# 🌟 New model additionBART model implemented in fairseq introduced by FAIR## Model descriptionissue is to request adding mBART model existing as a part of fairseq lib. (https://github.com/pytorch/fairseq/tree/master/examples/mbart)(https://arxiv.org/abs/2001.08210)pretrained BART checkpoint.&lt;!– Important information –&gt;model code follows the original BART model code which is already a part of transformers repo. However, it introduces a couple more features like multilingual denoising and translation from pretrained BART. ## Open source status- [x] the model implementation is available: (give details)(https://github.com/pytorch/fairseq/commit/5e79322b3a4a9e9a11525377d3dda7ac520b921c) PR shows the main pieces that were added to the fairseq to make mBART work considering BART which is already existing in the codebase. However, a few additional mBART commits were added afterward.- [x] the model weights are available: (give details)(https://github.com/pytorch/fairseq/tree/master/examples/mbart#pre-trained-models)- [x] who are the authors: (mention them, if possible by @gh-username)AI Research (@MultiPath)\n\n\n# ❓ Questions & Helpcompute cross entropy loss from the hard labels in distillation code?self.alpha_clm &gt; 0.0:shift_logits = s_logits[…, :-1, :].contiguous()shift_labels = lm_labels[…, 1:].contiguous()loss_clm = self.lm_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))loss += self.alpha_clm * loss_clmmodel outputs loss when passed with the labels.&lt;!– The GitHub issue tracker is primarly intended for bugs, feature requests,new models and benchmarks, and migration questions. For all other questions,we direct you to Stack Overflow (SO) where a whole community of PyTorch andTensorflow enthusiast can help you out. Make sure to tag your question with theright deep learning framework as well as the huggingface-transformers tag: https://stackoverflow.com/questions/tagged/huggingface-transformers If your question wasn’t answered after a period of time on Stack Overflow, youcan always open a question on GitHub. You should then link to the SO question that you posted.–&gt;## Details&lt;!– Description of your issue –&gt;&lt;!– You should first ask your question on SO, and only ifyou didn’t get an answer ask it here on GitHub. –&gt;*A link to original question on Stack Overflow**:\n\n\n\n\nperformed_via_github_app\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\npull_request\n\n\nNone\n\n\nNone\n\n\nNone\n\n\n\n\n\n\n\n\npandas.DataFrame.explode\n\nDocumentation\nTransform each element of a list-like into a row, replicating index values.\n\nGet the 20 most frequent labels in the dataset\ndf_counts = df_issues[\"labels\"].explode().value_counts()\nprint(f\"Number of labels: {len(df_counts)}\")\n# Display the top-20 label categories\ndf_counts.to_frame().head(20)\n    Number of labels: 65\n\n\n\n\n\n\n\n\nlabels\n\n\n\n\n\n\nwontfix\n\n\n2284\n\n\n\n\nmodel card\n\n\n649\n\n\n\n\nCore: Tokenization\n\n\n106\n\n\n\n\nNew model\n\n\n98\n\n\n\n\nCore: Modeling\n\n\n64\n\n\n\n\nHelp wanted\n\n\n52\n\n\n\n\nGood First Issue\n\n\n50\n\n\n\n\nUsage\n\n\n46\n\n\n\n\nCore: Pipeline\n\n\n42\n\n\n\n\nFeature request\n\n\n41\n\n\n\n\nTensorFlow\n\n\n41\n\n\n\n\nTests\n\n\n40\n\n\n\n\nPyTorch\n\n\n37\n\n\n\n\nDeepSpeed\n\n\n33\n\n\n\n\nseq2seq\n\n\n32\n\n\n\n\nShould Fix\n\n\n30\n\n\n\n\nmarian\n\n\n29\n\n\n\n\nDiscussion\n\n\n28\n\n\n\n\nDocumentation\n\n\n28\n\n\n\n\nExamples\n\n\n24\n\n\n\n\n\n\n\ndf_counts[:2].sum() / df_counts.sum()\n    0.7185203331700147\nNote: * There are 65 unique tags (i.e., classes) in the dataset. * The dataset is highly imbalanced, with the two most common classes accounting for more than 70% of the dataset. * Some labels (e.g., “Good First” or “Help Wanted”) are potentially too difficult to predict from the issue’s description, while others (e.g., “model card”) might only require simple rules to classify.\n\nFilter the dataset to a subset of labels\nlabel_map = {\"Core: Tokenization\": \"tokenization\",\n             \"New model\": \"new model\",\n             \"Core: Modeling\": \"model training\",\n             \"Usage\": \"usage\",\n             \"Core: Pipeline\": \"pipeline\",\n             \"TensorFlow\": \"tensorflow or tf\",\n             \"PyTorch\": \"pytorch\",\n             \"Examples\": \"examples\",\n             \"Documentation\": \"documentation\"}\n\ndef filter_labels(x):\n    return [label_map[label] for label in x if label in label_map]\n\ndf_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\nall_labels = list(label_map.values())\n\nCheck the distribution of the filtered dataset\ndf_counts = df_issues[\"labels\"].explode().value_counts()\ndf_counts.to_frame().T\n\n\n\n\n\n\n\n\ntokenization\n\n\nnew model\n\n\nmodel training\n\n\nusage\n\n\npipeline\n\n\ntensorflow or tf\n\n\npytorch\n\n\ndocumentation\n\n\nexamples\n\n\n\n\n\n\nlabels\n\n\n106\n\n\n98\n\n\n64\n\n\n46\n\n\n42\n\n\n41\n\n\n37\n\n\n28\n\n\n24\n\n\n\n\n\n\n\ndf_counts[:2].sum() / df_counts.sum()\n    0.41975308641975306\nNote: The filtered dataset is more balanced, with the two most common classes accounting for less than 42% of the dataset.\n\nCreate a new column to indicate whether an issue is unlabeled\ndf_issues[\"split\"] = \"unlabeled\"\nmask = df_issues[\"labels\"].apply(lambda x: len(x)) &gt; 0\ndf_issues.loc[mask, \"split\"] = \"labeled\"\ndf_issues[\"split\"].value_counts().to_frame()\n\n\n\n\n\n\n\n\nsplit\n\n\n\n\n\n\nunlabeled\n\n\n9489\n\n\n\n\nlabeled\n\n\n441\n\n\n\n\n\n\n\ndf_issues[\"split\"].value_counts()[0] / len(df_issues)\n    0.9555891238670695\nNote: Over 95% of issues are unlabeled.\n\nInspect a labeled example\nfor column in [\"title\", \"body\", \"labels\"]:\n    print(f\"{column}: {df_issues[column].iloc[26][:500]}\\n\")\n    title: Add new CANINE model\n    \n    body: # 🌟 New model addition\n    \n    ## Model description\n    \n    Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting:\n    \n    &gt; Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually en\n    \n    labels: ['new model']\nNote: * This GitHub issue is proposing a new model architecture. * Both the title and description contain helpful information for the label classifier.\n\nConcatenate the title and description for each issue into a new column\ndf_issues[\"text\"] = (df_issues.apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1))\n\nRemove any duplicate rows based on the text column values\nlen_before = len(df_issues)\ndf_issues = df_issues.drop_duplicates(subset=\"text\")\nprint(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")\n    Removed 1.88% duplicates.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nPlot the number of words per issue\n(df_issues[\"text\"].str.split().apply(len).hist(bins=np.linspace(0, 750, 50), grid=False, edgecolor=\"C0\"))\nplt.title(\"Words per issue\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Number of issues\")\nplt.show()\n\n\n\n\n\nNote: * The text for most issues is short, but some have more than 500 words. * Issues with error messages and code snippets are often longer. * Most of the examples should fit into the typical context size of 512 tokens.\n\n\n\n\nCreating Training Sets\n\nThere is no guaranteed balance for all labels when splitting the dataset.\nWe can use the scikit-multilearn library to approximate a balanced split.\n\n\nscikit-multilearn library\n\nHomepage\nA multi-label classification library built on top of the scikit-learn ecosystem.\n\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\n\nMultiLabelBinarizer\n\nDocumentation\nTransform between iterable of iterables and a multilabel format.\nTakes a list of names and creates a vector with zeros for absent labels and ones for present labels.\n\n\nCreate a MultiLabelBinarizer to learn the mapping from label to ID\nmlb = MultiLabelBinarizer()\nmlb.fit([all_labels])\n    MultiLabelBinarizer()\n\nCheck the label mappings\nmlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])\n    array([[0, 0, 0, 1, 0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0, 1, 0, 0, 0]])\n\npd.DataFrame(mlb.transform([[label] for label in all_labels]).T, columns=all_labels).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\ntokenization\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\nnew model\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nmodel training\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nusage\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\npipeline\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\ntensorflow or tf\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\npytorch\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nexamples\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\ndocumentation\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n\nfrom skmultilearn.model_selection import iterative_train_test_split\n\n\niterative_train_test_split\n\nDocumentation\n\nDefine a function to iteratively generate a balanced train/test split\ndef balanced_split(df, test_size=0.5):\n    ind = np.expand_dims(np.arange(len(df)), axis=1)\n    labels = mlb.transform(df[\"labels\"])\n    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, \n                                                           test_size)\n    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]\nfrom sklearn.model_selection import train_test_split\n\nSplit the data into supervised and unsupervised datasets\ndf_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\ndf_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\ndf_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n\nCreate balanced training, validation, and test sets\nnp.random.seed(0)\ndf_train, df_tmp = balanced_split(df_sup, test_size=0.5)\ndf_valid, df_test = balanced_split(df_tmp, test_size=0.5)\n\nfrom datasets import Dataset, DatasetDict\n\n\n\nDataset.from_pandas\n\nDocumentation\nConvert pandas.DataFrame to a pyarrow.Table to create a Dataset.\n\n\nprint_source(Dataset.from_pandas)\n    @classmethod\n    def from_pandas(cls, df: pd.DataFrame, features: Optional[Features]=None,\n        info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None,\n        preserve_index: Optional[bool]=None) -&gt;'Dataset':\n        if info is not None and features is not None and info.features != features:\n            raise ValueError(\n                f\"\"\"Features specified in `features` and `info.features` can't be different:\n    {features}\n    {info.features}\"\"\"\n                )\n        features = (features if features is not None else info.features if info\n             is not None else None)\n        if info is None:\n            info = DatasetInfo()\n        info.features = features\n        table = InMemoryTable.from_pandas(df=df, preserve_index=preserve_index,\n            schema=features.arrow_schema if features is not None else None)\n        return cls(table, info=info, split=split)\n\nInitialize a DatasetDict with the dataset splits\nds = DatasetDict({\n    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\nds\n    DatasetDict({\n        train: Dataset({\n            features: ['text', 'labels'],\n            num_rows: 223\n        })\n        valid: Dataset({\n            features: ['text', 'labels'],\n            num_rows: 106\n        })\n        test: Dataset({\n            features: ['text', 'labels'],\n            num_rows: 111\n        })\n        unsup: Dataset({\n            features: ['text', 'labels'],\n            num_rows: 9303\n        })\n    })\n\n\n\n\nCreating Training Slices\nCreate training slices with different numbers of samples\nnp.random.seed(0)\nall_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\nindices_pool = all_indices\nlabels = mlb.transform(ds[\"train\"][\"labels\"])\ntrain_samples = [8, 16, 32, 64, 128]\ntrain_slices, last_k = [], 0\n\nfor i, k in enumerate(train_samples):\n    # Split off samples necessary to fill the gap to the next split size\n    indices_pool, labels, new_slice, _ = iterative_train_test_split(\n        indices_pool, labels, (k-last_k)/len(labels))\n    last_k = k\n    if i==0: train_slices.append(new_slice)\n    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n        \n# Add full dataset as last slice\ntrain_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\ntrain_slices = [np.squeeze(train_slice) for train_slice in train_slices]\nNote: It is not always possible to find a balanced split with a given split size.\n\nprint(\"Target split sizes:\")\nprint(train_samples)\nprint(\"Actual split sizes:\")\nprint([len(x) for x in train_slices])\n    Target split sizes:\n    [8, 16, 32, 64, 128, 223]\n    Actual split sizes:\n    [10, 19, 36, 68, 134, 223]"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#implementing-a-naive-bayesline",
    "href": "posts/transformers-book-notes/chapter-9/index.html#implementing-a-naive-bayesline",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Implementing a Naive Bayesline",
    "text": "Implementing a Naive Bayesline\n\nA baseline based on regular expressions, handcrafted rules, or a simple model might work well enough to solve a given problem.\n\nThese are generally easier to deploy and maintain than transformer models.\n\nBaseline models provide quick sanity checks when exploring more complex models.\n\nA more complex model like BERT should perform better than a simple logistic regression classifier on the same dataset.\n\nA Naive Bayes Classifier is a great baseline model for text classification as it is simple, quick to train, and reasonably robust to changes in input.\n\nCreate a new ids column with the multilabel vectors for each training sample\ndef prepare_labels(batch):\n    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n    return batch\n\nds = ds.map(prepare_labels, batched=True)\n\nds['train'][:5]['labels'], ds['train'][:5]['label_ids']\n    ([['new model'], ['new model'], ['new model'], ['new model'], ['examples']],\n     [[0, 0, 0, 1, 0, 0, 0, 0, 0],\n      [0, 0, 0, 1, 0, 0, 0, 0, 0],\n      [0, 0, 0, 1, 0, 0, 0, 0, 0],\n      [0, 0, 0, 1, 0, 0, 0, 0, 0],\n      [0, 1, 0, 0, 0, 0, 0, 0, 0]])\n\nfrom collections import defaultdict\n\nCreate dictionaries to store micro and macro \\(F_{1}\\)-scores\n\nThe micro \\(F_{1}\\)-score tracks performance for on the frequent labels\nThe macro \\(F_{1}\\)-score tracks performance on all the labels regardless of frequency\n\n\nmacro_scores, micro_scores = defaultdict(list), defaultdict(list)\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nsklearn.naive_bayes.MultinomialNB\n\nDocumentation\nCreate a Naive Bayes classifier for multinomial models.\n\n\n\nsklearn.metrics._classification.classification_report\n\nDocumentation\nBuild a text report showing the main classification metrics.\n\n\n\nskmultilearn.problem_transform.br.BinaryRelevance\n\nDocumentation\nTreat each label as a separate single-class classification problem\n\n\n\nsklearn.feature_extraction.text.CountVectorizer\n\nDocumentation\nCreate a vector where each entry corresponds to the frequency with which a token appeared in the text.\nCount vectorization is a bag-of-words approach since all information on the order of the words is lost.\n\n\n# count_vect = CountVectorizer()\n# pd.DataFrame(count_vect.fit_transform(ds['train'].select(train_slices[0])[\"text\"]).toarray())\n\nTrain a baseline Naive Bayes Classifier for each of the training slices\nfor train_slice in train_slices:\n    # Get training slice and test data\n    ds_train_sample = ds[\"train\"].select(train_slice)\n    y_train = np.array(ds_train_sample[\"label_ids\"])\n    y_test = np.array(ds[\"test\"][\"label_ids\"])\n    # Use a simple count vectorizer to encode our texts as token counts\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n    # Create and train our model!\n    classifier = BinaryRelevance(classifier=MultinomialNB())\n    classifier.fit(X_train_counts, y_train)\n    # Generate predictions and evaluate\n    y_pred_test = classifier.predict(X_test_counts)\n    clf_report = classification_report(\n        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\n        output_dict=True)\n    # Store metrics\n    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n\nPlot the performance of the baseline classifiers\ndef plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\n    for run in micro_scores.keys():\n        if run == current_model:\n            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n        else:\n            ax0.plot(sample_sizes, micro_scores[run], label=run, \n                     linestyle=\"dashed\")\n            ax1.plot(sample_sizes, macro_scores[run], label=run, \n                     linestyle=\"dashed\")\n\n    ax0.set_title(\"Micro F1 scores\")\n    ax1.set_title(\"Macro F1 scores\")\n    ax0.set_ylabel(\"Test set F1 score\")\n    ax0.legend(loc=\"lower right\")\n    for ax in [ax0, ax1]:\n        ax.set_xlabel(\"Number of training samples\")\n        ax.set_xscale(\"log\")\n        ax.set_xticks(sample_sizes)\n        ax.set_xticklabels(sample_sizes)\n        ax.minorticks_off()\n    plt.tight_layout()\n    plt.show()\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\n\n\n\n\n\nNote:\n\nThe number of samples is on a logarithmic scale.\nThe micro and macro F1 scores improve as the number of training samples increases.\nThe results are slightly noisy since each slice can have a different class distribution."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#working-with-no-labeled-data",
    "href": "posts/transformers-book-notes/chapter-9/index.html#working-with-no-labeled-data",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Working with No Labeled Data",
    "text": "Working with No Labeled Data\n\nZero-shot classification is suitable when there is no labeled data at all.\nZero-shot classification uses a pretrained model without additional fine-tuning on a task-specific corpus.\n\n\nZero-shot Fill-Mask Prediction\n\nThe pretrained model needs to be aware of the topic in the context to predict a missing token.\nWe can trick the model into classifying a document by providing a sentence like: &gt; “This section was about the topic [MASK].”\nThe model should give a reasonable suggestion for the document’s topic.\nCredit: Joe Davison\n\n\nfrom transformers import pipeline\n\nCreate a Masked Language Modeling pipeline\npipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\npipe, type(pipe.model), pipe.device\n    (&lt;transformers.pipelines.fill_mask.FillMaskPipeline at 0x7fd190bcd3a0&gt;,\n     transformers.models.bert.modeling_bert.BertForMaskedLM,\n     device(type='cpu'))\n\n\ntransformers.pipelines.fill_mask.FillMaskPipeline\n\nDocumentation\nCreate a masked language modeling prediction pipeline\n\nPredict the topic for a movie about animals base on its description\nmovie_desc = \"The main characters of the movie madacascar \\\nare a lion, a zebra, a giraffe, and a hippo. \"\nprompt = \"The movie is about [MASK].\"\n\noutput = pipe(movie_desc + prompt)\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n    Token animals:  0.103%\n    Token lions:    0.066%\n    Token birds:    0.025%\n    Token love: 0.015%\n    Token hunting:  0.013%\nNote: The model successfully performs zero-shot classification and only predicts tokens related to animals.\n\nCheck the probability that the movie description is about specific topics\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n    Token animals:  0.103%\n    Token cars: 0.001%\nNote: The model is confident the movie is not about cars.\n\nPredict the topic for a movie about cars based on its description\nmovie_desc = \"In the movie transformers aliens \\\ncan morph into a wide range of vehicles.\"\n\noutput = pipe(movie_desc + prompt)\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n    Token aliens:   0.221%\n    Token cars: 0.139%\n    Token robots:   0.099%\n    Token vehicles: 0.074%\n    Token transformers: 0.059%\n\nCheck the probability that the movie description is about specific topics\nmovie_desc = \"In the movie transformers aliens \\\ncan morph into a wide range of vehicles.\"\n\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n    Token cars: 0.139%\n    Token animals:  0.006%\n\n\n\n\nText Entailment\n\nThe model determines whether two text passages are likely to follow or contradict each other.\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\nXNLI: Evaluating Cross-lingual Sentence Representations\nTypical datasets for text entailment tasks include the Multi-Genere NLI Corpus (MNLI) and the Cross-Lingual NLI Corpus (XNLI).\nEach sample in the dataset contains a premise, a hypothesis, and a label.\nThe label can be either entailment, neutral, or contradiction.\n\nThe entailment label indicates the hypothesis text is necessarily correct under the premise.\nThe contradiction label indicates the hypothesis is necessarily false or inappropriate under the premise.\nThe neutral label indicates the hypothesis is unrelated to the premise.\n\n\n\n\n\n\n\n\n\n\nPremise\nHypothesis\nLabel\n\n\n\n\nHis favorite color is blue.\nHe is into heavy metal\nneutral\n\n\nShe finds the joke hilarious.\nShe thinkg the joke is not funny at all.\ncontradiction\n\n\nThe house was recently built.\nThe house is new.\nentailment\n\n\n\n\nZero-shot classification with Text Entailment\n\nWe can use a model trained on the MNLI dataset to build a classifier without needing any labels.\nWe treat the input text as a premise and formulate a hypothesis as: &gt; “This example is about {label}.”\nWe insert the class name for the label.\nThe resulting entailment score indicates how likely the premise is about the topic.\nWe need to test different classes sequentially, meaning we need to execute a forward pass for each test.\nThe choice of label names can significantly impact prediction accuracy.\nChoosing labels with a semantic meaning is generally the best approach.\n\n\nfrom transformers import pipeline\n\nCreate a zero-shot classification pipeline\n# Use GPU if available\npipe = pipeline(\"zero-shot-classification\", device=0, fp16=True)\npipe, type(pipe.model), pipe.device\n(&lt;transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline at 0x7fd189242d00&gt;, transformers.models.bart.modeling_bart.BartForSequenceClassification, device(type='cuda', index=0))\n\nGet the default hypothesis template\ninspect.signature(pipe.preprocess).parameters['hypothesis_template']\n    &lt;Parameter \"hypothesis_template='This example is {}.'\"&gt;\n\n\ntransformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline\n\nDocumentation\nCreate a Natural-Language-Inference (NLI)-based zero-shot classification pipeline.\nThe pipeline takes any combination of sequences and labels.\nThe pipeline poses each combination as a premise-hypothesis pair and passes them to the pretrained model.\n\n\npd.Series(ds[\"train\"][0]['text']).to_frame().style.hide(axis='columns').hide(axis='rows')\n\nlen(ds[\"train\"][0]['text'].split(' ')), len(pipe.tokenizer(ds[\"train\"][0]['text'])['input_ids'])\n    (244, 562)\n\npd.DataFrame(ds[\"train\"][0]['text'].split(' ')).T.style.hide(axis='columns')\n\ninput_ids = pipe.tokenizer(ds[\"train\"][0]['text'])['input_ids']\npd.DataFrame(pipe.tokenizer.convert_ids_to_tokens(input_ids)).T.style.hide(axis='columns')\n\nTest each possible label using text entailment\nsample = ds[\"train\"][0]\nprint(f\"Labels: {sample['labels']}\")\noutput = pipe(sample[\"text\"], candidate_labels=all_labels, multi_label=True)\nprint(output[\"sequence\"][:400])\nprint(\"\\nPredictions:\")\n\nfor label, score in zip(output[\"labels\"], output[\"scores\"]):\n    print(f\"{label}, {score:.2f}\")\n    Labels: ['new model']\n    Add new CANINE model\n    \n    # 🌟 New model addition\n    \n    ## Model description\n    \n    Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting:\n    \n    &gt; Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokeni\n    \n    Predictions:\n    new model, 0.98\n    tensorflow or tf, 0.37\n    examples, 0.34\n    usage, 0.30\n    pytorch, 0.25\n    documentation, 0.25\n    model training, 0.24\n    tokenization, 0.17\n    pipeline, 0.16\nNote:\n\nThe model is confident the text is about a new model, but it also produces relatively high scores for labels not found in the text.\nThe highly technical domain of the text is very different from the original text distribution in the MNLI dataset.\nWe can feed input with code to the model since we use a subword tokenizer.\nTokenization might be inefficient with code since only a tiny fraction of the pretraining dataset contains code snippets.\nCode blocks can contain important information, such as frameworks used in the code.\n\n\nDefine a function to feed a single example through the zero-shot pipeline\ndef zero_shot_pipeline(example):\n    output = pipe(example[\"text\"], all_labels, multi_label=True)\n    example[\"predicted_labels\"] = output[\"labels\"]\n    example[\"scores\"] = output[\"scores\"]\n    return example\n\nFeed the whole validation set through the pipeline\nds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)\nNote: We can determine which labels to assign to each example using a minimum threshold value or selecting the top-k predictions.\n\nDefine a function to determine which set of labels to assign to each example using either a threshold value or top-k value\ndef get_preds(example, threshold=None, topk=None):\n    preds = []\n    if threshold:\n        for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n            if score &gt;= threshold:\n                preds.append(label)\n    elif topk:\n        for i in range(topk):\n            preds.append(example[\"predicted_labels\"][i])\n    else:\n        raise ValueError(\"Set either `threshold` or `topk`.\")\n    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}\n\nDefine a function that returns the scikit-learn classification report\ndef get_clf_report(ds):\n    y_true = np.array(ds[\"label_ids\"])\n    y_pred = np.array(ds[\"pred_label_ids\"])\n    return classification_report(\n        y_true, y_pred, target_names=mlb.classes_, zero_division=0, \n        output_dict=True)\n\nTest using top-k values to select labels\nmacros, micros = [], []\ntopks = [1, 2, 3, 4]\nfor topk in topks:\n    ds_zero_shot = ds_zero_shot.map(get_preds, batched=False,\n                                    fn_kwargs={'topk': topk})\n    clf_report = get_clf_report(ds_zero_shot)\n    micros.append(clf_report['micro avg']['f1-score'])\n    macros.append(clf_report['macro avg']['f1-score'])\nplt.plot(topks, micros, label='Micro F1')\nplt.plot(topks, macros, label='Macro F1')\nplt.xlabel(\"Top-k\")\nplt.ylabel(\"F1-score\")\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\nNote: We obtain the best results using only the highest score per example (i.e., top-1), given most examples only have one label.\n\nTest using a threshold value to select labels\nmacros, micros = [], []\nthresholds = np.linspace(0.01, 1, 100)\nfor threshold in thresholds:\n    ds_zero_shot = ds_zero_shot.map(get_preds,\n                                    fn_kwargs={\"threshold\": threshold})\n    clf_report = get_clf_report(ds_zero_shot)\n    micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n    macros.append(clf_report[\"macro avg\"][\"f1-score\"])\nplt.plot(thresholds, micros, label=\"Micro F1\")\nplt.plot(thresholds, macros, label=\"Macro F1\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"F1-score\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\nNote: The threshold approach performs slightly worse than the top-1 approach.\n\nbest_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.')\nbest_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')\n    Best threshold (micro): 0.75 with F1-score 0.46.\n    Best threshold (micro): 0.72 with F1-score 0.42.\nNote: A threshold value of around 0.8 provides the best tradeoff between precision and recall.\n\nCompare the zero-shot classifier to the baseline Naive Bayes model\nds_zero_shot = ds['test'].map(zero_shot_pipeline)\nds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={'topk': 1})\nclf_report = get_clf_report(ds_zero_shot)\nfor train_slice in train_slices:\n    macro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n    micro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])\nplot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")\n\n\n\n\n\nNote:\n\nThe zero-shot pipeline outperforms the baseline when using less than 60 labeled examples universally outperforms the baseline when considering both micro and macro F1 scores.\nThe baseline model performs better on the more common classes when using more than 60 examples.\nThe zero-shot classification pipeline is sensitive to the names of labels and might perform better when using different or several names in parallel and aggregating them.\nUsing a different hypothesis_template might improve performance."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#working-with-a-few-labels",
    "href": "posts/transformers-book-notes/chapter-9/index.html#working-with-a-few-labels",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Working with a Few Labels",
    "text": "Working with a Few Labels\n\nThere are often a few labeled examples available, at least, for most NLP projects.\nThe labels might come directly from a client, a cross-company team, from hand annotating a few examples.\n\n\nData Augmentation\n\nWe can use data augmentation to generate new training examples from existing ones. Perturbing words or characters can completely change the meaning. Noise introduced by data augmentation is less likely to change the meaning when the text is more than a few sentences.\n\n\nBack Translation\n\nBack translation involves translating the original text into one or more target languages and translating it back to the source language.\nBack translation works best for high-resource languages or corpora that don’t contain too many domain-specific words.\nWe can implement back translation models using machine translation models like MSM100.\n\n\n\nToken Perturbations\n\nToken perturbations involve randomly choosing and performing simple transformations like synonym replacement, word insertion, swap, or deletion.\nLibraries like NlpAug and TextAttack provide various recipes for token perturbations.\nEDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n\n\n\n\n\n\n\n\n\nAugmentation\nSentence\n\n\n\n\nNone\nEven if you defeat me Megatron, others will rise to defeat your tyranny\n\n\nSynonym replace\nEven if you kill me, Megatron, others will prove to defeat your tyranny\n\n\nRandom Insert\nEven if you defeat me Megatron, others humanity will rise to defeat your tyranny\n\n\nRandom Swap\nYou even if defeat me Megatron, others will rise defeat to tyranny your\n\n\nRandom delete\nEven if you me Megatron, others to defeat tyranny\n\n\nBack translate (German)\nEven if you defeat me, other will rise up to defeat your tyranny\n\n\n\nDisable Tokenizers Parallelism\n%env TOKENIZERS_PARALLELISM=false\n    env: TOKENIZERS_PARALLELISM=false\n\nfrom transformers import set_seed\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as nafc\nimport nltk\n\nDownload a perceptron model for tagging words and the Wordnet corpora\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\n    True\n\nnltk.download('omw-1.4')\n    True\n\n!ls ~/nltk_data/corpora/wordnet\n    adj.exc       cntlist.rev  data.noun  index.adv    index.verb  noun.exc\n    adv.exc       data.adj     data.verb  index.noun   lexnames    README\n    citation.bib  data.adv     index.adj  index.sense  LICENSE     verb.exc\n\n!head -5 ~/nltk_data/corpora/wordnet/noun.exc\n    aardwolves aardwolf\n    abaci abacus\n    aboideaux aboideau\n    aboiteaux aboiteau\n    abscissae abscissa\n\nReset random seed\nset_seed(3)\n\nDefine original text\ntext = \"Even if you defeat me Megatron, others will rise to defeat your tyranny\"\n\nInitialize augmentation dictionary\naugs = {}\n\n\n\nnlpaug.augmenter.word.synonym.SynonymAug\n\nDocumentation\nCreate an augmenter that leverages semantic meaning to substitute words.\n\nAdd Synonym Replacement Augmentation using the WordNet corpora\naugs[\"synonym_replace\"] = naw.SynonymAug(aug_src='wordnet')\naugs[\"synonym_replace\"].augment(text)\n    'Even if you kill me Megatron, others will prove to defeat your tyranny'\n\n\n\nnlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug\n\nDocumentation\nCreate an augmenter that finds the top n similar words using contextual word embeddings\n\nAdd Random Insert Augmentation using the contextual word embeddings of DistilBERT\naugs[\"random_insert\"] = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", \n                                device=\"cpu\", action=\"insert\", aug_max=1)\naugs[\"random_insert\"].augment(text)\n    'even if you defeat me megatron, others humanity will rise to defeat your tyranny'\n\n\n\nnlpaug.augmenter.word.random.RandomWordAug\n\nDocumentation\nRandomly apply substitute, swap, delete or crop augmentation\n\nThe default augmentation is to delete words.\n\n\nRandomly swap words\naugs[\"random_swap\"] = naw.RandomWordAug(action=\"swap\")\naugs[\"random_swap\"].augment(text)\n    'You even if defeat me Megatron, others will rise defeat to tyranny your'\n\nRandomly delete words\naugs[\"random_delete\"] = naw.RandomWordAug()\naugs[\"random_delete\"].augment(text)\n    'Even if you me Megatron, others to defeat tyranny'\n\n\n\nnlpaug.augmenter.word.back_translation.BackTranslationAug\n\nDocumentation\nUse two translation models to apply back translation\n\n\naugs[\"bt_en_de\"] = naw.BackTranslationAug(\n    from_model_name='facebook/wmt19-en-de', \n    to_model_name='facebook/wmt19-de-en'\n)\naugs[\"bt_en_de\"].augment(text)\n    'Even if you defeat me, others will rise up to defeat your tyranny'\n\nfor k,v in augs.items():\n    print(f\"Original text: {text}\")\n    print(f\"{k}: {v.augment(text)}\")\n    print(\"\")\n    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny\n    synonym_replace: Even if you defeat me Megatron, others will go up to vote out your tyranny\n    \n    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny\n    random_insert: even if you defeat me megatron, others will rise to defeat of your tyranny\n    \n    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny\n    random_swap: If even you defeat me Megatron, others will rise to defeat tyranny your\n    \n    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny\n    random_delete: If you me Megatron, will to defeat your tyranny\n    \n    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny\n    bt_en_de: Even if you defeat me, others will rise up to defeat your tyranny\n\nReset random seed\nset_seed(3)\n\nAdd Random Synonym Replacement using the contextual word embeddings of DistilBERT\naug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", \n                                device=\"cpu\", action=\"substitute\")\n\ntext = \"Transformers are the most popular toys\"\nprint(f\"Original text: {text}\")\nprint(f\"Augmented text: {aug.augment(text)}\")\n    Original text: Transformers are the most popular toys\n    Augmented text: transformers'the most popular toys\n\nDefine a function to apply synonym replacement augmentation to a batch\ndef augment_text(batch, transformations_per_example=1):\n    text_aug, label_ids = [], []\n    for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n        text_aug += [text]\n        label_ids += [labels]\n        for _ in range(transformations_per_example):\n            text_aug += [aug.augment(text)]\n            label_ids += [labels]\n    return {\"text\": text_aug, \"label_ids\": label_ids}\n\nTrain the baseline Naive Bayes Classifier using synonym replacement augmentation\nfor train_slice in train_slices:\n    # Get training slice and test data\n    ds_train_sample = ds[\"train\"].select(train_slice)\n    # Flatten augmentations and align labels!\n    ds_train_aug = (ds_train_sample.map(\n        augment_text, batched=True, remove_columns=ds_train_sample.column_names)\n                    .shuffle(seed=42))\n    y_train = np.array(ds_train_aug[\"label_ids\"])\n    y_test = np.array(ds[\"test\"][\"label_ids\"])\n    # Use a simple count vectorizer to encode our texts as token counts\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(ds_train_aug[\"text\"])\n    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n    # Create and train our model!\n    classifier = BinaryRelevance(classifier=MultinomialNB())\n    classifier.fit(X_train_counts, y_train)\n    # Generate predictions and evaluate\n    y_pred_test = classifier.predict(X_test_counts)\n    clf_report = classification_report(\n        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\n        output_dict=True)\n    # Store metrics\n    macro_scores[\"Naive Bayes + Aug\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Naive Bayes + Aug\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n\nCompare the results\nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")\n\n\n\n\n\nNote:\n\nA small amount of data augmentation improves the F1 score of the Naive Bayes Classifier.\nThe Naive Bayes Classifier overtakes the zero-shot pipeline for the macro F1 score at around 220 training samples.\n\n\n\n\n\nUsing Embeddings as a Lookup Table\n\nLarge language models like GPT-3 are excellent at solving tasks with limited data because they learn representations of text that encode information across many dimensions.\nWe can use embeddings of large language models to develop a semantic search engine, find similar documents or comments, or classify text.\nThis approach does not require fine-tuning models to leverage the few labeled data points.\nThe embeddings should ideally be from a pretrained on a similar domain to the target dataset.\nOpenAI API Classification Endopint\n\n\nSteps to classify text using embeddings:\n\nUse the language model to embed all labeled texts.\nPerform a nearest-neighbor search over the stored embeddings.\nAggregate the labels of the nearest neighbors to get a prediction.\n\n\nWe need to embed new text we want to classify and assign labels based on the labels of its nearest neighbors.\nIt is crucial to calibrate the number of neighbors for the nearest-neighbors search.\n\nUsing too few neighbors might result in noisy predictions.\nUsing too many neighbors might mix neighboring groups.\n\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nInstantiate a tokenizer and model using a GPT-2 checkpoint trained on Python code\nmodel_ckpt = \"miguelvictor/python-gpt2-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModel.from_pretrained(model_ckpt)\nNote: Transformer models like GPT-2 return one embedding vector per token, and we want a single embedding for the entire output.\n\nDefine a function to create a single-vector representation for model output using average pooling\n\nWe don’t want to include padding tokens in the average.\n\n\ndef mean_pooling(model_output, attention_mask):\n    # Extract the token embeddings\n    token_embeddings = model_output[0]\n    # Compute the attention mask\n    input_mask_expanded = (attention_mask\n                           .unsqueeze(-1)\n                           .expand(token_embeddings.size())\n                           .float())\n    # Sum the embeddings, but ignore masked tokens\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    # Return the average as a single vector\n    return sum_embeddings / sum_mask\n\nDefine a function to embed sample text\ndef embed_text(examples):\n    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n                       max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        model_output = model(**inputs)\n    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n    return {\"embedding\": pooled_embeds.cpu().numpy()}\n\nUse the end-of-string token as the padding token since GPT-style models don’t have one\ntokenizer.pad_token = tokenizer.eos_token\n\nGet the embeddings for each split\nembs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\nembs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\nembs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)\n\n\n\nDataset.add_faiss_index\n\nDocumentation\nAdd a dense index using FAISS for fast retrieval.\nFAISS is a library for efficient similarity search of dense vectors.\n\nGitHub Repository\nBillion-scale similarity search with GPUs\n\n\n\nimport faiss\nCreate a FAISS index using the embeddings for the train split\nembs_train.add_faiss_index(\"embedding\")\n    Dataset({\n        features: ['text', 'labels', 'label_ids', 'embedding'],\n        num_rows: 223\n    })\n\n\n\ndatasets.search.IndexableMixin.get_nearest_examples\n\nDocumentation\nFind the nearest examples in the dataset to the query.\n\nPerform a nearest-neighbor lookup\ni, k = 0, 3 # Select the first query and 3 nearest neighbors\nrn, nl = \"\\r\\n\\r\\n\", \"\\n\" # Used to remove newlines in text for compact display\n\nquery =  np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\nscores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\n\nprint(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\nprint(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn, nl)} [...]\\n\")\nprint(\"=\"*50)\nprint(f\"Retrieved documents:\")\nfor score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n    print(\"=\"*50)\n    print(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n    print(f\"SCORE: {score:.2f}\")\n    print(f\"LABELS: {label}\")\n    QUERY LABELS: ['new model']\n    QUERY TEXT:\n    Implementing efficient self attention in T5\n    \n    # 🌟 New model addition\n    My teammates and I (including @ice-americano) would like to use efficient self attention methods such as Linformer, Performer and [...]\n    \n    ==================================================\n    Retrieved documents:\n    ==================================================\n    TEXT:\n    Add Linformer model\n    \n    # 🌟 New model addition\n    ## Model description\n    ### Linformer: Self-Attention with Linear Complexity\n    Paper published June 9th on ArXiv: https://arxiv.org/abs/2006.04768\n    La [...]\n    SCORE: 54.92\n    LABELS: ['new model']\n    ==================================================\n    TEXT:\n    Add FAVOR+ / Performer attention\n    \n    # 🌟 FAVOR+ / Performer attention addition\n    Are there any plans to add this new attention approximation block to Transformers library?\n    ## Model description\n    The n [...]\n    SCORE: 57.90\n    LABELS: ['new model']\n    ==================================================\n    TEXT:\n    Implement DeLighT: Very Deep and Light-weight Transformers\n    \n    # 🌟 New model addition\n    ## Model description\n    DeLight, that delivers similar or better performance than transformer-based models with sign [...]\n    SCORE: 60.12\n    LABELS: ['new model']\nNote:\n\nThe three retrieved documents all have the same labels as they should.\nThe query and the retrieved documents all relate to adding new and efficient transformer models.\n\n\nDefine a function that returns the sample predictions using a label occurrence threshold\ndef get_sample_preds(sample, m):\n    return (np.sum(sample[\"label_ids\"], axis=0) &gt;= m).astype(int)\n\n\n\ndatasets.search.IndexableMixin.get_nearest_examples_batch\n\nDocumentation\n\nDefine a function to test different k and threshold values for nearest-neighbor search\ndef find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n    max_k = min(len(ds_train), max_k)\n    perf_micro = np.zeros((max_k, max_k))\n    perf_macro = np.zeros((max_k, max_k))\n    for k in range(1, max_k):\n        for m in range(1, k + 1):\n            _, samples = ds_train.get_nearest_examples_batch(\"embedding\",\n                                                             valid_queries, k=k)\n            y_pred = np.array([get_sample_preds(s, m) for s in samples])\n            clf_report = classification_report(valid_labels, y_pred,\n                target_names=mlb.classes_, zero_division=0, output_dict=True)\n            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n    return perf_micro, perf_macro\n\nTest different k and threshold values\nvalid_labels = np.array(embs_valid[\"label_ids\"])\nvalid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\nperf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\n\nPlot the results\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\nax0.imshow(perf_micro)\nax1.imshow(perf_macro)\n\nax0.set_title(\"micro scores\")\nax0.set_ylabel(\"k\")\nax1.set_title(\"macro scores\")\nfor ax in [ax0, ax1]:\n    ax.set_xlim([0.5, 17 - 0.5])\n    ax.set_ylim([17 - 0.5, 0.5])\n    ax.set_xlabel(\"m\")\nplt.show()\n\n\n\n\n\nNote:\n\nChoosing a threshold value \\(m\\) that is too large or small for a given \\(k\\) value yields suboptimal results.\nA ratio of approximately \\(m/k = 1/3\\) achieves the best results.\n\n\nFind the best k and threshold values\nk, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\nprint(f\"Best k: {k}, best m: {m}\")\n    Best k: 15, best m: 5\nNote: We get the best performance when we retrieve the 15 nearest neighbors and then assign the labels that occurred at least five times.\n\nEvaluate the embedding lookup performance using different training slices\n# Drop the FAISS index\nembs_train.drop_index(\"embedding\")\ntest_labels = np.array(embs_test[\"label_ids\"])\ntest_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n\nfor train_slice in train_slices:\n    # Create a FAISS index from training slice \n    embs_train_tmp = embs_train.select(train_slice)\n    embs_train_tmp.add_faiss_index(\"embedding\")\n    # Get best k, m values with validation set\n    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n    # Get predictions on test set\n    _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\",\n                                                           test_queries,\n                                                           k=int(k))\n    y_pred = np.array([get_sample_preds(s, m) for s in samples])\n    # Evaluate predictions\n    clf_report = classification_report(test_labels, y_pred,\n        target_names=mlb.classes_, zero_division=0, output_dict=True,)\n    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n\nCompare performance to previous methods\nplot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")\n\n\n\n\n\nNote:\n\nThe embedding lookup is competitive on the micro F1 scores while only having two “learnable” parameters, k and m, but performs slightly worse on the macro scores.\nThe method that works best in practice strongly depends on the domain.\nThe zero-shot pipeline might work much better on tasks closer to the pretraining domain.\nThe embeddings quality depends on the model and the original training data.\n\n\n\n\n\nEfficient Similarity Search with FAISS\n\nWe usually speed up text search by creating an inverted index that maps terms to documents.\nAn inverted index works just like an index at the end of a book, where each word maps to the pages/documents it occurs in.\nWe can quickly find which documents the search terms appear in when performing a query.\nAn inverted index works well with discrete objects such as words but does not work with continuous ones like vectors.\nWe need to look for similar matches instead of exact matches since each document likely has a unique vector.\nFAISS avoids comparing the query vector to every vector in the database with several tricks.\nFAISS speeds up the comparison process by applying k-means clustering to the dataset, grouping the embeddings by similarity.\nWe get a centroid vector for each group, which is the average of all group members.\nWe can then search across the k centroids for the one that is most similar to our query and then search within the corresponding group.\nThis approach reduces the number of comparisons from n to \\(k + \\frac{n}{k}\\).\n\nThe minimum k value is \\(k = \\sqrt{n}\\).\n\nFAISS also provides a GPU-enabled version for increased speed and several options to compress vectors with advanced quantization schemes.\nGuidelines to choose an index\n\n\n\nFine-Tuning a Vanilla Transformer\n\nWe can fine-tune a pretrained transformer model when we have labeled data.\nStarting with a pretrained BERT-like model is often a good idea.\nThe target corpus should not be too different from the pretraining corpus.\nThe Hugging Face hub has many models pretrained on different corpora.\n\n\nimport torch\nfrom transformers import (AutoTokenizer, AutoConfig,\n                          AutoModelForSequenceClassification)\n\nLoad the pretrained tokenizer for the standard BERT checkpoint\nmodel_ckpt = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nTokenize the dataset\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\nds_enc = ds.map(tokenize, batched=True)\nds_enc = ds_enc.remove_columns(['labels', 'text'])\n\nChange the label_ids column data type to float for the multilabel loss function\nds_enc.set_format(\"torch\")\nds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)},\n                    remove_columns=[\"label_ids\"])\nds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")\n\nfrom transformers import Trainer, TrainingArguments\n\nKeep the best model based on the micro \\(F_{1}\\)-score\ntraining_args_fine_tune = TrainingArguments(\n    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\n    lr_scheduler_type='constant', per_device_train_batch_size=4,\n    per_device_eval_batch_size=32, weight_decay=0.0, \n    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\n    load_best_model_at_end=True, metric_for_best_model='micro f1',\n    save_total_limit=1, log_level='error')\nfrom scipy.special import expit as sigmoid\n\nDefine a function to compute the \\(F_{1}\\)-scores\ndef compute_metrics(pred):\n    y_true = pred.label_ids\n    # Normalize the model predictions\n    y_pred = sigmoid(pred.predictions)\n    y_pred = (y_pred&gt;0.5).astype(float)\n    \n    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\n                                     zero_division=0, output_dict=True)\n    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n            \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}\n\nIntitialize a BertConfig for Multi-label classification\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\nconfig\n    BertConfig {\n      \"_name_or_path\": \"bert-base-uncased\",\n      \"architectures\": [\n        \"BertForMaskedLM\"\n      ],\n      \"attention_probs_dropout_prob\": 0.1,\n      \"classifier_dropout\": null,\n      \"gradient_checkpointing\": false,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"id2label\": {\n        \"0\": \"LABEL_0\",\n        \"1\": \"LABEL_1\",\n        \"2\": \"LABEL_2\",\n        \"3\": \"LABEL_3\",\n        \"4\": \"LABEL_4\",\n        \"5\": \"LABEL_5\",\n        \"6\": \"LABEL_6\",\n        \"7\": \"LABEL_7\",\n        \"8\": \"LABEL_8\"\n      },\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"label2id\": {\n        \"LABEL_0\": 0,\n        \"LABEL_1\": 1,\n        \"LABEL_2\": 2,\n        \"LABEL_3\": 3,\n        \"LABEL_4\": 4,\n        \"LABEL_5\": 5,\n        \"LABEL_6\": 6,\n        \"LABEL_7\": 7,\n        \"LABEL_8\": 8\n      },\n      \"layer_norm_eps\": 1e-12,\n      \"max_position_embeddings\": 512,\n      \"model_type\": \"bert\",\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"pad_token_id\": 0,\n      \"position_embedding_type\": \"absolute\",\n      \"problem_type\": \"multi_label_classification\",\n      \"transformers_version\": \"4.18.0\",\n      \"type_vocab_size\": 2,\n      \"use_cache\": true,\n      \"vocab_size\": 30522\n    }\n\nTrain a classifier from scratch for each training slice\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n                                                               config=config).to(device)\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer,\n        args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"])\n    \n    old_collator = trainer.data_collator\n    trainer.data_collator = lambda data: dict(old_collator(data))\n\n    trainer.train()\n    pred = trainer.predict(ds_enc[\"test\"])\n    metrics = compute_metrics(pred)\n    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])\n\nCompare the results to previous methods\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")\n\n\n\n\n\nNote: The fine-tuned model is competitive when we have at least 64 training examples.\n\n\n\nIn-Context and Few-Shot Learning with Prompts\n\nThe GPT-3 paper found that large-language models can effectively learn from examples presented in a prompt.\n\nLanguage Models are Few-Shot Learners\n\nThe larger the model, the better it is at using in-context examples.\nAn alternative to using labeled data is to create examples of the prompts and desired predictions and continue training the language model on those examples.\n\nADAPET beats GPT-3 on a wide variety of tasks using this approach.\n\nImproving and Simplifying Pattern Exploiting Training\n\nResearchers at Hugging Face found that this approach can be more data-efficient than fine-tuning a custom head.\n\nHow Many Data Points is a Prompt Worth?"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#leveraging-unlabeled-data",
    "href": "posts/transformers-book-notes/chapter-9/index.html#leveraging-unlabeled-data",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Leveraging Unlabeled Data",
    "text": "Leveraging Unlabeled Data\n\nDomain adaptation involves continuing the pretraining process of predicting masked words using unlabeled data from the target domain.\nWe can then reuse the adapted model for many use cases.\nDomain adaptation can help boost model performance with unlabeled data and little effort.\n\n\nFine-Tuning a Language Model\n\nWe need to mask the [CLS] and [SEP] tokens from the loss, so we don’t train the model to predict them when doing masked language modeling.\nWe can get a mask when tokenizing by setting return_special_tokens_mask=True.\nWe can use a specialized data collator to prepare the elements in a batch for masked language modeling on the fly.\nThe data collator needs to mask tokens in the input sequence and have the target tokens in the outputs.\n\nDefine a function to tokenize the text and get the special token mask information\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True,\n                     max_length=128, return_special_tokens_mask=True)\n\nRetokenize the text\nds_mlm = ds.map(tokenize, batched=True)\nds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])\nds_mlm\n    DatasetDict({\n        train: Dataset({\n            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n            num_rows: 223\n        })\n        valid: Dataset({\n            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n            num_rows: 106\n        })\n        test: Dataset({\n            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n            num_rows: 111\n        })\n        unsup: Dataset({\n            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n            num_rows: 9303\n        })\n    })\n\npd.DataFrame(ds_mlm['train']['special_tokens_mask'][0]).T.style.hide(axis='columns').hide(axis='rows')\n\n\n\n\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n\npd.DataFrame(tokenizer.convert_ids_to_tokens(ds_mlm['train']['input_ids'][0])).T.style.hide(axis='columns').hide(axis='rows')\n\n\n\n\n\n\n\n\n[CLS]\n\n\nadd\n\n\nnew\n\n\ncanine\n\n\nmodel\n\n\n#\n\n\n[UNK]\n\n\nnew\n\n\nmodel\n\n\naddition\n\n\n#\n\n\n#\n\n\nmodel\n\n\ndescription\n\n\ngoogle\n\n\nrecently\n\n\nproposed\n\n\na\n\n\nnew\n\n\n*\n\n\n*\n\n\nc\n\n\n*\n\n\n*\n\n\nhara\n\n\n##cter\n\n\n*\n\n\n*\n\n\na\n\n\n*\n\n\n*\n\n\nrc\n\n\n##hit\n\n\n##ect\n\n\n##ure\n\n\nwith\n\n\n*\n\n\n*\n\n\nn\n\n\n*\n\n\n*\n\n\no\n\n\ntoken\n\n\n##ization\n\n\n*\n\n\n*\n\n\ni\n\n\n*\n\n\n*\n\n\nn\n\n\n*\n\n\n*\n\n\nn\n\n\n*\n\n\n*\n\n\neu\n\n\n##ral\n\n\n*\n\n\n*\n\n\ne\n\n\n*\n\n\n*\n\n\nnc\n\n\n##oder\n\n\n##s\n\n\narchitecture\n\n\n(\n\n\ncanine\n\n\n)\n\n\n.\n\n\nnot\n\n\nonly\n\n\nthe\n\n\ntitle\n\n\nis\n\n\nexciting\n\n\n:\n\n\n\n\n\npipeline\n\n\n##d\n\n\nnl\n\n\n##p\n\n\nsystems\n\n\nhave\n\n\nlargely\n\n\nbeen\n\n\nsuperseded\n\n\nby\n\n\nend\n\n\n-\n\n\nto\n\n\n-\n\n\nend\n\n\nneural\n\n\nmodeling\n\n\n,\n\n\nyet\n\n\nnearly\n\n\nall\n\n\ncommonly\n\n\n-\n\n\nused\n\n\nmodels\n\n\nstill\n\n\nrequire\n\n\nan\n\n\nexplicit\n\n\ntoken\n\n\n##ization\n\n\nstep\n\n\n.\n\n\nwhile\n\n\nrecent\n\n\ntoken\n\n\n##ization\n\n\napproaches\n\n\nbased\n\n\non\n\n\ndata\n\n\n-\n\n\nderived\n\n\nsub\n\n\n##word\n\n\nlexi\n\n\n##con\n\n\n##s\n\n\nare\n\n\n[SEP]\n\n\n\n\n\n\n\n\nfrom transformers import DataCollatorForLanguageModeling, set_seed\n\n\ntransformers.data.data_collator.DataCollatorForLanguageModeling\n\nDocumentation\nCreate a data collator for language modeling.\n\nInitialize the data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n                                                mlm_probability=0.15)\n\nReset random seed\nset_seed(3)\n\nTest the data collator\n# Switch the return formats of the tokenizer and data collator to NumPy\ndata_collator.return_tensors = \"np\"\ninputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\n\noutputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\n\npd.DataFrame({\n    \"Original tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n    \"Masked tokens\": tokenizer.convert_ids_to_tokens(outputs[\"input_ids\"][0]),\n    \"Original input_ids\": inputs[\"input_ids\"][0],\n    \"Masked input_ids\": outputs[\"input_ids\"][0],\n    \"Labels\": outputs[\"labels\"][0]}).T\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nOriginal tokens\n\n\n[CLS]\n\n\ntransformers\n\n\nare\n\n\nawesome\n\n\n!\n\n\n[SEP]\n\n\n\n\nMasked tokens\n\n\n[CLS]\n\n\ntransformers\n\n\nare\n\n\nawesome\n\n\n[MASK]\n\n\n[SEP]\n\n\n\n\nOriginal input_ids\n\n\n101\n\n\n19081\n\n\n2024\n\n\n12476\n\n\n999\n\n\n102\n\n\n\n\nMasked input_ids\n\n\n101\n\n\n19081\n\n\n2024\n\n\n12476\n\n\n103\n\n\n102\n\n\n\n\nLabels\n\n\n-100\n\n\n-100\n\n\n-100\n\n\n-100\n\n\n999\n\n\n-100\n\n\n\n\n\n\n\nSwitch the return formats of the data collator to PyTorch\ndata_collator.return_tensors = \"pt\"\nfrom transformers import AutoModelForMaskedLM\n\nInitialize the training arguments\ntraining_args = TrainingArguments(\n    output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32,\n    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\n    num_train_epochs=16, push_to_hub=True, log_level=\"error\", report_to=\"none\", fp16=True)\n\nInitialize the Trainer\ntrainer = Trainer(\n        model=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device),\n        tokenizer=tokenizer, args=training_args, data_collator=data_collator,\n        train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\n\nTrain the model\ntrainer.train()\n    TrainOutput(global_step=4656, training_loss=1.2827145487991805, metrics={'train_runtime': 561.7109, 'train_samples_per_second': 264.99, 'train_steps_per_second': 8.289, 'train_loss': 1.2827145487991805, 'epoch': 16.0})\n\nPush the trained model to Hugging Face Hub\ntrainer.push_to_hub(\"Training complete!\")\n    'https://huggingface.co/cj-mills/bert-base-uncased-issues-128/commit/c7ba31377378cb7fb9412e9524aaf76eed6956b7'\n\nInspect the training and validation losses from the training session\ndf_log = pd.DataFrame(trainer.state.log_history)\n\n# Drop missing values\n(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"]\n .plot(label=\"Validation\"))\ndf_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n \nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\n\n\n\nNote: Both the training and validation loss decreased significantly.\n\nFree unoccupied cached memory\ntorch.cuda.empty_cache()\n\n\n\n\nFine-Tuning a Classifier\nInitialize a BertConfig for multi-label classification with the custom checkpoint\nmodel_ckpt = \"bert-base-uncased\"\nmodel_ckpt = f'{model_ckpt}-issues-128'\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\nconfig\n    BertConfig {\n      \"_name_or_path\": \"bert-base-uncased-issues-128\",\n      \"architectures\": [\n        \"BertForMaskedLM\"\n      ],\n      \"attention_probs_dropout_prob\": 0.1,\n      \"classifier_dropout\": null,\n      \"gradient_checkpointing\": false,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.1,\n      \"hidden_size\": 768,\n      \"id2label\": {\n        \"0\": \"LABEL_0\",\n        \"1\": \"LABEL_1\",\n        \"2\": \"LABEL_2\",\n        \"3\": \"LABEL_3\",\n        \"4\": \"LABEL_4\",\n        \"5\": \"LABEL_5\",\n        \"6\": \"LABEL_6\",\n        \"7\": \"LABEL_7\",\n        \"8\": \"LABEL_8\"\n      },\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": 3072,\n      \"label2id\": {\n        \"LABEL_0\": 0,\n        \"LABEL_1\": 1,\n        \"LABEL_2\": 2,\n        \"LABEL_3\": 3,\n        \"LABEL_4\": 4,\n        \"LABEL_5\": 5,\n        \"LABEL_6\": 6,\n        \"LABEL_7\": 7,\n        \"LABEL_8\": 8\n      },\n      \"layer_norm_eps\": 1e-12,\n      \"max_position_embeddings\": 512,\n      \"model_type\": \"bert\",\n      \"num_attention_heads\": 12,\n      \"num_hidden_layers\": 12,\n      \"pad_token_id\": 0,\n      \"position_embedding_type\": \"absolute\",\n      \"problem_type\": \"multi_label_classification\",\n      \"torch_dtype\": \"float32\",\n      \"transformers_version\": \"4.18.0\",\n      \"type_vocab_size\": 2,\n      \"use_cache\": true,\n      \"vocab_size\": 30522\n    }\n\nFine-tune the classifier on each training slice\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n                                                               config=config).to(device)\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"],\n    )\n\n    trainer.train()\n    pred = trainer.predict(ds_enc['test'])\n    metrics = compute_metrics(pred)\n    # DA refers to domain adaptation\n    macro_scores['Fine-tune (DA)'].append(metrics['macro f1'])\n    micro_scores['Fine-tune (DA)'].append(metrics['micro f1'])\nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")\n\n\n\n\n\nNote: The fine-tuned classifier performs better than the vanilla BERT, especially in the low-data domain.\n\n\n\nAdvanced Methods\n\nUnsupervised data augmentation\n\nUnsupervised data augmentation (UDA) works off the idea that a model’s predictions should be consistent for an unlabeled example and a slightly distorted one.\nWe can introduce distortions using standard data augmentation strategies.\nWe enforce consistency by minimizing the KL divergence between the predictions of the original and distorted examples.\nWe incorporate the consistency requirement by augmenting the cross-entropy loss with an additional term from the unlabeled examples.\nThe model trains on the labeled data with the standard supervised approach, but we constrain the model to make consistent predictions on the unlabeled data.\nBERT models trained with UDA on a handful of labeled examples get similar performance to models trained on thousands of examples.\nUDA requires a data augmentation pipeline to generate distorted examples.\nTraining takes much longer since it requires multiple forward passes to generate the predicted distributions on the unlabeled and augmented examples.\n\n\n\nUncertainty-aware self-training\n\nUncertainty-aware self-training (UST) involves training a teacher model on labeled data and using that model to create pseudo-labels for unlabeled data.\nA student model then trains on the pseudo-labeled data.\nWe get an uncertainty measure of the student model’s predictions by feeding it the same input several times with dropout turned on.\nThe variance of the predictions gives a proxy for the certainty of the model on a specific sample.\nWe then sample the pseudo-labels using Bayesian Active Learning by Disagreement (BALD).\nThe teacher continuously gets better at creating pseudo-labels, improving the student’s performance.\nUST gets within a few percentages of models trained on datasets with thousands of labeled samples and beats UDA on several datasets.\nUncertainty-aware Self-training for Text Classification with Few Labels"
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#conclusion",
    "href": "posts/transformers-book-notes/chapter-9/index.html#conclusion",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "Conclusion",
    "text": "Conclusion\n\nSet up an evaluation pipeline to test different approaches for dealing with little to no labeled data.\nBuild a validation and test set early on.\nThere are tradeoffs between more complex approaches like UDA and UST and getting more data.\nIt might make more sense to create a small high-quality dataset rather than engineering a very complex method to compensate for the lack of data.\nAnnotating a few hundred examples usually takes a couple of hours to a few days.\nThere are many annotation tools available to speed up labeling new data."
  },
  {
    "objectID": "posts/transformers-book-notes/chapter-9/index.html#references",
    "href": "posts/transformers-book-notes/chapter-9/index.html#references",
    "title": "Notes on Transformers Book Ch. 9",
    "section": "References",
    "text": "References\n\nNatural Language Processing with Transformers Book\nThe Transformers book GitHub Repository\n\nPrevious: Notes on Transformers Book Ch. 8\nNext: Notes on Transformers Book Ch. 10"
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html",
    "href": "posts/triangle-motion-graphic-bpy/index.html",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "",
    "text": "Introduction\nImport Dependencies\nDefine Helper Functions\nSet up Scene\nCreate and Position Camera\nCreate Material With Emission Shader\nCreate a Cone With the Material\nTurn the Cone Into a Triangle\nDuplicate the Triangle\nCreate the Holdout Material\nAdd Keyframes\nConclusion"
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#introduction",
    "href": "posts/triangle-motion-graphic-bpy/index.html#introduction",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Introduction",
    "text": "Introduction\nI decided to recreate this short tutorial from YouTube to practice using the Blender Python API. This post goes through the code I came up with to replicate the tutorial."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#import-dependencies",
    "href": "posts/triangle-motion-graphic-bpy/index.html#import-dependencies",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Import Dependencies",
    "text": "Import Dependencies\nThe only dependencies strictly required for this tutorial are bpy and bmesh. The bpy package is the base API for Blender and the bmesh module provides access to Blender’s internal mesh editing API. I also used the math module from the Python Standard Library for one of my helper functions."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#define-helper-functions",
    "href": "posts/triangle-motion-graphic-bpy/index.html#define-helper-functions",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Define Helper Functions",
    "text": "Define Helper Functions\nI made some wrapper functions for the standard location, rotation, and scale transformations as well as getting the name of the active object.\nYou can get the name of the active object with bpy.context.active_object.name.\nThe three standard transformations can be accessed for individual objects with the following:\n\nbpy.data.objects[\"object_name\"].location\nbpy.data.objects[\"object_name\"].rotation_euler\nbpy.data.objects[\"object_name\"].scale\n\nI also made a function to empty the default collection so that nothing gets duplicated. Collections can be accessed with bpy.data.collections[\"collection_name\"] or bpy.data.collections[index].\nLastly, I made a function to easily add sequences of keyframes to a given object. The function uses the built-in setattr() method to set the desired value for the target object and uses the object.keyframe_insert() method to add the keyframe."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#set-up-scene",
    "href": "posts/triangle-motion-graphic-bpy/index.html#set-up-scene",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Set up Scene",
    "text": "Set up Scene\nThe first thing I do is set the Color Management property, View Transform, from the default value of Filmic to Standard. This setting can be accessed at bpy.data.scenes[\"Scene\"].view_settings.view_transform.\nThis tutorial requires transparency to be enabled. This can be done by setting bpy.data.scenes['Scene'].render.film_transparent to True.\nNext, I set the background to the desired color. In my case, it’s pure black. The background color is stored in bpy.data.worlds['World'].node_tree.nodes[\"Background\"].inputs[0].default_value.\nThe last setup step is to clear any objects added from the last time the script was run with the clear_collection() function."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#create-and-position-camera",
    "href": "posts/triangle-motion-graphic-bpy/index.html#create-and-position-camera",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Create and Position Camera",
    "text": "Create and Position Camera\nCameras can be added using the bpy.ops.object.camera_add() method. I then positioned the camera using the wrapper functions I defined earlier."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#create-material-with-emission-shader",
    "href": "posts/triangle-motion-graphic-bpy/index.html#create-material-with-emission-shader",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Create Material With Emission Shader",
    "text": "Create Material With Emission Shader\nI decided to add some color to the motion graphic so I needed to create a new material. It is recommended to check if the material exists before trying to create it. This can be done in one line as shown below.\nmaterial = bpy.data.materials.get(material_name) or bpy.data.materials.new(material_name)\nSince there’s is no light, I’ll add an Emission shader. This requires enabling nodes for the material with material.use_nodes = True.\nNext, I remove the default Principled_BSDF node as well as any Emission nodes from earlier runs. Nodes can be removed using the material.node_tree.nodes.remove() method.\nThe Emission node needs to be linked to the first slot in the Material Output node. Nodes are linked using the material.node_tree.links.new() method."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#create-a-cone-with-the-material",
    "href": "posts/triangle-motion-graphic-bpy/index.html#create-a-cone-with-the-material",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Create a Cone With the Material",
    "text": "Create a Cone With the Material\nThe motion graphic is made of two triangles with one being a duplicate of the other. The original triangle started off as a cone with 3 vertices. Cones can be added using the bpy.ops.mesh.primitive_cone_add() method.\nI then assign the previously created material to the cone. Materials can be added to an object with object.data.materials.append(material)."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#turn-the-cone-into-a-triangle",
    "href": "posts/triangle-motion-graphic-bpy/index.html#turn-the-cone-into-a-triangle",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Turn the Cone Into a Triangle",
    "text": "Turn the Cone Into a Triangle\nThe next step is to remove the tip of the cone. This requires modifying its mesh. Mesh data for the currently selected object is stored at bpy.context.object.data.\nTo edit the mesh, we need to get a BMesh representation. We first create an empty BMesh with bm = bmesh.new() and then fill it with the mesh using bm.from_mesh(mesh).\nWe can delete vertices with the bmesh.ops.delete() and setting the context to VERTS.\nThe mesh then needs to be updated with these alterations using bm.to_mesh(mesh). We need to free the BMesh representation we created with bm.free().\nFinally, I reset the origin of the triangle with bpy.ops.object.origin_set()."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#duplicate-the-triangle",
    "href": "posts/triangle-motion-graphic-bpy/index.html#duplicate-the-triangle",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Duplicate the Triangle",
    "text": "Duplicate the Triangle\nWe can make the second triangle with bpy.ops.object.duplicate()."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#create-the-holdout-material",
    "href": "posts/triangle-motion-graphic-bpy/index.html#create-the-holdout-material",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Create the Holdout Material",
    "text": "Create the Holdout Material\nWe need to add a Holdout material to the second triangle so we can see through anything behind it. The process is the same as adding the Emission shader."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#add-keyframes",
    "href": "posts/triangle-motion-graphic-bpy/index.html#add-keyframes",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Add Keyframes",
    "text": "Add Keyframes\nBefore adding the keyframes, I set the render frame rate as well the start and end frames for the scene. The frame rate is stored at bpy.context.scene.render.fps.\nThe start and end frames are stored in bpy.data.scenes['Scene'].frame_start and bpy.data.scenes['Scene'].frame_end respectively.\n\n\n\n\n\n\nX-ray Triangle\nWe only need to animate the rotation and scale for the x-ray triangle."
  },
  {
    "objectID": "posts/triangle-motion-graphic-bpy/index.html#conclusion",
    "href": "posts/triangle-motion-graphic-bpy/index.html#conclusion",
    "title": "Create a Triangle Motion Graphic with the Blender Python API",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial did not require learning any new parts of the API after the last tutorial I replicated. I guess in that sense, it was a waste of time. However, I still enjoyed working on it and I like the resulting motion graphic.\nTutorial Resources: GitHub Repository"
  },
  {
    "objectID": "posts/unity-barracuda-inference-base-walkthrough/index.html",
    "href": "posts/unity-barracuda-inference-base-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Barracuda Inference Base Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-barracuda-inference-base-walkthrough/index.html#introduction",
    "href": "posts/unity-barracuda-inference-base-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Barracuda Inference Base Package",
    "section": "Introduction",
    "text": "Introduction\nThe Barracuda Inference Base package provides a foundation for performing inference with the Barracuda inference library. It includes a flexible base class to extend with task-specific packages. Barracuda is a lightweight cross-platform neural network inference library for Unity.\nI use the Barracuda inference library in multiple tutorials. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Here are some demo videos from projects that extend this package.\nImage Classification\n\n\nVideo\n\n\nObject Detection\n\n\nVideo\n\n\nPose Estimation\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-barracuda-inference-base-walkthrough/index.html#package-overview",
    "href": "posts/unity-barracuda-inference-base-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Barracuda Inference Base Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains two C# scripts.\n\nBarracudaModelRunner.cs: This script provides an abstract class for running Barracuda neural network models in Unity.\nAddCustomDefineSymbol.cs: An Editor script that automatically adds a custom scripting define symbol to the project after the package installs."
  },
  {
    "objectID": "posts/unity-barracuda-inference-base-walkthrough/index.html#code-explanation",
    "href": "posts/unity-barracuda-inference-base-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Barracuda Inference Base Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Barracuda Inference Base package by examining the purpose and functionality of each C# script.\n\nBarracudaModelRunner.cs\nThis script provides an abstract class for running Barracuda neural network models in Unity. This class serves as a base class for others that perform inference using Barracuda to inherit from.\nThe complete code is available on GitHub at the link below.\n\nBarracudaModelRunner.cs\n\n\nModel Assets\nThese are serialized fields for the neural network model, channel order preference, and the execution backend.\n[Header(\"Model Assets\")]\n[Tooltip(\"The neural network model\")]\n[SerializeField] protected NNModel model;\n[Tooltip(\"Option to order tensor data channels first (EXPERIMENTAL)\")]\n[SerializeField] private bool useNCHW = true;\n[Tooltip(\"Execution backend for the model\")]\n[SerializeField] protected WorkerFactory.Type workerType = WorkerFactory.Type.Auto;\n\n\nModelBuilder and IWorker\nThese are Instances of ModelBuilder and IWorker for creating and executing models.\nprotected ModelBuilder modelBuilder;\nprotected IWorker engine;\n\n\nStart\nThe Start() method runs when the script is enabled. It loads and prepares the model and initializes the engine.\nprotected virtual void Start()\n{\n    LoadAndPrepareModel();\n    InitializeEngine();\n}\n\n\nLoadAndPrepareModel\nThis method loads and prepares the model for execution. Child classes can override this method to apply custom modifications to the model.\n/// &lt;summary&gt;\n/// Load and prepare the model for execution.\n/// Override this method to apply custom modifications to the model.\n/// &lt;/summary&gt;\nprotected virtual void LoadAndPrepareModel()\n{\n    Model runtimeModel = ModelLoader.Load(model);\n    modelBuilder = new ModelBuilder(runtimeModel);\n}\n\n\nInitializeWorker\nThis method initializes the worker for executing the model with the specified backend and channel order.\n/// &lt;summary&gt;\n/// Initialize the worker for executing the model with the specified backend and channel order.\n/// &lt;/summary&gt;\n/// &lt;param name=\"model\"&gt;The target model representation.&lt;/param&gt;\n/// &lt;param name=\"workerType\"&gt;The target compute backend.&lt;/param&gt;\n/// &lt;param name=\"useNCHW\"&gt;The channel order for the compute backend (default is true).&lt;/param&gt;\n/// &lt;returns&gt;An initialized worker instance.&lt;/returns&gt;\nprotected IWorker InitializeWorker(Model model, WorkerFactory.Type workerType, bool useNCHW = true)\n{\n    // Validate worker type\n    workerType = WorkerFactory.ValidateType(workerType);\n\n    // Set channel order if required\n    if (useNCHW)\n    {\n        ComputeInfo.channelsOrder = ComputeInfo.ChannelsOrder.NCHW;\n    }\n\n    // Create and return the worker instance\n    return WorkerFactory.CreateWorker(workerType, model);\n}\n\n\nInitializeEngine\nThis method initializes the inference engine by creating a worker instance.\n/// &lt;summary&gt;\n/// Initialize the inference engine.\n/// &lt;/summary&gt;\nprotected virtual void InitializeEngine()\n{\n    engine = WorkerFactory.CreateWorker(workerType, modelBuilder.model);\n    engine = InitializeWorker(modelBuilder.model, workerType, useNCHW);\n}\n\n\nExecuteModel\nThese overloaded methods execute the model with the provided input Tensor(s). Child classes can override them to implement custom input and output processing.\n/// &lt;summary&gt;\n/// Execute the model with the given input Tensor.\n/// Override this method to implement custom input and output processing.\n/// &lt;/summary&gt;\n/// &lt;param name=\"input\"&gt;The input Tensor for the model.&lt;/param&gt;\npublic virtual void ExecuteModel(Tensor input)\n{\n    engine.Execute(input);\n}\n\n/// &lt;summary&gt;\n/// Execute the model with the given input Tensor.\n/// Override this method to implement custom input and output processing.\n/// &lt;/summary&gt;\n/// &lt;param name=\"input\"&gt;The input Tensor for the model.&lt;/param&gt;\npublic virtual void ExecuteModel(IDictionary&lt;string, Tensor&gt; inputs)\n{\n    engine.Execute(inputs);\n}\n\n\nOnDisable\nThis method runs when the component is disabled. It cleans up resources by disposing of the engine.\n/// &lt;summary&gt;\n/// Clean up resources when the component is disabled.\n/// &lt;/summary&gt;\nprotected virtual void OnDisable()\n{\n    engine.Dispose();\n}\n\n\n\n\nAddCustomDefineSymbol.cs\nThis Editor script contains a class that adds a custom define symbol to the project. We can use this custom symbol to prevent code that relies on this package from executing unless the Barracuda Inference Base package is present. The complete code is available on GitHub at the link below.\n\nAddCustomDefineSymbol.cs\n\nusing UnityEditor;\nusing UnityEngine;\n\nnamespace CJM.BarracudaInference\n{\n    public class DependencyDefineSymbolAdder\n    {\n        private const string CustomDefineSymbol = \"CJM_BARRACUDA_INFERENCE\";\n\n        [InitializeOnLoadMethod]\n        public static void AddCustomDefineSymbol()\n        {\n            // Get the currently selected build target group\n            var buildTargetGroup = EditorUserBuildSettings.selectedBuildTargetGroup;\n            // Retrieve the current scripting define symbols for the selected build target group\n            var defines = PlayerSettings.GetScriptingDefineSymbolsForGroup(buildTargetGroup);\n\n            // Check if the CustomDefineSymbol is already present in the defines string\n            if (!defines.Contains(CustomDefineSymbol))\n            {\n                // Append the CustomDefineSymbol to the defines string, separated by a semicolon\n                defines += $\";{CustomDefineSymbol}\";\n                // Set the updated defines string as the new scripting define symbols for the selected build target group\n                PlayerSettings.SetScriptingDefineSymbolsForGroup(buildTargetGroup, defines);\n                // Log a message in the Unity console to inform the user that the custom define symbol has been added\n                Debug.Log($\"Added custom define symbol '{CustomDefineSymbol}' to the project.\");\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/unity-barracuda-inference-base-walkthrough/index.html#conclusion",
    "href": "posts/unity-barracuda-inference-base-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Barracuda Inference Base Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Barracuda Inference Base package. The package provides a foundation for performing inference with the Barracuda inference library with a flexible base class to extend with task-specific packages.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-barracuda-inference-base\n\nYou can find the code for the demo projects shown in the videos at the beginning of this post linked below.\n\nBarracuda Image Classification Demo: A simple Unity project demonstrating how to perform image classification with the barracuda-inference-image-classification package.\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package.\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package."
  },
  {
    "objectID": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html",
    "href": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Barracuda Inference Image Classification Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#introduction",
    "href": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Barracuda Inference Image Classification Package",
    "section": "Introduction",
    "text": "Introduction\nThe Barracuda Inference Image Classification package extends the functionality of unity-barracuda-inference-base to perform image classification using computer vision models.\nImage classification has numerous potential uses in Unity applications, from gesture recognition to analyzing user-generated content. This package makes it easy to add image classification functionality to Unity applications. Here is a demo video from a project that uses this package for gesture classification.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#package-overview",
    "href": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Barracuda Inference Image Classification Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains two C# scripts.\n\nMultiClassImageClassifier.cs: This script provides functionality to perform image classification using the Barracuda inference engine.\nPackageInstaller.cs: An Editor utility script for automatically installing a list of dependency packages defined in a JSON file."
  },
  {
    "objectID": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#code-explanation",
    "href": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Barracuda Inference Image Classification Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Barracuda Inference Image Classification package by examining the purpose and functionality of each C# script.\n\nMultiClassImageClassifier.cs\nThis script defines the MultiClassImageClassifier class, which extends the BarracudaModelRunner class from the Barracuda Inference Base package to perform image classification. The complete code is available on GitHub at the link below.\n\nMultiClassImageClassifier.cs\n\n\nSerialized Fields\nThe MultiClassImageClassifier class includes a field to add class labels with a JSON file.\n[Tooltip(\"JSON file with class labels\")]\n[SerializeField] private TextAsset classLabels;\nIt also includes a field to control how often to unload memory assets when using Barracuda’s Pixel Shader backend. The Pixel Shader backend enables GPU inference on platforms that don’t support Compute Shaders. However, there seems to be a bug in the current version of Barracuda, which does not release unused assets when using this backend. Left unchecked, this can fill up both system and GPU memory. We can address this by manually freeing memory. Doing that every frame can hurt performance, so we’ll only do it at set intervals.\n[Tooltip(\"Interval (in frames) for unloading unused assets with Pixel Shader backend\")]\n[SerializeField] private int pixelShaderUnloadInterval = 100;\n\n\nPrivate Variables\n// A counter for the number of frames processed.\nprivate int frameCounter = 0;\n\n// Indicates if the system supports asynchronous GPU readback\nprivate bool supportsAsyncGPUReadback = false;\n\n// The name of the transpose layer.\nprivate const string TransposeLayer = \"transpose\";\n// The softmax layer.\nprivate string SoftmaxLayer = \"softmaxLayer\";\n// The name of the output layer.\nprivate string outputLayer;\n\n// Helper class for deserializing class labels from the JSON file\nprivate class ClassLabels { public string[] classes; }\n\n// The class labels\nprivate string[] classes;\n\n// Texture formats for output processing\nprivate TextureFormat textureFormat = TextureFormat.RGBA32;\nprivate RenderTextureFormat renderTextureFormat = RenderTextureFormat.ARGB32;\n\n// Output textures for processing on CPU and GPU\nprivate Texture2D outputTextureCPU;\nprivate RenderTexture outputTextureGPU;\n\n\nStart\nThis method initializes necessary components at the start of the script, such as checking async GPU readback support, loading class labels, and creating output textures.\n/// &lt;summary&gt;\n/// Initialize necessary components during the start of the script.\n/// &lt;/summary&gt;\nprotected override void Start()\n{\n    base.Start();\n    CheckAsyncGPUReadbackSupport(); // Check if async GPU readback is supported\n    LoadClassLabels(); // Load class labels from JSON file\n    CreateOutputTextures(); // Initialize output texture\n}\n\n\nCheckAsyncGPUReadbackSupport\nThis method checks if the system supports asynchronous GPU readback\n// Check if the system supports async GPU readback\npublic bool CheckAsyncGPUReadbackSupport()\n{\n    supportsAsyncGPUReadback = SystemInfo.supportsAsyncGPUReadback && supportsAsyncGPUReadback;\n    return supportsAsyncGPUReadback;\n}\n\n\nLoadAndPrepareModel\nThis method loads the model and prepares it for execution. It applies the softmax function to the output layer if it’s not already a softmax layer.\n/// &lt;summary&gt;\n/// Load the model and prepare it for execution by applying softmax to the output layer.\n/// &lt;/summary&gt;\nprotected override void LoadAndPrepareModel()\n{\n    // Load and prepare the model with the base implementation\n    base.LoadAndPrepareModel();\n\n    outputLayer = modelBuilder.model.outputs[0];\n\n    // Set worker type for WebGL\n    if (Application.platform == RuntimePlatform.WebGLPlayer)\n    {\n        workerType = WorkerFactory.Type.PixelShader;\n    }\n\n    // Check if the last layer is a Softmax layer\n    Layer lastLayer = modelBuilder.model.layers[modelBuilder.model.layers.Count - 1];\n    bool lastLayerIsSoftmax = lastLayer.activation == Layer.Activation.Softmax;\n\n    // Add the Softmax layer if the last layer is not already a Softmax layer\n    if (!lastLayerIsSoftmax)\n    {\n        // Add the Softmax layer\n        modelBuilder.Softmax(SoftmaxLayer, outputLayer);\n        outputLayer = SoftmaxLayer;\n    }\n\n    // Apply transpose operation on the output layer\n    modelBuilder.Transpose(TransposeLayer, outputLayer, new[] { 0, 1, 3, 2 });\n    outputLayer = TransposeLayer;\n}\n\n\nInitializeEngine\nThis method initializes the inference engine and checks if the model uses a Compute Shader backend.\n/// &lt;summary&gt;\n/// Initialize the inference engine and check if the model is using a Compute Shader backend.\n/// &lt;/summary&gt;\nprotected override void InitializeEngine()\n{\n    base.InitializeEngine();\n\n    // Check if async GPU readback is supported by the engine\n    supportsAsyncGPUReadback = engine.Summary().Contains(\"Unity.Barracuda.ComputeVarsWithSharedModel\");\n}\n\n\nLoadClassLabels\nThis method loads the class labels from the provided JSON file.\n/// &lt;summary&gt;\n/// Load the class labels from the provided JSON file.\n/// &lt;/summary&gt;\nprivate void LoadClassLabels()\n{\n    if (IsClassLabelsJsonNullOrEmpty())\n    {\n        Debug.LogError(\"Class labels JSON is null or empty.\");\n        return;\n    }\n\n    ClassLabels classLabelsObj = DeserializeClassLabels(classLabels.text);\n    UpdateClassLabels(classLabelsObj);\n}\n\n\nIsClassLabelsJsonNullOrEmpty\nThis method checks if the provided class labels JSON file is null or empty.\n/// &lt;summary&gt;\n/// Check if the provided class labels JSON file is null or empty.\n/// &lt;/summary&gt;\n/// &lt;returns&gt;True if the file is null or empty, otherwise false.&lt;/returns&gt;\nprivate bool IsClassLabelsJsonNullOrEmpty()\n{\n    return classLabels == null || string.IsNullOrWhiteSpace(classLabels.text);\n}\n\n\nDeserializeClassLabels\nThis method deserializes the provided class labels JSON string to a ClassLabels object.\n/// &lt;summary&gt;\n/// Deserialize the provided class labels JSON string to a ClassLabels object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"json\"&gt;The JSON string to deserialize.&lt;/param&gt;\n/// &lt;returns&gt;A deserialized ClassLabels object.&lt;/returns&gt;\nprivate ClassLabels DeserializeClassLabels(string json)\n{\n    try\n    {\n        return JsonUtility.FromJson&lt;ClassLabels&gt;(json);\n    }\n    catch (Exception ex)\n    {\n        Debug.LogError($\"Failed to deserialize class labels JSON: {ex.Message}\");\n        return null;\n    }\n}\n\n\nUpdateClassLabels\nThis method updates the classes array with the provided ClassLabels object.\n/// &lt;summary&gt;\n/// Update the classes array with the provided ClassLabels object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"classLabelsObj\"&gt;The ClassLabels object containing the class labels.&lt;/param&gt;\nprivate void UpdateClassLabels(ClassLabels classLabelsObj)\n{\n    if (classLabelsObj == null)\n    {\n        return;\n    }\n\n    classes = classLabelsObj.classes;\n}\n\n\nCreateOutputTextures\nThis method creates the output textures that will store the model output.\n/// &lt;summary&gt;\n/// Create the output textures that will store the model output.\n/// &lt;/summary&gt;\nprivate void CreateOutputTextures()\n{\n    outputTextureCPU = new Texture2D(classes.Length, 1, textureFormat, false);\n    outputTextureGPU = RenderTexture.GetTemporary(classes.Length, 1, 0, renderTextureFormat);\n}\n\n\nExecuteModel\nThis method executes the model on the provided input texture.\n/// &lt;summary&gt;\n/// Execute the model on the provided input texture and return the output array.\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputTexture\"&gt;The input texture for the model.&lt;/param&gt;\npublic void ExecuteModel(RenderTexture inputTexture)\n{\n    using (Tensor input = new Tensor(inputTexture, channels: 3))\n    {\n        base.ExecuteModel(input);\n    }\n}\n\n\nCopyOutputToArray\nThis method copies the model output to a float array.\n/// &lt;summary&gt;\n/// Copy the model output to a float array.\n/// &lt;/summary&gt;\npublic float[] CopyOutputToArray()\n{\n    // Retrieve the output tensor from the engine\n    using (Tensor output = engine.PeekOutput(outputLayer))\n    {\n        if (workerType == WorkerFactory.Type.PixelShader)\n        {\n            frameCounter++;\n            if (frameCounter % pixelShaderUnloadInterval == 0)\n            {\n                Resources.UnloadUnusedAssets();\n                frameCounter = 0;\n            }\n        }\n        // Download the data from the tensor\n        return output.data.Download(output.shape);\n    }\n}\n\n\nCopyOutputToTexture\nThis method copies the model output to a texture.\n/// &lt;summary&gt;\n/// Copy the model output to a texture.\n/// &lt;/summary&gt;\npublic void CopyOutputToTexture()\n{\n    using (Tensor output = engine.PeekOutput(outputLayer))\n    {\n        // Store output tensor data in a RenderTexture\n        output.ToRenderTexture(outputTextureGPU);\n    }\n}\n\n\nCopyOutputWithAsyncReadback\nThis method copies the model output using asynchronous GPU readback if the platform supports it.\n/// &lt;summary&gt;\n/// Copy the model output using async GPU readback. If not supported, defaults to synchronous readback.\n/// &lt;/summary&gt;\npublic float[] CopyOutputWithAsyncReadback()\n{\n    if (!supportsAsyncGPUReadback)\n    {\n        Debug.Log(\"Async GPU Readback not supported. Defaulting to synchronous readback\");\n        return CopyOutputToArray();\n    }\n\n    CopyOutputToTexture();\n\n    AsyncGPUReadback.Request(outputTextureGPU, 0, textureFormat, OnCompleteReadback);\n\n    // Extract the output tensor data from the texture\n    Color[] outputColors = outputTextureCPU.GetPixels();\n    return outputColors.Select(color =&gt; color.r).ToArray();\n}\n\n\nGetClassName\nThis method gets the class name corresponding to the provided class index.\n/// &lt;summary&gt;\n/// Get the class name corresponding to the provided class index.\n/// &lt;/summary&gt;\n/// &lt;param name=\"classIndex\"&gt;The index of the class to retrieve.&lt;/param&gt;\n/// &lt;returns&gt;The class name corresponding to the class index.&lt;/returns&gt;\npublic string GetClassName(int classIndex)\n{\n    return classes[classIndex];\n}\n\n\nOnCompleteReadback\nThis callback method handles the completion of async GPU readback.\n/// &lt;summary&gt;\n/// Callback method for handling the completion of async GPU readback.\n/// &lt;/summary&gt;\n/// &lt;param name=\"request\"&gt;The async GPU readback request.&lt;/param&gt;\nprivate void OnCompleteReadback(AsyncGPUReadbackRequest request)\n{\n    if (request.hasError)\n    {\n        Debug.Log(\"GPU readback error detected.\");\n        return;\n    }\n\n    if (outputTextureCPU != null)\n    {\n        try\n        {\n            // Load readback data into the output texture and apply changes\n            outputTextureCPU.LoadRawTextureData(request.GetData&lt;uint&gt;());\n            outputTextureCPU.Apply();\n        }\n        catch (UnityException ex)\n        {\n            if (ex.Message.Contains(\"LoadRawTextureData: not enough data provided (will result in overread).\"))\n            {\n                Debug.Log(\"Updating input data size to match the texture size.\");\n            }\n            else\n            {\n                Debug.LogError($\"Unexpected UnityException: {ex.Message}\");\n            }\n        }\n    }\n}\n\n\nOnDisable\nThis method cleans up resources when the script is disabled, such as releasing the temporary render texture.\n/// &lt;summary&gt;\n/// Clean up resources when the script is disabled.\n/// &lt;/summary&gt;\nprotected override void OnDisable()\n{\n    base.OnDisable();\n    // Release the temporary render texture\n    RenderTexture.ReleaseTemporary(outputTextureGPU);\n}\n\n\n\n\nPackageInstaller.cs\nIn this section, we will go through the PackageInstaller.cs script and explain how each part of the code works to install the required packages. The complete code is available on GitHub at the link below.\n\nPackageInstaller.cs\n\n\nSerializable Classes\nThe script defines two serializable classes to hold package data.\n// Serializable class to hold package data\n[System.Serializable]\npublic class PackageData\n{\n    public string packageName;\n    public string packageUrl;\n}\n\n// Serializable class to hold a list of PackageData objects\n[System.Serializable]\npublic class PackageList\n{\n    public List&lt;PackageData&gt; packages;\n}\nThese classes are for deserializing the JSON file containing the list of packages to install.\n\n\nPackageInstaller Class Variables\nThe PackageInstaller class contains several private static fields.\n// Stores the AddRequest object for the current package to install.\nprivate static AddRequest addRequest;\n// A list of PackageData objects to install.\nprivate static List&lt;PackageData&gt; packagesToInstall;\n// The index of the current package to install.\nprivate static int currentPackageIndex;\n\n// GUID of the JSON file containing the list of packages to install\nprivate const string PackagesJSONGUID = \"4a3b2c83681748b49d28cb6ed4f587d9\";\n\n\nInstallDependencies\nThe InstallDependencies() method executes when Unity loads without action from the user. It reads the package JSON file and calls the InstallNextPackage() method to install the packages.\n// Method called on load to install packages from the JSON file\n[InitializeOnLoadMethod]\npublic static void InstallDependencies()\n{\n    // Read the package JSON file\n    packagesToInstall = ReadPackageJson().packages;\n    // Initialize the current package index\n    currentPackageIndex = 0;\n    // Start installing the packages\n    InstallNextPackage();\n}\n\n\nInstallNextPackage\nThis method installs the next package in the list.\n// Method to install the next package in the list\nprivate static void InstallNextPackage()\n{\n    // Iterate through package list\n    if (currentPackageIndex &lt; packagesToInstall.Count)\n    {\n        PackageData packageData = packagesToInstall[currentPackageIndex];\n\n        // Check if the package is already installed\n        if (!IsPackageInstalled(packageData.packageName))\n        {\n            // Attempt to install package\n            addRequest = Client.Add(packageData.packageUrl);\n            EditorApplication.update += PackageInstallationProgress;\n        }\n        else\n        {\n            // Increment the current package index\n            currentPackageIndex++;\n            // Recursively call InstallNextPackage\n            InstallNextPackage();\n        }\n    }\n}\n\n\nPackageInstallationProgress\nThis method monitors the progress of the package installation and logs whether it was successful. It then triggers the installation process for the next package in the list.\n// Method to monitor the progress of package installation\nprivate static void PackageInstallationProgress()\n{\n    if (addRequest.IsCompleted)\n    {\n        // Log whether the package installation was successful\n        if (addRequest.Status == StatusCode.Success)\n        {\n            UnityEngine.Debug.Log($\"Successfully installed: {addRequest.Result.packageId}\");\n        }\n        else if (addRequest.Status &gt;= StatusCode.Failure)\n        {\n            UnityEngine.Debug.LogError($\"Failed to install package: {addRequest.Error.message}\");\n        }\n\n        // Unregister the method from the EditorApplication.update \n        EditorApplication.update -= PackageInstallationProgress;\n        // Increment the current package index\n        currentPackageIndex++;\n        // Install the next package in the list\n        InstallNextPackage();\n    }\n}\n\n\nIsPackageInstalled\nThis method verifies whether a package has already been installed or not.\n// Method to check if a package is already installed\nprivate static bool IsPackageInstalled(string packageName)\n{\n    // List the installed packages\n    var listRequest = Client.List(true, false);\n    while (!listRequest.IsCompleted) { }\n\n    if (listRequest.Status == StatusCode.Success)\n    {\n        // Check if the package is already installed\n        return listRequest.Result.Any(package =&gt; package.name == packageName);\n    }\n    else\n    {\n        UnityEngine.Debug.LogError($\"Failed to list packages: {listRequest.Error.message}\");\n    }\n\n    return false;\n}\n\n\nReadPackageJson\nThis method reads the JSON file containing the list of packages to install and returns a PackageList object.\n// Method to read the JSON file and return a PackageList object\nprivate static PackageList ReadPackageJson()\n{\n    // Convert the PackagesJSONGUID to an asset path\n    string assetPath = AssetDatabase.GUIDToAssetPath(PackagesJSONGUID);\n    // Read the JSON file content as a string\n    string jsonString = File.ReadAllText(assetPath);\n    // Deserialize the JSON string into a PackageList object\n    return JsonUtility.FromJson&lt;PackageList&gt;(jsonString);\n}"
  },
  {
    "objectID": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#conclusion",
    "href": "posts/unity-barracuda-inference-image-classification-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Barracuda Inference Image Classification Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Barracuda Inference Image Classification package. The package extends the functionality of unity-barracuda-inference-base to perform image classification using computer vision models.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-barracuda-inference-image-classification\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Image Classification Demo: A simple Unity project demonstrating how to perform image classification with the barracuda-inference-image-classification package."
  },
  {
    "objectID": "posts/unity-barracuda-inference-posenet-walkthrough/index.html",
    "href": "posts/unity-barracuda-inference-posenet-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Barracuda Inference PoseNet Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#introduction",
    "href": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Barracuda Inference PoseNet Package",
    "section": "Introduction",
    "text": "Introduction\nThe Barracuda Inference PoseNet package extends the functionality of unity-barracuda-inference-base to perform 2D human pose estimation using PoseNet models.\nPose estimation has numerous potential uses in Unity applications, including motion capture and animation, educational apps, and augmented reality, to name a few. Here is a demo video from a project that uses this package.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#package-overview",
    "href": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Barracuda Inference PoseNet Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains two C# scripts.\n\nPoseNetPoseEstimator.cs: This script provides functionality to perform 2D human pose estimation with PoseNet models using the Barracuda inference engine.\nPackageInstaller.cs: An Editor utility script for automatically installing a list of dependency packages defined in a JSON file."
  },
  {
    "objectID": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#code-explanation",
    "href": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Barracuda Inference PoseNet Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Barracuda Inference PoseNet package by examining the purpose and functionality of each C# script.\n\nPoseNetPoseEstimator.cs\nThis script defines the PoseNetPoseEstimator class, which extends the BarracudaModelRunner class from the Barracuda Inference Base package to perform 2D human pose estimation using PoseNet models. This class also depends on the human-pose-2d-toolkit package. The complete code is available on GitHub at the link below.\n\nPoseNetPoseEstimator.cs\n\n\nSerialized Fields\nThe class has several serialized fields for configuring the model, and processing output.\n[Header(\"PoseNet Model Configuration\")]\n[SerializeField, Tooltip(\"Index of the heatmap layer in the neural network\")]\nprivate int heatmapLayerIndex = 0;\n\n[SerializeField, Tooltip(\"Index of the offsets layer in the neural network\")]\nprivate int offsetsLayerIndex = 1;\n\n[SerializeField, Tooltip(\"Index of the forward displacement layer in the neural network\")]\nprivate int displacementFWDLayerIndex = 3;\n\n[SerializeField, Tooltip(\"Index of the backward displacement layer in the neural network\")]\nprivate int displacementBWDLayerIndex = 2;\n\n[Header(\"Output Processing\")]\n[SerializeField, Tooltip(\"TextAsset containing the class labels for pose estimation\")]\nprivate TextAsset classLabels;\nIt also includes a field to control how often to unload memory assets when using Barracuda’s Pixel Shader backend. The Pixel Shader backend enables GPU inference on platforms that don’t support Compute Shaders. However, there seems to be a bug in the current version of Barracuda, which does not release unused assets when using this backend. Left unchecked, this can fill up both system and GPU memory. We can address this by manually freeing memory. Doing that every frame can hurt performance, so we’ll only do it at set intervals.\n[Header(\"Settings\")]\n[SerializeField, Tooltip(\"Interval at which pixel shaders are unloaded\")]\nprivate int pixelShaderUnloadInterval = 100;\n\n\nPrivate Variables\nThere are internal variables for handling class labels and parent-children relationships of pose skeleton points.\n// Internal Variables\nprivate int frameCounter = 0;\nprivate class ClassLabels { public string[] classes; }\nprivate const int kLocalMaximumRadius = 1;\n\n// Parent-children relationships of the pose keypoints\nprivate readonly (int, int)[] parentChildrenTuples = {\n    (0, 1), (1, 3), (0, 2), (2, 4), (0, 5), (5, 7),\n    (7, 9), (5, 11), (11, 13), (13, 15), (0, 6),\n    (6, 8), (8, 10), (6, 12), (12, 14), (14, 16)\n};\n\nprivate const int maxStride = 32;\nprivate const string SigmoidLayer = \"sigmoid\";\n\n// Layer names for the neural network\nprivate string offsetsLayer;\nprivate string displacementFWDLayer;\nprivate string displacementBWDLayer;\n\n// Class labels array\nprivate string[] classes;\n\n// Smallest dimension of the input image\nprivate int minDim = 0;\n\n\nStart\nThis method initializes the pose estimation component by calling the Start() method of the parent class and loading class labels.\n/// &lt;summary&gt;\n/// Initializes the pose estimation component.\n/// &lt;/summary&gt;\nprotected override void Start()\n{\n    base.Start();\n    LoadClassLabels();\n}\n\n\nLoadAndPrepareModel\nThis method loads and prepares the PoseNet model for pose estimation. It sets the worker type to PixelShader if running on WebGL, gets the output layers for the heatmap, offsets, forward displacement, and backward displacement, and adds a sigmoid layer if the last layer is not sigmoid.\n/// &lt;summary&gt;\n/// Loads and prepares the model for pose estimation.\n/// &lt;/summary&gt;\nprotected override void LoadAndPrepareModel()\n{\n    base.LoadAndPrepareModel();\n\n    // Set worker type to PixelShader if running on WebGL\n    if (Application.platform == RuntimePlatform.WebGLPlayer)\n    {\n        workerType = WorkerFactory.Type.PixelShader;\n    }\n\n    // Get the output layer of the heatmap\n    string outputLayer = modelBuilder.model.outputs[heatmapLayerIndex];\n\n    // Find the heatmap layer in the model\n    Layer heatmapLayer = FindLayerByName(modelBuilder.model, outputLayer);\n    bool lastLayerIsSigmoid = heatmapLayer.activation == Layer.Activation.Sigmoid;\n\n    // Add a sigmoid layer if the last layer is not sigmoid\n    if (!lastLayerIsSigmoid)\n    {\n        modelBuilder.Sigmoid(SigmoidLayer, outputLayer);\n    }\n\n    // Get the names of the output layers for offsets, forward, and backward displacements\n    offsetsLayer = modelBuilder.model.outputs[offsetsLayerIndex];\n    displacementFWDLayer = modelBuilder.model.outputs[displacementFWDLayerIndex];\n    displacementBWDLayer = modelBuilder.model.outputs[displacementBWDLayerIndex];\n}\n\n\nFindLayerByName\nThis method searches for a layer in the model by its name and returns the layer if found.\n/// &lt;summary&gt;\n/// Finds a layer in the model by its name.\n/// &lt;/summary&gt;\n/// &lt;param name=\"model\"&gt;The model to search for the layer.&lt;/param&gt;\n/// &lt;param name=\"layerName\"&gt;The name of the layer to find.&lt;/param&gt;\n/// &lt;returns&gt;The layer with the given name or null if not found.&lt;/returns&gt;\nLayer FindLayerByName(Model model, string layerName)\n{\n    foreach (Layer layer in model.layers)\n    {\n        if (layer.name == layerName)\n        {\n            return layer;\n        }\n    }\n\n    return null;\n}\n\n\nLoadClassLabels\nThis method loads the class labels from the TextAsset, deserializes the JSON, and updates the classes array.\n/// &lt;summary&gt;\n/// Loads the class labels from the TextAsset and updates the classes array.\n/// &lt;/summary&gt;\nprivate void LoadClassLabels()\n{\n    if (IsClassLabelsJsonNullOrEmpty())\n    {\n        Debug.LogError(\"Class labels JSON is null or empty.\");\n        return;\n    }\n\n    ClassLabels classLabelsObj = DeserializeClassLabels(classLabels.text);\n    UpdateClassLabels(classLabelsObj);\n}\n\n\nIsClassLabelsJsonNullOrEmpty\nThis method checks if the provided class label JSON file is null or empty.\n/// &lt;summary&gt;\n/// Checks if the class labels JSON is null or empty.\n/// &lt;/summary&gt;\n/// &lt;returns&gt;True if the JSON is null or empty, false otherwise.&lt;/returns&gt;\nprivate bool IsClassLabelsJsonNullOrEmpty()\n{\n    return classLabels == null || string.IsNullOrWhiteSpace(classLabels.text);\n}\n\n\nDeserializeClassLabels\nThis method deserializes the provided class label JSON string to a ClassLabels object.\n/// &lt;summary&gt;\n/// Deserializes the class labels JSON into a ClassLabels object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"json\"&gt;The class labels JSON string.&lt;/param&gt;\n/// &lt;returns&gt;A ClassLabels object, or null if deserialization fails.&lt;/returns&gt;\nprivate ClassLabels DeserializeClassLabels(string json)\n{\n    try\n    {\n        return JsonUtility.FromJson&lt;ClassLabels&gt;(json);\n    }\n    catch (Exception ex)\n    {\n        Debug.LogError($\"Failed to deserialize class labels JSON: {ex.Message}\");\n        return null;\n    }\n}\n\n\nUpdateClassLabels\nThis method updates the classes array with the provided ClassLabels object.\n/// &lt;summary&gt;\n/// Updates the classes array with the contents of the given ClassLabels object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"classLabelsObj\"&gt;The ClassLabels object containing class labels.&lt;/param&gt;\nprivate void UpdateClassLabels(ClassLabels classLabelsObj)\n{\n    if (classLabelsObj == null)\n    {\n        return;\n    }\n\n    classes = classLabelsObj.classes;\n}\n\n\nExecuteModel\nThis method executes the PoseNet model with the given input texture.\n/// &lt;summary&gt;\n/// Executes the model with the given input texture.\n/// &lt;/summary&gt;\n/// &lt;param name=\"inputTexture\"&gt;The input texture to process.&lt;/param&gt;\npublic void ExecuteModel(RenderTexture inputTexture)\n{\n    minDim = Mathf.Min(inputTexture.width, inputTexture.height);\n\n    using (Tensor input = new Tensor(inputTexture, channels: 3))\n    {\n        base.ExecuteModel(input);\n    }\n}\n\n\nProcessOutput\nThis method processes the output tensors and returns an array of detected human poses. It can use either single-pose decoding or multiple-pose decoding.\n/// &lt;summary&gt;\n/// Processes the output tensors and returns an array of detected human poses.\n/// &lt;/summary&gt;\n/// &lt;param name=\"useMultiPoseDecoding\"&gt;True to use multiple pose decoding, false to use single pose decoding.&lt;/param&gt;\n/// &lt;param name=\"maxPoses\"&gt;The maximum number of poses to detect.&lt;/param&gt;\n/// &lt;returns&gt;An array of detected human poses.&lt;/returns&gt;\npublic HumanPose2D[] ProcessOutput(float scoreThreshold, int nmsRadius, int maxPoses = 20, bool useMultiPoseDecoding = true)\n{\n    // Initialize a list to store the detected human poses\n    List&lt;HumanPose2D&gt; humanPoses = new List&lt;HumanPose2D&gt;();\n\n    // Get the output tensors from the neural network\n    using Tensor heatmaps = engine.PeekOutput(SigmoidLayer);\n    using Tensor offsets = engine.PeekOutput(offsetsLayer);\n    using Tensor displacementFWD = engine.PeekOutput(displacementFWDLayer);\n    using Tensor displacementBWD = engine.PeekOutput(displacementBWDLayer);\n\n    // Calculate the stride based on the dimensions of the heatmaps\n    int minHeatMapDim = Mathf.Min(heatmaps.width, heatmaps.height);\n    int stride = (minDim - 1) / (minHeatMapDim - 1);\n    stride -= (stride % 8);\n\n    // Decide whether to use single pose decoding or multiple pose decoding\n    if (useMultiPoseDecoding)\n    {\n        // Decode multiple poses and store them in the humanPoses list\n        humanPoses = DecodeMultiplePoses(\n            heatmaps, offsets,\n            displacementFWD, displacementBWD,\n            stride, maxPoses, scoreThreshold, nmsRadius);   \n    }\n    else\n    {\n        // Decode a single pose and add it to the humanPoses list\n        HumanPose2D pose = new HumanPose2D\n        {\n            index = 0,\n            bodyParts = DecodeSinglePose(heatmaps, offsets, stride)\n        };\n        humanPoses.Add(pose);\n    }\n\n    // Unload unused assets if needed\n    UnloadUnusedAssetsIfNeeded();\n\n    // Convert the list of human poses to an array and return it\n    return humanPoses.ToArray();\n}\n\n\nUnloadUnusedAssetsIfNeeded\nThis method unloads unused assets if needed based on the worker type and frame counter.\n/// &lt;summary&gt;\n/// Unloads unused assets if needed based on the worker type and frame counter.\n/// &lt;/summary&gt;\nprivate void UnloadUnusedAssetsIfNeeded()\n{\n    if (workerType != WorkerFactory.Type.PixelShader) return;\n\n    frameCounter++;\n    if (frameCounter % pixelShaderUnloadInterval == 0)\n    {\n        Resources.UnloadUnusedAssets();\n        frameCounter = 0;\n    }\n}\n\n\nDecodeSinglePose\nThis method decodes a single human pose from the given heatmaps and offsets tensors and returns an array of body parts.\n/// &lt;summary&gt;\n/// Decodes a single human pose from the given heatmaps and offsets tensors.\n/// &lt;/summary&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps tensor.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;returns&gt;An array of body parts for the decoded pose.&lt;/returns&gt;\npublic BodyPart2D[] DecodeSinglePose(Tensor heatmaps, Tensor offsets, int stride)\n{\n    int numBodyParts = heatmaps.channels;\n    BodyPart2D[] bodyParts = new BodyPart2D[numBodyParts];\n\n    for (int c = 0; c &lt; numBodyParts; c++)\n    {\n        BodyPart2D part = FindHighestConfidenceBodyPart(heatmaps, c);\n        part.coordinates = GetImageCoords(part, stride, offsets);\n        bodyParts[c] = part;\n    }\n\n    return bodyParts;\n}\n\n\nFindHighestConfidenceBodyPart\nThis method finds the body part with the highest confidence for the given channel in the heatmaps tensor and returns the body part.\n/// &lt;summary&gt;\n/// Finds the body part with the highest confidence for the given channel in the heatmaps tensor.\n/// &lt;/summary&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps tensor.&lt;/param&gt;\n/// &lt;param name=\"channel\"&gt;The channel representing the body part to search for.&lt;/param&gt;\n/// &lt;returns&gt;The body part with the highest confidence.&lt;/returns&gt;\nprivate BodyPart2D FindHighestConfidenceBodyPart(Tensor heatmaps, int channel)\n{\n    BodyPart2D part = new BodyPart2D { index = channel, prob = 0 };\n\n    for (int y = 0; y &lt; heatmaps.height; y++)\n    {\n        for (int x = 0; x &lt; heatmaps.width; x++)\n        {\n            float confidence = heatmaps[0, y, x, channel];\n            if (confidence &gt; part.prob)\n            {\n                part.prob = confidence;\n                part.coordinates.x = x;\n                part.coordinates.y = y;\n            }\n        }\n    }\n\n    return part;\n}\n\n\nGetOffsetVector\nThis method returns the offset vector for the given coordinates and keypoint in the offsets tensor.\n/// &lt;summary&gt;\n/// Returns the offset vector for the given coordinates and keypoint in the offsets tensor.\n/// &lt;/summary&gt;\n/// &lt;param name=\"y\"&gt;The y-coordinate.&lt;/param&gt;\n/// &lt;param name=\"x\"&gt;The x-coordinate.&lt;/param&gt;\n/// &lt;param name=\"keypoint\"&gt;The keypoint index.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;returns&gt;The offset vector for the specified keypoint.&lt;/returns&gt;\npublic Vector2 GetOffsetVector(int y, int x, int keypoint, Tensor offsets)\n{\n    int channelOffset = offsets.channels / 2;\n    return new Vector2(offsets[0, y, x, keypoint + channelOffset], offsets[0, y, x, keypoint]);\n}\n\n\nGetImageCoords\nThis method converts body part coordinates to image coordinates using the given stride and offsets tensor.\n/// &lt;summary&gt;\n/// Converts body part coordinates to image coordinates using the given stride and offsets tensor.\n/// &lt;/summary&gt;\n/// &lt;param name=\"part\"&gt;The body part with heatmap coordinates.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;returns&gt;The image coordinates for the given body part.&lt;/returns&gt;\npublic Vector2 GetImageCoords(BodyPart2D part, int stride, Tensor offsets)\n{\n    Vector2 offsetVector = GetOffsetVector((int)part.coordinates.y, (int)part.coordinates.x, part.index, offsets);\n    return (part.coordinates * stride) + offsetVector;\n}\n\n\nGetStridedIndexNearPoint\nThis method gets the stridden index near a given point, given the stride, tensor height, and tensor width.\n/// &lt;summary&gt;\n/// Gets the strided index near a given point.\n/// &lt;/summary&gt;\n/// &lt;param name=\"point\"&gt;The point for which the strided index is calculated.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;The height of the tensor.&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;The width of the tensor.&lt;/param&gt;\n/// &lt;returns&gt;The strided index as a Vector2Int.&lt;/returns&gt;\npublic Vector2Int GetStridedIndexNearPoint(Vector2 point, int stride, int height, int width)\n{\n    return new Vector2Int(\n        Mathf.Clamp(Mathf.RoundToInt(point.x / stride), 0, width - 1),\n        Mathf.Clamp(Mathf.RoundToInt(point.y / stride), 0, height - 1)\n    );\n}\n\n\nGetDisplacement\nThis method gets the displacement for the specified edge and point in the displacements tensor and returns it as a Vector2.\n/// &lt;summary&gt;\n/// Gets the displacement for the specified edge and point in the displacements tensor.\n/// &lt;/summary&gt;\n/// &lt;param name=\"edgeId\"&gt;The edge index.&lt;/param&gt;\n/// &lt;param name=\"point\"&gt;The point as a Vector2Int.&lt;/param&gt;\n/// &lt;param name=\"displacements\"&gt;The displacements tensor.&lt;/param&gt;\n/// &lt;returns&gt;The displacement as a Vector2.&lt;/returns&gt;\npublic Vector2 GetDisplacement(int edgeId, Vector2Int point, Tensor displacements)\n{\n    int numEdges = displacements.channels / 2;\n    return new Vector2(\n        displacements[0, point.y, point.x, numEdges + edgeId],\n        displacements[0, point.y, point.x, edgeId]\n    );\n}\n\n\nTraverseToTargetBodyPart2D\nThis method takes an edge index, a source body part, a target body part index, and tensors for scores, offsets, stride, and displacements. It calculates the displaced point by adding the displacement value to the source body part coordinates and returns the target body part as a BodyPart2D instance.\n/// &lt;summary&gt;\n/// Traverses to the target body part from the source body part using the given edge.\n/// &lt;/summary&gt;\n/// &lt;param name=\"edgeId\"&gt;The edge index.&lt;/param&gt;\n/// &lt;param name=\"sourceBodyPart2D\"&gt;The source body part.&lt;/param&gt;\n/// &lt;param name=\"targetBodyPart2DId\"&gt;The target body part index.&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;The scores tensor.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"displacements\"&gt;The displacements tensor.&lt;/param&gt;\n/// &lt;returns&gt;The target body part as a BodyPart2D.&lt;/returns&gt;\npublic BodyPart2D TraverseToTargetBodyPart2D(\n    int edgeId, BodyPart2D sourceBodyPart2D, int targetBodyPart2DId,\n    Tensor scores, Tensor offsets, int stride,\n    Tensor displacements)\n{\n    // Get height and width from the scores tensor\n    int height = scores.height;\n    int width = scores.width;\n\n    // Calculate the source body part indices in the strided space\n    Vector2Int sourceBodyPart2DIndices = GetStridedIndexNearPoint(sourceBodyPart2D.coordinates, stride, height, width);\n\n    // Get the displacement for the given edge\n    Vector2 displacement = GetDisplacement(edgeId, sourceBodyPart2DIndices, displacements);\n\n    // Calculate the displaced point by adding the displacement to the source body part coordinates\n    Vector2 displacedPoint = sourceBodyPart2D.coordinates + displacement;\n\n    // Calculate the displaced point indices in the strided space\n    Vector2Int displacedPointIndices = GetStridedIndexNearPoint(displacedPoint, stride, height, width);\n\n    // Get the offset vector for the target body part\n    Vector2 offsetVector = GetOffsetVector(displacedPointIndices.y, displacedPointIndices.x, targetBodyPart2DId, offsets);\n\n    // Get the score for the target body part\n    float score = scores[0, displacedPointIndices.y, displacedPointIndices.x, targetBodyPart2DId];\n\n    // Calculate the target body part coordinates by adding the offset vector to the displaced point indices\n    Vector2 targetBodyPart2D = (displacedPointIndices * stride) + offsetVector;\n\n    // Return the target body part as a BodyPart2D instance\n    return new BodyPart2D(targetBodyPart2DId, targetBodyPart2D, score);\n}\n\n\nDecodePose\nThis method takes a root body part, tensors for scores, offsets, stride, the forward and backward displacements, and returns an array of BodyPart2D instances for the decoded pose.\n/// &lt;summary&gt;\n/// Decodes the pose given a root body part, scores, offsets, stride, and displacements tensors.\n/// &lt;/summary&gt;\n/// &lt;param name=\"root\"&gt;The root BodyPart2D.&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;The scores tensor.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"displacementsFwd\"&gt;The forward displacements tensor.&lt;/param&gt;\n/// &lt;param name=\"displacementsBwd\"&gt;The backward displacements tensor.&lt;/param&gt;\n/// &lt;returns&gt;An array of BodyPart2D for the decoded pose.&lt;/returns&gt;\npublic BodyPart2D[] DecodePose(\n    BodyPart2D root, Tensor scores, Tensor offsets,\n    int stride, Tensor displacementsFwd, Tensor displacementsBwd)\n{\n    // Get the number of body parts from the scores tensor\n    int numBodyParts = scores.channels;\n\n    // Initialize an array of BodyPart2D instances for storing the decoded pose\n    BodyPart2D[] instanceBodyParts = new BodyPart2D[numBodyParts];\n\n    // Compute the root point coordinates in the image and store it in the array\n    Vector2 rootPoint = GetImageCoords(root, stride, offsets);\n    instanceBodyParts[root.index] = new BodyPart2D(root.index, rootPoint, root.prob);\n\n    // Get the number of edges from parentChildrenTuples\n    int numEdges = parentChildrenTuples.Length;\n\n    // Traverse the edges in both directions to decode the pose\n    TraverseEdges(instanceBodyParts, scores, offsets, stride, displacementsBwd, numEdges, reverse: true);\n    TraverseEdges(instanceBodyParts, scores, offsets, stride, displacementsFwd, numEdges, reverse: false);\n\n    // Return the decoded pose as an array of BodyPart2D instances\n    return instanceBodyParts;\n}\n\n\nTraverseEdges\nThis method traverses edges from the source to the target body part, updating the position and probability of the target body part in the instanceBodyParts array.\n/// &lt;summary&gt;\n/// Traverses edges from the source to the target body part.\n/// &lt;/summary&gt;\n/// &lt;param name=\"instanceBodyParts\"&gt;An array of BodyPart2D instances.&lt;/param&gt;\n/// &lt;param name=\"scores\"&gt;The scores tensor.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"displacements\"&gt;The displacements tensor.&lt;/param&gt;\n/// &lt;param name=\"numEdges\"&gt;The number of edges.&lt;/param&gt;\n/// &lt;param name=\"reverse\"&gt;Whether to reverse the traversal direction.&lt;/param&gt;\nprivate void TraverseEdges(\n    BodyPart2D[] instanceBodyParts, Tensor scores, Tensor offsets,\n    int stride, Tensor displacements, int numEdges, bool reverse)\n{\n    // Set the start, end, and step of the edge traversal based on the reverse flag\n    int edgeStart = reverse ? numEdges - 1 : 0;\n    int edgeEnd = reverse ? -1 : numEdges;\n    int edgeStep = reverse ? -1 : 1;\n\n    // Traverse the edges in the specified direction\n    for (int edge = edgeStart; edge != edgeEnd; edge += edgeStep)\n    {\n        (int sourceBodyPartId, int targetBodyPartId) = parentChildrenTuples[edge];\n\n        // Swap source and target body part IDs if traversing in reverse\n        if (reverse)\n        {\n            (sourceBodyPartId, targetBodyPartId) = (targetBodyPartId, sourceBodyPartId);\n        }\n\n        // If the source body part has a probability greater than 0 and the target body part has not been detected,\n        // traverse to the target body part and update its position and probability in the instanceBodyParts array\n        if (instanceBodyParts[sourceBodyPartId].prob &gt; 0.0f &&\n            instanceBodyParts[targetBodyPartId].prob == 0.0f)\n        {\n            instanceBodyParts[targetBodyPartId] = TraverseToTargetBodyPart2D(\n                edge, instanceBodyParts[sourceBodyPartId], targetBodyPartId,\n                scores, offsets, stride, displacements);\n        }\n    }\n}\n\n\nScoreIsMaximumInLocalWindow\nThe ScoreIsMaximumInLocalWindow method checks if a given score is the maximum in a local window around the pose skeleton point.\n/// &lt;summary&gt;\n/// Checks if a score is the maximum in a local window around the keypoint.\n/// &lt;/summary&gt;\n/// &lt;param name=\"keypointId\"&gt;The keypoint index.&lt;/param&gt;\n/// &lt;param name=\"score\"&gt;The score to check.&lt;/param&gt;\n/// &lt;param name=\"heatmapY\"&gt;The y-coordinate of the keypoint in the heatmap.&lt;/param&gt;\n/// &lt;param name=\"heatmapX\"&gt;The x-coordinate of the keypoint in the heatmap.&lt;/param&gt;\n/// &lt;param name=\"localMaximumRadius\"&gt;The radius of the local window to search.&lt;/param&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps tensor.&lt;/param&gt;\n/// &lt;returns&gt;True if the score is the maximum in the local window, false otherwise.&lt;/returns&gt;\npublic bool ScoreIsMaximumInLocalWindow(int keypointId, float score, int heatmapY, int heatmapX,\n    int localMaximumRadius, Tensor heatmaps)\n{\n    int yStart = Mathf.Max(heatmapY - localMaximumRadius, 0);\n    int yEnd = Mathf.Min(heatmapY + localMaximumRadius + 1, heatmaps.height);\n\n    // Iterate through the local window around the keypoint\n    for (int yCurrent = yStart; yCurrent &lt; yEnd; ++yCurrent)\n    {\n        int xStart = Mathf.Max(heatmapX - localMaximumRadius, 0);\n        int xEnd = Mathf.Min(heatmapX + localMaximumRadius + 1, heatmaps.width);\n\n        for (int xCurrent = xStart; xCurrent &lt; xEnd; ++xCurrent)\n        {\n            // If any value in the local window is greater than the score,\n            // it is not the maximum\n            if (heatmaps[0, yCurrent, xCurrent, keypointId] &gt; score)\n            {\n                return false;\n            }\n        }\n    }\n\n    // If none of the values in the local window are greater, the score is the maximum\n    return true;\n}\n\n\nBuildPartList\nThis method builds a list of BodyPart2D instances with scores above the specified threshold and which are the maximum in their local window.\n/// &lt;summary&gt;\n/// Builds a list of BodyPart2D instances that have a score above the threshold and are the maximum in their local window.\n/// &lt;/summary&gt;\n/// &lt;param name=\"scoreThreshold\"&gt;The minimum score threshold.&lt;/param&gt;\n/// &lt;param name=\"localMaximumRadius\"&gt;The radius of the local window to search.&lt;/param&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps tensor.&lt;/param&gt;\n/// &lt;returns&gt;A list of BodyPart2D instances that meet the conditions.&lt;/returns&gt;\npublic List&lt;BodyPart2D&gt; BuildPartList(float scoreThreshold, int localMaximumRadius, Tensor heatmaps)\n{\n    List&lt;BodyPart2D&gt; list = new List&lt;BodyPart2D&gt;();\n\n    // Iterate through the channels, height, and width of the heatmaps tensor\n    for (int c = 0; c &lt; heatmaps.channels; c++)\n    {\n        for (int y = 0; y &lt; heatmaps.height; y++)\n        {\n            for (int x = 0; x &lt; heatmaps.width; x++)\n            {\n                float score = heatmaps[0, y, x, c];\n\n                // If the score is greater or equal to the threshold and is the maximum in the local window,\n                // add it to the list\n                if (score &gt;= scoreThreshold &&\n                    ScoreIsMaximumInLocalWindow(c, score, y, x, localMaximumRadius, heatmaps))\n                {\n                    list.Add(new BodyPart2D(c, new Vector2(x, y), score));\n                }\n            }\n        }\n    }\n\n    return list;\n}\n\n\nWithinNmsRadiusOfCorrespondingPoint\nThis method checks if a given vector is within the non-maximum suppression radius of a corresponding point in any pose.\n/// &lt;summary&gt;\n/// Checks if a given vector is within the non-maximum suppression radius of a corresponding point in any pose.\n/// &lt;/summary&gt;\n/// &lt;param name=\"poses\"&gt;A list of HumanPose2D instances.&lt;/param&gt;\n/// &lt;param name=\"squaredNmsRadius\"&gt;The squared non-maximum suppression radius.&lt;/param&gt;\n/// &lt;param name=\"vec\"&gt;The vector to be checked.&lt;/param&gt;\n/// &lt;param name=\"keypointId\"&gt;The keypoint index.&lt;/param&gt;\n/// &lt;returns&gt;True if the vector is within the radius of a corresponding point in any pose, false otherwise.&lt;/returns&gt;\npublic bool WithinNmsRadiusOfCorrespondingPoint(\n    List&lt;HumanPose2D&gt; poses, float squaredNmsRadius, Vector2 vec, int keypointId)\n{\n    return poses.Any(pose =&gt; (vec - pose.bodyParts[keypointId].coordinates).sqrMagnitude &lt;= squaredNmsRadius);\n}\n\n\nDecodeMultiplePoses\nThis method decodes multiple human poses from the model output.\n/// &lt;summary&gt;\n/// Decodes multiple human poses from the given heatmaps, offsets, and displacements tensors.\n/// &lt;/summary&gt;\n/// &lt;param name=\"heatmaps\"&gt;The heatmaps tensor.&lt;/param&gt;\n/// &lt;param name=\"offsets\"&gt;The offsets tensor.&lt;/param&gt;\n/// &lt;param name=\"displacementsFwd\"&gt;The forward displacements tensor.&lt;/param&gt;\n/// &lt;param name=\"displacementBwd\"&gt;The backward displacements tensor.&lt;/param&gt;\n/// &lt;param name=\"stride\"&gt;The stride for decoding the pose.&lt;/param&gt;\n/// &lt;param name=\"maxPoseDetections\"&gt;The maximum number of pose detections.&lt;/param&gt;\n/// &lt;param name=\"scoreThreshold\"&gt;The minimum score threshold for a part to be considered.&lt;/param&gt;\n/// &lt;param name=\"nmsRadius\"&gt;The non-maximum suppression radius.&lt;/param&gt;\n/// &lt;returns&gt;A list of decoded HumanPose2D instances.&lt;/returns&gt;\npublic List&lt;HumanPose2D&gt; DecodeMultiplePoses(\n    Tensor heatmaps, Tensor offsets,\n    Tensor displacementsFwd, Tensor displacementBwd,\n    int stride, int maxPoseDetections,\n    float scoreThreshold = 0.5f, int nmsRadius = 20)\n{\n    List&lt;HumanPose2D&gt; humanPoses = new List&lt;HumanPose2D&gt;();\n    float squaredNmsRadius = nmsRadius * nmsRadius;\n\n    List&lt;BodyPart2D&gt; bodyPartList = BuildPartList(scoreThreshold, kLocalMaximumRadius, heatmaps);\n    bodyPartList.Sort((a, b) =&gt; b.prob.CompareTo(a.prob));\n\n    // Continue decoding poses until the maximum number of detections is reached or the body part list is empty\n    while (humanPoses.Count &lt; maxPoseDetections && bodyPartList.Count &gt; 0)\n    {\n        BodyPart2D root = bodyPartList[0];\n        bodyPartList.RemoveAt(0);\n\n        Vector2 rootImageCoords = GetImageCoords(root, stride, offsets);\n\n        // If the root is not within the non-maximum suppression radius of any existing pose,\n        // decode the pose and add it to the list of human poses\n        if (!WithinNmsRadiusOfCorrespondingPoint(humanPoses, squaredNmsRadius, rootImageCoords, root.index))\n        {\n            HumanPose2D pose = new HumanPose2D\n            {\n                index = humanPoses.Count,\n                bodyParts = DecodePose(root, heatmaps, offsets, stride, displacementsFwd, displacementBwd)\n            };\n            humanPoses.Add(pose);\n        }\n    }\n\n    return humanPoses;\n}\n\n\nCropInputDims\nThis method crops input dimensions to be divisible by the maximum stride.\n/// &lt;summary&gt;\n/// Crop input dimensions to be divisible by the maximum stride.\n/// &lt;/summary&gt;\npublic Vector2Int CropInputDims(Vector2Int inputDims)\n{\n    inputDims[0] -= inputDims[0] % maxStride;\n    inputDims[1] -= inputDims[1] % maxStride;\n\n    return inputDims;\n}\n\n\n\n\nPackageInstaller.cs\nIn this section, we will go through the PackageInstaller.cs script and explain how each part of the code works to install the required packages. The complete code is available on GitHub at the link below.\n\nPackageInstaller.cs\n\n\nSerializable Classes\nThe script defines two serializable classes to hold package data.\n// Serializable class to hold package data\n[System.Serializable]\npublic class PackageData\n{\n    public string packageName;\n    public string packageUrl;\n}\n\n// Serializable class to hold a list of PackageData objects\n[System.Serializable]\npublic class PackageList\n{\n    public List&lt;PackageData&gt; packages;\n}\nThese classes are for deserializing the JSON file containing the list of packages to install.\n\n\nPackageInstaller Class Variables\nThe PackageInstaller class contains several private static fields.\n// Stores the AddRequest object for the current package to install.\nprivate static AddRequest addRequest;\n// A list of PackageData objects to install.\nprivate static List&lt;PackageData&gt; packagesToInstall;\n// The index of the current package to install.\nprivate static int currentPackageIndex;\n\n// GUID of the JSON file containing the list of packages to install\nprivate const string PackagesJSONGUID = \"0d78f4ab62d44aba8a8e95e6a8abfe8a\";\n\n\nInstallDependencies\nThe InstallDependencies() method executes when Unity loads without action from the user. It reads the package JSON file and calls the InstallNextPackage() method to install the packages.\n// Method called on load to install packages from the JSON file\n[InitializeOnLoadMethod]\npublic static void InstallDependencies()\n{\n    // Read the package JSON file\n    packagesToInstall = ReadPackageJson().packages;\n    // Initialize the current package index\n    currentPackageIndex = 0;\n    // Start installing the packages\n    InstallNextPackage();\n}\n\n\nInstallNextPackage\nThis method installs the next package in the list.\n// Method to install the next package in the list\nprivate static void InstallNextPackage()\n{\n    // Iterate through package list\n    if (currentPackageIndex &lt; packagesToInstall.Count)\n    {\n        PackageData packageData = packagesToInstall[currentPackageIndex];\n\n        // Check if the package is already installed\n        if (!IsPackageInstalled(packageData.packageName))\n        {\n            // Attempt to install package\n            addRequest = Client.Add(packageData.packageUrl);\n            EditorApplication.update += PackageInstallationProgress;\n        }\n        else\n        {\n            // Increment the current package index\n            currentPackageIndex++;\n            // Recursively call InstallNextPackage\n            InstallNextPackage();\n        }\n    }\n}\n\n\nPackageInstallationProgress\nThis method monitors the progress of the package installation and logs whether it was successful. It then triggers the installation process for the next package in the list.\n// Method to monitor the progress of package installation\nprivate static void PackageInstallationProgress()\n{\n    if (addRequest.IsCompleted)\n    {\n        // Log whether the package installation was successful\n        if (addRequest.Status == StatusCode.Success)\n        {\n            UnityEngine.Debug.Log($\"Successfully installed: {addRequest.Result.packageId}\");\n        }\n        else if (addRequest.Status &gt;= StatusCode.Failure)\n        {\n            UnityEngine.Debug.LogError($\"Failed to install package: {addRequest.Error.message}\");\n        }\n\n        // Unregister the method from the EditorApplication.update \n        EditorApplication.update -= PackageInstallationProgress;\n        // Increment the current package index\n        currentPackageIndex++;\n        // Install the next package in the list\n        InstallNextPackage();\n    }\n}\n\n\nIsPackageInstalled\nThis method verifies whether a package has already been installed or not.\n// Method to check if a package is already installed\nprivate static bool IsPackageInstalled(string packageName)\n{\n    // List the installed packages\n    var listRequest = Client.List(true, false);\n    while (!listRequest.IsCompleted) { }\n\n    if (listRequest.Status == StatusCode.Success)\n    {\n        // Check if the package is already installed\n        return listRequest.Result.Any(package =&gt; package.name == packageName);\n    }\n    else\n    {\n        UnityEngine.Debug.LogError($\"Failed to list packages: {listRequest.Error.message}\");\n    }\n\n    return false;\n}\n\n\nReadPackageJson\nThis method reads the JSON file containing the list of packages to install and returns a PackageList object.\n// Method to read the JSON file and return a PackageList object\nprivate static PackageList ReadPackageJson()\n{\n    // Convert the PackagesJSONGUID to an asset path\n    string assetPath = AssetDatabase.GUIDToAssetPath(PackagesJSONGUID);\n    // Read the JSON file content as a string\n    string jsonString = File.ReadAllText(assetPath);\n    // Deserialize the JSON string into a PackageList object\n    return JsonUtility.FromJson&lt;PackageList&gt;(jsonString);\n}"
  },
  {
    "objectID": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#conclusion",
    "href": "posts/unity-barracuda-inference-posenet-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Barracuda Inference PoseNet Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Barracuda Inference PoseNet package. The package extends the functionality of unity-barracuda-inference-base to perform 2D human pose estimation using PoseNet models.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-barracuda-inference-posenet\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package."
  },
  {
    "objectID": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html",
    "href": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Bounding Box 2D Toolkit Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#introduction",
    "href": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Bounding Box 2D Toolkit Package",
    "section": "Introduction",
    "text": "Introduction\nThe Unity Bounding Box 2D Toolkit package provides an easy-to-use and customizable solution to work with and visualize 2D bounding boxes on a Unity canvas.\nSome of my tutorials involve using 2D object detection models in Unity applications. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Check out the demo video below to see this package in action.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#package-overview",
    "href": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Bounding Box 2D Toolkit Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains three C# scripts and prefabs to construct 2D bounding boxes.\n\nC# Scripts\n\nBBox2DUtils.cs: This script provides functionality to work with 2D bounding boxes for object detection tasks.\nBoundingBox2DVisualizer.cs: This script creates, updates, and manages UI elements for visualizing 2D bounding boxes.\nAddCustomDefineSymbol.cs: An Editor script that automatically adds a custom scripting define symbol to the project after the package installs.\n\n\n\nPrefabs\n\nBoundingBoxBorderPrefab.prefab: The BoundingBox2DVisualizer script uses this prefab to construct the sides of bounding boxes.\nDotPrefab.prefab: An Image prefab used to display a dot at the center of bounding boxes\nLabelPrefab.prefab: A text prefab used to display the class label associated probability score for a bounding box.\nLabelBackgroundPrefab.prefab: An Image prefab used as the background for the LabelPrefab.\nBBox2DVisualizer.prefab: This prefab helps simplify adding 2D bounding box visualization to a Unity scene. The prefab already has the BoundingBox2DVisualizer script attached and has a child Canvas component."
  },
  {
    "objectID": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#code-explanation",
    "href": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Bounding Box 2D Toolkit Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Unity Bounding Box 2D Toolkit package by examining the purpose and functionality of each C# script.\n\nBBox2DUtils.cs\nThe BBox2DUtils.cs script provides functionality to work with 2D bounding boxes for object detection tasks. It contains two structs and a utility class. The complete code is available on GitHub at the link below.\n\nBBox2DUtils.cs\n\n\nBBox2D struct\nThis struct contains the coordinates (x0, y0), width, height, class index, and probability value for a 2D bounding box.\n/// &lt;summary&gt;\n/// A struct that represents a 2D bounding box.\n/// &lt;/summary&gt;\npublic struct BBox2D\n{\n    public float x0;\n    public float y0;\n    public float width;\n    public float height;\n    public int index;\n    public float prob;\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the BBox2D struct.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"x0\"&gt;The x-coordinate of the top-left corner.&lt;/param&gt;\n    /// &lt;param name=\"y0\"&gt;The y-coordinate of the top-left corner.&lt;/param&gt;\n    /// &lt;param name=\"width\"&gt;The width of the bounding box.&lt;/param&gt;\n    /// &lt;param name=\"height\"&gt;The height of the bounding box.&lt;/param&gt;\n    /// &lt;param name=\"index\"&gt;The class index of the object.&lt;/param&gt;\n    /// &lt;param name=\"prob\"&gt;The probability of the object belonging to the given class.&lt;/param&gt;\n    public BBox2D(float x0, float y0, float width, float height, int index, float prob)\n    {\n        this.x0 = x0;\n        this.y0 = y0;\n        this.width = width;\n        this.height = height;\n        this.index = index;\n        this.prob = prob;\n    }\n}\n\n\nBBox2DInfo struct\nThis struct contains a BBox2D object, class label, and color.\n/// &lt;summary&gt;\n/// A struct for 2D bounding box information.\n/// &lt;/summary&gt;\npublic struct BBox2DInfo\n{\n    public BBox2D bbox;\n    public string label;\n    public Color color;\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the BBox2DInfo struct.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"boundingBox\"&gt;The 2D bounding box.&lt;/param&gt;\n    /// &lt;param name=\"label\"&gt;The class label.&lt;/param&gt;\n    /// &lt;param name=\"width\"&gt;The bounding box color.&lt;/param&gt;\n    public BBox2DInfo(BBox2D boundingBox, string label = \"\", Color color = new Color())\n    {\n        this.bbox = boundingBox;\n        this.label = label;\n        this.color = color;\n    }\n}\n\n\nBBox2DUtility class\nThis class provides various utility methods for working with bounding boxes.\n\nCalcUnionArea\nThis method calculates the union area between two bounding boxes.\n/// &lt;summary&gt;\n/// Calculates the union area between two bounding boxes.\n/// &lt;/summary&gt;\n/// &lt;param name=\"a\"&gt;The first bounding box.&lt;/param&gt;\n/// &lt;param name=\"b\"&gt;The second bounding box.&lt;/param&gt;\n/// &lt;returns&gt;The union area between the two bounding boxes.&lt;/returns&gt;\npublic static float CalcUnionArea(BBox2D a, BBox2D b)\n{\n    // Calculate the coordinates and dimensions of the union area\n    float x = Mathf.Min(a.x0, b.x0);\n    float y = Mathf.Min(a.y0, b.y0);\n    float w = Mathf.Max(a.x0 + a.width, b.x0 + b.width) - x;\n    float h = Mathf.Max(a.y0 + a.height, b.y0 + b.height) - y;\n\n    // Calculate the union area of two bounding boxes\n    return w * h;\n}\n\n\nCalcInterArea\nThis method calculates the intersection area between two bounding boxes.\n/// &lt;summary&gt;\n/// Calculates the intersection area between two bounding boxes.\n/// &lt;/summary&gt;\n/// &lt;param name=\"a\"&gt;The first bounding box.&lt;/param&gt;\n/// &lt;param name=\"b\"&gt;The second bounding box.&lt;/param&gt;\n/// &lt;returns&gt;The intersection area between the two bounding boxes.&lt;/returns&gt;\npublic static float CalcInterArea(BBox2D a, BBox2D b)\n{\n    // Calculate the coordinates and dimensions of the intersection area\n    float x = Mathf.Max(a.x0, b.x0);\n    float y = Mathf.Max(a.y0, b.y0);\n    float w = Mathf.Min(a.x0 + a.width, b.x0 + b.width) - x;\n    float h = Mathf.Min(a.y0 + a.height, b.y0 + b.height) - y;\n\n    // Calculate the intersection area of two bounding boxes\n    return w * h;\n}\n\n\nNMSSortedBoxes\nThis method performs Non-Maximum Suppression (NMS) on a sorted list of bounding box proposals, retaining only those with an intersection over union (IoU) value below the threshold.\n/// &lt;summary&gt;\n/// Performs Non-Maximum Suppression (NMS) on a sorted list of bounding box proposals.\n/// &lt;/summary&gt;\n/// &lt;param name=\"proposals\"&gt;A sorted list of BBox2D objects representing the bounding box proposals.&lt;/param&gt;\n/// &lt;param name=\"nms_thresh\"&gt;The NMS threshold for filtering proposals (default is 0.45).&lt;/param&gt;\n/// &lt;returns&gt;A list of integers representing the indices of the retained proposals.&lt;/returns&gt;\npublic static List&lt;int&gt; NMSSortedBoxes(List&lt;BBox2D&gt; proposals, float nms_thresh = 0.45f)\n{\n    // Iterate through the proposals and perform non-maximum suppression\n    List&lt;int&gt; proposal_indices = new List&lt;int&gt;();\n\n    for (int i = 0; i &lt; proposals.Count; i++)\n    {\n        // Calculate the intersection and union areas\n        BBox2D a = proposals[i];\n        bool keep = proposal_indices.All(j =&gt;\n        {\n            BBox2D b = proposals[j];\n            float inter_area = CalcInterArea(a, b);\n            float union_area = CalcUnionArea(a, b);\n            // Keep the proposal if its IoU with all previous proposals is below the NMS threshold\n            return inter_area / union_area &lt;= nms_thresh;\n        });\n\n        // If the proposal passes the NMS check, add its index to the list\n        if (keep) proposal_indices.Add(i);\n    }\n\n    return proposal_indices;\n}\n\n\nScaleBoundingBox\nThis method scales and optionally mirrors the bounding box of a detected object to match the in-game screen and display resolutions.\n/// &lt;summary&gt;\n/// Scales and optionally mirrors the bounding box of a detected object to match the in-game screen and display resolutions.\n/// &lt;/summary&gt;\n/// &lt;param name=\"boundingBox\"&gt;A BBox2D object containing the bounding box information for a detected object.&lt;/param&gt;\n/// &lt;param name=\"inputDims\"&gt;The dimensions of the input image used for object detection.&lt;/param&gt;\n/// &lt;param name=\"screenDims\"&gt;The dimensions of the in-game screen where the bounding boxes will be displayed.&lt;/param&gt;\n/// &lt;param name=\"offset\"&gt;An offset to apply to the bounding box coordinates when scaling.&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;A boolean flag to indicate if the bounding boxes should be mirrored horizontally (default is false).&lt;/param&gt;\npublic static BBox2D ScaleBoundingBox(BBox2D boundingBox, Vector2Int inputDims, Vector2 screenDims, Vector2Int offset, bool mirrorScreen)\n{\n    // The smallest dimension of the screen\n    float minScreenDim = Mathf.Min(screenDims.x, screenDims.y);\n    // The smallest input dimension\n    int minInputDim = Mathf.Min(inputDims.x, inputDims.y);\n    // Calculate the scale value between the in-game screen and input dimensions\n    float minImgScale = minScreenDim / minInputDim;\n    // Calculate the scale value between the in-game screen and display\n    float displayScaleX = Screen.width / screenDims.x;\n    float displayScaleY = Screen.height / screenDims.y;\n    float displayScale = Mathf.Min(displayScaleX, displayScaleY);\n\n\n    // Scale bounding box to in-game screen resolution and flip the bbox coordinates vertically\n    float x0 = (boundingBox.x0 + offset.x) * minImgScale;\n    float y0 = (inputDims.y - (boundingBox.y0 - offset.y)) * minImgScale;\n    float width = boundingBox.width * minImgScale;\n    float height = boundingBox.height * minImgScale;\n\n    // Mirror bounding box across screen\n    if (mirrorScreen)\n    {\n        x0 = screenDims.x - x0 - width;\n    }\n\n    // Scale bounding box to display resolution\n    boundingBox.x0 = x0 * displayScale;\n    boundingBox.y0 = y0 * displayScale;\n    boundingBox.width = width * displayScale;\n    boundingBox.height = height * displayScale;\n\n    // Offset the bounding box coordinates based on the difference between the in-game screen and display\n    boundingBox.x0 += (Screen.width - screenDims.x * displayScale) / 2;\n    boundingBox.y0 += (Screen.height - screenDims.y * displayScale) / 2;\n\n    return boundingBox;\n}\n\n\n\n\n\nBoundingBox2DVisualizer.cs\nThe BoundingBox2DVisualizer script is a Unity C# MonoBehaviour class that displays 2D bounding boxes and labels on a Unity canvas. It creates, updates, and manages UI elements for visualizing them based on the provided BBox2DInfo array. This class supports customizable settings such as bounding box transparency and the ability to toggle the display of bounding boxes. The complete code is available on GitHub at the link below.\n\nBoundingBox2DVisualizer.cs\n\n\nSerialized Fields\nThe BoundingBox2DVisualizer class contains serialized fields referencing UI components, prefabs, and settings.\n// UI components\n[Header(\"Components\")]\n[Tooltip(\"Container for holding the bounding box UI elements\")]\n[SerializeField] private RectTransform boundingBoxContainer;\n[Tooltip(\"Container for holding the label UI elements\")]\n[SerializeField] private RectTransform labelContainer;\n\n// Prefabs for creating UI elements\n[Header(\"Prefabs\")]\n[Tooltip(\"Prefab for the bounding box UI element\")]\n[SerializeField] private RectTransform boundingBoxPrefab;\n[Tooltip(\"Prefab for the label UI element\")]\n[SerializeField] private TMP_Text labelPrefab;\n[Tooltip(\"Prefab for the label background UI element\")]\n[SerializeField] private Image labelBackgroundPrefab;\n[Tooltip(\"Prefab for the dot UI element\")]\n[SerializeField] private Image dotPrefab;\n\n// Settings for customizing the bounding box visualizer\n[Header(\"Settings\")]\n[Tooltip(\"Flag to control whether bounding boxes should be displayed or not\")]\n[SerializeField] private bool displayBoundingBoxes = true;\n[Tooltip(\"Transparency value for the bounding boxes, ranging from 0 (completely transparent) to 1 (completely opaque)\")]\n[SerializeField, Range(0f, 1f)] private float bboxTransparency = 1f;\n\n\nGUID Constants\nThese are the GUIDs of the default assets. They are used to set default values for the bounding box, label, label background, and dot prefabs in the Unity Editor.\n// GUIDs of the default assets for the bounding box, label, label background, and dot prefabs\nprivate const string BoundingBoxPrefabGUID = \"be0edeacc0f249fab31ac75426ad8a2a\";\nprivate const string LabelPrefabGUID = \"4e39b47d4b984862aeab14255855fcc9\";\nprivate const string LabelBackgroundPrefabGUID = \"9074ea186151430084312ba891bad58e\";\nprivate const string DotPrefabGUID = \"3eb64b4f1a4e4e2595066ed269be9532\";\n\n\nLists for UI Elements\n// Lists for storing and managing instantiated UI elements\nprivate List&lt;RectTransform&gt; boundingBoxes = new List&lt;RectTransform&gt;(); // List of instantiated bounding box UI elements\nprivate List&lt;TMP_Text&gt; labels = new List&lt;TMP_Text&gt;(); // List of instantiated label UI elements\nprivate List&lt;Image&gt; labelBackgrounds = new List&lt;Image&gt;(); // List of instantiated label background UI elements\nprivate List&lt;Image&gt; dots = new List&lt;Image&gt;(); // List of instantiated dot UI elements\n\n\nReset\nThis method sets the default assets from the project using their GUIDs. It uses AssetDatabase to find them and set the default values. This method will only work in the Unity Editor, not in a build.\n/// &lt;summary&gt;\n/// Reset is called when the user hits the Reset button in the Inspector's context menu\n/// or when adding the component the first time. This function is only called in editor mode.\n/// &lt;/summary&gt;\nprivate void Reset()\n{\n    // Load default assets only in the Unity Editor, not in a build\n#if UNITY_EDITOR\n    boundingBoxPrefab = LoadDefaultAsset&lt;RectTransform&gt;(BoundingBoxPrefabGUID);\n    labelPrefab = LoadDefaultAsset&lt;TMP_Text&gt;(LabelPrefabGUID);\n    labelBackgroundPrefab = LoadDefaultAsset&lt;Image&gt;(LabelBackgroundPrefabGUID);\n    dotPrefab = LoadDefaultAsset&lt;Image&gt;(DotPrefabGUID);\n#endif\n}\n\n\nLoadDefaultAsset\nThis method provides a generic way to load default assets for the specified fields using their GUIDs.\n/// &lt;summary&gt;\n/// Loads the default asset for the specified type using its GUID.\n/// &lt;/summary&gt;\n/// &lt;typeparam name=\"T\"&gt;The type of asset to be loaded.&lt;/typeparam&gt;\n/// &lt;param name=\"guid\"&gt;The GUID of the default asset.&lt;/param&gt;\n/// &lt;returns&gt;The loaded asset of the specified type.&lt;/returns&gt;\n/// &lt;remarks&gt;\n/// This method is only executed in the Unity Editor, not in builds.\n/// &lt;/remarks&gt;\nprivate T LoadDefaultAsset&lt;T&gt;(string guid) where T : UnityEngine.Object\n{\n#if UNITY_EDITOR\n    // Load the asset from the AssetDatabase using its GUID\n    return UnityEditor.AssetDatabase.LoadAssetAtPath&lt;T&gt;(UnityEditor.AssetDatabase.GUIDToAssetPath(guid));\n#else\n    return null;\n#endif\n}\n\n\nUpdateBoundingBoxVisualizations\nThis method updates the visualization of bounding boxes based on the given BBox2DInfo array.\n/// &lt;summary&gt;\n/// Update the visualization of bounding boxes based on the given BBox2DInfo array.\n/// &lt;/summary&gt;\n/// &lt;param name=\"bboxInfoArray\"&gt;An array of BBox2DInfo objects containing bounding box information&lt;/param&gt;\npublic void UpdateBoundingBoxVisualizations(BBox2DInfo[] bboxInfoArray)\n{\n    // Depending on the displayBoundingBoxes flag, either update or disable bounding box UI elements\n    if (displayBoundingBoxes)\n    {\n        UpdateBoundingBoxes(bboxInfoArray);\n    }\n    else\n    {\n        // Disable bounding boxes, labels, and label backgrounds for all existing UI elements\n        for (int i = 0; i &lt; boundingBoxes.Count; i++)\n        {\n            boundingBoxes[i].gameObject.SetActive(false);\n            labelBackgrounds[i].gameObject.SetActive(false);\n            labels[i].gameObject.SetActive(false);\n            dots[i].gameObject.SetActive(false);\n        }\n    }\n}\n\n\nScreenToCanvasPoint\nThis method converts a screen point to a local one in the RectTransform space of the given canvas.\n/// &lt;summary&gt;\n/// Convert a screen point to a local point in the RectTransform space of the given canvas.\n/// &lt;/summary&gt;\n/// &lt;param name=\"canvas\"&gt;The RectTransform object of the canvas&lt;/param&gt;\n/// &lt;param name=\"screenPoint\"&gt;The screen point to be converted&lt;/param&gt;\n/// &lt;returns&gt;A Vector2 object representing the local point in the RectTransform space of the canvas&lt;/returns&gt;\nprivate Vector2 ScreenToCanvasPoint(RectTransform canvas, Vector2 screenPoint)\n{\n    RectTransformUtility.ScreenPointToLocalPointInRectangle(canvas, screenPoint, null, out Vector2 localPoint);\n    return localPoint;\n}\n\n\nUpdateBoundingBoxes\nThis method updates bounding box UI elements to match the provided BBox2DInfo array. It creates or removes bounding box UI elements to match the number of detected objects and updates bounding boxes, labels, and label backgrounds. It also disables UI elements if not needed.\n/// &lt;summary&gt;\n/// Update bounding box UI elements to match the provided BBox2DInfo array.\n/// &lt;/summary&gt;\n/// &lt;param name=\"bboxInfoArray\"&gt;An array of BBox2DInfo objects containing bounding box information&lt;/param&gt;\nprivate void UpdateBoundingBoxes(BBox2DInfo[] bboxInfoArray)\n{\n    // Create or remove bounding box UI elements to match the number of detected objects\n    while (boundingBoxes.Count &lt; bboxInfoArray.Length)\n    {\n        RectTransform newBoundingBox = Instantiate(boundingBoxPrefab, boundingBoxContainer);\n        boundingBoxes.Add(newBoundingBox);\n\n        Image newLabelBackground = Instantiate(labelBackgroundPrefab, labelContainer);\n        labelBackgrounds.Add(newLabelBackground);\n\n        TMP_Text newLabel = Instantiate(labelPrefab, labelContainer);\n        labels.Add(newLabel);\n\n        Image newDot = Instantiate(dotPrefab, boundingBoxContainer);\n        dots.Add(newDot);\n    }\n\n    // Update bounding boxes, labels, and label backgrounds for each detected object, or disable UI elements if not needed\n    for (int i = 0; i &lt; boundingBoxes.Count; i++)\n    {\n        if (i &lt; bboxInfoArray.Length)\n        {\n            BBox2DInfo bboxInfo = bboxInfoArray[i];\n\n            // Get UI elements for the current bounding box, label, and label background\n            RectTransform boundingBox = boundingBoxes[i];\n            TMP_Text label = labels[i];\n            Image labelBackground = labelBackgrounds[i];\n            Image dot = dots[i];\n\n            UpdateBoundingBox(boundingBox, bboxInfo);\n            UpdateLabelAndBackground(label, labelBackground, bboxInfo);\n            UpdateDot(dot, bboxInfo);\n\n            // Enable bounding box, label, and label background UI elements\n            boundingBox.gameObject.SetActive(true);\n            labelBackground.gameObject.SetActive(true);\n            label.gameObject.SetActive(true);\n            dots[i].gameObject.SetActive(true);\n        }\n        else\n        {\n            // Disable UI elements for extra bounding boxes, labels, and label backgrounds\n            boundingBoxes[i].gameObject.SetActive(false);\n            labelBackgrounds[i].gameObject.SetActive(false);\n            labels[i].gameObject.SetActive(false);\n            dots[i].gameObject.SetActive(false);\n        }\n    }\n}\n\n\nUpdateBoundingBox\nThis method updates the bounding box UI element with the information from the given BBox2DInfo object. It converts the screen point to a local one in the RectTransform space of the bounding box container and sets the color of the bounding box with the specified transparency.\n/// &lt;summary&gt;\n/// Update the bounding box UI element with the information from the given BBox2DInfo object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"boundingBox\"&gt;The RectTransform object representing the bounding box UI element&lt;/param&gt;\n/// &lt;param name=\"bboxInfo\"&gt;The BBox2DInfo object containing the information for the bounding box&lt;/param&gt;\nprivate void UpdateBoundingBox(RectTransform boundingBox, BBox2DInfo bboxInfo)\n{\n    // Convert the screen point to a local point in the RectTransform space of the bounding box container\n    Vector2 localPosition = ScreenToCanvasPoint(boundingBoxContainer, new Vector2(bboxInfo.bbox.x0, bboxInfo.bbox.y0));\n    boundingBox.anchoredPosition = localPosition;\n    boundingBox.sizeDelta = new Vector2(bboxInfo.bbox.width, bboxInfo.bbox.height);\n\n    // Set the color of the bounding box with the specified transparency\n    Color color = GetColorWithTransparency(bboxInfo.color);\n    Image[] sides = boundingBox.GetComponentsInChildren&lt;Image&gt;();\n    foreach (Image side in sides)\n    {\n        side.color = color;\n    }\n}\n\n\nUpdateLabelAndBackground\nThis method updates the label and label background UI elements with the information from the provided BBox2DInfo object. It sets the label text, position, color, and the label’s background position, size, and color with the specified transparency.\n/// &lt;summary&gt;\n/// Update the label and label background UI elements with the information from the given BBox2DInfo object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"label\"&gt;The TMP_Text object representing the label UI element&lt;/param&gt;\n/// &lt;param name=\"labelBackground\"&gt;The Image object representing the label background UI element&lt;/param&gt;\n/// &lt;param name=\"bboxInfo\"&gt;The BBox2DInfo object containing the information for the label and label background&lt;/param&gt;\nprivate void UpdateLabelAndBackground(TMP_Text label, Image labelBackground, BBox2DInfo bboxInfo)\n{\n    // Convert the screen point to a local point in the RectTransform space of the bounding box container\n    Vector2 localPosition = ScreenToCanvasPoint(boundingBoxContainer, new Vector2(bboxInfo.bbox.x0, bboxInfo.bbox.y0));\n\n    // Set the label text and position\n    label.text = $\"{bboxInfo.label}: {(bboxInfo.bbox.prob * 100).ToString(\"0.##\")}%\";\n    label.rectTransform.anchoredPosition = new Vector2(localPosition.x, localPosition.y - label.preferredHeight);\n\n    // Set the label color based on the grayscale value of the bounding box color\n    Color color = GetColorWithTransparency(bboxInfo.color);\n    label.color = color.grayscale &gt; 0.5 ? Color.black : Color.white;\n\n    // Set the label background position and size\n    labelBackground.rectTransform.anchoredPosition = new Vector2(localPosition.x, localPosition.y - label.preferredHeight);\n    labelBackground.rectTransform.sizeDelta = new Vector2(Mathf.Max(label.preferredWidth, bboxInfo.bbox.width), label.preferredHeight);\n\n    // Set the label background color with the specified transparency\n    labelBackground.color = color;\n}\n\n\nUpdateDot\nThis method updates the dot UI element based on the provided BBox2DInfo object.\n/// &lt;summary&gt;\n/// Update the dot UI element with the information from the given BBox2DInfo object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"dot\"&gt;The Image object representing the dot UI element&lt;/param&gt;\n/// &lt;param name=\"bboxInfo\"&gt;The BBox2DInfo object containing the information for the bounding box&lt;/param&gt;\nprivate void UpdateDot(Image dot, BBox2DInfo bboxInfo)\n{\n    // Calculate the center of the bounding box\n    Vector2 center = new Vector2(bboxInfo.bbox.x0 + bboxInfo.bbox.width / 2, bboxInfo.bbox.y0 - bboxInfo.bbox.height / 2);\n\n    // Convert the screen point to a local point in the RectTransform space of the bounding box container\n    Vector2 localPosition = ScreenToCanvasPoint(boundingBoxContainer, center);\n\n    // Set the dot position\n    dot.rectTransform.anchoredPosition = localPosition;\n\n    // Set the dot color with the specified transparency\n    Color color = GetColorWithTransparency(bboxInfo.color);\n    dot.color = color;\n}\n\n\nGetColorWithTransparency\nThis method is a utility function that returns a new color based on the input color with the adjusted transparency.\n/// &lt;summary&gt;\n/// Get a new color based on the input color with the adjusted transparency.\n/// &lt;/summary&gt;\n/// &lt;param name=\"color\"&gt;The input color to be modified&lt;/param&gt;\n/// &lt;returns&gt;A new color with the specified transparency&lt;/returns&gt;\nprivate Color GetColorWithTransparency(Color color)\n{\n    color.a = bboxTransparency;\n    return color;\n}\n\n\n\n\nAddCustomDefineSymbol.cs\nThis Editor script contains a class that adds a custom define symbol to the project. We can use this custom symbol to prevent code that relies on this package from executing unless the Bounding Box 2D Toolkit package is present. The complete code is available on GitHub at the link below.\n\nEditor/AddCustomDefineSymbol.cs\n\nusing UnityEditor;\nusing UnityEngine;\n\nnamespace CJM.BBox2DToolkit\n{\n    public class DependencyDefineSymbolAdder\n    {\n        private const string CustomDefineSymbol = \"CJM_BBOX_2D_TOOLKIT\";\n\n        [InitializeOnLoadMethod]\n        public static void AddCustomDefineSymbol()\n        {\n            // Get the currently selected build target group\n            var buildTargetGroup = EditorUserBuildSettings.selectedBuildTargetGroup;\n            // Retrieve the current scripting define symbols for the selected build target group\n            var defines = PlayerSettings.GetScriptingDefineSymbolsForGroup(buildTargetGroup);\n\n            // Check if the CustomDefineSymbol is already present in the defines string\n            if (!defines.Contains(CustomDefineSymbol))\n            {\n                // Append the CustomDefineSymbol to the defines string, separated by a semicolon\n                defines += $\";{CustomDefineSymbol}\";\n                // Set the updated defines string as the new scripting define symbols for the selected build target group\n                PlayerSettings.SetScriptingDefineSymbolsForGroup(buildTargetGroup, defines);\n                // Log a message in the Unity console to inform the user that the custom define symbol has been added\n                Debug.Log($\"Added custom define symbol '{CustomDefineSymbol}' to the project.\");\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#conclusion",
    "href": "posts/unity-bounding-box-2d-toolkit-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Bounding Box 2D Toolkit Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Unity Bounding Box 2D Toolkit package. The package provides an easy-to-use and customizable solution to work with and visualize 2D bounding boxes on a Unity canvas.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-bounding-box-2d-toolkit\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/unity-cv-image-gallery-walkthrough/index.html",
    "href": "posts/unity-cv-image-gallery-walkthrough/index.html",
    "title": "Code Walkthrough: Unity CV Image Gallery Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-cv-image-gallery-walkthrough/index.html#introduction",
    "href": "posts/unity-cv-image-gallery-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity CV Image Gallery Package",
    "section": "Introduction",
    "text": "Introduction\nThe Unity CV Image Gallery package provides an interactive image gallery and Scroll View prefab to facilitate the testing of computer vision applications, such as image classification, object detection, and pose estimation in Unity.\nMany of my tutorials involve using computer vision models in Unity applications. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Check out the demo video below to see this package in action.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-cv-image-gallery-walkthrough/index.html#package-overview",
    "href": "posts/unity-cv-image-gallery-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity CV Image Gallery Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains two C# scripts and a Scroll View prefab. It is designed to be easily integrated into your existing Unity projects and can be customized to suit your needs.\n\nC# Scripts\n\nImageGallery.cs: This script creates and manages the interactive image gallery. It attaches to a Scroll View -&gt; Viewport -&gt; Content object in a Unity scene.\nPackageInstaller.cs: An Editor utility script for automatically installing a list of dependency packages defined in a JSON file.\n\n\n\nScroll View Prefab\nThe Scroll View prefab helps simplify adding an image gallery to a Unity scene. The prefab contains an ImagePrefab object and already has the ImageGallery.cs script attached to the Scroll View -&gt; Viewport -&gt; Content object. This prefab can be easily added to your Unity project, allowing you to quickly set up an interactive image gallery for testing computer vision applications. You can customize the prefab by modifying the serialized fields in the ImageGallery.cs script.\n\n\nDependencies\nThe package depends on the Unity Media Display package. This package simplifies creating and managing demo screens for displaying test images, videos, and webcam streams in Unity projects.\n\nGitHub Repository\nCode Walkthrough"
  },
  {
    "objectID": "posts/unity-cv-image-gallery-walkthrough/index.html#code-explanation",
    "href": "posts/unity-cv-image-gallery-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity CV Image Gallery Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Unity CV Image Gallery package by examining the purpose and functionality of each C# script.\n\nImageGallery.cs\nIn this section, we will go through the ImageGallery.cs script and explain how each part of the code works to create and manage the interactive image gallery. The complete code is available on GitHub at the link below.\n\nImageGallery.cs\n\n\nClass Variables\nThe ImageGallery class contains several serialized fields to configure the gallery.\n[Header(\"Scene\")]\n[Tooltip(\"The screen GameObject where the selected image will be displayed\")]\n[SerializeField] private GameObject screenObject;\n[Tooltip(\"The camera GameObject used to display the selected image\")]\n[SerializeField] private GameObject cameraObject;\n[Tooltip(\"The content panel GameObject where the image gallery is located\")]\n[SerializeField] private GameObject contentPanel;\n[Tooltip(\"The image prefab used to create each image in the gallery\")]\n[SerializeField] private GameObject imagePrefab;\n[Tooltip(\"A list of sprites to populate the image gallery.\")]\n[SerializeField] private List&lt;Sprite&gt; imageSprites;\n[Tooltip(\" The spacing between images in the gallery.\")]\n[SerializeField] private float spacing = 5f;\n[Tooltip(\"The specified width for each image in the gallery.\")]\n[SerializeField] private float specifiedWidth = 100f;\n\n\nStart\nStart() runs when the script first executes. It calls several methods to initialize the gallery.\nprivate void Start()\n{\n    // Configures the content panel with a VerticalLayoutGroup component.\n    SetupContentPanel();\n    // Populates the gallery with images using the provided sprites.\n    PopulateImageGallery();\n    // Adjusts the content panel height by summing the vertical dimensions of all gallery images and spacing.\n    AdjustContentHeight();\n    // Assigns click events to the images in the gallery to update the screen texture.\n    AssignButtonClickEvents();\n}\n\n\nSetupContentPanel\nThis method sets up the content panel with a VerticalLayoutGroup component. It configures the component’s properties, such as spacing, childAlignment, and childControlHeight, to arrange the images in the gallery with proper spacing and alignment.\n/// &lt;summary&gt;\n/// Set up the content panel with a VerticalLayoutGroup component.\n/// &lt;/summary&gt;\nprivate void SetupContentPanel()\n{\n    VerticalLayoutGroup verticalLayoutGroup = contentPanel.AddComponent&lt;VerticalLayoutGroup&gt;();\n    verticalLayoutGroup.spacing = spacing;\n    verticalLayoutGroup.childAlignment = TextAnchor.UpperCenter;\n    verticalLayoutGroup.childControlHeight = false;\n    verticalLayoutGroup.childControlWidth = false;\n    verticalLayoutGroup.childForceExpandHeight = false;\n    verticalLayoutGroup.childForceExpandWidth = false;\n}\n\n\nPopulateImageGallery\nThis method populates the image gallery with the sprites in the imageSprites list.\n/// &lt;summary&gt;\n/// Populate the image gallery with the sprites provided in imageSprites.\n/// &lt;/summary&gt;\nprivate void PopulateImageGallery()\n{\n    foreach (Sprite sprite in imageSprites)\n    {\n        // Instantiates a new GameObject using the imagePrefab\n        GameObject newImageObject = Instantiate(imagePrefab, contentPanel.transform);\n        Image newImage = newImageObject.GetComponent&lt;Image&gt;();\n        newImageObject.SetActive(true);\n        // Assign the curent sprite\n        newImage.sprite = sprite;\n        // Preserves the aspect ratio\n        newImage.preserveAspect = true;\n        // Use  the sprite's name for easier identification\n        newImageObject.name = sprite.name;\n\n        // Adjust the image size based on the specified width\n        RectTransform rectTransform = newImageObject.GetComponent&lt;RectTransform&gt;();\n        float aspectRatio = sprite.rect.height / sprite.rect.width;\n        rectTransform.sizeDelta = new Vector2(specifiedWidth, specifiedWidth * aspectRatio);\n    }\n}\n\n\nAdjustContentHeight\nThis method adjusts the content panel’s height based on the total vertical size of the images and spacing.\n/// &lt;summary&gt;\n/// Adjust the content panel height based on the total height of the images and spacing.\n/// &lt;/summary&gt;\nprivate void AdjustContentHeight()\n{\n    RectTransform contentPanelRectTransform = contentPanel.GetComponent&lt;RectTransform&gt;();\n    float totalHeight = 0f;\n\n    // Calculate the total height of all the images in the gallery\n    for (int i = 0; i &lt; contentPanelRectTransform.childCount; i++)\n    {\n        RectTransform childRect = contentPanelRectTransform.GetChild(i).GetComponent&lt;RectTransform&gt;();\n        totalHeight += childRect.sizeDelta.y;\n    }\n\n    // Add the spacing between the images to the total height\n    totalHeight += spacing * (contentPanelRectTransform.childCount - 1);\n    // Updates the content panel to accommodate the total height\n    contentPanelRectTransform.sizeDelta = new Vector2(contentPanelRectTransform.sizeDelta.x, totalHeight);\n}\n\n\nAssignButtonClickEvents\nThis method assigns click events to the images in the gallery to update the screen texture to display the selected image.\n/// &lt;summary&gt;\n/// Assigns click events to the images in the gallery to update the screen texture when clicked.\n/// &lt;/summary&gt;\nprivate void AssignButtonClickEvents()\n{\n    Image[] images = transform.GetComponentsInChildren&lt;Image&gt;();\n\n    foreach (Image image in images)\n    {\n        // Add a Button component if the image doesn't already have one\n        Button button = image.GetComponent&lt;Button&gt;();\n        if (button == null)\n        {\n            button = image.gameObject.AddComponent&lt;Button&gt;();\n        }\n\n        // Add a listener to update the screen texture to display the selected image when clicked\n        button.onClick.AddListener(() =&gt; MediaDisplayManager.UpdateScreenTexture(screenObject, image.mainTexture, cameraObject, false));\n    }\n}\n\n\n\n\nPackageInstaller.cs\nIn this section, we will go through the PackageInstaller.cs script and explain how each part of the code works to install the required packages. The complete code is available on GitHub at the link below.\n\nPackageInstaller.cs\n\n\nSerializable Classes\nThe script defines two serializable classes to hold package data.\n// Serializable class to hold package data\n[System.Serializable]\npublic class PackageData\n{\n    public string packageName;\n    public string packageUrl;\n}\n\n// Serializable class to hold a list of PackageData objects\n[System.Serializable]\npublic class PackageList\n{\n    public List&lt;PackageData&gt; packages;\n}\nThese classes are for deserializing the JSON file containing the list of packages to install.\n\n\nPackageInstaller Class Variables\nThe PackageInstaller class contains several private static fields.\n// Stores the AddRequest object for the current package to install.\nprivate static AddRequest addRequest;\n// A list of PackageData objects to install.\nprivate static List&lt;PackageData&gt; packagesToInstall;\n// The index of the current package to install.\nprivate static int currentPackageIndex;\n\n// GUID of the JSON file containing the list of packages to install\nprivate const string PackagesJSONGUID = \"f0b282a4fbb4473584f52e3fd0ab3087\";\n\n\nInstallDependencies\nThe InstallDependencies() method executes when Unity loads without action from the user. It reads the package JSON file and calls the InstallNextPackage() method to install the packages.\n// Method called on load to install packages from the JSON file\n[InitializeOnLoadMethod]\npublic static void InstallDependencies()\n{\n    // Read the package JSON file\n    packagesToInstall = ReadPackageJson().packages;\n    // Initialize the current package index\n    currentPackageIndex = 0;\n    // Start installing the packages\n    InstallNextPackage();\n}\n\n\nInstallNextPackage\nThis method installs the next package in the list.\n// Method to install the next package in the list\nprivate static void InstallNextPackage()\n{\n    // Iterate through package list\n    if (currentPackageIndex &lt; packagesToInstall.Count)\n    {\n        PackageData packageData = packagesToInstall[currentPackageIndex];\n\n        // Check if the package is already installed\n        if (!IsPackageInstalled(packageData.packageName))\n        {\n            // Attempt to install package\n            addRequest = Client.Add(packageData.packageUrl);\n            EditorApplication.update += PackageInstallationProgress;\n        }\n        else\n        {\n            // Increment the current package index\n            currentPackageIndex++;\n            // Recursively call InstallNextPackage\n            InstallNextPackage();\n        }\n    }\n}\n\n\nPackageInstallationProgress\nThis method monitors the progress of the package installation and logs whether it was successful. It then triggers the installation process for the next package in the list.\n// Method to monitor the progress of package installation\nprivate static void PackageInstallationProgress()\n{\n    if (addRequest.IsCompleted)\n    {\n        // Log whether the package installation was successful\n        if (addRequest.Status == StatusCode.Success)\n        {\n            UnityEngine.Debug.Log($\"Successfully installed: {addRequest.Result.packageId}\");\n        }\n        else if (addRequest.Status &gt;= StatusCode.Failure)\n        {\n            UnityEngine.Debug.LogError($\"Failed to install package: {addRequest.Error.message}\");\n        }\n\n        // Unregister the method from the EditorApplication.update \n        EditorApplication.update -= PackageInstallationProgress;\n        // Increment the current package index\n        currentPackageIndex++;\n        // Install the next package in the list\n        InstallNextPackage();\n    }\n}\n\n\nIsPackageInstalled\nThis method verifies whether a package has already been installed or not.\n// Method to check if a package is already installed\nprivate static bool IsPackageInstalled(string packageName)\n{\n    // List the installed packages\n    var listRequest = Client.List(true, false);\n    while (!listRequest.IsCompleted) { }\n\n    if (listRequest.Status == StatusCode.Success)\n    {\n        // Check if the package is already installed\n        return listRequest.Result.Any(package =&gt; package.name == packageName);\n    }\n    else\n    {\n        UnityEngine.Debug.LogError($\"Failed to list packages: {listRequest.Error.message}\");\n    }\n\n    return false;\n}\n\n\nReadPackageJson\nThis method reads the JSON file containing the list of packages to install and returns a PackageList object.\n// Method to read the JSON file and return a PackageList object\nprivate static PackageList ReadPackageJson()\n{\n    // Convert the PackagesJSONGUID to an asset path\n    string assetPath = AssetDatabase.GUIDToAssetPath(PackagesJSONGUID);\n    // Read the JSON file content as a string\n    string jsonString = File.ReadAllText(assetPath);\n    // Deserialize the JSON string into a PackageList object\n    return JsonUtility.FromJson&lt;PackageList&gt;(jsonString);\n}"
  },
  {
    "objectID": "posts/unity-cv-image-gallery-walkthrough/index.html#conclusion",
    "href": "posts/unity-cv-image-gallery-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity CV Image Gallery Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Unity CV Image Gallery package. The package helps facilitate testing computer vision applications in Unity projects by providing an interactive image gallery.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-cv-image-gallery\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below, along with links for other demo projects that use the Unity CV Image Gallery package.\n\nUnity Media Display Demo: A simple demo project demonstrating how to use the unity-media-display and unity-cv-image-gallery packages in Unity.\nBarracuda Image Classification Demo: A simple Unity project demonstrating how to perform image classification with the barracuda-inference-image-classification package.\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package.\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html",
    "href": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Deep Learning Image Preprocessor Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#introduction",
    "href": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Deep Learning Image Preprocessor Package",
    "section": "Introduction",
    "text": "Introduction\nThe Deep Learning Image Preprocessor package provides Shaders and Compute Shaders for various image processing tasks, such as cropping, normalizing, and flipping images in Unity.\nMany of my tutorials involve using computer vision models in Unity applications. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content.\nIn this post, I walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#package-overview",
    "href": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Deep Learning Image Preprocessor Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains a C# script and various processing shaders.\n\nC# Script\n\nImageProcessor.cs: This script provides utility functions to process images using compute shaders or materials.\n\n\n\nShaders\n\nCropImage.shader: This shader is responsible for cropping images based on provided offset and size values.\nNormalizeImage.shader: This shader normalizes the color values of an input texture using the provided mean, standard deviation, and scaling values.\nProcessingShader.compute: This compute shader offers various image processing functionality, including normalizing input images, cropping images based on the provided offset and size values, and flipping images around the x-axis."
  },
  {
    "objectID": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#code-explanation",
    "href": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Deep Learning Image Preprocessor Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Deep Learning Image Preprocessor package by examining the purpose and functionality of the C# script and shaders.\n\nImageProcessor.cs\nThe script defines a public class ImageProcessor that inherits from MonoBehaviour. This class handles the processing of images using shaders. The complete code is available on GitHub at the link below.\n\nImageProcessor.cs\n\n\nSerialized Fields\nThe ImageProcessor class contains a set of serialized fields for the shaders and normalization parameters.\n[Header(\"Processing Shaders\")]\n[Tooltip(\"The compute shader for image processing\")]\n[SerializeField] private ComputeShader processingComputeShader;\n[Tooltip(\"The shader for image normalization\")]\n[SerializeField] private Shader normalizeShader;\n[Tooltip(\"The shader for image cropping\")]\n[SerializeField] private Shader cropShader;\n\n[Header(\"Normalization Parameters\")]\n[Tooltip(\"JSON file with the mean and std values for normalization\")]\n[SerializeField] private TextAsset normStatsJson = null;\n\n\nPrivate Fields and Constants\nThe script also defines several private fields and constants related to shaders and normalization parameters.\n// GUIDs of the default assets used for shaders and normalization\nprivate const string ProcessingComputeShaderGUID = \"2c418cec15ae44419d94328d0e8dcea8\";\nprivate const string NormalizeShaderGUID = \"45d8405a4cc64ecfa477b712e0465c05\";\nprivate const string CropShaderGUID = \"0685d34a035b4cefa942d94390282c12\";\nprivate const string NormStatsJsonGUID = \"9c8f1a57cb884c9b8a4439cae327a2f8\";\n\n// The material for image normalization\nprivate Material normalizeMaterial;\n// The material for image cropping\nprivate Material cropMaterial;\n\n[System.Serializable]\nprivate class NormStats\n{\n    public float[] mean;\n    public float[] std;\n    public float scale;\n}\n\n// The mean values for normalization\nprivate float[] mean = new float[] { 0f, 0f, 0f };\n// The standard deviation values for normalization\nprivate float[] std = new float[] { 1f, 1f, 1f };\n// Value used to scale normalized input\nprivate float scale = 1f;\n\n// Buffer for mean values used in compute shader\nprivate ComputeBuffer meanBuffer;\n// Buffer for standard deviation values used in compute shader\nprivate ComputeBuffer stdBuffer;\n\n\nReset\nThis method sets the default assets from the project using their GUIDs. It uses AssetDatabase to find them and set the default values. This method will only work in the Unity Editor, not in a build.\n/// &lt;summary&gt;\n/// Reset is called when the user hits the Reset button in the Inspector's context menu\n/// or when adding the component the first time. This function is only called in editor mode.\n/// &lt;/summary&gt;\nprivate void Reset()\n{\n    // Load default assets only in the Unity Editor, not in a build\n#if UNITY_EDITOR\n    processingComputeShader = LoadDefaultAsset&lt;ComputeShader&gt;(ProcessingComputeShaderGUID);\n    normalizeShader = LoadDefaultAsset&lt;Shader&gt;(NormalizeShaderGUID);\n    cropShader = LoadDefaultAsset&lt;Shader&gt;(CropShaderGUID);\n    normStatsJson = LoadDefaultAsset&lt;TextAsset&gt;(NormStatsJsonGUID);\n#endif\n}\n\n\nLoadDefaultAsset\nThis method provides a generic way to load default assets for the specified fields using their GUIDs.\n/// &lt;summary&gt;\n/// Loads the default asset for the specified type using its GUID.\n/// &lt;/summary&gt;\n/// &lt;typeparam name=\"T\"&gt;The type of asset to be loaded.&lt;/typeparam&gt;\n/// &lt;param name=\"guid\"&gt;The GUID of the default asset.&lt;/param&gt;\n/// &lt;returns&gt;The loaded asset of the specified type.&lt;/returns&gt;\n/// &lt;remarks&gt;\n/// This method is only executed in the Unity Editor, not in builds.\n/// &lt;/remarks&gt;\nprivate T LoadDefaultAsset&lt;T&gt;(string guid) where T : UnityEngine.Object\n{\n#if UNITY_EDITOR\n    // Load the asset from the AssetDatabase using its GUID\n    return UnityEditor.AssetDatabase.LoadAssetAtPath&lt;T&gt;(UnityEditor.AssetDatabase.GUIDToAssetPath(guid));\n#else\n    return null;\n#endif\n}\n\n\nStart\nThe script initializes the shaders and normalization parameters in the Start() method, which runs when the script initializes.\n/// &lt;summary&gt;\n/// Called when the script is initialized.\n/// &lt;/summary&gt;\nprivate void Start()\n{\n    normalizeMaterial = new Material(normalizeShader);\n    cropMaterial = new Material(cropShader);\n\n    LoadNormStats();\n    InitializeProcessingShaders();\n}\n\n\nLoadNormStats\nThis method loads the normalization statistics from the provided JSON file by deserializing the JSON content and updating the normalization statistics.\n/// &lt;summary&gt;\n/// Load the normalization stats from the provided JSON file.\n/// &lt;/summary&gt;\nprivate void LoadNormStats()\n{\n    if (IsNormStatsJsonNullOrEmpty())\n    {\n        return;\n    }\n\n    NormStats normStats = DeserializeNormStats(normStatsJson.text);\n    UpdateNormalizationStats(normStats);\n}\n\n\nIsNormStatsJsonNullOrEmpty\nThis method checks if the provided JSON file (normStatsJson) is null or empty.\n/// &lt;summary&gt;\n/// Check if the provided JSON file is null or empty.\n/// &lt;/summary&gt;\n/// &lt;returns&gt;True if the file is null or empty, otherwise false.&lt;/returns&gt;\nprivate bool IsNormStatsJsonNullOrEmpty()\n{\n    return normStatsJson == null || string.IsNullOrWhiteSpace(normStatsJson.text);\n}\n\n\nDeserializeNormStats\nThis method deserializes the provided JSON string into a NormStats object. It catches any exceptions that might occur during the deserialization process and logs an error message, if any.\n/// &lt;summary&gt;\n/// Deserialize the provided JSON string to a NormStats object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"json\"&gt;The JSON string to deserialize.&lt;/param&gt;\n/// &lt;returns&gt;A deserialized NormStats object.&lt;/returns&gt;\nprivate NormStats DeserializeNormStats(string json)\n{\n    try\n    {\n        return JsonUtility.FromJson&lt;NormStats&gt;(json);\n    }\n    catch (Exception ex)\n    {\n        Debug.LogError($\"Failed to deserialize normalization stats JSON: {ex.Message}\");\n        return null;\n    }\n}\n\n\nUpdateNormalizationStats\nThis method updates the mean and standard deviation arrays with the provided NormStats object.\n/// &lt;summary&gt;\n/// Update the mean and standard deviation with the provided NormStats object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"normStats\"&gt;The NormStats object containing the mean and standard deviation.&lt;/param&gt;\nprivate void UpdateNormalizationStats(NormStats normStats)\n{\n    if (normStats == null)\n    {\n        return;\n    }\n\n    mean = normStats.mean;\n    std = normStats.std;\n    // Disable scaling if no scale value is provided\n    scale = normStats.scale == 0f ? 1f : normStats.scale;\n}\n\n\nInitializeProcessingShaders\nThis method initializes the processing shaders by setting the mean and standard deviation values for normalization and creating compute buffers for those values.\n/// &lt;summary&gt;\n/// Initializes the processing shaders by setting the mean and standard deviation values.\n/// &lt;/summary&gt;\nprivate void InitializeProcessingShaders()\n{\n    normalizeMaterial.SetVector(\"_Mean\", new Vector4(mean[0], mean[1], mean[2], 0));\n    normalizeMaterial.SetVector(\"_Std\", new Vector4(std[0], std[1], std[2], 0));\n    normalizeMaterial.SetFloat(\"_Scale\", scale);\n\n    if (SystemInfo.supportsComputeShaders)\n    {\n        int kernelIndex = processingComputeShader.FindKernel(\"NormalizeImage\");\n\n        meanBuffer = CreateComputeBuffer(mean);\n        stdBuffer = CreateComputeBuffer(std);\n\n        processingComputeShader.SetBuffer(kernelIndex, \"_Mean\", meanBuffer);\n        processingComputeShader.SetBuffer(kernelIndex, \"_Std\", stdBuffer);\n        processingComputeShader.SetFloat(\"_Scale\", scale);\n    }\n}\n\n\nCreateComputeBuffer\nThis method creates a ComputeBuffer and sets the provided data (a float array). It returns the created compute buffer.\n/// &lt;summary&gt;\n/// Creates a compute buffer and sets the provided data.\n/// &lt;/summary&gt;\n/// &lt;param name=\"data\"&gt;The data to set in the compute buffer.&lt;/param&gt;\n/// &lt;returns&gt;A compute buffer with the provided data.&lt;/returns&gt;\nprivate ComputeBuffer CreateComputeBuffer(float[] data)\n{\n    ComputeBuffer buffer = new ComputeBuffer(data.Length, sizeof(float));\n    buffer.SetData(data);\n    return buffer;\n}\n\n\nProcessImageComputeShader\nThis method prepares an image with a compute shader with the specified function name. It creates a temporary render texture, binds the source and destination textures to the compute shader, dispatches the shader, and blits the processed image back to the original image.\n/// &lt;summary&gt;\n/// Processes an image using a compute shader with the specified function name.\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The image to be processed.&lt;/param&gt;\n/// &lt;param name=\"functionName\"&gt;The name of the function in the compute shader to use for processing.&lt;/param&gt;\npublic void ProcessImageComputeShader(RenderTexture image, string functionName)\n{\n    int kernelHandle = processingComputeShader.FindKernel(functionName);\n    // Create a temporary render texture\n    RenderTexture result = GetTemporaryRenderTexture(image);\n\n    // Bind the source and destination textures to the compute shader\n    BindTextures(kernelHandle, image, result);\n    // Dispatche the shader\n    DispatchShader(kernelHandle, result);\n    // Blit the processed image back to the original image\n    Graphics.Blit(result, image);\n\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nProcessImageShader\nThis method processes an image using a material. It creates a temporary render texture, applies the normalization material to the input image, and copies the resulting image back to the original image.\n/// &lt;summary&gt;\n/// Processes an image using a material.\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The image to be processed.&lt;/param&gt;\npublic void ProcessImageShader(RenderTexture image)\n{\n    // Create a temporary render texture\n    RenderTexture result = GetTemporaryRenderTexture(image, false);\n    RenderTexture.active = result;\n    // Apply the normalization material to the input image\n    Graphics.Blit(image, result, normalizeMaterial);\n    // Copy the result back to the original image\n    Graphics.Blit(result, image);\n\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nGetTemporaryRenderTexture\nThis method creates a temporary render texture with identical dimensions to the image. It takes an optional boolean parameter enableRandomWrite to enable or disable random access write into the RenderTexture.\n/// &lt;summary&gt;\n/// Creates a temporary render texture with the same dimensions as the given image.\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The image to match dimensions with.&lt;/param&gt;\n/// &lt;param name=\"enableRandomWrite\"&gt;Enable random access write into the RenderTexture.&lt;/param&gt;\n/// &lt;returns&gt;A temporary render texture.&lt;/returns&gt;\nprivate RenderTexture GetTemporaryRenderTexture(RenderTexture image, bool enableRandomWrite = true)\n{\n    // Create a temporary render texture\n    RenderTexture result = RenderTexture.GetTemporary(image.width, image.height, 24, RenderTextureFormat.ARGBHalf);\n    // Set random write access\n    result.enableRandomWrite = enableRandomWrite;\n    result.Create();\n    return result;\n}\n\n\nBindTextures\nThis method binds the source and destination textures to the compute shader with the provided kernel handle. It sets the _OutputImage and _InputImage properties of the compute shader with the destination and source textures, respectively.\n/// &lt;summary&gt;\n/// Binds the source and destination textures to the compute shader.\n/// &lt;/summary&gt;\n/// &lt;param name=\"kernelHandle\"&gt;The kernel handle of the compute shader.&lt;/param&gt;\n/// &lt;param name=\"source\"&gt;The source texture to be processed.&lt;/param&gt;\n/// &lt;param name=\"destination\"&gt;The destination texture for the processed result.&lt;/param&gt;\nprivate void BindTextures(int kernelHandle, RenderTexture source, RenderTexture destination)\n{\n    processingComputeShader.SetTexture(kernelHandle, \"_OutputImage\", destination);\n    processingComputeShader.SetTexture(kernelHandle, \"_InputImage\", source);\n}\n\n\nDispatchShader\nThis method dispatches the compute shader based on the dimensions of the result texture. It calculates the thread groups in the X and Y dimensions and runs the compute shader using the provided kernel handle.\n/// &lt;summary&gt;\n/// Dispatches the compute shader based on the dimensions of the result texture.\n/// &lt;/summary&gt;\n/// &lt;param name=\"kernelHandle\"&gt;The kernel handle of the compute shader.&lt;/param&gt;\n/// &lt;param name=\"result\"&gt;The result render texture.&lt;/param&gt;\nprivate void DispatchShader(int kernelHandle, RenderTexture result)\n{\n    // Calculate the thread groups in the X and Y dimensions\n    int threadGroupsX = Mathf.CeilToInt((float)result.width / 8);\n    int threadGroupsY = Mathf.CeilToInt((float)result.height / 8);\n    // Execute the compute shader\n    processingComputeShader.Dispatch(kernelHandle, threadGroupsX, threadGroupsY, 1);\n}\n\n\nCalculateInputDims\nThis method calculates the input dimensions of the processed image based on the original image dimensions, given a target dimension.\n/// &lt;summary&gt;\n/// Calculates the input dimensions of the processed image based on the original image dimensions.\n/// &lt;/summary&gt;\n/// &lt;param name=\"imageDims\"&gt;The dimensions of the original image.&lt;/param&gt;\n/// &lt;returns&gt;The calculated input dimensions for the processed image.&lt;/returns&gt;\npublic Vector2Int CalculateInputDims(Vector2Int imageDims, int targetDim = 224)\n{\n    targetDim = Mathf.Max(targetDim, 64);\n    float scaleFactor = (float)targetDim / Mathf.Min(imageDims.x, imageDims.y);\n    return Vector2Int.RoundToInt(new Vector2(imageDims.x * scaleFactor, imageDims.y * scaleFactor));\n}\n\n\nCropImageComputeShader\nThis method crops an image using a compute shader. It binds the source and destination textures to the compute shader, sets the offset and size parameters, dispatches the shader, and copies the result to the cropped image.\n/// &lt;summary&gt;\n/// Crops an image using a compute shader with the given offset and size.\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The original image to be cropped.&lt;/param&gt;\n/// &lt;param name=\"croppedImage\"&gt;The cropped output image.&lt;/param&gt;\n/// &lt;param name=\"offset\"&gt;The offset for the crop area in the original image.&lt;/param&gt;\n/// &lt;param name=\"size\"&gt;The size of the crop area.&lt;/param&gt;\npublic void CropImageComputeShader(RenderTexture image, RenderTexture croppedImage, Vector2Int offset, Vector2Int size)\n{\n    int kernelHandle = processingComputeShader.FindKernel(\"CropImage\");\n    RenderTexture result = GetTemporaryRenderTexture(croppedImage);\n\n    // Bind the source and destination textures to the compute shader\n    BindTextures(kernelHandle, image, result);\n    // Set the offset and size parameters\n    processingComputeShader.SetInts(\"_CropOffset\", new int[] { offset.x, offset.y });\n    processingComputeShader.SetInts(\"_CropSize\", new int[] { size.x, size.y });\n    // Execute the compute shader\n    DispatchShader(kernelHandle, result);\n    // Copy the result to the cropped image texture\n    Graphics.Blit(result, croppedImage);\n\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nCropImageShader\nThis method crops an image using a material. It sets the offset and size parameters on the crop material, creates a temporary render texture, applies the crop material to the input image, and blits the result back to the cropped image.\n/// &lt;summary&gt;\n/// Crops an image using a shader with the given offset and size.\n/// &lt;/summary&gt;\n/// &lt;param name=\"image\"&gt;The original image to be cropped.&lt;/param&gt;\n/// &lt;param name=\"croppedImage\"&gt;The cropped output image.&lt;/param&gt;\n/// &lt;param name=\"offset\"&gt;The offset for the crop area in the original image (float array with two elements).&lt;/param&gt;\n/// &lt;param name=\"size\"&gt;The size of the crop area (float array with two elements).&lt;/param&gt;\n\npublic void CropImageShader(RenderTexture image, RenderTexture croppedImage, float[] offset, float[] size)\n{\n    // Set the offset and size parameters on the crop material\n    cropMaterial.SetVector(\"_Offset\", new Vector4(offset[0], offset[1], 0, 0));\n    cropMaterial.SetVector(\"_Size\", new Vector4(size[0], size[1], 0, 0));\n\n    // Create a temporary render texture\n    RenderTexture result = GetTemporaryRenderTexture(croppedImage, false);\n    RenderTexture.active = result;\n\n    // Apply the crop material to the input image\n    Graphics.Blit(image, result, cropMaterial);\n    // Copy the result to the cropped image texture\n    Graphics.Blit(result, croppedImage);\n\n    RenderTexture.ReleaseTemporary(result);\n}\n\n\nOnDisable\nThis method runs when the script is disabled. If the current platform supports compute shaders, it releases the compute buffers.\n/// &lt;summary&gt;\n/// Called when the script is disabled.\n/// &lt;/summary&gt;\nprivate void OnDisable()\n{\n    ReleaseComputeBuffers();\n}\n\n\nReleaseComputeBuffers\nThis method releases the compute buffers when compute shaders are supported.\n/// &lt;summary&gt;\n/// Releases the compute buffers if compute shaders are supported.\n/// &lt;/summary&gt;\nprivate void ReleaseComputeBuffers()\n{\n    if (SystemInfo.supportsComputeShaders)\n    {\n        meanBuffer?.Release();\n        stdBuffer?.Release();\n    }\n}\n\n\n\n\nCropImage.shader\nThis shader crops an input image based on the specified offset and size values. The complete code is available on GitHub at the link below.\n\nCropImage.shader\n\n\nDefine Properties\nProperties {\n    // The input texture to crop\n    _MainTex (\"Texture\", 2D) = \"white\" {}\n    // A vector representing the x and y offsets for the cropping area\n    _Offset (\"Offset\", Vector) = (0, 0, 0, 0)\n    // A vector representing the width and height of the cropping area\n    _Size (\"Size\", Vector) = (0, 0, 0, 0)\n}\n\n\nSubShader Configuration\nIn the SubShader block, culling and depth are disabled to ensure that the shader will always render regardless of the camera’s position and orientation.\n// No culling or depth\nCull Off ZWrite Off ZTest Always\n\n\nPass Block\nThe Pass block contains the vertex and fragment shaders that process the input texture and crops it based on the provided _Offset and _Size values.\nThe vertex shader receives the input vertex data and passes it through to the fragment shader. It is a simple pass-through shader that does not modify the input data.\nThe fragment shader is responsible for cropping the input texture based on the provided _Offset and _Size values.\nPass {\n    CGPROGRAM\n    #pragma vertex vert\n    #pragma fragment frag\n\n    #include \"UnityCG.cginc\"\n\n    // Uniform variables for the offset and size of the cropping area\n    float2 _Offset;\n    float2 _Size;\n\n    // Contains the vertex position and texture coordinates\n    struct appdata {\n        float4 vertex : POSITION;\n        float2 uv : TEXCOORD0;\n    };\n\n    // Contains the transformed vertex position and texture coordinates\n    struct v2f {\n        float2 uv : TEXCOORD0;\n        float4 vertex : SV_POSITION;\n    };\n\n    v2f vert (appdata v) {\n        v2f o;\n        // Transform the input vertex position to clip space\n        o.vertex = UnityObjectToClipPos(v.vertex);\n        // Copy the input texture coordinates to the output structure\n        o.uv = v.uv;\n        return o;\n    }\n\n    sampler2D _MainTex;\n\n    // Fragment shader function\n    fixed4 frag (v2f i) : SV_Target {\n        // Calculate the input position based on the offset and size\n        float2 inputPos = i.uv * _Size + _Offset;\n        // Sample the input image and return the cropped color values\n        return tex2D(_MainTex, inputPos);\n    }\n    ENDCG\n}\n\n\n\n\nNormalizeImage.shader\nThis shader normalizes an input image’s color channels based on the specified mean, standard deviation, and scale values. The complete code is available on GitHub at the link below.\n\nNormalizeImage.shader\n\n\nDefine Properties\nProperties\n{\n    // The input image texture\n    _MainTex(\"Texture\", 2D) = \"white\" {}\n    // A vector representing the mean of the color channels (r, g, b, a).\n    _Mean(\"Mean\", Vector) = (0, 0, 0, 0)\n    // A vector representing the standard deviation of the color channels (r, g, b, a).\n    _Std(\"Std\", Vector) = (1, 1, 1, 1)\n    // A float range to control the scaling of the output color values.\n    _Scale(\"Scale\", Range(0, 10)) = 1\n}\n\n\nSubShader Configuration\nThe shader disables culling and depth testing since it’s for 2D image processing.\n// No culling or depth\nCull Off ZWrite Off ZTest Always\n\n\nPass Block\nThe Pass block contains the vertex and fragment shaders that process the input texture and normalizes it based on the provided _Mean, _Std, and _Scale values.\nThe vertex shader receives the input vertex data and passes it through to the fragment shader. It is a simple pass-through shader that does not modify the input data.\nThe fragment shader is responsible for cropping the input texture based on the provided _Mean, _Std, and _Scale values.\nPass\n{\n    CGPROGRAM\n    #pragma vertex vert\n    #pragma fragment frag\n\n    #include \"UnityCG.cginc\"\n\n    // Uniform variables to hold the mean and standard deviation values for each color channel (r, g, b)\n    float4 _Mean;\n    float4 _Std;\n    float _Scale;\n\n    // Contains the vertex position and texture coordinates\n    struct appdata {\n        float4 vertex : POSITION;\n        float2 uv : TEXCOORD0;\n    };\n\n    // Contains the transformed vertex position and texture coordinates\n    struct v2f {\n        float2 uv : TEXCOORD0;\n        float4 vertex : SV_POSITION;\n    };\n\n    v2f vert (appdata v) {\n        v2f o;\n        // Transform the input vertex position to clip space\n        o.vertex = UnityObjectToClipPos(v.vertex);\n        // Copy the input texture coordinates to the output structure\n        o.uv = v.uv;\n        return o;\n    }\n\n    sampler2D _MainTex;\n\n    // Fragment shader function\n    float4 frag(v2f i) : SV_Target\n    {\n        // Sample the input image\n        float4 col = tex2D(_MainTex, i.uv);\n        // Normalize each color channel (r, g, b) and scale\n        col.rgb = ((col.rgb - _Mean.rgb) / _Std.rgb) * _Scale;\n        // Return the normalized color values\n        return col;\n    }\n    ENDCG\n}\n\n\n\n\nProcessingShader.compute\nThis Compute Shader implements multiple image processing operations. The complete code is available on GitHub at the link below.\n\nProcessingShader.compute\n\n\nResources\n// Input image texture\nTexture2D&lt;float4&gt; _InputImage;\n\n// Output image texture\nRWTexture2D&lt;float4&gt; _OutputImage;\n\n// Structured buffer to hold the mean values for each color channel (r, g, b)\nRWStructuredBuffer&lt;float&gt; _Mean;\n\n// Structured buffer to hold the standard deviation values for each color channel (r, g, b)\nRWStructuredBuffer&lt;float&gt; _Std;\n\n// Float variable that represents the scaling factor to apply to the normalized pixel values\nfloat _Scale;\n\n// The (x, y) coordinates of the top-left corner of the cropping region\nint2 _CropOffset;\n// The size (width, height) of the cropping region\nint2 _CropSize;\n\n\nNormalizeImage\nThis kernel normalizes the input image by applying a mean and standard deviation to each color channel (red, green, blue). It also scales the normalized pixel values by a given scale factor.\n// Normalize the input image\n[numthreads(8, 8, 1)]\nvoid NormalizeImage(uint3 id : SV_DispatchThreadID)\n{\n    float4 inputPixel = _InputImage[id.xy];\n    \n    // Create float4 variables for mean and standard deviation\n    float4 mean = float4(_Mean[0], _Mean[1], _Mean[2], 0.0);\n    float4 std = float4(_Std[0], _Std[1], _Std[2], 1.0);\n\n    float4 normalizedPixel = (inputPixel - mean) / std;\n\n    // Apply scaling and leave the alpha channel unchanged\n    _OutputImage[id.xy] = float4(normalizedPixel.rgb * _Scale, inputPixel.a);\n}\n\n\nCropImage\nThis kernel crops the input image by applying a given offset and size vector to produce a smaller image.\n// Crop the input image\n[numthreads(8, 8, 1)]\nvoid CropImage(uint3 id : SV_DispatchThreadID)\n{\n    if (id.x &lt; (uint)_CropSize.x && id.y &lt; (uint)_CropSize.y)\n    {\n        int2 inputPos = id.xy + _CropOffset;\n        _OutputImage[id.xy] = _InputImage[inputPos];\n    }\n}\n\n\nFlipXAxis\nThis kernel flips the input image along the x-axis, effectively mirroring it.\n// Flip the input image around the x-axis\n[numthreads(8, 8, 1)]\nvoid FlipXAxis(uint3 id : SV_DispatchThreadID)\n{\n    uint width;\n    uint height;\n    _InputImage.GetDimensions(width, height);\n\n    // Compute the flipped pixel coordinates\n    int2 flippedCoords = int2(id.x, height - id.y - 1);\n    _OutputImage[id.xy] = _InputImage[flippedCoords];\n}"
  },
  {
    "objectID": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#conclusion",
    "href": "posts/unity-deep-learning-image-preprocessor-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Deep Learning Image Preprocessor Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Deep Learning Image Preprocessor package. The package contains utility functions and shaders for preparing image input to perform inference with deep learning models in Unity.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-deep-learning-image-preprocessor\n\nYou can explore demo projects that use this package at the links below.\n\nBarracuda Image Classification Demo: A simple Unity project demonstrating how to perform image classification with the barracuda-inference-image-classification package.\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package.\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html",
    "href": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Human Pose 2D Toolkit Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#introduction",
    "href": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Human Pose 2D Toolkit Package",
    "section": "Introduction",
    "text": "Introduction\nThe Unity Human Pose 2D Toolkit provides an easy-to-use and customizable solution to work with and visualize 2D human poses on a Unity canvas.\nSome of my tutorials involve using 2D pose estimation models in Unity applications. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Check out the demo video below to see this package in action.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#package-overview",
    "href": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Human Pose 2D Toolkit Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains three C# scripts and prefabs to construct 2D human poses.\n\nC# Scripts\n\nHumanPose2DUtils.cs: This script provides functionality to work with 2D pose skeletons for pose estimation tasks.\nHumanPose2DVisualizer.cs: This script displays 2D human pose skeletons on a Unity canvas.\nAddCustomDefineSymbol.cs: An Editor script that automatically adds a custom scripting define symbol to the project after the package installs.\n\n\n\nPrefabs\n\nBonePrefab.prefab: The HumanPose2DVisualizer.cs script uses this prefab to construct the bones connecting points in pose skeletons.\nJointPrefab.prefab: An Image prefab used to visualize the points in pose skeletons.\nPoseContainerPrefab.prefab: This prefab is for pose containers that hold the joints and bones for pose skeletons.\nHumanPose2DVisualizer.prefab: This prefab helps simplify adding 2D human pose visualization to a Unity scene. The prefab already has the HumanPose2DVisualizer script attached and has a child Canvas component."
  },
  {
    "objectID": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#code-explanation",
    "href": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Human Pose 2D Toolkit Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Unity Human Pose 2D Toolkit package by examining the purpose and functionality of each C# script.\n\nHumanPose2DUtils.cs\nThe HumanPose2DUtils.cs script provides functionality to work with 2D pose skeletons for pose estimation tasks. It contains utility classes and structs for managing 2D human pose data. The complete code is available on GitHub at the link below.\n\nHumanPose2DUtils.cs\n\n\nBodyPart2D struct\nThis struct represents a single body part in 2D space with its index, coordinates, and probability.\n/// &lt;summary&gt;\n/// Represents a single body part in 2D space with its index, coordinates, and probability.\n/// &lt;/summary&gt;\npublic struct BodyPart2D\n{\n    public int index; // The index of the body part\n    public Vector2 coordinates; // The 2D coordinates of the body part\n    public float prob; // The probability of the detected body part\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the BodyPart2D struct.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"index\"&gt;The index of the body part.&lt;/param&gt;\n    /// &lt;param name=\"coordinates\"&gt;The 2D coordinates of the body part.&lt;/param&gt;\n    /// &lt;param name=\"prob\"&gt;The probability of the detected body part.&lt;/param&gt;\n    public BodyPart2D(int index, Vector2 coordinates, float prob)\n    {\n        this.index = index;\n        this.coordinates = coordinates;\n        this.prob = prob;\n    }\n}\n\n\nHumanPose2D struct\nThis struct represents a detected human pose in 2D space with its index and an array of body parts.\n/// &lt;summary&gt;\n/// Represents a detected human pose in 2D space with its index and an array of body parts.\n/// &lt;/summary&gt;\npublic struct HumanPose2D\n{\n    public int index; // The index of the detected human pose\n    public BodyPart2D[] bodyParts; // An array of the body parts that make up the human pose\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the HumanPose2D struct.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"index\"&gt;The index of the detected human pose.&lt;/param&gt;\n    /// &lt;param name=\"bodyParts\"&gt;An array of body parts that make up the human pose.&lt;/param&gt;\n    public HumanPose2D(int index, BodyPart2D[] bodyParts)\n    {\n        this.index = index;\n        this.bodyParts = bodyParts;\n    }\n}\n\n\nHumanPose2DUtility static class\nThis class contains a single static method that scales and optionally mirrors the coordinates of a body part in a pose skeleton to match the in-game screen and display resolutions.\npublic static class HumanPose2DUtility\n{\n\n    /// &lt;summary&gt;\n    /// Scales and optionally mirrors the coordinates of a body part in a pose skeleton to match the in-game screen and display resolutions.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"coordinates\"&gt;The (x,y) coordinates for a BodyPart object.&lt;/param&gt;\n    /// &lt;param name=\"inputDims\"&gt;The dimensions of the input image used for pose estimation.&lt;/param&gt;\n    /// &lt;param name=\"screenDims\"&gt;The dimensions of the in-game screen where the body part will be displayed.&lt;/param&gt;\n    /// &lt;param name=\"offset\"&gt;An offset to apply to the body part coordinates when scaling.&lt;/param&gt;\n    /// &lt;param name=\"mirrorScreen\"&gt;A boolean flag to indicate if the body part coordinates should be mirrored horizontally (default is false).&lt;/param&gt;\n    public static Vector2 ScaleBodyPartCoords(Vector2 coordinates, Vector2Int inputDims, Vector2 screenDims, Vector2Int offset, bool mirrorScreen)\n    {\n        // The smallest dimension of the screen\n        float minScreenDim = Mathf.Min(screenDims.x, screenDims.y);\n        // The smallest input dimension\n        int minInputDim = Mathf.Min(inputDims.x, inputDims.y);\n        // Calculate the scale value between the in-game screen and input dimensions\n        float minImgScale = minScreenDim / minInputDim;\n        // Calculate the scale value between the in-game screen and display\n        float displayScaleX = Screen.width / screenDims.x;\n        float displayScaleY = Screen.height / screenDims.y;\n        float displayScale = Mathf.Min(displayScaleX, displayScaleY);\n\n\n        // Scale body part coordinates to in-game screen resolution and flip the coordinates vertically\n        float x = (coordinates.x + offset.x) * minImgScale;\n        float y = (inputDims.y - (coordinates.y - offset.y)) * minImgScale;\n\n        // Mirror bounding box across screen\n        if (mirrorScreen)\n        {\n            x = screenDims.x - x;\n        }\n\n        // Scale coordinates to display resolution\n        coordinates.x = x * displayScale;\n        coordinates.y = y * displayScale;\n\n        // Offset the coordinates coordinates based on the difference between the in-game screen and display\n        coordinates.x += (Screen.width - screenDims.x * displayScale) / 2;\n        coordinates.y += (Screen.height - screenDims.y * displayScale) / 2;\n\n        return coordinates;\n    }\n}\n\n\n\n\nHumanPose2DVisualizer.cs\nThe HumanPose2DVisualizer script is a Unity C# MonoBehaviour class that displays 2D human pose skeletons on a Unity canvas. It creates, updates, and manages UI elements for visualizing them based on the provided HumanPose2D array. The complete code is available on GitHub at the link below.\n\nHumanPose2DVisualizer.cs\n\n\nSerialized Fields\nThe script contains several fields for prefabs and configuring pose skeleton visualizations.\n// Main canvas to display poses\n[Header(\"UI Components\")]\n[Tooltip(\"The main canvas to display poses\")]\n[SerializeField] private Canvas canvas;\n\n// Prefabs for pose containers, joints, and bones\n[Tooltip(\"The prefab for the pose container, which holds the joints and bones\")]\n[SerializeField] private RectTransform poseContainerPrefab;\n[Tooltip(\"The prefab for the joint image\")]\n[SerializeField] private Image jointPrefab;\n[Tooltip(\"The prefab for the bone RectTransform\")]\n[SerializeField] private RectTransform bonePrefab;\n\n// Configuration and styling\n[Header(\"Configuration\")]\n[Tooltip(\"The JSON file containing body part connection information\")]\n[SerializeField] private TextAsset bodyPartConnectionsFile;\n[Tooltip(\"The color of the bones\")]\n[SerializeField] private Color boneColor = Color.green;\n[Tooltip(\"The color of the joints\")]\n[SerializeField] private Color jointColor = Color.green;\n\n\nSerialized Classes\nThere are a couple of nested serialized classes to store body part connection information from a JSON file.\n// Serializable classes to store body part connection information from JSON\n[System.Serializable]\nclass BodyPartConnection\n{\n    public int from; // Index of the starting body part\n    public int to;   // Index of the ending body part\n}\n\n[System.Serializable]\nclass BodyPartConnectionList\n{\n    public List&lt;BodyPartConnection&gt; bodyPartConnections; // List of body part connections\n}\n\n\nPrivate Variables\n// Variables to store runtime instances and data\nprivate List&lt;BodyPartConnection&gt; bodyPartConnections; // List of body part connections\nprivate List&lt;RectTransform&gt; poseContainers = new List&lt;RectTransform&gt;(); // List of instantiated pose containers\nprivate List&lt;List&lt;Image&gt;&gt; joints = new List&lt;List&lt;Image&gt;&gt;(); // Nested list of instantiated joint images\nprivate List&lt;List&lt;RectTransform&gt;&gt; bones = new List&lt;List&lt;RectTransform&gt;&gt;(); // Nested list of instantiated bone RectTransforms\nprivate float confidenceThreshold; // Confidence threshold for displaying poses\n\n\nGUID Constants\nThese are the GUIDs of the default assets.\n// GUIDs of the default assets\nprivate const string PoseContainerPrefabGUID = \"12c840be0a8d4adc879fc14fb79a316d\";\nprivate const string JointPrefabGUID = \"d90f7f2e5b8f4daa885f9441f0f33427\";\nprivate const string BonePrefabGUID = \"ed947d23b5354617b130aa8ee0cc610b\";\nprivate const string BodyPartConnectionsFileGUID = \"0fc008c60a8e44589674b0f455384a5b\";\n\n\nReset\nThis method sets the default assets from the project using their GUIDs. It uses AssetDatabase to find them and set the default values. This method will only work in the Unity Editor, not in a build.\n/// &lt;summary&gt;\n/// Reset is called when the user hits the Reset button in the Inspector's context menu\n/// or when adding the component the first time. This function is only called in editor mode.\n/// &lt;/summary&gt;\nprivate void Reset()\n{\n    // Load default assets only in the Unity Editor, not in a build\n#if UNITY_EDITOR\n    poseContainerPrefab = LoadDefaultAsset&lt;RectTransform&gt;(PoseContainerPrefabGUID);\n    jointPrefab = LoadDefaultAsset&lt;Image&gt;(JointPrefabGUID);\n    bonePrefab = LoadDefaultAsset&lt;RectTransform&gt;(BonePrefabGUID);\n    bodyPartConnectionsFile = LoadDefaultAsset&lt;TextAsset&gt;(BodyPartConnectionsFileGUID);\n#endif\n}\n\n\nLoadDefaultAsset\nThis method provides a generic way to load default assets for the specified fields using their GUIDs.\n/// &lt;summary&gt;\n/// Loads the default asset for the specified type using its GUID.\n/// &lt;/summary&gt;\n/// &lt;typeparam name=\"T\"&gt;The type of asset to be loaded.&lt;/typeparam&gt;\n/// &lt;param name=\"guid\"&gt;The GUID of the default asset.&lt;/param&gt;\n/// &lt;returns&gt;The loaded asset of the specified type.&lt;/returns&gt;\n/// &lt;remarks&gt;\n/// This method is only executed in the Unity Editor, not in builds.\n/// &lt;/remarks&gt;\nprivate T LoadDefaultAsset&lt;T&gt;(string guid) where T : UnityEngine.Object\n{\n#if UNITY_EDITOR\n    // Load the asset from the AssetDatabase using its GUID\n    return UnityEditor.AssetDatabase.LoadAssetAtPath&lt;T&gt;(UnityEditor.AssetDatabase.GUIDToAssetPath(guid));\n#else\n    return null;\n#endif\n}\n\n\nStart\nThis method runs when the script initializes and loads the body part connection list from the JSON file.\nprivate void Start()\n{\n    LoadBodyPartConnectionList();\n}\n\n\nLoadBodyPartConnectionList\nThis method deserializes the JSON file specifying the body part connections for pose skeletons.\n/// &lt;summary&gt;\n/// Load the JSON file\n/// &lt;summary&gt;\nprivate void LoadBodyPartConnectionList()\n{\n    if (IsJsonNullOrEmpty())\n    {\n        Debug.LogError(\"JSON file is null or empty.\");\n        return;\n    }\n\n    bodyPartConnections = DeserializeBodyPartConnectionsList(bodyPartConnectionsFile.text).bodyPartConnections;\n}\n\n\nIsJsonNullOrEmpty\nThis method checks if the JSON file is null or empty.\n/// &lt;summary&gt;\n/// Check if JSON file is null or empty\n/// &lt;summary&gt;\nprivate bool IsJsonNullOrEmpty()\n{\n    return bodyPartConnectionsFile == null || string.IsNullOrWhiteSpace(bodyPartConnectionsFile.text);\n}\n\n\nDeserializeBodyPartConnectionsList\nThis method deserializes the JSON string into a BodyPartConnectionList.\n/// &lt;summary&gt;\n/// Deserialize the JSON string\n/// &lt;summary&gt;\nprivate BodyPartConnectionList DeserializeBodyPartConnectionsList(string json)\n{\n    try\n    {\n        return JsonUtility.FromJson&lt;BodyPartConnectionList&gt;(json);\n    }\n    catch (Exception ex)\n    {\n        Debug.LogError($\"Failed to deserialize class labels JSON: {ex.Message}\");\n        return null;\n    }\n}\n\n\nUpdatePoseVisualizations\nThis method updates pose visualizations based on the provided human poses and a confidence threshold.\n/// &lt;summary&gt;\n/// Updates the pose visualizations based on the provided human poses and a confidence threshold.\n/// &lt;/summary&gt;\n/// &lt;param name=\"humanPoses\"&gt;An array of human poses to visualize&lt;/param&gt;\n/// &lt;param name=\"confidenceThreshold\"&gt;The minimum confidence required to display a pose (default is 0.5f)&lt;/param&gt;\npublic void UpdatePoseVisualizations(HumanPose2D[] humanPoses, float confidenceThreshold = 0.5f)\n{\n    this.confidenceThreshold = confidenceThreshold;\n\n    // Instantiate pose containers, joint images, and bone RectTransforms as needed to match the number of humanPoses\n    while (poseContainers.Count &lt; humanPoses.Length)\n    {\n        RectTransform newPoseContainer = Instantiate(poseContainerPrefab, canvas.transform);\n        poseContainers.Add(newPoseContainer);\n        joints.Add(new List&lt;Image&gt;());\n        bones.Add(new List&lt;RectTransform&gt;());\n    }\n\n    for (int i = 0; i &lt; poseContainers.Count; i++)\n    {\n        if (i &lt; humanPoses.Length)\n        {\n            // Get references to joint and bone containers for the current pose\n            RectTransform jointContainer = poseContainers[i].Find(\"JointContainer\").GetComponent&lt;RectTransform&gt;();\n            RectTransform boneContainer = poseContainers[i].Find(\"BoneContainer\").GetComponent&lt;RectTransform&gt;();\n\n            // Update the joint positions and visibility\n            UpdateJoints(humanPoses[i].bodyParts, jointContainer, joints[i]);\n            // Update the bone positions, rotations, and visibility\n            UpdateBones(humanPoses[i].bodyParts, boneContainer, joints[i], bones[i]);\n\n            // Set the pose container active\n            poseContainers[i].gameObject.SetActive(true);\n        }\n        else\n        {\n            // Set the pose container inactive for unused containers\n            poseContainers[i].gameObject.SetActive(false);\n        }\n    }\n}\n\n\nScreenToCanvasPoint\nThis method convert a screen point to a local one within the given canvas RectTransform.\n/// &lt;summary&gt;\n/// Converts a screen point to a local point within the given canvas RectTransform.\n/// &lt;/summary&gt;\n/// &lt;param name=\"canvas\"&gt;The canvas RectTransform to convert the point to&lt;/param&gt;\n/// &lt;param name=\"screenPoint\"&gt;The screen point to convert&lt;/param&gt;\n/// &lt;returns&gt;A Vector2 representing the local point within the canvas RectTransform&lt;/returns&gt;\nprivate Vector2 ScreenToCanvasPoint(RectTransform canvas, Vector2 screenPoint)\n{\n    RectTransformUtility.ScreenPointToLocalPointInRectangle(canvas, screenPoint, null, out Vector2 localPoint);\n    return localPoint;\n}\n\n\nUpdateJoints\nThis method updates joint visualizations based on the provided body parts, adjusting their positions and visibility.\n/// &lt;summary&gt;\n/// Updates the joint visualizations based on the provided body parts, adjusting their positions and visibility.\n/// &lt;/summary&gt;\n/// &lt;param name=\"bodyParts\"&gt;An array of body parts containing position and probability data&lt;/param&gt;\n/// &lt;param name=\"jointContainer\"&gt;The RectTransform containing joint images&lt;/param&gt;\n/// &lt;param name=\"jointsList\"&gt;A list of instantiated joint images&lt;/param&gt;\nprivate void UpdateJoints(BodyPart2D[] bodyParts, RectTransform jointContainer, List&lt;Image&gt; jointsList)\n{\n    // Instantiate joint images as needed to match the number of bodyParts\n    while (jointsList.Count &lt; bodyParts.Length)\n    {\n        Image newJoint = Instantiate(jointPrefab, jointContainer);\n        jointsList.Add(newJoint);\n    }\n\n    for (int i = 0; i &lt; jointsList.Count; i++)\n    {\n        if (bodyParts[i].prob &gt;= confidenceThreshold)\n        {\n            Image joint = jointsList[i];\n            RectTransform jointRect = joint.rectTransform;\n            // Update joint position\n            jointRect.anchoredPosition = ScreenToCanvasPoint(jointContainer, bodyParts[i].coordinates);\n            // Update joint color\n            joint.color = jointColor;\n            // Set the joint game object active\n            joint.gameObject.SetActive(true);\n        }\n        else\n        {\n            // Set the joint game object inactive if below the confidence threshold\n            jointsList[i].gameObject.SetActive(false);\n        }\n    }\n}\n\n\nUpdateBones\nThis method updates bone visualizations based on the provided body parts and joint positions, adjusting their positions, rotations, and visibility.\n/// &lt;summary&gt;\n/// Updates the bone visualizations based on the provided body parts and joint positions, adjusting their positions, rotations, and visibility.\n/// &lt;/summary&gt;\n/// &lt;param name=\"bodyParts\"&gt;An array of body parts containing position and probability data&lt;/param&gt;\n/// &lt;param name=\"boneContainer\"&gt;The RectTransform containing bone RectTransforms&lt;/param&gt;\n/// &lt;param name=\"jointsList\"&gt;A list of instantiated joint images&lt;/param&gt;\n/// &lt;param name=\"bonesList\"&gt;A list of instantiated bone RectTransforms&lt;/param&gt;\nprivate void UpdateBones(BodyPart2D[] bodyParts, RectTransform boneContainer, List&lt;Image&gt; jointsList, List&lt;RectTransform&gt; bonesList)\n{\n    // Instantiate bone RectTransforms as needed to match the number of bodyPartConnections\n    while (bonesList.Count &lt; bodyPartConnections.Count)\n    {\n        RectTransform newBone = Instantiate(bonePrefab, boneContainer);\n        bonesList.Add(newBone);\n    }\n\n    for (int i = 0; i &lt; bonesList.Count; i++)\n    {\n        Image fromJoint = jointsList[bodyPartConnections[i].from];\n        Image toJoint = jointsList[bodyPartConnections[i].to];\n\n        // If both connected joints are active, display the bone\n        if (fromJoint.IsActive() && toJoint.IsActive())\n        {\n            RectTransform bone = bonesList[i];\n            Vector2 fromJointPos = bodyParts[bodyPartConnections[i].from].coordinates;\n            Vector2 toJointPos = bodyParts[bodyPartConnections[i].to].coordinates;\n            Vector2 direction = toJointPos - fromJointPos;\n            float distance = direction.magnitude;\n            float angle = Mathf.Atan2(direction.y, direction.x) * Mathf.Rad2Deg;\n\n            // Update bone size based on the distance between joints\n            bone.sizeDelta = new Vector2(distance, bone.sizeDelta.y);\n\n            // Calculate the bone position and update it\n            Vector2 bonePos = new Vector2((fromJointPos.x + toJointPos.x) / 2, (fromJointPos.y + toJointPos.y) / 2);\n            bone.anchoredPosition = ScreenToCanvasPoint(boneContainer, bonePos);\n\n            // Update bone rotation based on the angle between joints\n            bone.localEulerAngles = new Vector3(0, 0, angle);\n            bone.GetComponent&lt;Image&gt;().color = boneColor;\n            // Set the bone game object active\n            bone.gameObject.SetActive(true);\n        }\n        else\n        {\n            // Set the bone game object inactive if below the confidence threshold\n            bonesList[i].gameObject.SetActive(false);\n        }\n    }\n}\n\n\n\n\nAddCustomDefineSymbol.cs\nThis Editor script contains a class that adds a custom define symbol to the project. We can use this custom symbol to prevent code that relies on this package from executing unless the Human Pose 2D Toolkit package is present. The complete code is available on GitHub at the link below.\n\nAddCustomDefineSymbol.cs\n\nusing UnityEditor;\nusing UnityEngine;\n\nnamespace CJM.HumanPose2DToolkit\n{\n    public class DependencyDefineSymbolAdder\n    {\n        private const string CustomDefineSymbol = \"CJM_HUMAN_POSE_2D_TOOLKIT\";\n\n        [InitializeOnLoadMethod]\n        public static void AddCustomDefineSymbol()\n        {\n            // Get the currently selected build target group\n            var buildTargetGroup = EditorUserBuildSettings.selectedBuildTargetGroup;\n            // Retrieve the current scripting define symbols for the selected build target group\n            var defines = PlayerSettings.GetScriptingDefineSymbolsForGroup(buildTargetGroup);\n\n            // Check if the CustomDefineSymbol is already present in the defines string\n            if (!defines.Contains(CustomDefineSymbol))\n            {\n                // Append the CustomDefineSymbol to the defines string, separated by a semicolon\n                defines += $\";{CustomDefineSymbol}\";\n                // Set the updated defines string as the new scripting define symbols for the selected build target group\n                PlayerSettings.SetScriptingDefineSymbolsForGroup(buildTargetGroup, defines);\n                // Log a message in the Unity console to inform the user that the custom define symbol has been added\n                Debug.Log($\"Added custom define symbol '{CustomDefineSymbol}' to the project.\");\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#conclusion",
    "href": "posts/unity-human-pose-2d-toolkit-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Human Pose 2D Toolkit Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Unity Human Pose 2D Toolkit package. The package provides an easy-to-use and customizable solution to work with and visualize 2D human poses on a Unity canvas.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-human-pose-2d-toolkit\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package."
  },
  {
    "objectID": "posts/unity-media-display-walkthrough/index.html",
    "href": "posts/unity-media-display-walkthrough/index.html",
    "title": "Code Walkthrough: Unity Media Display Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-media-display-walkthrough/index.html#introduction",
    "href": "posts/unity-media-display-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity Media Display Package",
    "section": "Introduction",
    "text": "Introduction\nThe Unity Media Display package simplifies the creation and management of demo screens for displaying test images, videos, and webcam streams in Unity projects. As many of my tutorials use a demo screen, this package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Check out the demo video below to see this package in action.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-media-display-walkthrough/index.html#package-overview",
    "href": "posts/unity-media-display-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity Media Display Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains several C# scripts that work together to enable easy management of demo screens.\n\nBaseScreenManager.cs: This abstract class is the foundation for managing in-scene screen objects. It provides essential functionality and basic structure for derived classes, allowing you to create custom screen managers tailored to your project’s needs.\nMediaDisplayManager.cs: This script offers utility methods for setting up and managing a screen object in Unity. It handles initializing and updating screen textures, transformations, and webcam streams.\nTextureChangeEvent.cs: This script triggers when a material’s main texture changes, allowing for efficient texture updates and seamless transitions between different media sources.\nAddCustomDefineSymbol.cs: An Editor script that automatically adds a custom scripting define symbol to the project after the package installs.\nShaderIncludePostprocessor.cs: This build postprocessor script includes the shader used by the screen material in the build process and only runs in the Unity Editor."
  },
  {
    "objectID": "posts/unity-media-display-walkthrough/index.html#code-explanation",
    "href": "posts/unity-media-display-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity Media Display Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Unity Media Display package by examining the purpose and functionality of each C# script. We’ll discuss how these scripts work together to enable seamless management of demo screens for displaying test images, videos, and webcam streams.\n\nBaseScreenManager.cs\nThis script provides an abstract class for managing in-scene screen objects, allowing you to create custom screen managers tailored to your project’s needs. The complete code is available on GitHub at the link below.\n\nunity-media-display/Runtime/Scripts/BaseScreenManager.cs\n\nThe BaseScreenManager class includes several serialized fields for configuring scene components and settings, webcam settings, and toggle-key settings.\n// Scene components and settings\n[Header(\"Scene\")]\n[Tooltip(\"Screen object in the scene\")]\n[SerializeField] protected GameObject screenObject;\n[Tooltip(\"Camera object in the scene\")]\n[SerializeField] protected GameObject cameraObject;\n[Tooltip(\"A test texture to display on the screen\")]\n[SerializeField] protected Texture testTexture;\n[Tooltip(\"A framerate cap to reduce lag\")]\n[SerializeField] protected int maxFrameRate = 500;\n\n// Webcam settings\n[Header(\"Webcam\")]\n[Tooltip(\"Option to use webcam as input\")]\n[SerializeField] protected bool useWebcam = false;\n[Tooltip(\"Requested webcam dimensions\")]\n[SerializeField] protected Vector2Int webcamDims = new Vector2Int(1280, 720);\n[Tooltip(\"Requested webcam framerate\")]\n[SerializeField, Range(0, 60)] protected int webcamFrameRate = 60;\n\n// Toggle key settings\n[Header(\"Toggle Key\")]\n[Tooltip(\"Key to toggle between image and webcam feed\")]\n[SerializeField] protected KeyCode toggleKey = KeyCode.Space;\nIt also contains protected variables for managing the current texture, webcam devices, and the webcam texture.\nprotected Texture currentTexture;\nprotected WebCamDevice[] webcamDevices;\nprotected WebCamTexture webcamTexture;\nprotected string currentWebcam;\nLastly, the script has some constant values for working with webcam devices.\n// The value to multiply the webcam frame rate by to determine the application's target framerate.\nprotected const int WebcamFrameRateMultiplier = 4;\n// The value for the width of an uninitialized webcam device\nprotected const int UninitializedWebcamWidth = 16;\nThe script includes several methods:\n\nOnEnable\nSubscribes to the TextureChangeEvent for handling changes in the mainTexture of a Material.\n// Subscribe to the texture change event\nprotected virtual void OnEnable()\n{\n    TextureChangeEvent.OnMainTextureChanged += HandleMainTextureChanged;\n}\n\n\nInitialize\nSets the application’s target frame rate, configures the webcam devices, and initializes the current webcam if the useWebcam option is enabled.\n// Initializes the application's target frame rate and configures the webcam devices.\nprotected virtual void Initialize()\n{\n    // Limit the application's target frame rate to reduce lag.\n    Application.targetFrameRate = maxFrameRate;\n    // Get the list of available webcam devices.\n    webcamDevices = WebCamTexture.devices;\n    // If no webcam devices are available, disable the useWebcam option.\n    useWebcam = webcamDevices.Length &gt; 0 ? useWebcam : false;\n    // Set the current webcam device to the first available device, if any.\n    currentWebcam = useWebcam ? webcamDevices[0].name : \"\";\n}\n\n\nUpdateDisplay\nUpdates the display with the current texture, sets up the webcam if necessary, and starts a coroutine to update the screen texture asynchronously.\n// Updates the display with the current texture (either a test texture or the webcam feed).\nprotected virtual void UpdateDisplay()\n{\n    // Set up the webcam if necessary.\n    SetupWebcam();\n    // Update the current texture based on the useWebcam option.\n    UpdateCurrentTexture();\n    // Start a coroutine to asynchronously update the screen texture.\n    StartCoroutine(UpdateScreenTextureAsync());\n}\n\n\nSetupWebcam\nIf the useWebcam option is enabled and there are available webcam devices, this method initializes the webcam device and stops it if the option is disabled.\n// Sets up the webcam if the useWebcam option is enabled.\nprotected void SetupWebcam()\n{\n    // If there are no webcam devices, return immediately.\n    if (webcamDevices.Length == 0) return;\n\n    // If the useWebcam option is enabled, initialize the webcam.\n    if (useWebcam)\n    {\n        // Initialize the webcam and check if it started playing.\n        bool webcamPlaying = MediaDisplayManager.InitializeWebcam(ref webcamTexture, currentWebcam, webcamDims, webcamFrameRate);\n        // If the webcam failed to start playing, disable the useWebcam option.\n        useWebcam = webcamPlaying ? useWebcam : false;\n    }\n    // If the useWebcam option is disabled and the webcam texture is playing, stop the webcam.\n    else if (webcamTexture != null && webcamTexture.isPlaying)\n    {\n        webcamTexture.Stop();\n    }\n}\n\n\nUpdateCurrentTexture\nThis method updates the current texture and target frame rate based on the useWebcam option.\n// Updates the current texture and target frame rate based on the useWebcam option.\nprotected void UpdateCurrentTexture()\n{\n    // Set the current texture to the webcam texture if useWebcam is enabled, otherwise use the test texture.\n    currentTexture = useWebcam ? webcamTexture : testTexture;\n    // Set the target frame rate based on whether the webcam is being used or not.\n    Application.targetFrameRate = useWebcam ? webcamFrameRate * WebcamFrameRateMultiplier : maxFrameRate;\n}\n\n\nUpdateScreenTextureAsync\nA coroutine that waits until the webcamTexture is ready (if useWebcam is enabled) and updates the screen texture with the current texture (either the test texture or the webcam feed).\n// Coroutine to update the screen texture asynchronously.\nprotected IEnumerator UpdateScreenTextureAsync()\n{\n    // Wait until the webcamTexture is ready if useWebcam is enabled.\n    while (useWebcam && webcamTexture.isPlaying && webcamTexture.width &lt;= UninitializedWebcamWidth)\n    {\n        yield return null;\n    }\n\n    // Update the screen texture with the current texture (image or webcam feed).\n    MediaDisplayManager.UpdateScreenTexture(screenObject, currentTexture, cameraObject, useWebcam);\n}\n\n\nHandleMainTextureChanged\nThis method handles the TextureChangeEvent, updating the current texture and stopping the webcam if the new main texture differs from the webcam texture.\n// Handle the texture change event.\nprotected virtual void HandleMainTextureChanged(Material material)\n{\n    // Update the current texture.\n    currentTexture = material.mainTexture;\n    // If the new main texture is different from the webcam texture and the webcam is playing, stop the webcam.\n    if (webcamTexture && material.mainTexture != webcamTexture && webcamTexture.isPlaying)\n    {\n        useWebcam = false;\n        webcamTexture.Stop();\n    }\n}\n\n\nOnDisable\nLastly, the OnDisable() method unsubscribes from the TextureChangeEvent when the script is disabled.\n// Unsubscribe from the texture change event when the script is disabled.\nprotected virtual void OnDisable()\n{\n    TextureChangeEvent.OnMainTextureChanged -= HandleMainTextureChanged;\n}\n\n\n\n\nMediaDisplayManager.cs\nThe MediaDisplayManager script contains a static utility class that provides various methods to set up and manage media displays in Unity. This script sets screen textures, updates screen transforms, and initializes camera and webcam configurations. The complete code is available on GitHub at the link below.\n\nunity-media-display/Runtime/Scripts/MediaDisplayManager.cs\n\n\nSetScreenTexture\nThis method sets the texture for the screen object and raises a texture change event.\n/// &lt;summary&gt;\n/// Sets the texture for the screen object and raises a texture change event.\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenObject\"&gt;The GameObject to be used as the screen.&lt;/param&gt;\n/// &lt;param name=\"displayTexture\"&gt;The Texture to display on the screen.&lt;/param&gt;\npublic static void SetScreenTexture(GameObject screenObject, Texture displayTexture)\n{\n    // Attempt to get the MeshRenderer component from the screen object\n    if (screenObject.TryGetComponent&lt;MeshRenderer&gt;(out MeshRenderer meshRenderer))\n    {\n        // Create a new material with the Unlit/Texture shader and set its main texture\n        Material screenMaterial = new Material(Shader.Find(\"Unlit/Texture\"))\n        {\n            mainTexture = displayTexture\n        };\n\n        // Assign the material to the mesh renderer\n        meshRenderer.material = screenMaterial;\n\n        // Raise a texture change event to inform other scripts\n        TextureChangeEvent.RaiseMainTextureChangedEvent(meshRenderer.material);\n    }\n}\n\n\nUpdateScreenTransform\nThis method updates the screen object’s rotation, scale, and position based on the display texture and mirror settings.\n/// &lt;summary&gt;\n/// Updates the screen object's rotation, scale, and position based on the display texture and mirror settings.\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenObject\"&gt;The GameObject to be used as the screen.&lt;/param&gt;\n/// &lt;param name=\"displayTexture\"&gt;The Texture displayed on the screen.&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;Optional parameter to mirror the screen horizontally. Default is false.&lt;/param&gt;\npublic static void UpdateScreenTransform(GameObject screenObject, Texture displayTexture, bool mirrorScreen = false)\n{\n    // Get the width and height of the display texture\n    float width = displayTexture.width;\n    float height = displayTexture.height;\n\n    // Set the rotation, scale, and position of the screen object\n    screenObject.transform.rotation = Quaternion.Euler(0, mirrorScreen ? 180f : 0f, 0);\n    screenObject.transform.localScale = new Vector3(width, height, mirrorScreen ? -1f : 1f);\n    screenObject.transform.position = new Vector3(width / 2, height / 2, 1);\n}\n\n\nInitializeCamera\nThis method initializes the camera used for displaying the screen.\n/// &lt;summary&gt;\n/// Initializes the camera used for displaying the screen.\n/// &lt;/summary&gt;\n/// &lt;param name=\"cameraObject\"&gt;The GameObject with a Camera component to be used for displaying the screen.&lt;/param&gt;\n/// &lt;param name=\"screenDimensions\"&gt;The dimensions of the screen.&lt;/param&gt;\npublic static void InitializeCamera(GameObject cameraObject, Vector2Int screenDimensions)\n{\n    // Attempt to get the Camera component from the camera object\n    if (cameraObject.TryGetComponent&lt;Camera&gt;(out Camera camera))\n    {\n        // Set the position of the camera object\n        Vector3 position = new Vector3(screenDimensions.x / 2, screenDimensions.y / 2, -10f);\n        cameraObject.transform.position = position;\n\n        // Configure the camera for orthographic mode\n        camera.orthographic = true;\n\n        // Calculate the aspect ratios of the screen object and the camera's viewport\n        float screenAspectRatio = (float)screenDimensions.x / screenDimensions.y;\n        float cameraAspectRatio = (float)camera.pixelWidth / camera.pixelHeight;\n\n        // Set the camera's orthographic size based on the aspect ratios\n        if (screenAspectRatio &gt; cameraAspectRatio)\n        {\n            // Wider screen object\n            camera.orthographicSize = screenDimensions.x / 2 / camera.aspect;\n        }\n        else\n        {\n            // Taller screen object\n            camera.orthographicSize = screenDimensions.y / 2;\n        }\n    }\n}\n\n\nInitializeWebcam\nThis method initializes and plays a webcam stream with the specified settings.\n/// &lt;summary&gt;\n/// Initializes and plays a webcam stream with the specified settings.\n/// &lt;/summary&gt;\n/// &lt;param name=\"webcamTexture\"&gt;A reference to the WebCamTexture instance to be initialized and played.&lt;/param&gt;\n/// &lt;param name=\"deviceName\"&gt;The name of the webcam device to be used for streaming.&lt;/param&gt;\n/// &lt;param name=\"webcamDimensions\"&gt;The desired resolution of the webcam stream.&lt;/param&gt;\n/// &lt;param name=\"webcamFrameRate\"&gt;The desired frame rate of the webcam stream. Default is 60.&lt;/param&gt;\n/// &lt;returns&gt;Returns true if the webcam stream has started playing, false otherwise.&lt;/returns&gt;\npublic static bool InitializeWebcam(ref WebCamTexture webcamTexture, string deviceName, Vector2Int webcamDimensions, int webcamFrameRate = 60)\n{\n    // If the webcam texture is not null and it is playing, stop it\n    if (webcamTexture != null && webcamTexture.isPlaying)\n    {\n        webcamTexture.Stop();\n    }\n\n    // Create a new WebCamTexture instance with the specified settings\n    webcamTexture = new WebCamTexture(deviceName, webcamDimensions.x, webcamDimensions.y, webcamFrameRate);\n\n    // Start playing the webcam stream\n    webcamTexture.Play();\n\n    // Return true if the webcam stream has started playing, false otherwise\n    return webcamTexture.isPlaying;\n}\n\n\nUpdateScreenTexture\nThis method updates the texture, transform, and camera object.\n/// &lt;summary&gt;\n/// Updates the texture, transform, and camera of the screen object.\n/// &lt;/summary&gt;\n/// &lt;param name=\"screenObject\"&gt;The GameObject to be used as the screen.&lt;/param&gt;\n/// &lt;param name=\"displayTexture\"&gt;The Texture displayed on the screen.&lt;/param&gt;\n/// &lt;param name=\"cameraObject\"&gt;The GameObject with a Camera component to be used for displaying the screen.&lt;/param&gt;\n/// &lt;param name=\"mirrorScreen\"&gt;Optional parameter to mirror the screen horizontally. Default is false.&lt;/param&gt;\npublic static void UpdateScreenTexture(GameObject screenObject, Texture displayTexture, GameObject cameraObject, bool mirrorScreen = false)\n{\n    // Update the texture of the screen object\n    SetScreenTexture(screenObject, displayTexture);\n\n    // Update the transform of the screen object based on the new texture dimensions and mirror settings\n    UpdateScreenTransform(screenObject, displayTexture, mirrorScreen);\n\n    // Get the screen dimensions from the updated screen object\n    Vector2Int screenDimensions = new Vector2Int(displayTexture.width, displayTexture.height);\n\n    // Initialize the camera for displaying the screen\n    InitializeCamera(cameraObject, screenDimensions);\n}\n\n\n\n\nTextureChangeEvent.cs\nThe TextureChangeEvent class defines and manages a custom event that triggers when the main texture of a Material object changes. This event allows other scripts to be notified and respond accordingly when the displayed texture updates. The complete code is available on GitHub at the link below.\n\nunity-media-display/Runtime/Scripts/TextureChangeEvent.cs\n\n\nOnMainTextureChangedDelegate: This delegate defines the signature for the event handlers that get called when the main texture changes.\nOnMainTextureChanged: A static event of the OnMainTextureChangedDelegate type. It allows other scripts to subscribe and respond to texture change events.\nRaiseMainTextureChangedEvent: This static method gets called when the main texture of a Material object changes. The method invokes the OnMainTextureChanged event, calling any subscribed event handlers and passing the updated material as an argument.\n\npublic class TextureChangeEvent : MonoBehaviour\n{\n    // Define a delegate with the desired signature\n    public delegate void OnMainTextureChangedDelegate(Material material);\n\n    // Create a static event with the delegate type\n    public static event OnMainTextureChangedDelegate OnMainTextureChanged;\n\n    // Method to call when the mainTexture has been changed\n    public static void RaiseMainTextureChangedEvent(Material material)\n    {\n        OnMainTextureChanged?.Invoke(material);\n    }\n}\n\n\n\nAddCustomDefineSymbol.cs\nThis Editor script contains a class that adds a custom define symbol to the project. We can use this custom symbol to prevent code that relies on this package from executing unless the Unity Media Display package is present. The complete code is available on GitHub at the link below.\n\nunity-media-display/Editor/AddCustomDefineSymbol.cs\n\npublic class DependencyDefineSymbolAdder\n{\n    // Constant string representing the custom define symbol.\n    private const string CustomDefineSymbol = \"CJM_UNITY_MEDIA_DISPLAY\";\n\n    // This method is called on Unity editor load to ensure the custom define symbol is added.\n    [InitializeOnLoadMethod]\n    public static void AddCustomDefineSymbol()\n    {\n        // Get the currently selected build target group.\n        var buildTargetGroup = EditorUserBuildSettings.selectedBuildTargetGroup;\n\n        // Retrieve the current scripting define symbols for the selected build target group.\n        var defines = PlayerSettings.GetScriptingDefineSymbolsForGroup(buildTargetGroup);\n\n        // Check if the custom define symbol is already in the list of define symbols.\n        if (!defines.Contains(CustomDefineSymbol))\n        {\n            // Append the custom define symbol to the list of define symbols.\n            defines += $\";{CustomDefineSymbol}\";\n\n            // Set the updated list of define symbols for the selected build target group.\n            PlayerSettings.SetScriptingDefineSymbolsForGroup(buildTargetGroup, defines);\n\n            // Log the successful addition of the custom define symbol.\n            Debug.Log($\"Added custom define symbol '{CustomDefineSymbol}' to the project.\");\n        }\n    }\n}\n\n\n\nShaderIncludePostprocessor.cs\nThe ShaderIncludePostprocessor.cs script includes the shader used by the screen material in the build by adding it to the list of Always Included Shaders in the Graphics Settings. This script implements the IPostprocessBuildWithReport interface and runs only in the Unity Editor. The complete code is available on GitHub at the link below.\n\nunity-media-display/Runtime/Editor/ShaderIncludePostprocessor.cs\n\nThe ShaderIncludePostprocessor class includes a property to specify the order in which the build postprocessor gets called.\n/// &lt;summary&gt;\n/// The order in which this postprocessor is called.\n/// &lt;/summary&gt;\npublic int callbackOrder { get { return 0; } }\n\nOnPostprocessBuild\nThis method gets called after the build completes. It calls the AddShaderToAlwaysIncludedShaders method with the \"Unlit/Texture\" shader name, ensuring the built project includes it.\n/// &lt;summary&gt;\n/// Called after the build has been completed.\n/// &lt;/summary&gt;\n/// &lt;param name=\"report\"&gt;The BuildReport object containing information about the build.&lt;/param&gt;\npublic void OnPostprocessBuild(BuildReport report)\n{\n    AddShaderToAlwaysIncludedShaders(\"Unlit/Texture\");\n}\n\n\nAddShaderToAlwaysIncludedShaders\nThis method adds the specified shader to the Always Included Shaders list in the Graphics Settings.\n/// &lt;summary&gt;\n/// Adds the specified shader to the list of always included shaders in GraphicsSettings.\n/// &lt;/summary&gt;\n/// &lt;param name=\"shaderName\"&gt;The name of the shader to add to the always included shaders list.&lt;/param&gt;\nprivate static void AddShaderToAlwaysIncludedShaders(string shaderName)\n{\n    Shader shader = Shader.Find(shaderName);\n    if (shader == null)\n    {\n        Debug.LogWarning($\"Shader '{shaderName}' not found.\");\n        return;\n    }\n\n    // Load the GraphicsSettings asset\n    var graphicsSettings = AssetDatabase.LoadAssetAtPath&lt;GraphicsSettings&gt;(\"ProjectSettings/GraphicsSettings.asset\");\n    if (graphicsSettings == null)\n    {\n        Debug.LogWarning(\"GraphicsSettings.asset not found.\");\n        return;\n    }\n\n    // Create a serialized object from the GraphicsSettings asset\n    SerializedObject serializedGraphicsSettings = new SerializedObject(graphicsSettings);\n    // Find the \"m_AlwaysIncludedShaders\" property in the serialized object\n    SerializedProperty alwaysIncludedShadersProp = serializedGraphicsSettings.FindProperty(\"m_AlwaysIncludedShaders\");\n\n    // Iterate through the shaders in the Always Included Shaders list\n    for (int i = 0; i &lt; alwaysIncludedShadersProp.arraySize; i++)\n    {\n        SerializedProperty shaderProp = alwaysIncludedShadersProp.GetArrayElementAtIndex(i);\n        if (shaderProp.objectReferenceValue == shader)\n        {\n            Debug.Log($\"Shader '{shaderName}' is already in the Always Included Shaders list.\");\n            return;\n        }\n    }\n\n    // Add the specified shader to the Always Included Shaders list\n    alwaysIncludedShadersProp.InsertArrayElementAtIndex(alwaysIncludedShadersProp.arraySize);\n    SerializedProperty newShaderProp = alwaysIncludedShadersProp.GetArrayElementAtIndex(alwaysIncludedShadersProp.arraySize - 1);\n    newShaderProp.objectReferenceValue = shader;\n    serializedGraphicsSettings.ApplyModifiedProperties();\n\n    Debug.Log($\"Shader '{shaderName}' has been added to the Always Included Shaders list.\");\n}"
  },
  {
    "objectID": "posts/unity-media-display-walkthrough/index.html#conclusion",
    "href": "posts/unity-media-display-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity Media Display Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Unity Media Display package. The package helps simplify creating and managing demo screens for displaying test images, video, and webcam streams in Unity projects.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-media-display\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below, along with links for other demo projects that use the Unity Media Display package.\n\nUnity Media Display Demo: A simple demo project demonstrating how to use the unity-media-display and unity-cv-image-gallery packages in Unity.\nBarracuda Image Classification Demo: A simple Unity project demonstrating how to perform image classification with the barracuda-inference-image-classification package.\nBarracuda Inference PoseNet Demo: A simple Unity project demonstrating how to perform 2D human pose estimation with the barracuda-inference-posenet package.\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/unity-yolox-utils-walkthrough/index.html",
    "href": "posts/unity-yolox-utils-walkthrough/index.html",
    "title": "Code Walkthrough: Unity YOLOX Utilities Package",
    "section": "",
    "text": "Introduction\nPackage Overview\nCode Explanation\nConclusion"
  },
  {
    "objectID": "posts/unity-yolox-utils-walkthrough/index.html#introduction",
    "href": "posts/unity-yolox-utils-walkthrough/index.html#introduction",
    "title": "Code Walkthrough: Unity YOLOX Utilities Package",
    "section": "Introduction",
    "text": "Introduction\nThe Unity YOLOX Utilities package provides utility functions to work with YOLOX object detection models in Unity.\nI use YOLOX models in multiple tutorials. This package makes that shared functionality more modular and reusable, allowing me to streamline my tutorial content. Here is a demo video from a project that uses this package.\n\n\nVideo\n\n\nIn this post, I’ll walk through the package code, providing a solid understanding of its components and their roles."
  },
  {
    "objectID": "posts/unity-yolox-utils-walkthrough/index.html#package-overview",
    "href": "posts/unity-yolox-utils-walkthrough/index.html#package-overview",
    "title": "Code Walkthrough: Unity YOLOX Utilities Package",
    "section": "Package Overview",
    "text": "Package Overview\nThe package contains three C# scripts.\n\nYOLOXUtils.cs: This script provides a utility class for YOLOX-related operations.\nAddCustomDefineSymbol.cs: An Editor script that automatically adds a custom scripting define symbol to the project after the package installs.\nPackageInstaller.cs: An Editor utility script for automatically installing a list of dependency packages defined in a JSON file."
  },
  {
    "objectID": "posts/unity-yolox-utils-walkthrough/index.html#code-explanation",
    "href": "posts/unity-yolox-utils-walkthrough/index.html#code-explanation",
    "title": "Code Walkthrough: Unity YOLOX Utilities Package",
    "section": "Code Explanation",
    "text": "Code Explanation\nIn this section, we will delve deeper into the Unity YOLOX Utilities package by examining the purpose and functionality of each C# script.\n\nYOLOXUtils.cs\nThis script utilizes the Unity Bounding Box 2D Toolkit package and contains two main components: the GridCoordinateAndStride struct and the YOLOXUtility class.\nThe complete code is available on GitHub at the link below.\n\nYOLOXUtils.cs\n\n\nGridCoordinateAndStride struct\nThis struct represents the grid coordinates (x and y) and the stride of the grid cell.\n/// &lt;summary&gt;\n/// A struct for grid coordinates and stride information.\n/// &lt;/summary&gt;\npublic struct GridCoordinateAndStride\n{\n    public int xCoordinate;\n    public int yCoordinate;\n    public int stride;\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the GridCoordinateAndStride struct.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"xCoordinate\"&gt;The x-coordinate of the grid.&lt;/param&gt;\n    /// &lt;param name=\"yCoordinate\"&gt;The y-coordinate of the grid.&lt;/param&gt;\n    /// &lt;param name=\"stride\"&gt;The stride value for the grid.&lt;/param&gt;\n    public GridCoordinateAndStride(int xCoordinate, int yCoordinate, int stride)\n    {\n        this.xCoordinate = xCoordinate;\n        this.yCoordinate = yCoordinate;\n        this.stride = stride;\n    }\n}\n\n\nYOLOXUtility class\nThis static utility class provides methods for YOLOX-related operations.\n\nGenerateGridCoordinatesWithStrides\nThis method generates a list of GridCoordinateAndStride objects based on the input strides, grid height, and grid width.\n/// &lt;summary&gt;\n/// Generates a list of GridCoordinateAndStride objects based on input strides, height, and width.\n/// &lt;/summary&gt;\n/// &lt;param name=\"strides\"&gt;An array of stride values.&lt;/param&gt;\n/// &lt;param name=\"height\"&gt;The height of the grid.&lt;/param&gt;\n/// &lt;param name=\"width\"&gt;The width of the grid.&lt;/param&gt;\n/// &lt;returns&gt;A list of GridCoordinateAndStride objects.&lt;/returns&gt;\npublic static List&lt;GridCoordinateAndStride&gt; GenerateGridCoordinatesWithStrides(int[] strides, int height, int width)\n{\n    // Generate a list of GridCoordinateAndStride objects by iterating through possible grid positions and strides\n    return strides.SelectMany(stride =&gt; Enumerable.Range(0, height / stride)\n                                                   .SelectMany(g1 =&gt; Enumerable.Range(0, width / stride)\n                                                                                .Select(g0 =&gt; new GridCoordinateAndStride(g0, g1, stride)))).ToList();\n}\n\n\nGenerateBoundingBoxProposals\nThis method generates a list of bounding box proposals based on the model output, grid strides, and other parameters.\n/// &lt;summary&gt;\n/// Generates a list of bounding box proposals based on the model output, grid strides, and other parameters.\n/// &lt;/summary&gt;\n/// &lt;param name=\"modelOutput\"&gt;The output of the YOLOX model.&lt;/param&gt;\n/// &lt;param name=\"gridCoordsAndStrides\"&gt;A list of GridCoordinateAndStride objects.&lt;/param&gt;\n/// &lt;param name=\"numClasses\"&gt;The number of object classes.&lt;/param&gt;\n/// &lt;param name=\"numBBoxFields\"&gt;The number of bounding box fields.&lt;/param&gt;\n/// &lt;param name=\"confidenceThreshold\"&gt;The confidence threshold for filtering proposals.&lt;/param&gt;\n/// &lt;returns&gt;A list of BBox2D objects representing the generated proposals.&lt;/returns&gt;\npublic static List&lt;BBox2D&gt; GenerateBoundingBoxProposals(float[] modelOutput, List&lt;GridCoordinateAndStride&gt; gridCoordsAndStrides, int numClasses, int numBBoxFields, float confidenceThreshold)\n{\n    int proposalLength = numClasses + numBBoxFields;\n\n    // Process the model output to generate a list of BBox2D objects\n    return gridCoordsAndStrides.Select((grid, anchorIndex) =&gt;\n    {\n        int startIndex = anchorIndex * proposalLength;\n\n        // Calculate coordinates and dimensions of the bounding box\n        float centerX = (modelOutput[startIndex] + grid.xCoordinate) * grid.stride;\n        float centerY = (modelOutput[startIndex + 1] + grid.yCoordinate) * grid.stride;\n        float w = Mathf.Exp(modelOutput[startIndex + 2]) * grid.stride;\n        float h = Mathf.Exp(modelOutput[startIndex + 3]) * grid.stride;\n\n        // Initialize BBox2D object\n        BBox2D obj = new BBox2D(\n            centerX - w * 0.5f,\n            centerY - h * 0.5f,\n            w, h, 0, 0);\n\n        // Compute objectness and class probabilities for each bounding box\n        float box_objectness = modelOutput[startIndex + 4];\n\n        for (int classIndex = 0; classIndex &lt; numClasses; classIndex++)\n        {\n            float boxClassScore = modelOutput[startIndex + numBBoxFields + classIndex];\n            float boxProb = box_objectness * boxClassScore;\n\n            // Update the object with the highest probability and class label\n            if (boxProb &gt; obj.prob)\n            {\n                obj.index = classIndex;\n                obj.prob = boxProb;\n            }\n        }\n\n        return obj;\n    })\n    .Where(obj =&gt; obj.prob &gt; confidenceThreshold) // Filter by confidence threshold\n    .OrderByDescending(x =&gt; x.prob) // Sort by probability\n    .ToList();\n}\n\n\n\n\n\nAddCustomDefineSymbol.cs\nThis Editor script contains a class that adds a custom define symbol to the project. We can use this custom symbol to prevent code that relies on this package from executing unless the YOLOX Utilities package is present. The complete code is available on GitHub at the link below.\n\nAddCustomDefineSymbol.cs\n\nusing UnityEditor;\nusing UnityEngine;\n\nnamespace CJM.YOLOXUtils\n{\n    public class DependencyDefineSymbolAdder\n    {\n        private const string CustomDefineSymbol = \"CJM_YOLOX_UTILS\";\n\n        [InitializeOnLoadMethod]\n        public static void AddCustomDefineSymbol()\n        {\n            // Get the currently selected build target group\n            var buildTargetGroup = EditorUserBuildSettings.selectedBuildTargetGroup;\n            // Retrieve the current scripting define symbols for the selected build target group\n            var defines = PlayerSettings.GetScriptingDefineSymbolsForGroup(buildTargetGroup);\n\n            // Check if the CustomDefineSymbol is already present in the defines string\n            if (!defines.Contains(CustomDefineSymbol))\n            {\n                // Append the CustomDefineSymbol to the defines string, separated by a semicolon\n                defines += $\";{CustomDefineSymbol}\";\n                // Set the updated defines string as the new scripting define symbols for the selected build target group\n                PlayerSettings.SetScriptingDefineSymbolsForGroup(buildTargetGroup, defines);\n                // Log a message in the Unity console to inform the user that the custom define symbol has been added\n                Debug.Log($\"Added custom define symbol '{CustomDefineSymbol}' to the project.\");\n            }\n        }\n    }\n}\n\n\n\nPackageInstaller.cs\nIn this section, we will go through the PackageInstaller.cs script and explain how each part of the code works to install the required packages. The complete code is available on GitHub at the link below.\n\nPackageInstaller.cs\n\n\nSerializable Classes\nThe script defines two serializable classes to hold package data.\n// Serializable class to hold package data\n[System.Serializable]\npublic class PackageData\n{\n    public string packageName;\n    public string packageUrl;\n}\n\n// Serializable class to hold a list of PackageData objects\n[System.Serializable]\npublic class PackageList\n{\n    public List&lt;PackageData&gt; packages;\n}\nThese classes are for deserializing the JSON file containing the list of packages to install.\n\n\nPackageInstaller Class Variables\nThe PackageInstaller class contains several private static fields.\n// Stores the AddRequest object for the current package to install.\nprivate static AddRequest addRequest;\n// A list of PackageData objects to install.\nprivate static List&lt;PackageData&gt; packagesToInstall;\n// The index of the current package to install.\nprivate static int currentPackageIndex;\n\n// GUID of the JSON file containing the list of packages to install\nprivate const string PackagesJSONGUID = \"487301ab13cf457b9c2ed07a3ec5c004\";\n\n\nInstallDependencies\nThe InstallDependencies() method executes when Unity loads without action from the user. It reads the package JSON file and calls the InstallNextPackage() method to install the packages.\n// Method called on load to install packages from the JSON file\n[InitializeOnLoadMethod]\npublic static void InstallDependencies()\n{\n    // Read the package JSON file\n    packagesToInstall = ReadPackageJson().packages;\n    // Initialize the current package index\n    currentPackageIndex = 0;\n    // Start installing the packages\n    InstallNextPackage();\n}\n\n\nInstallNextPackage\nThis method installs the next package in the list.\n// Method to install the next package in the list\nprivate static void InstallNextPackage()\n{\n    // Iterate through package list\n    if (currentPackageIndex &lt; packagesToInstall.Count)\n    {\n        PackageData packageData = packagesToInstall[currentPackageIndex];\n\n        // Check if the package is already installed\n        if (!IsPackageInstalled(packageData.packageName))\n        {\n            // Attempt to install package\n            addRequest = Client.Add(packageData.packageUrl);\n            EditorApplication.update += PackageInstallationProgress;\n        }\n        else\n        {\n            // Increment the current package index\n            currentPackageIndex++;\n            // Recursively call InstallNextPackage\n            InstallNextPackage();\n        }\n    }\n}\n\n\nPackageInstallationProgress\nThis method monitors the progress of the package installation and logs whether it was successful. It then triggers the installation process for the next package in the list.\n// Method to monitor the progress of package installation\nprivate static void PackageInstallationProgress()\n{\n    if (addRequest.IsCompleted)\n    {\n        // Log whether the package installation was successful\n        if (addRequest.Status == StatusCode.Success)\n        {\n            UnityEngine.Debug.Log($\"Successfully installed: {addRequest.Result.packageId}\");\n        }\n        else if (addRequest.Status &gt;= StatusCode.Failure)\n        {\n            UnityEngine.Debug.LogError($\"Failed to install package: {addRequest.Error.message}\");\n        }\n\n        // Unregister the method from the EditorApplication.update \n        EditorApplication.update -= PackageInstallationProgress;\n        // Increment the current package index\n        currentPackageIndex++;\n        // Install the next package in the list\n        InstallNextPackage();\n    }\n}\n\n\nIsPackageInstalled\nThis method verifies whether a package has already been installed or not.\n// Method to check if a package is already installed\nprivate static bool IsPackageInstalled(string packageName)\n{\n    // List the installed packages\n    var listRequest = Client.List(true, false);\n    while (!listRequest.IsCompleted) { }\n\n    if (listRequest.Status == StatusCode.Success)\n    {\n        // Check if the package is already installed\n        return listRequest.Result.Any(package =&gt; package.name == packageName);\n    }\n    else\n    {\n        UnityEngine.Debug.LogError($\"Failed to list packages: {listRequest.Error.message}\");\n    }\n\n    return false;\n}\n\n\nReadPackageJson\nThis method reads the JSON file containing the list of packages to install and returns a PackageList object.\n// Method to read the JSON file and return a PackageList object\nprivate static PackageList ReadPackageJson()\n{\n    // Convert the PackagesJSONGUID to an asset path\n    string assetPath = AssetDatabase.GUIDToAssetPath(PackagesJSONGUID);\n    // Read the JSON file content as a string\n    string jsonString = File.ReadAllText(assetPath);\n    // Deserialize the JSON string into a PackageList object\n    return JsonUtility.FromJson&lt;PackageList&gt;(jsonString);\n}"
  },
  {
    "objectID": "posts/unity-yolox-utils-walkthrough/index.html#conclusion",
    "href": "posts/unity-yolox-utils-walkthrough/index.html#conclusion",
    "title": "Code Walkthrough: Unity YOLOX Utilities Package",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provided an in-depth walkthrough of the code for the Unity YOLOX Utilities package. The package provides utility functions to work with YOLOX object detection models in Unity.\nYou can continue to explore the package by going to its GitHub repository linked below, where you will also find instructions for installing it using the Unity Package Manager.\n\nGitHub Repository: unity-yolox-utils\n\nYou can find the code for the demo project shown in the video at the beginning of this post linked below.\n\nBarracuda Inference YOLOX Demo: A simple Unity project demonstrating how to perform object detection with the barracuda-inference-yolox package."
  },
  {
    "objectID": "posts/version-control-notes/index.html",
    "href": "posts/version-control-notes/index.html",
    "title": "Notes on Version Control",
    "section": "",
    "text": "Overview\nThe Perfect Commit\nBranching Strategies\nPull Requests\nMerge Conflicts\nMerge versus Rebase"
  },
  {
    "objectID": "posts/version-control-notes/index.html#overview",
    "href": "posts/version-control-notes/index.html#overview",
    "title": "Notes on Version Control",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Tobias Gunther’s video covering tools and concepts for mastering version control with git."
  },
  {
    "objectID": "posts/version-control-notes/index.html#the-perfect-commit",
    "href": "posts/version-control-notes/index.html#the-perfect-commit",
    "title": "Notes on Version Control",
    "section": "The Perfect Commit",
    "text": "The Perfect Commit\n\nAdd the right changes\n\nGoal is to make a commit that makes sens\nShould only contain changes from a single topic\nDon’t cram everything into single commit\n\nMakes it more difficult to what changes were made in retrospect\n\nUse Git staging area concept\n\nallows you to select specific files or parts of files for the next commit\n\ninclude specific file\n\ngit add &lt;filepath&gt;\n\ninclude part of a file\n\ngit add -p &lt;filepath&gt;\nsteps through every chunk of change in the file and asks whether to add it\n\n\nCompose a good commit message\n\nSubject: concise summary of what happened\n\nIf you are struggling to keep it short, you might have too many topics in the commit\nExample: “Add captcha for email signup”\n\nBody: more detailed explanation\n\nWhat is now different than before?\nWhat is the reason for the change?\nIs there anything to watch out for or is there anything particularly remarkable?\nAdd an empty line after the subject to let git know that you are now writing the body\nExample:\n\nEmail signups now require a captcha to be completed:\n\nsignup.php uses our captcha library\ninvalid signup attempts are now blocked"
  },
  {
    "objectID": "posts/version-control-notes/index.html#branching-strategies",
    "href": "posts/version-control-notes/index.html#branching-strategies",
    "title": "Notes on Version Control",
    "section": "Branching Strategies",
    "text": "Branching Strategies\n\nNeed to have a clear convention for how your team will work with branches\nGit allows you to create branches, but does not tell your how to use them\nYou need a written best practice of how work is ideally structured in your team to avoid mistakes and collisions\nYour branching workflow is highly dependent on your team, size of team, your project, and how you handle releases\nHaving a clear convention for branches helps onboard new team members\nConsider your project, release cycle, and team\nTake inspiration from existing branching strategies and create your own\n\n\nIntegrating Changes & Structuring Releases\n\nThe best approach depends on the needs and requirements of your team and project\nOption 1: Mainline development\n\nNew code is integrated quickly into the mainline, production code\nfew branches\nrelatively small commits\n\ncannot risk big and bloated commits when constantly integrating into production code\n\nhigh-quality testing and QA standards\n\nOption 2: State, Release, and Feature Branches\n\nbranches enhance structures and workflows\ndifferent types of branches fulfill different types of jobs\n\nfeatures and experiments are kept in different branches\nreleases can be planned and managed in their own different branches\ndifferent states in the development workflow can be represented by branches\n\n\n\n\n\nTypes of Branches\n\nLong running\n\nexist through the complete lifetime of the project\noften, they mirror “stages” in your dev life cycle\ncommon convention connected to long running branches\n\noften no direct commits\ncommits are only added through merges or rebases\nyou don’t want to add untested code to production\nmight want to release new code in batches\n\n\nShort-lived\n\nfor new features, bug fixes, refactorings, experiments\nwill be deleted after integration (merge/rebase)\ntypically based on a long running branch\n\n\n\n\nTwo Example Branching Strategies\n\nGitHub Flow\n\nvery simple, very learn\nonly one long running branch (”main”) + feature branches\n\nGitFlow\n\nmore structure, more rules\nlong-running: “main” + “develop”\n\nmain: a relfection of the current production state\ndevelop: feature branches are based on it and will be merged back into it\n\nalso the starting point for any new releases\nproduction ready versions are merged into main\n\n\nshort-lived: features, releases, hotfixes"
  },
  {
    "objectID": "posts/version-control-notes/index.html#pull-requests",
    "href": "posts/version-control-notes/index.html#pull-requests",
    "title": "Notes on Version Control",
    "section": "Pull Requests",
    "text": "Pull Requests\n\npull request are not a core git feature\nthey are provided by your git hosting platform\nwill work and look a bit different on different platforms\nwithout a pull request, you would jump right to merging your code\nthey are a way to communicate about code and review it\na way to contribute code to repositories you don’t have write access to\n\nfork: your personal copy of a repository\n\nYou can make changes in your forked version and open a pull request to include those changes into the original\n\n\npull requests are based on branches, not individual commits\npush branch to your remote fork\n\ngit push --set-upstream &lt;remote-branch&gt; &lt;local-branch&gt;\n\nrequest to include changes in original repository"
  },
  {
    "objectID": "posts/version-control-notes/index.html#merge-conflicts",
    "href": "posts/version-control-notes/index.html#merge-conflicts",
    "title": "Notes on Version Control",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts\n\nWhen they might occur\n\nwhen integrating commits from different sources\n\ngit merge\ngit rebase\ngit pull\ngit stash apply\ngit cherry-pick\n\nGit will mostly figure things out on its own\nCan happen when contradictory changes happen\n\nGit cannot decide which change it should keep\n\nGit will immediately tell you when a conflict has occurred\nExisting conflicts can be view with git status\n\nWhat they actually are\n\njust characters in a file\nGit marks the problematic areas in a file\n\nHow to solve them\n\nUndo a merge\n\ngit merge --abort\n\nUndo a rebase\n\ngit rebase --abort\n\nClean up files that have been marked by Git"
  },
  {
    "objectID": "posts/version-control-notes/index.html#merge-versus-rebase",
    "href": "posts/version-control-notes/index.html#merge-versus-rebase",
    "title": "Notes on Version Control",
    "section": "Merge versus Rebase",
    "text": "Merge versus Rebase\n\nMerge\n\nGit looks for three commits\n\nThe common ancestor commit\nThe last commit on branch A\nThe last commit on branch B\n\nGit creates a new commit that contains the differences between branch A and B\n\ncalled a merge commit\nautomatically generated\nwould need to look at the commit history for both branches\n\n\nRebase\n\nnot better or worse than merge, just different\nmakes history look like a straight line of commits without any branches\nrewrites commit history\nSteps\n\nGit removes all commits on branch A and temporarily saves them\nGit applies new commits from branch B\nNew commits from branch A are positioned on top of the commits from branch B\n\nDo not rebase commits that you have already pushed to a shared repository\nUse it for cleaning up your local commit history before merging it into a shared team branch\n\n\nReferences:\n\nGit for Professionals Tutorial - Tools & Concepts for Mastering Version Control with Git"
  },
  {
    "objectID": "posts/wave-function-collapse-for-3d-notes/index.html",
    "href": "posts/wave-function-collapse-for-3d-notes/index.html",
    "title": "Notes on WaveFunctionCollapse for 3D",
    "section": "",
    "text": "Overview\nSudoku\nWaveFunctionCollapse Algorithm"
  },
  {
    "objectID": "posts/wave-function-collapse-for-3d-notes/index.html#overview",
    "href": "posts/wave-function-collapse-for-3d-notes/index.html#overview",
    "title": "Notes on WaveFunctionCollapse for 3D",
    "section": "Overview",
    "text": "Overview\nHere are some notes I took while watching Martin Donald’s video providing an overview of the WaveFunctionCollapse algorithm as well as implementation considerations when using it with 3D modules."
  },
  {
    "objectID": "posts/wave-function-collapse-for-3d-notes/index.html#sudoku",
    "href": "posts/wave-function-collapse-for-3d-notes/index.html#sudoku",
    "title": "Notes on WaveFunctionCollapse for 3D",
    "section": "Sudoku",
    "text": "Sudoku\n\nSolitaire game played on a 9x9 grid\n\nThe 81 cells are grouped into 3x3 boxes\n\nGoal: Fill each space with a single number in range \\([1,9]\\)\nConstraints\n\nEvery row, column and box must contain all numbers 1-9\nNo number may ever appear twice in the same row, column or box\n\nEach cell in an empty board could potentially any number in the range \\([1,9]\\)\n\nEach cell is in a superposition (occupying all nine possible states at once)\n\nTypically, there are a few cells already with numbers\n\nTheir superpositions are already collapsed to a single possibility\nThis reduces the number of possible values for the other cells in the relevant row, column, and square\n\nExample: If a cell contains the number 5, no other cell in that row, column, or square can be a 5\n\nAfter reducing the possible values for each cell in the board based on the initial cell values, select a cell with the lowest number of remaining possible values (i.e. lowest entropy)\nRandomly select one of the remaining possible values for that cell\n\nThis again reduces the number of possible values for the other cells in the relevant row, column, and square\n\nEventually each cell will only contain one possible value"
  },
  {
    "objectID": "posts/wave-function-collapse-for-3d-notes/index.html#wavefunctioncollapse-algorithm",
    "href": "posts/wave-function-collapse-for-3d-notes/index.html#wavefunctioncollapse-algorithm",
    "title": "Notes on WaveFunctionCollapse for 3D",
    "section": "WaveFunctionCollapse Algorithm",
    "text": "WaveFunctionCollapse Algorithm\n\nA procedural solver that takes a procedural solver that takes a grid of cells, each occupying a super position containing all possible states for that cell\nEach tile (potential state) comes with its own set of adjacency rules\n\nOnly certain tiles can be above it\nOnly certain tiles can be below it\nOnly certain tiles can be left of it\nOnly certain tiles can be right of it\n\nThe algorithm looks for the cell with the lowest number of possible tiles\n\nIt will randomly pick a cell with the lowest number, if there is more than one\n\nThe algorithm then randomly picks one the possible tiles for that cell\nThe algorithm then updates the list of possible tiles for the surrounding cells based on the selected tile\nThe algorithm repeats until each cell contains only one possible tile\n\n\nAdjacency Constraints\n\nTells the algorithm which tiles or modules can slot together for each side (e.g. 4 for 2D square tiles and 6 for 3D cube modules)\nCan have the model determine the adjacency constraints by looking at a hand crafted output\n\nThe model breaks the example down into tiles and keeps track of which tiles are placed next to each other and considers those combinations valid\n\nCan use a socket system\n\nMark each side of a tile or module with an identifier (e.g. a number)\nTiles and modules can only slot together if the connecting sides both have the same identifier value\nCould increase granularity by having three identifiers for each side, so that the outer edges and middle of each side are considered rather than the side as a whole\n\n\n\n\n3D Modules\n\nEach cube module needs to have six lists of valid neighbors, one for each side of the cube.\nWhenever a cell is collapsed to one potential module, we remove modules from neighboring cells that are incompatible that are not present in the list of valid neighbors for the selected module\nTo create the lists of valid neighbors, label the side of each module with socket identifiers\n\nLoop over each module in the set of available modules\nStore the positions of each vertex the sits along the edges of each of the six boundaries\nStore and label the boundaries\nThis will build a dictionary of socket identifiers and side profiles\nAdd a tag to indicate symmetrical sockets\n\nCheck for symmetry by mirroring the vertex positions of each socket and check if it is still the same\nIf a socket is not symmetrical, store the mirrored and unmirrored versions as two different sockets,\nIndicate mirrored socket with a specific tag\n\nFor top and bottom boundaries, store four rotated versions of each socket, indicating the which rotation index with a tag\n\nVertical sockets will be considered valid if they have the same socket name and rotation index\n\n\n\nPrototypes\n\nThe metadata for modules\n\nThe associated 3D mesh object\nThe rotation value\nThe six lists of valid neighbors\n\nAllows us to get around needing to export four different meshes for each module\n\nCreate four prototypes that reference the same mesh with different rotation index\n\nStore prototypes in a JSON file and load in game engine as a dictionary\n\nBuilding Prototypes\n\nCreate four prototype entries for each module, one for each rotation\n\nWill start with the mesh name, rotation index, and list of socket identifiers\n\n\"proto_0\" = {\n    mesh: \"myMesh.obj\",\n    rotation: 0,\n    sockets: [\n        posX: \"0\",\n        negX: \"1s\",\n        posY: \"1s\"\n        negY: \"0f\"\n        posZ: \"-1\"\n        negZ: \"v0_0\"\n    ]\n}\nCompare each prototype six times, one for each side of the module cube\n\nFor each side check if the connecting socket identifiers are valid\n\nCheck for special conditions for symmetrical, asymmetrical, and vertrical\n\nAdd relevant prototypes to the neighbor list for the valid side for the current prototype\n\n\"proto_0\" = {\n    mesh: \"myMesh.obj\",\n    rotation: 0,\n    sockets: [\n        posX: \"0\"\n        negX: \"1s\"\n        posY: \"1s\"\n        negY: \"0f\"\n        posZ: \"-1\"\n        negZ: \"v0_0\"\n    ],\n    neighbor_list = [\n        posX: [..],\n        negX: [..],\n        posY: [..],\n        negY: [..],\n        posZ: [..],\n        negZ: [..]\n    ]\n}\nNote: A module might not contain vertices along every face\n\nStore the socket identifier as -1\nAdd a blank prototype with no mesh reference where all socket identifiers are set to -1 to represent empty space\n\"p-1\" = {\n    \"mesh_name: \"-1\",\n    \"mesh_rotation\": 0,\n    \"posX\": \"-1f\",\n    \"negX\": \"-1f\",\n    \"posY\": \"-1f\",\n    \"negY\": \"-1f\",\n    \"posZ\": \"-1f\",\n    \"negZ\": \"-1f\",\n    \"constrain_to\": \"-1\"\n    \"constrain_from\": \"-1\",\n    \"weight\": 1,\n    \"valid_neighbors\": [...]\n}\n\n\n\n\nWFC Demos on Itch:\nWave Function Collapse - Mixed Initiative Demo\nWave Function Collapse - Simple Tiled Model\nReferences:\n\nSuperpositions, Sudoku, the Wave Function Collapse algorithm.\nInfinite procedurally generated city with the Wave Function Collapse algorithm\nWave - by Oskar Stålberg\nWaveFunctionCollapse\nThe Wavefunction Collapse Algorithm explained very clearly\nUnity WaveFunctionCollapse"
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html",
    "href": "posts/weekly-recaps/recap-1/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "Introduction\nStyle Transfer Updates\nfastai to Unity\nBlender Python API\nUnreal Engine\nPose Estimation\nConclusion"
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#introduction",
    "href": "posts/weekly-recaps/recap-1/index.html#introduction",
    "title": "Weekly Recap",
    "section": "Introduction",
    "text": "Introduction\nIt’s been a couple weeks since I’ve posted anything. I’ve been going back and forth working on a few different projects and felt like I should keep better track of my activities. I want to start doing a weekly recap so that I have a record of what I spend time on. That was, after all, one of the reasons I started this blog in the first place. Ideally, I would do daily recaps. However, that doesn’t seem realistic for me just yet."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#style-transfer-updates",
    "href": "posts/weekly-recaps/recap-1/index.html#style-transfer-updates",
    "title": "Weekly Recap",
    "section": "Style Transfer Updates",
    "text": "Style Transfer Updates\nI spent some more time trying to improve the fast style model. I tried different style transfer methods in addition to customizing the fast_neural_style model. I still didn’t get satisfactory results for the digital lynx painting.\n\nI was starting to burn a lot of time on this so I decided to take a break. I’ll probably just write the end-to-end tutorial with the current level of quality."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#fastai-to-unity",
    "href": "posts/weekly-recaps/recap-1/index.html#fastai-to-unity",
    "title": "Weekly Recap",
    "section": "fastai to Unity",
    "text": "fastai to Unity\nI started developing a workflow for training models using the fastai library and implementing them in Unity. fastai is a highly modular deep learning library built on top of PyTorch. It’s main goals are to simplify the training of deep learning models and to be easily customizable.\n\n\n\n\n\nIt incorporates a lot of best practices as defaults and is often more convenient than raw PyTorch. The highly modular nature of the library is great when training models, but can be annoying when studying the source code. For example, it can take some digging to find the specific code for processing input and output for a model. Fortunately, the source code is easy to dig through.\nTo get started, I trained and exported models from their computer vision tutorial page. For classification tasks, fastai creates a mapping of the different classes and stores it in a list. This makes it easy to map the output of the model in Unity. It took some digging through the source code to determine how fastai handles key point estimation tasks. It scales the pixel coordinates for a key point from the input resolution down to [-1,1] by default. It then scales the output of the model back up to the source resolution.\nThe accuracy of the models in Unity was lower than I expected. The single label classification model did a decent job, but the accuracy for the multi-label classification model was lower than I’d like. The single key point estimation model did such a poor job that I assumed I didn’t implement the processing steps correctly. I rechecked multiple times and eventually determined that the sample dataset used in the tutorial wasn’t good enough. The model seemed to learn that the key point was generally in the same spot for every training image. The experience highlighted the need for high quality training datasets."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#blender-python-api",
    "href": "posts/weekly-recaps/recap-1/index.html#blender-python-api",
    "title": "Weekly Recap",
    "section": "Blender Python API",
    "text": "Blender Python API\nI started learning how to use the Python API for Blender. It basically lets you do everything in Blender through code rather than through the graphical interface. This can be extremely useful for automating repetitive tasks among other things. You can see a list of features from the API documentation below.\n\nCurrent Blender Python API features:\n\nEdit any data the user interface can (Scenes, Meshes, Particles etc.).\nModify user preferences, keymaps and themes.\nRun tools with own settings.\nCreate user interface elements such as menus, headers and panels.\nCreate new tools.\nCreate interactive tools.\nCreate new rendering engines that integrate with Blender.\nSubscribe to changes to data and it’s properties.\nDefine new settings in existing Blender data.\nDraw in the 3D Viewport using Python.\n\nI plan on using the Python API to try creating synthetic training datasets in Blender. I’ll start with simple image classification tasks, but the goal is to generate datasets for human pose estimation using customizable character models.\nI spent some time messing around in Blender to get acquainted with the API. I did not realize how memory intensive it is to have thousands of metallic objects in a single scene."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#unreal-engine",
    "href": "posts/weekly-recaps/recap-1/index.html#unreal-engine",
    "title": "Weekly Recap",
    "section": "Unreal Engine",
    "text": "Unreal Engine\nI’d like to learn how to use Unreal Engine this year. Unity has been great, but Epic Games has been able to dump a ton of money into Unreal Engine thanks to Fortnite. I’m curious what the difference in capabilities are especially with Unreal Engine 5 being released this year.\nI’d also like to see how it compares with Unity and Blender for generating synthetic datasets. The concept of synthetic datasets hasn’t been around long. However, one of the few companies that generates synthetic data for machine learning uses Unreal Engine for their workflow.\nI don’t think Unreal Engine has an equivalent of Unity’s Barracuda Inference library. However, it uses C++ for its programming language so I should be able to use the C++ interface for PyTorch. I believe I can still train models in Python like usual and then export it to C++. C++ is generally more of a hassle than the C# language used for Unity. The upside is that performance might be better and I wouldn’t be limited by the current supported operators for the Barracuda library. I’m a bit rusty with C++ so I’ll probably need to brush up on it as I go."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#pose-estimation",
    "href": "posts/weekly-recaps/recap-1/index.html#pose-estimation",
    "title": "Weekly Recap",
    "section": "Pose Estimation",
    "text": "Pose Estimation\nThere are some pretrained models for facial and hand key points that I’ve been meaning to get working in Unity. There were some compatibility issues/bugs with the Barracuda library that made me hold off on them. I’d like to try and knock those out soon. There have also been some cool 3D pose estimation models that have come out over the past year."
  },
  {
    "objectID": "posts/weekly-recaps/recap-1/index.html#conclusion",
    "href": "posts/weekly-recaps/recap-1/index.html#conclusion",
    "title": "Weekly Recap",
    "section": "Conclusion",
    "text": "Conclusion\nI’m not really sure what I’ll end up including in these recaps. I might start keeping lists of cool/useful things I come across during the week. I tend to forget about most things I bookmark. Perhaps posting them here will make them easier to keep track of.\nI should probably start working the end to end tutorial for in-game style transfer. The tutorial will cover training the model in PyTorch and exporting it to ONNX for use in Unity. This time I’ll be using the video stylization model in Unity. I’ll probably take a break from style transfer after that. It’s really easy to burn through time tuning style transfer models to get desirable images.\nAfter that, I’ll make an introductory tutorial for training models with the fastai library and using them in Unity. I’ll stick with the models trained in the fastai computer vision tutorials to ensure it’s easily reproducible."
  },
  {
    "objectID": "posts/weekly-recaps/recap-2/index.html",
    "href": "posts/weekly-recaps/recap-2/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "End-to-End Style Transfer Tutorial\nPreparing for Job Hunting\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-2/index.html#end-to-end-style-transfer-tutorial",
    "href": "posts/weekly-recaps/recap-2/index.html#end-to-end-style-transfer-tutorial",
    "title": "Weekly Recap",
    "section": "End-to-End Style Transfer Tutorial",
    "text": "End-to-End Style Transfer Tutorial\nI started planning out how I want to structure the tutorial. I want to provide a guide for how to tune the style transfer model to get more desirable results. To that end, I’ve been generating several examples of stylized images with different settings. For example, here’s the difference that adjusting the resolution of the style image can make.\n\n360p\n\n\n\n\n\n\n\n1440p\n\n\n\n\n\nI also got a bit sidetracked experimenting with customized model architectures. I still haven’t found a model that consistently generates superior results across different styles."
  },
  {
    "objectID": "posts/weekly-recaps/recap-2/index.html#preparing-for-job-hunting",
    "href": "posts/weekly-recaps/recap-2/index.html#preparing-for-job-hunting",
    "title": "Weekly Recap",
    "section": "Preparing for Job Hunting",
    "text": "Preparing for Job Hunting\nAssuming that the pandemic doesn’t mess things up too much, I should be finishing my degree this year. With that in mind, I’ve started learning about all the things that go into job hunting these days. I should probably start using social media platforms like LinkedIn and Twitter to “build my brand”. It might also be a good idea to invest more time in making this blog look better."
  },
  {
    "objectID": "posts/weekly-recaps/recap-2/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-2/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nLearning Apps, Games, and Simulations\nThis is something that I’ve become increasingly interested since I started learning how to use Unity last year. Most current forms of online education are merely poor imitations of the traditional in-person format. They don’t take advantage of the possibilities unlocked with virtual environments. Here’s some applications that do a better job of leveraging those possibilities.\n\nThe 50 best mastery-based apps for PreK-12th grades\nA list of high quality educational apps that help make learning fun and engaging.\n\n\niCircuit 3D\nA virtual workbench for designing and testing electronics projects. It’s currently iOS only but the developer is looking to bring it to other platforms in the future.\n\n\n\nMachine Learning\n\nThe unreasonable effectiveness of synthetic data with Daeil Kim\nI’m finally catching up on episodes of the Gradient Descent podcast. This episode is an interview with the CEO of one of the few companies the specializes in creating synthetic training data. Apparently, they use Unreal Engine in their workflow. I might need to make learning Unreal Engine a higher priority.\n\n\nPolyGen: An Autoregressive Generative Model of 3D Meshes\n\n\n\n\n\nI learned about this project from the above podcast episode. I haven’t tried it yet, but I’m interested in where these types of models are heading.\n\n\nspaCy v3.0\nThe open-source NLP library, spaCy, has release version 3.0. You can check out what’s new in the link above.\n\n\nLoss Landscape Explorer\nA project that aims to produce high quality visualizations of deep learning optimization processes.\n\n\n\nSelf-directed Learning\n\nThe Ultimate Hack for Learning In Public\nOne of the reasons, I started this blog was to practice learning in public. I’ve never enjoyed writing, but the process of making tutorials here has been undeniably beneficial. If you want to give learning in public a shot, this post provides some great advice for how to proceed.\n\n\nIllustrated Essays by Maggie Appleton\n\n\n\n\n\nI learned about this site from the previous link. I like this idea of using illustrated essays to help build intuition for abstract concepts. It’s like the learning apps above but requires less compute power. I haven’t done any in-depth explanations yet, but I might try this format when I do. I’m curious how difficult it would be to make an application that helps students build their own visual metaphors for abstract concepts.\n\n\nReadwise\nI’ve never been good about using spaced repetition tools, despite how effective they are for memorizing new topics. However, this seems like a good tool for remembering what you read.\n\n\n\nSoftware Development\n\npypi template\nA minimal template for creating a pypi package\nmake_pr\nMake a new pr from an existing cloned repository by simply doing make_pr.\n\n\nVisualizing sorting algorithms using Python, NumPy, and Matplotlib"
  },
  {
    "objectID": "posts/weekly-recaps/recap-3/index.html",
    "href": "posts/weekly-recaps/recap-3/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "End-to-End Style Transfer Tutorial\nC++\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-3/index.html#end-to-end-style-transfer-tutorial",
    "href": "posts/weekly-recaps/recap-3/index.html#end-to-end-style-transfer-tutorial",
    "title": "Weekly Recap",
    "section": "End-to-End Style Transfer Tutorial",
    "text": "End-to-End Style Transfer Tutorial\nI spent most of this week working on the in-game style transfer tutorial. I trimmed down the code for the two style transfer models that will be used. There was a decent amount of code that wasn’t needed for the video style transfer model. Hopefully that will make it a bit easier to browse through for anyone that wants to.\nI also have the accompanying jupyter notebook pretty much completed. All the code is there. I just need to add the text for the tutorial.\nI plan to use Google Colab for the tutorial so anyone following it won’t be required to get PyTorch running on their computer. I need to run the notebook on Colab to make sure the default compute resources are adequate.\nI’m probably going to use Unity’s FPS Microgame for this tutorial. It should take less time to download and open than the Kinematica demo. I decided to go with Unity’s video recorder for capturing the training images from the target video game. That should mean that Unity is the only applications that needs be installed to complete the tutorial. Well, that and a web browser. The rest of the tutorial should be pretty similar to the basic tutorial I did previously.\nI also came across an implementation of the model that Unity started with for their in-game style transfer project. It actually does such a good job applying styles that I was tempted to redo the jupyter notebook I’d just completed to use it instead.\n\n\n\n\n\nHowever, it looks like it takes considerably more time to fully train than the single style models that I’m currently using. Therefore, I’ll leave the tutorial as is and probably make a separate post for using this model in the future. I still need to convert the model and training code to PyTorch anyways."
  },
  {
    "objectID": "posts/weekly-recaps/recap-3/index.html#c",
    "href": "posts/weekly-recaps/recap-3/index.html#c",
    "title": "Weekly Recap",
    "section": "C++",
    "text": "C++\nI started to brush up on C++ in preparation for working with Unreal Engine and PyTorch’s C++ interface. I’m looking forward to finally using this language on something other than homework. C++ was actually the first programming language I learned but I never used it outside of class. Python was the first language I used to make stuff on my own time. It was a bit odd picking up Python initially since so much is handled automatically. I constantly felt like I was forgetting to do something.\nI’d also like to see what performance gains there might be from using Nvidia’s TensorRT library. It supports ONNX models so I should be able to directly compare performance vs Barracuda. Of course I’d lose the multi-platform compatibility that Unity provides with the Barracuda library. I didn’t fully appreciate that feature until I learned that someone was following my PoseNet tutorial on a MacBook."
  },
  {
    "objectID": "posts/weekly-recaps/recap-3/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-3/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nGPT-Neo\nA group of independent researchers is developing an open source, free-to-use version of OpenAI’s GPT-3 language model.\nAI-based Drawing Aids\nI learned about this drawing and painting software over the weekend called Clip Studio Paint. It has a lot of features to enhance and speed up creating art. One of the features that caught my eye was the ability to pose a virtual 3D human figure based on a reference image.\n\n\nMaking Data-Driven Essays\nA three-part series designed to help familiarize people with the tools to make visual, data-driven essays created by The Pudding.\n\nPart 1: Working with Data\n\n\nPart 2: Design\n\n\nPart 3: Storytelling"
  },
  {
    "objectID": "posts/weekly-recaps/recap-4/index.html",
    "href": "posts/weekly-recaps/recap-4/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "End-to-End Style Transfer Tutorial\nWorking With the Blender Python API\nLearning About Freelancing\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-4/index.html#end-to-end-style-transfer-tutorial",
    "href": "posts/weekly-recaps/recap-4/index.html#end-to-end-style-transfer-tutorial",
    "title": "Weekly Recap",
    "section": "End-to-End Style Transfer Tutorial",
    "text": "End-to-End Style Transfer Tutorial\nAs I noted in my latest experiment log, I decided to cut the video style transfer model from the end-to-end tutorial. Instead I’ve been modifying the fast_neural_style model in an effort to improve output quality and increase inference speed. If nothing else, the framerate will be higher than in my basic style transfer tutorial."
  },
  {
    "objectID": "posts/weekly-recaps/recap-4/index.html#working-with-the-blender-python-api",
    "href": "posts/weekly-recaps/recap-4/index.html#working-with-the-blender-python-api",
    "title": "Weekly Recap",
    "section": "Working With the Blender Python API",
    "text": "Working With the Blender Python API\nI tried following a couple basic motion graphic tutorials for Blender using only the Python API. I thought they would be fun little exercises to get more familiar with the API. It took quite a bit longer this way as I had to look up basically every single step. Fortunately, it was easy enough to track down answers online.\n\nShape-Key Motion Graphic Loading Icon in Blender 2.9 Eevee - Tutorial\n\n\n\n\n\n\n\nTriangle Motion Graphic Animation - Blender 2.9 Eevee Tutorial\n\n\n\n\n\nFiguring out how to add shader nodes to materials was a bit of a pain. I didn’t realize that the names for the nodes were different than what gets displayed in the UI. For example, an Emission node is actually called ShaderNodeEmission. Unlike basically everything else, the Python tooltips did not indicate that. I also learned that you need to manually link new shaders to the material. I’ll probably make separate posts going through the code after I clean it up."
  },
  {
    "objectID": "posts/weekly-recaps/recap-4/index.html#learning-about-freelancing",
    "href": "posts/weekly-recaps/recap-4/index.html#learning-about-freelancing",
    "title": "Weekly Recap",
    "section": "Learning About Freelancing",
    "text": "Learning About Freelancing\nI spent some time learning about what goes into running a freelance business. I watched the 3-hour guide provided by FreeCodeCamp (link). Turns out there is a lot that goes into running a freelance business if you want to do it properly. I recommend checking out the guide if your curious about freelancing. It’s targeted towards web developers, but a lot of the information if relevant to freelancing in general."
  },
  {
    "objectID": "posts/weekly-recaps/recap-4/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-4/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nML Learner’s Digest\nRadek Osmulski knows a thing or two about learning machine learning. He learned out how to program and do deep learning using online resources and is now the AI Research Engineering Lead at the Earth Species Project. He recently started a newsletter that focuses on how to get better at machine learning. I also recommend checking out his Hacker Noon post, Going From Not Being Able to Code to Deep Learning Hero.\n\n\nMetaHumans\nThe number of reasons for me to learn Unreal Engine keeps increasing. Unreal Engine showed off their new MetaHuman Creator tool that should allow users to create photorealistic humans in a fraction of the time. They claim it can be done in a matter of minutes. The digital humans will be fully rigged and include hair and clothing. This could be huge for generating synthetic datasets involving humans. I’m curious if there will be an API for interacting with the tool to help automate the process of generating random humans. The tool isn’t publicly available but there is currently a sample project available (link). It includes to complete digital humans generated using MetaHuman Creator.\nIt seems like the tool will be cloud-streamed only which is a bit unfortunate. I’d prefer to have the option to run it locally when possible. However, you apparently get the source data in the form of a Maya file that includes meshes, skeleton, facial rig, animation controls, and materials.\nYou can check out a walkthrough of the controls for the facial rig included with MetaHumans on the Unreal Engine YouTube channel (link).\n\n\nPytorch to fastai, Bridging the Gap\nA blog post by Zachary Mueller on understanding how to incorporate regular PyTorch code into a workflow with the fastai library.\n\n\nPython Concurrency: The Tricky Bits\nA blog post by Hamel Husain that explores threads, processes, and coroutines in Python. That reminds me, I need to see how Blender behaves with concurrency when using the Python API.\n\n\nJarvisCloud\nThis is a 1-click GPU cloud platform, I saw recommended on Twitter. I supports PyTorch, Fastai, and TensorFlow and the user just needs to select the desired specs. The interface is about as simple as it can get the prices seem reasonable."
  },
  {
    "objectID": "posts/weekly-recaps/recap-5/index.html",
    "href": "posts/weekly-recaps/recap-5/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "Introduction\nEnd-to-End Style Transfer Tutorial\nNext Project\nLinks of the Last Few Weeks"
  },
  {
    "objectID": "posts/weekly-recaps/recap-5/index.html#introduction",
    "href": "posts/weekly-recaps/recap-5/index.html#introduction",
    "title": "Weekly Recap",
    "section": "Introduction",
    "text": "Introduction\nWow it is really easy to get off schedule with these weekly recaps. I let over three weeks go by without posting one. Maybe I should try doing daily recaps just so I don’t fall out of the routine as easily. It’s too tempting to put off these weekly posts when I’m in the middle of an actual project."
  },
  {
    "objectID": "posts/weekly-recaps/recap-5/index.html#end-to-end-style-transfer-tutorial",
    "href": "posts/weekly-recaps/recap-5/index.html#end-to-end-style-transfer-tutorial",
    "title": "Weekly Recap",
    "section": "End-to-End Style Transfer Tutorial",
    "text": "End-to-End Style Transfer Tutorial\nI finally completed the end-to-end style transfer tutorial. I’m a bit irritated that I let myself dump so much time into experimenting with different style transfer models. I think it would have been better to invest that time explaining how others can conduct their own experiments. I might make a post examining how I could have better approached the project and avoided going so far over my time budget. Perhaps it would have helped to set an actual time budget.\nI’m in the process of making a follow up post explaining how to use the video style transfer model instead of the model used in the tutorial. I don’t plan on trying to make a super optimized version of the model like in the tutorial. My previous experiments didn’t yield great results when significantly reducing the size of the model. However, I dumped so much time into it that I figure I should at least make a post showing how someone else can tinker with it. I plan to have that finished next week."
  },
  {
    "objectID": "posts/weekly-recaps/recap-5/index.html#next-project",
    "href": "posts/weekly-recaps/recap-5/index.html#next-project",
    "title": "Weekly Recap",
    "section": "Next Project",
    "text": "Next Project\nBefore moving on to a completely new project, I plan to make some updates and additions to the PoseNet tutorial series. I realized that there were some unnecessary steps that could be removed for a small performance gain. I also realized I completely forgot to explain how to use the more efficient MobileNet version of the model instead of the ResNet50 version. I didn’t use the MobileNet version in the tutorial because it’s less accurate. However, a reader expressed interest in performing inference with the C# Burst backend. The smaller model would be a much better choice in that instance.\nAfter I update the PoseNet tutorial, I plan to work on getting a facial pose estimation model working with the Barracuda library. Hopefully, that will take less time than the style transfer tutorial. If the model proves incompatible with the current versions of Barracuda, I’ll look into making a C++ plugin so I can use the PyTorch C++ frontend instead."
  },
  {
    "objectID": "posts/weekly-recaps/recap-5/index.html#links-of-the-last-few-weeks",
    "href": "posts/weekly-recaps/recap-5/index.html#links-of-the-last-few-weeks",
    "title": "Weekly Recap",
    "section": "Links of the Last Few Weeks",
    "text": "Links of the Last Few Weeks\n\nDeep Learning\n\nPyTorch 1.8\nThe latest version of PyTorch has been released and comes with a lot of new updates including beta support for AMD GPUs.\n\n\nPyTorch3D: A library for deep learning with 3D data\n\n\n\n\n\nLast year, the FacebookAI research team introduced a deep learning library that aims to make working with 3D data a lot easier. I didn’t learn about it until I decided to click on a video YouTube had been recommending for weeks. The library supports batching of inputs with different sizes. This is helpful since cropping a 3D mesh isn’t as straight forward as cropping 2D images. The library also supports several common operations for 3D data as well as a differentiable rendering API. My mind instantly jumped to wondering what it would take to integrate this with Blender. I’ll plan to make time soon so that I can work through the available tutorials.\n\n\nNeural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans\n\n\n\n\n\n\n\nTraining models on Multiple GPUs using fastai\nThis blog post explores different approaches to training models using multiple GPUs.\n\n\nIntroducing Noisy Imagenette\nThis is a new version of the Imagenette library with noisy labels.\n\n\nTaming Transformers for High-Resolution Image Synthesis\nThis is an impressive new model that can be trained to perform a wide variety of image generation tasks. Check out the related Two Minute Papers video to see what it can do.\n\n\nStyleGAN Components\nThis blog post covers the key components of a StyleGAN model and uses them to build a basic version of the model.\n\n\nMultimodal Neurons in Artificial Neural Networks\nA recent post by OpenAI that explores how their Contrastive Language-Image Pre-Training (CLIP) model responds to the same concept whether presented literally, symbolically, or conceptually.\n\n\n\nProgramming\n\nAll of the python 3.9 standard library\nOrganized and hyperlinked index to every module, function, and class in the Python standard library\n\n\nDev Simulator: A Coding RPG\nThis is an upcoming RPG where players build a real full stack web app while playing through the storyline with 8-bit co-workers.\n\n\nGit scraping\nThis five minute video demonstrates how to schedule web scrapers using GitHub Actions.\n\n\nManim\nAn animation engine for creating precise animations in Python. This is a fork of the animation engine used in the videos on the 3Blue1Brown YouTube channel.\n\n\n\nUnreal Engine\n\nBuilding Simulation Applications with Unreal Engine\n\n\nHow to use Nvidia DLSS in Unreal Engine"
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html",
    "href": "posts/weekly-recaps/recap-6/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "Introduction\nEnd-to-End Style Transfer Tutorial Addendum\nCrop Image on GPU\nFlip Image with Compute Shaders\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html#introduction",
    "href": "posts/weekly-recaps/recap-6/index.html#introduction",
    "title": "Weekly Recap",
    "section": "Introduction",
    "text": "Introduction\nWell I haven’t got around to trying to make daily recaps. I actually didn’t even remember that I was thinking about that until I start writing this post. I made a note on my white board so I don’t forget tomorrow. On the plus side I managed to complete three tutorial posts over the weekend which I think is a record for me."
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html#end-to-end-style-transfer-tutorial-addendum",
    "href": "posts/weekly-recaps/recap-6/index.html#end-to-end-style-transfer-tutorial-addendum",
    "title": "Weekly Recap",
    "section": "End-to-End Style Transfer Tutorial Addendum",
    "text": "End-to-End Style Transfer Tutorial Addendum\nI completed the follow up post I mentioned in the last recap post covering how to use the video style transfer model that I had been experimenting instead of the model used in the main tutorial. I don’t recommend using the video model over the one from the tutorial but the instructions are there now for anyone interested."
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html#crop-image-on-gpu",
    "href": "posts/weekly-recaps/recap-6/index.html#crop-image-on-gpu",
    "title": "Weekly Recap",
    "section": "Crop Image on GPU",
    "text": "Crop Image on GPU\nI had worked out how to efficiently crop images in Unity with a GPU while working on my PoseNet tutorial. I didn’t end up using it as I decided to just squish the camera input into a square instead. However, this might not always be ideal so I decided to document how in case I need it in the future."
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html#flip-image-with-compute-shaders",
    "href": "posts/weekly-recaps/recap-6/index.html#flip-image-with-compute-shaders",
    "title": "Weekly Recap",
    "section": "Flip Image with Compute Shaders",
    "text": "Flip Image with Compute Shaders\nI also worked out how to flip images using Compute Shaders while working on my PoseNet tutorial. Again, I ended up not using it in the tutorial. I realized after the fact that I could just flip the output of the model instead. This is much less work than flipping the whole image.\nI had actually started to write a tutorial for this a while ago but ended up scrapping it. For some reason, Google still picked up the page and someone apparently tried to view it recently. I felt bad that there was nothing but a 404 page waiting for them so I ended up making this over weekend as well."
  },
  {
    "objectID": "posts/weekly-recaps/recap-6/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-6/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nMachine Learning\n\nKeypoint regression with heatmaps in fastai v2\nThis post covers how to implement heatmap regression to perform human pose estimation with the fastai library.\n\n\nTowards the end of deep learning and the beginning of AGI\nAn interesting post exploring how recent neuroscience research may point the way towards defeating adversarial examples and achieving a more resilient, consistent and flexible form of artificial intelligence.\n\n\nA Downloadable Version of Google’s C4 Dataset\nA colossal, cleaned version of Common Crawl’s web crawl corpus.\n\n\n“Adam” and friends\nThis blog post covers how to re-implement Stochastic Gradient Descent, Momentum, RMSprop, and the Adam optimizer from scratch.\n\n\nSelf Supervised Learning with Fastai\nThis GitHub repository contains implementations of popular SOTA self-supervised learning algorithms as Fastai Callbacks.\n\n\n\nUnity\n\nLearn to Write Unity Compute Shaders\nThis Udemy course teaches you how to write Unity Compute Shaders to create particle effects, flocking, fluid simulations, post processing image filters, and create a Physics engine.\n\n\n\nBlender\n\nVirtuCamera App\nThis app lets you use your iPhone/iPad to control the virtual camera in Blender, Autodesk, and Maya like an actual camera. You can explore the scene in real-time and record camera motions.\n\n\nBlender FREE Virtual Camera Setup on Android - Tutorial\nA free setup for an Android-based virtual camera.\n\n\n\nAstronomy\n\nMilky Way, 12 years, 1250 hours of exposures and 125 x 22 degrees of sky\nAn incredible gigapixel mosaic image of the Milky Way."
  },
  {
    "objectID": "posts/weekly-recaps/recap-7/index.html",
    "href": "posts/weekly-recaps/recap-7/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "Compute Shader Course\nUnreal Engine First Impressions\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-7/index.html#compute-shader-course",
    "href": "posts/weekly-recaps/recap-7/index.html#compute-shader-course",
    "title": "Weekly Recap",
    "section": "Compute Shader Course",
    "text": "Compute Shader Course\nI’ve been making my first pass through the compute shader course on Udemy. I’m just watching the videos the first time and I’ll go back through more slowly afterwards. The course moves a bit quickly if, like me, you are pretty new to compute shaders and haven’t worked with other types of shaders at all. I’m tempted to take their other course on shader development as other types of shaders get used throughout the course. While it’s going to take me some time to fully internalize all the concepts, I’m enjoying learning about all the things you can do with compute shaders."
  },
  {
    "objectID": "posts/weekly-recaps/recap-7/index.html#unreal-engine-first-impressions",
    "href": "posts/weekly-recaps/recap-7/index.html#unreal-engine-first-impressions",
    "title": "Weekly Recap",
    "section": "Unreal Engine First Impressions",
    "text": "Unreal Engine First Impressions\nI finally installed Unreal Engine and checked out one of the starter projects. The first big difference I noticed compared to Unity, is that new projects take a much longer time to open. It takes so long that I might look at replacing my quad-core CPU if I start using Unreal Engine on a regular basis. My computer also crashed the first time. Fortunately, existing projects seem to open pretty quickly. The interface and keyboard navigation feel pretty similar to Unity so it shouldn’t take too long to get comfortable with it. I have not had time to play around with Unreal Engine beyond that, but hopefully I will soon."
  },
  {
    "objectID": "posts/weekly-recaps/recap-7/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-7/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nBlender\n\nHuman Generator\nA new Blender add-on that aims to make it to generate and pose unique human models.\n\n\n\nUnity\n\nWaifu2xBarracuda\nAn implementation of the waifu2x super resolution model using the Barracuda library."
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html",
    "href": "posts/weekly-recaps/recap-8/index.html",
    "title": "Weekly Recap",
    "section": "",
    "text": "Introduction\nExperimenting with AsyncGPUReadback\nPixel Effects with JavaScript and HTML Canvas\nConclusion\nLinks of the Week"
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html#introduction",
    "href": "posts/weekly-recaps/recap-8/index.html#introduction",
    "title": "Weekly Recap",
    "section": "Introduction",
    "text": "Introduction\nWell I would like to say that I have had a super productive month. Unfortunately, I have hardly been able to work on anything at all. At least I’m done with finals now so, hopefully, I’ll be able to make up for it."
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html#experimenting-with-asyncgpureadback",
    "href": "posts/weekly-recaps/recap-8/index.html#experimenting-with-asyncgpureadback",
    "title": "Weekly Recap",
    "section": "Experimenting with AsyncGPUReadback",
    "text": "Experimenting with AsyncGPUReadback\nIn preparation for updating the PoseNet tutorial, I took another shot at getting around the performance bottleneck of reading the model output from the GPU. This time, I tried using the AsyncGPUReadback class. Keijiro Takahashi who works at Unity Technologies Japan had a small demo project on GitHub that I was able to learn from. As the name suggests, it allows you to read data from the GPU without blocking the main thread. I was successfully able to read model output back from the GPU using this method by first copying it to a RenderTexture. There was essentially no performance cost so I was pretty excited. Unfortunately, this approach does not seem feasible at least for the PoseNet model. The Barracuda library does not seem to support reading model output to a 3-dimensional RenderTexture. That means the model output needs to be sliced up and read separately. This approach does not scale well and the PoseNet model actually has a lot of outputs. I did go through the effort creating separate slices for each key point predicted by the model. However, there was actually a significant performance hit when reading from the GPU so many times. It would probably work fine for a model with only 2-dimensional outputs or only a few 3-dimensional ones. I’ll have to try it out with a simple image classifier."
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html#pixel-effects-with-javascript-and-html-canvas",
    "href": "posts/weekly-recaps/recap-8/index.html#pixel-effects-with-javascript-and-html-canvas",
    "title": "Weekly Recap",
    "section": "Pixel Effects with JavaScript and HTML Canvas",
    "text": "Pixel Effects with JavaScript and HTML Canvas\nI’ve started thinking of ways to personalize my blog a bit. That lead me to this tutorial on freeCodeCamp. The tutorial shows how to create pixel rain effects and interactive particle effects from scratch with vanilla JavaScript. I don’t have much experience with JavaScript so it was really fun learning about some of the more artistic applications. I recommend checking out the creator’s personal YouTube channel."
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html#conclusion",
    "href": "posts/weekly-recaps/recap-8/index.html#conclusion",
    "title": "Weekly Recap",
    "section": "Conclusion",
    "text": "Conclusion\nWell it’s been just over a month since my first “daily recap” and I have not posted any since. That first week turned out to be rather poor timing, but I don’t have any excuses for the rest of the month. I’m going to try starting fresh tomorrow."
  },
  {
    "objectID": "posts/weekly-recaps/recap-8/index.html#links-of-the-week",
    "href": "posts/weekly-recaps/recap-8/index.html#links-of-the-week",
    "title": "Weekly Recap",
    "section": "Links of the Week",
    "text": "Links of the Week\n\nUnity\n\nFaceMeshBarracuda\nWell it looks like Keijiro Takahashi beat me to implementing the Facemesh model with the Barracuda library. Looking at his recent projects on GitHub, he appears to be working on implementing every mode from the MediaPipe library. I guess I’ll have to put a bit more effort into my future tutorials on those so that they add something new.\n\n\n\nDeep Learning\n\nfastdebug\nA helpful library that is meant to make life easier when dealing with Pytorch and fastai errors.\n\n\n\nBlender\n\nHyperBole! The Hyperball Hates You\nA master applying their craft."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Christian Mills, an independent deep-learning consultant passionate about delivering cutting-edge AI solutions for businesses and creative projects. I have honed my expertise in end-to-end AI implementation, from data gathering and model selection to training, deployment, and integration with real-time applications made with Unity and Unreal Engine.\nMy goal is to empower clients with the knowledge and tools necessary to harness the full potential of deep learning in their projects. Whether you are a startup looking to streamline your processes or a game developer seeking to enhance your virtual worlds, I am here to help you navigate the ever-evolving landscape of artificial intelligence."
  },
  {
    "objectID": "about.html#free-tutorials",
    "href": "about.html#free-tutorials",
    "title": "About",
    "section": "Free Tutorials",
    "text": "Free Tutorials\nI firmly believe in sharing my knowledge and expertise with the community. To that end, I regularly publish free tutorials on my blog. These tutorials are valuable for those looking to expand their understanding of AI and its potential applications."
  },
  {
    "objectID": "about.html#why-choose-me",
    "href": "about.html#why-choose-me",
    "title": "About",
    "section": "Why Choose Me?",
    "text": "Why Choose Me?\n\nExpertise: Years of experience have equipped me with the knowledge and skills necessary to deliver exceptional results for your projects.\nCustomized Solutions: I work closely with clients to develop tailored AI solutions that meet their unique requirements and objectives.\nClear communication: I believe in maintaining open lines of communication with clients to ensure a smooth, collaborative process from start to finish.\nCommitment to Success: Your success is my priority, and I am dedicated to helping you achieve your goals with my services."
  },
  {
    "objectID": "about_backup.html",
    "href": "about_backup.html",
    "title": "About",
    "section": "",
    "text": "I am Christian Mills, an independent deep-learning consultant passionate about delivering cutting-edge AI solutions for businesses and creative projects. I have honed my expertise in end-to-end AI implementation, from data gathering and model selection to training, deployment, and integration with real-time applications made with Unity and Unreal Engine.\nMy goal is to empower clients with the knowledge and tools necessary to harness the full potential of deep learning in their projects. Whether you are a startup looking to streamline your processes or a game developer seeking to enhance your virtual worlds, I am here to help you navigate the ever-evolving landscape of artificial intelligence."
  },
  {
    "objectID": "about_backup.html#services",
    "href": "about_backup.html#services",
    "title": "About",
    "section": "Services",
    "text": "Services\nAs a deep learning consultant, I offer a range of services tailored to meet the specific needs of each client, including but not limited to the following:\n\nData Gathering and Preprocessing: Guiding you in obtaining and preparing high-quality datasets for optimal model training.\nModel Selection and Training: Help you choose the best deep learning models for your project, followed by comprehensive training for maximum accuracy and efficiency.\nDeployment and Integration: Assisting in deploying your trained model into real-time applications, including seamless integration with Unity and Unreal Engine game engines.\nOngoing Support and Optimization: Providing continuous support, ensuring your AI solutions remain up-to-date and optimized for peak performance."
  },
  {
    "objectID": "about_backup.html#free-tutorials",
    "href": "about_backup.html#free-tutorials",
    "title": "About",
    "section": "Free Tutorials",
    "text": "Free Tutorials\nI firmly believe in sharing my knowledge and expertise with the community. To that end, I regularly publish free tutorials on my blog. These tutorials are valuable for those looking to expand their understanding of AI and its potential applications."
  },
  {
    "objectID": "about_backup.html#why-choose-me",
    "href": "about_backup.html#why-choose-me",
    "title": "About",
    "section": "Why Choose Me?",
    "text": "Why Choose Me?\n\nExpertise: Years of experience have equipped me with the knowledge and skills necessary to deliver exceptional results for your projects.\nCustomized Solutions: I work closely with clients to develop tailored AI solutions that meet their unique requirements and objectives.\nClear communication: I believe in maintaining open lines of communication with clients to ensure a smooth, collaborative process from start to finish.\nCommitment to Success: Your success is my priority, and I am dedicated to helping you achieve your goals with my services."
  },
  {
    "objectID": "about_backup.html#get-in-touch",
    "href": "about_backup.html#get-in-touch",
    "title": "About",
    "section": "Get in Touch",
    "text": "Get in Touch\n\n\nReady to explore how you can leverage deep learning to transform your ideas into reality? Schedule a 30-minute consultation to:\n\nDiscuss your project’s vision, objectives, and challenges.\nExplore high-level deep-learning opportunities and roadmaps.\nDetermine if my services align with your needs.\n\nPlease note that this session provides an overview, not in-depth technical advice or solutions. If we decide to work together, we can create a tailored plan that addresses your project’s unique challenges and goals."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Christian Mills",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nSetting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows\n\n\n\n\n\n\n\nmamba\n\n\nconda\n\n\ngetting-started\n\n\ntutorial\n\n\n\n\nLearn how to install the Mamba package manager on Windows, set up a local Python environment, and install PyTorch and Jupyter for machine learning projects.\n\n\n\n\n\n\nMay 15, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nGetting Started with Google Colab\n\n\n\n\n\n\n\ngoogle-colab\n\n\ngetting-started\n\n\ntutorial\n\n\n\n\nLearn the fundamentals of Google Colab, a free cloud-based Jupyter Notebook environment, to write, run, and share Python code in your browser without any setup or installation.\n\n\n\n\n\n\nMay 14, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Barracuda Inference PoseNet Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Barracuda Inference PoseNet package, which extends the functionality of unity-barracuda-inference-base to perform 2D human pose estimation using PoseNet models.\n\n\n\n\n\n\nMay 7, 2023\n\n\n29 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Human Pose 2D Toolkit Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Human Pose 2D Toolkit package, which provides an easy-to-use and customizable solution to work with and visualize 2D human poses on a Unity canvas.\n\n\n\n\n\n\nMay 6, 2023\n\n\n19 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Barracuda Inference YOLOX Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Barracuda Inference YOLOX package, which extends the functionality of unity-barracuda-inference-base to perform object detection using YOLOX models.\n\n\n\n\n\n\nMay 6, 2023\n\n\n17 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Barracuda Inference Image Classification Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Barracuda Inference Image Classification package, which extends the functionality of unity-barracuda-inference-base to perform image classification using computer vision models.\n\n\n\n\n\n\nMay 6, 2023\n\n\n15 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity YOLOX Utilities Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity YOLOX Utilities package, which provides utility functions to work with YOLOX object detection models in Unity.\n\n\n\n\n\n\nMay 5, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Bounding Box 2D Toolkit Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Bounding Box 2D Toolkit package, which provides an easy-to-use and customizable solution to work with and visualize 2D bounding boxes on a Unity canvas.\n\n\n\n\n\n\nMay 5, 2023\n\n\n21 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Barracuda Inference Base Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Barracuda Inference Base package, which provides a foundation for performing inference with the Barracuda inference library.\n\n\n\n\n\n\nMay 5, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Deep Learning Image Preprocessor Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Deep Learning Image Preprocessor package, a utility for preparing image input to perform inference with deep learning models in Unity.\n\n\n\n\n\n\nMay 4, 2023\n\n\n20 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity CV Image Gallery Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity CV Image Gallery package, an interactive image gallery to facilitate testing computer vision applications in Unity projects.\n\n\n\n\n\n\nMay 3, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nCode Walkthrough: Unity Media Display Package\n\n\n\n\n\n\n\nunity\n\n\nwalkthrough\n\n\n\n\nWalk through the code for the Unity Media Display package, a tool for displaying test images, videos, and webcam streams in Unity projects.\n\n\n\n\n\n\nApr 25, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\nExploring the Impact of Different Image Augmentations on Hand Gesture Recognition\n\n\n\n\n\n\n\nminiai\n\n\n\n\nExplore how data augmentation can improve a computer vision model’s ability to generalize to new input using a hand gesture dataset.\n\n\n\n\n\n\nMar 9, 2023\n\n\n62 min\n\n\n\n\n\n\n\n\nTraining a Mask R-CNN Model on a Custom Dataset With IceVision\n\n\n\n\n\n\n\nicevision\n\n\nmask-rcnn\n\n\nobject-detection\n\n\ninstance-segmentation\n\n\ntutorial\n\n\n\n\nTrain a Mask R-CNN model on a custom dataset using the IceVision library and perform inference with ONNX Runtime.\n\n\n\n\n\n\nDec 2, 2022\n\n\n51 min\n\n\n\n\n\n\n\n\nTesting Intel’s Arc A770 GPU for Deep Learning Pt. 1\n\n\n\n\n\n\n\ndirectml\n\n\nopenvino\n\n\npytorch\n\n\nunity\n\n\n\n\nI tested inference performance with OpenVINO and DirectML on the A770 and attempted to train models using PyTorch-DirectML.\n\n\n\n\n\n\nOct 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nUsing TensorFlow.js for In-Browser Object Detection in Unity\n\n\n\n\n\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\nyolox\n\n\nobject-detection\n\n\ntutorial\n\n\n\n\nCreate a TensorFlow.js plugin for the Unity game engine to perform object detection with a YOLOX model.\n\n\n\n\n\n\nOct 16, 2022\n\n\n46 min\n\n\n\n\n\n\n\n\nUsing Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nBuild a Unity project as a WebGL application and host it using GitHub Pages.\n\n\n\n\n\n\nOct 6, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nUsing Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nCreate a TensorFlow.js plugin for the Unity game engine.\n\n\n\n\n\n\nOct 5, 2022\n\n\n35 min\n\n\n\n\n\n\n\n\nUsing Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nTrain a hand gesture classifier using fastai and export it to TensorFlow.js.\n\n\n\n\n\n\nOct 4, 2022\n\n\n37 min\n\n\n\n\n\n\n\n\nReal-Time Object Detection in Unity With ONNX and DirectML Pt. 2\n\n\n\n\n\n\n\nonnx\n\n\ndirectml\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nPerform object detection in a Unity project with ONNX Runtime and DirectML.\n\n\n\n\n\n\nAug 19, 2022\n\n\n39 min\n\n\n\n\n\n\n\n\nReal-Time Object Detection in Unity With ONNX and DirectML Pt. 1\n\n\n\n\n\n\n\nonnx\n\n\ndirectml\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using ONNX Runtime and DirectML.\n\n\n\n\n\n\nAug 18, 2022\n\n\n26 min\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nPerform object detection in a Unity project with OpenVINO.\n\n\n\n\n\n\nAug 10, 2022\n\n\n39 min\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\n\n\n\n\n\n\nAug 9, 2022\n\n\n24 min\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nTrain a YOLOX model using IceVision and export it to OpenVINO.\n\n\n\n\n\n\nAug 8, 2022\n\n\n77 min\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the Unity project from the fastai-to-unity tutorial to classify images with OpenVINO.\n\n\n\n\n\n\nJul 18, 2022\n\n\n20 min\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform image classification with OpenVINO.\n\n\n\n\n\n\nJul 17, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the training code from the fastai-to-unity tutorial to export the model to OpenVINO.\n\n\n\n\n\n\nJul 17, 2022\n\n\n42 min\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the Unity project from the fastai-to-unity tutorial to classify images with LibTorch.\n\n\n\n\n\n\nJun 28, 2022\n\n\n16 min\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform image classification using LibTorch.\n\n\n\n\n\n\nJun 28, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the training code from the fastai-to-unity tutorial to export the model to a TorchScript module.\n\n\n\n\n\n\nJun 28, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nBuild a Unity project as a WebGL application and host it using GitHub Pages.\n\n\n\n\n\n\nJun 8, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nClassify images in a Unity project with the Barracuda inference library.\n\n\n\n\n\n\nJun 7, 2022\n\n\n34 min\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nTrain an image classifier using the fastai library and export it to ONNX.\n\n\n\n\n\n\nJun 6, 2022\n\n\n37 min\n\n\n\n\n\n\n\n\nGetting Started With Deep Learning in Unity\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\n\n\nAn overview of Unity’s Barracuda inference library for executing deep learning models on user devices.\n\n\n\n\n\n\nMay 28, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on No ML Degree Book\n\n\n\n\n\n\n\nprofessional-growth\n\n\nnotes\n\n\n\n\nMy notes on Emil Wallner’s guide on how to land your first machine learning job without a degree.\n\n\n\n\n\n\nMay 27, 2022\n\n\n22 min\n\n\n\n\n\n\n\n\nNotes on The Hugging Face Deep RL Class Pt.2\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nreinforcement-learning\n\n\nnotes\n\n\n\n\nUnit 2 introduces monte carlo, temporal difference learning, and Q-learning.\n\n\n\n\n\n\nMay 26, 2022\n\n\n310 min\n\n\n\n\n\n\n\n\nNotes on The Hugging Face Deep RL Class Pt.1\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nreinforcement-learning\n\n\nnotes\n\n\n\n\nUnit 1 introduces the basic concepts for reinforcement learning and covers how to train an agent for the classic lunar lander environment.\n\n\n\n\n\n\nMay 5, 2022\n\n\n22 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 11\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 11 explores scaling up transformers, methods to make self-attention more efficient, and multimodel transformers.\n\n\n\n\n\n\nApr 26, 2022\n\n\n30 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 10\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 10 covers how to train a GPT-like model to generate Python source code from scratch.\n\n\n\n\n\n\nApr 25, 2022\n\n\n71 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 9\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.\n\n\n\n\n\n\nApr 22, 2022\n\n\n95 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 8\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 8 covers different methods to make transformer models more efficient in production.\n\n\n\n\n\n\nApr 14, 2022\n\n\n62 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 7\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 7 covers building a question-answering model that finds answers to questions in customer reviews.\n\n\n\n\n\n\nApr 12, 2022\n\n\n72 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 6\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 6 covers building an encoder-decoder model to condense dialogues between several people into a crisp summary.\n\n\n\n\n\n\nApr 10, 2022\n\n\n44 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 5\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 5 covers different methods for generating text with GPT-2.\n\n\n\n\n\n\nApr 8, 2022\n\n\n23 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 4\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.\n\n\n\n\n\n\nApr 7, 2022\n\n\n105 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 3\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 3 covers the Transformer architecture and different types of transformer models available on the Hugging Face Hub.\n\n\n\n\n\n\nApr 6, 2022\n\n\n30 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 2\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 2 covers training a model to classify emotions expressed in Twitter messages.\n\n\n\n\n\n\nApr 1, 2022\n\n\n79 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 1\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.\n\n\n\n\n\n\nMar 30, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 19\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 19 covers creating a fast.ai Learner from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n46 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 18\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 18 covers interpreting convolutional neural networks using class-activation maps.\n\n\n\n\n\n\nMar 29, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 17\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 17 covers building a neural network from the foundations.\n\n\n\n\n\n\nMar 29, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 16\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 16 covers momentum, RMSProp, Adam, decoupled weight decay, and fast.ai callbacks.\n\n\n\n\n\n\nMar 29, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 15\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 15 provides a deep dive into different application architectures in the fast.ai library.\n\n\n\n\n\n\nMar 29, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 14\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 14 covers building a ResNet from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n43 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 13\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 13 provides a deep dive into convolutional neural networks.\n\n\n\n\n\n\nMar 29, 2022\n\n\n59 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 12\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 12 covers building and training an LSTM from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 11\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 11 covers data munging with fast.ai’s mid-level data API.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 10\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 10 covers text preprocessing and training an RNN for text classification.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 09\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 9 provides a deep dive into tabular modeling.\n\n\n\n\n\n\nMar 29, 2022\n\n\n58 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 8\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 8 provides a deep dive into collaborative filtering.\n\n\n\n\n\n\nMar 28, 2022\n\n\n43 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 7\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 7 covers data normalization, progressive resizing, test-time augmentation, mixup, and label smoothing.\n\n\n\n\n\n\nMar 14, 2022\n\n\n26 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 6\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 6 covers multi-label classification and image regression.\n\n\n\n\n\n\nMar 14, 2022\n\n\n20 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 5\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 5 covers creating a custom DataBlock for an image classifier, pre-sizing, cross-entropy loss, model interpretation, picking learning rates, transfer learning, and discriminative learning rates.\n\n\n\n\n\n\nMar 14, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 4\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 4 covers broadcasting, stochastic gradient descent, the MNIST loss function, and the sigmoid activation functions.\n\n\n\n\n\n\nMar 14, 2022\n\n\n84 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 3\n\n\n\n\n\n\n\nai\n\n\nethics\n\n\nfastai\n\n\nnotes\n\n\n\n\nChapter 3 defines data ethics, introduces essential topics in data ethics, and explains how to identify and address ethical issues.\n\n\n\n\n\n\nMar 14, 2022\n\n\n16 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 2\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 2 covers how to train an image classification model using a custom dataset and turn it into an online application.\n\n\n\n\n\n\nMar 14, 2022\n\n\n23 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 1\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 1 covers the history of artificial neural networks, approaches for learning ML/DL, and using the fast.ai library for multiple deep learning tasks.\n\n\n\n\n\n\nMar 14, 2022\n\n\n35 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet WebGL Tutorial\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nModify the Barracuda PoseNet project to run in a browser using WebGL.\n\n\n\n\n\n\nMar 10, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\nNotes on the Streamlit API\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\nnumpy\n\n\npandas\n\n\nnotes\n\n\n\n\nMy notes and reference examples for working with the Streamlit API.\n\n\n\n\n\n\nJan 2, 2022\n\n\n15 min\n\n\n\n\n\n\n\n\nNotes on Creating Data Science Apps With Streamlit\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\nnumpy\n\n\npandas\n\n\nnotes\n\n\n\n\nMy notes from Chanin Nantasenamat’s video on creating data science web apps with Streamlit.\n\n\n\n\n\n\nJan 2, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\nNotes on Backtracking Problems in Python\n\n\n\n\n\n\n\npython\n\n\nnotes\n\n\n\n\nMy notes from Lynn Zheng’s video on solving LeetCode backtracking problems.\n\n\n\n\n\n\nDec 31, 2021\n\n\n28 min\n\n\n\n\n\n\n\n\nNotes on Version Control\n\n\n\n\n\n\n\ngit\n\n\nnotes\n\n\n\n\nMy notes from Tobias Gunther’s video covering tools and concepts for version control with git.\n\n\n\n\n\n\nDec 29, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on Practical Procedural Generation\n\n\n\n\n\n\n\nprocedural-generation\n\n\nnotes\n\n\n\n\nMy notes from Kate Compton’s talk on practical techniques for procedural generation.\n\n\n\n\n\n\nDec 29, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on How to Teach 5 Semesters of Game Design in 1 Class\n\n\n\n\n\n\n\neducation\n\n\ngame-dev\n\n\nnotes\n\n\n\n\nMy notes from Jason Wiser’s talk on how he teaches a class that covers 5 semesters worth of game design in a single semester.\n\n\n\n\n\n\nDec 29, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nNotes on Git Branches\n\n\n\n\n\n\n\ngit\n\n\nnotes\n\n\n\n\nMy notes from Tobias Gunther’s video covering git branches.\n\n\n\n\n\n\nDec 29, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nNotes on Advanced Git Tools\n\n\n\n\n\n\n\ngit\n\n\nnotes\n\n\n\n\nMy notes from Tobias Gunther’s video covering advanced git tools.\n\n\n\n\n\n\nDec 29, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on 1D Nonlinear Transformations for Games\n\n\n\n\n\n\n\ngame-dev\n\n\nnotes\n\n\n\n\nMy notes from Squirrel Eiserloh’s presentation on 1D nonlinear transformations for game development.\n\n\n\n\n\n\nDec 29, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nNotes on WaveFunctionCollapse for 3D\n\n\n\n\n\n\n\nprocedural-generation\n\n\nnotes\n\n\n\n\nMy notes from Martin Donald’s video on using the WaveFunctionCollapse algorithm for 3D modules.\n\n\n\n\n\n\nDec 28, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on Dungeon Generation via WaveFunctionCollapse\n\n\n\n\n\n\n\nprocedural-generation\n\n\nnotes\n\n\n\n\nMy notes from Brian Bucklew’s talk on procedurally generating dungeon levels using the WaveFunctionCollapse algorithm.\n\n\n\n\n\n\nDec 28, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nNotes on StyleGANv2\n\n\n\n\n\n\n\nai\n\n\nnotes\n\n\n\n\nMy notes on the overview of StyleGANv2 by Henry AI Labs.\n\n\n\n\n\n\nDec 16, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\nNotes on Making Money as a Freelance Developer\n\n\n\n\n\n\n\nprofessional-growth\n\n\nnotes\n\n\n\n\nMy notes on some tips for making money as a freelance developer by Luke Ciciliano on freeCodeCamp.\n\n\n\n\n\n\nDec 16, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nNotes on How To Speak\n\n\n\n\n\n\n\nprofessional-growth\n\n\nnotes\n\n\n\n\nMy notes for Patrick Winston’s course on how to speak.\n\n\n\n\n\n\nDec 16, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nNotes on the Procedural Tools Used to Make Far Cry 5\n\n\n\n\n\n\n\nprocedural-generation\n\n\ngame-dev\n\n\nnotes\n\n\n\n\nMy notes from Etienne Carrier’s overview of the procedural tools developed to create Far Cry 5.\n\n\n\n\n\n\nDec 9, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nNotes on Procedural Map Generation Techniques\n\n\n\n\n\n\n\ngame-dev\n\n\nprocedural-generation\n\n\nnotes\n\n\n\n\nMy notes on Herbert Wolverson’s talk on procedural map generation techniques from the 2020 virtual Roguelike Celebration.\n\n\n\n\n\n\nDec 9, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on Machine Learning and Level Generation\n\n\n\n\n\n\n\nai\n\n\ngame-dev\n\n\nnotes\n\n\n\n\nMy notes from Ben Berman’s 2017 talk on machine learning and level generation.\n\n\n\n\n\n\nDec 9, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nNotes on Learning Generative Models of 3D Structures\n\n\n\n\n\n\n\nai\n\n\nnotes\n\n\n\n\nMy notes from an overview of the Learning Generative Models of 3D Structures paper.\n\n\n\n\n\n\nDec 9, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on How A.I. Will Change the 3D Industry\n\n\n\n\n\n\n\nai\n\n\n3d\n\n\nprocedural-generation\n\n\nnotes\n\n\n\n\nMy notes from Andrew Price’s talk at Blender Conference 2018 on how A.I. will change the 3D industry.\n\n\n\n\n\n\nDec 9, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on Customizing Your GitHub Profile\n\n\n\n\n\n\n\ngithub\n\n\nnotes\n\n\n\n\nMy notes from learning how to customize my GitHub profile.\n\n\n\n\n\n\nDec 9, 2021\n\n\n13 min\n\n\n\n\n\n\n\n\nOpenVINO Object Detection in the Unity Editor (Outdated)\n\n\n\n\n\n\n\nopenvino\n\n\nobject-detection\n\n\nyolox\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post covers how to modify the existing YOLOX project so that the plugin can be using in the Unity Editor.\n\n\n\n\n\n\nDec 6, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nOpenVINO Object Detection in Unity Using the In-Game Camera\n\n\n\n\n\n\n\nopenvino\n\n\nobject-detection\n\n\nyolox\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post covers the changes needed to use the in-game camera as input for the YOLOX model.\n\n\n\n\n\n\nDec 6, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nOpenVINO Object Detection for Unity Tutorial Pt.3 (Outdated)\n\n\n\n\n\n\n\nopenvino\n\n\nobject-detection\n\n\nyolox\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post demonstrates how to create a Unity project to access the DLL as a plugin.\n\n\n\n\n\n\nOct 6, 2021\n\n\n59 min\n\n\n\n\n\n\n\n\nOpenVINO Object Detection for Unity Tutorial Pt.2 (Outdated)\n\n\n\n\n\n\n\nopenvino\n\n\nobject-detection\n\n\nyolox\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post walks through the steps needed to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pretrained deep learning model.\n\n\n\n\n\n\nOct 6, 2021\n\n\n27 min\n\n\n\n\n\n\n\n\nOpenVINO Object Detection for Unity Tutorial Pt.1 (Outdated)\n\n\n\n\n\n\n\nopenvino\n\n\nobject-detection\n\n\nyolox\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post covers the prerequisite software, pretrained object detection models, and test videos used in the tutorial.\n\n\n\n\n\n\nOct 6, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 7\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nCreate pose skeletons and manipulate them using output from a PoseNet model.\n\n\n\n\n\n\nJul 30, 2021\n\n\n21 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 6\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the post-processing steps for multi-pose estimation with PoseNet.\n\n\n\n\n\n\nJul 30, 2021\n\n\n21 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 5\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the post-processing steps for single pose estimation with PoseNet.\n\n\n\n\n\n\nJul 29, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 4\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nLoad, modify, and execute the PoseNet models.\n\n\n\n\n\n\nJul 28, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 3\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the preprocessing steps for the MobileNet and ResNet PoseNet models.\n\n\n\n\n\n\nJul 27, 2021\n\n\n19 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 2\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nSet up a video player and webcam in Unity.\n\n\n\n\n\n\nJul 20, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 1\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nThis tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library.\n\n\n\n\n\n\nJul 20, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nOpenVINO Plugin for Unity Tutorial Pt.3\n\n\n\n\n\n\n\nopenvino\n\n\nstyle-transfer\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post demonstrates how to access the DLL as a plugin inside a Unity project.\n\n\n\n\n\n\nJul 6, 2021\n\n\n47 min\n\n\n\n\n\n\n\n\nOpenVINO Plugin for Unity Tutorial Pt.1\n\n\n\n\n\n\n\nopenvino\n\n\nstyle-transfer\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post covers the prerequisite software that need to be installed and how to convert a pretrained model from ONNX format to the OpenVINO Intermediate Representation format.\n\n\n\n\n\n\nJul 6, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nOpenVINO Plugin for Unity Tutorial Pt. 2\n\n\n\n\n\n\n\nopenvino\n\n\nstyle-transfer\n\n\ntutorial\n\n\nunity\n\n\n\n\nThis post walks through the steps needed to create a Dynamic link library (DLL) in Visual Studio to perform inference with the pretrained deep learning model.\n\n\n\n\n\n\nJul 6, 2021\n\n\n20 min\n\n\n\n\n\n\n\n\nTargeted In-Game Style Transfer Tutorial\n\n\n\n\n\n\n\nstyle-transfer\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to select which game objects get stylized in the Unity scene.\n\n\n\n\n\n\nJul 4, 2021\n\n\n15 min\n\n\n\n\n\n\n\n\nDaily Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I worked on today.\n\n\n\n\n\n\nApr 29, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\nDaily Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I worked on today.\n\n\n\n\n\n\nApr 28, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\nDaily Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I worked on today.\n\n\n\n\n\n\nApr 27, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nDaily Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I worked on today.\n\n\n\n\n\n\nApr 26, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past week.\n\n\n\n\n\n\nApr 25, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past week.\n\n\n\n\n\n\nApr 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate a Triangle Motion Graphic with the Blender Python API\n\n\n\n\n\n\n\nblender\n\n\npython\n\n\ntutorial\n\n\n\n\nThis post covers how to create a simple triangle motion graphic in Blender using the Python API.\n\n\n\n\n\n\nMar 27, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nCreate a Shape Key Motion Graphic with the Blender Python API\n\n\n\n\n\n\n\nblender\n\n\npython\n\n\ntutorial\n\n\n\n\nThis post covers how to create a simple shape-key motion graphic in Blender using the Python API.\n\n\n\n\n\n\nMar 24, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nDaily Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I worked on today.\n\n\n\n\n\n\nMar 23, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past week.\n\n\n\n\n\n\nMar 22, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nHow to Flip an Image With a Compute Shader\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to flip an image with a compute shader in Unity.\n\n\n\n\n\n\nMar 21, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nHow to Crop Images With a GPU in Unity\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to efficiently crop images in Unity with a GPU.\n\n\n\n\n\n\nMar 20, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Addendum\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to use a different style transfer model that is specialized for video.\n\n\n\n\n\n\nMar 20, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past few weeks.\n\n\n\n\n\n\nMar 12, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.3\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how implement the style transfer model in Unity with the Barracuda library.\n\n\n\n\n\n\nMar 3, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.2\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to train an artistic style transfer model with PyTorch in Google Colab.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n32 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to use the Unity Recorder tool to generate additional training data for our style transfer model.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.1\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis tutorial series covers how to train your own style transfer model with PyTorch and implement it in Unity using the Barracuda library.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nRevising the style transfer tutorial, working with the Blender Python API, and learning about freelancing.\n\n\n\n\n\n\nFeb 17, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.6\n\n\n\n\n\n\n\nstyle-transfer\n\n\nlog\n\n\n\n\nTesting out an arbitrary style transfer model and a change in plans for the end-to-end style transfer tutorial.\n\n\n\n\n\n\nFeb 17, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past week.\n\n\n\n\n\n\nFeb 8, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past week.\n\n\n\n\n\n\nFeb 1, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nWeekly Recap\n\n\n\n\n\n\n\nrecap\n\n\n\n\nA summary of what I’ve been working on for the past couple weeks.\n\n\n\n\n\n\nJan 24, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.5\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nI got the video stylization model to work properly in Unity and found some weaknesses in the fast neural style model.\n\n\n\n\n\n\nJan 12, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.4\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nExamining results from my initial attempts to optimize the few-shot video stylization model.\n\n\n\n\n\n\nJan 9, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.3\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nExamining results from my initial attempts to optimize the fast neural style transfer model.\n\n\n\n\n\n\nJan 6, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nUsing PyTorch with CUDA on WSL2\n\n\n\n\n\n\n\nlog\n\n\ntutorial\n\n\npytorch\n\n\nwsl2\n\n\n\n\nThis post covers my experience getting PyTorch to run with CUDA on WSL2.\n\n\n\n\n\n\nDec 31, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.2\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nExamining results from longer training sessions and Unity’s implementation in Kinematica demo.\n\n\n\n\n\n\nDec 20, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer Experiments Pt.1\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nTrying out custom datasets, a new model, and Unity’s style transfer project.\n\n\n\n\n\n\nDec 18, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBasic In-Game Style Transfer Tutorial (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post provides a basic method for performing in-game style transfer.\n\n\n\n\n\n\nDec 8, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 8 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to handle video input with different aspect ratios.\n\n\n\n\n\n\nNov 20, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 7 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to use a webcam feed as input for the PoseNet model.\n\n\n\n\n\n\nNov 15, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 6 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to create a pose skeleton by drawing lines between key points.\n\n\n\n\n\n\nNov 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 5 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to map the key point locations to GameObjects.\n\n\n\n\n\n\nNov 13, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 4 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to process the output of the PoseNet model.\n\n\n\n\n\n\nNov 12, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 3 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to perform inference with the PoseNet model.\n\n\n\n\n\n\nNov 6, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 2.5 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to view preprocessed images during runtime.\n\n\n\n\n\n\nNov 5, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 2 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to implement the preprocessing steps for the PoseNet model.\n\n\n\n\n\n\nNov 4, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial Pt. 1 (Outdated)\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis first post covers how to set up a video player in Unity. We’ll be using the video player to check the accuracy of the PoseNet model.\n\n\n\n\n\n\nOct 25, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nHow to Convert TensorFlow Models to ONNX with tf2onnx\n\n\n\n\n\n\n\ntensorflow\n\n\nonnx\n\n\ntutorial\n\n\n\n\nThis post covers how to use tf2onnx to convert a TensorFlow SavedModel to ONNX.\n\n\n\n\n\n\nOct 21, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nIn-Game Style Transfer\n\n\n\n\n\n\n\nunity\n\n\nstyle-transfer\n\n\n\n\nSome optimization is still required.\n\n\n\n\n\n\nOct 19, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBarracuda Pose Estimation Project Log Pt. 5\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nBack to it.\n\n\n\n\n\n\nOct 19, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBarracuda Pose Estimation Project Log Pt. 4\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nI’m an idiot.\n\n\n\n\n\n\nSep 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nBarracuda Pose Estimation Project Log Pt. 3\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nI might be doing something wrong.\n\n\n\n\n\n\nSep 22, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBarracuda Pose Estimation Project Log Pt. 2\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nTrying to remove bottlenecks.\n\n\n\n\n\n\nSep 21, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBarracuda Pose Estimation Project Log Pt. 1\n\n\n\n\n\n\n\nunity\n\n\nlog\n\n\n\n\nThe journey so far.\n\n\n\n\n\n\nSep 16, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nHow to Convert a TensorFlow.js Graph Model to a TensorFlow SavedModel\n\n\n\n\n\n\n\ntensorflow\n\n\ntutorial\n\n\n\n\nA simple example of how to convert a TensorFlow.js graph model to a TensorFlow SavedModel.\n\n\n\n\n\n\nSep 15, 2020\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/fastai-book-notes.html",
    "href": "series/notes/fastai-book-notes.html",
    "title": "Deep Learning for Coders with fastai & PyTorch",
    "section": "",
    "text": "Notes on fastai Book Ch. 1\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 1 covers the history of artificial neural networks, approaches for learning ML/DL, and using the fast.ai library for multiple deep learning tasks.\n\n\n\n\n\n\nMar 14, 2022\n\n\n35 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 2\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 2 covers how to train an image classification model using a custom dataset and turn it into an online application.\n\n\n\n\n\n\nMar 14, 2022\n\n\n23 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 3\n\n\n\n\n\n\n\nai\n\n\nethics\n\n\nfastai\n\n\nnotes\n\n\n\n\nChapter 3 defines data ethics, introduces essential topics in data ethics, and explains how to identify and address ethical issues.\n\n\n\n\n\n\nMar 14, 2022\n\n\n16 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 4\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 4 covers broadcasting, stochastic gradient descent, the MNIST loss function, and the sigmoid activation functions.\n\n\n\n\n\n\nMar 14, 2022\n\n\n84 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 5\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 5 covers creating a custom DataBlock for an image classifier, pre-sizing, cross-entropy loss, model interpretation, picking learning rates, transfer learning, and discriminative learning rates.\n\n\n\n\n\n\nMar 14, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 6\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 6 covers multi-label classification and image regression.\n\n\n\n\n\n\nMar 14, 2022\n\n\n20 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 7\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 7 covers data normalization, progressive resizing, test-time augmentation, mixup, and label smoothing.\n\n\n\n\n\n\nMar 14, 2022\n\n\n26 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 8\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 8 provides a deep dive into collaborative filtering.\n\n\n\n\n\n\nMar 28, 2022\n\n\n43 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 09\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 9 provides a deep dive into tabular modeling.\n\n\n\n\n\n\nMar 29, 2022\n\n\n58 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 10\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 10 covers text preprocessing and training an RNN for text classification.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 11\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 11 covers data munging with fast.ai’s mid-level data API.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 12\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 12 covers building and training an LSTM from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n49 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 13\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 13 provides a deep dive into convolutional neural networks.\n\n\n\n\n\n\nMar 29, 2022\n\n\n59 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 14\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 14 covers building a ResNet from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n43 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 15\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 15 provides a deep dive into different application architectures in the fast.ai library.\n\n\n\n\n\n\nMar 29, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 16\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 16 covers momentum, RMSProp, Adam, decoupled weight decay, and fast.ai callbacks.\n\n\n\n\n\n\nMar 29, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 17\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 17 covers building a neural network from the foundations.\n\n\n\n\n\n\nMar 29, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 18\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 18 covers interpreting convolutional neural networks using class-activation maps.\n\n\n\n\n\n\nMar 29, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\nNotes on fastai Book Ch. 19\n\n\n\n\n\n\n\nai\n\n\nfastai\n\n\nnotes\n\n\npytorch\n\n\n\n\nChapter 19 covers creating a fast.ai Learner from scratch.\n\n\n\n\n\n\nMar 29, 2022\n\n\n46 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/game-dev-notes.html",
    "href": "series/notes/game-dev-notes.html",
    "title": "Game Development",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nNotes on Machine Learning and Level Generation\n\n\n\n\n\nMy notes from Ben Berman’s 2017 talk on machine learning and level generation.\n\n\n\n\n\n\nDec 9, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nNotes on Procedural Map Generation Techniques\n\n\n\n\n\nMy notes on Herbert Wolverson’s talk on procedural map generation techniques from the 2020 virtual Roguelike Celebration.\n\n\n\n\n\n\nDec 9, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on the Procedural Tools Used to Make Far Cry 5\n\n\n\n\n\nMy notes from Etienne Carrier’s overview of the procedural tools developed to create Far Cry 5.\n\n\n\n\n\n\nDec 9, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nNotes on 1D Nonlinear Transformations for Games\n\n\n\n\n\nMy notes from Squirrel Eiserloh’s presentation on 1D nonlinear transformations for game development.\n\n\n\n\n\n\nDec 29, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nNotes on How to Teach 5 Semesters of Game Design in 1 Class\n\n\n\n\n\nMy notes from Jason Wiser’s talk on how he teaches a class that covers 5 semesters worth of game design in a single semester.\n\n\n\n\n\n\nDec 29, 2021\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/index.html",
    "href": "series/notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Natural Language Processing with Transformers\n\n\n\n\n\nMy notes from the book Natural Language Processing with Transformers: Building Language Applications with Hugging Face by Lewis Tunstall, Leandro von Werra, and Thomas Wolf.\n\n\n\n\n\n\nMar 30, 2022\n\n\n\n\n\n\n\n\nDeep Learning for Coders with fastai & PyTorch\n\n\n\n\n\nMy notes from the book Deep Learning for Coders with fastai & PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger.\n\n\n\n\n\n\nMar 14, 2022\n\n\n\n\n\n\n\n\nProfessional Growth\n\n\n\n\n\nMy notes from resources on professional growth.\n\n\n\n\n\n\nDec 16, 2021\n\n\n\n\n\n\n\n\nGame Development\n\n\n\n\n\nMy notes from resources on game development.\n\n\n\n\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\nProcedural Generation\n\n\n\n\n\nMy notes from resources on procedural generation.\n\n\n\n\n\n\nDec 9, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/procedural-generation-notes.html",
    "href": "series/notes/procedural-generation-notes.html",
    "title": "Procedural Generation",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nNotes on How A.I. Will Change the 3D Industry\n\n\n\n\n\nMy notes from Andrew Price’s talk at Blender Conference 2018 on how A.I. will change the 3D industry.\n\n\n\n\n\n\nDec 9, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on Procedural Map Generation Techniques\n\n\n\n\n\nMy notes on Herbert Wolverson’s talk on procedural map generation techniques from the 2020 virtual Roguelike Celebration.\n\n\n\n\n\n\nDec 9, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on the Procedural Tools Used to Make Far Cry 5\n\n\n\n\n\nMy notes from Etienne Carrier’s overview of the procedural tools developed to create Far Cry 5.\n\n\n\n\n\n\nDec 9, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nNotes on Dungeon Generation via WaveFunctionCollapse\n\n\n\n\n\nMy notes from Brian Bucklew’s talk on procedurally generating dungeon levels using the WaveFunctionCollapse algorithm.\n\n\n\n\n\n\nDec 28, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nNotes on WaveFunctionCollapse for 3D\n\n\n\n\n\nMy notes from Martin Donald’s video on using the WaveFunctionCollapse algorithm for 3D modules.\n\n\n\n\n\n\nDec 28, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nNotes on Practical Procedural Generation\n\n\n\n\n\nMy notes from Kate Compton’s talk on practical techniques for procedural generation.\n\n\n\n\n\n\nDec 29, 2021\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/professional-growth-notes.html",
    "href": "series/notes/professional-growth-notes.html",
    "title": "Professional Growth",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nNotes on How To Speak\n\n\n\n\n\nMy notes for Patrick Winston’s course on how to speak.\n\n\n\n\n\n\nDec 16, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nNotes on Making Money as a Freelance Developer\n\n\n\n\n\nMy notes on some tips for making money as a freelance developer by Luke Ciciliano on freeCodeCamp.\n\n\n\n\n\n\nDec 16, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nNotes on No ML Degree Book\n\n\n\n\n\nMy notes on Emil Wallner’s guide on how to land your first machine learning job without a degree.\n\n\n\n\n\n\nMay 27, 2022\n\n\n22 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/notes/transformers-book-notes.html",
    "href": "series/notes/transformers-book-notes.html",
    "title": "Natural Language Processing with Transformers",
    "section": "",
    "text": "Notes on Transformers Book Ch. 1\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.\n\n\n\n\n\n\nMar 30, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 2\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 2 covers training a model to classify emotions expressed in Twitter messages.\n\n\n\n\n\n\nApr 1, 2022\n\n\n79 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 3\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 3 covers the Transformer architecture and different types of transformer models available on the Hugging Face Hub.\n\n\n\n\n\n\nApr 6, 2022\n\n\n30 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 4\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.\n\n\n\n\n\n\nApr 7, 2022\n\n\n105 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 5\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 5 covers different methods for generating text with GPT-2.\n\n\n\n\n\n\nApr 8, 2022\n\n\n23 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 6\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 6 covers building an encoder-decoder model to condense dialogues between several people into a crisp summary.\n\n\n\n\n\n\nApr 10, 2022\n\n\n44 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 7\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 7 covers building a question-answering model that finds answers to questions in customer reviews.\n\n\n\n\n\n\nApr 12, 2022\n\n\n72 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 8\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 8 covers different methods to make transformer models more efficient in production.\n\n\n\n\n\n\nApr 14, 2022\n\n\n62 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 9\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.\n\n\n\n\n\n\nApr 22, 2022\n\n\n95 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 10\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 10 covers how to train a GPT-like model to generate Python source code from scratch.\n\n\n\n\n\n\nApr 25, 2022\n\n\n71 min\n\n\n\n\n\n\n\n\nNotes on Transformers Book Ch. 11\n\n\n\n\n\n\n\nai\n\n\nhuggingface\n\n\nnlp\n\n\nnotes\n\n\n\n\nChapter 11 explores scaling up transformers, methods to make self-attention more efficient, and multimodel transformers.\n\n\n\n\n\n\nApr 26, 2022\n\n\n30 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/fastai-to-unity-tutorial.html",
    "href": "series/tutorials/fastai-to-unity-tutorial.html",
    "title": "Fastai to Unity Beginner Tutorial",
    "section": "",
    "text": "Fastai to Unity Beginner Tutorial Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nTrain an image classifier using the fastai library and export it to ONNX.\n\n\n\n\n\n\nJun 6, 2022\n\n\n37 min\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nClassify images in a Unity project with the Barracuda inference library.\n\n\n\n\n\n\nJun 7, 2022\n\n\n34 min\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nBuild a Unity project as a WebGL application and host it using GitHub Pages.\n\n\n\n\n\n\nJun 8, 2022\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/barracuda-posenet-tutorial-v2-series.html",
    "href": "series/tutorials/barracuda-posenet-tutorial-v2-series.html",
    "title": "Barracuda PoseNet Tutorial 2nd Edition",
    "section": "",
    "text": "Barracuda PoseNet Tutorial 2nd Edition Pt. 1\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nThis tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library.\n\n\n\n\n\n\nJul 20, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 2\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nSet up a video player and webcam in Unity.\n\n\n\n\n\n\nJul 20, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 3\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the preprocessing steps for the MobileNet and ResNet PoseNet models.\n\n\n\n\n\n\nJul 27, 2021\n\n\n19 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 4\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nLoad, modify, and execute the PoseNet models.\n\n\n\n\n\n\nJul 28, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 5\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the post-processing steps for single pose estimation with PoseNet.\n\n\n\n\n\n\nJul 29, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 6\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nImplement the post-processing steps for multi-pose estimation with PoseNet.\n\n\n\n\n\n\nJul 30, 2021\n\n\n21 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition Pt. 7\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\ntutorial\n\n\n\n\nCreate pose skeletons and manipulate them using output from a PoseNet model.\n\n\n\n\n\n\nJul 30, 2021\n\n\n21 min\n\n\n\n\n\n\n\n\nBarracuda PoseNet WebGL Tutorial\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nModify the Barracuda PoseNet project to run in a browser using WebGL.\n\n\n\n\n\n\nMar 10, 2022\n\n\n19 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/end-to-end-in-game-style-transfer-tutorial.html",
    "href": "series/tutorials/end-to-end-in-game-style-transfer-tutorial.html",
    "title": "End-to-End In-Game Style Transfer Tutorial",
    "section": "",
    "text": "End-to-End In-Game Style Transfer Tutorial Pt.1\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis tutorial series covers how to train your own style transfer model with PyTorch and implement it in Unity using the Barracuda library.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.1.5 (Optional)\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to use the Unity Recorder tool to generate additional training data for our style transfer model.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.2\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to train an artistic style transfer model with PyTorch in Google Colab.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n32 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Pt.3\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how implement the style transfer model in Unity with the Barracuda library.\n\n\n\n\n\n\nMar 3, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial Addendum\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to use a different style transfer model that is specialized for video.\n\n\n\n\n\n\nMar 20, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\nTargeted In-Game Style Transfer Tutorial\n\n\n\n\n\n\n\nstyle-transfer\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to select which game objects get stylized in the Unity scene.\n\n\n\n\n\n\nJul 4, 2021\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/fastai-libtorch-unity-tutorial.html",
    "href": "series/tutorials/fastai-libtorch-unity-tutorial.html",
    "title": "How to Create a LibTorch Plugin for Unity on Windows",
    "section": "",
    "text": "How to Create a LibTorch Plugin for Unity on Windows Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the training code from the fastai-to-unity tutorial to export the model to a TorchScript module.\n\n\n\n\n\n\nJun 28, 2022\n\n\n33 min\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform image classification using LibTorch.\n\n\n\n\n\n\nJun 28, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the Unity project from the fastai-to-unity tutorial to classify images with LibTorch.\n\n\n\n\n\n\nJun 28, 2022\n\n\n16 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/fastai-openvino-unity-tutorial.html",
    "href": "series/tutorials/fastai-openvino-unity-tutorial.html",
    "title": "How to Create an OpenVINO Plugin for Unity on Windows",
    "section": "",
    "text": "How to Create an OpenVINO Plugin for Unity on Windows Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the training code from the fastai-to-unity tutorial to export the model to OpenVINO.\n\n\n\n\n\n\nJul 17, 2022\n\n\n42 min\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform image classification with OpenVINO.\n\n\n\n\n\n\nJul 17, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\ntutorial\n\n\n\n\nModify the Unity project from the fastai-to-unity tutorial to classify images with OpenVINO.\n\n\n\n\n\n\nJul 18, 2022\n\n\n20 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/icevision-mask-rcnn-series.html",
    "href": "series/tutorials/icevision-mask-rcnn-series.html",
    "title": "Training a Mask R-CNN Model on a Custom Dataset With IceVision",
    "section": "",
    "text": "Training a Mask R-CNN Model on a Custom Dataset With IceVision\n\n\n\n\n\n\n\nicevision\n\n\nmask-rcnn\n\n\nobject-detection\n\n\ninstance-segmentation\n\n\ntutorial\n\n\n\n\nTrain a Mask R-CNN model on a custom dataset using the IceVision library and perform inference with ONNX Runtime.\n\n\n\n\n\n\nDec 2, 2022\n\n\n51 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/icevision-openvino-unity-series.html",
    "href": "series/tutorials/icevision-openvino-unity-series.html",
    "title": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO",
    "section": "",
    "text": "A Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 1\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nTrain a YOLOX model using IceVision and export it to OpenVINO.\n\n\n\n\n\n\nAug 8, 2022\n\n\n77 min\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 2\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using OpenVINO.\n\n\n\n\n\n\nAug 9, 2022\n\n\n24 min\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO Pt. 3\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nPerform object detection in a Unity project with OpenVINO.\n\n\n\n\n\n\nAug 10, 2022\n\n\n39 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/index.html",
    "href": "series/tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Training a Mask R-CNN Model on a Custom Dataset With IceVision\n\n\n\n\n\n\n\nicevision\n\n\nmask-rcnn\n\n\nobject-detection\n\n\ninstance-segmentation\n\n\n\n\nTrain a Mask R-CNN model on a custom dataset using the IceVision library and perform inference with ONNX Runtime.\n\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\nTensorFlow.js in Unity\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\nyolox\n\n\nobject-detection\n\n\n\n\nIn this tutorial series, we explore how to create TensorFlow.js plugins for the Unity game engine.\n\n\n\n\n\n\nOct 4, 2022\n\n\n\n\n\n\n\n\nReal-Time Object Detection in Unity With ONNX and DirectML\n\n\n\n\n\n\n\nonnx\n\n\ndirectml\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\n\n\nIn this two-part tutorial series, I will show you how to implement real-time object detection in Unity using ONNX Runtime and DirectML.\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\nA Step-by-Step Guide to Object Detection in Unity with IceVision and OpenVINO\n\n\n\n\n\n\n\nicevision\n\n\nopenvino\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\n\n\nIn this three-part tutorial series, we will explore how to use IceVision and OpenVINO to perform end-to-end object detection in Unity.\n\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\nHow to Create an OpenVINO Plugin for Unity on Windows\n\n\n\n\n\n\n\nfastai\n\n\nopenvino\n\n\nunity\n\n\n\n\nThis tutorial series is a follow-up to the Fastai to Unity Beginner Tutorial, which covers using OpenVINO, an open-source toolkit for optimizing model inference, instead of Unity’s Barracuda library.\n\n\n\n\n\n\nJul 17, 2022\n\n\n\n\n\n\n\n\nHow to Create a LibTorch Plugin for Unity on Windows\n\n\n\n\n\n\n\nfastai\n\n\nlibtorch\n\n\nunity\n\n\n\n\nThis tutorial series is a follow-up to the Fastai to Unity Beginner Tutorial, which covers using LibTorch, the C++ distribution of PyTorch.\n\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\nFastai to Unity Beginner Tutorial\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\nbarracuda\n\n\n\n\nIn this tutorial series, we will walk through training an image classifier using the fastai library and implementing it in a Unity game engine project using the Barracuda inference library.\n\n\n\n\n\n\nJun 6, 2022\n\n\n\n\n\n\n\n\nBarracuda PoseNet Tutorial 2nd Edition\n\n\n\n\n\n\n\nunity\n\n\nbarracuda\n\n\n\n\nThis tutorial series provides step-by-step instructions for how to perform human pose estimation in Unity with the Barracuda inference library.\n\n\n\n\n\n\nJul 20, 2021\n\n\n\n\n\n\n\n\nMiscellaneous Blender Tutorials\n\n\n\n\n\n\n\nblender\n\n\npython\n\n\n\n\nA collection of miscellaneous tutorials for Blender.\n\n\n\n\n\n\nMar 24, 2021\n\n\n\n\n\n\n\n\nMiscellaneous Unity Tutorials\n\n\n\n\n\n\n\nunity\n\n\n\n\nA collection of miscellaneous tutorials for the Unity game engine.\n\n\n\n\n\n\nMar 20, 2021\n\n\n\n\n\n\n\n\nEnd-to-End In-Game Style Transfer Tutorial\n\n\n\n\n\n\n\nstyle-transfer\n\n\npytorch\n\n\nunity\n\n\n\n\nThis tutorial series covers how to train your own style transfer model with PyTorch and implement it in Unity using the Barracuda library.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/miscellaneous-blender-tutorials.html",
    "href": "series/tutorials/miscellaneous-blender-tutorials.html",
    "title": "Miscellaneous Blender Tutorials",
    "section": "",
    "text": "Create a Shape Key Motion Graphic with the Blender Python API\n\n\n\n\n\n\n\nblender\n\n\npython\n\n\ntutorial\n\n\n\n\nThis post covers how to create a simple shape-key motion graphic in Blender using the Python API.\n\n\n\n\n\n\nMar 24, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nCreate a Triangle Motion Graphic with the Blender Python API\n\n\n\n\n\n\n\nblender\n\n\npython\n\n\ntutorial\n\n\n\n\nThis post covers how to create a simple triangle motion graphic in Blender using the Python API.\n\n\n\n\n\n\nMar 27, 2021\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/miscellaneous-unity-tutorials.html",
    "href": "series/tutorials/miscellaneous-unity-tutorials.html",
    "title": "Miscellaneous Unity Tutorials",
    "section": "",
    "text": "How to Crop Images With a GPU in Unity\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to efficiently crop images in Unity with a GPU.\n\n\n\n\n\n\nMar 20, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nHow to Flip an Image With a Compute Shader\n\n\n\n\n\n\n\nunity\n\n\ntutorial\n\n\n\n\nThis post covers how to flip an image with a compute shader in Unity.\n\n\n\n\n\n\nMar 21, 2021\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/onnx-directml-unity-series.html",
    "href": "series/tutorials/onnx-directml-unity-series.html",
    "title": "Real-Time Object Detection in Unity With ONNX and DirectML",
    "section": "",
    "text": "Real-Time Object Detection in Unity With ONNX and DirectML Pt. 1\n\n\n\n\n\n\n\nonnx\n\n\ndirectml\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nCreate a dynamic link library (DLL) file in Visual Studio to perform object detection with a YOLOX model using ONNX Runtime and DirectML.\n\n\n\n\n\n\nAug 18, 2022\n\n\n26 min\n\n\n\n\n\n\n\n\nReal-Time Object Detection in Unity With ONNX and DirectML Pt. 2\n\n\n\n\n\n\n\nonnx\n\n\ndirectml\n\n\nyolox\n\n\nobject-detection\n\n\nunity\n\n\ntutorial\n\n\n\n\nPerform object detection in a Unity project with ONNX Runtime and DirectML.\n\n\n\n\n\n\nAug 19, 2022\n\n\n39 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/tutorials/tensorflow-js-unity-series.html",
    "href": "series/tutorials/tensorflow-js-unity-series.html",
    "title": "TensorFlow.js in Unity",
    "section": "",
    "text": "Using Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 1\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nTrain a hand gesture classifier using fastai and export it to TensorFlow.js.\n\n\n\n\n\n\nOct 4, 2022\n\n\n37 min\n\n\n\n\n\n\n\n\nUsing Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 2\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nCreate a TensorFlow.js plugin for the Unity game engine.\n\n\n\n\n\n\nOct 5, 2022\n\n\n35 min\n\n\n\n\n\n\n\n\nUsing Fastai and TensorFlow.js for Hand Gesture Recognition in Unity Pt. 3\n\n\n\n\n\n\n\nfastai\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\ntutorial\n\n\n\n\nBuild a Unity project as a WebGL application and host it using GitHub Pages.\n\n\n\n\n\n\nOct 6, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nUsing TensorFlow.js for In-Browser Object Detection in Unity\n\n\n\n\n\n\n\nunity\n\n\ntensorflow\n\n\nwebgl\n\n\nyolox\n\n\nobject-detection\n\n\ntutorial\n\n\n\n\nCreate a TensorFlow.js plugin for the Unity game engine to perform object detection with a YOLOX model.\n\n\n\n\n\n\nOct 16, 2022\n\n\n46 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Deep Learning Consulting Services",
    "section": "",
    "text": "As a deep learning consultant, I offer a range of services tailored to meet the specific needs of each client, including but not limited to the following:"
  },
  {
    "objectID": "services.html#why-choose-me",
    "href": "services.html#why-choose-me",
    "title": "Deep Learning Consulting Services",
    "section": "Why Choose Me?",
    "text": "Why Choose Me?\n\nExpertise: Years of experience have equipped me with the knowledge and skills necessary to deliver exceptional results for your projects.\nCustomized Solutions: I work closely with clients to develop tailored AI solutions that meet their unique requirements and objectives.\nClear communication: I believe in maintaining open lines of communication with clients to ensure a smooth, collaborative process from start to finish.\nCommitment to Success: Your success is my priority, and I am dedicated to helping you achieve your goals with my services."
  },
  {
    "objectID": "services.html#get-in-touch",
    "href": "services.html#get-in-touch",
    "title": "Deep Learning Consulting Services",
    "section": "Get in Touch",
    "text": "Get in Touch\n\n\nReady to explore how you can leverage deep learning to transform your ideas into reality? Schedule a 30-minute consultation to:\n\nDiscuss your project’s vision, objectives, and challenges.\nExplore high-level deep-learning opportunities and roadmaps.\nDetermine if my services align with your needs.\n\nPlease note that this session provides an overview, not in-depth technical advice or solutions. If we decide to work together, we can create a tailored plan that addresses your project’s unique challenges and goals."
  }
]