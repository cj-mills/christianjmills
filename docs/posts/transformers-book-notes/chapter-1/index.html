<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-03-30">
<meta name="description" content="Chapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.">

<title>Christian Mills - Notes on Transformers Book Ch. 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on Transformers Book Ch. 1">
<meta property="og:description" content="Chapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.">
<meta property="og:image" content="christianjmills.com/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta property="og:image:height" content="295">
<meta property="og:image:width" content="300">
<meta name="twitter:title" content="Christian Mills - Notes on Transformers Book Ch. 1">
<meta name="twitter:description" content="Chapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.">
<meta name="twitter:image" content="christianjmills.com/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:image-height" content="295">
<meta name="twitter:image-width" content="300">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"><i class="bi bi-envelope-fill" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 1</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 1 covers essential advancements for transformers, recurrent architectures, the encoder-decoder framework, attention mechanisms, transfer learning in NLP, and the HuggingFace ecosystem.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 30, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<ul>
<li><a href="#key-advancements">Key Advancements</a></li>
<li><a href="#recurrent-architectures">Recurrent Architectures</a></li>
<li><a href="#the-encoder-decoder-framework">The Encoder-Decoder Framework</a></li>
<li><a href="#attention-mechanisms">Attention Mechanisms</a></li>
<li><a href="#transfer-learning-in-nlp">Transfer Learning in NLP</a></li>
<li><a href="#bridging-the-gap-with-hugging-face-transformers">Bridging the Gap With Hugging Face Transformers</a></li>
<li><a href="#a-tour-of-transformer-applications">A Tour of Transformer Applications</a></li>
<li><a href="#the-hugging-face-ecosystem">The Hugging Face Ecosystem</a></li>
<li><a href="#main-challenges-with-transformers">Main Challenges with Transformers</a></li>
<li><a href="#references">References</a></li>
</ul>
<section id="key-advancements" class="level2">
<h2 class="anchored" data-anchor-id="key-advancements">Key Advancements</h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>
<ul>
<li>published in June 2017 by researchers at Google</li>
<li>introduced the Transformer architecture for sequence modeling</li>
<li>outperformed Recurrent Neural Networks (RNNs) on machine translation tasks, both in terms of translation quality and training cost</li>
</ul></li>
<li><a href="https://arxiv.org/abs/1801.06146">Universal Language Model Fine-tuning for Text Classification</a>
<ul>
<li>published in January 2018 by Jeremy Howard and Sebastian Ruder</li>
<li>introduced an effective training method called ULMFiT</li>
<li>showed that training Long Short-Term Memory Networks (LSTMs) on a very large and diverse corpus could produce state-of-the-art text classifiers with little labeled data</li>
<li>inspired other research groups to combine transformers with unsupervised learning</li>
</ul></li>
<li><a href="https://openai.com/blog/language-unsupervised/">Improving Language Understanding with Unsupervised Learning</a>
<ul>
<li>published by OpenAI in June 2018</li>
<li>introduced Generative Pretrained Transformer (GPT)</li>
</ul></li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
<ul>
<li>published by researchers at Google in October 2018</li>
</ul></li>
<li>Combining the Transformer architecture with unsupervised learning removed the need to train task-specific architectures from scratch.</li>
<li>Pretrained Transformers broke almost every benchmark in NLP by a significant margin.</li>
</ul>
</section>
<section id="recurrent-architectures" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-architectures">Recurrent Architectures</h2>
<ul>
<li>Recurrent architectures such as LSTMs were state of the art in Natural Language Processing (NLP) before Transformers.</li>
<li>Recurrent architectures contain a feedback loop that allows information to propagate from one step to another.
<ul>
<li>ideal for sequential data like text.</li>
</ul></li>
<li>A Recurrent Neural Network receives an input token and feeds it through the network.</li>
<li>The network outputs a vector called a hidden state, and it passes some information back to itself through a feedback loop.</li>
<li>The information passed through the feedback loop allows an RNN to keep track of details from previous steps and use it to make predictions.</li>
<li>Many still use recurrent architectures for NLP, speech processing, and time-series tasks.</li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>
<ul>
<li>provides an overview of RNNs and demonstrates how to train a language model on several datasets</li>
</ul></li>
<li>RNNs were critical in developing systems to translate a sequence of words from one language to another.
<ul>
<li>known as machine translation</li>
</ul></li>
<li>The computations for recurrent models are inherently sequential and cannot parallelize across the input.
<ul>
<li>The inability to parallelize computations is a fundamental shortcoming of recurrent models.</li>
</ul></li>
</ul>
</section>
<section id="the-encoder-decoder-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-encoder-decoder-framework">The Encoder-Decoder Framework</h2>
<ul>
<li><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>
<ul>
<li>published in 2014 by researchers at Google</li>
</ul></li>
<li>An encoder-decoder is also known as a sequence-to-sequence architecture.</li>
<li>This type of architecture is well-suited for situations where the input and output are both sequences of arbitrary length.</li>
<li>An encoder encodes information from an input sequence into a numerical representation.
<ul>
<li>This numerical representation is often called the last hidden state.</li>
</ul></li>
<li>The decoder uses the numerical representation to generate the output sequence.</li>
<li>The encoder and decoder can use any neural network architecture that can model sequences.</li>
<li>The final hidden state of the encoder is the only information the decoder has access to when generating the output.
<ul>
<li>It has to represent the meaning of the whole input sequence.</li>
<li>This requirement creates an information bottleneck that can limit performance for longer sequences.</li>
</ul></li>
</ul>
</section>
<section id="attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="attention-mechanisms">Attention Mechanisms</h2>
<ul>
<li>Attention mechanisms allow the decoder to access all of the encoder’s hidden states, not just the last one.</li>
<li>The decoder assigns a different amount of weight, called attention, to each state at every decoding time-step.</li>
<li>The attention values allow the decoder to prioritize which encoder state to use.</li>
<li>Attention-based models focus on which input tokens are most relevant at each time step.
<ul>
<li>They can learn nontrivial alignments between the words in a generated translation and those in a source sentence.
<ul>
<li>Example: An attention-based decoder can align the words “zone” and “Area” even when ordered differently in the source sentence and the translation.</li>
</ul></li>
</ul></li>
<li>Transformers use a special kind of attention called self-attention and do not use any form of recurrence.
<ul>
<li>Self-attention operates on all states in the same layer of a neural network.</li>
<li>The outputs of the self-attention mechanisms serve as input to feed-forward networks.</li>
<li>This architecture trains much faster than recurrent models.</li>
</ul></li>
</ul>
</section>
<section id="transfer-learning-in-nlp" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning-in-nlp">Transfer Learning in NLP</h2>
<ul>
<li>Transfer learning involves using the knowledge a model learned from a previous task on a new one.
<ul>
<li>Computer vision models first train on large-scale datasets such as <a href="https://image-net.org/">ImageNet</a> to learn the basic features of images before being fine-tuned on a downstream task.</li>
<li>It was not initially clear how to perform transfer learning for NLP tasks.</li>
</ul></li>
<li>Fine-tuned models are typically more accurate than supervised models trained from scratch on the same amount of labeled data.</li>
<li>We adapt a pretrained model to a new task by splitting the model into a body and a head.</li>
<li>The head is the task-specific portion of the network.</li>
<li>The body contains broad features from the source domain learned during training.</li>
<li>We can use the body weights to initialize a new model head for a new task.</li>
<li>Transfer learning typically produces high-quality models that we can efficiently train on many downstream tasks.</li>
<li>The ULMFit paper provided a general framework to perform transfer learning with NLP models.
<ol type="1">
<li>A model first trains to predict the next word based on those preceding it in a large-scale generic corpus to learn the basic features of the language.
<ul>
<li>This task is called language modeling.</li>
</ul></li>
<li>The pretrained model then trains on the same task using an in-domain corpus.</li>
<li>Lastly, we fine-tune the model with a classification layer for the target task.</li>
</ol></li>
<li>The ULMFit transfer learning framework provided the missing piece for transformers to take off.</li>
<li>Both GPT and BERT combine self-attention with transfer learning and set a new state of the art across many NLP benchmarks.</li>
<li>GPT only uses the decoder part of the Transformer architecture and the language modeling approach as ULMFiT.</li>
<li>BERT uses the encoder part of the Transformer architecture and a form of language modeling called masked language modeling.
<ul>
<li>Masked language modeling requires the model to fill in randomly missing words in a text.</li>
</ul></li>
<li>GPT trained on the BookCorpus dataset while BERT trained on the BookCorpus dataset and English Wikipedia.
<ul>
<li>The <a href="https://arxiv.org/abs/1506.06724">BookCorpus dataset</a> consists of thousands of unpublished books across many genres.</li>
</ul></li>
</ul>
</section>
<section id="bridging-the-gap-with-hugging-face-transformers" class="level2">
<h2 class="anchored" data-anchor-id="bridging-the-gap-with-hugging-face-transformers">Bridging the Gap With Hugging Face Transformers</h2>
<ul>
<li>Applying a novel machine learning architecture to a new application can be complicated and requires custom logic for each model and task.
<ol type="1">
<li>Implement the model architecture in code.
<ul>
<li>PyTorch and TensorFlow are the most common frameworks for this.</li>
</ul></li>
<li>Load pretrained weights if available.</li>
<li>Preprocess the inputs, pass them through the model, and apply task-specific postprocessing.</li>
<li>Implement data loaders and define loss functions and optimizers to train the model.</li>
</ol></li>
<li>Code released by research groups is rarely standardized and often requires days of engineering to adapt to new use cases.
<ul>
<li>Different research labs often release their models in incompatible frameworks, making it difficult for practitioners to port these models to their applications.</li>
</ul></li>
<li>Hugging Face Transformers provides a standardized interface to a wide range of transformer models, including code and tools to adapt these models to new applications.
<ul>
<li>The availability of a standardized interface catalyzed the explosion of research into transformers and made it easy for NLP practitioners to integrate these models into real-life applications.</li>
</ul></li>
<li>The library supports the PyTorch, TensorFlow, and JAX deep learning frameworks and provides task-specific model heads to fine-tune transformers on downstream tasks.</li>
</ul>
</section>
<section id="a-tour-of-transformer-applications" class="level2">
<h2 class="anchored" data-anchor-id="a-tour-of-transformer-applications">A Tour of Transformer Applications</h2>
<ul>
<li>Hugging Face Transformers has a layered API that allows users to interact with the library at various levels of abstraction.</li>
</ul>
<section id="pipelines" class="level3">
<h3 class="anchored" data-anchor-id="pipelines"><a href="https://huggingface.co/docs/transformers/main_classes/pipelines">Pipelines</a></h3>
<ul>
<li>Pipelines abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model.</li>
<li>Hugging Face provides pipelines for several tasks.</li>
<li>Instantiate a pipeline by calling the <code>pipeline()</code> function and providing the name of the desired task.
<ul>
<li><code>'audio-classification'</code></li>
<li><code>'automatic-speech-recognition'</code></li>
<li><code>'feature-extraction'</code></li>
<li><code>'text-classification'</code></li>
<li><code>'token-classification'</code></li>
<li><code>'question-answering'</code></li>
<li><code>'table-question-answering'</code></li>
<li><code>'fill-mask'</code></li>
<li><code>'summarization'</code></li>
<li><code>'translation'</code></li>
<li><code>'text2text-generation'</code></li>
<li><code>'text-generation'</code></li>
<li><code>'zero-shot-classification'</code></li>
<li><code>'conversational'</code></li>
<li><code>'image-classification'</code></li>
<li><code>'object-detection'</code></li>
</ul></li>
<li>The names for the supported tasks are available in the <code>transformers.pipelines.SUPPORTED_TASKS</code> dictionary.</li>
<li>The pipeline automatically downloads the model weights for the selected task and caches them for future use.</li>
<li>Each pipeline takes a string of text or a list of strings as input and returns a list of predictions.
<ul>
<li>Each prediction is in a Python dictionary along with the corresponding confidence score.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>&lt;function transformers.pipelines.pipeline(task: str, model: Optional = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, use_auth_token: Union[bool, str, NoneType] = None, model_kwargs: Dict[str, Any] = {}, **kwargs) -&gt; transformers.pipelines.base.Pipeline&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(transformers.pipelines.SUPPORTED_TASKS.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>['audio-classification',
 'automatic-speech-recognition',
 'feature-extraction',
 'text-classification',
 'token-classification',
 'question-answering',
 'table-question-answering',
 'fill-mask',
 'summarization',
 'translation',
 'text2text-generation',
 'text-generation',
 'zero-shot-classification',
 'conversational',
 'image-classification',
 'object-detection']</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>transformers.pipelines.TASK_ALIASES</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>{'sentiment-analysis': 'text-classification', 'ner': 'token-classification'}</code></pre>
<section id="sample-text-customer-review" class="level5">
<h5 class="anchored" data-anchor-id="sample-text-customer-review">Sample Text: Customer Review</h5>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""Dear Amazon, last week I ordered an Optimus Prime action figure </span><span class="ch">\</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="st">from your online store in Germany. Unfortunately, when I opened the package, </span><span class="ch">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="st">I discovered to my horror that I had been sent an action figure of Megatron </span><span class="ch">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">instead! As a lifelong enemy of the Decepticons, I hope you can understand my </span><span class="ch">\</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">dilemma. To resolve the issue, I demand an exchange of Megatron for the </span><span class="ch">\</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">Optimus Prime figure I ordered. Enclosed are copies of my records concerning </span><span class="ch">\</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="st">this purchase. I expect to hear from you soon. Sincerely, Bumblebee."""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="text-classification-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="text-classification-pipeline">Text Classification Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.TextClassificationPipeline">Documentation</a></li>
<li>The text-classification pipeline supports sentiment analysis, multiclass, and multilabel classification and performs sentiment analysis by default.</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify the customer review as positive or negative</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> classifier(text)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(outputs)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
label
</th>
<th>
score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
NEGATIVE
</td>
<td>
0.901546
</td>
</tr>
</tbody>

</table>
</div>
</section>
<section id="named-entity-recognition-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="named-entity-recognition-pipeline">Named Entity Recognition Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.TokenClassificationPipeline">Documentation</a></li>
<li>Named entity recognition (NER) involves extracting real-world objects like products, places, and people from a piece of text.</li>
<li>Default Entity Labels
<ul>
<li><code>MISC</code>: Miscellaneous</li>
<li><code>PER</code>: Person</li>
<li><code>ORG</code>: Organization</li>
<li><code>LOC</code>: Location</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a named entity recognizer that groups words according to the model's predictions</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ner_tagger <span class="op">=</span> pipeline(<span class="st">"ner"</span>, aggregation_strategy<span class="op">=</span><span class="st">"simple"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The <code>simple</code> aggregation strategy might end up splitting words undesirably.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ner_tagger.model.config.id2label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> ner_tagger(text)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(outputs)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
entity_group
</th>
<th>
score
</th>
<th>
word
</th>
<th>
start
</th>
<th>
end
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
ORG
</td>
<td>
0.879010
</td>
<td>
Amazon
</td>
<td>
5
</td>
<td>
11
</td>
</tr>
<tr>
<th>
1
</th>
<td>
MISC
</td>
<td>
0.990859
</td>
<td>
Optimus Prime
</td>
<td>
36
</td>
<td>
49
</td>
</tr>
<tr>
<th>
2
</th>
<td>
LOC
</td>
<td>
0.999755
</td>
<td>
Germany
</td>
<td>
90
</td>
<td>
97
</td>
</tr>
<tr>
<th>
3
</th>
<td>
MISC
</td>
<td>
0.556570
</td>
<td>
Mega
</td>
<td>
208
</td>
<td>
212
</td>
</tr>
<tr>
<th>
4
</th>
<td>
PER
</td>
<td>
0.590256
</td>
<td>
##tron
</td>
<td>
212
</td>
<td>
216
</td>
</tr>
<tr>
<th>
5
</th>
<td>
ORG
</td>
<td>
0.669693
</td>
<td>
Decept
</td>
<td>
253
</td>
<td>
259
</td>
</tr>
<tr>
<th>
6
</th>
<td>
MISC
</td>
<td>
0.498349
</td>
<td>
##icons
</td>
<td>
259
</td>
<td>
264
</td>
</tr>
<tr>
<th>
7
</th>
<td>
MISC
</td>
<td>
0.775362
</td>
<td>
Megatron
</td>
<td>
350
</td>
<td>
358
</td>
</tr>
<tr>
<th>
8
</th>
<td>
MISC
</td>
<td>
0.987854
</td>
<td>
Optimus Prime
</td>
<td>
367
</td>
<td>
380
</td>
</tr>
<tr>
<th>
9
</th>
<td>
PER
</td>
<td>
0.812096
</td>
<td>
Bumblebee
</td>
<td>
502
</td>
<td>
511
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The words <code>Megatron</code>, and <code>Decepticons</code> were split into separate words.</p>
<p><strong>Note:</strong> The <code>##</code> symbols are produced by the model’s tokenizer.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ner_tagger.tokenizer.vocab_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>28996</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(ner_tagger.tokenizer.vocab, index<span class="op">=</span>[<span class="dv">0</span>]).T.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Rees
</th>
<td>
24646
</td>
</tr>
<tr>
<th>
seeded
</th>
<td>
14937
</td>
</tr>
<tr>
<th>
Ruby
</th>
<td>
11374
</td>
</tr>
<tr>
<th>
Libraries
</th>
<td>
27927
</td>
</tr>
<tr>
<th>
foil
</th>
<td>
20235
</td>
</tr>
<tr>
<th>
collapsed
</th>
<td>
7322
</td>
</tr>
<tr>
<th>
membership
</th>
<td>
5467
</td>
</tr>
<tr>
<th>
Birth
</th>
<td>
20729
</td>
</tr>
<tr>
<th>
Texans
</th>
<td>
25904
</td>
</tr>
<tr>
<th>
Saul
</th>
<td>
18600
</td>
</tr>
</tbody>

</table>
</div>
</section>
<section id="question-answering-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="question-answering-pipeline">Question Answering Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline">Documentation</a></li>
<li>Question answering involves having a model find the answer to a specified question using a given passage of text.</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>reader <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What does the customer want?"</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> reader(question<span class="op">=</span>question, context<span class="op">=</span>text)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([outputs])    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
score
</th>
<th>
start
</th>
<th>
end
</th>
<th>
answer
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.631291
</td>
<td>
335
</td>
<td>
358
</td>
<td>
an exchange of Megatron
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> This particular kind of question answering is called extractive question answering. The answer is extracted directly from the text.</p>
</section>
<section id="summarization-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="summarization-pipeline">Summarization Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.SummarizationPipeline">Documentation</a></li>
<li>Text summarization involves generating a short version of a long passage of text while retaining all the relevant facts.</li>
<li>Tasks requiring a model to generate new text are more challenging than extractive ones.</li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Limit the generated summary to 45 words</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> summarizer(text, max_length<span class="op">=</span><span class="dv">45</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>][<span class="st">'summary_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code> Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.</code></pre>
<p><strong>Note:</strong> The model captured the essence of the customer message but directly copied some of the original text.</p>
</section>
<section id="translation-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="translation-pipeline">Translation Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.TranslationPipeline">Documentation</a></li>
<li>The model generates a translation of a piece of text in the target language.</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a translator that translates English to German</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Override the default model selection</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>translator <span class="op">=</span> pipeline(<span class="st">"translation_en_to_de"</span>, model<span class="op">=</span><span class="st">"Helsinki-NLP/opus-mt-en-de"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Require the model to generate a translation at least 100 words long</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> translator(text, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>, min_length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>][<span class="st">'translation_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt. Anbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, bald von Ihnen zu hören. Aufrichtig, Bumblebee.</code></pre>
<p><strong>Note:</strong> The model supposedly did a good job translating the text. (I don’t speak German.)</p>
</section>
<section id="text-generation-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="text-generation-pipeline">Text Generation Pipeline</h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.TextGenerationPipeline">Documentation</a></li>
<li>The model generates new text to complete a provided text prompt.</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> set_seed</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the random seed to get reproducible results</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> <span class="st">"Dear Bumblebee, I am sorry to hear that your order was mixed up."</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> text <span class="op">+</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">Customer service response:</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> response</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.

Customer service response:
Dear Bumblebee, I am sorry to hear that your order was mixed up. The order was completely mislabeled, which is very common in our online store, but I can appreciate it because it was my understanding from this site and our customer service of the previous day that your order was not made correct in our mind and that we are in a process of resolving this matter. We can assure you that your order</code></pre>
</section>
</section>
</section>
<section id="the-hugging-face-ecosystem" class="level2">
<h2 class="anchored" data-anchor-id="the-hugging-face-ecosystem">The Hugging Face Ecosystem</h2>
<ul>
<li>Hugging Face Transformers is surrounded by an ecosystem of helpful tools that support the modern machine learning workflow.</li>
<li>This ecosystem consists of a family of code libraries and a hub of pretrained model weights, datasets, scripts for evaluation, other resources.</li>
</ul>
<section id="the-hugging-face-hub" class="level3">
<h3 class="anchored" data-anchor-id="the-hugging-face-hub">The Hugging Face Hub</h3>
<ul>
<li>The Hub hosts over 20,000 freely available models plus datasets and scripts for computing metrics.</li>
<li>Model and dataset cards document the contents of the models and datasets.</li>
<li>Filters are available for tasks, frameworks, datasets, and more designed to help quickly navigate the Hub.</li>
<li>Users can directly try out any model through task-specific widgets.</li>
</ul>
</section>
<section id="hugging-face-tokenizers" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-tokenizers">Hugging Face Tokenizers</h3>
<ul>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/tokenizer">Documentation</a></li>
<li>Tokenizers split the raw input text into smaller pieces called tokens.</li>
<li>Tokens can be words, parts of words, or single characters.</li>
<li>Hugging Face Tokenizers takes care of all the preprocessing and postprocessing steps, such as normalizing the inputs and transforming the model outputs to the required format.</li>
<li>The <a href="https://github.com/huggingface/tokenizers">Tokenizers library</a> uses a <a href="https://www.rust-lang.org/">Rust</a> backend for fast tokenization.</li>
</ul>
</section>
<section id="hugging-face-datasets" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-datasets">Hugging Face Datasets</h3>
<ul>
<li><a href="https://huggingface.co/docs/datasets/index">Documentation</a></li>
<li>The Datasets library provides a standard interface for <a href="https://huggingface.co/datasets">thousands</a> of datasets to simplify loading, processing, and storing datasets.</li>
<li>Smart caching removes the need to perform preprocessing steps each time your run your code.</li>
<li>Memory mapping helps avoid RAM limitations by storing the contents of a file in virtual memory and enables multiple processes to modify the file more efficiently.</li>
<li>The library is interoperable with frameworks like <a href="https://pandas.pydata.org/">Pandas</a> and <a href="https://numpy.org/">NumPy</a>.</li>
<li>Scripts are available for many metrics to help make experiments more reproducible and trustworthy.</li>
</ul>
</section>
<section id="hugging-face-accelerate" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-accelerate">Hugging Face Accelerate</h3>
<ul>
<li><a href="https://huggingface.co/docs/accelerate/index">Documentation</a></li>
<li>The Accelerate library adds a layer of abstraction to training loops, which takes care of all the custom logic necessary for the training infrastructure.</li>
</ul>
</section>
</section>
<section id="main-challenges-with-transformers" class="level2">
<h2 class="anchored" data-anchor-id="main-challenges-with-transformers">Main Challenges with Transformers</h2>
<section id="language" class="level4">
<h4 class="anchored" data-anchor-id="language">Language</h4>
<ul>
<li>It is hard to find pretrained models for languages other than English.</li>
</ul>
</section>
<section id="data-availability" class="level4">
<h4 class="anchored" data-anchor-id="data-availability">Data Availability</h4>
<ul>
<li>Even with transfer learning, transformers still need a lot of data compared to humans to perform a task.</li>
</ul>
</section>
<section id="working-with-long-documents" class="level4">
<h4 class="anchored" data-anchor-id="working-with-long-documents">Working With Long Documents</h4>
<ul>
<li>Self-attention becomes computationally expensive when working on full-length documents.</li>
</ul>
</section>
<section id="opacity" class="level4">
<h4 class="anchored" data-anchor-id="opacity">Opacity</h4>
<ul>
<li>It is hard or impossible to determine precisely why a model made a given prediction.</li>
</ul>
</section>
<section id="bias" class="level4">
<h4 class="anchored" data-anchor-id="bias">Bias</h4>
<ul>
<li>Biases present in the training data imprint into the model.</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<!-- Cloudflare Web Analytics -->
<script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script>
<!-- End Cloudflare Web Analytics -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2022, Christian J. Mills
  </li>  
</ul>
      </div>
  </div>
</footer>



</body></html>