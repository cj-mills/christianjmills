<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-08-31">
<meta name="description" content="Lecture #3 provides a beginner-friendly introduction to CUDA programming with PyTorch, demonstrating how to write and execute CUDA kernels within a Python environment for tasks like image processing and matrix multiplication.">

<title>GPU MODE Lecture 3: Getting Started With CUDA for Python Programmers – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-d551e32f15e27e893f08ce3c93a41c1c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="GPU MODE Lecture 3: Getting Started With CUDA for Python Programmers – Christian Mills">
<meta property="og:description" content="Lecture #3 provides a beginner-friendly introduction to CUDA programming with PyTorch, demonstrating how to write and execute CUDA kernels within a Python environment for tasks like image processing and matrix multiplication.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="GPU MODE Lecture 3: Getting Started With CUDA for Python Programmers – Christian Mills">
<meta name="twitter:description" content="Lecture #3 provides a beginner-friendly introduction to CUDA programming with PyTorch, demonstrating how to write and execute CUDA kernels within a Python environment for tasks like image processing and matrix multiplication.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#exercise-1-rgb-to-grayscale-conversion" id="toc-exercise-1-rgb-to-grayscale-conversion" class="nav-link" data-scroll-target="#exercise-1-rgb-to-grayscale-conversion">Exercise 1: RGB to Grayscale Conversion</a>
  <ul>
  <li><a href="#understanding-the-problem" id="toc-understanding-the-problem" class="nav-link" data-scroll-target="#understanding-the-problem">1. Understanding the Problem</a></li>
  <li><a href="#loading-and-displaying-the-image" id="toc-loading-and-displaying-the-image" class="nav-link" data-scroll-target="#loading-and-displaying-the-image">2. Loading and Displaying the Image</a></li>
  <li><a href="#grayscale-conversion-in-python" id="toc-grayscale-conversion-in-python" class="nav-link" data-scroll-target="#grayscale-conversion-in-python">3. Grayscale Conversion in Python</a></li>
  <li><a href="#understanding-cuda-and-gpus" id="toc-understanding-cuda-and-gpus" class="nav-link" data-scroll-target="#understanding-cuda-and-gpus">4. Understanding CUDA and GPUs</a></li>
  <li><a href="#simulating-a-cuda-kernel-in-python" id="toc-simulating-a-cuda-kernel-in-python" class="nav-link" data-scroll-target="#simulating-a-cuda-kernel-in-python">5. Simulating a CUDA Kernel in Python</a></li>
  <li><a href="#cuda-blocks-and-threads" id="toc-cuda-blocks-and-threads" class="nav-link" data-scroll-target="#cuda-blocks-and-threads">6. CUDA Blocks and Threads</a></li>
  <li><a href="#cuda-setup-in-a-notebook" id="toc-cuda-setup-in-a-notebook" class="nav-link" data-scroll-target="#cuda-setup-in-a-notebook">7. CUDA Setup in a Notebook</a></li>
  <li><a href="#writing-and-compiling-cuda-kernels" id="toc-writing-and-compiling-cuda-kernels" class="nav-link" data-scroll-target="#writing-and-compiling-cuda-kernels">8. Writing and Compiling CUDA Kernels</a></li>
  <li><a href="#running-the-cuda-kernel" id="toc-running-the-cuda-kernel" class="nav-link" data-scroll-target="#running-the-cuda-kernel">9. Running the CUDA Kernel</a></li>
  </ul></li>
  <li><a href="#exercise-2-matrix-multiplication" id="toc-exercise-2-matrix-multiplication" class="nav-link" data-scroll-target="#exercise-2-matrix-multiplication">Exercise 2: Matrix Multiplication</a>
  <ul>
  <li><a href="#understanding-matrix-multiplication" id="toc-understanding-matrix-multiplication" class="nav-link" data-scroll-target="#understanding-matrix-multiplication">1. Understanding Matrix Multiplication</a></li>
  <li><a href="#matrix-multiplication-in-python" id="toc-matrix-multiplication-in-python" class="nav-link" data-scroll-target="#matrix-multiplication-in-python">2. Matrix Multiplication in Python</a></li>
  <li><a href="#matrix-multiplication-with-a-cuda-kernel" id="toc-matrix-multiplication-with-a-cuda-kernel" class="nav-link" data-scroll-target="#matrix-multiplication-with-a-cuda-kernel">3. Matrix Multiplication with a CUDA Kernel</a></li>
  <li><a href="#matrix-multiplication-in-cuda" id="toc-matrix-multiplication-in-cuda" class="nav-link" data-scroll-target="#matrix-multiplication-in-cuda">4. Matrix Multiplication in CUDA</a></li>
  <li><a href="#comparison-with-pytorchs-operator" id="toc-comparison-with-pytorchs-operator" class="nav-link" data-scroll-target="#comparison-with-pytorchs-operator">5. Comparison with PyTorch’s <code>@</code> Operator</a></li>
  <li><a href="#optimizing-cuda-kernels-with-shared-memory" id="toc-optimizing-cuda-kernels-with-shared-memory" class="nav-link" data-scroll-target="#optimizing-cuda-kernels-with-shared-memory">6. Optimizing CUDA Kernels with Shared Memory</a></li>
  <li><a href="#d-vs.-2d-blocks-and-threads" id="toc-d-vs.-2d-blocks-and-threads" class="nav-link" data-scroll-target="#d-vs.-2d-blocks-and-threads">7. 1D vs.&nbsp;2D Blocks and Threads</a></li>
  </ul></li>
  <li><a href="#conclusion-and-next-steps" id="toc-conclusion-and-next-steps" class="nav-link" data-scroll-target="#conclusion-and-next-steps">Conclusion and Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">GPU MODE Lecture 3: Getting Started With CUDA for Python Programmers</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">cuda</div>
    <div class="quarto-category">pytorch</div>
  </div>
  </div>

<div>
  <div class="description">
    Lecture #3 provides a beginner-friendly introduction to CUDA programming with PyTorch, demonstrating how to write and execute CUDA kernels within a Python environment for tasks like image processing and matrix multiplication.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 31, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>GPU MODE Lecture Notes</strong></a>: My notes from the <strong>GPU MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.</li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#exercise-1-rgb-to-grayscale-conversion">Exercise 1: RGB to Grayscale Conversion</a><br>
</li>
<li><a href="#exercise-2-matrix-multiplication">Exercise 2: Matrix Multiplication</a><br>
</li>
<li><a href="#conclusion-and-next-steps">Conclusion and Next Steps</a></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Resource Links">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Resource Links
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>YouTube Recording:</strong> <a href="https://www.youtube.com/watch?v=4sgKnKbR-WE&amp;t=2715s">Lecture 3: Getting Started With CUDA for Python Programmers</a></li>
<li><strong>Jupyter Notebook:</strong> <a href="https://github.com/cuda-mode/lectures/blob/main/lecture_003/pmpp.ipynb">lecture_003/pmpp.ipynb</a></li>
<li><strong>Google Colab:</strong> <a href="https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing">lecture_003/pmpp.ipynb</a>
<ul>
<li>Select the T4 GPU runtime in Colab.</li>
</ul></li>
<li><strong>Textbook:</strong> <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processors</a></li>
</ul>
</div>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li><strong>CUDA (Compute Unified Device Architecture):</strong> A parallel computing platform and programming model developed by NVIDIA for programming NVIDIA GPUs.</li>
<li>Enables high-performance computing and maximum flexibility.</li>
<li>Has a reputation for being difficult to learn, but can be approachable with the right techniques.</li>
<li><strong>Prerequisites:</strong>
<ul>
<li><strong>Basic PyTorch Knowledge</strong>:
<ul>
<li>Familiarity with tensors, indexing, and basic operations.</li>
<li><strong>Recommended:</strong> <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a> (especially Part 1).</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<ul>
<li><p><strong>Import Dependencies:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Core libraries for various functionalities</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch  <span class="co"># PyTorch library for deep learning</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os     <span class="co"># Operating system interfaces</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math   <span class="co"># Mathematical functions</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gzip   <span class="co"># Compression/decompression using gzip</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle <span class="co"># Object serialization</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting library</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For downloading files from URLs</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># File and directory handling</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Specific PyTorch imports</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor  <span class="co"># Tensor data structure</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Computer vision libraries</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision <span class="im">as</span> tv  <span class="co"># PyTorch's computer vision library</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> tvf  <span class="co"># Functional image transformations</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> io  <span class="co"># I/O operations for images and videos</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># For loading custom CUDA extensions</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.cpp_extension <span class="im">import</span> load_inline, CUDA_HOME</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the CUDA install path </span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(CUDA_HOME)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>/home/innom-dt/mambaforge/envs/cuda-mode</code></pre></li>
</ul>
</section>
<section id="exercise-1-rgb-to-grayscale-conversion" class="level2">
<h2 class="anchored" data-anchor-id="exercise-1-rgb-to-grayscale-conversion">Exercise 1: RGB to Grayscale Conversion</h2>
<section id="understanding-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-problem">1. Understanding the Problem</h3>
<ul>
<li><strong>Goal:</strong> Convert an RGB color image to a grayscale image.</li>
<li><strong>Formula:</strong> Grayscale (luminance) = 0.2989 * Red + 0.5870 * Green +0.1140 * Blue</li>
<li>This formula is a standard way to calculate luminance from RGB values.</li>
</ul>
</section>
<section id="loading-and-displaying-the-image" class="level3">
<h3 class="anchored" data-anchor-id="loading-and-displaying-the-image">2. Loading and Displaying the Image</h3>
<ul>
<li><p><strong>Image Source:</strong> A puppy image downloaded from a URL.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> download_image(url: <span class="bu">str</span>, path: Path) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Download an image from a given URL and save it to the specified path.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">        url (str): The URL of the image to download.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        path (Path): The local path where the image will be saved.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> path.exists():</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        urlretrieve(url, path)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># URL of the image to be downloaded</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the local path where the image will be saved</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>path_img <span class="op">=</span> Path(<span class="st">'puppy.jpg'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the image if it doesn't exist locally</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>download_image(url, path_img)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Loading:</strong> Use <code>torchvision.io.read_image</code> to load the image.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the downloaded image</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> io.read_image(<span class="st">'puppy.jpg'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Image Shape:</strong> The image is a 3D tensor with dimensions (channels, height, width).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the image (channels, height, width)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img.shape)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display a small portion of the image data (top-left corner)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>img[:<span class="dv">2</span>,:<span class="dv">3</span>,:<span class="dv">4</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>torch.Size([3, 1066, 1600])

tensor([[[117, 119, 117, 113],
         [119, 129, 129, 113],
         [130, 126, 122, 115]],

        [[ 83,  85,  85,  80],
         [ 85,  97,  97,  82],
         [ 98,  93,  89,  83]]], dtype=torch.uint8)</code></pre>
<ul>
<li><strong>Shape:</strong> (3, 1066, 1600) means 3 channels (RGB), 1066 rows (height), 1600 columns (width).</li>
</ul></li>
<li><p><strong>Data Type:</strong> The image pixels are stored as unsigned 8-bit integers (bytes).</p></li>
<li><p><strong>Displaying:</strong> Use a custom <code>showImage</code> function that uses <code>matplotlib</code> to display the image.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_img(x, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>), <span class="op">**</span>kwargs):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Display an image using matplotlib.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x (Tensor): The image tensor to display.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">        figsize (tuple): The size of the figure (width, height).</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        **kwargs: Additional keyword arguments to pass to plt.imshow().</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>figsize)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(x.shape) <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)  <span class="co"># Convert from CHW to HWC format</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x.cpu(), <span class="op">**</span>kwargs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the image to a height of 150 pixels while maintaining aspect ratio</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>img2 <span class="op">=</span> tvf.resize(img, <span class="dv">150</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dimensions of the resized image</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>ch, h, w <span class="op">=</span> img2.shape</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the channel count, height, width, and total number of pixels</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ch, h, w, h<span class="op">*</span>w)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resized image</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>show_img(img2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_8_1.png" class="img-fluid figure-img"></p>
<figcaption>resized image</figcaption>
</figure>
</div>
<ul>
<li>Resizes the image to a smaller size (smallest dimension = 150) for faster processing in the initial Python example.</li>
</ul></li>
</ul>
</section>
<section id="grayscale-conversion-in-python" class="level3">
<h3 class="anchored" data-anchor-id="grayscale-conversion-in-python">3. Grayscale Conversion in Python</h3>
<ul>
<li><p><strong>Approach:</strong> Iterate through every pixel and apply the gray scale formula.</p></li>
<li><p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_py(x):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert an RGB image to grayscale.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.Tensor): Input RGB image tensor of shape (C, H, W).</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: Grayscale image tensor of shape (H, W).</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    c, h, w <span class="op">=</span> x.shape</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> h <span class="op">*</span> w</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.flatten()  <span class="co"># Flatten the input tensor</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> torch.empty(n, dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)  <span class="co"># Initialize result tensor</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert RGB to grayscale using weighted sum</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        res[i] <span class="op">=</span> <span class="fl">0.2989</span> <span class="op">*</span> x[i] <span class="op">+</span> <span class="fl">0.5870</span> <span class="op">*</span> x[i <span class="op">+</span> n] <span class="op">+</span> <span class="fl">0.1140</span> <span class="op">*</span> x[i <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> n]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.view(h, w)  <span class="co"># Reshape result to original dimensions</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Flatten the image into a 1D vector to simplify indexing.
<ul>
<li>Flattening arranges the pixels in memory linearly
<ul>
<li>(channel 1, row 1, col 1, channel 1, row 1, col 2, etc.).</li>
</ul></li>
</ul></li>
<li>Calculate the luminance for each pixel using the formula and store it in an output vector.</li>
<li>Reshape the output vector back into a 2D matrix (height, width).</li>
</ul></li>
<li><p><strong>Performance:</strong> This Python implementation is very slow (726 ms for a small image).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert image to grayscale and display</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>img_g <span class="op">=</span> rgb2grey_py(img2)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>show_img(img_g, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 724 ms, sys: 51 μs, total: 724 ms
Wall time: 726 ms</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_13_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
</ul>
</section>
<section id="understanding-cuda-and-gpus" class="level3">
<h3 class="anchored" data-anchor-id="understanding-cuda-and-gpus">4. Understanding CUDA and GPUs</h3>
<ul>
<li><strong>CUDA Speedup:</strong> CUDA enables significant speedup by utilizing the parallel processing capabilities of GPUs.</li>
<li><strong>GPU Architecture:</strong>
<ul>
<li><strong>Streaming Multiprocessors (SMs):</strong> Independent processing units within a GPU (e.g., 82 in an RTX 3090).</li>
<li><strong>CUDA Cores:</strong> Parallel processing units within each SM (e.g., 128 per SM in an RTX 3090).</li>
<li>An RTX 3090 has 10,496 CUDA cores that can operate simultaneously.</li>
</ul></li>
<li><strong>CUDA Programming Model:</strong>
<ul>
<li><strong>Kernels:</strong> Functions designed to be executed in parallel on many CUDA cores.</li>
<li><strong>Parallel Execution:</strong> CUDA automatically distributes the kernel execution across multiple CUDA cores.</li>
<li><strong>Memory Modification:</strong> Kernels primarily modify memory; they do not return values directly.</li>
</ul></li>
</ul>
</section>
<section id="simulating-a-cuda-kernel-in-python" class="level3">
<h3 class="anchored" data-anchor-id="simulating-a-cuda-kernel-in-python">5. Simulating a CUDA Kernel in Python</h3>
<ul>
<li><p><strong>Limitations:</strong> This simulation does not run in parallel and is not faster than the original Python loop.</p></li>
<li><p><strong>Purpose:</strong> To demonstrate the conceptual behavior of a CUDA kernel.</p></li>
<li><p><strong><code>runKernel</code> Function:</strong> Simulates a CUDA kernel execution in Python using a single for loop.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_kernel(f, times, <span class="op">*</span>args):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Run a kernel function multiple times.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">        f (function): Kernel function to run.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">        times (int): Number of times to run the kernel.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Additional arguments to pass to the kernel function.</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(times):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        f(i, <span class="op">*</span>args)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Takes a function, the number of times to run it, and arguments as input.</li>
<li>Calls the function repeatedly with an index and the provided arguments.</li>
</ul></li>
<li><p><strong>Grayscale Conversion (Simulated Kernel)</strong>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_k(i, x, out, n):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Kernel function for RGB to grayscale conversion.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">        i (int): Current index.</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.Tensor): Flattened input RGB tensor.</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        out (torch.Tensor): Output grayscale tensor.</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        n (int): Number of pixels in a single channel.</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    out[i] <span class="op">=</span> <span class="fl">0.2989</span> <span class="op">*</span> x[i] <span class="op">+</span> <span class="fl">0.5870</span> <span class="op">*</span> x[i <span class="op">+</span> n] <span class="op">+</span> <span class="fl">0.1140</span> <span class="op">*</span> x[i <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> n]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_pyk(x):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert an RGB image to grayscale using a kernel approach.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.Tensor): Input RGB image tensor of shape (C, H, W).</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: Grayscale image tensor of shape (H, W).</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    c, h, w <span class="op">=</span> x.shape</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> h <span class="op">*</span> w</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.flatten()  <span class="co"># Flatten the input tensor</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> torch.empty(n, dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)  <span class="co"># Initialize result tensor</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the kernel function to convert RGB to grayscale</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    run_kernel(rgb2grey_k, h <span class="op">*</span> w, x, res, n)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.view(h, w)  <span class="co"># Reshape result to original dimensions</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert image to grayscale using kernel approach and display</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>img_g <span class="op">=</span> rgb2grey_pyk(img2)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>show_img(img_g, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_19_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><strong>Call <code>runKernel</code></strong>: Pass the grayscale conversion logic as the kernel function.</li>
<li><strong>No Parallelism</strong>: This simulation does not achieve true parallelism, but demonstrates the concept.</li>
</ul></li>
</ul>
</section>
<section id="cuda-blocks-and-threads" class="level3">
<h3 class="anchored" data-anchor-id="cuda-blocks-and-threads">6. CUDA Blocks and Threads</h3>
<ul>
<li><p><strong>Blocks and Threads:</strong> CUDA organizes kernel execution into blocks and threads.</p>
<ul>
<li><strong>Blocks:</strong> Groups of threads.</li>
<li><strong>Threads:</strong> Individual execution units within a block.</li>
</ul></li>
<li><p><strong>Kernel Runner with Blocks and Threads:</strong></p>
<ul>
<li>CUDA kernel execution is simulated using nested for loops (one for blocks, one for threads).</li>
<li>Each thread gets a unique index calculated from its block index and thread index.</li>
</ul></li>
<li><p><strong>Reason for Blocks and Threads:</strong></p>
<ul>
<li><strong>Shared Memory:</strong> Threads within a block share a small, fast memory space (shared memory).
<ul>
<li>256 KB Register File &amp; 128 KB of L1/Shared Memory in an RTX 3090</li>
</ul></li>
<li><strong>Synchronization:</strong> Threads within a block can synchronize their execution.</li>
<li><strong>Streaming Multiprocessor (SM) Execution:</strong> All threads within a block are executed on the same SM.</li>
</ul></li>
<li><p><strong>Choosing Block and Thread Dimensions:</strong></p>
<ul>
<li><strong>Threads per Block:</strong> Often set to 256 as a default.</li>
<li><strong>Number of Blocks:</strong> Calculated based on the total number of iterations needed and the threads per block.</li>
</ul></li>
<li><p><strong>Guard Block:</strong> An <code>if</code> statement within the kernel to prevent out-of-bounds memory access due to potential block size mismatches.</p></li>
<li><p><strong>Python Block Kernel:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blk_kernel(f, blocks, threads, <span class="op">*</span>args):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a GPU-like block and thread execution model.</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function emulates the behavior of GPU kernels by executing a given function</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    across a specified number of blocks and threads.</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">        f (function): The function to be executed in a block-thread manner.</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        blocks (int): The number of blocks to simulate.</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">        threads (int): The number of threads per block to simulate.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Additional arguments to be passed to the function f.</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(blocks):</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(threads):</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>            f(i, j, threads, <span class="op">*</span>args)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_bk(blockidx, threadidx, blockdim, x, out, n):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert RGB to greyscale for a single pixel in a block-thread execution model.</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function calculates the greyscale value for a single pixel using the formula:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    grey = 0.2989*R + 0.5870*G + 0.1140*B</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">        blockidx (int): The current block index.</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">        threadidx (int): The current thread index within the block.</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">        blockdim (int): The number of threads per block.</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.Tensor): The flattened input RGB image tensor.</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">        out (torch.Tensor): The output tensor to store the greyscale result.</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">        n (int): The total number of pixels in the image.</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> blockidx <span class="op">*</span> blockdim <span class="op">+</span> threadidx  <span class="co"># Calculate global index</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> n:  <span class="co"># Ensure we're within the image bounds</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate greyscale value using standard coefficients</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        out[i] <span class="op">=</span> <span class="fl">0.2989</span><span class="op">*</span>x[i] <span class="op">+</span> <span class="fl">0.5870</span><span class="op">*</span>x[i<span class="op">+</span>n] <span class="op">+</span> <span class="fl">0.1140</span><span class="op">*</span>x[i<span class="op">+</span><span class="dv">2</span><span class="op">*</span>n]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_pybk(x):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert an RGB image to greyscale using a block-thread execution model.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function simulates GPU-like parallel processing to convert an RGB image</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">    to greyscale efficiently.</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.Tensor): The input RGB image tensor with shape (3, height, width).</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: The resulting greyscale image tensor with shape (height, width).</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    c, h, w <span class="op">=</span> x.shape</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> h <span class="op">*</span> w</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.flatten()  <span class="co"># Flatten the input tensor</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare output tensor</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> torch.empty(n, dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up block and thread dimensions</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    threads <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> <span class="bu">int</span>(math.ceil(h<span class="op">*</span>w<span class="op">/</span>threads))</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the conversion using our simulated block-thread model</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    blk_kernel(rgb2grey_bk, blocks, threads, x, res, n)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.view(h, w)  <span class="co"># Reshape the result back to 2D</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to greyscale</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>img_g <span class="op">=</span> rgb2grey_pybk(img2)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the greyscale image</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>show_img(img_g, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_26_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
</ul>
</section>
<section id="cuda-setup-in-a-notebook" class="level3">
<h3 class="anchored" data-anchor-id="cuda-setup-in-a-notebook">7. CUDA Setup in a Notebook</h3>
<ul>
<li><p><strong>Setup:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set CUDA launch blocking to '1' for synchronous kernel launches</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'CUDA_LAUNCH_BLOCKING'</span>] <span class="op">=</span> <span class="st">'1'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>q wurlitzer ninja</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the wurlitzer extension for capturing and redirecting output</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext wurlitzer</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Set environment variable <code>CUDA_LAUNCH_BLOCKING=1</code> for debugging (slows down execution).</li>
<li>Install <code>ninja</code> (build tool) and <code>wurlitzer</code> (to enable printing from CUDA code in notebooks).</li>
</ul></li>
<li><p><strong><code>load_cuda</code> Function:</strong> A wrapper around <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline"><code>torch.utils.cpp_extension.load_inline</code></a> to simplify loading CUDA code.</p>
<ul>
<li><code>load_inline</code>: A powerful function for compiling and loading CUDA code directly from Python strings.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_cuda(cuda_src, cpp_src, funcs, opt<span class="op">=</span><span class="va">False</span>, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Load CUDA and C++ source code as a Python extension.</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function compiles and loads CUDA and C++ source code as a Python extension,</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    allowing for the use of custom CUDA kernels in Python.</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">        cuda_src (str): CUDA source code as a string.</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">        cpp_src (str): C++ source code as a string.</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co">        funcs (list): List of function names to be exposed from the extension.</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">        opt (bool, optional): Whether to enable optimization flags. Defaults to False.</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co">        verbose (bool, optional): Whether to print verbose output during compilation. Defaults to False.</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co">        module: Loaded Python extension module containing the compiled functions.</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use load_inline to compile and load the CUDA and C++ source code</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> load_inline(cuda_sources<span class="op">=</span>[cuda_src], cpp_sources<span class="op">=</span>[cpp_src], functions<span class="op">=</span>funcs,</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>                       extra_cuda_cflags<span class="op">=</span>[<span class="st">"-O2"</span>] <span class="cf">if</span> opt <span class="cf">else</span> [], verbose<span class="op">=</span>verbose, name<span class="op">=</span><span class="st">"inline_ext"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Common C++ Code:</strong> Define common C++ code, including header files, macros for checking tensor properties (CUDA, contiguous), and a macro for ceiling division.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define CUDA boilerplate code and utility macros</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>cuda_begin <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;torch/extension.h&gt;</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;stdio.h&gt;</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">#include &lt;c10/cuda/CUDAException.h&gt;</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to check if a tensor is a CUDA tensor</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to check if a tensor is contiguous in memory</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to check both CUDA and contiguity requirements</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="vs">// Utility function for ceiling division</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="vs">inline unsigned int cdiv</span><span class="kw">(</span><span class="vs">unsigned int a, unsigned int b</span><span class="kw">)</span><span class="vs"> { return </span><span class="kw">(</span><span class="vs">a </span><span class="op">+</span><span class="vs"> b - 1</span><span class="kw">)</span><span class="vs"> / b;}</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="writing-and-compiling-cuda-kernels" class="level3">
<h3 class="anchored" data-anchor-id="writing-and-compiling-cuda-kernels">8. Writing and Compiling CUDA Kernels</h3>
<ul>
<li><p><strong>Writing CUDA Kernels:</strong></p>
<ul>
<li>Use ChatGPT to convert Python kernel code to C++ CUDA code (or write it manually if comfortable with C++).</li>
<li>Adapt the code to CUDA syntax (e.g., <code>blockidx.x</code>, <code>blockdim.x</code>, <code>threadidx.x</code>, data types, semicolons).</li>
<li>Use <code>unsigned char*</code> for <code>uint8</code> (byte) data type in C++.</li>
<li><strong><code>__global__</code>:</strong> A CUDA keyword that indicates a kernel function callable from the CPU and executed on the GPU.</li>
</ul></li>
<li><p><strong>Calling CUDA Kernels:</strong></p>
<ul>
<li>Use triple angle brackets (<code>&lt;&lt;&lt;...&gt;&gt;&gt;</code>) to launch a CUDA kernel.</li>
<li>Specify the number of blocks and threads per block within the brackets.</li>
<li>Pass tensor arguments using <code>.data_ptr&lt;data_type&gt;()</code> to get C++ pointers.</li>
<li>Use <code>input.options()</code> to create output tensors with the same data type and device as the input tensor.</li>
<li>Use <code>TORCH_CHECK</code> to check for CUDA errors after kernel execution.</li>
</ul></li>
<li><p><strong>CUDA Source Code:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA kernel definition for RGB to grayscale conversion</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @brief CUDA kernel for converting RGB image to grayscale</span><span class="dv">.</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> </span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> This kernel applies the luminance formula to convert RGB values to grayscale:</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> Gray = 0</span><span class="dv">.</span><span class="vs">2989 </span><span class="op">*</span><span class="vs"> R </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">5870 </span><span class="op">*</span><span class="vs"> G </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">1140 </span><span class="op">*</span><span class="vs"> B</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param x Pointer to the input RGB image data </span><span class="kw">(</span><span class="vs">interleaved R, G, B channels</span><span class="kw">)</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param out Pointer to the output grayscale image data</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param n Total number of pixels in the image</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void rgb_to_grayscale_kernel</span><span class="kw">(</span><span class="vs">unsigned char</span><span class="op">*</span><span class="vs"> x, unsigned char</span><span class="op">*</span><span class="vs"> out, int n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate global thread ID</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="vs">    int i = blockIdx</span><span class="dv">.</span><span class="vs">x </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">x;</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Ensure we don't process beyond the image bounds</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">i &lt; n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Apply luminance formula</span><span class="dv">.</span><span class="vs"> Note: x</span><span class="pp">[i]</span><span class="vs">, x</span><span class="pp">[i+n]</span><span class="vs">, and x</span><span class="pp">[i+2*n]</span><span class="vs"> correspond to R, G, and B channels respectively</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="vs">        out</span><span class="pp">[i]</span><span class="vs"> = 0</span><span class="dv">.</span><span class="vs">2989 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i]</span><span class="vs"> </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">5870 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i+n]</span><span class="vs"> </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">1140 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i+2*n]</span><span class="vs">;</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @brief C</span><span class="op">++</span><span class="vs"> wrapper function to call the CUDA kernel for RGB to grayscale conversion</span><span class="dv">.</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> </span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> This function prepares the data and launches the CUDA kernel to perform the conversion</span><span class="dv">.</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param input Input RGB image tensor </span><span class="kw">(</span><span class="vs">expected shape: </span><span class="pp">[3, height, width]</span><span class="kw">)</span></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @return torch::Tensor Grayscale image tensor </span><span class="kw">(</span><span class="vs">shape: </span><span class="pp">[height, width]</span><span class="kw">)</span></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor rgb_to_grayscale</span><span class="kw">(</span><span class="vs">torch::Tensor input</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Verify input tensor properties </span><span class="kw">(</span><span class="vs">shape, type, etc</span><span class="dv">.</span><span class="kw">)</span></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">input</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Extract image dimensions</span></span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = input</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a><span class="vs">    int w = input</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">2</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="vs">    printf</span><span class="kw">(</span><span class="vs">"h</span><span class="op">*</span><span class="vs">w: %d</span><span class="op">*</span><span class="vs">%d</span><span class="ch">\n</span><span class="vs">", h, w</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Create output tensor for grayscale image</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::empty</span><span class="kw">(</span><span class="vs">{h,w}, input</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Set number of threads per block</span></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a><span class="vs">    int threads = 256;</span></span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch kernel with calculated grid size</span></span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a><span class="vs">    rgb_to_grayscale_kernel&lt;&lt;&lt;cdiv</span><span class="kw">(</span><span class="vs">w</span><span class="op">*</span><span class="vs">h,threads</span><span class="kw">)</span><span class="vs">, threads&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a><span class="vs">        input</span><span class="dv">.</span><span class="vs">data_ptr&lt;unsigned char&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;unsigned char&gt;</span><span class="kw">()</span><span class="vs">, w</span><span class="op">*</span><span class="vs">h</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check for any errors in kernel launch or execution</span></span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>C++ Source Code:</strong> Define a C++ header that lists the CUDA functions to be made available to Python.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># C++ function declaration for the RGB to grayscale conversion</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> <span class="st">"torch::Tensor rgb_to_grayscale(torch::Tensor input);"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Compiling with <code>load_cuda</code>:</strong> Use the <code>load_cuda</code> function to compile the CUDA and C++ code into a Python module.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the CUDA module with the defined kernel and C++ function</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [<span class="st">'rgb_to_grayscale'</span>], verbose<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Using /home/innom-dt/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Creating extension directory /home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/build.ninja...
/home/innom-dt/mambaforge/envs/cuda-mode/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1961: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module inline_ext...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module inline_ext...</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the path to the extension module</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Module Path: </span><span class="sc">{</span>module<span class="sc">.</span><span class="va">__file__</span><span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Module Path: /home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/inline_ext.so</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the content of the module folder as a Pandas DataFrame</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(Path(module.<span class="va">__file__</span>).parent.iterdir())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Files
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/main.o
</td>
</tr>
<tr>
<th>
1
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/.ninja_deps
</td>
</tr>
<tr>
<th>
2
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/build.ninja
</td>
</tr>
<tr>
<th>
3
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/cuda.cuda.o
</td>
</tr>
<tr>
<th>
4
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/cuda.cu
</td>
</tr>
<tr>
<th>
5
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/.ninja_log
</td>
</tr>
<tr>
<th>
6
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/inline_ext.so
</td>
</tr>
<tr>
<th>
7
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu124/inline_ext/main.cpp
</td>
</tr>
</tbody>
</table>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># List all non-private attributes of the loaded module</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>[o <span class="cf">for</span> o <span class="kw">in</span> <span class="bu">dir</span>(module) <span class="cf">if</span> o[<span class="dv">0</span>]<span class="op">!=</span><span class="st">'_'</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>['rgb_to_grayscale']</code></pre></li>
</ul>
</section>
<section id="running-the-cuda-kernel" class="level3">
<h3 class="anchored" data-anchor-id="running-the-cuda-kernel">9. Running the CUDA Kernel</h3>
<ul>
<li><p><strong>Ensure Contiguity and CUDA Device:</strong> Put the input tensor on the CUDA device and make it contiguous.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the input image is contiguous and move it to CUDA device</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>imgc <span class="op">=</span> img.contiguous().cuda()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Kernel Execution:</strong> Call the compiled CUDA function from Python, passing the input tensor and other required arguments.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Time the execution of the RGB to grayscale conversion</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> module.rgb_to_grayscale(imgc).cpu()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract height and width of the resulting grayscale image</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>h, w <span class="op">=</span> res.shape</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>h, w, h<span class="op">*</span>w  <span class="co"># Display height, width, and total number of pixels</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 740 μs, sys: 0 ns, total: 740 μs
Wall time: 636 μs

(1066, 1600, 1705600)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting grayscale image</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>show_img(res, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_43_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><strong>Performance:</strong> The CUDA kernel execution is significantly faster on the full size image than the Python implementation on the smaller image (636 μs vs.&nbsp;726 ms).</li>
</ul></li>
</ul>
</section>
</section>
<section id="exercise-2-matrix-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2-matrix-multiplication">Exercise 2: Matrix Multiplication</h2>
<section id="understanding-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="understanding-matrix-multiplication">1. Understanding Matrix Multiplication</h3>
<ul>
<li><p><strong>Definition:</strong> A fundamental linear algebra operation used extensively in deep learning.</p></li>
<li><p><strong>Process:</strong> Involves calculating the dot product of rows of one matrix with columns of another matrix.</p></li>
<li><p><strong>Example:</strong> Multiplying a 5x784 matrix with a 784x10 matrix results in a 5x10 matrix.</p></li>
</ul>
</section>
<section id="matrix-multiplication-in-python" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-in-python">2. Matrix Multiplication in Python</h3>
<ul>
<li><p><strong>MNIST Dataset:</strong> Uses the MNIST dataset of handwritten digits (28x28 images) for the example.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gzip,pickle</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># URL for downloading the MNIST dataset</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>MNIST_URL <span class="op">=</span> <span class="st">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a directory to store the data</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>path_data <span class="op">=</span> Path(<span class="st">'data'</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>path_data.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the path for the gzipped MNIST file</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>path_gz <span class="op">=</span> path_data<span class="op">/</span><span class="st">'mnist.pkl.gz'</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the MNIST dataset if it doesn't exist</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> path_gz.exists():</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    urlretrieve(MNIST_URL, path_gz)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and extract the MNIST data</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> gzip.<span class="bu">open</span>(path_gz, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load training, validation, and test sets (ignoring test set)</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    ((x_train, y_train), (x_valid, y_valid), _) <span class="op">=</span> pickle.load(f, encoding<span class="op">=</span><span class="st">'latin-1'</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert data to PyTorch tensors</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>x_train, y_train, x_valid, y_valid <span class="op">=</span> <span class="bu">map</span>(tensor, (x_train, y_train, x_valid, y_valid))</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print shape and data type of training data</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>x_train.shape, x_train.<span class="bu">type</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>(torch.Size([50000, 784]), 'torch.FloatTensor')</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape training images to 2D format (28x28 pixels)</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> x_train.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>imgs.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>torch.Size([50000, 28, 28])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first image in the dataset</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>show_img(imgs[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray_r'</span>, figsize<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_50_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
<li><p><strong>Weight Matrix:</strong> A randomly initialized weight matrix (784x10) is used.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize random weights for a neural network (784 input features, 10 output classes)</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> torch.randn(<span class="dv">784</span>, <span class="dv">10</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor([[-1.5256, -0.7502, -0.6540,  ..., -1.6091, -0.7121,  0.3037],
        [-0.7773, -0.2515, -0.2223,  ..., -1.1608,  0.6995,  0.1991],
        [ 0.8657,  0.2444, -0.6629,  ..., -1.4465,  0.0612, -0.6177],
        ...,
        [ 0.5063,  0.4656, -0.2634,  ...,  0.6452,  0.4298, -1.2936],
        [ 0.5171,  1.0315,  0.8120,  ..., -0.1046,  2.2588, -0.2793],
        [-1.4899,  0.3898, -0.5454,  ..., -0.1923, -0.5076,  0.5439]])</code></pre></li>
<li><p><strong>Base Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage and performance measurement</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="op">=</span> x_valid[:<span class="dv">5</span>]  <span class="co"># Select first 5 rows from x_valid</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>m2 <span class="op">=</span> weights  <span class="co"># Assign weights to m2</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print shapes of input matrices</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of m1: </span><span class="sc">{</span>m1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, Shape of m2: </span><span class="sc">{</span>m2<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Shape of m1: torch.Size([5, 784]), Shape of m2: torch.Size([784, 10])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Unpack dimensions of input matrices</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>ar, ac <span class="op">=</span> m1.shape  <span class="co"># Number of rows and columns in m1</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>br, bc <span class="op">=</span> m2.shape  <span class="co"># Number of rows and columns in m2</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dimensions: (ar=</span><span class="sc">{</span>ar<span class="sc">}</span><span class="ss">, ac=</span><span class="sc">{</span>ac<span class="sc">}</span><span class="ss">), (br=</span><span class="sc">{</span>br<span class="sc">}</span><span class="ss">, bc=</span><span class="sc">{</span>bc<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Dimensions: (ar=5, ac=784), (br=784, bc=10)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize result tensor</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> torch.zeros(ar, bc)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of result tensor: </span><span class="sc">{</span>t1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Shape of result tensor: torch.Size([5, 10])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform matrix multiplication using nested loops</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(ar):         <span class="co"># 5 iterations (rows of m1)</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(bc):     <span class="co"># 10 iterations (columns of m2)</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(ac): <span class="co"># 784 iterations (columns of m1 / rows of m2)</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>            t1[i, j] <span class="op">+=</span> m1[i, k] <span class="op">*</span> m2[k, j]</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of result after multiplication: </span><span class="sc">{</span>t1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Shape of result after multiplication: torch.Size([5, 10])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure numpy and torch print options for better readability</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>torch.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>, sci_mode<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the result</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result of matrix multiplication:"</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Result of matrix multiplication:
tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix multiplication implementation</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul(a, b):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication of two 2D tensors.</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="co">    a (torch.Tensor): First input tensor with shape (ar, ac)</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co">    b (torch.Tensor): Second input tensor with shape (br, bc)</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="co">    torch.Tensor: Resulting tensor after matrix multiplication with shape (ar, bc)</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Note: This function assumes that the number of columns in 'a' equals the number of rows in 'b'.</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>    (ar, ac), (br, bc) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.zeros(ar, bc)  <span class="co"># Initialize result tensor with zeros</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform matrix multiplication using nested loops</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(ar):</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(bc):</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(ac):</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>                c[i, j] <span class="op">+=</span> a[i, k] <span class="op">*</span> b[k, j]  <span class="co"># Accumulate the product of corresponding elements</span></span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure execution time of matmul function</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>time _ <span class="op">=</span> matmul(m1, m2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 443 ms, sys: 0 ns, total: 443 ms
Wall time: 443 ms</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate total number of operations</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>total_ops <span class="op">=</span> ar <span class="op">*</span> bc <span class="op">*</span> ac</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total number of operations: </span><span class="sc">{</span>total_ops<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Total number of operations: 39200</code></pre>
<ul>
<li>Uses nested loops to iterate through rows of the first matrix and columns of the second matrix.</li>
<li>Calculates the dot product for each element in the output matrix.</li>
<li><strong>Performance:</strong> Slow for large matrices (around 1 second for 39,200 innermost operations).</li>
</ul></li>
</ul>
</section>
<section id="matrix-multiplication-with-a-cuda-kernel" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-with-a-cuda-kernel">3. Matrix Multiplication with a CUDA Kernel</h3>
<ul>
<li><p><strong>Kernel Design:</strong></p>
<ul>
<li>The innermost loop (dot product calculation) is implemented as a CUDA kernel.</li>
<li>Each CUDA thread will calculate the dot product for one element in the output matrix.</li>
</ul></li>
<li><p><strong>2D Blocks and Threads:</strong></p>
<ul>
<li>CUDA allows for 2D (or even 3D) blocks and threads.</li>
<li>Blocks and threads are indexed using x and y coordinates (e.g., block(3, 4), thread(6, 12)).</li>
</ul></li>
<li><p><strong>Kernel Runner with 2D Blocks and Threads:</strong></p>
<ul>
<li>Uses four nested for loops to iterate through blocks and threads in both x and y dimensions.</li>
<li>Passes block and thread index information to the kernel.</li>
</ul></li>
<li><p><strong>CUDA Kernel Implementation:</strong></p>
<ul>
<li>Uses a guard block to prevent out-of-bounds memory access.</li>
<li>Calculates row and column indices from block and thread indices.</li>
<li>Performs the dot product calculation and stores the result in the output matrix.</li>
</ul></li>
<li><p><strong>2D Python Kernel:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> SimpleNamespace <span class="im">as</span> ns</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blk_kernel2d(f, blocks, threads, <span class="op">*</span>args):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a 2D block-based kernel execution.</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function emulates the behavior of a GPU kernel by iterating over blocks and threads</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="co">    in a 2D grid, calling the provided function 'f' for each thread.</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="co">        f (callable): The function to be executed for each thread.</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a><span class="co">        blocks (ns): Namespace object representing the number of blocks in x and y dimensions.</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a><span class="co">        threads (ns): Namespace object representing the number of threads per block in x and y dimensions.</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Additional arguments to be passed to the function 'f'.</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i0 <span class="kw">in</span> <span class="bu">range</span>(blocks.y):</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i1 <span class="kw">in</span> <span class="bu">range</span>(blocks.x):</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j0 <span class="kw">in</span> <span class="bu">range</span>(threads.y):</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> j1 <span class="kw">in</span> <span class="bu">range</span>(threads.x):</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Call the function 'f' for each thread, passing block and thread indices</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>                    f(ns(x<span class="op">=</span>i1, y<span class="op">=</span>i0), ns(x<span class="op">=</span>j1, y<span class="op">=</span>j0), threads, <span class="op">*</span>args)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_bk(blockidx, threadidx, blockdim, m, n, out, h, w, k):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication for a single thread in a block.</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function calculates the dot product for a specific element in the output matrix.</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="co">        blockidx (ns): Namespace object representing the block index.</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="co">        threadidx (ns): Namespace object representing the thread index within the block.</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="co">        blockdim (ns): Namespace object representing the block dimensions.</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="co">        m (array): Flattened input matrix 1.</span></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="co">        n (array): Flattened input matrix 2.</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a><span class="co">        out (array): Flattened output matrix.</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a><span class="co">        h (int): Height of the output matrix.</span></span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a><span class="co">        w (int): Width of the output matrix.</span></span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): Shared dimension of input matrices.</span></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate global row and column indices</span></span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> blockidx.y <span class="op">*</span> blockdim.y <span class="op">+</span> threadidx.y</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> blockidx.x <span class="op">*</span> blockdim.x <span class="op">+</span> threadidx.x</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if the current thread is within the output matrix dimensions</span></span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (r <span class="op">&gt;=</span> h <span class="kw">or</span> c <span class="op">&gt;=</span> w):</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform dot product calculation</span></span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>        o <span class="op">+=</span> m[r<span class="op">*</span>k<span class="op">+</span>i] <span class="op">*</span> n[i<span class="op">*</span>w<span class="op">+</span>c]</span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the result in the output matrix</span></span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a>    out[r<span class="op">*</span>w<span class="op">+</span>c] <span class="op">=</span> o</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d(m, n):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform 2D matrix multiplication using a block-based approach.</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function implements matrix multiplication by simulating a GPU-like</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co">    block and thread structure.</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a><span class="co">        m (torch.Tensor): Input matrix 1.</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n (torch.Tensor): Input matrix 2.</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: Result of the matrix multiplication.</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Raises:</span></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a><span class="co">        AssertionError: If the inner dimensions of the input matrices don't match.</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure that the inner dimensions of the matrices match</span></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span></span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the output matrix</span></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype)</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define thread-per-block dimensions</span></span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> ns(x<span class="op">=</span><span class="dv">16</span>, y<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the number of blocks needed</span></span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> ns(x<span class="op">=</span>math.ceil(w<span class="op">/</span>tpb.x), y<span class="op">=</span>math.ceil(h<span class="op">/</span>tpb.y))</span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the block kernel</span></span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a>    blk_kernel2d(matmul_bk, blocks, tpb,</span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a>                 m.flatten(), n.flatten(), output.flatten(), h, w, k)</span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-37"><a href="#cb67-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform matrix multiplication</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> matmul_2d(m1, m2)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the result (assuming 't1' is the expected output)</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>torch.isclose(t1, res).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre></li>
</ul>
</section>
<section id="matrix-multiplication-in-cuda" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-in-cuda">4. Matrix Multiplication in CUDA</h3>
<ul>
<li><p><strong>Optimized CPU Approach:</strong> Uses a broadcasting approach in Python for a faster CPU-based matrix multiplication to compare against the CUDA version.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul(a, b):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication of two 2D tensors.</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="co">        a (torch.Tensor): First input tensor with shape (ar, ac)</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co">        b (torch.Tensor): Second input tensor with shape (br, bc)</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: Result of matrix multiplication with shape (ar, bc)</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>    (ar, ac), (br, bc) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.zeros(ar, bc)  <span class="co"># Initialize result tensor with zeros</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(ar):</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiply each row of 'a' with all columns of 'b' and sum the results</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>        c[i] <span class="op">=</span> (a[i, :, <span class="va">None</span>] <span class="op">*</span> b).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the custom matmul function produces the same result as torch.matmul</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(t1, matmul(m1, m2)).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure the execution time of the custom matmul function</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>time _ <span class="op">=</span> matmul(m1, m2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 634 μs, sys: 266 μs, total: 900 μs
Wall time: 568 μs</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use x_train as m1 and perform matrix multiplication</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="op">=</span> x_train</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>tr <span class="op">=</span> matmul(m1, m2)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>tr.shape  <span class="co"># Display the shape of the resulting tensor</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>torch.Size([50000, 10])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure the execution time with whole input matrices</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>time _ <span class="op">=</span> matmul(m1, m2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 712 ms, sys: 0 ns, total: 712 ms
Wall time: 641 ms</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the total number of scalar multiplications in the matrix multiplication</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>ar, ac <span class="op">=</span> m1.shape</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>br, bc <span class="op">=</span> m2.shape</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>ar <span class="op">*</span> bc <span class="op">*</span> ac  <span class="co"># Number of multiplications: (rows of m1) * (cols of m2) * (cols of m1)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>392000000</code></pre></li>
<li><p><strong>CUDA Kernel Conversion:</strong> Convert the Python kernel to C++ CUDA code using ChatGPT (or manually).</p></li>
<li><p><strong>Implementing the CUDA Kernel:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># CUDA kernel for matrix multiplication</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>  cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="vs">  __global__ void matmul_k</span><span class="kw">(</span><span class="vs">float</span><span class="op">*</span><span class="vs"> m, float</span><span class="op">*</span><span class="vs"> n, float</span><span class="op">*</span><span class="vs"> out, int h, int w, int k</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Calculate global thread indices</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="vs">      int r = blockIdx</span><span class="dv">.</span><span class="vs">y </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">y </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">y;</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="vs">      int c = blockIdx</span><span class="dv">.</span><span class="vs">x </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">x;</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Boundary check</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="vs">      if </span><span class="kw">(</span><span class="vs">r &gt;= h </span><span class="cf">||</span><span class="vs"> c &gt;= w</span><span class="kw">)</span><span class="vs"> return;</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Perform dot product for this element</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="vs">      float o = 0;</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="vs">      for </span><span class="kw">(</span><span class="vs">int i = 0; i &lt; k; </span><span class="op">++</span><span class="vs">i</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a><span class="vs">          o </span><span class="op">+</span><span class="vs">= m</span><span class="pp">[r*k + i]</span><span class="vs"> </span><span class="op">*</span><span class="vs"> n</span><span class="pp">[i*w + c]</span><span class="vs">;</span></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="vs">      }</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a><span class="vs">      out</span><span class="pp">[r*w + c]</span><span class="vs"> = o;</span></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a><span class="vs">  }</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a><span class="vs">  /</span><span class="op">**</span></span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span><span class="vs"> Perform matrix multiplication using CUDA</span><span class="dv">.</span></span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span><span class="vs"> @param m First input tensor</span></span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span><span class="vs"> @param n Second input tensor</span></span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span><span class="vs"> @return Result of matrix multiplication</span></span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a><span class="vs">   </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a><span class="vs">  torch::Tensor matmul</span><span class="kw">(</span><span class="vs">torch::Tensor m, torch::Tensor n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a><span class="vs">      CHECK_INPUT</span><span class="kw">(</span><span class="vs">m</span><span class="kw">)</span><span class="vs">; CHECK_INPUT</span><span class="kw">(</span><span class="vs">n</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Extract dimensions</span></span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a><span class="vs">      int h = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a><span class="vs">      int w = n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a><span class="vs">      int k = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Ensure matrices are compatible for multiplication</span></span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a><span class="vs">      TORCH_CHECK</span><span class="kw">(</span><span class="vs">k == n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, "Size mismatch!"</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Initialize output tensor</span></span>
<span id="cb81-38"><a href="#cb81-38" aria-hidden="true" tabindex="-1"></a><span class="vs">      auto output = torch::zeros</span><span class="kw">(</span><span class="vs">{h, w}, m</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb81-39"><a href="#cb81-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-40"><a href="#cb81-40" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Define thread block and grid dimensions</span></span>
<span id="cb81-41"><a href="#cb81-41" aria-hidden="true" tabindex="-1"></a><span class="vs">      dim3 tpb</span><span class="kw">(</span><span class="vs">16, 16</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-42"><a href="#cb81-42" aria-hidden="true" tabindex="-1"></a><span class="vs">      dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;</span></span>
<span id="cb81-43"><a href="#cb81-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-44"><a href="#cb81-44" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Launch CUDA kernel</span></span>
<span id="cb81-45"><a href="#cb81-45" aria-hidden="true" tabindex="-1"></a><span class="vs">      matmul_k&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb81-46"><a href="#cb81-46" aria-hidden="true" tabindex="-1"></a><span class="vs">          m</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, n</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, h, w, k</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb81-47"><a href="#cb81-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-48"><a href="#cb81-48" aria-hidden="true" tabindex="-1"></a><span class="vs">      // Check for CUDA errors</span></span>
<span id="cb81-49"><a href="#cb81-49" aria-hidden="true" tabindex="-1"></a><span class="vs">      C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb81-50"><a href="#cb81-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-51"><a href="#cb81-51" aria-hidden="true" tabindex="-1"></a><span class="vs">      return output;</span></span>
<span id="cb81-52"><a href="#cb81-52" aria-hidden="true" tabindex="-1"></a><span class="vs">  }</span></span>
<span id="cb81-53"><a href="#cb81-53" aria-hidden="true" tabindex="-1"></a><span class="vs">  '''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># C++ function declaration for the CUDA kernel</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> <span class="st">"torch::Tensor matmul(torch::Tensor m, torch::Tensor n);"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Uses <code>DIM3</code> structures to specify the number of threads per block in x and y dimensions.</li>
<li>Calculates the number of blocks in x and y dimensions using ceiling division.</li>
<li>Launches the CUDA kernel with the calculated block and thread dimensions.</li>
</ul></li>
<li><p><strong>Performance:</strong> The CUDA kernel is significantly faster than both the Python implementation and the optimized CPU approach (1.21 ms vs.&nbsp;641 ms).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the CUDA module</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [<span class="st">'matmul'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>/home/innom-dt/mambaforge/envs/cuda-mode/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1961: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare input tensors for CUDA</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>m1c, m2c <span class="op">=</span> m1.contiguous().cuda(), m2.contiguous().cuda()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the result against a reference implementation</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(tr, module.matmul(m1c, m2c).cpu(), atol<span class="op">=</span><span class="fl">1e-5</span>).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure execution time of the CUDA matrix multiplication</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> module.matmul(m1c, m2c).cpu()</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>res.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 1.45 ms, sys: 138 μs, total: 1.59 ms
Wall time: 1.21 ms

torch.Size([50000, 10])</code></pre></li>
</ul>
</section>
<section id="comparison-with-pytorchs-operator" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-pytorchs-operator">5. Comparison with PyTorch’s <code>@</code> Operator</h3>
<ul>
<li><p><strong>PyTorch’s Matrix Multiplication:</strong> PyTorch provides the <code>@</code> operator for efficient matrix multiplication.</p></li>
<li><p><strong>Performance:</strong> PyTorch’s <code>@</code> operator is even faster than the custom CUDA kernel (636 μs vs.&nbsp;1.21 ms).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare CUDA matrix multiplication with PyTorch's built-in operation</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(tr, (m1c <span class="op">@</span> m2c).cpu(), atol<span class="op">=</span><span class="fl">1e-5</span>).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure the execution time of PyTorch's built-in matrix multiplication</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> _ <span class="op">=</span> (m1c <span class="op">@</span> m2c).cpu()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>636 μs ± 62.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
</ul>
</section>
<section id="optimizing-cuda-kernels-with-shared-memory" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-cuda-kernels-with-shared-memory">6. Optimizing CUDA Kernels with Shared Memory</h3>
<ul>
<li><strong>Shared Memory:</strong> A small, fast memory space shared by threads within a block.</li>
<li><strong>Optimization Potential:</strong> PyTorch’s matrix multiplication likely uses shared memory effectively to achieve higher performance.</li>
<li><strong>Caching:</strong> Shared memory can be used to cache frequently accessed data, reducing reliance on slower global memory.</li>
</ul>
</section>
<section id="d-vs.-2d-blocks-and-threads" class="level3">
<h3 class="anchored" data-anchor-id="d-vs.-2d-blocks-and-threads">7. 1D vs.&nbsp;2D Blocks and Threads</h3>
<ul>
<li><p><strong>Flexibility:</strong> CUDA allows for 1D, 2D, or 3D blocks and threads.</p></li>
<li><p><strong>Simplicity:</strong> In some cases, 1D blocks and threads can lead to simpler code compared to 2D or 3D.</p></li>
<li><p><strong>Choice:</strong> The choice between 1D, 2D, or 3D blocks and threads depends on the specific problem and coding preferences.</p></li>
<li><p><strong>2D Block Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> CUDA kernel function to convert RGB image to grayscale</span><span class="dv">.</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param x Input RGB image data</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param out Output grayscale image data</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param w Image width</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param h Image height</span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> Note: This function is executed on the GPU for each pixel in parallel</span><span class="dv">.</span></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void rgb_to_grayscale_kernel</span><span class="kw">(</span><span class="vs">unsigned char</span><span class="op">*</span><span class="vs"> x, unsigned char</span><span class="op">*</span><span class="vs"> out, int w, int h</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate the current pixel coordinates</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    int c = blockIdx</span><span class="dv">.</span><span class="vs">x </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">x;  // Column index</span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a><span class="vs">    int r = blockIdx</span><span class="dv">.</span><span class="vs">y </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">y </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">y;  // Row index</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">c &lt; w &amp;&amp; r &lt; h</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a><span class="vs">        int i = r </span><span class="op">*</span><span class="vs"> w </span><span class="op">+</span><span class="vs"> c;  // Linear index for the current pixel</span></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="vs">        int n = h </span><span class="op">*</span><span class="vs"> w;      // Total number of pixels in the image</span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Convert RGB to grayscale using the luminosity method</span></span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Weights: 0</span><span class="dv">.</span><span class="vs">2989 </span><span class="kw">(</span><span class="vs">Red</span><span class="kw">)</span><span class="vs">, 0</span><span class="dv">.</span><span class="vs">5870 </span><span class="kw">(</span><span class="vs">Green</span><span class="kw">)</span><span class="vs">, 0</span><span class="dv">.</span><span class="vs">1140 </span><span class="kw">(</span><span class="vs">Blue</span><span class="kw">)</span></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a><span class="vs">        out</span><span class="pp">[i]</span><span class="vs"> = 0</span><span class="dv">.</span><span class="vs">2989 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i]</span><span class="vs"> </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">5870 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i + n]</span><span class="vs"> </span><span class="op">+</span><span class="vs"> 0</span><span class="dv">.</span><span class="vs">1140 </span><span class="op">*</span><span class="vs"> x</span><span class="pp">[i + 2 * n]</span><span class="vs">;</span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> Convert an RGB image to grayscale using CUDA</span><span class="dv">.</span></span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param input Input RGB image tensor of shape </span><span class="kw">(</span><span class="vs">3, H, W</span><span class="kw">)</span></span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @return Grayscale image tensor of shape </span><span class="kw">(</span><span class="vs">H, W</span><span class="kw">)</span></span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> Note: This function launches the CUDA kernel to perform the conversion</span><span class="dv">.</span></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor rgb_to_grayscale</span><span class="kw">(</span><span class="vs">torch::Tensor input</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">input</span><span class="kw">)</span><span class="vs">;  // Verify input tensor properties</span></span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = input</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;  // Image height</span></span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a><span class="vs">    int w = input</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">2</span><span class="kw">)</span><span class="vs">;  // Image width</span></span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Create an empty tensor for the output grayscale image</span></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a><span class="vs">    torch::Tensor output = torch::empty</span><span class="kw">(</span><span class="vs">{h, w}, input</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Define the thread block dimensions</span></span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 tpb</span><span class="kw">(</span><span class="vs">16, 16</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate the number of blocks needed to cover the entire image</span></span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;</span></span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch the CUDA kernel</span></span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a><span class="vs">    rgb_to_grayscale_kernel&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a><span class="vs">        input</span><span class="dv">.</span><span class="vs">data_ptr&lt;unsigned char&gt;</span><span class="kw">()</span><span class="vs">,</span></span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a><span class="vs">        output</span><span class="dv">.</span><span class="vs">data_ptr&lt;unsigned char&gt;</span><span class="kw">()</span><span class="vs">,</span></span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a><span class="vs">        w, h</span></span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-57"><a href="#cb94-57" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check for any CUDA errors during kernel launch</span></span>
<span id="cb94-58"><a href="#cb94-58" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb94-59"><a href="#cb94-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-60"><a href="#cb94-60" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb94-61"><a href="#cb94-61" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb94-62"><a href="#cb94-62" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># C++ function declaration for the RGB to grayscale conversion</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> <span class="st">"torch::Tensor rgb_to_grayscale(torch::Tensor input);"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the CUDA module</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [<span class="st">'rgb_to_grayscale'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the input image to grayscale and move it to CPU</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> module.rgb_to_grayscale(imgc).cpu()</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting grayscale image</span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>show_img(res, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_89_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="conclusion-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-next-steps">Conclusion and Next Steps</h2>
<ul>
<li><strong>CUDA Accessibility:</strong> CUDA programming is becoming increasingly important for implementing advanced deep learning techniques.</li>
<li><strong>Python-based Development:</strong> Writing CUDA kernels can be made easier by starting with Python code and converting it to C++.</li>
<li><strong>Notebook Environment:</strong> CUDA development can be done effectively in Jupyter Notebooks.</li>
<li><strong>Local and Cloud Development:</strong> CUDA code can be run on local machines with GPUs or on cloud instances.</li>
<li><strong>Conda for CUDA Setup:</strong> Conda is a recommended tool for managing CUDA environments.
<ul>
<li><strong>Tutorial:</strong> <a href="../../../posts/cuda-python-setup-tutorial/ubuntu/">Setting Up CUDA for Python on Ubuntu</a></li>
</ul></li>
<li><strong>Further Learning:</strong>
<ul>
<li>Explore other CUDA mode lectures.</li>
<li>Try implementing projects like 4-bit quantization, flash attention, etc.</li>
<li>Read and understand other people’s CUDA code (e.g., flash attention, bits and bytes, GPTQ).</li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, an Applied AI Consultant and Educator.</p>
<p>Whether I’m writing an in-depth tutorial or sharing detailed notes, my goal is the same: to bring clarity to complex topics and find practical, valuable insights.</p>
<p>If you need a strategic partner with my approach to thinking and problem-solving for your AI project, I’m here to help. Let’s talk about de-risking your roadmap and building a real-world solution.</p>
<p>Start the conversation with my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> or learn more <a href="../../../about.html">about my approach</a>.</p>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/christianjmills\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>