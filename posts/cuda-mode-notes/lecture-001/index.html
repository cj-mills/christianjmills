<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-04-26">
<meta name="description" content="Lecture #1 provides a practical introduction to integrating and profiling custom CUDA kernels within PyTorch programs, using tools like load_inline, Triton, and NVIDIA Nsight Compute.">

<title>GPU MODE Lecture 1: How to profile CUDA kernels in PyTorch – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-d551e32f15e27e893f08ce3c93a41c1c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="GPU MODE Lecture 1: How to profile CUDA kernels in PyTorch – Christian Mills">
<meta property="og:description" content="Lecture #1 provides a practical introduction to integrating and profiling custom CUDA kernels within PyTorch programs, using tools like load_inline, Triton, and NVIDIA Nsight Compute.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="GPU MODE Lecture 1: How to profile CUDA kernels in PyTorch – Christian Mills">
<meta name="twitter:description" content="Lecture #1 provides a practical introduction to integrating and profiling custom CUDA kernels within PyTorch programs, using tools like load_inline, Triton, and NVIDIA Nsight Compute.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lecture-information" id="toc-lecture-information" class="nav-link active" data-scroll-target="#lecture-information">Lecture Information</a></li>
  <li><a href="#profiling-pytorch-square-with-autograd-profiler" id="toc-profiling-pytorch-square-with-autograd-profiler" class="nav-link" data-scroll-target="#profiling-pytorch-square-with-autograd-profiler">Profiling PyTorch Square with Autograd Profiler</a>
  <ul>
  <li><a href="#torch-autograd-profiler" id="toc-torch-autograd-profiler" class="nav-link" data-scroll-target="#torch-autograd-profiler">Torch Autograd Profiler</a></li>
  <li><a href="#profile-squaring-a-pytorch-tensor" id="toc-profile-squaring-a-pytorch-tensor" class="nav-link" data-scroll-target="#profile-squaring-a-pytorch-tensor">Profile Squaring a PyTorch Tensor</a>
  <ul class="collapse">
  <li><a href="#analyzing-torch.square-vs.-manual-multiplication-and-pythons-power-function" id="toc-analyzing-torch.square-vs.-manual-multiplication-and-pythons-power-function" class="nav-link" data-scroll-target="#analyzing-torch.square-vs.-manual-multiplication-and-pythons-power-function">Analyzing <code>torch.square()</code> vs.&nbsp;manual multiplication and Python’s power function</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#pytorch-profiler" id="toc-pytorch-profiler" class="nav-link" data-scroll-target="#pytorch-profiler">PyTorch Profiler</a>
  <ul>
  <li><a href="#profiling-the-torch.square-function" id="toc-profiling-the-torch.square-function" class="nav-link" data-scroll-target="#profiling-the-torch.square-function">Profiling the <code>torch.square()</code> function</a>
  <ul class="collapse">
  <li><a href="#default-usage" id="toc-default-usage" class="nav-link" data-scroll-target="#default-usage">Default usage</a></li>
  <li><a href="#non-default-profiler-schedule" id="toc-non-default-profiler-schedule" class="nav-link" data-scroll-target="#non-default-profiler-schedule">Non-default profiler schedule</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#integrating-cuda-kernels-in-pytorch" id="toc-integrating-cuda-kernels-in-pytorch" class="nav-link" data-scroll-target="#integrating-cuda-kernels-in-pytorch">Integrating CUDA Kernels in PyTorch</a>
  <ul>
  <li><a href="#hello-world-example" id="toc-hello-world-example" class="nav-link" data-scroll-target="#hello-world-example">Hello World Example</a></li>
  <li><a href="#custom-cuda-kernel-for-square-operation" id="toc-custom-cuda-kernel-for-square-operation" class="nav-link" data-scroll-target="#custom-cuda-kernel-for-square-operation">Custom CUDA kernel for Square Operation</a></li>
  <li><a href="#alternatives" id="toc-alternatives" class="nav-link" data-scroll-target="#alternatives">Alternatives</a></li>
  </ul></li>
  <li><a href="#triton" id="toc-triton" class="nav-link" data-scroll-target="#triton">Triton</a>
  <ul>
  <li><a href="#code-example-square-operation-using-triton" id="toc-code-example-square-operation-using-triton" class="nav-link" data-scroll-target="#code-example-square-operation-using-triton">Code Example: Square operation using Triton</a></li>
  <li><a href="#triton-debugger" id="toc-triton-debugger" class="nav-link" data-scroll-target="#triton-debugger">Triton debugger</a></li>
  <li><a href="#code-example-exploration-of-generated-square-kernel-ptx" id="toc-code-example-exploration-of-generated-square-kernel-ptx" class="nav-link" data-scroll-target="#code-example-exploration-of-generated-square-kernel-ptx">Code Example: Exploration of generated Square Kernel PTX</a></li>
  <li><a href="#code-example-auto-generate-a-triton-kernel-using-torch.compile" id="toc-code-example-auto-generate-a-triton-kernel-using-torch.compile" class="nav-link" data-scroll-target="#code-example-auto-generate-a-triton-kernel-using-torch.compile">Code Example: Auto-generate a triton kernel using <code>torch.compile()</code></a></li>
  </ul></li>
  <li><a href="#optimization-profiling-with-nsight-compute" id="toc-optimization-profiling-with-nsight-compute" class="nav-link" data-scroll-target="#optimization-profiling-with-nsight-compute">Optimization &amp; Profiling with Nsight Compute</a>
  <ul>
  <li><a href="#logs-example" id="toc-logs-example" class="nav-link" data-scroll-target="#logs-example">Logs Example:</a></li>
  <li><a href="#visual-profiler" id="toc-visual-profiler" class="nav-link" data-scroll-target="#visual-profiler">Visual Profiler</a></li>
  <li><a href="#moving-from-pytorch-to-triton-to-cuda" id="toc-moving-from-pytorch-to-triton-to-cuda" class="nav-link" data-scroll-target="#moving-from-pytorch-to-triton-to-cuda">Moving from PyTorch to Triton to CUDA</a></li>
  </ul></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa">Q&amp;A</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">GPU MODE Lecture 1: How to profile CUDA kernels in PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">cuda</div>
    <div class="quarto-category">pytorch</div>
  </div>
  </div>

<div>
  <div class="description">
    Lecture #1 provides a practical introduction to integrating and profiling custom CUDA kernels within PyTorch programs, using tools like <code>load_inline</code>, Triton, and NVIDIA Nsight Compute.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>GPU MODE Lecture Notes</strong></a>: My notes from the <strong>GPU MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.<br>
</li>
</ul>
</div>
</div>
<ul>
<li><a href="#lecture-information">Lecture Information</a></li>
<li><a href="#profiling-pytorch-square-with-autograd-profiler">Profiling PyTorch Square with Autograd Profiler</a></li>
<li><a href="#pytorch-profiler">PyTorch Profiler</a></li>
<li><a href="#integrating-cuda-kernels-in-pytorch">Integrating CUDA Kernels in PyTorch</a></li>
<li><a href="#triton">Triton</a></li>
<li><a href="#optimization-profiling-with-nsight-compute">Optimization &amp; Profiling with Nsight Compute</a></li>
<li><a href="#qa">Q&amp;A</a></li>
</ul>
<section id="lecture-information" class="level2">
<h2 class="anchored" data-anchor-id="lecture-information">Lecture Information</h2>
<ul>
<li><strong>Speaker:</strong> Mark Saroufim</li>
<li><strong>Topic:</strong> Integrate and profile custom CUDA kernels in PyTorch programs.</li>
<li><strong>Resources:</strong>
<ul>
<li><strong>Lecture Slides:</strong> <a href="https://docs.google.com/presentation/d/110dnMW94LX1ySWxu9La17AVUxjgSaQDLOotFC3BZZD4/edit#slide=id.p">CUDA Mode: Lecture 1</a></li>
<li><strong>Textbook:</strong> <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processors</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/cuda-mode/lectures/tree/main/lecture_001">GPU MODE Lecture 1</a></li>
<li><strong>Discord Channel:</strong> <a href="https://discord.gg/cudamode">GPU MODE</a></li>
<li><strong>YouTube Channel:</strong> <a href="https://www.youtube.com/@CUDAMODE">GPU MODE</a></li>
</ul></li>
</ul>
</section>
<section id="profiling-pytorch-square-with-autograd-profiler" class="level2">
<h2 class="anchored" data-anchor-id="profiling-pytorch-square-with-autograd-profiler">Profiling PyTorch Square with Autograd Profiler</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=YtsNCWs9HVY4oJoT&amp;t=597">9:57</a></li>
<li>Profiling provides a way to visually understand “in a blackbox kind of way”
<ul>
<li>Don’t need to know all the details of how a GPU or CUDA works to do something useful with it</li>
</ul></li>
</ul>
<section id="torch-autograd-profiler" class="level3">
<h3 class="anchored" data-anchor-id="torch-autograd-profiler">Torch Autograd Profiler</h3>
<ul>
<li>Provides insights into kernel execution time on CPU and GPU, number of calls, and dependencies.</li>
<li>CUDA is asynchronous, requiring specialized profiling tools
<ul>
<li>Can’t use the Python time module
<ul>
<li>Would only measure the overhead to launch the CUDA kernel, not the time it takes to run the kernel</li>
</ul></li>
<li>Need to use <code>torch.cuda.Event</code>
<ul>
<li>Start and end events</li>
</ul></li>
<li>Call <code>torch.cuda.synchronize()</code> to ensure all operations finish before measuring performance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Class</th>
<th>Description</th>
<th>Documentation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>Event</code></td>
<td>CUDA events are synchronization markers that can be used to monitor the device’s progress, to accurately measure timing, and to synchronize CUDA streams.</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#torch.cuda.Event">link</a></td>
</tr>
<tr class="even">
<td><code>Profiler</code></td>
<td>A profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU.</td>
<td><a href="https://pytorch.org/docs/stable/autograd.html#profiler">link</a></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled" title="Warmups">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Warmups
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Problem:</strong> The first time you call CUDA in a PyTorch function, it’s going to initialize the CUDA context, distorting performance measurements</li>
<li><strong>Solution:</strong> Run the target function a few times to initialize the CUDA context before taking performance measurements.</li>
</ul>
</div>
</div>
</section>
<section id="profile-squaring-a-pytorch-tensor" class="level3">
<h3 class="anchored" data-anchor-id="profile-squaring-a-pytorch-tensor">Profile Squaring a PyTorch Tensor</h3>
<ul>
<li>Profiling squaring a PyTorch tensor using the Python multiplication operation, the <code>torch.square</code> method, and the Python power operation.</li>
</ul>
<section id="analyzing-torch.square-vs.-manual-multiplication-and-pythons-power-function" class="level4">
<h4 class="anchored" data-anchor-id="analyzing-torch.square-vs.-manual-multiplication-and-pythons-power-function">Analyzing <code>torch.square()</code> vs.&nbsp;manual multiplication and Python’s power function</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> time_pytorch_function(func, <span class="bu">input</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Measure the execution time of a PyTorch function.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        func (callable): The PyTorch function to be timed.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        input: The input to the function.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        float: The execution time in milliseconds.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Since CUDA is asynchronous, we can't use Python's time module to measure time.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instead, we use PyTorch's CUDA events to measure the time.</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create a start event</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create an end event</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a warmup to ensure the GPU is ready</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        func(<span class="bu">input</span>)  <span class="co"># Run the function 5 times to warm up the GPU</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start the timer</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    start.record()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    func(<span class="bu">input</span>)  <span class="co"># Run the function to be timed</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    end.record()  <span class="co"># Stop the timer</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()  <span class="co"># Wait for the kernel to finish</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> start.elapsed_time(end)  <span class="co"># Return the elapsed time in milliseconds</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a large sample tensor</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">10000</span>, <span class="dv">10000</span>).cuda()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define function to square tensor using manual multiplication operation</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> square_2(a):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Square the input using multiplication.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        a: The input value to be squared.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        The squared value of the input.</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> a  <span class="co"># Return the square of the input using multiplication</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define function to square tensor using Python's power function</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> square_3(a):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Square the input using exponentiation.</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">        a: The input value to be squared.</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">        The squared value of the input.</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">**</span> <span class="dv">2</span>  <span class="co"># Return the square of the input using exponentiation</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Profiling torch.square"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Profile each function using the PyTorch profiler to measure performance</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># and identify potential bottlenecks</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.autograd.profiler.profile(use_cuda<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> prof:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Create a PyTorch profiler context manager to profile the torch.square function.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): If True, uses CUDA for profiling (if available).</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Profile torch.square function</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    torch.square(b)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the profiling results, sorted by CUDA time and limited to the top 10 rows</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cuda_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>=============
Profiling torch.square
=============

STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:324] Completed Stage: Post Processing</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 13%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Self CPU %</th>
<th>Self CPU</th>
<th>CPU total %</th>
<th>CPU total</th>
<th>CPU time avg</th>
<th>Self CUDA</th>
<th>Self CUDA %</th>
<th>CUDA total</th>
<th>CUDA time avg</th>
<th># of Calls</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>aten::square</td>
<td>1.63%</td>
<td>15.000us</td>
<td>9.22%</td>
<td>85.000us</td>
<td>85.000us</td>
<td>15.000us</td>
<td>1.56%</td>
<td>962.000us</td>
<td>962.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>aten::pow</td>
<td>5.53%</td>
<td>51.000us</td>
<td>7.38%</td>
<td>68.000us</td>
<td>68.000us</td>
<td>941.000us</td>
<td>97.82%</td>
<td>947.000us</td>
<td>947.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>aten::result_type</td>
<td>0.11%</td>
<td>1.000us</td>
<td>0.11%</td>
<td>1.000us</td>
<td>1.000us</td>
<td>4.000us</td>
<td>0.42%</td>
<td>4.000us</td>
<td>4.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>aten::to</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2.000us</td>
<td>0.21%</td>
<td>2.000us</td>
<td>2.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>cudaEventRecord</td>
<td>1.30%</td>
<td>12.000us</td>
<td>1.30%</td>
<td>12.000us</td>
<td>1.500us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>8</td>
</tr>
<tr class="even">
<td>cudaLaunchKernel</td>
<td>1.41%</td>
<td>13.000us</td>
<td>1.41%</td>
<td>13.000us</td>
<td>13.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>cudaDeviceSynchronize</td>
<td>90.02%</td>
<td>830.000us</td>
<td>90.02%</td>
<td>830.000us</td>
<td>830.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
</tr>
<tr class="odd">
<td>Self CPU time total: <br>Self CUDA time total:</td>
<td>922.000us<br>962.000us</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Profiling Results">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Profiling Results
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The table shows the underlying C++ functions executed when calling the <code>torch.square()</code> function in Python</li>
<li>The <code>torch.square()</code> function executes the <code>aten::square</code> C++ function
<ul>
<li>The <code>aten::square</code> calls the <code>aten::pow</code> function with a value of 2</li>
<li>Source Code: <a href="https://github.com/pytorch/pytorch/blob/b8b04b26fbf160874f7f1a9db61e49801fd4fcbe/aten/src/ATen/native/UnaryOps.cpp#L771">aten::square</a></li>
</ul></li>
</ul>
</div>
</div>
<hr>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Profiling a * a"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use PyTorch's autograd profiler to profile the execution of the square_2 function</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># with CUDA enabled</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.autograd.profiler.profile(use_cuda<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> prof:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the square_2 function and profile its execution</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    square_2(b)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the profiling results, sorted by CUDA time and limited to the top 10 rows</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cuda_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>=============
Profiling a * a
=============


STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:324] Completed Stage: Post Processing</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 13%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Self CPU %</th>
<th>Self CPU</th>
<th>CPU total %</th>
<th>CPU total</th>
<th>CPU time avg</th>
<th>Self CUDA</th>
<th>Self CUDA %</th>
<th>CUDA total</th>
<th>CUDA time avg</th>
<th># of Calls</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>aten::mul</td>
<td>5.26%</td>
<td>40.000us</td>
<td>9.99%</td>
<td>76.000us</td>
<td>76.000us</td>
<td>851.000us</td>
<td>100.00%</td>
<td>851.000us</td>
<td>851.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>cudaEventRecord</td>
<td>1.71%</td>
<td>13.000us</td>
<td>1.71%</td>
<td>13.000us</td>
<td>6.500us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>cudaLaunchKernel</td>
<td>4.73%</td>
<td>36.000us</td>
<td>4.73%</td>
<td>36.000us</td>
<td>36.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>cudaDeviceSynchronize</td>
<td>88.30%</td>
<td>672.000us</td>
<td>88.30%</td>
<td>672.000us</td>
<td>672.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
</tr>
<tr class="even">
<td>Self CPU time total: <br>Self CUDA time total:</td>
<td>761.000us<br>851.000us</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Profiling Results">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Profiling Results
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The manual multiplication operation executes the <code>aten::mul</code> C++ function</li>
</ul>
</div>
</div>
<hr>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Profiling a ** 2"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"============="</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use PyTorch's autograd profiler to profile the execution of the square_3 function</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># with CUDA enabled</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.autograd.profiler.profile(use_cuda<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> prof:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the square_3 function and profile its execution</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    square_3(b)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the profiling results, sorted by CUDA time and limited to the top 10 rows</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cuda_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>=============
Profiling a ** 2
=============


STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-24 18:37:25 1254869:1254869 ActivityProfilerController.cpp:324] Completed Stage: Post Processing</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 13%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Self CPU %</th>
<th>Self CPU</th>
<th>CPU total %</th>
<th>CPU total</th>
<th>CPU time avg</th>
<th>Self CUDA</th>
<th>Self CUDA %</th>
<th>CUDA total</th>
<th>CUDA time avg</th>
<th># of Calls</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>aten::pow</td>
<td>6.34%</td>
<td>47.000us</td>
<td>8.64%</td>
<td>64.000us</td>
<td>64.000us</td>
<td>855.000us</td>
<td>99.77%</td>
<td>857.000us</td>
<td>857.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>aten::result_type</td>
<td>0.13%</td>
<td>1.000us</td>
<td>0.13%</td>
<td>1.000us</td>
<td>1.000us</td>
<td>1.000us</td>
<td>0.12%</td>
<td>1.000us</td>
<td>1.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>aten::to</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1.000us</td>
<td>0.12%</td>
<td>1.000us</td>
<td>1.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>cudaEventRecord</td>
<td>1.89%</td>
<td>14.000us</td>
<td>1.89%</td>
<td>14.000us</td>
<td>2.333us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>6</td>
</tr>
<tr class="odd">
<td>cudaLaunchKernel</td>
<td>1.75%</td>
<td>13.000us</td>
<td>1.75%</td>
<td>13.000us</td>
<td>13.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>cudaDeviceSynchronize</td>
<td>89.88%</td>
<td>666.000us</td>
<td>89.88%</td>
<td>666.000us</td>
<td>666.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="odd">
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
</tr>
<tr class="even">
<td>Self CPU time total: <br>Self CUDA time total:</td>
<td>741.000us<br>857.000us</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Profiling Results">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Profiling Results
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The the power function executes the same <code>aten::pow</code> C++ function called by <code>aten::square</code></li>
</ul>
</div>
</div>
<hr>
</section>
</section>
</section>
<section id="pytorch-profiler" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-profiler">PyTorch Profiler</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=PBWrtlYHwvtdtVTZ&amp;t=842">14:02</a></li>
<li>PyTorch Profiler:
<ul>
<li>Documentation: (<a href="https://pytorch.org/docs/stable/profiler.html">link</a>)</li>
<li>Visual profiler generating Chrome traces for detailed analysis.
<ul>
<li>Creates a JSON file, which you drag and drop into the Chrome browser at the following link:
<ul>
<li><a href="chrome://tracing/">chrome://tracing/</a></li>
</ul></li>
</ul></li>
<li>Provides information on memory copies, kernel launches, and flow events.</li>
<li>Does not provide information on the kernel performance or how to improve it.</li>
</ul></li>
</ul>
<section id="profiling-the-torch.square-function" class="level3">
<h3 class="anchored" data-anchor-id="profiling-the-torch.square-function">Profiling the <code>torch.square()</code> function</h3>
<ul>
<li>Send tensor to GPU</li>
<li>Compute the square of the tensor</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.profiler <span class="im">import</span> profile, ProfilerActivity</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="default-usage" class="level4">
<h4 class="anchored" data-anchor-id="default-usage">Default usage</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Default way to use profiler</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profile(activities<span class="op">=</span>[ProfilerActivity.CPU, ProfilerActivity.CUDA]) <span class="im">as</span> prof:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> torch.square(torch.randn(<span class="dv">10000</span>, <span class="dv">10000</span>).cuda())</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>prof.export_chrome_trace(<span class="st">"default_trace.json"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>STAGE:2024-04-25 14:18:13 33490:33490 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-04-25 14:18:18 33490:33490 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-25 14:18:18 33490:33490 ActivityProfilerController.cpp:324] Completed Stage: Post Processing</code></pre>
</section>
<section id="non-default-profiler-schedule" class="level4">
<h4 class="anchored" data-anchor-id="non-default-profiler-schedule">Non-default profiler schedule</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">## With warmup and skip</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Non-default profiler schedule allows user to turn profiler on and off</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># on different iterations of the training loop;</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># trace_handler is called every time a new trace becomes available</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trace_handler(prof):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(prof.key_averages().table(</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        sort_by<span class="op">=</span><span class="st">"self_cuda_time_total"</span>, row_limit<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    prof.export_chrome_trace(<span class="st">"non_default_trace_"</span> <span class="op">+</span> <span class="bu">str</span>(prof.step_num) <span class="op">+</span> <span class="st">".json"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.profiler.profile(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    activities<span class="op">=</span>[</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        torch.profiler.ProfilerActivity.CPU,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        torch.profiler.ProfilerActivity.CUDA,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In this example with wait=1, warmup=1, active=2, repeat=1,</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># profiler will skip the first step/iteration,</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># start warming up on the second, record</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the third and the forth iterations,</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># after which the trace will become available</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and on_trace_ready (when set) is called;</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the cycle repeats starting with the next step</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    schedule<span class="op">=</span>torch.profiler.schedule(</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        wait<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        warmup<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        active<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        repeat<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    on_trace_ready<span class="op">=</span>trace_handler</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># used when outputting for tensorboard</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    ) <span class="im">as</span> p:</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            torch.square(torch.randn(<span class="dv">10000</span>, <span class="dv">10000</span>).cuda())</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># send a signal to the profiler that the next iteration has started</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            p.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>STAGE:2024-04-25 14:18:19 33490:33490 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-04-25 14:18:19 33490:33490 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-25 14:18:19 33490:33490 ActivityProfilerController.cpp:324] Completed Stage: Post Processing</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Self CPU %</th>
<th>Self CPU</th>
<th>CPU total %</th>
<th>CPU total</th>
<th>CPU time avg</th>
<th>Self CUDA</th>
<th>Self CUDA %</th>
<th>CUDA total</th>
<th>CUDA time avg</th>
<th># of Calls</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>aten::copy_</td>
<td>0.00%</td>
<td>40.000us</td>
<td>7.82%</td>
<td>66.271ms</td>
<td>33.136ms</td>
<td>66.023ms</td>
<td>97.48%</td>
<td>66.023ms</td>
<td>33.011ms</td>
<td>2</td>
</tr>
<tr class="even">
<td>Memcpy HtoD (Pageable -&gt; Device)</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>66.023ms</td>
<td>97.48%</td>
<td>66.023ms</td>
<td>33.011ms</td>
<td>2</td>
</tr>
<tr class="odd">
<td>aten::pow</td>
<td>0.01%</td>
<td>77.000us</td>
<td>0.01%</td>
<td>120.000us</td>
<td>60.000us</td>
<td>1.704ms</td>
<td>2.52%</td>
<td>1.704ms</td>
<td>852.000us</td>
<td>2</td>
</tr>
<tr class="even">
<td>void at::native::vectorized_elementwise_kernel&lt;4, at…</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1.704ms</td>
<td>2.52%</td>
<td>1.704ms</td>
<td>852.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>ProfilerStep*</td>
<td>2.12%</td>
<td>17.998ms</td>
<td>99.90%</td>
<td>846.892ms</td>
<td>423.446ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>67.727ms</td>
<td>33.864ms</td>
<td>2</td>
</tr>
<tr class="even">
<td>aten::randn</td>
<td>0.00%</td>
<td>25.000us</td>
<td>89.94%</td>
<td>762.421ms</td>
<td>381.211ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>aten::empty</td>
<td>0.00%</td>
<td>37.000us</td>
<td>0.00%</td>
<td>37.000us</td>
<td>18.500us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="even">
<td>aten::normal_</td>
<td>89.93%</td>
<td>762.359ms</td>
<td>89.93%</td>
<td>762.359ms</td>
<td>381.180ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>aten::to</td>
<td>0.00%</td>
<td>25.000us</td>
<td>7.83%</td>
<td>66.347ms</td>
<td>16.587ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>66.023ms</td>
<td>16.506ms</td>
<td>4</td>
</tr>
<tr class="even">
<td>aten::_to_copy</td>
<td>0.00%</td>
<td>23.000us</td>
<td>7.82%</td>
<td>66.322ms</td>
<td>33.161ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>66.023ms</td>
<td>33.011ms</td>
<td>2</td>
</tr>
<tr class="odd">
<td>aten::empty_strided</td>
<td>0.00%</td>
<td>28.000us</td>
<td>0.00%</td>
<td>28.000us</td>
<td>14.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="even">
<td>cudaMemcpyAsync</td>
<td>7.81%</td>
<td>66.193ms</td>
<td>7.81%</td>
<td>66.193ms</td>
<td>33.096ms</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>cudaStreamSynchronize</td>
<td>0.00%</td>
<td>38.000us</td>
<td>0.00%</td>
<td>38.000us</td>
<td>19.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="even">
<td>aten::square</td>
<td>0.00%</td>
<td>6.000us</td>
<td>0.01%</td>
<td>126.000us</td>
<td>63.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>1.704ms</td>
<td>852.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>aten::result_type</td>
<td>0.00%</td>
<td>2.000us</td>
<td>0.00%</td>
<td>2.000us</td>
<td>1.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="even">
<td>cudaLaunchKernel</td>
<td>0.00%</td>
<td>41.000us</td>
<td>0.00%</td>
<td>41.000us</td>
<td>20.500us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>2</td>
</tr>
<tr class="odd">
<td>cudaDeviceSynchronize</td>
<td>0.10%</td>
<td>841.000us</td>
<td>0.10%</td>
<td>841.000us</td>
<td>841.000us</td>
<td>0.000us</td>
<td>0.00%</td>
<td>0.000us</td>
<td>0.000us</td>
<td>1</td>
</tr>
<tr class="even">
<td>——————————————-</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
<td>————</td>
</tr>
<tr class="odd">
<td><strong>Self CPU time total:</strong> 847.733ms<br><strong>Self CUDA time total:</strong> 67.727ms</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="callout callout-style-default callout-note callout-titled" title="Profiling Results">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Profiling Results
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><code>Memcpy HtoD (Pageable -&gt; Device)</code></p>
<ul>
<li><p>Host to device copy</p></li>
<li><p>Pageable memory is on host but can be copied freely in and out of RAM</p></li>
<li><p>Equivalent to the <code>.cuda()</code> call for sending a tensor to the GPU</p></li>
</ul></li>
<li><p>aten::square is a call to aten::pow</p></li>
<li><p>A CUDA kernel gets launched called <code>native::vectorized_elementwise_kernel&lt;4,..&gt;</code></p>
<ul>
<li>4 is the number of blocks</li>
<li>Source Code: <a href="https://github.com/pytorch/pytorch/blob/main/caffe2/utils/math/elementwise.cu">elementwise CUDA kernel</a></li>
</ul></li>
<li><p>This approach does not necessarily give us an idea of kernel performance of how we could improve it.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Chrome Trace">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Chrome Trace
</div>
</div>
<div class="callout-body-container callout-body">
<div style="overflow-x:auto; max-height:500px">
<p><img src="./images/torch-profiler-chrome-trace-screenshot.png"></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="integrating-cuda-kernels-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="integrating-cuda-kernels-in-pytorch">Integrating CUDA Kernels in PyTorch</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=z3eqrEXpK10a0ZAJ&amp;t=1068">17:48</a></li>
<li>CUDA is typically written using C/C++</li>
<li><strong>PyBind:</strong> Create Python bindings for C++ code.
<ul>
<li>Documentation: (<a href="https://pybind11.readthedocs.io/en/stable/">link</a>)</li>
</ul></li>
<li><strong>torch.utils.cpp_extension.load_inline :</strong>
<ul>
<li>Documentation: (<a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline">link</a>)</li>
<li>Pass C++ source code, CUDA C/C++ code, and specify the functions to expose in Python</li>
<li>Automatically generates C++ source files with required pybind Python bindings</li>
<li>Automatically generates CUDA source files with required headers</li>
<li>Automatically generates <code>build.ninja</code> script for compiling the C++ code</li>
<li>Automatically builds the extension</li>
</ul></li>
</ul>
<section id="hello-world-example" class="level3">
<h3 class="anchored" data-anchor-id="hello-world-example">Hello World Example</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.cpp_extension <span class="im">import</span> load_inline</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>cpp_source <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="st">std::string hello_world() {</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="st">  return "Hello World!";</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>build_dir <span class="op">=</span> Path(<span class="st">'./load_inline_hello_world_cuda'</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>build_dir.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a custom C++ module directly from inline sources</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_inline(</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'module'</span>,                  <span class="co"># Name of the module to be created</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    cpp_sources<span class="op">=</span>[cpp_source],       <span class="co"># List of C++ source code strings</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    functions<span class="op">=</span>[<span class="st">'hello_world'</span>],      <span class="co"># List of function names to be bound to Python</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,                   <span class="co"># Enable verbose output to help with debugging</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    build_directory<span class="op">=</span><span class="bu">str</span>(build_dir)  <span class="co"># Directory to store the build artifacts</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Emitting ninja build file load_inline_hello_world_cuda/build.ninja...
Building extension module module...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)


[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/TH -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/THC -isystem /home/innom-dt/mambaforge/envs/pytorch-env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /mnt/980_1TB_1/Notes/CUDA_MODE/Lecture_1/load_inline_hello_world_cuda/main.cpp -o main.o 
[2/2] c++ main.o -shared -L/home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o module.so


Loading extension module module...</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(module.hello_world())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Hello World!</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the path to the extension module</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Module Path: </span><span class="sc">{</span>module<span class="sc">.</span><span class="va">__file__</span><span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Module Path: /mnt/980_1TB_1/Notes/CUDA_MODE/Lecture_1/load_inline_hello_world_cuda/module.so</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the content of the module folder as a Pandas DataFrame</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([path.name <span class="cf">for</span> path <span class="kw">in</span> Path(module.<span class="va">__file__</span>).parent.iterdir()])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div>
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
.ninja_deps
</td>
</tr>
<tr>
<th>
1
</th>
<td>
.ninja_log
</td>
</tr>
<tr>
<th>
2
</th>
<td>
build.ninja
</td>
</tr>
<tr>
<th>
3
</th>
<td>
main.cpp
</td>
</tr>
<tr>
<th>
4
</th>
<td>
main.o
</td>
</tr>
<tr>
<th>
5
</th>
<td>
module.so
</td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Build File">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Build File
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode ini code-with-copy"><code class="sourceCode ini"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="dt">ninja_required_version </span><span class="ot">=</span><span class="st"> </span><span class="fl">1.3</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="dt">cxx </span><span class="ot">=</span><span class="st"> c++</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="dt">cflags </span><span class="ot">=</span><span class="st"> -DTORCH_EXTENSION_NAME=module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/TH -isystem /home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/include/THC -isystem /home/innom-dt/mambaforge/envs/pytorch-env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="dt">post_cflags </span><span class="ot">=</span><span class="st"> </span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="dt">cuda_dlink_post_cflags </span><span class="ot">=</span><span class="st"> </span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="dt">ldflags </span><span class="ot">=</span><span class="st"> -shared -L/home/innom-dt/mambaforge/envs/pytorch-env/lib/python3.11/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="dt">rule compile</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="dt">  command </span><span class="ot">=</span><span class="st"> $cxx -MMD -MF $out.d $cflags -c $in -o $out $post_cflags</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="dt">  depfile </span><span class="ot">=</span><span class="st"> $out.d</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="dt">  deps </span><span class="ot">=</span><span class="st"> gcc</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="dt">rule link</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="dt">  command </span><span class="ot">=</span><span class="st"> $cxx $in $ldflags -o $out</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="dt">build main.o: compile /mnt/980_1TB_1/Notes/CUDA_MODE/Lecture_1/load_inline_hello_world_cuda/main.cpp</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="dt">build module.so: link main.o</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="dt">default module.so</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/extension.h&gt;</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">std::</span>string hello_world<span class="op">()</span> <span class="op">{</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="st">"Hello World!"</span><span class="op">;</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>PYBIND11_MODULE<span class="op">(</span>TORCH_EXTENSION_NAME<span class="op">,</span> m<span class="op">)</span> <span class="op">{</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>m<span class="op">.</span>def<span class="op">(</span><span class="st">"hello_world"</span><span class="op">,</span> torch<span class="op">::</span>wrap_pybind_function<span class="op">(</span>hello_world<span class="op">),</span> <span class="st">"hello_world"</span><span class="op">);</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
</section>
<section id="custom-cuda-kernel-for-square-operation" class="level3">
<h3 class="anchored" data-anchor-id="custom-cuda-kernel-for-square-operation">Custom CUDA kernel for Square Operation</h3>
<ul>
<li>CUDA Kernel</li>
<li>Wrapper function to prepare PyTorch tensor as input for CUDA kernel</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.cpp_extension <span class="im">import</span> load_inline</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the CUDA kernel and C++ wrapper</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>cuda_source <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="st">// Define a CUDA kernel function to square each element of a matrix.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="st">// This kernel will be executed by multiple threads in a parallel manner on the GPU.</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="st">//</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="st">// @param matrix The input matrix (flattened as a 1D array).</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="st">// @param result The output matrix (flattened as a 1D array) where the squared values will be stored.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="st">// @param width The width of the matrix.</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="st">// @param height The height of the matrix.</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="st">__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="st">    // Calculate the row index of the matrix element to be processed by this thread</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="st">    int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="st">    // Calculate the column index of the matrix element to be processed by this thread</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="st">    int col = blockIdx.x * blockDim.x + threadIdx.x;</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="st">    // Ensure the thread corresponds to a valid matrix element</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="st">    if (row &lt; height &amp;&amp; col &lt; width) {</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="st">        // Linear index of the element in the flattened array</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="st">        int idx = row * width + col;</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="st">        // Square the matrix element and store the result</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="st">        result[idx] = matrix[idx] * matrix[idx];</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="st">// Function to square each element of a matrix using GPU acceleration.</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="st">// It utilizes the PyTorch library for matrix operations and CUDA for parallel computation.</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="st">//</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a><span class="st">// @param matrix A 2D tensor representing the matrix whose elements are to be squared.</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="st">// @return A 2D tensor representing the matrix with each element squared.</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="st">torch::Tensor square_matrix(torch::Tensor matrix) {</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="st">    // Extract the dimensions of the input matrix</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="st">    const auto height = matrix.size(0);</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a><span class="st">    const auto width = matrix.size(1);</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="st">    // Create an output tensor with the same dimensions and properties as the input matrix</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="st">    auto result = torch::empty_like(matrix);</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a><span class="st">    // Define the size of the CUDA blocks and grid</span></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a><span class="st">    // Each block contains 16x16 threads, a common choice for many kernels</span></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a><span class="st">    dim3 threads_per_block(16, 16);</span></span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a><span class="st">    // Calculate the number of blocks in each dimension</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a><span class="st">    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a><span class="st">                          (height + threads_per_block.y - 1) / threads_per_block.y);</span></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a><span class="st">    // Launch the CUDA kernel</span></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a><span class="st">    // Pass pointers to the device memory, dimensions, and configure the grid and blocks</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a><span class="st">    square_matrix_kernel&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a><span class="st">        matrix.data_ptr&lt;float&gt;(), result.data_ptr&lt;float&gt;(), width, height);</span></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a><span class="st">    // Return the result as a PyTorch tensor</span></span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a><span class="st">    return result;</span></span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>cpp_source <span class="op">=</span> <span class="st">"torch::Tensor square_matrix(torch::Tensor matrix);"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>build_dir <span class="op">=</span> Path(<span class="st">'./load_inline_cuda'</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>build_dir.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the defined C++/CUDA extension as a PyTorch extension.</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This enables using the `square_matrix` function as if it were a native PyTorch function.</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>square_matrix_extension <span class="op">=</span> load_inline(</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'square_matrix_extension'</span>,   <span class="co"># Unique name for the extension</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    cpp_sources<span class="op">=</span>cpp_source,           <span class="co"># C++ source code containing the CPU implementation</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    cuda_sources<span class="op">=</span>cuda_source,         <span class="co"># CUDA source code for GPU implementation</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    functions<span class="op">=</span>[<span class="st">'square_matrix'</span>],      <span class="co"># List of functions to expose to Python</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    with_cuda<span class="op">=</span><span class="va">True</span>,                   <span class="co"># Enable CUDA support</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    extra_cuda_cflags<span class="op">=</span>[<span class="st">"-O2"</span>],        <span class="co"># Compiler flags for optimizing the CUDA code</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    build_directory<span class="op">=</span><span class="bu">str</span>(build_dir),   <span class="co"># Directory to store the compiled extension</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], [<span class="fl">4.</span>, <span class="fl">5.</span>, <span class="fl">6.</span>]], device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(square_matrix_extension.square_matrix(a))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor([[ 1.,  4.,  9.],
        [16., 25., 36.]], device='cuda:0')</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the path to the extension module</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Module Path: </span><span class="sc">{</span>square_matrix_extension<span class="sc">.</span><span class="va">__file__</span><span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Module Path: /mnt/980_1TB_1/Notes/CUDA_MODE/Lecture_1/load_inline_cuda/square_matrix_extension.so</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the content of the module folder as a Pandas DataFrame</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([path.name <span class="cf">for</span> path <span class="kw">in</span> Path(square_matrix_extension.<span class="va">__file__</span>).parent.iterdir()])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div>
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
.ninja_deps
</td>
</tr>
<tr>
<th>
1
</th>
<td>
.ninja_log
</td>
</tr>
<tr>
<th>
2
</th>
<td>
build.ninja
</td>
</tr>
<tr>
<th>
3
</th>
<td>
cuda.cu
</td>
</tr>
<tr>
<th>
4
</th>
<td>
cuda.cuda.o
</td>
</tr>
<tr>
<th>
5
</th>
<td>
main.cpp
</td>
</tr>
<tr>
<th>
6
</th>
<td>
main.o
</td>
</tr>
<tr>
<th>
7
</th>
<td>
square_matrix_extension.so
</td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="CUDA Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>CUDA Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/types.h&gt;</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda.h&gt;</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda_runtime.h&gt;</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Define a CUDA kernel function to square each element of a matrix.</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">// This kernel will be executed by multiple threads in a parallel manner on the GPU.</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co">//</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">// @param matrix The input matrix (flattened as a 1D array).</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">// @param result The output matrix (flattened as a 1D array) where the squared values will be stored.</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">// @param width The width of the matrix.</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">// @param height The height of the matrix.</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>__global__ <span class="dt">void</span> square_matrix_kernel<span class="op">(</span><span class="at">const</span> <span class="dt">float</span><span class="op">*</span> matrix<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> result<span class="op">,</span> <span class="dt">int</span> width<span class="op">,</span> <span class="dt">int</span> height<span class="op">)</span> <span class="op">{</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the row index of the matrix element to be processed by this thread</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> row <span class="op">=</span> blockIdx<span class="op">.</span>y <span class="op">*</span> blockDim<span class="op">.</span>y <span class="op">+</span> threadIdx<span class="op">.</span>y<span class="op">;</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the column index of the matrix element to be processed by this thread</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> col <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the thread corresponds to a valid matrix element</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>row <span class="op">&lt;</span> height <span class="op">&amp;&amp;</span> col <span class="op">&lt;</span> width<span class="op">)</span> <span class="op">{</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Linear index of the element in the flattened array</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> idx <span class="op">=</span> row <span class="op">*</span> width <span class="op">+</span> col<span class="op">;</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Square the matrix element and store the result</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        result<span class="op">[</span>idx<span class="op">]</span> <span class="op">=</span> matrix<span class="op">[</span>idx<span class="op">]</span> <span class="op">*</span> matrix<span class="op">[</span>idx<span class="op">];</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="co">// Function to square each element of a matrix using GPU acceleration.</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="co">// It utilizes the PyTorch library for matrix operations and CUDA for parallel computation.</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a><span class="co">//</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a><span class="co">// @param matrix A 2D tensor representing the matrix whose elements are to be squared.</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="co">// @return A 2D tensor representing the matrix with each element squared.</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>torch<span class="op">::</span>Tensor square_matrix<span class="op">(</span>torch<span class="op">::</span>Tensor matrix<span class="op">)</span> <span class="op">{</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Extract the dimensions of the input matrix</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> height <span class="op">=</span> matrix<span class="op">.</span>size<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> width <span class="op">=</span> matrix<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create an output tensor with the same dimensions and properties as the input matrix</span></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> result <span class="op">=</span> torch<span class="op">::</span>empty_like<span class="op">(</span>matrix<span class="op">);</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Define the size of the CUDA blocks and grid</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Each block contains 16x16 threads, a common choice for many kernels</span></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>    dim3 threads_per_block<span class="op">(</span><span class="dv">16</span><span class="op">,</span> <span class="dv">16</span><span class="op">);</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the number of blocks in each dimension</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>    dim3 number_of_blocks<span class="op">((</span>width <span class="op">+</span> threads_per_block<span class="op">.</span>x <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> threads_per_block<span class="op">.</span>x<span class="op">,</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>                          <span class="op">(</span>height <span class="op">+</span> threads_per_block<span class="op">.</span>y <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> threads_per_block<span class="op">.</span>y<span class="op">);</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Launch the CUDA kernel</span></span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Pass pointers to the device memory, dimensions, and configure the grid and blocks</span></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>    square_matrix_kernel<span class="op">&lt;&lt;&lt;</span>number_of_blocks<span class="op">,</span> threads_per_block<span class="op">&gt;&gt;&gt;(</span></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>        matrix<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">float</span><span class="op">&gt;(),</span> result<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">float</span><span class="op">&gt;(),</span> width<span class="op">,</span> height<span class="op">);</span></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Return the result as a PyTorch tensor</span></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="op">;</span></span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/extension.h&gt;</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>torch<span class="op">::</span>Tensor square_matrix<span class="op">(</span>torch<span class="op">::</span>Tensor matrix<span class="op">);</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>PYBIND11_MODULE<span class="op">(</span>TORCH_EXTENSION_NAME<span class="op">,</span> m<span class="op">)</span> <span class="op">{</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>m<span class="op">.</span>def<span class="op">(</span><span class="st">"square_matrix"</span><span class="op">,</span> torch<span class="op">::</span>wrap_pybind_function<span class="op">(</span>square_matrix<span class="op">),</span> <span class="st">"square_matrix"</span><span class="op">);</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
</section>
<section id="alternatives" class="level3">
<h3 class="anchored" data-anchor-id="alternatives">Alternatives</h3>
<ul>
<li>Numba
<ul>
<li>Website: (<a href="https://numba.pydata.org/">link</a>)</li>
<li>Write CUDA kernels directly in Python.</li>
<li>Easier syntax compared to C++, but may have performance limitations.</li>
</ul></li>
</ul>
</section>
</section>
<section id="triton" class="level2">
<h2 class="anchored" data-anchor-id="triton">Triton</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=fPptD3s7ZD2Ppa35&amp;t=1574">26:14</a></li>
<li>Documentation: (<a href="https://triton-lang.org/main/index.html">link</a>)</li>
<li>Python-based domain-specific language (DSL) for GPU programming.
<ul>
<li>Accessed through Python functions</li>
</ul></li>
<li>Block-based programming language</li>
<li>Does not generate CUDA
<ul>
<li>Generates a PTX kernel (CUDA assembly)</li>
</ul></li>
<li>Has a cache called <code>.triton</code>
<ul>
<li>Stores all individual LLVM IRs (Intermediate Representations) including the PTX</li>
</ul></li>
<li>The most important optimization for machine learning code is often fusions
<ul>
<li>Include more operations in a single kernel</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Additional Tips">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Additional Tips
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Start with simple kernels and gradually increase complexity.</li>
<li>Leverage existing PyTorch kernels and Triton tutorials as learning resources.</li>
<li>Focus on kernel fusion to improve performance by combining multiple operations.</li>
<li>Consider using torch.compile to automatically generate Triton kernels from PyTorch code.</li>
</ul>
</div>
</div>
<section id="code-example-square-operation-using-triton" class="level3">
<h3 class="anchored" data-anchor-id="code-example-square-operation-using-triton">Code Example: Square operation using Triton</h3>
<ul>
<li>Operates over rows instead of threads</li>
<li>The block size had a significant impact on performance</li>
</ul>
</section>
<section id="triton-debugger" class="level3">
<h3 class="anchored" data-anchor-id="triton-debugger">Triton debugger</h3>
<ul>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>  triton.jit(interpret<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li>Allows you to inspect code using Python breakpoints</li>
<li>Almost everything is a <code>WrappedTensor</code>
<ul>
<li>Inspect variables with <code>var_name.tensor</code></li>
</ul></li>
<li>Environment variable:
<ul>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>  <span class="va">TRITON_INTERPRET</span><span class="op">=</span>1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul></li>
<li>Can’t use print statements
<ul>
<li>Store variables in global memory and read them</li>
</ul></li>
</ul>
</section>
<section id="code-example-exploration-of-generated-square-kernel-ptx" class="level3">
<h3 class="anchored" data-anchor-id="code-example-exploration-of-generated-square-kernel-ptx">Code Example: Exploration of generated Square Kernel PTX</h3>
<ul>
<li>Element-wise matrix square</li>
<li>Triton is leveraging 8 registers at a time</li>
<li>Store global: writing the variables back to global memory
<ul>
<li>Can see what are the actual registers being used directly</li>
</ul></li>
<li>Recommendation:
<ul>
<li>Use ChatGPT (or other) to annotate the generated PTX assembly code</li>
</ul></li>
</ul>
</section>
<section id="code-example-auto-generate-a-triton-kernel-using-torch.compile" class="level3">
<h3 class="anchored" data-anchor-id="code-example-auto-generate-a-triton-kernel-using-torch.compile">Code Example: Auto-generate a triton kernel using <code>torch.compile()</code></h3>
<ul>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Print the code generated by torch.compile to the console</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="va">TORCH_LOGS</span><span class="op">=</span><span class="st">"output_code"</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="va">TORCH_LOGS</span><span class="op">=</span><span class="st">"OUTPUT_CODE"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>  torch.<span class="bu">compile</span>(torch.square)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li>Generated triton kernel does not operate row-by-row</li>
<li>Includes data type heuristics</li>
<li>Includes compiler heuristics</li>
</ul>
</section>
</section>
<section id="optimization-profiling-with-nsight-compute" class="level2">
<h2 class="anchored" data-anchor-id="optimization-profiling-with-nsight-compute">Optimization &amp; Profiling with Nsight Compute</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=-JjWySLFpKxp9RST&amp;t=2356">39:16</a></li>
<li>NVIDIA NSight Compute Profiler</li>
<li><a href="https://developer.nvidia.com/tools-overview/nsight-compute/get-started">Download NVIDIA Nsight Compute</a>
<ul>
<li><a href="https://developer.nvidia.com/downloads/assets/tools/secure/nsight-compute/2024_1_1/nsight-compute-linux-2024.1.1.4-33998838.run">Linux</a></li>
<li><a href="https://developer.nvidia.com/downloads/assets/tools/secure/nsight-compute/2024_1_1/nsight-compute-win64-2024.1.1.4-33998838.msi">Windows</a></li>
</ul></li>
<li><a href="https://pypi.org/project/jupyterlab-nvidia-nsight/">NVIDIA Nsight Tools JupyterLab Extension</a></li>
<li>Default Usage
<ul>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ncu</span> python train.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul></li>
<li>Does not work on most cloud providers</li>
</ul>
<section id="logs-example" class="level3">
<h3 class="anchored" data-anchor-id="logs-example">Logs Example:</h3>
<ul>
<li>Shows L1 Cache throughput, L2 Cache throughput, among others</li>
<li>Contains actionable hints:
<ul>
<li><pre class="text"><code>  OPT This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full waves across all SMs. Look at Launch Statistics for more details.</code></pre></li>
<li>Gives percentages that provide a performance ceiling to target</li>
<li>Tail effect and achieved occupancy
<ul>
<li>often controlled by padding
<ul>
<li>We can control padding</li>
</ul></li>
</ul></li>
<li>Long scoreboard stalls:
<ul>
<li>coalesce reads and writes</li>
<li>use shared memory</li>
<li>controlled by triton</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="visual-profiler" class="level3">
<h3 class="anchored" data-anchor-id="visual-profiler">Visual Profiler</h3>
<ul>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ncu</span> <span class="at">--set</span> full <span class="at">-o</span> output <span class="va">$(</span><span class="fu">which</span> python<span class="va">)</span> train.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li>Can see the memory throughput, compute throughput, block size, etc.</li>
<li>Can examine individual lines of code</li>
</ul>
</section>
<section id="moving-from-pytorch-to-triton-to-cuda" class="level3">
<h3 class="anchored" data-anchor-id="moving-from-pytorch-to-triton-to-cuda">Moving from PyTorch to Triton to CUDA</h3>
<ul>
<li>Try triton first if PyTorch is not enough</li>
<li>Use NCU profiler to see what performance improvements can be made over the triton attempt</li>
<li>Consider moving to CUDA if the hints suggest tweaking something (e.g., long scoreboard stalls) that triton controls</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>CUDA</th>
<th>Triton</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Memory Coalescing</td>
<td>Manual</td>
<td>Automatic</td>
</tr>
<tr class="even">
<td>Shared Memory Management</td>
<td>Manual</td>
<td>Automatic</td>
</tr>
<tr class="odd">
<td>Scheduling (Withing SMs)</td>
<td>Manual</td>
<td>Automatic</td>
</tr>
<tr class="even">
<td>Scheduling (Across SMs)</td>
<td>Manual</td>
<td>Manual</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="qa" class="level2">
<h2 class="anchored" data-anchor-id="qa">Q&amp;A</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/LuhJEEJQgUM?si=bgCHDc8KtYaEqtb0&amp;t=2697">44:57</a></li>
<li>Relationship between triton and <code>torch.compile</code>
<ul>
<li>Compilers are quite dumb
<ul>
<li><code>torch.square</code>
<ul>
<li>torch compile does not know what this operation is since it is not a primitive operation
<ul>
<li>turns it into a <code>torch.mul</code> operation</li>
<li>reads the <code>torch.mul</code> operation and writes a string to disk for using <code>triton.mul</code> and you run that</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>How often do you find the triton code generated with <code>torch.compile</code> readable enough to be a useful starting point?
<ul>
<li>Almost always</li>
<li>Compilers can’t do things that are too clever</li>
<li>The main thing to look for is fusing as many things as possible into as few kernels as possible</li>
<li>The variable names are things like <code>temp1</code> and <code>temp0</code>
<ul>
<li>Add you own comments using LLM like ChatGPT</li>
</ul></li>
</ul></li>
<li>Did you compare the performance of triton vs CUDA for square kernel?
<ul>
<li>Did not</li>
</ul></li>
<li>Does CUDA also generate PTX code?
<ul>
<li>Yes, but do not know how to look at it.</li>
</ul></li>
<li>When does <code>torch.compile</code> break?
<ul>
<li>The design philosophy of torch compile is that you should not need to change your code</li>
<li>Writing your code with torch compile in mind can result in SOTA performance
<ul>
<li>SAM, Stable Diffusion, etc.</li>
</ul></li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, an Applied AI Consultant and Educator.</p>
<p>Whether I’m writing an in-depth tutorial or sharing detailed notes, my goal is the same: to bring clarity to complex topics and find practical, valuable insights.</p>
<p>If you need a strategic partner with my approach to thinking and problem-solving for your AI project, I’m here to help. Let’s talk about de-risking your roadmap and building a real-world solution.</p>
<p>Start the conversation with my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> or learn more <a href="../../../about.html">about my approach</a>.</p>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/christianjmills\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>