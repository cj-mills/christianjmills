<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-09-01">
<meta name="description" content="Lecture #5 explores how to optimize matrix multiplication in CUDA for Python programmers using shared memory and tiling, comparing implementations in pure Python, CUDA C, and the Numba library.">

<title>GPU MODE Lecture 5: Going Further with CUDA for Python Programmers – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-d551e32f15e27e893f08ce3c93a41c1c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="GPU MODE Lecture 5: Going Further with CUDA for Python Programmers – Christian Mills">
<meta property="og:description" content="Lecture #5 explores how to optimize matrix multiplication in CUDA for Python programmers using shared memory and tiling, comparing implementations in pure Python, CUDA C, and the Numba library.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="GPU MODE Lecture 5: Going Further with CUDA for Python Programmers – Christian Mills">
<meta name="twitter:description" content="Lecture #5 explores how to optimize matrix multiplication in CUDA for Python programmers using shared memory and tiling, comparing implementations in pure Python, CUDA C, and the Numba library.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-and-overview" id="toc-introduction-and-overview" class="nav-link active" data-scroll-target="#introduction-and-overview">Introduction and Overview</a></li>
  <li><a href="#resources-and-setup" id="toc-resources-and-setup" class="nav-link" data-scroll-target="#resources-and-setup">Resources and Setup</a></li>
  <li><a href="#matrix-multiplication-example" id="toc-matrix-multiplication-example" class="nav-link" data-scroll-target="#matrix-multiplication-example">Matrix Multiplication Example</a>
  <ul>
  <li><a href="#previous-approaches-recap" id="toc-previous-approaches-recap" class="nav-link" data-scroll-target="#previous-approaches-recap">Previous Approaches (Recap)</a></li>
  </ul></li>
  <li><a href="#optimizing-with-shared-memory" id="toc-optimizing-with-shared-memory" class="nav-link" data-scroll-target="#optimizing-with-shared-memory">Optimizing with Shared Memory</a>
  <ul>
  <li><a href="#tiling" id="toc-tiling" class="nav-link" data-scroll-target="#tiling">Tiling</a></li>
  <li><a href="#implementing-tiling-in-python" id="toc-implementing-tiling-in-python" class="nav-link" data-scroll-target="#implementing-tiling-in-python">Implementing Tiling in Python</a></li>
  <li><a href="#refactoring-the-python-kernel" id="toc-refactoring-the-python-kernel" class="nav-link" data-scroll-target="#refactoring-the-python-kernel">Refactoring the Python Kernel</a></li>
  <li><a href="#cuda-like-python-implementation-with-threads" id="toc-cuda-like-python-implementation-with-threads" class="nav-link" data-scroll-target="#cuda-like-python-implementation-with-threads">CUDA-Like Python Implementation with Threads</a></li>
  <li><a href="#implementing-tiling-in-cuda" id="toc-implementing-tiling-in-cuda" class="nav-link" data-scroll-target="#implementing-tiling-in-cuda">Implementing Tiling in CUDA</a></li>
  <li><a href="#dynamic-shared-memory-performance-issue-and-solution-update-from-the-future" id="toc-dynamic-shared-memory-performance-issue-and-solution-update-from-the-future" class="nav-link" data-scroll-target="#dynamic-shared-memory-performance-issue-and-solution-update-from-the-future">Dynamic Shared Memory Performance Issue and Solution (Update from the Future)</a></li>
  </ul></li>
  <li><a href="#implementing-tiling-with-numba" id="toc-implementing-tiling-with-numba" class="nav-link" data-scroll-target="#implementing-tiling-with-numba">Implementing Tiling with Numba</a></li>
  <li><a href="#qa-session" id="toc-qa-session" class="nav-link" data-scroll-target="#qa-session">Q&amp;A Session</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">GPU MODE Lecture 5: Going Further with CUDA for Python Programmers</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">cuda</div>
  </div>
  </div>

<div>
  <div class="description">
    Lecture #5 explores how to optimize matrix multiplication in CUDA for Python programmers using shared memory and tiling, comparing implementations in pure Python, CUDA C, and the Numba library.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 1, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>GPU MODE Lecture Notes</strong></a>: My notes from the <strong>GPU MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.</li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction-and-overview">Introduction and Overview</a></li>
<li><a href="#resources-and-setup">Resources and Setup</a></li>
<li><a href="#matrix-multiplication-example">Matrix Multiplication Example</a><br>
</li>
<li><a href="#optimizing-with-shared-memory">Optimizing with Shared Memory</a><br>
</li>
<li><a href="#implementing-tiling-with-numba">Implementing Tiling with Numba</a></li>
<li><a href="#qa-session">Q&amp;A Session</a></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Resource Links:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Resource Links:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>YouTube Recording:</strong> <a href="https://www.youtube.com/watch?v=wVsR-YhaHlM">Lecture 5: Going Further with CUDA for Python Programmers</a></li>
<li><strong>Jupyter Notebook:</strong> <a href="https://github.com/cuda-mode/lectures/blob/main/lecture_005/matmul_l5.ipynb">lecture_005/matmul_l5.ipynb</a></li>
<li><strong>utils.py:</strong> <a href="https://github.com/cuda-mode/lectures/blob/main/utils.py">utils.py</a></li>
</ul>
</div>
</div>
<section id="introduction-and-overview" class="level2">
<h2 class="anchored" data-anchor-id="introduction-and-overview">Introduction and Overview</h2>
<ul>
<li><strong>Going Further with CUDA for Python Programmers:</strong> This lecture builds upon the foundational knowledge presented in “<a href="https://www.youtube.com/watch?v=4sgKnKbR-WE">Getting Started with CUDA for Python Programmers</a>” and focuses on optimizing CUDA code for performance by leveraging fast memory.</li>
<li><strong>Prerequisites:</strong> Familiarity with basic CUDA concepts and Python programming, including thread utilization.</li>
<li><strong>Recommended Resources:</strong>
<ul>
<li>“<a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processes</a>” (book), Chapter 5.</li>
<li><a href="https://www.youtube.com/watch?v=lTmYrKwjSOU">CUDA Mode lecture by Thomas Viehmann</a> (covers Chapter 4 &amp; 5).</li>
</ul></li>
<li><strong>Lecture Focus:</strong> Utilizing <strong>shared memory</strong>, a faster memory type within the GPU, to improve performance.</li>
<li><strong>Memory Hierarchy:</strong>
<ul>
<li><strong>Global Memory:</strong> Default memory used in CUDA, relatively fast but not the fastest.
<ul>
<li>Accessed by all threads.</li>
<li>(e.g., with <code>tensor.cuda()</code> in PyTorch)</li>
</ul></li>
<li><strong>Shared Memory:</strong> Significantly faster than global memory (about 10x).
<ul>
<li>Accessible only by threads within a specific <strong>block</strong> (on a streaming multiprocessor).</li>
</ul></li>
</ul></li>
<li><strong>Importance of Memory Access Speed:</strong> Due to the high processing speed of GPUs, memory access becomes a performance bottleneck. Utilizing shared memory effectively is crucial for optimization.</li>
</ul>
</section>
<section id="resources-and-setup" class="level2">
<h2 class="anchored" data-anchor-id="resources-and-setup">Resources and Setup</h2>
<ul>
<li><p><strong>Repository:</strong> CUDA Mode lectures repository, specifically lecture 5 notebook.</p>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/cuda-mode/lectures">https://github.com/cuda-mode/lectures</a></li>
</ul></li>
<li><p><strong><a href="https://github.com/cuda-mode/lectures/blob/main/utils.py">utils.py</a>:</strong> Contains helper functions (e.g., ceiling division, CUDA code loading, prefix for CUDA code).</p></li>
<li><p><strong><code>dim3</code>:</strong> Python namedtuple representing a 3D grid (x, y, z) for blocks and threads, mirroring CUDA’s Dim3 structure.</p></li>
<li><p><strong>Debugging Tools:</strong> Wurlitzer for printing from CUDA kernels, CUDA launch blocking for debugging.</p></li>
<li><p><strong>Setup Code:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os      <span class="co"># Operating system interfaces</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math    <span class="co"># Mathematical functions</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys     <span class="co"># System-specific parameters and functions</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch   <span class="co"># PyTorch library for tensor computations and neural networks</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re      <span class="co"># Regular expression operations</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np  <span class="co"># NumPy library for numerical computations</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> SimpleNamespace <span class="im">as</span> ns  <span class="co"># Allows creation of attribute-accessible objects</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple  <span class="co"># Factory function for creating tuple subclasses with named fields</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a custom 3D dimension namedtuple with default values</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>dim3 <span class="op">=</span> namedtuple(<span class="st">'dim3'</span>, [<span class="st">'x'</span>, <span class="st">'y'</span>, <span class="st">'z'</span>], defaults<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D dimension instance</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> dim3(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the full dimension object</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>d</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>dim3(x=2, y=3, z=1)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display x and y components of the dimension</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>d.x, d.y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>(2, 3)</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure NumPy print options for cleaner output</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure PyTorch print options for cleaner output and disable scientific notation</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>torch.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">140</span>, sci_mode<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import utility functions</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> show_img, load_cuda, cuda_begin, cdiv</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the wurlitzer IPython extension for capturing C-level output</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext wurlitzer</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a random seed for reproducibility</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>&lt;torch._C.Generator at 0x728ffff23630&gt;</code></pre></li>
</ul>
</section>
<section id="matrix-multiplication-example" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication-example">Matrix Multiplication Example</h2>
<ul>
<li><p><strong>Problem:</strong> Multiplying a 5120x256 matrix (M1) by a 256x5120 matrix (M2).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a large random tensor (5120x256)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="op">=</span> torch.rand(<span class="dv">5120</span>, <span class="dv">256</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the first 4 rows of m1</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>m1s <span class="op">=</span> m1[:<span class="dv">4</span>]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create another large random tensor (256x5120)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>m2 <span class="op">=</span> torch.rand(<span class="dv">256</span>, <span class="dv">5120</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the first 4 columns of m2</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>m2s <span class="op">=</span> m2[:, :<span class="dv">4</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
<section id="previous-approaches-recap" class="level3">
<h3 class="anchored" data-anchor-id="previous-approaches-recap">Previous Approaches (Recap)</h3>
<ul>
<li><p><strong>Naive Matrix Multiplication Kernel:</strong></p>
<ul>
<li>Calculates dot product for each element in the output matrix.</li>
<li>Accesses global memory repeatedly within the inner loop, leading to performance issues.</li>
</ul></li>
<li><p><strong>Pure Python Baseline:</strong> Extremely slow, uses a small sample of the matrices (4x4) for demonstration.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blk_kernel2d(f, blocks, threads, <span class="op">*</span>args):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a 2D GPU kernel execution on CPU.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function emulates the behavior of a 2D GPU kernel by iterating over</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    blocks and threads in a nested loop structure.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">        f (function): The kernel function to be executed.</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">        blocks (dim3): The number of blocks in x and y dimensions.</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">        threads (dim3): The number of threads per block in x and y dimensions.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Additional arguments to be passed to the kernel function.</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i0 <span class="kw">in</span> <span class="bu">range</span>(blocks.y):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i1 <span class="kw">in</span> <span class="bu">range</span>(blocks.x):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j0 <span class="kw">in</span> <span class="bu">range</span>(threads.y):</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> j1 <span class="kw">in</span> <span class="bu">range</span>(threads.x):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Execute the kernel function for each thread</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>                    f(dim3(i1,i0), dim3(j1,j0), threads, <span class="op">*</span>args)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_bk(blockIdx, threadIdx, blockDim, m, n, out, h, w, k):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication for a single element in the output matrix.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function calculates one element of the output matrix by multiplying</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    a row from the first matrix with a column from the second matrix.</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        blockIdx (dim3): The current block index.</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">        threadIdx (dim3): The current thread index within the block.</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">        blockDim (dim3): The dimensions of the block.</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): Flattened first input matrix.</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Flattened second input matrix.</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">        out (Tensor): Flattened output matrix.</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">        h (int): Height of the output matrix.</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co">        w (int): Width of the output matrix.</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): Common dimension of input matrices.</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate global thread indices</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> threadIdx.y</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> threadIdx.x</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if the thread is within the output matrix dimensions</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (r <span class="op">&gt;=</span> h <span class="kw">or</span> c <span class="op">&gt;=</span> w):</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform dot product of row from m and column from n</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        o <span class="op">+=</span> m[r<span class="op">*</span>k<span class="op">+</span>i] <span class="op">*</span> n[i<span class="op">*</span>w<span class="op">+</span>c]</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the result in the output matrix</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    out[r<span class="op">*</span>w<span class="op">+</span>c] <span class="op">=</span> o</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d(m, n):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication using a simulated 2D GPU kernel.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function sets up the execution configuration and launches the</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    matrix multiplication kernel.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): First input matrix.</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Second input matrix.</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: Result of matrix multiplication.</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Raises:</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">        AssertionError: If the inner dimensions of input matrices don't match.</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize output matrix</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up thread and block dimensions</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> dim3(<span class="dv">16</span>, <span class="dv">16</span>)  <span class="co"># Threads per block</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))  <span class="co"># Number of blocks</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch the kernel</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    blk_kernel2d(matmul_bk, blocks, tpb,</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>                 m.flatten(), n.flatten(), output.flatten(), h, w, k)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the result by comparing with PyTorch's built-in matrix multiplication</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(matmul_2d(m1s, m2s), m1s<span class="op">@</span>m2s).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre>
<ul>
<li><strong>Simple Kernel Runner:</strong> Iterates through simulated blocks and threads, calling a kernel function (not a real CUDA kernel).</li>
</ul></li>
<li><p><strong>CUDA Kernel Runner:</strong> Similar to the simple kernel runner but uses CUDA’s syntax for launching kernels (triple angle brackets).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA kernel definition and PyTorch C++ extension implementation</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void matmul_k</span><span class="kw">(</span><span class="vs">float</span><span class="op">*</span><span class="vs"> m, float</span><span class="op">*</span><span class="vs"> n, float</span><span class="op">*</span><span class="vs"> out, int h, int w, int k</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate global thread indices</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="vs">    int r = blockIdx</span><span class="dv">.</span><span class="vs">y</span><span class="op">*</span><span class="vs">blockDim</span><span class="dv">.</span><span class="vs">y </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">y;</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="vs">    int c = blockIdx</span><span class="dv">.</span><span class="vs">x</span><span class="op">*</span><span class="vs">blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> threadIdx</span><span class="dv">.</span><span class="vs">x;</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check if thread is within matrix bounds</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">r &gt;= h </span><span class="cf">||</span><span class="vs"> c &gt;= w</span><span class="kw">)</span><span class="vs"> return;</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Perform dot product for this element</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="vs">    float o = 0;</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    for </span><span class="kw">(</span><span class="vs">int i = 0; i &lt; k; </span><span class="op">++</span><span class="vs">i</span><span class="kw">)</span><span class="vs"> o </span><span class="op">+</span><span class="vs">= m</span><span class="pp">[r*k+i]</span><span class="vs"> </span><span class="op">*</span><span class="vs"> n</span><span class="pp">[i*w+c]</span><span class="vs">;</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    out</span><span class="pp">[r*w+c]</span><span class="vs"> = o;</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor matmul</span><span class="kw">(</span><span class="vs">torch::Tensor m, torch::Tensor n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">m</span><span class="kw">)</span><span class="vs">; CHECK_INPUT</span><span class="kw">(</span><span class="vs">n</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="vs">    int w = n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    int k = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="vs">    TORCH_CHECK</span><span class="kw">(</span><span class="vs">k==n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, "Size mismatch!"</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::zeros</span><span class="kw">(</span><span class="vs">{h, w}, m</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Define thread block and grid dimensions</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 tpb</span><span class="kw">(</span><span class="vs">16,16</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch CUDA kernel</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="vs">    matmul_k&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="vs">        m</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, n</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, h, w, k</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fname <span class="op">=</span> <span class="st">'matmul'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sig(fname, src):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Extract the function signature from the source code.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">        fname (str): The name of the function to extract.</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        src (str): The source code to search.</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">        str: The function signature with a semicolon appended, or None if not found.</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> re.findall(<span class="vs">rf'</span><span class="dv">^</span><span class="kw">(</span><span class="dv">.</span><span class="op">+</span><span class="dv">\s</span><span class="op">+</span><span class="sc">{</span>fname<span class="sc">}</span><span class="ch">\(</span><span class="dv">.</span><span class="op">*?</span><span class="ch">\)</span><span class="kw">)</span><span class="dv">\s</span><span class="op">*</span><span class="ch">{{</span><span class="op">?</span><span class="dv">\s</span><span class="op">*</span><span class="dv">$</span><span class="vs">'</span>, src, re.MULTILINE)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res[<span class="dv">0</span>]<span class="op">+</span><span class="st">';'</span> <span class="cf">if</span> res <span class="cf">else</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> get_sig(fname, cuda_src)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>cpp_src</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>'torch::Tensor matmul(torch::Tensor m, torch::Tensor n);'</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the CUDA module</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [fname])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Move tensors to GPU and ensure they are contiguous</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>m1c, m2c <span class="op">=</span> m1.contiguous().cuda(), m2.contiguous().cuda()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the shape of the output</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>module.matmul(m1c, m2c).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>torch.Size([5120, 5120])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify correctness by comparing with PyTorch's built-in matrix multiplication</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(module.matmul(m1c, m2c), m1c<span class="op">@</span>m2c).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True, device='cuda:0')</code></pre>
<ul>
<li><strong>CUDA Kernel (Naive):</strong> ChatGPT-generated CUDA code based on the naive Python kernel.</li>
</ul></li>
<li><p><strong>Performance:</strong> CUDA version is significantly faster than pure Python.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>n <span class="dv">10</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Benchmark the custom CUDA matmul implementation</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>module.matmul(m1c, m2c)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>3 ms ± 177 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
</ul>
</section>
</section>
<section id="optimizing-with-shared-memory" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-with-shared-memory">Optimizing with Shared Memory</h2>
<section id="tiling" class="level3">
<h3 class="anchored" data-anchor-id="tiling">Tiling</h3>
<ul>
<li><strong>Problem:</strong> Repeated global memory access in the inner loop of the matrix multiplication kernel.</li>
<li><strong>Solution:</strong> <strong>Tiling</strong> – dividing the matrices into smaller <strong>tiles</strong> and performing the multiplication tile-by-tile.</li>
<li><strong>Tile Width (TW):</strong> The dimension of a square tile (e.g., 16x16).</li>
<li><strong>Process:</strong>
<ol type="1">
<li>Load a tile from <code>m1</code> and a tile from <code>m2</code> into shared memory.</li>
<li>Calculate the partial dot products for all elements within the output tile using the shared memory tiles.</li>
<li>Repeat for all tiles, accumulating the partial dot products to get the final result.</li>
</ol></li>
<li><strong>Benefits:</strong>
<ul>
<li>Each input element is read from global memory only once.</li>
<li>Dot products are calculated using much faster shared memory.</li>
</ul></li>
</ul>
</section>
<section id="implementing-tiling-in-python" class="level3">
<h3 class="anchored" data-anchor-id="implementing-tiling-in-python">Implementing Tiling in Python</h3>
<ul>
<li><p><strong>Dynamic Shared Memory Simulation:</strong> Using NumPy or PyTorch tensor views to simulate dynamic shared memory allocation in CUDA.</p></li>
<li><p><strong>Shared Memory Kernel Runner:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blk_kernel2d_shar(f, blocks, threads, sh_sz, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Execute a 2D block kernel with shared memory.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">        f (function): The kernel function to execute</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">        blocks (dim3): Number of blocks in x and y dimensions</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">        threads (dim3): Number of threads per block</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">        sh_sz (int): Size of shared memory</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Additional positional arguments for the kernel function</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        **kwargs: Additional keyword arguments for the kernel function</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i0 <span class="kw">in</span> <span class="bu">range</span>(blocks.y):</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i1 <span class="kw">in</span> <span class="bu">range</span>(blocks.x):</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>            shared <span class="op">=</span> torch.zeros(sh_sz)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>            f(dim3(i1, i0), threads, shared, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Iterates through blocks.</li>
<li>Creates a simulated shared memory array.</li>
<li>Calls the kernel function, passing the shared memory.</li>
</ul></li>
<li><p><strong>Tiled Matrix Multiplication Kernel (Python):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_tiled_bk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform tiled matrix multiplication using block-wise computation.</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co">        blockIdx (dim3): Current block index</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co">        blockDim (dim3): Block dimensions</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co">        shared (Tensor): Shared memory tensor</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): First input matrix (flattened)</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Second input matrix (flattened)</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co">        out (Tensor): Output matrix (flattened)</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co">        h (int): Height of the first matrix</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="co">        w (int): Width of the second matrix</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): Shared dimension of the two matrices</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int): Tile width</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    shar_sz <span class="op">=</span> tw <span class="op">*</span> tw</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    ms, ns <span class="op">=</span> shared[:shar_sz], shared[shar_sz:]  <span class="co"># Split shared memory for both matrices</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ph <span class="kw">in</span> <span class="bu">range</span>(cdiv(k, tw)):</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> ph <span class="op">*</span> tw</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill shared memory with tiles from input matrices</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tr <span class="kw">in</span> <span class="bu">range</span>(blockDim.y):</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tc <span class="kw">in</span> <span class="bu">range</span>(blockDim.x):</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>                r, c <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> tr, blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> tc</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>                ms[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> m[tc<span class="op">+</span>idx <span class="op">+</span> r<span class="op">*</span>k] <span class="cf">if</span> r <span class="op">&lt;</span> h <span class="kw">and</span> idx<span class="op">+</span>tc <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>                ns[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> n[(tr<span class="op">+</span>idx)<span class="op">*</span>w <span class="op">+</span> c] <span class="cf">if</span> c <span class="op">&lt;</span> w <span class="kw">and</span> idx<span class="op">+</span>tr <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute dot products using shared memory</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tr <span class="kw">in</span> <span class="bu">range</span>(blockDim.y):</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tc <span class="kw">in</span> <span class="bu">range</span>(blockDim.x):</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>                r, c <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> tr, blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> tc</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tw):</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> r<span class="op">*</span>w<span class="op">+</span>c <span class="op">&lt;</span> <span class="bu">len</span>(out):</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>                        out[r<span class="op">*</span>w<span class="op">+</span>c] <span class="op">+=</span> ms[tr<span class="op">*</span>tw<span class="op">+</span>i] <span class="op">*</span> ns[tw<span class="op">*</span>i<span class="op">+</span>tc]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Fill Shared Memory:</strong>
<ul>
<li>Loops through each tile.</li>
<li>Calculates the starting index (<strong>idx</strong>) of the tile in the original matrix.</li>
<li>Loops through threads within the tile.</li>
<li>Calculates the row (<strong>r</strong>) and column (<strong>c</strong>) in the original matrix based on the tile index and thread index.</li>
<li>Copies the corresponding elements from the input matrices to the shared memory tiles (<code>ms</code>, <code>ns</code>).</li>
<li><strong>Padding:</strong> Fills elements outside the matrix boundaries with zeros.</li>
</ul></li>
<li><strong>Dot Product from Shared Memory:</strong>
<ul>
<li>Loops through threads within the tile.</li>
<li>Calculates the row and column in the output matrix.</li>
<li>Performs the dot product using elements from the shared memory tiles.</li>
</ul></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d(m, n, tw<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform 2D matrix multiplication using tiled block-wise computation.</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): First input matrix</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Second input matrix</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int, optional): Tile width. Defaults to 16.</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: Result of matrix multiplication</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> dim3(tw, tw)  <span class="co"># Threads per block</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))  <span class="co"># Number of blocks</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    blk_kernel2d_shar(matmul_tiled_bk, blocks, tpb, tw<span class="op">*</span>tw<span class="op">*</span><span class="dv">2</span>,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>                      m.flatten(), n.flatten(), output.flatten(),</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>                      h, w, k, tw<span class="op">=</span>tw)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a tensor 'a' with 5 zeros</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.zeros(<span class="dv">5</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split 'a' into two parts: 'b' (first 3 elements) and 'c' (last 2 elements)</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>b, c <span class="op">=</span> a[:<span class="dv">3</span>], a[<span class="dv">3</span>:]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify specific elements in 'b' and 'c'</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>b[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>c[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The value of 'a' is now implicitly modified due to tensor slicing</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>a</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor([0., 2., 0., 6., 0.])</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check shapes of matrices m1s and m2s</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>m1s.shape, m2.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>(torch.Size([<span class="dv">4</span>, <span class="dv">256</span>]), torch.Size([<span class="dv">256</span>, <span class="dv">5120</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Result:</strong> The Python tiled matrix multiplication produces the same result as the previous versions.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify if the custom matmul_2d function produces the same result as PyTorch's built-in matrix multiplication</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(matmul_2d(m1s, m2s, tw<span class="op">=</span><span class="dv">16</span>), m1s<span class="op">@</span>m2s).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre></li>
</ul>
</section>
<section id="refactoring-the-python-kernel" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-the-python-kernel">Refactoring the Python Kernel</h3>
<ul>
<li><p><strong><code>run_threads</code> Function:</strong> Introduced to abstract the looping through threads within a tile.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_threads(f, blockDim, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate thread execution in a 2D block.</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co">        f (callable): Function to be executed by each thread.</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co">        blockDim (object): Object containing x and y dimensions of the block.</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Variable length argument list to be passed to f.</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co">        **kwargs: Arbitrary keyword arguments to be passed to f.</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i0 <span class="kw">in</span> <span class="bu">range</span>(blockDim.y):</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i1 <span class="kw">in</span> <span class="bu">range</span>(blockDim.x):</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>            f(i0, i1, <span class="op">*</span>args, <span class="op">**</span>kwargs)  <span class="co"># Execute function for each thread</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Refactored Kernel:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_tiled_bk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform tiled matrix multiplication for a single block.</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co">        blockIdx (object): Block index in the grid.</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co">        blockDim (object): Dimensions of the block.</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co">        shared (list): Shared memory for the block.</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): First input matrix.</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Second input matrix.</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co">        out (Tensor): Output matrix.</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co">        h (int): Height of the output matrix.</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co">        w (int): Width of the output matrix.</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): Common dimension of input matrices.</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int): Tile width.</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    shar_sz <span class="op">=</span> tw<span class="op">*</span>tw</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    ms, ns <span class="op">=</span> shared[:shar_sz], shared[shar_sz:]  <span class="co"># Split shared memory for matrices m and n</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_rc(tr, tc):</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Calculate global row and column indices from thread indices."""</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> blockIdx.y<span class="op">*</span>blockDim.y <span class="op">+</span> tr, blockIdx.x<span class="op">*</span>blockDim.x <span class="op">+</span> tc</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fill_shared_tk(tr, tc, ph):</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Fill shared memory with a tile of input matrices."""</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        r, c <span class="op">=</span> get_rc(tr, tc)</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load elements from matrix m, use 0 if out of bounds</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        ms[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> m[tc <span class="op">+</span> ph<span class="op">*</span>tw <span class="op">+</span> r<span class="op">*</span>k] <span class="cf">if</span> r <span class="op">&lt;</span> h <span class="kw">and</span> (ph<span class="op">*</span>tw<span class="op">+</span>tc) <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load elements from matrix n, use 0 if out of bounds</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>        ns[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> n[(tr <span class="op">+</span> ph<span class="op">*</span>tw)<span class="op">*</span>w <span class="op">+</span> c] <span class="cf">if</span> c <span class="op">&lt;</span> w <span class="kw">and</span> (ph<span class="op">*</span>tw<span class="op">+</span>tr) <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dotprod_tk(tr, tc):</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute partial dot product for a tile."""</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>        r, c <span class="op">=</span> get_rc(tr, tc)</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tw):</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> r<span class="op">*</span>w<span class="op">+</span>c <span class="op">&lt;</span> <span class="bu">len</span>(out):</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>                out[r<span class="op">*</span>w<span class="op">+</span>c] <span class="op">+=</span> ms[tr<span class="op">*</span>tw<span class="op">+</span>i] <span class="op">*</span> ns[tw<span class="op">*</span>i<span class="op">+</span>tc]  <span class="co"># Accumulate dot product</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over tiles in the k dimension</span></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ph <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">int</span>(math.ceil(k<span class="op">/</span>tw))):</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>        run_threads(fill_shared_tk, blockDim, ph)  <span class="co"># Load tile into shared memory</span></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>        run_threads(dotprod_tk, blockDim)  <span class="co"># Compute partial dot products</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Uses <code>run_threads</code> to simplify the code and make it more readable.</li>
<li>Separates the “fill shared memory” and “dot product” logic into distinct functions.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d(m, n, tw<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform 2D matrix multiplication using tiled algorithm.</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co">        m (Tensor): First input matrix.</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n (Tensor): Second input matrix.</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int, optional): Tile width. Defaults to 16.</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: Result of matrix multiplication.</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span>  <span class="co"># Ensure matrices can be multiplied</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype)  <span class="co"># Initialize output matrix</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> dim3(tw, tw)  <span class="co"># Define threads per block</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))  <span class="co"># Calculate number of blocks needed</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch kernel for tiled matrix multiplication</span></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    blk_kernel2d_shar(matmul_tiled_bk, blocks, tpb, tw<span class="op">*</span>tw<span class="op">*</span><span class="dv">2</span>,</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>                      m.flatten(), n.flatten(), output.flatten(),</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>                      h, w, k, tw<span class="op">=</span>tw)</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Result:</strong> The refactored kernel is functionally equivalent to the previous version.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check shapes of input matrices</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>m1s.shape, m2s.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>(torch.Size([4, 256]), torch.Size([256, 4]))</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the result of matmul_2d against PyTorch's built-in matrix multiplication</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(matmul_2d(m1s, m2s, tw<span class="op">=</span><span class="dv">16</span>), m1s<span class="op">@</span>m2s).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre></li>
</ul>
</section>
<section id="cuda-like-python-implementation-with-threads" class="level3">
<h3 class="anchored" data-anchor-id="cuda-like-python-implementation-with-threads">CUDA-Like Python Implementation with Threads</h3>
<ul>
<li><p><strong>Motivation:</strong> CUDA kernels don’t have explicit loops for threads; threads are executed concurrently.</p></li>
<li><p><strong>Simulating Concurrent Threads:</strong> Python’s <code>threading</code> library is used to simulate concurrent thread execution.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> threading</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> threading <span class="im">import</span> Barrier, Thread  <span class="co"># For thread synchronization and creation</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> concurrent.futures <span class="im">import</span> ThreadPoolExecutor  <span class="co"># For managing a pool of worker threads</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(x, sb):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A function that prints a number, its negative, and its tenfold value using a synchronization barrier.</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x (int): The input number to be processed.</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co">        sb (threading.Barrier): A synchronization barrier to coordinate threads.</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co">    This function demonstrates the use of a barrier for thread synchronization.</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    sb.wait()  <span class="co"># Wait for all threads to reach this point</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="op">-</span>x)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    sb.wait()  <span class="co"># Wait again for all threads to reach this point</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x<span class="op">*</span><span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the number of threads to use</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Barrier object for synchronizing 'num' threads</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>sb <span class="op">=</span> Barrier(num)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a ThreadPoolExecutor to manage a pool of worker threads</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ThreadPoolExecutor(num) <span class="im">as</span> ex:</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the function g for each number in range(1, num+1) using the thread pool</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The lambda function is used to pass both the number and the Barrier object to g</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># list() is used to force immediate execution of all tasks</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">list</span>(ex.<span class="bu">map</span>(<span class="kw">lambda</span> i: g(i, sb), <span class="bu">range</span>(<span class="dv">1</span>, num<span class="op">+</span><span class="dv">1</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>1
2
3
-3
-1
-2
10
20
30</code></pre></li>
<li><p><strong>Synchronization Barrier:</strong> A <code>Barrier</code> object is used to synchronize threads, ensuring that all threads complete the “fill shared memory” step before proceeding to the “dot product” step.</p></li>
<li><p><strong>Kernel Runner:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blk_kernel2d_shar(f, blocks, tpb, sh_sz, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Execute a 2D block kernel function with shared memory.</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co">        f (function): The kernel function to be executed.</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co">        blocks (dim3): The number of blocks in x and y dimensions.</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co">        tpb (dim3): Threads per block in x and y dimensions.</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="co">        sh_sz (int): Size of shared memory.</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co">        *args: Variable length argument list for the kernel function.</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co">        **kwargs: Arbitrary keyword arguments for the kernel function.</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="co">    This function creates a grid of threads to execute the given kernel function.</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i0 <span class="kw">in</span> <span class="bu">range</span>(blocks.y):</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i1 <span class="kw">in</span> <span class="bu">range</span>(blocks.x):</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>            shar <span class="op">=</span> torch.zeros(sh_sz)  <span class="co"># Shared memory for the block</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>            syncb <span class="op">=</span> Barrier(tpb.y <span class="op">*</span> tpb.x)  <span class="co"># Synchronization barrier for threads in a block</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create threads for each element in the block</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>            threads <span class="op">=</span> [Thread(target<span class="op">=</span>f, args<span class="op">=</span>(dim3(i1,i0), dim3(p,o), tpb, shar, syncb, <span class="op">*</span>args), kwargs<span class="op">=</span>kwargs)</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>                       <span class="cf">for</span> o <span class="kw">in</span> <span class="bu">range</span>(tpb.y) <span class="cf">for</span> p <span class="kw">in</span> <span class="bu">range</span>(tpb.x)]</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Start and join all threads in the block</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tr <span class="kw">in</span> threads: tr.start()</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tr <span class="kw">in</span> threads: tr.join()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Creates a synchronization barrier.</li>
<li>Creates a thread for each element within a tile.</li>
<li>Passes the block index, thread index, shared memory, synchronization barrier, and kernel arguments to each thread.</li>
</ul></li>
<li><p><strong>Kernel (Python with Threads):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_tiled_bk(blockIdx, threadIdx, blockDim, shared, syncb, m, n, out, h, w, k, tw):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform tiled matrix multiplication for a single block.</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co">        blockIdx (dim3): Block index in the grid.</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co">        threadIdx (dim3): Thread index within the block.</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co">        blockDim (dim3): Dimensions of the block.</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co">        shared (torch.Tensor): Shared memory for the block.</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co">        syncb (threading.Barrier): Synchronization barrier for threads in the block.</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co">        m (torch.Tensor): First input matrix (flattened).</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co">        n (torch.Tensor): Second input matrix (flattened).</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co">        out (torch.Tensor): Output matrix (flattened).</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="co">        h (int): Height of the first matrix.</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co">        w (int): Width of the second matrix.</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): Shared dimension of the matrices.</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int): Tile width.</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="co">    This function computes a portion of the matrix multiplication result for a single block.</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>    tc, tr <span class="op">=</span> threadIdx.x, threadIdx.y</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> blockIdx.y <span class="op">*</span> blockDim.y <span class="op">+</span> tr</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> blockIdx.x <span class="op">*</span> blockDim.x <span class="op">+</span> tc</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    shar_sz <span class="op">=</span> tw <span class="op">*</span> tw</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>    ms, ns <span class="op">=</span> shared[:shar_sz], shared[shar_sz:]  <span class="co"># Split shared memory for two matrices</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ph <span class="kw">in</span> <span class="bu">range</span>(cdiv(k, tw)):</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load data into shared memory</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>        ms[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> m[tc <span class="op">+</span> ph<span class="op">*</span>tw <span class="op">+</span> r<span class="op">*</span>k] <span class="cf">if</span> r <span class="op">&lt;</span> h <span class="kw">and</span> (ph<span class="op">*</span>tw<span class="op">+</span>tc) <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>        ns[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> n[(tr <span class="op">+</span> ph<span class="op">*</span>tw)<span class="op">*</span>w <span class="op">+</span> c] <span class="cf">if</span> c <span class="op">&lt;</span> w <span class="kw">and</span> (ph<span class="op">*</span>tw<span class="op">+</span>tr) <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>        syncb.wait()  <span class="co"># Synchronize threads after loading data</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute partial dot product</span></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tw):</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>            p <span class="op">+=</span> ms[tr<span class="op">*</span>tw<span class="op">+</span>i] <span class="op">*</span> ns[tw<span class="op">*</span>i<span class="op">+</span>tc]</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>        syncb.wait()  <span class="co"># Synchronize threads before next iteration</span></span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (r <span class="op">&lt;</span> h <span class="kw">and</span> c <span class="op">&lt;</span> w):</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>        out[r<span class="op">*</span>w <span class="op">+</span> c] <span class="op">=</span> p  <span class="co"># Store the result in the output matrix</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Calculates row and column in the output matrix based on block and thread indices.</li>
<li>Fills shared memory (same as before).</li>
<li>Waits at the synchronization barrier (<code>syncb.wait()</code>).</li>
<li>Performs the dot product using shared memory.</li>
<li>Waits at the synchronization barrier again.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d(m, n, tw<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform 2D matrix multiplication using tiled algorithm.</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">        m (torch.Tensor): First input matrix.</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n (torch.Tensor): Second input matrix.</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">        tw (int, optional): Tile width. Defaults to 16.</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: Result of matrix multiplication.</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co">    This function orchestrates the tiled matrix multiplication using block kernels.</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> dim3(tw, tw)  <span class="co"># Threads per block</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> dim3(cdiv(w, tpb.x), cdiv(h, tpb.y))  <span class="co"># Number of blocks</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    blk_kernel2d_shar(matmul_tiled_bk, blocks, tpb, tw<span class="op">*</span>tw<span class="op">*</span><span class="dv">2</span>,</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>                      m.flatten(), n.flatten(), output.flatten(),</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>                      h, w, k, tw<span class="op">=</span>tw)</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Result:</strong> The Python implementation using threads simulates CUDA’s concurrent thread execution and produces the same result.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the correctness of the implementation</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(matmul_2d(m1s, m2s, tw<span class="op">=</span><span class="dv">8</span>), m1s<span class="op">@</span>m2s).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True)</code></pre></li>
</ul>
</section>
<section id="implementing-tiling-in-cuda" class="level3">
<h3 class="anchored" data-anchor-id="implementing-tiling-in-cuda">Implementing Tiling in CUDA</h3>
<ul>
<li><p><strong>CUDA Kernel (Tiled):</strong> ChatGPT-generated CUDA code based on the tiled Python kernel.</p>
<blockquote class="blockquote">
<p>Code auto-generated by ChatGPT 4, using the following prompt:</p>
<blockquote class="blockquote">
<p>Convert the following python code to CUDA C, keeping formatting and variable names the same where possible. You can remove <code>blockIdx, threadIdx, blockDim, shared</code> from the argument list, since they’re already provided by CUDA. Change <code>syncb.wait()</code> to <code>__syncthreads</code>. Use <code>extern __shared__ float shared[]</code> to create the <code>shared</code> array. Use the C ternary operator to replace the Python equivalent where appropriate. If the Python code uses any non-standard functions, you can assume the same functions are also available to the translated C code with the same name and signature.</p>
</blockquote>
<p>The generated code worked first time, although we did some minor cleanups afterwards (e.g.&nbsp;renaming <code>shared</code> to <code>ms</code>).</p>
</blockquote></li>
<li><p><strong>Dynamic Shared Memory Allocation:</strong> Uses <code>extern __shared__ float ms[];</code> to declare shared memory dynamically. The size is specified when launching the kernel.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA kernel code for matrix multiplication</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @brief CUDA kernel for matrix multiplication</span><span class="dv">.</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> </span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param m Pointer to the first input matrix</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param n Pointer to the second input matrix</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param out Pointer to the output matrix</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param h Height of the first matrix</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param w Width of the second matrix</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param k Width of the first matrix / Height of the second matrix</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param tw Tile width for shared memory optimization</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void matmul_k</span><span class="kw">(</span><span class="vs">float </span><span class="op">*</span><span class="vs">m, float </span><span class="op">*</span><span class="vs">n, float </span><span class="op">*</span><span class="vs">out, int h, int w, int k, int tw</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="vs">    int tc=threadIdx</span><span class="dv">.</span><span class="vs">x, tr=threadIdx</span><span class="dv">.</span><span class="vs">y;</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    int r=blockIdx</span><span class="dv">.</span><span class="vs">y</span><span class="op">*</span><span class="vs">blockDim</span><span class="dv">.</span><span class="vs">y</span><span class="op">+</span><span class="vs">tr, c=blockIdx</span><span class="dv">.</span><span class="vs">x</span><span class="op">*</span><span class="vs">blockDim</span><span class="dv">.</span><span class="vs">x</span><span class="op">+</span><span class="vs">tc;</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="vs">    extern __shared__ float ms</span><span class="pp">[];  // Shared memory for the first matrix</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a><span class="pp">    float *ns = &amp;ms[tw*tw]</span><span class="vs">;  // Shared memory for the second matrix</span></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    float p = 0</span><span class="dv">.</span><span class="vs">0f;  // Accumulator for the dot product</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="vs">    for </span><span class="kw">(</span><span class="vs">int ph = 0; ph &lt; cdiv</span><span class="kw">(</span><span class="vs">k,tw</span><span class="kw">)</span><span class="vs">; </span><span class="op">++</span><span class="vs">ph</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a><span class="vs">        int idx = ph</span><span class="op">*</span><span class="vs">tw;</span></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Load data into shared memory, with bounds checking</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a><span class="vs">        ms</span><span class="pp">[tr*tw + tc]</span><span class="vs"> = r&lt;h &amp;&amp; idx</span><span class="op">+</span><span class="vs">tc&lt;k </span><span class="op">?</span><span class="vs"> m</span><span class="pp">[ tc+idx + r*k ]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a><span class="vs">        ns</span><span class="pp">[tr*tw + tc]</span><span class="vs"> = c&lt;w &amp;&amp; idx</span><span class="op">+</span><span class="vs">tr&lt;k </span><span class="op">?</span><span class="vs"> n</span><span class="pp">[(tr+idx)*w + c]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure all threads have loaded data</span></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Compute partial dot product</span></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a><span class="vs">        for </span><span class="kw">(</span><span class="vs">int i=0; i&lt;tw; </span><span class="op">++</span><span class="vs">i</span><span class="kw">)</span><span class="vs"> p </span><span class="op">+</span><span class="vs">= ms</span><span class="pp">[tr*tw + i]</span><span class="vs"> </span><span class="op">*</span><span class="vs"> ns</span><span class="pp">[tw*i + tc]</span><span class="vs">;</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure all threads have finished computation</span></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Write result to global memory</span></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">r&lt;h &amp;&amp; c&lt;w</span><span class="kw">)</span><span class="vs"> out</span><span class="pp">[r*w + c]</span><span class="vs"> = p;</span></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch C++ extension for dynamic matrix multiplication</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">+=</span> <span class="vs">r'''</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @brief Perform matrix multiplication using CUDA</span><span class="dv">.</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> </span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param m First input tensor</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param n Second input tensor</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @return torch::Tensor Result of matrix multiplication</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor matmul_dyn</span><span class="kw">(</span><span class="vs">torch::Tensor m, torch::Tensor n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">m</span><span class="kw">)</span><span class="vs">; CHECK_INPUT</span><span class="kw">(</span><span class="vs">n</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h=m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, w=n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">, k=m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    TORCH_CHECK</span><span class="kw">(</span><span class="vs">k==n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, "Size mismatch!"</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::zeros</span><span class="kw">(</span><span class="vs">{h, w}, m</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    /</span><span class="op">*</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Commented out section demonstrating basic idea of dynamic size calculation</span></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a><span class="vs">    cudaDeviceProp devProp;</span></span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a><span class="vs">    CUDA_ERR</span><span class="kw">(</span><span class="vs">cudaGetDeviceProperties</span><span class="kw">(</span><span class="vs">&amp;devProp, 0</span><span class="kw">))</span><span class="vs">;</span></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a><span class="vs">    int maxThreads = devProp</span><span class="dv">.</span><span class="vs">maxThreadsPerBlock;</span></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    size_t requiredSize = static_cast&lt;size_t&gt;</span><span class="kw">(</span><span class="vs">maxThreads</span><span class="kw">)</span><span class="vs"> </span><span class="op">*</span><span class="vs"> 2 </span><span class="op">*</span><span class="vs"> sizeof</span><span class="kw">(</span><span class="vs">float</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a><span class="vs">    size_t size = min</span><span class="kw">(</span><span class="vs">devProp</span><span class="dv">.</span><span class="vs">sharedMemPerBlock, requiredSize</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a><span class="vs">    int TW = std::sqrt</span><span class="kw">(</span><span class="vs">maxThreads</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Fixed size configuration</span></span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a><span class="vs">    int TW = 16;  // Tile width</span></span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a><span class="vs">    size_t size = TW</span><span class="op">*</span><span class="vs">TW </span><span class="op">*</span><span class="vs"> 2 </span><span class="op">*</span><span class="vs"> sizeof</span><span class="kw">(</span><span class="vs">float</span><span class="kw">)</span><span class="vs">;  // Shared memory size</span></span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 tpb</span><span class="kw">(</span><span class="vs">TW,TW</span><span class="kw">)</span><span class="vs">;  // Threads per block</span></span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;  // Number of blocks</span></span>
<span id="cb58-31"><a href="#cb58-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-32"><a href="#cb58-32" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch CUDA kernel</span></span>
<span id="cb58-33"><a href="#cb58-33" aria-hidden="true" tabindex="-1"></a><span class="vs">    matmul_k&lt;&lt;&lt;blocks,tpb,size&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb58-34"><a href="#cb58-34" aria-hidden="true" tabindex="-1"></a><span class="vs">        m</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, n</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, h, w, k, TW</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb58-35"><a href="#cb58-35" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb58-36"><a href="#cb58-36" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb58-37"><a href="#cb58-37" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb58-38"><a href="#cb58-38" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Name of the function to be called</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>fname <span class="op">=</span> <span class="st">'matmul_dyn'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate C++ function signature</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> get_sig(fname, cuda_src)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [fname], opt<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code># Test for correctness by comparing with PyTorch's built-in matrix multiplication
torch.isclose(module.matmul_dyn(m1c,m2c), m1c@m2c).all()</code></pre>
<pre class="text"><code>tensor(True, device='cuda:0')</code></pre></li>
<li><p><strong>Static Shared Memory Allocation:</strong> Declares shared memory arrays with fixed sizes at compile time (e.g., <code>__shared__ float ms[tw][tw];</code>).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA kernel and PyTorch extension for efficient matrix multiplication</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="vs">constexpr int tw = 16;  // Tile width for shared memory optimization</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> CUDA kernel for matrix multiplication using shared memory tiling</span><span class="dv">.</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param m Pointer to the first input matrix</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param n Pointer to the second input matrix</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param out Pointer to the output matrix</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param h Height of the first input matrix and output matrix</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param w Width of the second input matrix and output matrix</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param k Width of the first input matrix / Height of the second input matrix</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void matmul_ks</span><span class="kw">(</span><span class="vs">float </span><span class="op">*</span><span class="vs">m, float </span><span class="op">*</span><span class="vs">n, float </span><span class="op">*</span><span class="vs">out, int h, int w, int k</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    __shared__ float ms</span><span class="pp">[tw][tw]</span><span class="vs">, ns</span><span class="pp">[tw][tw]</span><span class="vs">;  // Shared memory for tiling</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    int tc = threadIdx</span><span class="dv">.</span><span class="vs">x, tr = threadIdx</span><span class="dv">.</span><span class="vs">y;</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="vs">    int r = blockIdx</span><span class="dv">.</span><span class="vs">y </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">y </span><span class="op">+</span><span class="vs"> tr, c = blockIdx</span><span class="dv">.</span><span class="vs">x </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> tc;</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="vs">    float p = 0</span><span class="dv">.</span><span class="vs">0f;  // Accumulator for dot product</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Iterate over tiles</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a><span class="vs">    for </span><span class="kw">(</span><span class="vs">int ph = 0; ph &lt; cdiv</span><span class="kw">(</span><span class="vs">k, tw</span><span class="kw">)</span><span class="vs">; </span><span class="op">++</span><span class="vs">ph</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="vs">        int idx = ph </span><span class="op">*</span><span class="vs"> tw;</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Load data into shared memory, with bounds checking</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="vs">        ms</span><span class="pp">[tr][tc]</span><span class="vs"> = r &lt; h &amp;&amp; idx </span><span class="op">+</span><span class="vs"> tc &lt; k </span><span class="op">?</span><span class="vs"> m</span><span class="pp">[tc + idx + r * k]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a><span class="vs">        ns</span><span class="pp">[tr][tc]</span><span class="vs"> = c &lt; w &amp;&amp; idx </span><span class="op">+</span><span class="vs"> tr &lt; k </span><span class="op">?</span><span class="vs"> n</span><span class="pp">[(tr + idx) * w + c]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure all threads have loaded data</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Compute partial dot product for this tile</span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="vs">        for </span><span class="kw">(</span><span class="vs">int i = 0; i &lt; tw; </span><span class="op">++</span><span class="vs">i</span><span class="kw">)</span><span class="vs"> p </span><span class="op">+</span><span class="vs">= ms</span><span class="pp">[tr][i]</span><span class="vs"> </span><span class="op">*</span><span class="vs"> ns</span><span class="pp">[i][tc]</span><span class="vs">;</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure computation is complete before next iteration</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Write result to global memory</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">r &lt; h &amp;&amp; c &lt; w</span><span class="kw">)</span><span class="vs"> out</span><span class="pp">[r * w + c]</span><span class="vs"> = p;</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a><span class="vs">/</span><span class="op">**</span></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> PyTorch extension for static matrix multiplication using CUDA</span><span class="dv">.</span></span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param m First input tensor</span></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @param n Second input tensor</span></span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs"> @return Resulting tensor from matrix multiplication</span></span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a><span class="vs"> </span><span class="op">*</span><span class="vs">/</span></span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor matmul_static</span><span class="kw">(</span><span class="vs">torch::Tensor m, torch::Tensor n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">m</span><span class="kw">)</span><span class="vs">; CHECK_INPUT</span><span class="kw">(</span><span class="vs">n</span><span class="kw">)</span><span class="vs">;  // Validate input tensors</span></span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, w = n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">, k = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a><span class="vs">    TORCH_CHECK</span><span class="kw">(</span><span class="vs">k == n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, "Size mismatch!"</span><span class="kw">)</span><span class="vs">;  // Ensure matrices can be multiplied</span></span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::zeros</span><span class="kw">(</span><span class="vs">{h, w}, m</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;  // Initialize output tensor</span></span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Set up CUDA kernel launch parameters</span></span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 tpb</span><span class="kw">(</span><span class="vs">tw, tw</span><span class="kw">)</span><span class="vs">;  // Threads per block</span></span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;  // Number of blocks</span></span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch CUDA kernel</span></span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a><span class="vs">    matmul_ks&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;</span><span class="kw">(</span><span class="vs">m</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, n</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, h, w, k</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;  // Check for CUDA errors</span></span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Name of the function to be exported</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>fname <span class="op">=</span> <span class="st">'matmul_static'</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate C++ source code for the CUDA extension</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> get_sig(fname, cuda_src)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the CUDA module</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [fname])</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify correctness by comparing with PyTorch's built-in matrix multiplication</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>torch.isclose(module.matmul_static(m1c, m2c), m1c <span class="op">@</span> m2c).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True, device='cuda:0')</code></pre></li>
<li><p><strong>Synchronization:</strong> <code>__syncthreads();</code> ensures all threads within a block have finished a step before proceeding to the next.</p></li>
<li><p><strong>Performance:</strong></p>
<ul>
<li><p>Dynamic shared memory version is unexpectedly slower than the naive CUDA version.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>n <span class="dv">10</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Benchmark the custom CUDA matrix multiplication</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>module.matmul_dyn(m1c,m2c)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()  <span class="co"># Ensure CUDA operations are completed before timing</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>3.2 ms ± 57.5 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
<li><p>Static shared memory version with a fixed tile width is faster.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>n <span class="dv">10</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Benchmark the custom matrix multiplication</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>module.matmul_static(m1c, m2c)</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()  <span class="co"># Ensure CUDA operations are complete before timing</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>2.1 ms ± 23.9 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
</ul></li>
</ul>
</section>
<section id="dynamic-shared-memory-performance-issue-and-solution-update-from-the-future" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-shared-memory-performance-issue-and-solution-update-from-the-future">Dynamic Shared Memory Performance Issue and Solution (Update from the Future)</h3>
<ul>
<li><p><strong>Cause:</strong> CUDA struggles to optimize dynamic shared memory allocation when the tile width is not known at compile time, leading to slower performance.</p></li>
<li><p><strong>Solution:</strong> Use C++ templates to make the tile width a template parameter, enabling the compiler to generate optimized code for specific tile widths.</p></li>
<li><p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA kernel for matrix multiplication</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_begin <span class="op">+</span> <span class="vs">r'''</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="vs">template&lt;int tw&gt;</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void matmul_k</span><span class="kw">(</span><span class="vs">float </span><span class="op">*</span><span class="vs">m, float </span><span class="op">*</span><span class="vs">n, float </span><span class="op">*</span><span class="vs">out, int h, int w, int k</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Thread and block indices</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="vs">    int tc = threadIdx</span><span class="dv">.</span><span class="vs">x, tr = threadIdx</span><span class="dv">.</span><span class="vs">y;</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="vs">    int r = blockIdx</span><span class="dv">.</span><span class="vs">y </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">y </span><span class="op">+</span><span class="vs"> tr, c = blockIdx</span><span class="dv">.</span><span class="vs">x </span><span class="op">*</span><span class="vs"> blockDim</span><span class="dv">.</span><span class="vs">x </span><span class="op">+</span><span class="vs"> tc;</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Shared memory allocation</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="vs">    extern __shared__ float ms</span><span class="pp">[];</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="pp">    float *ns = &amp;ms[tw*tw]</span><span class="vs">;</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    float p = 0</span><span class="dv">.</span><span class="vs">0f;  // Accumulator for dot product</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Iterate over blocks of the input matrices</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    for </span><span class="kw">(</span><span class="vs">int ph = 0; ph &lt; cdiv</span><span class="kw">(</span><span class="vs">k,tw</span><span class="kw">)</span><span class="vs">; </span><span class="op">++</span><span class="vs">ph</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a><span class="vs">        int idx = ph </span><span class="op">*</span><span class="vs"> tw;</span></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Load data into shared memory</span></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a><span class="vs">        ms</span><span class="pp">[tr*tw + tc]</span><span class="vs"> = r &lt; h &amp;&amp; idx</span><span class="op">+</span><span class="vs">tc &lt; k </span><span class="op">?</span><span class="vs"> m</span><span class="pp">[tc+idx + r*k]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a><span class="vs">        ns</span><span class="pp">[tr*tw + tc]</span><span class="vs"> = c &lt; w &amp;&amp; idx</span><span class="op">+</span><span class="vs">tr &lt; k </span><span class="op">?</span><span class="vs"> n</span><span class="pp">[(tr+idx)*w + c]</span><span class="vs"> : 0</span><span class="dv">.</span><span class="vs">0f;</span></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure all threads have loaded data</span></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Compute partial dot product</span></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a><span class="vs">        for </span><span class="kw">(</span><span class="vs">int i = 0; i &lt; tw; </span><span class="op">++</span><span class="vs">i</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a><span class="vs">            p </span><span class="op">+</span><span class="vs">= ms</span><span class="pp">[tr*tw + i]</span><span class="vs"> </span><span class="op">*</span><span class="vs"> ns</span><span class="pp">[tw*i + tc]</span><span class="vs">;</span></span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a><span class="vs">        }</span></span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a><span class="vs">        __syncthreads</span><span class="kw">()</span><span class="vs">;  // Ensure all threads have used the data</span></span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Write result to global memory</span></span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a><span class="vs">    if </span><span class="kw">(</span><span class="vs">r &lt; h &amp;&amp; c &lt; w</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a><span class="vs">        out</span><span class="pp">[r*w + c]</span><span class="vs"> = p;</span></span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># C++ wrapper function for the CUDA kernel</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">+=</span> <span class="vs">r'''</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor matmul_dyn1</span><span class="kw">(</span><span class="vs">torch::Tensor m, torch::Tensor n</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">m</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT</span><span class="kw">(</span><span class="vs">n</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Get dimensions of input matrices</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, w = n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">, k = m</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">1</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check if matrices can be multiplied</span></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a><span class="vs">    TORCH_CHECK</span><span class="kw">(</span><span class="vs">k == n</span><span class="dv">.</span><span class="vs">size</span><span class="kw">(</span><span class="vs">0</span><span class="kw">)</span><span class="vs">, "Size mismatch!"</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Create output tensor</span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::zeros</span><span class="kw">(</span><span class="vs">{h, w}, m</span><span class="dv">.</span><span class="vs">options</span><span class="kw">())</span><span class="vs">;</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    int TW = 16;  // Thread block width </span><span class="kw">(</span><span class="vs">TODO: Calculate this dynamically</span><span class="kw">)</span></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    size_t size = TW</span><span class="op">*</span><span class="vs">TW</span><span class="op">*</span><span class="vs">2 </span><span class="op">*</span><span class="vs"> sizeof</span><span class="kw">(</span><span class="vs">float</span><span class="kw">)</span><span class="vs"> </span><span class="op">+</span><span class="vs"> 1;  // Shared memory size</span></span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Define thread block and grid dimensions</span></span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 tpb</span><span class="kw">(</span><span class="vs">TW, TW</span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    dim3 blocks</span><span class="kw">(</span><span class="vs">cdiv</span><span class="kw">(</span><span class="vs">w, tpb</span><span class="dv">.</span><span class="vs">x</span><span class="kw">)</span><span class="vs">, cdiv</span><span class="kw">(</span><span class="vs">h, tpb</span><span class="dv">.</span><span class="vs">y</span><span class="kw">))</span><span class="vs">;</span></span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Lambda function to launch kernel</span></span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto f = </span><span class="pp">[&amp;]</span><span class="kw">(</span><span class="vs">auto kf</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a><span class="vs">        kf&lt;&lt;&lt;blocks, tpb, size&gt;&gt;&gt;</span><span class="kw">(</span></span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a><span class="vs">            m</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, n</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, output</span><span class="dv">.</span><span class="vs">data_ptr&lt;float&gt;</span><span class="kw">()</span><span class="vs">, h, w, k</span></span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a><span class="vs">        </span><span class="kw">)</span><span class="vs">;</span></span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a><span class="vs">    };</span></span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch kernel based on thread block size</span></span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a><span class="vs">    switch</span><span class="kw">(</span><span class="vs">TW</span><span class="kw">)</span><span class="vs"> {</span></span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a><span class="vs">        case 8: f</span><span class="kw">(</span><span class="vs">matmul_k&lt;8&gt;</span><span class="kw">)</span><span class="vs">; break;</span></span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a><span class="vs">        case 16: f</span><span class="kw">(</span><span class="vs">matmul_k&lt;16&gt;</span><span class="kw">)</span><span class="vs">; break;</span></span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a><span class="vs">        case 32: f</span><span class="kw">(</span><span class="vs">matmul_k&lt;32&gt;</span><span class="kw">)</span><span class="vs">; break;</span></span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a><span class="vs">        default: break;</span></span>
<span id="cb72-36"><a href="#cb72-36" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb72-37"><a href="#cb72-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-38"><a href="#cb72-38" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check for CUDA errors</span></span>
<span id="cb72-39"><a href="#cb72-39" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="kw">()</span><span class="vs">;</span></span>
<span id="cb72-40"><a href="#cb72-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-41"><a href="#cb72-41" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb72-42"><a href="#cb72-42" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb72-43"><a href="#cb72-43" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>Define a C++ template function with tile width as a template parameter.</li>
<li>Support a fixed set of tile widths and compile a separate kernel version for each.</li>
<li>Use a lambda function to call the appropriate kernel version based on the chosen tile width.</li>
</ul></li>
<li><p><strong>Benefits:</strong> Enables optimized performance while allowing for some flexibility in tile width selection.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure execution time of the following code</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define function name</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>fname <span class="op">=</span> <span class="st">'matmul_dyn1'</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate C++ function signature</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> get_sig(fname, cuda_src)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CUDA module with optimization</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_cuda(cuda_src, cpp_src, [fname], opt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the function from the loaded module</span></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>func <span class="op">=</span> <span class="bu">getattr</span>(module, fname)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>CPU times: user 49.5 ms, sys: 63.7 ms, total: 113 ms
Wall time: 41.1 s</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify correctness of the custom matrix multiplication</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(func(m1c, m2c), m1c <span class="op">@</span> m2c).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True, device='cuda:0')</code></pre>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>n <span class="dv">10</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure execution time of the custom matrix multiplication</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>func(m1c, m2c)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure all CUDA operations are completed</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>2.06 ms ± 51.2 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
</ul>
</section>
</section>
<section id="implementing-tiling-with-numba" class="level2">
<h2 class="anchored" data-anchor-id="implementing-tiling-with-numba">Implementing Tiling with Numba</h2>
<ul>
<li><p><strong><a href="https://numba.readthedocs.io/en/stable/index.html">Numba</a>:</strong> An alternative library for writing CUDA code directly in Python.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install numba</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> <span class="st">"numpy&lt;2.1"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba.cuda <span class="im">import</span> as_cuda_array <span class="im">as</span> ca</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>CUDA Kernel (Numba):</strong> Python code decorated with <code>@cuda.jit</code> to indicate it’s a CUDA kernel.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_k_numba(m, n, out, tw):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication on GPU using CUDA.</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This kernel function multiplies matrices 'm' and 'n', storing the result in 'out'.</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="co">    It uses shared memory and tiling for improved performance.</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="co">    m (ndarray): First input matrix</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="co">    n (ndarray): Second input matrix</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="co">    out (ndarray): Output matrix to store the result</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="co">    tw (int): Tile width for shared memory optimization</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Note: This function is designed to be called from a host function, not directly.</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get CUDA thread and block information</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    cbi, cbd, tid <span class="op">=</span> cuda.blockIdx, cuda.blockDim, cuda.threadIdx</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>    tc, tr <span class="op">=</span> tid.x, tid.y</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate global row and column indices</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>    r, c <span class="op">=</span> cbi.y <span class="op">*</span> cbd.y <span class="op">+</span> tr, cbi.x <span class="op">*</span> cbd.x <span class="op">+</span> tc</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get input matrix dimensions</span></span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Allocate shared memory for tile-based computation</span></span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a>    shar <span class="op">=</span> cuda.shared.array(<span class="dv">0</span>, dtype<span class="op">=</span>np.float32)</span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a>    ms, ns <span class="op">=</span> shar[:tw<span class="op">*</span>tw], shar[tw<span class="op">*</span>tw:<span class="dv">2</span><span class="op">*</span>tw<span class="op">*</span>tw]  <span class="co"># Split shared memory for both input matrices</span></span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize partial sum</span></span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.float32(<span class="fl">0.0</span>)</span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over tiles</span></span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ph <span class="kw">in</span> <span class="bu">range</span>(math.ceil(k<span class="op">/</span>tw)):</span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> ph <span class="op">*</span> tw</span>
<span id="cb81-38"><a href="#cb81-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-39"><a href="#cb81-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load data into shared memory, with boundary checks</span></span>
<span id="cb81-40"><a href="#cb81-40" aria-hidden="true" tabindex="-1"></a>        ms[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> m[r, tc<span class="op">+</span>idx] <span class="cf">if</span> r <span class="op">&lt;</span> h <span class="kw">and</span> idx<span class="op">+</span>tc <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb81-41"><a href="#cb81-41" aria-hidden="true" tabindex="-1"></a>        ns[tr<span class="op">*</span>tw<span class="op">+</span>tc] <span class="op">=</span> n[tr<span class="op">+</span>idx, c] <span class="cf">if</span> c <span class="op">&lt;</span> w <span class="kw">and</span> idx<span class="op">+</span>tr <span class="op">&lt;</span> k <span class="cf">else</span> <span class="fl">0.</span></span>
<span id="cb81-42"><a href="#cb81-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-43"><a href="#cb81-43" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()  <span class="co"># Ensure all threads have loaded data</span></span>
<span id="cb81-44"><a href="#cb81-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-45"><a href="#cb81-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute partial dot product for this tile</span></span>
<span id="cb81-46"><a href="#cb81-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(tw):</span>
<span id="cb81-47"><a href="#cb81-47" aria-hidden="true" tabindex="-1"></a>            p <span class="op">+=</span> ms[tr<span class="op">*</span>tw<span class="op">+</span>i] <span class="op">*</span> ns[i<span class="op">*</span>tw<span class="op">+</span>tc]</span>
<span id="cb81-48"><a href="#cb81-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-49"><a href="#cb81-49" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()  <span class="co"># Ensure all threads have used the data before next iteration</span></span>
<span id="cb81-50"><a href="#cb81-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-51"><a href="#cb81-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the result if within output matrix bounds</span></span>
<span id="cb81-52"><a href="#cb81-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r <span class="op">&lt;</span> h <span class="kw">and</span> c <span class="op">&lt;</span> w:</span>
<span id="cb81-53"><a href="#cb81-53" aria-hidden="true" tabindex="-1"></a>        out[r, c] <span class="op">=</span> p</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Shared Memory:</strong> <code>cuda.shared.array</code> creates dynamic shared memory arrays.</li>
<li><strong>Synchronization:</strong> <code>cuda.syncthreads()</code> for thread synchronization.</li>
</ul></li>
<li><p><strong>Kernel Launching:</strong> Uses square brackets instead of triple angle brackets (e.g., <code>kernel[blocks, threadsperblock, stream, shared_mem_size](...)</code>).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul_2d_numba(m, n, tw<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform matrix multiplication using CUDA.</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function prepares the CUDA kernel call for matrix multiplication.</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="co">    m (Tensor): First input matrix (PyTorch tensor on CUDA)</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="co">    n (Tensor): Second input matrix (PyTorch tensor on CUDA)</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="co">    tw (int): Tile width for shared memory optimization (default: 16)</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor: Result of matrix multiplication</span></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Raises:</span></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a><span class="co">    AssertionError: If input matrices have mismatched inner dimensions</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>    h, k <span class="op">=</span> m.shape</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>    k2, w <span class="op">=</span> n.shape</span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> k <span class="op">==</span> k2, <span class="st">"Size mismatch!"</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize output matrix</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> torch.zeros(h, w, dtype<span class="op">=</span>m.dtype, device<span class="op">=</span>m.device)</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up CUDA kernel parameters</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>    dyn_shared_mem_size <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> tw <span class="op">*</span> tw <span class="op">*</span> <span class="dv">4</span>  <span class="co"># Size of shared memory in bytes</span></span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>    tpb <span class="op">=</span> tw, tw  <span class="co"># Threads per block</span></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> cdiv(w, tpb[<span class="dv">0</span>]), cdiv(h, tpb[<span class="dv">1</span>])  <span class="co"># Calculate grid dimensions</span></span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch CUDA kernel</span></span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a>    matmul_k_numba[blocks, tpb, <span class="dv">0</span>, dyn_shared_mem_size](ca(m), ca(n), ca(out), tw)</span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify correctness of the implementation</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(matmul_2d_numba(m1c, m2c), m1c<span class="op">@</span>m2c).<span class="bu">all</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>tensor(True, device='cuda:0')</code></pre></li>
<li><p><strong>Performance:</strong> The Numba version with dynamic shared memory is slower than the optimized CUDA C version but still provides CUDA-level speed.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>n <span class="dv">10</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Benchmark the implementation</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>matmul_2d_numba(m1c, m2c)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()  <span class="co"># Ensure all CUDA operations are completed before timing</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>7.8 ms ± 80.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre></li>
<li><p><strong>Benefits:</strong></p>
<ul>
<li>Faster compilation times compared to PyTorch’s CUDA C/C++ approach.
<ul>
<li>Allows for faster iteration during development.</li>
</ul></li>
<li>No need to flatten tensors (supports multidimensional indexing).</li>
<li>Access to tensor shape information within the kernel.</li>
</ul></li>
<li><p><strong><a href="https://numba.readthedocs.io/en/stable/cuda/simulator.html">CUDA Simulator</a>:</strong> Numba provides a built-in CUDA simulator by setting the environment variable <code>NUMBA_ENABLE_CUDASIM=1</code>.</p>
<ul>
<li>Executes CUDA code as pure Python on the CPU, allowing for debugging and experimentation with small datasets.</li>
</ul></li>
<li><p><strong>Development Workflow:</strong></p>
<ol type="1">
<li>Develop and debug CUDA kernels in Numba with the simulator enabled.</li>
<li>Disable the simulator to run the code on the GPU.</li>
<li>Optionally, convert the Numba code to CUDA C/C++ using ChatGPT for deployment.</li>
</ol></li>
</ul>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<ul>
<li><strong>Shipping Numba Kernels and AOT Compilation:</strong>
<ul>
<li><strong>AOT Compilation:</strong> Numba’s AOT was discussed as a potential deployment simplification solution.</li>
<li><strong>AOT Deprecation:</strong> Numba’s AOT is deprecated (February 2024), with a replacement planned but unspecified.</li>
</ul></li>
<li><strong>Performance Comparisons and Optimization Opportunities:</strong>
<ul>
<li><strong>Optimization Tools:</strong> TVM and Mojo GPU’s auto-tune (expected late February/March 2024) were mentioned as potential optimization aids.</li>
</ul></li>
<li><strong>PyTorch’s Matrix Multiplication Implementation:</strong>
<ul>
<li>PyTorch primarily uses cuBLAS.</li>
<li><strong>Torch Compile and Inductor:</strong> Torch Compile’s experimental mode (torch.inductor.config) was mentioned as a potential alternative backend.</li>
<li><strong>Profiling for Backend Identification:</strong> PyTorch’s profiler can reveal the backend used through function signatures.</li>
</ul></li>
<li><strong>Compilation Speed and Iterative Development:</strong>
<ul>
<li><strong>Compilation Speed Importance:</strong> Fast compilation was emphasized as crucial for iterative development.</li>
<li><strong>Fast Compilation Benefits:</strong> Fast compilation, aided by tools like the CUDA simulator and Numba’s CUDA JIT, enhances productivity and reduces debugging time.</li>
</ul></li>
<li><strong>ChatGPT’s Role in CUDA Development:</strong>
<ul>
<li><strong>ChatGPT’s Code Generation Capabilities:</strong> ChatGPT is useful for code conversion and API usage but less effective for novel algorithms.</li>
</ul></li>
<li><strong>Numba vs.&nbsp;Triton:</strong>
<ul>
<li><strong>Different Purposes:</strong> Numba and Triton were recognized as valuable tools with distinct strengths, suitable for different use cases. Triton’s limitations in expressing certain CUDA constructs (e.g., 4-bit discretization) were noted.</li>
<li><strong>Complementary Tools:</strong> Numba and Triton were seen as complementary, each offering unique advantages.</li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, an Applied AI Consultant and Educator.</p>
<p>Whether I’m writing an in-depth tutorial or sharing detailed notes, my goal is the same: to bring clarity to complex topics and find practical, valuable insights.</p>
<p>If you need a strategic partner with my approach to thinking and problem-solving for your AI project, I’m here to help. Let’s talk about de-risking your roadmap and building a real-world solution.</p>
<p>Start the conversation with my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> or learn more <a href="../../../about.html">about my approach</a>.</p>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/christianjmills\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>