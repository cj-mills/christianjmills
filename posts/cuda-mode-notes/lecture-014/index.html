<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-11-15">
<meta name="description" content="Lecture #14 provides a practical introduction to writing and optimizing GPU kernels using Triton, featuring comparisons with CUDA and hands-on examples like tensor copying, image processing, and matrix multiplication.">

<title>GPU MODE Lecture 14: Practitioners Guide to Triton – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-61f2d351c58b11e1d25c66c489878dfa.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-fb8cbff63e0d11b0ded76255c6f80362.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="GPU MODE Lecture 14: Practitioners Guide to Triton – Christian Mills">
<meta property="og:description" content="Lecture #14 provides a practical introduction to writing and optimizing GPU kernels using Triton, featuring comparisons with CUDA and hands-on examples like tensor copying, image processing, and matrix multiplication.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="GPU MODE Lecture 14: Practitioners Guide to Triton – Christian Mills">
<meta name="twitter:description" content="Lecture #14 provides a practical introduction to writing and optimizing GPU kernels using Triton, featuring comparisons with CUDA and hands-on examples like tensor copying, image processing, and matrix multiplication.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#overview-of-the-talk" id="toc-overview-of-the-talk" class="nav-link" data-scroll-target="#overview-of-the-talk">Overview of the Talk</a></li>
  <li><a href="#why-and-when-to-use-triton" id="toc-why-and-when-to-use-triton" class="nav-link" data-scroll-target="#why-and-when-to-use-triton">Why and When to Use Triton</a>
  <ul>
  <li><a href="#what-is-triton" id="toc-what-is-triton" class="nav-link" data-scroll-target="#what-is-triton">What is Triton?</a></li>
  <li><a href="#comparing-triton-to-cuda" id="toc-comparing-triton-to-cuda" class="nav-link" data-scroll-target="#comparing-triton-to-cuda">Comparing Triton to CUDA</a></li>
  <li><a href="#torch.compile-vs.-triton" id="toc-torch.compile-vs.-triton" class="nav-link" data-scroll-target="#torch.compile-vs.-triton"><code>torch.compile()</code> vs.&nbsp;Triton</a></li>
  <li><a href="#when-to-use-triton" id="toc-when-to-use-triton" class="nav-link" data-scroll-target="#when-to-use-triton">When to Use Triton</a></li>
  <li><a href="#rough-edges-in-triton" id="toc-rough-edges-in-triton" class="nav-link" data-scroll-target="#rough-edges-in-triton">Rough Edges in Triton</a></li>
  </ul></li>
  <li><a href="#how-to-write-triton-kernels" id="toc-how-to-write-triton-kernels" class="nav-link" data-scroll-target="#how-to-write-triton-kernels">How to Write Triton Kernels</a>
  <ul>
  <li><a href="#debugging-triton-kernels" id="toc-debugging-triton-kernels" class="nav-link" data-scroll-target="#debugging-triton-kernels">Debugging Triton Kernels</a></li>
  <li><a href="#programming-model" id="toc-programming-model" class="nav-link" data-scroll-target="#programming-model">Programming Model</a>
  <ul class="collapse">
  <li><a href="#cuda-vs.-triton" id="toc-cuda-vs.-triton" class="nav-link" data-scroll-target="#cuda-vs.-triton">CUDA vs.&nbsp;Triton</a></li>
  <li><a href="#example-adding-two-vectors" id="toc-example-adding-two-vectors" class="nav-link" data-scroll-target="#example-adding-two-vectors">Example: Adding Two Vectors</a></li>
  <li><a href="#jargon" id="toc-jargon" class="nav-link" data-scroll-target="#jargon">Jargon</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#practical-examples" id="toc-practical-examples" class="nav-link" data-scroll-target="#practical-examples">Practical Examples</a>
  <ul>
  <li><a href="#example-1-copying-a-tensor" id="toc-example-1-copying-a-tensor" class="nav-link" data-scroll-target="#example-1-copying-a-tensor">Example 1: Copying a Tensor</a>
  <ul class="collapse">
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#steps" id="toc-steps" class="nav-link" data-scroll-target="#steps">Steps</a></li>
  <li><a href="#debugging" id="toc-debugging" class="nav-link" data-scroll-target="#debugging">Debugging</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul></li>
  <li><a href="#example-2-grayscaling-an-image" id="toc-example-2-grayscaling-an-image" class="nav-link" data-scroll-target="#example-2-grayscaling-an-image">Example 2: Grayscaling an Image</a>
  <ul class="collapse">
  <li><a href="#objective-1" id="toc-objective-1" class="nav-link" data-scroll-target="#objective-1">Objective</a></li>
  <li><a href="#steps-1" id="toc-steps-1" class="nav-link" data-scroll-target="#steps-1">Steps</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a></li>
  <li><a href="#execution" id="toc-execution" class="nav-link" data-scroll-target="#execution">Execution</a></li>
  </ul></li>
  <li><a href="#example-3-matrix-multiplication" id="toc-example-3-matrix-multiplication" class="nav-link" data-scroll-target="#example-3-matrix-multiplication">Example 3: Matrix Multiplication</a>
  <ul class="collapse">
  <li><a href="#objective-2" id="toc-objective-2" class="nav-link" data-scroll-target="#objective-2">Objective</a></li>
  <li><a href="#decomposition-strategy" id="toc-decomposition-strategy" class="nav-link" data-scroll-target="#decomposition-strategy">Decomposition Strategy</a></li>
  <li><a href="#naive-matrix-multiplication" id="toc-naive-matrix-multiplication" class="nav-link" data-scroll-target="#naive-matrix-multiplication">Naive Matrix Multiplication</a></li>
  <li><a href="#implementation-1" id="toc-implementation-1" class="nav-link" data-scroll-target="#implementation-1">Implementation</a></li>
  </ul></li>
  <li><a href="#example-4-faster-matrix-multiplication" id="toc-example-4-faster-matrix-multiplication" class="nav-link" data-scroll-target="#example-4-faster-matrix-multiplication">Example 4: Faster Matrix Multiplication</a>
  <ul class="collapse">
  <li><a href="#swizzling-for-cache-optimization" id="toc-swizzling-for-cache-optimization" class="nav-link" data-scroll-target="#swizzling-for-cache-optimization">Swizzling for Cache Optimization</a></li>
  <li><a href="#adjusted-kernel" id="toc-adjusted-kernel" class="nav-link" data-scroll-target="#adjusted-kernel">Adjusted Kernel</a></li>
  <li><a href="#validation" id="toc-validation" class="nav-link" data-scroll-target="#validation">Validation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#benchmarking" id="toc-benchmarking" class="nav-link" data-scroll-target="#benchmarking">Benchmarking</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#tools-and-methods" id="toc-tools-and-methods" class="nav-link" data-scroll-target="#tools-and-methods">Tools and Methods</a></li>
  <li><a href="#findings-rtx-4090" id="toc-findings-rtx-4090" class="nav-link" data-scroll-target="#findings-rtx-4090">Findings (RTX 4090)</a>
  <ul class="collapse">
  <li><a href="#benchmark-1" id="toc-benchmark-1" class="nav-link" data-scroll-target="#benchmark-1">Benchmark #1</a></li>
  <li><a href="#benchmark-2" id="toc-benchmark-2" class="nav-link" data-scroll-target="#benchmark-2">Benchmark #2</a></li>
  <li><a href="#benchmark-3" id="toc-benchmark-3" class="nav-link" data-scroll-target="#benchmark-3">Benchmark #3</a></li>
  </ul></li>
  <li><a href="#profiling-tools" id="toc-profiling-tools" class="nav-link" data-scroll-target="#profiling-tools">Profiling Tools</a></li>
  </ul></li>
  <li><a href="#auto-tuning" id="toc-auto-tuning" class="nav-link" data-scroll-target="#auto-tuning">Auto-Tuning</a>
  <ul>
  <li><a href="#concept" id="toc-concept" class="nav-link" data-scroll-target="#concept">Concept</a></li>
  <li><a href="#implementation-in-triton" id="toc-implementation-in-triton" class="nav-link" data-scroll-target="#implementation-in-triton">Implementation in Triton</a></li>
  <li><a href="#observations" id="toc-observations" class="nav-link" data-scroll-target="#observations">Observations</a></li>
  <li><a href="#tips" id="toc-tips" class="nav-link" data-scroll-target="#tips">Tips</a></li>
  </ul></li>
  <li><a href="#conclusion-and-resources" id="toc-conclusion-and-resources" class="nav-link" data-scroll-target="#conclusion-and-resources">Conclusion and Resources</a>
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#further-learning" id="toc-further-learning" class="nav-link" data-scroll-target="#further-learning">Further Learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">GPU MODE Lecture 14: Practitioners Guide to Triton</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">cuda</div>
  </div>
  </div>

<div>
  <div class="description">
    Lecture #14 provides a practical introduction to writing and optimizing GPU kernels using Triton, featuring comparisons with CUDA and hands-on examples like tensor copying, image processing, and matrix multiplication.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>GPU MODE Lecture Notes</strong></a>: My notes from the <strong>GPU MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.</li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#overview-of-the-talk">Overview of the Talk</a></li>
<li><a href="#why-and-when-to-use-triton">Why and When to Use Triton</a><br>
</li>
<li><a href="#how-to-write-triton-kernels">How to Write Triton Kernels</a><br>
</li>
<li><a href="#practical-examples">Practical Examples</a><br>
</li>
<li><a href="#benchmarking">Benchmarking</a><br>
</li>
<li><a href="#auto-tuning">Auto-Tuning</a><br>
</li>
<li><a href="#conclusion-and-resources">Conclusion and Resources</a></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Resource Links:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Resource Links:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>YouTube Recording:</strong> <a href="https://www.youtube.com/watch?v=DdTsX6DQk24">Lecture 14: Practitioners Guide to Triton</a></li>
<li><strong>Code:</strong> <a href="https://github.com/gpu-mode/lectures/tree/main/lecture_014">gpu-mode/lectures/lecture_014</a></li>
</ul>
</div>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li><strong>Speaker</strong>: <a href="https://www.umerha.com/">Umer Adil</a>
<ul>
<li>Former <strong>management consultant</strong> until October 2023.</li>
<li>Transitioned to <strong>technical AI work</strong> focusing on <strong>open-source contributions</strong>.
<ul>
<li>Contributed to projects like <strong><a href="https://www.langchain.com/">LangChain</a></strong> and <strong><a href="https://github.com/gpt-engineer-org/gpt-engineer">GPT Engineer</a></strong>.</li>
<li>Became a <strong>maintainer</strong> for GPT Engineer.</li>
<li>Currently contributing to <strong>Hugging Face’s <a href="https://github.com/huggingface/diffusers">diffusers</a></strong>.
<ul>
<li>Implemented features like <strong>ControlNetXS</strong>, <strong>LoRAs</strong>, etc.</li>
</ul></li>
</ul></li>
<li>Contact:
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/UmerHA/">UmerHA</a></li>
<li><strong>Twitter</strong>: <a href="https://x.com/UmerHAdil"><span class="citation" data-cites="UmerHAdil">@UmerHAdil</span></a></li>
</ul></li>
<li><strong>Support Umer</strong>:
<ul>
<li>Independent open-source contributor.</li>
<li><strong>Ko-fi</strong>: <a href="https://ko-fi.com/umerha">https://ko-fi.com/umerha</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-talk" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-the-talk">Overview of the Talk</h2>
<ul>
<li><strong>Title</strong>: A Practitioner’s Guide to Triton</li>
<li><strong>Agenda</strong>:
<ul>
<li><strong>Why and When to Use Triton</strong></li>
<li><strong>How to Write Triton Kernels</strong>
<ul>
<li><strong>Programming Model</strong></li>
</ul></li>
<li><strong>Practical Examples</strong>:
<ul>
<li>Copying a Tensor</li>
<li>Grayscaling an Image</li>
<li>Fast Matrix-Matrix Multiplication Kernel</li>
</ul></li>
<li><strong>Benchmarking and Auto-Tuning</strong>
<ul>
<li>Performance Measurement</li>
<li>Kernel Optimization</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="why-and-when-to-use-triton" class="level2">
<h2 class="anchored" data-anchor-id="why-and-when-to-use-triton">Why and When to Use Triton</h2>
<section id="what-is-triton" class="level3">
<h3 class="anchored" data-anchor-id="what-is-triton">What is Triton?</h3>
<ul>
<li><strong>Triton</strong> is a language for programming <strong>GPUs</strong>.
<ul>
<li>More convenient than <strong>CUDA</strong>.</li>
<li>Allows writing <strong>Python-like code</strong> that compiles to <strong>PTX</strong> (Parallel Thread Execution).
<ul>
<li>PTX is the same intermediate representation used by CUDA.</li>
</ul></li>
</ul></li>
<li><strong>Triton Compiler</strong>:
<ul>
<li><strong>Optimizes code</strong> by rearranging it for better performance without changing its meaning.</li>
<li>Targets the <strong>same hardware</strong> as CUDA.</li>
</ul></li>
</ul>
</section>
<section id="comparing-triton-to-cuda" class="level3">
<h3 class="anchored" data-anchor-id="comparing-triton-to-cuda">Comparing Triton to CUDA</h3>
<ul>
<li><strong>CUDA</strong>:
<ul>
<li>Like a <strong>high-end camera</strong>.
<ul>
<li>Offers <strong>thousands of knobs</strong> for fine-grained control.</li>
<li>Achieves the <strong>absolute best performance</strong>.</li>
<li><strong>Harder</strong> to write and debug.</li>
</ul></li>
</ul></li>
<li><strong>Triton</strong>:
<ul>
<li>Like a <strong>high-end smartphone camera</strong>.
<ul>
<li><strong>Easier</strong> to use with fewer controls.</li>
<li>Provides <strong>very good performance</strong> with less effort.</li>
<li><strong>Easier</strong> to write and debug.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="torch.compile-vs.-triton" class="level3">
<h3 class="anchored" data-anchor-id="torch.compile-vs.-triton"><code>torch.compile()</code> vs.&nbsp;Triton</h3>
<ul>
<li><strong><code>torch.compile()</code></strong>:
<ul>
<li>Optimizes your <strong>PyTorch code</strong> but not the underlying <strong>kernels</strong>.</li>
<li>Changes your code to make the best use of existing GPU kernels.</li>
<li>Sometimes writes simple new kernels using <strong>Triton</strong>.</li>
</ul></li>
<li><strong>Triton</strong>:
<ul>
<li>Allows writing <strong>custom kernels</strong> for performance-critical parts.</li>
<li>Offers more control over kernel behavior.</li>
</ul></li>
</ul>
</section>
<section id="when-to-use-triton" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-triton">When to Use Triton</h3>
<ul>
<li><strong>Optimization Steps</strong>:
<ol type="1">
<li><strong>Use <code>torch.compile()</code></strong>:
<ul>
<li>Start by using <code>torch.compile()</code> to optimize your code.</li>
</ul></li>
<li><strong>Adapt Your Code</strong>:
<ul>
<li>Rewrite code to be more suitable for <code>torch.compile()</code>.
<ul>
<li>E.g., eliminate <strong>graph breaks</strong> to enable <strong>CUDA graphs</strong>.</li>
</ul></li>
</ul></li>
<li><strong>Profile and Identify Bottlenecks</strong>:
<ul>
<li>Find slow parts of your code using profiling tools.</li>
<li>Write <strong>custom Triton kernels</strong> for these parts.</li>
</ul></li>
<li><strong>Consider CUDA</strong>:
<ul>
<li>If still not fast enough, write custom <strong>CUDA kernels</strong>.</li>
</ul></li>
</ol></li>
<li><strong>Note</strong>: For maximum performance from the start, you may choose <strong>CUDA</strong> directly.</li>
</ul>
</section>
<section id="rough-edges-in-triton" class="level3">
<h3 class="anchored" data-anchor-id="rough-edges-in-triton">Rough Edges in Triton</h3>
<ul>
<li><strong>New-ish Project</strong>:
<ul>
<li>Contains <strong>rough edges</strong>; code may not behave as expected.</li>
<li>Expected to become more polished over time.</li>
</ul></li>
<li><strong>Recommendation</strong>:
<ul>
<li><strong>Debugging</strong> is important; use “simulator mode” when possible.</li>
<li>Be aware of limitations on older GPUs or with certain operations.</li>
</ul></li>
</ul>
</section>
</section>
<section id="how-to-write-triton-kernels" class="level2">
<h2 class="anchored" data-anchor-id="how-to-write-triton-kernels">How to Write Triton Kernels</h2>
<section id="debugging-triton-kernels" class="level3">
<h3 class="anchored" data-anchor-id="debugging-triton-kernels">Debugging Triton Kernels</h3>
<ul>
<li><p><strong>Simulator Mode</strong>:</p>
<ul>
<li>Set environment variable <code>TRITON_INTERPRET='1'</code>.
<ul>
<li>Enables debugging by running kernels on the <strong>CPU</strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li><strong>Debug</strong> and <strong>print</strong> variables like in CPU programs.</li>
<li>Easier to <strong>set breakpoints</strong> and inspect program flow.</li>
</ul></li>
<li><p><strong>Utility Functions</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'TRITON_INTERPRET'</span>] <span class="op">=</span> <span class="st">'1'</span> <span class="co"># needs to be set *before* triton is imported</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.core.debugger <span class="im">import</span> set_trace</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_pid_conds(conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Test if conditions on program IDs (PIDs) are fulfilled.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        conds (str): String containing conditions to check. Multiple conditions are separated by commas.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">                     Each condition consists of an operator and a number.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        pid_0 (list): First program ID value in a single-element list. Default: [0]</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">        pid_1 (list): Second program ID value in a single-element list. Default: [0]</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">        pid_2 (list): Third program ID value in a single-element list. Default: [0]</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Examples:</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">        '=0'     -&gt; Checks if pid_0 equals 0</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">        ',&gt;1'    -&gt; Checks if pid_1 is greater than 1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">        '&gt;1,=0'  -&gt; Checks if pid_0 &gt; 1 AND pid_1 = 0</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">        bool: True if all conditions are met, False otherwise</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract PID values from lists</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    pids <span class="op">=</span> pid_0[<span class="dv">0</span>], pid_1[<span class="dv">0</span>], pid_2[<span class="dv">0</span>]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove spaces and split conditions by comma</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    conds <span class="op">=</span> conds.replace(<span class="st">' '</span>,<span class="st">''</span>).split(<span class="st">','</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check each condition against corresponding PID</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (cond, pid) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(conds, pids)):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cond <span class="op">==</span> <span class="st">''</span>: <span class="cf">continue</span>  <span class="co"># Skip empty conditions</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split condition into operator and threshold value</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        op, threshold <span class="op">=</span> cond[<span class="dv">0</span>], <span class="bu">int</span>(cond[<span class="dv">1</span>:])</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validate operator</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        valid_ops <span class="op">=</span> [<span class="st">'&lt;'</span>, <span class="st">'&gt;'</span>, <span class="st">'&gt;='</span>, <span class="st">'&lt;='</span>, <span class="st">'='</span>, <span class="st">'!='</span>]</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> op <span class="kw">not</span> <span class="kw">in</span> valid_ops:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Rules may only use these ops: </span><span class="sc">{</span>valid_ops<span class="sc">}</span><span class="ss">. Invalid rule: '</span><span class="sc">{</span>cond<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert '=' to '==' for Python evaluation</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        op <span class="op">=</span> <span class="st">'=='</span> <span class="cf">if</span> op <span class="op">==</span> <span class="st">'='</span> <span class="cf">else</span> op</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate condition</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">eval</span>(<span class="ss">f'</span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>op<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>threshold<span class="sc">}</span><span class="ss">'</span>): </span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong><code>check_tensors_gpu_ready</code></strong>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_tensors_gpu_ready(<span class="op">*</span>tensors):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Verify that all input tensors are contiguous and on GPU.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        *tensors: Variable number of PyTorch tensors to check</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Raises:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        AssertionError: If any tensor is not contiguous or not on GPU</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> tensors:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> t.is_contiguous(), <span class="st">"A tensor is not contiguous"</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip GPU check if in simulator mode</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.environ.get(<span class="st">'TRITON_INTERPRET'</span>) <span class="op">==</span> <span class="st">'1'</span>:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> t.is_cuda, <span class="st">"A tensor is not on cuda"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Ensure data is ready for GPU execution.
<ol type="1">
<li>Assert all tensors are contiguous in memory</li>
<li>Assert all tensors are on GPU, if not simulating</li>
</ol></li>
</ul></li>
<li><p><strong><code>print_if</code></strong>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_if(txt, conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Print text if specified PID conditions are met.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Useful for debugging specific threads in GPU kernels.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        txt (str): Text to print</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">        conds (str): Conditions string (same format as test_pid_conds)</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        pid_0, pid_1, pid_2 (list): Program ID values to check</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> test_pid_conds(conds, pid_0, pid_1, pid_2):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(txt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Output variable values for debugging, depending on conditions on PIDs</li>
</ul></li>
<li><p><strong><code>breakpoint_if</code></strong>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> breakpoint_if(conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Set a breakpoint if specified PID conditions are met.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Useful for debugging specific threads in GPU kernels.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        conds (str): Conditions string (same format as test_pid_conds)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">        pid_0, pid_1, pid_2 (list): Program ID values to check</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> test_pid_conds(conds, pid_0, pid_1, pid_2):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        set_trace()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Pause execution at specific points, depending on conditions on PIDs</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="programming-model" class="level3">
<h3 class="anchored" data-anchor-id="programming-model">Programming Model</h3>
<section id="cuda-vs.-triton" class="level4">
<h4 class="anchored" data-anchor-id="cuda-vs.-triton">CUDA vs.&nbsp;Triton</h4>
<ul>
<li><strong>CUDA</strong>:
<ul>
<li><strong>Two-tiered Decomposition</strong>:
<ul>
<li><strong>Blocks</strong>: Groups of threads.</li>
<li><strong>Threads</strong>: Operate on <strong>scalar values</strong>.</li>
</ul></li>
<li>Threads within a block share the same <strong>Streaming Multiprocessor (SM)</strong> and <strong>shared memory</strong>.</li>
</ul></li>
<li><strong>Triton</strong>:
<ul>
<li><strong>One-tiered Decomposition</strong>:
<ul>
<li><strong>Programs</strong> (equivalent to blocks in CUDA).</li>
</ul></li>
<li>Operates on <strong>vectors</strong> instead of scalars.</li>
<li><strong>Vectorized Operations</strong>:
<ul>
<li>All operations (loading, computing, storing) are performed on vectors.</li>
</ul></li>
<li>No explicit management of threads or shared memory.</li>
</ul></li>
</ul>
</section>
<section id="example-adding-two-vectors" class="level4">
<h4 class="anchored" data-anchor-id="example-adding-two-vectors">Example: Adding Two Vectors</h4>
<ul>
<li><p><strong>Task</strong>:</p>
<ul>
<li>Add vectors <strong>X</strong> and <strong>Y</strong> of size <strong>8</strong> to produce <strong>Z</strong>.</li>
</ul></li>
<li><p><strong>CUDA Approach</strong>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_cuda_k(x, y, z, n, bs):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A simplified Python representation of a CUDA kernel that adds two vectors element by element.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    This function demonstrates how parallel processing works in CUDA, where multiple blocks and</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    threads process different parts of the data simultaneously.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        x (array): First input vector to be added</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">        y (array): Second input vector to be added</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        z (array): Output vector where results will be stored</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">        n (int): Total size of the input vectors</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        bs (int): Block size - number of threads per block</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">                 (determines how many elements each block processes)</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">        If n = 8 and bs = 4:</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">        - We'll have 2 blocks (block_id: 0,1)</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">        - Each block has 4 threads (thread_id: 0,1,2,3)</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">        - Each thread processes one element</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">        block 0 handles indices 0-3, block 1 handles indices 4-7</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In CUDA, these values would be automatically set based on the GPU's</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># thread and block configuration. Here they're placeholders.</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    block_id <span class="op">=</span> ...  <span class="co"># Block ID (e.g., 0 or 1 if we have 2 blocks)</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    thread_id <span class="op">=</span> ... <span class="co"># Thread ID within the block (e.g., 0,1,2,3 if block size is 4)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate which element this specific thread should process</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: If block_id=1, bs=4, thread_id=2:</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># offs = 1 * 4 + 2 = 6 (this thread processes the 7th element)</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> block_id <span class="op">*</span> bs <span class="op">+</span> thread_id</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only process if we're within the vector bounds</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This check is necessary because the number of threads might be more</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># than the actual data we need to process</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> offs <span class="op">&lt;</span> n:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each thread reads its assigned values from the input vectors</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        x_value <span class="op">=</span> x[offs]  <span class="co"># Get value from first input vector</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        y_value <span class="op">=</span> y[offs]  <span class="co"># Get value from second input vector</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform the addition operation</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        z_value <span class="op">=</span> x_value <span class="op">+</span> y_value</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the result in the output vector</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        z[offs] <span class="op">=</span> z_value</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: In actual CUDA programming, all variables above are scalars</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (single values, not arrays). Each thread works with just one element,</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># but many threads run in parallel to process the entire array quickly.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Use <strong>2 blocks</strong> with <strong>4 threads</strong> each (block size of 4).</li>
<li>Each thread computes on a <strong>scalar value</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/2_cuda_variables.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
<li><p><strong>Triton Approach</strong>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_triton_k(x, y, z, n, bs):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A representation of a Triton kernel that adds two vectors element by element.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Unlike CUDA, Triton operates on vectors (groups of elements) rather than individual scalars,</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    which can lead to more efficient code execution.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">        x (array): First input vector to be added</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">        y (array): Second input vector to be added</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">        z (array): Output vector where results will be stored</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">        n (int): Total size of the input vectors</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">        bs (int): Block size - number of elements to process in each block</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">                 (This determines the size of the vectors we operate on)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Key Differences from CUDA:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">    - Triton processes vectors (multiple elements at once) instead of single values</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    - Operations are vectorized, meaning they work on entire arrays simultaneously</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">    - No explicit thread_id needed as Triton handles multiple elements per block</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the block ID for this kernel instance</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tl.program_id(0) is Triton's way of identifying which block we're processing</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    block_id <span class="op">=</span> tl.program_id(<span class="dv">0</span>)  <span class="co"># Example: 0 or 1 if processing in two blocks</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a vector of offsets for this block</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tl.arange(0, bs) creates a vector like [0, 1, 2, ..., bs-1]</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For example, if block_id=1 and bs=4:</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># offs = 4 + [0,1,2,3] = [4,5,6,7]</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> block_id <span class="op">*</span> bs <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs)  <span class="co"># Vector of indices to process</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a mask for valid elements</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This returns a vector of boolean values</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: if n=6 and offs=[4,5,6,7], mask=[True,True,False,False]</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n  <span class="co"># Vector of bools showing which elements are valid</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load multiple elements at once from input vectors</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x[offs] loads multiple values in parallel</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: if offs=[4,5,6,7], this loads four elements at once</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> x[offs]  <span class="co"># Load vector of values from first input</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    y_values <span class="op">=</span> y[offs]  <span class="co"># Load vector of values from second input</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform vectorized addition</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This adds entire vectors element-wise in one operation</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: [1,2,3,4] + [5,6,7,8] = [6,8,10,12]</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    z_value <span class="op">=</span> x_value <span class="op">+</span> y_value  <span class="co"># Add vectors element-wise</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store results back to memory</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Writes multiple elements at once</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The mask ensures we only write valid results</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    z[offs] <span class="op">=</span> z_value  <span class="co"># Store vector of results</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: All operations above work on vectors (multiple elements at once)</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is more efficient than CUDA's scalar operations because:</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Fewer memory transactions are needed</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Vector operations can utilize SIMD instructions</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Less overhead from individual thread management</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Use <strong>2 programs</strong> (no threads).</li>
<li>Each program operates on a <strong>vector</strong> of size <strong>4</strong>.</li>
<li><strong>Offsets</strong> and <strong>masks</strong> are vectors.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/3_triton_variables.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div></li>
</ul>
</section>
<section id="jargon" class="level4">
<h4 class="anchored" data-anchor-id="jargon">Jargon</h4>
<ul>
<li><strong>Program</strong>:
<ul>
<li>A kernel instance processing a block of data.</li>
</ul></li>
<li><strong>PID (Program ID)</strong>:
<ul>
<li>Equivalent to <strong>Block ID</strong> in CUDA.</li>
</ul></li>
<li><strong>Vectorized Operations</strong>:
<ul>
<li>Simultaneous operations on multiple data points.</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="practical-examples" class="level2">
<h2 class="anchored" data-anchor-id="practical-examples">Practical Examples</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cdiv(n, d):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute ceiling division between two numbers.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">        n: Numerator</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">        d: Denominator</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Ceiling division result</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (n <span class="op">+</span> d <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> d</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>x, y, x<span class="op">+</span>y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(tensor([1, 2, 3, 4, 5, 6]),
 tensor([0, 1, 0, 1, 0, 1]),
 tensor([1, 3, 3, 5, 5, 7]))</code></pre>
<section id="example-1-copying-a-tensor" class="level3">
<h3 class="anchored" data-anchor-id="example-1-copying-a-tensor">Example 1: Copying a Tensor</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy(x, bs, kernel_fn):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Launch a Triton kernel to copy data from one GPU tensor to another.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input tensor to copy from</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">        bs: Block size - number of elements processed per GPU thread block</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">        kernel_fn: Triton kernel function to execute</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">        z: New tensor containing copied data</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create output tensor with same properties as input</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Verify tensors are GPU-ready</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(x, z)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate grid dimensions for GPU execution</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> x.numel()  <span class="co"># Total number of elements</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    n_blocks <span class="op">=</span> cdiv(n, bs)  <span class="co"># Number of thread blocks needed</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> (n_blocks,)  <span class="co"># 1D grid configuration</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch kernel on GPU</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    kernel_fn[grid](x, z, n, bs)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="objective" class="level4">
<h4 class="anchored" data-anchor-id="objective">Objective</h4>
<ul>
<li>Copy tensor <strong>X</strong> of shape <strong>N</strong> to tensor <strong>Z</strong>.</li>
</ul>
</section>
<section id="steps" class="level4">
<h4 class="anchored" data-anchor-id="steps">Steps</h4>
<ol type="1">
<li><strong>Define Kernel</strong>:
<ul>
<li>Use <code>@triton.jit</code> decorator.</li>
<li>Function arguments are pointers to tensors and size parameters.</li>
</ul></li>
<li><strong>Calculate Offsets</strong>:
<ul>
<li>Compute offsets using <strong>PID</strong> and <strong>block size</strong>.</li>
<li><code>offsets = pid * block_size + tl.arange(0, block_size)</code></li>
</ul></li>
<li><strong>Create Mask</strong>:
<ul>
<li>Prevent out-of-bounds access.</li>
<li><code>mask = offsets &lt; N</code></li>
</ul></li>
<li><strong>Load and Store Data</strong>:
<ul>
<li>Load data from <strong>X</strong>: <code>x = tl.load(X + offsets, mask=mask)</code></li>
<li>Store data to <strong>Z</strong>: <code>tl.store(Z + offsets, x, mask=mask)</code></li>
</ul></li>
<li><strong>Launch Kernel</strong>:
<ul>
<li>Determine grid size: <code>grid = (num_blocks,)</code></li>
<li>Call kernel with <code>grid</code> and <code>block_size</code>.</li>
</ul></li>
</ol>
</section>
<section id="debugging" class="level4">
<h4 class="anchored" data-anchor-id="debugging">Debugging</h4>
<ul>
<li><p><strong>Intentional Bug</strong>: Incorrect offset calculation</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic kernel with incorrect offset calculation</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span>  <span class="co"># This decorator converts the Python function into GPU code</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Initial version of copy kernel - demonstrates common mistake.</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Important Notes:</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - The @triton.jit decorator transforms this Python function into GPU code</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - Only a limited set of operations are allowed inside GPU kernels:</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        * Basic arithmetic and logic operations are allowed</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        * Python print() and debugging tools like breakpoints are NOT allowed</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">        * Use specialized Triton functions for GPU operations</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">        x_ptr: Pointer to input tensor data (Triton automatically converts tensor to pointer)</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">        z_ptr: Pointer to output tensor data</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">        n: Total number of elements</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">        bs: Block size (marked as compile-time constant with tl.constexpr)</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Note: This version has a bug - it processes the same elements in each block!</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)  <span class="co"># Get current block ID</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> tl.arange(<span class="dv">0</span>, bs)  <span class="co"># Creates offsets [0, 1, ..., bs-1]</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n  <span class="co"># Prevent out-of-bounds access</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask)  <span class="co"># Load input values</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask)  <span class="co"># Store to output</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [2] | offs = [0 1], mask = [ True  True], x = [1 2]</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([1, 2, 0, 0, 0, 0])</code></pre>
<ul>
<li>Incorrectly calculating offsets without considering <strong>PID</strong>.</li>
<li>Only the first block of data is copied.</li>
</ul></li>
<li><p><strong>Intentional Bug</strong>: Incorrect stride calculation</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Incorrect stride calculation</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Second version - demonstrates another common mistake.</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Key Concepts:</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - When we pass a torch tensor to the kernel, Triton automatically converts it</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">      to a pointer to its first element (that's why we receive x_ptr, not x)</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - GPU kernels run in parallel across many blocks, so correct memory access</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">      patterns are crucial</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Note: This version incorrectly uses 'n' instead of 'bs' for stride calculation,</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">    causing blocks to process wrong sections of memory.</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> pid <span class="op">*</span> n <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs)  <span class="co"># Wrong! Stride should use 'bs', not 'n'</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [6 7], mask = [False False], x = [0 0]
pid = [2] | offs = [12 13], mask = [False False], x = [0 0]</code></pre></li>
<li><p><strong>Solution</strong>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correct implementation</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Final correct version of the copy kernel.</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">    GPU Kernel Rules and Concepts:</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    1. The @triton.jit decorator converts this Python function into GPU code</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Inside GPU kernels:</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">       - You can't use regular Python print() or debuggers</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">       - You must use special Triton functions (tl.*) for operations</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">       - Tensor inputs are automatically converted to memory pointers</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Each block processes a different chunk of data in parallel:</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">       - Block 0 processes elements [0:bs]</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">       - Block 1 processes elements [bs:2*bs]</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">       - Block 2 processes elements [2*bs:3*bs]</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)  <span class="co"># Get current block ID</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> pid <span class="op">*</span> bs <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs)  <span class="co"># Calculate correct offsets for this block</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n  <span class="co"># Prevent out-of-bounds access</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask)  <span class="co"># Load input values</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask)  <span class="co"># Store to output</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [2 3], mask = [ True  True], x = [3 4]
pid = [2] | offs = [4 5], mask = [ True  True], x = [5 6]</code></pre>
<ul>
<li>Adjust offsets to include <code>pid * block_size</code>.</li>
</ul></li>
</ul>
</section>
<section id="key-takeaways" class="level4">
<h4 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h4>
<ul>
<li><strong>Offset Calculation</strong> is crucial.</li>
<li>Use <strong>masks</strong> to handle data boundaries.</li>
<li><strong>Debugging</strong> is facilitated by simulator mode.</li>
</ul>
</section>
</section>
<section id="example-2-grayscaling-an-image" class="level3">
<h3 class="anchored" data-anchor-id="example-2-grayscaling-an-image">Example 2: Grayscaling an Image</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Umer mentioned needing to restart the notebook kernel before running this example because:</p>
<blockquote class="blockquote">
<ol type="1">
<li><em>torchvision can’t be imported, probably due to a circular dependency. -&gt; I currently don’t know why, need to dig deeper.</em></li>
<li><em>the simulated triton kernel below fails, because a float can’t be mutliplied to a uint vector -&gt; Works on GPU w/o simulation, so seems to be a <code>TRITON_INTERPRET</code> bug.</em></li>
</ol>
</blockquote>
<p>However, the underlying issues seem to have been resolved in more recent updates.</p>
</div>
</div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required libraries</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve  <span class="co"># For downloading files from URLs</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision <span class="im">as</span> tv</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> tvf</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> io</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton  <span class="co"># GPU acceleration library</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image URL and download if not already present</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>path_img <span class="op">=</span> Path(<span class="st">'puppy.jpg'</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> path_img.exists():</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    urlretrieve(url, path_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the image using torchvision</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> io.read_image(<span class="st">'puppy.jpg'</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape (channels, height, width): </span><span class="sc">{</span>img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>img[:<span class="dv">2</span>,:<span class="dv">3</span>,:<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>torch.Size([3, 1066, 1600])

tensor([[[117, 119, 117, 113],
         [119, 129, 129, 113],
         [130, 126, 122, 115]],

        [[ 83,  85,  85,  80],
         [ 85,  97,  97,  82],
         [ 98,  93,  89,  83]]], dtype=torch.uint8)</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_img(x, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>), <span class="op">**</span>kwargs):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Display an image using matplotlib</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Image tensor</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">        figsize: Figure size in inches (width, height)</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">        **kwargs: Additional arguments passed to plt.imshow()</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>figsize)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert from CHW (channels, height, width) to HWC format if needed</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(x.shape) <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x.cpu(), <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="objective-1" class="level4">
<h4 class="anchored" data-anchor-id="objective-1">Objective</h4>
<ul>
<li>Convert a color image to grayscale using a Triton kernel.</li>
</ul>
</section>
<section id="steps-1" class="level4">
<h4 class="anchored" data-anchor-id="steps-1">Steps</h4>
<ol type="1">
<li><p><strong>Load Image Data</strong>:</p>
<ul>
<li>Use an image (e.g., a puppy image) as input.</li>
</ul></li>
<li><p><strong>Calculate 2D Offsets</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/4_offset_2d.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li>Compute row and column offsets.</li>
<li>Use broadcasting to create a grid of offsets.</li>
</ul></li>
<li><p><strong>Create Masks</strong>:</p>
<ul>
<li>Handle image boundaries to avoid out-of-bounds access.</li>
</ul></li>
<li><p><strong>Load Color Channels</strong>:</p>
<ul>
<li>Load <strong>R</strong>, <strong>G</strong>, and <strong>B</strong> values using offsets.</li>
</ul></li>
<li><p><strong>Compute Grayscale Values</strong>:</p>
<ul>
<li>Apply formula: <code>grayscale = 0.2989*R + 0.5870*G + 0.1140*B</code></li>
</ul></li>
<li><p><strong>Store Grayscale Data</strong>:</p>
<ul>
<li>Write the grayscale values back to the output tensor.</li>
</ul></li>
</ol>
</section>
<section id="implementation" class="level4">
<h4 class="anchored" data-anchor-id="implementation">Implementation</h4>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">    GPU kernel for converting RGB image to grayscale</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">        x_ptr: Pointer to input RGB image data</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">        out_ptr: Pointer to output grayscale image data</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">        h: Image height</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">        w: Image width</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">        bs0: Block size for height dimension</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">        bs1: Block size for width dimension</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get program IDs for parallel processing</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    pid_0 <span class="op">=</span> tl.program_id(<span class="dv">0</span>)  <span class="co"># Block ID in height dimension</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    pid_1 <span class="op">=</span> tl.program_id(<span class="dv">1</span>)  <span class="co"># Block ID in width dimension</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate offsets for this block</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    offs_0 <span class="op">=</span> pid_0 <span class="op">*</span> bs0 <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs0)  <span class="co"># Offsets in height dimension</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    offs_1 <span class="op">=</span> pid_1 <span class="op">*</span> bs1 <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs1)  <span class="co"># Offsets in width dimension</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate 2D offset matrix</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> w <span class="op">*</span> offs_0[:,<span class="va">None</span>] <span class="op">+</span> offs_1[<span class="va">None</span>, :]</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create masks to handle image boundaries</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    mask_0 <span class="op">=</span> offs_0 <span class="op">&lt;</span> h</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>    mask_1 <span class="op">=</span> offs_1 <span class="op">&lt;</span> w</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask_0[:,<span class="va">None</span>] <span class="op">&amp;</span> mask_1[<span class="va">None</span>,:]</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load RGB channels</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">0</span><span class="op">*</span>h<span class="op">*</span>w <span class="op">+</span> offs, mask<span class="op">=</span>mask)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">1</span><span class="op">*</span>h<span class="op">*</span>w <span class="op">+</span> offs, mask<span class="op">=</span>mask)</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>h<span class="op">*</span>w <span class="op">+</span> offs, mask<span class="op">=</span>mask)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to grayscale using standard weights</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># These weights represent human perception of color:</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Red: 29.89%, Green: 58.70%, Blue: 11.40%</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="fl">0.2989</span><span class="op">*</span>r <span class="op">+</span> <span class="fl">0.5870</span><span class="op">*</span>g <span class="op">+</span> <span class="fl">0.1140</span><span class="op">*</span>b</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the result</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    tl.store(out_ptr <span class="op">+</span> offs, out, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="notes" class="level4">
<h4 class="anchored" data-anchor-id="notes">Notes</h4>
<ul>
<li><strong>Vectorized Operations</strong> simplify processing of 2D data.</li>
<li><strong>Masks</strong> ensure safe memory access.</li>
<li><strong>GPU Compatibility</strong>:
<ul>
<li>Some operations may not work in simulator mode or on older GPUs.</li>
</ul></li>
</ul>
</section>
<section id="execution" class="level4">
<h4 class="anchored" data-anchor-id="execution">Execution</h4>
<ul>
<li><p><strong>Kernel Launch</strong>:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey(x, bs):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert RGB image to grayscale using GPU acceleration</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input RGB image tensor (channels, height, width)</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">        bs: Tuple of block sizes (height, width) for GPU processing</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Grayscale image tensor (height, width)</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    c, h, w <span class="op">=</span> x.shape</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create output tensor</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> torch.empty((h,w), dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define processing grid based on block sizes</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (cdiv(h, meta[<span class="st">'bs0'</span>]), cdiv(w, meta[<span class="st">'bs1'</span>]))</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch GPU kernel</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    rgb2grey_k[grid](x, out, h, w, bs0<span class="op">=</span>bs[<span class="dv">0</span>], bs1<span class="op">=</span>bs[<span class="dv">1</span>])</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out.view(h,w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Define grid dimensions based on image size.</li>
</ul></li>
<li><p><strong>Result</strong>:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize image to a smaller size for faster processing</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> tvf.resize(img, <span class="dv">150</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>ch, h, w <span class="op">=</span> img.shape  <span class="co"># Get channels, height, and width</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>ch,h,w,h<span class="op">*</span>w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(3, 150, 225, 33750)</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>show_img(img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_72_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert image to grayscale and display</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>grey_img <span class="op">=</span> rgb2grey(img.to(<span class="st">'cuda'</span>), bs<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>)).to(<span class="st">'cpu'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>show_img(grey_img, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_80_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li>Successfully converted grayscale image.</li>
</ul></li>
</ul>
</section>
</section>
<section id="example-3-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="example-3-matrix-multiplication">Example 3: Matrix Multiplication</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Had to restart the notebook kernel to produce expected results for this example.</p>
</div>
</div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="objective-2" class="level4">
<h4 class="anchored" data-anchor-id="objective-2">Objective</h4>
<ul>
<li>Implement an efficient matrix multiplication kernel.</li>
</ul>
</section>
<section id="decomposition-strategy" class="level4">
<h4 class="anchored" data-anchor-id="decomposition-strategy">Decomposition Strategy</h4>
<ul>
<li><p><strong>Matrices</strong>:</p>
<ul>
<li><strong>A</strong>: Size <strong>M x K</strong></li>
<li><strong>B</strong>: Size <strong>K x N</strong></li>
<li><strong>C</strong>: Result <strong>M x N</strong></li>
</ul></li>
<li><p><strong>Splitting</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/5_matmul_split.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li>Split <strong>C</strong> along <strong>M</strong> and <strong>N</strong> dimensions.</li>
<li>Map splits to programs (blocks).</li>
<li>Further split along <strong>K</strong> dimension (phases).</li>
</ul></li>
</ul>
</section>
<section id="naive-matrix-multiplication" class="level4">
<h4 class="anchored" data-anchor-id="naive-matrix-multiplication">Naive Matrix Multiplication</h4>
<ol type="1">
<li><strong>Define Kernel</strong>:
<ul>
<li>Use <code>@triton.jit</code> decorator.</li>
</ul></li>
<li><strong>Calculate Offsets</strong>:
<ul>
<li>Compute offsets for <strong>M</strong> and <strong>N</strong> axes.</li>
</ul></li>
<li><strong>Initialize Accumulator</strong>:
<ul>
<li>Set to zero before accumulation.</li>
</ul></li>
<li><strong>Loop Over K Dimension</strong>:
<ul>
<li>For each phase:
<ul>
<li>Load chunks of <strong>A</strong> and <strong>B</strong>.</li>
<li>Multiply and accumulate.</li>
</ul></li>
</ul></li>
<li><strong>Store Result</strong>:
<ul>
<li>Write the computed block to <strong>C</strong>.</li>
</ul></li>
</ol>
</section>
<section id="implementation-1" class="level4">
<h4 class="anchored" data-anchor-id="implementation-1">Implementation</h4>
<ul>
<li><p><strong>Helper Functions:</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Helper Functions for Computing Memory Offsets and Masks =====</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_1d_offset(size, n_prev_chunks):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate 1D memory offsets for a given chunk size and position.</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        size: Size of the current chunk</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n_prev_chunks: Number of previous chunks (used for position)</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Array of offsets for the current chunk</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n_prev_chunks <span class="op">*</span> size <span class="op">+</span> tl.arange(<span class="dv">0</span>, size)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_2d_offset(offs_0, offs_1, stride_0, stride_1<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate 2D memory offsets for matrix operations.</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="co">        offs_0, offs_1: Offsets in first and second dimensions</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co">        stride_0, stride_1: Stride values for memory layout</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co">        2D array of memory offsets</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tl.expand_dims(offs_0, <span class="dv">1</span>)<span class="op">*</span>stride_0 <span class="op">+</span> tl.expand_dims(offs_1, <span class="dv">0</span>)<span class="op">*</span>stride_1</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_1d_mask(offs, <span class="bu">max</span>):</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Create a mask for boundary checking in 1D.</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co">        offs: Current offsets</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="co">        max: Maximum valid offset</span></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Boolean mask indicating valid positions</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> offs <span class="op">&lt;</span> <span class="bu">max</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_2d_mask(offs_0, offs_1, max_0, max_1):</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a><span class="co">    Create a mask for boundary checking in 2D.</span></span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a><span class="co">        offs_0, offs_1: Current offsets in both dimensions</span></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a><span class="co">        max_0, max_1: Maximum valid offsets</span></span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Boolean mask indicating valid positions in 2D</span></span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (tl.expand_dims(offs_0, <span class="dv">1</span>) <span class="op">&lt;</span> max_0) <span class="op">&amp;</span> (tl.expand_dims(offs_1, <span class="dv">0</span>) <span class="op">&lt;</span> max_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Matrix Multiplication Kernel:</strong></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> naive_matmul_k(</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,  <span class="co"># Pointers to input/output matrices</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    m, n, k,              <span class="co"># Matrix dimensions: A(m×k), B(k×n), C(m×n)</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, <span class="co"># Memory strides for matrix A</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn, <span class="co"># Memory strides for matrix B</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn, <span class="co"># Memory strides for output matrix C</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr,     <span class="co"># Block size for M dimension</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    bn: tl.constexpr,     <span class="co"># Block size for N dimension</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    bk: tl.constexpr      <span class="co"># Block size for K dimension</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute matrix multiplication C = A × B using block-wise operations.</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co">    This kernel implements a basic matrix multiplication by:</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Breaking the computation into blocks</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Loading blocks into shared memory</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Computing partial results</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co">    4. Storing the results back to global memory</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co">        a_ptr, b_ptr: Input matrix pointers</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co">        c_ptr: Output matrix pointer</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="co">        m, n, k: Matrix dimensions</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="co">        stride_*: Memory strides for each matrix</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a><span class="co">        bm, bn, bk: Block sizes for tiled computation</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get program ID for the current thread block</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate offsets for the current block</span></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)  <span class="co"># Offset in M dimension</span></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)  <span class="co"># Offset in N dimension</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)      <span class="co"># Initial offset in K dimension</span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate memory offsets for input matrices</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize accumulator for partial results</span></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: allow_tf32 must be set to False for older GPUs</span></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Main computation loop - iterate over K dimension</span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load blocks from input matrices</span></span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)  <span class="co"># Load block from matrix A</span></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)  <span class="co"># Load block from matrix B</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute partial matrix multiplication for current block</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update offsets for next iteration</span></span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate output memory location and mask for boundary conditions</span></span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the result</span></span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul(a, b, matmul_k_fn, bs<span class="op">=</span><span class="dv">16</span>, group_sz<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">    High-level matrix multiplication function that handles kernel launch.</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co">        a, b: Input matrices</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co">        matmul_k_fn: Triton kernel function to use</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co">        bs: Block size for tiled computation</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co">        group_sz: Group size for advanced implementations</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Result of matrix multiplication</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Verify matrix dimensions are compatible</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> a.shape[<span class="dv">1</span>] <span class="op">==</span> b.shape[<span class="dv">0</span>], <span class="st">"matrix dims not compatible for matmul"</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(a, b)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get matrix dimensions</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    (m, k), (_, n) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize output matrix</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.empty((m, n), device<span class="op">=</span>a.device, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate grid dimensions for kernel launch</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (triton.cdiv(m, meta[<span class="st">'bm'</span>]), triton.cdiv(n, meta[<span class="st">'bn'</span>]))</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Handle optional group size parameter</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    group_sz <span class="op">=</span> {} <span class="cf">if</span> group_sz <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> {<span class="st">"group_sz"</span>: group_sz}</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch kernel</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn[grid](</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        a, b, c,                     <span class="co"># Input/output matrices</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        m, n, k,                     <span class="co"># Matrix dimensions</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>        a.stride(<span class="dv">0</span>), a.stride(<span class="dv">1</span>),    <span class="co"># Strides for matrix A</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>        b.stride(<span class="dv">0</span>), b.stride(<span class="dv">1</span>),    <span class="co"># Strides for matrix B</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        c.stride(<span class="dv">0</span>), c.stride(<span class="dv">1</span>),    <span class="co"># Strides for output matrix</span></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>        bm<span class="op">=</span>bs, bn<span class="op">=</span>bs, bk<span class="op">=</span>bs,         <span class="co"># Block sizes</span></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>group_sz</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Results:</strong></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simplified interface using partial application</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>naive_matmul <span class="op">=</span> partial(matmul, matmul_k_fn<span class="op">=</span>naive_matmul_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Small example</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>naive_matmul(a, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Larger example with verification</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare Triton implementation with PyTorch</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>triton_output <span class="op">=</span> naive_matmul(a, b)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>torch_output <span class="op">=</span> torch.matmul(a, b)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify results match within tolerance</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.allclose(triton_output, torch_output, atol<span class="op">=</span><span class="fl">5e-2</span>, rtol<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Triton and Torch match"</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Triton and Torch differ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>✅ Triton and Torch match</code></pre></li>
</ul>
</section>
</section>
<section id="example-4-faster-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="example-4-faster-matrix-multiplication">Example 4: Faster Matrix Multiplication</h3>
<section id="swizzling-for-cache-optimization" class="level4">
<h4 class="anchored" data-anchor-id="swizzling-for-cache-optimization">Swizzling for Cache Optimization</h4>
<ul>
<li><p><strong>Goal</strong>:</p>
<ul>
<li>Improve <strong>L2 cache</strong> utilization.</li>
</ul></li>
<li><p><strong>Swizzling</strong>:</p>
<ul>
<li>Reorder program execution to process blocks that share data closer in time.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/6_matmul_order.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_item(<span class="bu">id</span>): </span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"I'm processing item </span><span class="sc">{</span><span class="bu">id</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate normal sequential processing</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sequential processing:"</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>): </span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    process_item(i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>I'm processing item 0
I'm processing item 1
I'm processing item 2
I'm processing item 3
I'm processing item 4</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> change_id(old_id): </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">5</span><span class="op">-</span>old_id</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate reordered processing</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reordered processing:"</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>): </span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    process_item(change_id(i))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>I'm processing item 5
I'm processing item 4
I'm processing item 3
I'm processing item 2
I'm processing item 1</code></pre></li>
<li><p><strong>Implementation</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/7_swizzling_exmple.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Memory Access Pattern Optimization via Swizzling =====</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Demonstrates memory access pattern optimization using swizzling.</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Swizzling reorders thread blocks to improve memory locality and cache utilization.</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co">        x_ptr: Input tensor pointer</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">        z_ptr: Output tensor pointer</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co">        group_sz: Size of thread block groups for swizzling</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get current thread block coordinates and grid dimensions</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    num_pid_m, num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">0</span>), tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply 2D swizzling to reorder thread blocks</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    pid_m_, pid_n_ <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate memory offsets for original ordering</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    offs_m <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    offs_n <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> get_2d_offset(offs_m, offs_n, stride_0<span class="op">=</span>num_pid_n)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(offs_m, offs_n, max_0<span class="op">=</span>num_pid_m, max_1<span class="op">=</span>num_pid_n)</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate memory offsets for swizzled ordering</span></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>    offs_sw_m <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_m_)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>    offs_sw_n <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_n_)</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>    offs_sw <span class="op">=</span> get_2d_offset(offs_sw_m, offs_sw_n, stride_0<span class="op">=</span>num_pid_n)</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    mask_sw <span class="op">=</span> get_2d_mask(offs_sw_m, offs_sw_n, max_0<span class="op">=</span>num_pid_m, max_1<span class="op">=</span>num_pid_n)</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load from original pattern and store in swizzled pattern</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask<span class="op">=</span>mask)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs_sw, x, mask<span class="op">=</span>mask_sw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate swizzling effect</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>blocks_m, blocks_n <span class="op">=</span> <span class="dv">5</span>, <span class="dv">4</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(blocks_m<span class="op">*</span>blocks_n, device<span class="op">=</span><span class="st">'cuda'</span>).view(blocks_m, blocks_n)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Original matrix:"</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]], device='cuda:0')</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> <span class="op">-</span>torch.ones_like(x)  <span class="co"># Initialize output matrix with -1</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Empty output matrix:"</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1]], device='cuda:0')</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply swizzling</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>swizzle_k[(blocks_m,blocks_n)](x, z, group_sz<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Swizzled matrix:"</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[ 0,  3,  6,  9],
        [ 1,  4,  7, 10],
        [ 2,  5,  8, 11],
        [12, 14, 16, 18],
        [13, 15, 17, 19]], device='cuda:0')</code></pre>
<ul>
<li>Use <a href="https://triton-lang.org/main/python-api/generated/triton.language.swizzle2d.html"><code>tl.swizzle2d(pid, num_pid_m, num_pid_n, group_size)</code></a></li>
</ul></li>
</ul>
</section>
<section id="adjusted-kernel" class="level4">
<h4 class="anchored" data-anchor-id="adjusted-kernel">Adjusted Kernel</h4>
<ul>
<li><p><strong>Modify PID</strong>:</p>
<ul>
<li>Apply swizzling to <strong>PID</strong> before computing offsets.</li>
</ul></li>
<li><p><strong>Benefits</strong>:</p>
<ul>
<li>Reduces the number of unique data loads.</li>
<li>Increases cache hits, improving performance.</li>
</ul></li>
<li><p><strong>Grouped Matrix Multiplication with Swizzling:</strong></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_matmul_k(</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    m, n, k,</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, </span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    group_sz: tl.constexpr</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Matrix multiplication kernel with memory access pattern optimization using swizzling.</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co">    This implementation groups thread blocks to improve cache utilization.</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="co">        a_ptr, b_ptr: Input matrix pointers</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="co">        c_ptr: Output matrix pointer</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="co">        m, n, k: Matrix dimensions</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="co">        stride_*: Memory strides for each matrix</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="co">        bm, bn, bk: Block sizes for tiled computation</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a><span class="co">        group_sz: Size of thread block groups for swizzling</span></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get thread block coordinates and grid dimensions</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>    num_pid_m, num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">0</span>), tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply swizzling to optimize memory access pattern</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate block offsets</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate memory offsets for input matrices</span></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize accumulator</span></span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Main computation loop</span></span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)</span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store results</span></span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="validation" class="level4">
<h4 class="anchored" data-anchor-id="validation">Validation</h4>
<ul>
<li><p><strong>Testing</strong>:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create simplified interface for grouped matrix multiplication</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>grouped_matmul <span class="op">=</span> partial(matmul, matmul_k_fn<span class="op">=</span>grouped_matmul_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Small example</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing with small matrices:"</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Larger example with verification</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing with larger matrices:"</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>triton_output <span class="op">=</span> grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>torch_output <span class="op">=</span> torch.matmul(a, b)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify results</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.allclose(triton_output, torch_output, atol<span class="op">=</span><span class="fl">5e-2</span>, rtol<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Triton and Torch match"</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Triton and Torch differ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>✅ Triton and Torch match</code></pre>
<ul>
<li>Compare output with PyTorch’s <code>torch.matmul</code>.</li>
<li>Use various matrix sizes for thorough testing.</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="benchmarking" class="level2">
<h2 class="anchored" data-anchor-id="benchmarking">Benchmarking</h2>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose</h3>
<ul>
<li><strong>Measure and compare</strong> kernel performance.</li>
<li>Identify performance gains or bottlenecks.</li>
</ul>
</section>
<section id="tools-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="tools-and-methods">Tools and Methods</h3>
<ul>
<li><strong>Triton’s Benchmarking Utilities</strong>:
<ul>
<li>Provides functions to benchmark kernels over input ranges.</li>
</ul></li>
<li><strong>Parameters</strong>:
<ul>
<li>Test different matrix sizes and block sizes.</li>
</ul></li>
</ul>
</section>
<section id="findings-rtx-4090" class="level3">
<h3 class="anchored" data-anchor-id="findings-rtx-4090">Findings (RTX 4090)</h3>
<section id="benchmark-1" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-1">Benchmark #1</h4>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co">Performance Benchmarking for Matrix Multiplication Implementations</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co">Compares the performance of the three matrix multiplication implementations:</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co">1. Naive Triton implementation</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co">2. Grouped Triton implementation (with memory access optimization)</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="co">3. PyTorch's native implementation</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co">The benchmark measures performance in GB/s (gigabytes per second) across different matrix sizes.</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X-axis configuration</span></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>],  <span class="co"># What varies along x-axis</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)],  <span class="co"># Matrix sizes: 32, 64, 128, 256, 512, 1024, 2048</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>        x_log<span class="op">=</span><span class="va">True</span>,  <span class="co"># Use logarithmic scale for x-axis</span></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Different implementations to compare (creates different lines on plot)</span></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>,  <span class="co"># Parameter that determines which implementation to use</span></span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>        line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>],  <span class="co"># Possible implementation values</span></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>        line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],  <span class="co"># Labels for each implementation</span></span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot styling</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],  <span class="co"># Colors and line styles</span></span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>,  <span class="co"># Y-axis label showing throughput</span></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>        plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>,  <span class="co"># Name for saving the plot</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Additional arguments (empty in this case)</span></span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>{},</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Benchmark different matrix multiplication implementations.</span></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a><span class="co">        square_matrix_size: Size of the square matrices to multiply (N×N)</span></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a><span class="co">        provider: Which implementation to benchmark ('naive', 'grouped', or 'torch')</span></span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: (median_performance, min_performance, max_performance) in GB/s</span></span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb59-43"><a href="#cb59-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Performance calculation:</span></span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a><span class="co">    - Matrix multiplication requires reading 2 matrices and writing 1 matrix</span></span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a><span class="co">    - Each matrix has size N×N with 4 bytes per element (float32)</span></span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a><span class="co">    - Total memory moved = 3 * N * N * 4 bytes</span></span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a><span class="co">    - GB/s = (12 * N * N) / (time_in_ms * 1e6)  # 12 = 3 matrices * 4 bytes</span></span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create random input matrices</span></span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define measurement percentiles</span></span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]  <span class="co"># median, 20th percentile, 80th percentile</span></span>
<span id="cb59-56"><a href="#cb59-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-57"><a href="#cb59-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark the requested implementation</span></span>
<span id="cb59-58"><a href="#cb59-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:</span>
<span id="cb59-59"><a href="#cb59-59" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb59-60"><a href="#cb59-60" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: naive_matmul(a, b), </span>
<span id="cb59-61"><a href="#cb59-61" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb59-62"><a href="#cb59-62" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb59-63"><a href="#cb59-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>:</span>
<span id="cb59-64"><a href="#cb59-64" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb59-65"><a href="#cb59-65" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>), </span>
<span id="cb59-66"><a href="#cb59-66" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb59-67"><a href="#cb59-67" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb59-68"><a href="#cb59-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:</span>
<span id="cb59-69"><a href="#cb59-69" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb59-70"><a href="#cb59-70" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: torch.matmul(a,b), </span>
<span id="cb59-71"><a href="#cb59-71" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb59-72"><a href="#cb59-72" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb59-73"><a href="#cb59-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-74"><a href="#cb59-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert milliseconds to GB/s</span></span>
<span id="cb59-75"><a href="#cb59-75" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span>  <span class="co"># Formula explained in docstring</span></span>
<span id="cb59-76"><a href="#cb59-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-77"><a href="#cb59-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the benchmark</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Running performance benchmark..."</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This will test matrix sizes from 32×32 to 2048×2048"</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"For each size, we'll compare naive Triton, grouped Triton, and PyTorch implementations"</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_139_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.093750  0.093750  0.046875
1                64.0  0.187500  0.158940  0.083333
2               128.0  0.290909  0.284024  0.214286
3               256.0  0.333333  0.333333  0.300000
4               512.0  0.190855  0.193548  0.300000
5              1024.0  0.055379  0.055339  0.218182
6              2048.0  0.014159  0.014179  0.079470</code></pre>
</section>
<section id="benchmark-2" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-2">Benchmark #2</h4>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Impact of Batch Size =====</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X-axis: varying batch sizes</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'batch_size'</span>],</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>)],  <span class="co"># Testing batch sizes: 16, 32, 64</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>        x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compare different implementations</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>,</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>        line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>],</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>        line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot styling</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>,</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>,</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>{}</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(batch_size, provider):</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Benchmark matrix multiplication with varying batch sizes.</span></span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a><span class="co">    This benchmark keeps matrix size fixed (512×512) and varies the computation</span></span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a><span class="co">    batch size to understand its impact on performance.</span></span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size: Size of computation batches (block size)</span></span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a><span class="co">        provider: Which implementation to benchmark</span></span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: (median_performance, min_performance, max_performance) in GB/s</span></span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fixed matrix size of 512×512</span></span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark each implementation with varying batch sizes</span></span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:</span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span>batch_size),</span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb62-46"><a href="#cb62-46" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-47"><a href="#cb62-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>:</span>
<span id="cb62-48"><a href="#cb62-48" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb62-49"><a href="#cb62-49" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: grouped_matmul(a, b, bs<span class="op">=</span>batch_size, group_sz<span class="op">=</span><span class="dv">8</span>),</span>
<span id="cb62-50"><a href="#cb62-50" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb62-51"><a href="#cb62-51" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-52"><a href="#cb62-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:</span>
<span id="cb62-53"><a href="#cb62-53" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb62-54"><a href="#cb62-54" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: torch.matmul(a,b),</span>
<span id="cb62-55"><a href="#cb62-55" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb62-56"><a href="#cb62-56" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-57"><a href="#cb62-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-58"><a href="#cb62-58" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb62-59"><a href="#cb62-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the benchmark</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Running batch size impact benchmark..."</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing different batch sizes on 512×512 matrices"</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_143_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>matmul-performance:
   batch_size     Naive   Grouped  Torch
0        16.0  0.171429  0.176471    0.3
1        32.0  0.375000  0.360902    0.3
2        64.0  0.352941  0.352941    0.3</code></pre>
</section>
<section id="benchmark-3" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-3">Benchmark #3</h4>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Matrix Size Impact with Fixed Block Size =====</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X-axis: varying matrix sizes</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>],</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)],  <span class="co"># Matrix sizes from 32 to 2048</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>        x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compare different implementations</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>,</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>        line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>],</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>        line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot styling</span></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>,</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>,</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>{}</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Benchmark matrix multiplication with varying matrix sizes but fixed block size.</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a><span class="co">    This benchmark uses a fixed block size (64) while varying matrix dimensions</span></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a><span class="co">    to understand how different implementations scale with problem size.</span></span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a><span class="co">        square_matrix_size: Size of the square matrices to multiply (N×N)</span></span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a><span class="co">        provider: Which implementation to benchmark</span></span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: (median_performance, min_performance, max_performance) in GB/s</span></span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fixed block size of 64 for all implementations</span></span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>:</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>, bs<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:</span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: torch.matmul(a,b),</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Running matrix size scaling benchmark..."</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing different matrix sizes with fixed block size=64"</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_145_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.066298  0.062500  0.050633
1                64.0  0.129730  0.146341  0.083333
2               128.0  0.214286  0.214286  0.214286
3               256.0  0.272727  0.272727  0.300000
4               512.0  0.352941  0.352941  0.300000
5              1024.0  0.210066  0.206897  0.218182
6              2048.0  0.058680  0.058252  0.077623</code></pre>
<ul>
<li><strong>Performance Trends</strong>:
<ul>
<li><strong>Small Matrices</strong>:
<ul>
<li>Triton kernels can outperform PyTorch.</li>
</ul></li>
<li><strong>Large Matrices</strong>:
<ul>
<li>PyTorch may be faster due to highly optimized kernels.</li>
</ul></li>
</ul></li>
<li><strong>Cache Effects</strong>:
<ul>
<li>Performance drops when exceeding <strong>L1</strong> or <strong>L2 cache</strong> capacity.</li>
</ul></li>
<li><strong>Block Size Impact</strong>:
<ul>
<li>Larger block sizes generally improve performance.</li>
<li>Excessively large block sizes may cause <strong>out-of-memory</strong> errors.</li>
</ul></li>
</ul>
</section>
</section>
<section id="profiling-tools" class="level3">
<h3 class="anchored" data-anchor-id="profiling-tools">Profiling Tools</h3>
<ul>
<li><strong>NVIDIA Nsight Compute (NCU)</strong>:
<ul>
<li>Provides detailed performance metrics.</li>
<li>Helps identify optimization opportunities.</li>
</ul></li>
</ul>
</section>
</section>
<section id="auto-tuning" class="level2">
<h2 class="anchored" data-anchor-id="auto-tuning">Auto-Tuning</h2>
<section id="concept" class="level3">
<h3 class="anchored" data-anchor-id="concept">Concept</h3>
<ul>
<li><strong>Auto-Tuning</strong>:
<ul>
<li>Automatically finds the best kernel configurations for performance.</li>
</ul></li>
<li><strong>Parameters Tuned</strong>:
<ul>
<li><strong>Block sizes</strong>, <strong>tile sizes</strong>, and other kernel parameters.</li>
</ul></li>
</ul>
</section>
<section id="implementation-in-triton" class="level3">
<h3 class="anchored" data-anchor-id="implementation-in-triton">Implementation in Triton</h3>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co">Matrix Multiplication with Autotuning</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="co">This implementation adds automatic performance tuning by testing different configurations</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="co">of block sizes, group sizes, and other parameters to find optimal settings for different</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co">matrix sizes.</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Autotuned Matrix Multiplication Kernel =====</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.autotune</span>(</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    configs<span class="op">=</span>[</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Different configurations to try, varying block sizes and execution parameters</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">256</span>, <span class="st">'bk'</span>: <span class="dv">64</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">3</span>, num_warps<span class="op">=</span><span class="dv">8</span>),  <span class="co"># Larger blocks, fewer stages</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">256</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">128</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">64</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">128</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">32</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">32</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">5</span>, num_warps<span class="op">=</span><span class="dv">2</span>),  <span class="co"># Smaller blocks, more stages</span></span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">32</span>, <span class="st">'bn'</span>: <span class="dv">64</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, </span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>                     num_stages<span class="op">=</span><span class="dv">5</span>, num_warps<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Autotuning is based on input matrix dimensions</span></span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a>    key<span class="op">=</span>[<span class="st">'m'</span>, <span class="st">'n'</span>, <span class="st">'k'</span>],</span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_autotuned_matmul_k(</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,</span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>    m, n, k,</span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, </span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn,</span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn,</span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, </span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a>    group_sz: tl.constexpr</span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb68-43"><a href="#cb68-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb68-44"><a href="#cb68-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Autotuned matrix multiplication kernel that tries different configurations</span></span>
<span id="cb68-45"><a href="#cb68-45" aria-hidden="true" tabindex="-1"></a><span class="co">    to find the best performance for given matrix dimensions.</span></span>
<span id="cb68-46"><a href="#cb68-46" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb68-47"><a href="#cb68-47" aria-hidden="true" tabindex="-1"></a><span class="co">    The configurations vary:</span></span>
<span id="cb68-48"><a href="#cb68-48" aria-hidden="true" tabindex="-1"></a><span class="co">    - Block sizes (bm, bn, bk)</span></span>
<span id="cb68-49"><a href="#cb68-49" aria-hidden="true" tabindex="-1"></a><span class="co">    - Number of pipeline stages</span></span>
<span id="cb68-50"><a href="#cb68-50" aria-hidden="true" tabindex="-1"></a><span class="co">    - Number of warps</span></span>
<span id="cb68-51"><a href="#cb68-51" aria-hidden="true" tabindex="-1"></a><span class="co">    - Group size for memory access optimization</span></span>
<span id="cb68-52"><a href="#cb68-52" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb68-53"><a href="#cb68-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get thread block coordinates and grid dimensions</span></span>
<span id="cb68-54"><a href="#cb68-54" aria-hidden="true" tabindex="-1"></a>    pid_m <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb68-55"><a href="#cb68-55" aria-hidden="true" tabindex="-1"></a>    pid_n <span class="op">=</span> tl.program_id(<span class="dv">1</span>)</span>
<span id="cb68-56"><a href="#cb68-56" aria-hidden="true" tabindex="-1"></a>    num_pid_m <span class="op">=</span> tl.num_programs(<span class="dv">0</span>)</span>
<span id="cb68-57"><a href="#cb68-57" aria-hidden="true" tabindex="-1"></a>    num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb68-58"><a href="#cb68-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-59"><a href="#cb68-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply swizzling for memory access optimization</span></span>
<span id="cb68-60"><a href="#cb68-60" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)</span>
<span id="cb68-61"><a href="#cb68-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-62"><a href="#cb68-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate block offsets</span></span>
<span id="cb68-63"><a href="#cb68-63" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb68-64"><a href="#cb68-64" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb68-65"><a href="#cb68-65" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb68-66"><a href="#cb68-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-67"><a href="#cb68-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate memory offsets</span></span>
<span id="cb68-68"><a href="#cb68-68" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb68-69"><a href="#cb68-69" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb68-70"><a href="#cb68-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-71"><a href="#cb68-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Matrix multiplication computation</span></span>
<span id="cb68-72"><a href="#cb68-72" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb68-73"><a href="#cb68-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb68-74"><a href="#cb68-74" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)</span>
<span id="cb68-75"><a href="#cb68-75" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)</span>
<span id="cb68-76"><a href="#cb68-76" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb68-77"><a href="#cb68-77" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb68-78"><a href="#cb68-78" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb68-79"><a href="#cb68-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-80"><a href="#cb68-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store results</span></span>
<span id="cb68-81"><a href="#cb68-81" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb68-82"><a href="#cb68-82" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb68-83"><a href="#cb68-83" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_autotuned_matmul(a, b):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="co">    High-level wrapper for autotuned matrix multiplication.</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function handles:</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Input validation</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Output initialization</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Grid computation</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a><span class="co">    4. Kernel launch with autotuned parameters</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn <span class="op">=</span> grouped_autotuned_matmul_k</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validate inputs</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> a.shape[<span class="dv">1</span>] <span class="op">==</span> b.shape[<span class="dv">0</span>], <span class="st">"matrix dims not compatible for matmul"</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(a, b)</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get matrix dimensions</span></span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    (m, k), (_, n) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize output matrix</span></span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.empty((m, n), device<span class="op">=</span>a.device, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute grid dimensions</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (triton.cdiv(m, meta[<span class="st">'bm'</span>]), triton.cdiv(n, meta[<span class="st">'bn'</span>]))</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Launch kernel with autotuned parameters</span></span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn[grid](</span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a>        a, b, c,</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a>        m, n, k,</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a>        a.stride(<span class="dv">0</span>), a.stride(<span class="dv">1</span>),</span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a>        b.stride(<span class="dv">0</span>), b.stride(<span class="dv">1</span>),</span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>        c.stride(<span class="dv">0</span>), c.stride(<span class="dv">1</span>),</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block sizes and group size are autotuned</span></span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>a,b <span class="op">=</span> torch.ones(<span class="dv">3</span>,<span class="dv">4</span>, device<span class="op">=</span><span class="st">'cuda'</span>), torch.ones(<span class="dv">4</span>,<span class="dv">5</span>, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>a<span class="op">@</span>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0')</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>grouped_autotuned_matmul(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
<ul>
<li><strong>Define Configurations</strong>:
<ul>
<li>List possible values for parameters.</li>
</ul></li>
<li><strong>Auto-Tuner Decorator</strong>:
<ul>
<li>Use <code>@triton.autotune(configs=..., key=['M', 'N', 'K'])</code>.</li>
</ul></li>
<li><strong>Execution</strong>:
<ul>
<li>Triton tests each configuration to find the optimal one.</li>
<li>The best configuration is cached for reuse.</li>
</ul></li>
</ul>
</section>
<section id="observations" class="level3">
<h3 class="anchored" data-anchor-id="observations">Observations</h3>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== Performance Benchmark with Autotuning =====</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>],</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)],  <span class="co"># 32 to 2048</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>        x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>,</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'grouped-autotuned'</span>, <span class="st">'torch'</span>],</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Grouped &amp; Auto-Tuned'</span>, <span class="st">'Torch'</span>],</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'--'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>,</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>,</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>{}</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Benchmark comparing all implementations including autotuned version.</span></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Compares:</span></span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Naive Triton implementation</span></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Grouped Triton implementation</span></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Grouped &amp; Autotuned Triton implementation</span></span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a><span class="co">    4. PyTorch native implementation</span></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create test matrices</span></span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb74-32"><a href="#cb74-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-33"><a href="#cb74-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark each implementation</span></span>
<span id="cb74-34"><a href="#cb74-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:</span>
<span id="cb74-35"><a href="#cb74-35" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb74-36"><a href="#cb74-36" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb74-37"><a href="#cb74-37" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb74-38"><a href="#cb74-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-39"><a href="#cb74-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>:</span>
<span id="cb74-40"><a href="#cb74-40" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb74-41"><a href="#cb74-41" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>, bs<span class="op">=</span><span class="dv">64</span>),</span>
<span id="cb74-42"><a href="#cb74-42" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb74-43"><a href="#cb74-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-44"><a href="#cb74-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped-autotuned'</span>:</span>
<span id="cb74-45"><a href="#cb74-45" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb74-46"><a href="#cb74-46" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: grouped_autotuned_matmul(a, b),</span>
<span id="cb74-47"><a href="#cb74-47" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb74-48"><a href="#cb74-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-49"><a href="#cb74-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:</span>
<span id="cb74-50"><a href="#cb74-50" aria-hidden="true" tabindex="-1"></a>        ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(</span>
<span id="cb74-51"><a href="#cb74-51" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span>: torch.matmul(a,b),</span>
<span id="cb74-52"><a href="#cb74-52" aria-hidden="true" tabindex="-1"></a>            quantiles<span class="op">=</span>quantiles</span>
<span id="cb74-53"><a href="#cb74-53" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-54"><a href="#cb74-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-55"><a href="#cb74-55" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb74-56"><a href="#cb74-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the benchmark</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Running final performance comparison with autotuning..."</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_164_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>matmul-performance:
   square_matrix_size     Naive   Grouped  Grouped &amp; Auto-Tuned     Torch
0                32.0  0.065934  0.062500              0.079470  0.051724
1                64.0  0.134078  0.149068              0.145455  0.083333
2               128.0  0.214286  0.214286              0.214286  0.215247
3               256.0  0.272727  0.272727              0.300000  0.300000
4               512.0  0.352941  0.352941              0.375000  0.300000
5              1024.0  0.210526  0.207343              0.243500  0.220753
6              2048.0  0.058492  0.057971              0.065362  0.078689</code></pre>
<ul>
<li><strong>Performance Improvements</strong>:
<ul>
<li>Auto-tuning can significantly enhance performance.</li>
</ul></li>
<li><strong>Unexpected Results</strong>:
<ul>
<li>In some cases, auto-tuned kernels may perform worse.</li>
<li>Requires analysis to adjust configurations.</li>
</ul></li>
</ul>
</section>
<section id="tips" class="level3">
<h3 class="anchored" data-anchor-id="tips">Tips</h3>
<ul>
<li><strong>Problem Size Specificity</strong>:
<ul>
<li>Optimal configurations may vary with input sizes.</li>
</ul></li>
<li><strong>Best Practices</strong>:
<ul>
<li>Refer to Triton documentation and community resources.</li>
<li>Experiment with different configurations.</li>
</ul></li>
</ul>
</section>
</section>
<section id="conclusion-and-resources" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-resources">Conclusion and Resources</h2>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li><strong>Triton</strong> provides an accessible way to write efficient GPU kernels.</li>
<li>Offers a balance between <strong>ease of use</strong> and <strong>performance</strong>.</li>
<li><strong>Debugging</strong> and <strong>auto-tuning</strong> tools enhance development.</li>
<li><strong>Benchmarking</strong> is essential for performance validation.</li>
</ul>
</section>
<section id="further-learning" class="level3">
<h3 class="anchored" data-anchor-id="further-learning">Further Learning</h3>
<ul>
<li><strong>Resources</strong>:
<ul>
<li><strong><a href="https://triton-lang.org/main/index.html">Triton Documentation</a></strong>: Comprehensive guide and reference.</li>
<li><strong>Lectures and Talks</strong>:
<ul>
<li><a href="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1: How to profile CUDA kernels in PyTorch</a>
<ul>
<li><a href="../lecture-001/">Notes</a></li>
</ul></li>
<li><a href="https://www.youtube.com/watch?v=09wntC6BT5o">Lecture 9: Reductions</a>
<ul>
<li><a href="../lecture-009/">Notes</a></li>
</ul></li>
</ul></li>
<li><strong>LightLLM Triton Kernels:</strong> <a href="https://github.com/ModelTC/lightllm/tree/main/lightllm/common/basemodel/triton_kernel">lightllm/common/basemodel/triton_kernel</a></li>
<li><strong>unsloth Triton Kernels:</strong> <a href="https://github.com/unslothai/unsloth/tree/main/unsloth/kernels">unsloth/kernels</a></li>
<li><strong>Triton Puzzles:</strong> <a href="https://github.com/srush/Triton-Puzzles">srush/Triton-Puzzles</a></li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, a deep learning consultant specializing in practical AI implementations. I help clients leverage cutting-edge AI technologies to solve real-world problems.</p>
<p>Interested in working together? Fill out my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> form or learn more <a href="../../../about.html">about me</a>.</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>