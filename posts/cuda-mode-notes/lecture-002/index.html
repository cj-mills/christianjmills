<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-06-06">
<meta name="description" content="Lecture #2 provides an introduction to parallel programming with CUDA C, covering key concepts like heterogeneous computing, data parallelism, thread organization, and memory management, and showcasing examples such as vector addition, image blurring, and matrix multiplication.">

<title>Christian Mills - CUDA MODE Lecture 2: Ch.1-3 PMPP Book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - CUDA MODE Lecture 2: Ch.1-3 PMPP Book">
<meta property="og:description" content="Lecture #2 provides an introduction to parallel programming with CUDA C, covering key concepts like heterogeneous computing, data parallelism, thread organization, and memory management, and showcasing examples such as vector addition, image blurring, and matrix multiplication.">
<meta property="og:image" content="https://christianjmills.com/posts/cuda-mode-notes/social-media/cover.jpg">
<meta property="og:site_name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - CUDA MODE Lecture 2: Ch.1-3 PMPP Book">
<meta name="twitter:description" content="Lecture #2 provides an introduction to parallel programming with CUDA C, covering key concepts like heterogeneous computing, data parallelism, thread organization, and memory management, and showcasing examples such as vector addition, image blurring, and matrix multiplication.">
<meta name="twitter:image" content="https://christianjmills.com/posts/cuda-mode-notes/social-media/cover.jpg">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lecture-information" id="toc-lecture-information" class="nav-link active" data-scroll-target="#lecture-information">Lecture Information</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#history" id="toc-history" class="nav-link" data-scroll-target="#history">History</a></li>
  <li><a href="#rise-of-cuda" id="toc-rise-of-cuda" class="nav-link" data-scroll-target="#rise-of-cuda">Rise of CUDA</a></li>
  <li><a href="#amdahls-law" id="toc-amdahls-law" class="nav-link" data-scroll-target="#amdahls-law">Amdahl’s Law</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges">Challenges</a></li>
  <li><a href="#main-goals-of-the-book" id="toc-main-goals-of-the-book" class="nav-link" data-scroll-target="#main-goals-of-the-book">Main Goals of the Book</a></li>
  </ul></li>
  <li><a href="#heterogeneous-data-parallel-computing" id="toc-heterogeneous-data-parallel-computing" class="nav-link" data-scroll-target="#heterogeneous-data-parallel-computing">Heterogeneous Data Parallel Computing</a>
  <ul>
  <li><a href="#cuda-c" id="toc-cuda-c" class="nav-link" data-scroll-target="#cuda-c">CUDA C</a></li>
  <li><a href="#cuda-essentials-memory-allocation" id="toc-cuda-essentials-memory-allocation" class="nav-link" data-scroll-target="#cuda-essentials-memory-allocation">CUDA Essentials: Memory Allocation</a></li>
  <li><a href="#cuda-error-handling" id="toc-cuda-error-handling" class="nav-link" data-scroll-target="#cuda-error-handling">CUDA Error Handling</a></li>
  <li><a href="#kernel-functions-fn" id="toc-kernel-functions-fn" class="nav-link" data-scroll-target="#kernel-functions-fn">Kernel functions <code>fn&lt;&lt;&gt;&gt;</code></a></li>
  <li><a href="#kernel-coordinates" id="toc-kernel-coordinates" class="nav-link" data-scroll-target="#kernel-coordinates">Kernel Coordinates</a></li>
  <li><a href="#cuda-c-keywords-for-function-declaration" id="toc-cuda-c-keywords-for-function-declaration" class="nav-link" data-scroll-target="#cuda-c-keywords-for-function-declaration">CUDA C keywords for function declaration</a></li>
  <li><a href="#calling-kernels" id="toc-calling-kernels" class="nav-link" data-scroll-target="#calling-kernels">Calling Kernels</a></li>
  <li><a href="#compiler" id="toc-compiler" class="nav-link" data-scroll-target="#compiler">Compiler</a></li>
  <li><a href="#code-example-vector-addition" id="toc-code-example-vector-addition" class="nav-link" data-scroll-target="#code-example-vector-addition">Code Example: Vector addition</a></li>
  <li><a href="#code-example-kernel-to-convert-an-rgb-image-to-grayscale" id="toc-code-example-kernel-to-convert-an-rgb-image-to-grayscale" class="nav-link" data-scroll-target="#code-example-kernel-to-convert-an-rgb-image-to-grayscale">Code Example: Kernel to convert an RGB image to grayscale</a></li>
  </ul></li>
  <li><a href="#multidimensional-grids-and-data" id="toc-multidimensional-grids-and-data" class="nav-link" data-scroll-target="#multidimensional-grids-and-data">Multidimensional Grids and Data</a>
  <ul>
  <li><a href="#cuda-grid" id="toc-cuda-grid" class="nav-link" data-scroll-target="#cuda-grid">CUDA Grid</a></li>
  <li><a href="#built-in-variables" id="toc-built-in-variables" class="nav-link" data-scroll-target="#built-in-variables">Built-in Variables</a></li>
  <li><a href="#nd-arrays-in-memory" id="toc-nd-arrays-in-memory" class="nav-link" data-scroll-target="#nd-arrays-in-memory">nd-Arrays in Memory</a></li>
  <li><a href="#code-example-image-blur" id="toc-code-example-image-blur" class="nav-link" data-scroll-target="#code-example-image-blur">Code Example: Image Blur</a></li>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication">Matrix Multiplication</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">CUDA MODE Lecture 2: Ch.1-3 PMPP Book</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">cuda</div>
    <div class="quarto-category">pytorch</div>
  </div>
  </div>

<div>
  <div class="description">
    Lecture #2 provides an introduction to parallel programming with CUDA C, covering key concepts like heterogeneous computing, data parallelism, thread organization, and memory management, and showcasing examples such as vector addition, image blurring, and matrix multiplication.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 6, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>CUDA Mode Lecture Notes</strong></a>: My notes from the <strong>CUDA MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.</li>
</ul>
</div>
</div>
<ul>
<li><a href="#lecture-information">Lecture Information</a></li>
<li><a href="#introduction">Ch.1: Introduction</a></li>
<li><a href="#heterogeneous-data-parallel-computing">Ch.2: Heterogeneous Data Parallel Computing</a></li>
<li><a href="#multidimensional-grids-and-data">Ch.3: Multidimensional Grids and Data</a></li>
</ul>
<section id="lecture-information" class="level2">
<h2 class="anchored" data-anchor-id="lecture-information">Lecture Information</h2>
<p><strong>Speaker:</strong> Andreas Kopf</p>
<p><strong>Topic:</strong> PMPP Book Ch. 1-3</p>
<p><strong>Resources:</strong></p>
<ul>
<li><strong>Lecture Slides:</strong> <a href="https://docs.google.com/presentation/d/1deqvEHdqEC4LHUpStO6z3TT77Dt84fNAvTIAxBJgDck/edit#slide=id.g2b1444253e5_1_75">CUDA Mode: Lecture 2</a></li>
<li><strong>Textbook:</strong> <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processors</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/cuda-mode/lectures/tree/main/lecture_002">CUDA MODE Lecture 2</a></li>
<li><strong>Discord Channel:</strong> <a href="https://discord.gg/cudamode">CUDA MODE</a></li>
<li><strong>YouTube Channel:</strong> <a href="https://www.youtube.com/@CUDAMODE">CUDA MODE</a></li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=59R0B3U5I8SLKY8K&amp;t=60">1:00</a></li>
</ul>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<ul>
<li><p>Optimize GPU performance as much as possible</p></li>
<li><p>Applications:</p>
<ul>
<li>simulate and model worlds
<ul>
<li>games</li>
<li>weather</li>
<li>proteins</li>
<li>robotics</li>
</ul></li>
</ul></li>
<li><p>Bigger models are smarter</p>
<ul>
<li>speed and size improvements can have a significant impact on useability</li>
</ul></li>
<li><p>GPUs are the backbon of modern deep learning</p></li>
</ul>
</section>
<section id="history" class="level3">
<h3 class="anchored" data-anchor-id="history">History</h3>
<ul>
<li>Classic software uses sequential programs
<ul>
<li>executed one step at a time</li>
<li>relied on higher CPU clock rates for improved performance</li>
</ul></li>
<li>Higher clock rate trend for CPUs slowed in 2003 due to energy consumption and heat dissipation challenges
<ul>
<li>Increasing frequency would make the chip to hot to cool feasibly</li>
</ul></li>
<li>Multi-core CPU came up
<ul>
<li>Developers had to learn multi-threading
<ul>
<li>New challenges such as deadlocks and race conditions</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="rise-of-cuda" class="level3">
<h3 class="anchored" data-anchor-id="rise-of-cuda">Rise of CUDA</h3>
<ul>
<li>Compute Unified Device Architecture</li>
<li>CUDA is all about parallel programs
<ul>
<li>divide work among threads</li>
</ul></li>
<li>GPUs have much higher peak FLOPS than multi-core CPUs
<ul>
<li>Benefits highly parallelized programs</li>
<li>Not suitable for largely sequential programs</li>
</ul></li>
<li>CPU+GPU
<ul>
<li>Run sequential parts on CPU and numerically intensive parts on GPU</li>
</ul></li>
<li>GPGPU
<ul>
<li>Before CUDA tricks were used to compute with graphics APIs like OpenGL and Direct3D</li>
</ul></li>
<li>GPU programming is now attractive to developers due to massive availability</li>
</ul>
</section>
<section id="amdahls-law" class="level3">
<h3 class="anchored" data-anchor-id="amdahls-law">Amdahl’s Law</h3>
<ul>
<li><p><span class="math display">\[
speedup = ( Slow \ System \ Time )/(Fast \ System \ Time)
\]</span></p></li>
<li><p>achievable speedup is limited by the parallelizable portion of <span class="math inline">\(p\)</span></p></li>
<li><p><span class="math display">\[
speedup &lt; \frac{1}{1-p}
\]</span></p>
<ul>
<li>If <span class="math inline">\(p\)</span> is <span class="math inline">\(90\%\)</span>, <span class="math inline">\(speedup &lt; 10X\)</span></li>
</ul></li>
<li><p><span class="math inline">\(p &gt; 99\%\)</span> for many real applications</p>
<ul>
<li>especially for large datasets</li>
<li>speedups <span class="math inline">\(&gt; 100X\)</span> are attainable</li>
</ul></li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li>“If you do not care about performance, parallel programming is very easy”</li>
<li>In practice, designing parallel algorithms is harder than sequential algorithms
<ul>
<li>Parallelizing recurrent computations requires nonintuitive thinking
<ul>
<li>prefix sum
<ul>
<li><a href="https://en.wikipedia.org/wiki/Prefix_sum">Wikipedia Page</a></li>
</ul></li>
</ul></li>
</ul></li>
<li>Speed is often limited by memory latency/throughput (memory bound)
<ul>
<li>Often need to read something to the GPU, perform some computation, and the write back the result
<ul>
<li>LLM inference generates token by token</li>
</ul></li>
</ul></li>
<li>Input data characteristics can significantly influence performance of parallel programs
<ul>
<li>LLMs short or large sequences</li>
<li>Might need different kernels optimized for different input shapes</li>
</ul></li>
<li>Not all applications are “embarrassingly parallel”
<ul>
<li>Synchronization imposes overhead
<ul>
<li>Need to wait for GPU operations to complete</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="main-goals-of-the-book" class="level3">
<h3 class="anchored" data-anchor-id="main-goals-of-the-book">Main Goals of the Book</h3>
<ol type="1">
<li>Parallel programming &amp; computational thinking
<ul>
<li>Aims to build a foundation for parallel programming in general</li>
<li>Uses GPUs as a learning vehicle
<ul>
<li>Techniques apply to other accelerators</li>
<li>Concepts are introduced through hands-on CUDA examples</li>
</ul></li>
</ul></li>
<li>Correct &amp; reliable parallel programing
<ul>
<li>Debugging both functions and performance</li>
<li>Understanding where things are fast and slow and how to improve the slow parts</li>
</ul></li>
<li>Scalability
<ul>
<li>Regularize and localize memory access</li>
<li>How to organize memory</li>
</ul></li>
</ol>
</section>
</section>
<section id="heterogeneous-data-parallel-computing" class="level2">
<h2 class="anchored" data-anchor-id="heterogeneous-data-parallel-computing">Heterogeneous Data Parallel Computing</h2>
<ul>
<li><p>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=ZeFGj3WVYDF_TI96&amp;t=511">8:31</a></p></li>
<li><p>heterogeneous: CPU + GPU</p></li>
<li><p>data parallelism: break work down into computations that can be executed independently</p></li>
</ul>
<section id="cuda-c" class="level3">
<h3 class="anchored" data-anchor-id="cuda-c">CUDA C</h3>
<ul>
<li>extends ANSI C with minimal new syntax</li>
<li>Terminology
<ul>
<li>CPU=host</li>
<li>GPU=device</li>
<li>Kernels: device code functions</li>
</ul></li>
<li>CUDA C source can be a mixture of host &amp; device code</li>
<li>grid of threads
<ul>
<li>Many threads are launched to execute a kernel</li>
</ul></li>
<li>CPU &amp; GPU code runs concurrently (overlapped)
<ul>
<li>Kernels launch and run on GPU asynchronously</li>
<li>Need to wait for the kernels to finish before copying data back to CPU</li>
</ul></li>
<li>Don’t be afraid to launch many threads on GPU
<ul>
<li>One thread per output tensor is fine</li>
</ul></li>
</ul>
</section>
<section id="cuda-essentials-memory-allocation" class="level3">
<h3 class="anchored" data-anchor-id="cuda-essentials-memory-allocation">CUDA Essentials: Memory Allocation</h3>
<ul>
<li><p>NVIDIA devices come with their own DRAM (device) global memory</p></li>
<li><p><code>cudaMalloc</code> &amp; <code>cudaFree</code>:</p>
<ul>
<li><p><code>cudaMalloc</code>: Allocate device global memory</p></li>
<li><p><code>cudaFree</code>: Free device global memory</p></li>
<li><div class="sourceCode" id="cb1"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>  <span class="dt">float</span> <span class="op">*</span>A_d<span class="op">;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">size_t</span> size <span class="op">=</span> n <span class="op">*</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">);</span> <span class="co">// size in bytes</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  cudaMalloc<span class="op">((</span><span class="dt">void</span><span class="op">**)&amp;</span>A_d<span class="op">,</span> size<span class="op">);</span> <span class="co">// pointer to pointer</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  cudaFree<span class="op">(</span>A_d<span class="op">);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Code convention</p>
<ul>
<li><code>_d</code> for device pointer</li>
<li><code>_h</code> for host</li>
</ul></li>
<li><p><code>cudaMemcpy</code></p>
<ul>
<li><p>Copy data from CPU memory to GPU memory and vice versa</p></li>
<li><div class="sourceCode" id="cb2"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>  <span class="co">// copy input vectors to device (host -&gt; device)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  cudaMemcpy<span class="op">(</span>A_d<span class="op">,</span> A_h<span class="op">,</span> size<span class="op">,</span> cudaMemcpyHostToDevice<span class="op">);</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  cudaMemcpy<span class="op">(</span>B_d<span class="op">,</span> B_h<span class="op">,</span> size<span class="op">,</span> cudaMemcpyHostToDevice<span class="op">);</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">// transfer result back to CPU memory (device -&gt; host)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  cudaMemcpy<span class="op">(</span>C_h<span class="op">,</span> C_d<span class="op">,</span> size<span class="op">,</span> cudaMemcpyDeviceToHost<span class="op">);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="cuda-error-handling" class="level3">
<h3 class="anchored" data-anchor-id="cuda-error-handling">CUDA Error Handling</h3>
<ul>
<li>CUDA functions return <code>cudaError_t</code>
<ul>
<li><code>cudaSuccess</code> for successful operation</li>
</ul></li>
<li>Always check returned error status</li>
</ul>
</section>
<section id="kernel-functions-fn" class="level3">
<h3 class="anchored" data-anchor-id="kernel-functions-fn">Kernel functions <code>fn&lt;&lt;&gt;&gt;</code></h3>
<ul>
<li>Launching kernel
<ul>
<li>grid of threads is launched</li>
</ul></li>
<li>All threads execute the same code
<ul>
<li>SPMD: Single Program Multiple Data</li>
</ul></li>
<li>Threads are hierarchically organized into grid blocks &amp; thread blocks
<ul>
<li>Up to 1024 threads in a thread block</li>
</ul></li>
</ul>
</section>
<section id="kernel-coordinates" class="level3">
<h3 class="anchored" data-anchor-id="kernel-coordinates">Kernel Coordinates</h3>
<ul>
<li><p>Built-in variables available inside the kernel</p>
<ul>
<li><code>blockIdx</code>: the area code for a telephone
<ul>
<li>Note: Blocks are a logical organization of threads, not physical</li>
</ul></li>
<li><code>threadIdx</code>: the local phone number</li>
<li>These are ‘coordinates’ that allow threads to identify which portion of the data to process</li>
<li>Can use <code>blockIdx</code> and <code>threadIdx</code> to uniquely identify threads</li>
<li><code>blockDim</code>: tells us the number of threads in a block</li>
</ul></li>
<li><p>For vector addition, we can calculate the array index of the thread</p>
<ul>
<li><div class="sourceCode" id="cb3"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> i <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p>All threads in a grid execute the same kernel code</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/book-figure-2-9.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 2.9</figcaption>
</figure>
</div>
</section>
<section id="cuda-c-keywords-for-function-declaration" class="level3">
<h3 class="anchored" data-anchor-id="cuda-c-keywords-for-function-declaration">CUDA C keywords for function declaration</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Qualifier Keyword</th>
<th>Callable From</th>
<th>Executed On</th>
<th>Executed By</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>__host__</code> (default)</td>
<td>Host</td>
<td>Host</td>
<td>Caller host thread</td>
</tr>
<tr class="even">
<td><code>__global__</code></td>
<td>Host (or Device)</td>
<td>Device</td>
<td>New grid of device threads</td>
</tr>
<tr class="odd">
<td><code>__device__</code></td>
<td>Device</td>
<td>Device</td>
<td>Caller device thread</td>
</tr>
</tbody>
</table>
<ul>
<li><p><code>__global__</code> &amp; <code>__host__</code></p>
<ul>
<li>Tell the compiler whether the function should live on the device or host</li>
<li>Declare a kernel function with <code>__global__</code>
<ul>
<li>Calling a <code>__global__</code> function launches new grid of CUDA threads</li>
</ul></li>
</ul></li>
<li><p>Functions declared with <code>__device__</code> can be called from within CUDA thread</p>
<ul>
<li>Does not launch a new thread</li>
<li>Only accessible from within kernels</li>
</ul></li>
<li><p>If both <code>__host__</code> and <code>__device__</code> are used in a function declaration</p>
<ul>
<li>CPU and GPU versions will be compiled</li>
</ul></li>
</ul>
</section>
<section id="calling-kernels" class="level3">
<h3 class="anchored" data-anchor-id="calling-kernels">Calling Kernels</h3>
<ul>
<li><p>Kernel configuration is specified between <code>&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;</code></p></li>
<li><p>Number of blocks, number of threads in each block</p></li>
<li><div class="sourceCode" id="cb4"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Define the number of threads per block.</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Each block will have 256 threads.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>dim3 numThreads<span class="op">(</span><span class="dv">256</span><span class="op">);</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Calculate the number of blocks needed to cover the entire vector.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Use ceiling division to ensure that the number of blocks is sufficient</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">// to handle all elements of the vector 'n'.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">// The formula (n + numThreads.x - 1) / numThreads.x ensures this.</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>dim3 numBlocks<span class="op">((</span>n <span class="op">+</span> numThreads<span class="op">.</span>x <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> numThreads<span class="op">.</span>x<span class="op">);</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">// Launch the vector addition kernel with the calculated number of blocks and threads.</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">// This will execute the vecAddKernel function on the GPU with 'numBlocks' blocks,</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">// each containing 'numThreads.x' threads.</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>vecAddKernel<span class="op">&lt;&lt;&lt;</span>numBlocks<span class="op">,</span> numThreads<span class="op">&gt;&gt;&gt;(</span>A_d<span class="op">,</span> B_d<span class="op">,</span> C_d<span class="op">,</span> n<span class="op">);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="compiler" class="level3">
<h3 class="anchored" data-anchor-id="compiler">Compiler</h3>
<ul>
<li>nvcc
<ul>
<li>NVIDIA C Compiler</li>
<li>Use to compiler kernels into PTX (CUDA assembly)</li>
</ul></li>
<li>PTX
<ul>
<li>Parallel Thread Execution</li>
<li>Low-level VM &amp; instruction set</li>
</ul></li>
<li>Grahics driver translates PTX into executable binary code (SASS)
<ul>
<li>SASS is the low-level assembly language that compiles to binary microcode, which executes natively on NVIDIA GPU hardware.</li>
</ul></li>
</ul>
</section>
<section id="code-example-vector-addition" class="level3">
<h3 class="anchored" data-anchor-id="code-example-vector-addition">Code Example: Vector addition</h3>
<ul>
<li><p>main concept: replace loop with a grid of threads</p></li>
<li><p>easily parallelizable</p>
<ul>
<li>all additions can be computed independently</li>
</ul></li>
<li><p>Naive GPU vector addition</p>
<ol type="1">
<li>Allocate device memory for vectors</li>
<li>Transfer inputs from host to device</li>
<li>Launch kernel and perform addition operations</li>
<li>Copy outputs from device to host</li>
<li>Free device memory</li>
</ol>
<ul>
<li>The ratio of data transfer vs compute is not very good
<ul>
<li>Normally keep data on the GPU as long as possible to asynchronously schedule many kernel launches</li>
</ul></li>
</ul></li>
<li><p>Figure from slide 13:</p>
<ul>
<li>One thread per vector element</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/slide-13-figure.png" class="img-fluid figure-img"></p>
<figcaption>slide-13-figure</figcaption>
</figure>
</div></li>
<li><p>Data sizes might not be perfectly divisible by block sizes</p>
<ul>
<li>always check bounds</li>
</ul></li>
<li><p>Prevent threads of boundary block to read/write outside allocated memory</p></li>
<li><div class="sourceCode" id="cb5"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">/**</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@brief</span><span class="co"> CUDA kernel to compute the element-wise sum of two vectors.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"> *</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"> * This kernel function performs the pair-wise addition of elements from</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"> * vectors A and B, and stores the result in vector C.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"> *</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">A</span><span class="co"> Pointer to the first input vector (array) in device memory.</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">B</span><span class="co"> Pointer to the second input vector (array) in device memory.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">C</span><span class="co"> Pointer to the output vector (array) in device memory.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">n</span><span class="co"> The number of elements in the vectors.</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"> */</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>__global__</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> vecAddKernel<span class="op">(</span><span class="dt">float</span><span class="op">*</span> A<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> C<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the unique index for the thread</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> i <span class="op">=</span> threadIdx<span class="op">.</span>x <span class="op">+</span> blockDim<span class="op">.</span>x <span class="op">*</span> blockIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Check if the index is within the bounds of the arrays</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Perform the element-wise addition</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        C<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="code-example-kernel-to-convert-an-rgb-image-to-grayscale" class="level3">
<h3 class="anchored" data-anchor-id="code-example-kernel-to-convert-an-rgb-image-to-grayscale">Code Example: Kernel to convert an RGB image to grayscale</h3>
<ul>
<li><p>Each RGB pixel can be converted individually</p></li>
<li><p><span class="math display">\[
Luminance = r\cdot{0.21} + g\cdot{0.72} + b\cdot{0.07}
\]</span></p></li>
<li><p>Simple weighted sum</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/book-figure-2-2.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 2.2</figcaption>
</figure>
</div>
</section>
</section>
<section id="multidimensional-grids-and-data" class="level2">
<h2 class="anchored" data-anchor-id="multidimensional-grids-and-data">Multidimensional Grids and Data</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=k2a0vvryolFT8AsZ&amp;t=1495">24:55</a></li>
</ul>
<section id="cuda-grid" class="level3">
<h3 class="anchored" data-anchor-id="cuda-grid">CUDA Grid</h3>
<ul>
<li><p>2-level hierarchy</p>
<ul>
<li>Blocks and threads</li>
</ul></li>
<li><p>Idea: Map threads to multi-dimensional data (e.g., an image)</p></li>
<li><p>All threads in a grid execute the same kernel</p></li>
<li><p>Threads in the same block can access the same shared memory</p></li>
<li><p>Max block size: 1024</p></li>
<li><p>Built-in 3D coordinates of a thread</p>
<ul>
<li><code>blockIdx</code> and <code>threadIdx</code> identify which portion of the data to process</li>
</ul></li>
<li><p>shape of grid &amp; blocks</p>
<ul>
<li><code>gridDim</code>: number of blocks in the grid</li>
<li><code>blockDim</code>: number of threads in a block</li>
</ul></li>
<li><p>A multidimensional example of CUDA grid organization:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/book-figure-3-1.png" class="img-fluid figure-img" style="width:67.0%"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.1</figcaption>
</figure>
</div>
<ul>
<li><p>Grid can be different for each kernel launch</p>
<ul>
<li>Normally dependent on data shapes</li>
</ul></li>
<li><p>Typical grids contain thousands to millions of threads</p></li>
<li><p>Simple Strategy</p>
<ul>
<li>One thread per output element
<ul>
<li>One thread per pixel</li>
<li>One thread per tensor element</li>
</ul></li>
</ul></li>
<li><p>Threads can be scheduled in any order</p>
<ul>
<li>A larger thread index does not necessarily indicate the thread is running after a thread with a lower index</li>
</ul></li>
<li><p>Can use fewer than 3 dims (set others to 1)</p>
<ul>
<li><p>1D for sequences, 2D for images, etc.</p></li>
<li><div class="sourceCode" id="cb6"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>  dim3 grid<span class="op">(</span><span class="dv">32</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  dim3 block<span class="op">(</span><span class="dv">128</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  kernelFunction<span class="op">&lt;&lt;&lt;</span>grid<span class="op">,</span> block<span class="op">&gt;&gt;&gt;(..);</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Number of threads: 128*32 = 4096</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul>
</section>
<section id="built-in-variables" class="level3">
<h3 class="anchored" data-anchor-id="built-in-variables">Built-in Variables</h3>
<ul>
<li><p>Built-in variables inside kernels:</p>
<ul>
<li><div class="sourceCode" id="cb7"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>  blockIdx <span class="co">// dim3 block coordinate</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  threadIdx <span class="co">// dim3 thread coordinate</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  blockDim <span class="co">// number of threads in a block</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  gridDim <span class="co">// number of blocks in a grid</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><code>blockDim</code> and <code>gridDim</code> have the same values in all threads</li>
</ul></li>
</ul>
</section>
<section id="nd-arrays-in-memory" class="level3">
<h3 class="anchored" data-anchor-id="nd-arrays-in-memory">nd-Arrays in Memory</h3>
<ul>
<li>memory of multi-dim arrays under the hood is a flat 1-dimensional array</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/slide-28-actual-layout.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28: Actual layout in memory</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/slide-28-logical-layout.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28: Logical view of data</figcaption>
</figure>
</div>
<ul>
<li><p>2d array can be linearized in different ways</p>
<ul>
<li><pre class="text"><code>  A B C D E F G H I</code></pre></li>
<li><p>row-major</p>
<ul>
<li><pre class="text"><code>  A B C
  D E F
  G H I</code></pre></li>
<li>Most common</li>
</ul></li>
<li><p>column-major</p>
<ul>
<li><pre class="text"><code>  A D G
  B E H
  C F I</code></pre></li>
<li>Used in fortran</li>
</ul></li>
</ul></li>
<li><p>PyTorch tensors and numpy arrays use strides to specify how elements are laid out in memory</p>
<ul>
<li>For a <span class="math inline">\(4 \times 4\)</span> matrix, the stride would be <span class="math inline">\(4\)</span> to get to the next row.
<ul>
<li>After four elements, you end up in the next row.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="code-example-image-blur" class="level3">
<h3 class="anchored" data-anchor-id="code-example-image-blur">Code Example: Image Blur</h3>
<ul>
<li><p>mean filter example <code>blurKernel</code>:</p>
<ul>
<li><div class="sourceCode" id="cb11"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">// CUDA kernel to perform a simple box blur on an input image</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>__global__</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> blurKernel<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span> <span class="op">*</span>in<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">char</span> <span class="op">*</span>out<span class="op">,</span> <span class="dt">int</span> w<span class="op">,</span> <span class="dt">int</span> h<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the column and row index of the pixel this thread is processing</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> col <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> row <span class="op">=</span> blockIdx<span class="op">.</span>y <span class="op">*</span> blockDim<span class="op">.</span>y <span class="op">+</span> threadIdx<span class="op">.</span>y<span class="op">;</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the thread is within the image bounds</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>col <span class="op">&lt;</span> w <span class="op">&amp;&amp;</span> row <span class="op">&lt;</span> h<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> pixVal <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// Variable to accumulate the sum of pixel values</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> pixels <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// Variable to count the number of valid pixels in the blur region</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Loop over the surrounding pixels within the blur region</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> blurRow <span class="op">=</span> <span class="op">-</span>BLUR_SIZE<span class="op">;</span> blurRow <span class="op">&lt;=</span> BLUR_SIZE<span class="op">;</span> <span class="op">++</span>blurRow<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> blurCol <span class="op">=</span> <span class="op">-</span>BLUR_SIZE<span class="op">;</span> blurCol <span class="op">&lt;=</span> BLUR_SIZE<span class="op">;</span> <span class="op">++</span>blurCol<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> curRow <span class="op">=</span> row <span class="op">+</span> blurRow<span class="op">;</span> <span class="co">// Current row index in the blur region</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> curCol <span class="op">=</span> col <span class="op">+</span> blurCol<span class="op">;</span> <span class="co">// Current column index in the blur region</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>                <span class="co">// Check if the current pixel is within the image bounds</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="op">(</span>curRow <span class="op">&gt;=</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> curRow <span class="op">&lt;</span> h <span class="op">&amp;&amp;</span> curCol <span class="op">&gt;=</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> curCol <span class="op">&lt;</span> w<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>                    pixVal <span class="op">+=</span> in<span class="op">[</span>curRow <span class="op">*</span> w <span class="op">+</span> curCol<span class="op">];</span> <span class="co">// Accumulate the pixel value</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>                    <span class="op">++</span>pixels<span class="op">;</span> <span class="co">// Increment the count of valid pixels</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>                <span class="op">}</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Calculate the average pixel value and store it in the output image</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>row <span class="op">*</span> w <span class="op">+</span> col<span class="op">]</span> <span class="op">=</span> <span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">)(</span>pixVal <span class="op">/</span> pixels<span class="op">);</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p>each thread writes one output element, read multiple values</p></li>
<li><p>single plane in book, can be easily extended to multi-channel</p></li>
<li><p>shows row-major pixel memory access (in &amp; out pointers)</p></li>
<li><p>track of how many pixel values are summed</p></li>
<li><p>Handling boundary conditions for pixels near the edges of the image:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/book-figure-3-9.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.9</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.cpp_extension <span class="im">import</span> load_inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled" title="CUDA Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
CUDA Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/types.h&gt;</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda.h&gt;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda_runtime.h&gt;</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;c10/cuda/CUDAException.h&gt;</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;c10/cuda/CUDAStream.h&gt;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">// CUDA kernel for applying a mean filter to an image</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>__global__</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> mean_filter_kernel<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> output<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> width<span class="op">,</span> <span class="dt">int</span> height<span class="op">,</span> <span class="dt">int</span> radius<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the column, row, and channel this thread is responsible for</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> col <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> row <span class="op">=</span> blockIdx<span class="op">.</span>y <span class="op">*</span> blockDim<span class="op">.</span>y <span class="op">+</span> threadIdx<span class="op">.</span>y<span class="op">;</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> channel <span class="op">=</span> threadIdx<span class="op">.</span>z<span class="op">;</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Base offset for the current channel</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> baseOffset <span class="op">=</span> channel <span class="op">*</span> height <span class="op">*</span> width<span class="op">;</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the thread is within image bounds</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>col <span class="op">&lt;</span> width <span class="op">&amp;&amp;</span> row <span class="op">&lt;</span> height<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> pixVal <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// Accumulator for the pixel values</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> pixels <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// Counter for the number of pixels summed</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Iterate over the kernel window</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> blurRow <span class="op">=</span> <span class="op">-</span>radius<span class="op">;</span> blurRow <span class="op">&lt;=</span> radius<span class="op">;</span> blurRow <span class="op">+=</span> <span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> blurCol <span class="op">=</span> <span class="op">-</span>radius<span class="op">;</span> blurCol <span class="op">&lt;=</span> radius<span class="op">;</span> blurCol <span class="op">+=</span> <span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> curRow <span class="op">=</span> row <span class="op">+</span> blurRow<span class="op">;</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>                <span class="dt">int</span> curCol <span class="op">=</span> col <span class="op">+</span> blurCol<span class="op">;</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>                <span class="co">// Check if the current position is within image bounds</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="op">(</span>curRow <span class="op">&gt;=</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> curRow <span class="op">&lt;</span> height <span class="op">&amp;&amp;</span> curCol <span class="op">&gt;=</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> curCol <span class="op">&lt;</span> width<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>                    <span class="co">// Accumulate pixel value and count the number of pixels</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                    pixVal <span class="op">+=</span> input<span class="op">[</span>baseOffset <span class="op">+</span> curRow <span class="op">*</span> width <span class="op">+</span> curCol<span class="op">];</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                    pixels <span class="op">+=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>                <span class="op">}</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Write the averaged value to the output image</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        output<span class="op">[</span>baseOffset <span class="op">+</span> row <span class="op">*</span> width <span class="op">+</span> col<span class="op">]</span> <span class="op">=</span> <span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">)(</span>pixVal <span class="op">/</span> pixels<span class="op">);</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="co">// Helper function for ceiling unsigned integer division</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="kw">inline</span> <span class="dt">unsigned</span> <span class="dt">int</span> cdiv<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">int</span> a<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">int</span> b<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>a <span class="op">+</span> b <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> b<span class="op">;</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="co">// Main function to apply the mean filter to an image using CUDA</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>torch<span class="op">::</span>Tensor mean_filter<span class="op">(</span>torch<span class="op">::</span>Tensor image<span class="op">,</span> <span class="dt">int</span> radius<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the input image is on the GPU, is of byte type, and radius is positive</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    <span class="ot">assert</span><span class="op">(</span>image<span class="op">.</span>device<span class="op">().</span>type<span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kCUDA<span class="op">);</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>    <span class="ot">assert</span><span class="op">(</span>image<span class="op">.</span>dtype<span class="op">()</span> <span class="op">==</span> torch<span class="op">::</span>kByte<span class="op">);</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    <span class="ot">assert</span><span class="op">(</span>radius <span class="op">&gt;</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Get image dimensions and number of channels</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> channels <span class="op">=</span> image<span class="op">.</span>size<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> height <span class="op">=</span> image<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> width <span class="op">=</span> image<span class="op">.</span>size<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create an empty tensor to store the result</span></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> result <span class="op">=</span> torch<span class="op">::</span>empty_like<span class="op">(</span>image<span class="op">);</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Define the number of threads per block and number of blocks</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    dim3 threads_per_block<span class="op">(</span><span class="dv">16</span><span class="op">,</span> <span class="dv">16</span><span class="op">,</span> channels<span class="op">);</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>    dim3 number_of_blocks<span class="op">(</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>        cdiv<span class="op">(</span>width<span class="op">,</span> threads_per_block<span class="op">.</span>x<span class="op">),</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>        cdiv<span class="op">(</span>height<span class="op">,</span> threads_per_block<span class="op">.</span>y<span class="op">)</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>    <span class="op">);</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Launch the CUDA kernel</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>    mean_filter_kernel<span class="op">&lt;&lt;&lt;</span>number_of_blocks<span class="op">,</span> threads_per_block<span class="op">,</span> <span class="dv">0</span><span class="op">,</span> torch<span class="op">::</span>cuda<span class="op">::</span>getCurrentCUDAStream<span class="op">()&gt;&gt;&gt;(</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>        result<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>        image<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>        width<span class="op">,</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>        height<span class="op">,</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>        radius</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>    <span class="op">);</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Check for any CUDA errors (calls cudaGetLastError())</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>    C10_CUDA_KERNEL_LAUNCH_CHECK<span class="op">();</span></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Return the filtered image</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="op">;</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the CUDA kernel and C++ wrapper</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cuda_source <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="st">#include &lt;c10/cuda/CUDAException.h&gt;</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="st">#include &lt;c10/cuda/CUDAStream.h&gt;</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="st">// CUDA kernel for applying a mean filter to an image</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="st">__global__</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="st">void mean_filter_kernel(unsigned char* output, unsigned char* input, int width, int height, int radius) {</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="st">    // Calculate the column, row, and channel this thread is responsible for</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="st">    int col = blockIdx.x * blockDim.x + threadIdx.x;</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="st">    int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="st">    int channel = threadIdx.z;</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="st">    // Base offset for the current channel</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="st">    int baseOffset = channel * height * width;</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="st">    // Ensure the thread is within image bounds</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="st">    if (col &lt; width &amp;&amp; row &lt; height) {</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="st">        int pixVal = 0; // Accumulator for the pixel values</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="st">        int pixels = 0; // Counter for the number of pixels summed</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="st">        // Iterate over the kernel window</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="st">        for (int blurRow = -radius; blurRow &lt;= radius; blurRow += 1) {</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="st">            for (int blurCol = -radius; blurCol &lt;= radius; blurCol += 1) {</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="st">                int curRow = row + blurRow;</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="st">                int curCol = col + blurCol;</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="st">                // Check if the current position is within image bounds</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="st">                if (curRow &gt;= 0 &amp;&amp; curRow &lt; height &amp;&amp; curCol &gt;= 0 &amp;&amp; curCol &lt; width) {</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="st">                    // Accumulate pixel value and count the number of pixels</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="st">                    pixVal += input[baseOffset + curRow * width + curCol];</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="st">                    pixels += 1;</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="st">                }</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="st">            }</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="st">        }</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="st">        // Write the averaged value to the output image</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="st">        output[baseOffset + row * width + col] = (unsigned char)(pixVal / pixels);</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="st">// Helper function for ceiling unsigned integer division</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="st">inline unsigned int cdiv(unsigned int a, unsigned int b) {</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="st">    return (a + b - 1) / b;</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a><span class="st">// Main function to apply the mean filter to an image using CUDA</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="st">torch::Tensor mean_filter(torch::Tensor image, int radius) {</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="st">    // Ensure the input image is on the GPU, is of byte type, and radius is positive</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a><span class="st">    assert(image.device().type() == torch::kCUDA);</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="st">    assert(image.dtype() == torch::kByte);</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="st">    assert(radius &gt; 0);</span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="st">    // Get image dimensions and number of channels</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a><span class="st">    const auto channels = image.size(0);</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="st">    const auto height = image.size(1);</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="st">    const auto width = image.size(2);</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a><span class="st">    // Create an empty tensor to store the result</span></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="st">    auto result = torch::empty_like(image);</span></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="st">    // Define the number of threads per block and number of blocks</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a><span class="st">    dim3 threads_per_block(16, 16, channels);</span></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="st">    dim3 number_of_blocks(</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a><span class="st">        cdiv(width, threads_per_block.x),</span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a><span class="st">        cdiv(height, threads_per_block.y)</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="st">    );</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a><span class="st">    // Launch the CUDA kernel</span></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a><span class="st">    mean_filter_kernel&lt;&lt;&lt;number_of_blocks, threads_per_block, 0, torch::cuda::getCurrentCUDAStream()&gt;&gt;&gt;(</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="st">        result.data_ptr&lt;unsigned char&gt;(),</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="st">        image.data_ptr&lt;unsigned char&gt;(),</span></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="st">        width,</span></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="st">        height,</span></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="st">        radius</span></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="st">    );</span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a><span class="st">    // Check for any CUDA errors (calls cudaGetLastError())</span></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a><span class="st">    C10_CUDA_KERNEL_LAUNCH_CHECK();</span></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="st">    // Return the filtered image</span></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="st">    return result;</span></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>cpp_source <span class="op">=</span> <span class="st">"torch::Tensor mean_filter(torch::Tensor image, int radius);"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>build_dir <span class="op">=</span> Path(<span class="st">'./load_inline_cuda'</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>build_dir.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the defined C++/CUDA extension as a PyTorch extension.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This enables using the `mean_filter` function as if it were a native PyTorch function.</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>mean_filter_extension <span class="op">=</span> load_inline(</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'mean_filter_extension'</span>,   <span class="co"># Unique name for the extension</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    cpp_sources<span class="op">=</span>cpp_source,           <span class="co"># C++ source code containing the CPU implementation</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    cuda_sources<span class="op">=</span>cuda_source,         <span class="co"># CUDA source code for GPU implementation</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    functions<span class="op">=</span>[<span class="st">'mean_filter'</span>],      <span class="co"># List of functions to expose to Python</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    with_cuda<span class="op">=</span><span class="va">True</span>,                   <span class="co"># Enable CUDA support</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    extra_cuda_cflags<span class="op">=</span>[<span class="st">"-O2"</span>],        <span class="co"># Compiler flags for optimizing the CUDA code</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    build_directory<span class="op">=</span><span class="bu">str</span>(build_dir),   <span class="co"># Directory to store the compiled extension</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the path to the image file</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> Path(<span class="st">'./Grace_Hopper.jpg'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the image using PIL (Python Imaging Library)</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>test_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_6_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to a NumPy array, then to a PyTorch tensor</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Rearrange the tensor dimensions from (H, W, C) to (C, H, W) and move it to GPU</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(np.array(test_img)).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).contiguous().cuda()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the mean filter to the tensor using a kernel size of 8</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mean_filter_extension.mean_filter(x, <span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the filtered tensor back to a NumPy array, rearrange dimensions back to (H, W, C)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and create an image from the array using PIL</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>output_img <span class="op">=</span> Image.fromarray(y.cpu().permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy())</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>output_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_8_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication">Matrix Multiplication</h3>
<ul>
<li><p>Staple of science, engineering, and deep learning</p></li>
<li><p>Computer inner-products of rows and columns</p></li>
<li><p>Strategy: 1 thread per output matrix element</p></li>
<li><p>Example: Multiplying square matrices (rows == cols)</p>
<ul>
<li><div class="sourceCode" id="cb21"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">/**</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@brief</span><span class="co"> Matrix multiplication kernel function.</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"> *</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"> * This kernel performs the multiplication of two matrices M and N, storing the result in matrix P.</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"> *</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">M</span><span class="co"> Pointer to the first input matrix.</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">N</span><span class="co"> Pointer to the second input matrix.</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">P</span><span class="co"> Pointer to the output matrix.</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param</span><span class="co"> </span><span class="cv">Width</span><span class="co"> The width of the input and output matrices (assuming square matrices).</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"> */</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>__global__ <span class="dt">void</span> MatrixMulKernel<span class="op">(</span><span class="dt">float</span><span class="op">*</span> M<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> N<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> P<span class="op">,</span> <span class="dt">int</span> Width<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the row index of the P matrix element and M matrix element</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> row <span class="op">=</span> blockIdx<span class="op">.</span>y <span class="op">*</span> blockDim<span class="op">.</span>y <span class="op">+</span> threadIdx<span class="op">.</span>y<span class="op">;</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the column index of the P matrix element and N matrix element</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> col <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure that row and column indices are within bounds</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">((</span>row <span class="op">&lt;</span> Width<span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="op">(</span>col <span class="op">&lt;</span> Width<span class="op">))</span> <span class="op">{</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> Pvalue <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// Initialize the output value for element P[row][col]</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Perform the dot product of the row of M and column of N</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> Width<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            Pvalue <span class="op">+=</span> M<span class="op">[</span>row <span class="op">*</span> Width <span class="op">+</span> k<span class="op">]</span> <span class="op">*</span> N<span class="op">[</span>k <span class="op">*</span> Width <span class="op">+</span> col<span class="op">];</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Store the result in the P matrix</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>        P<span class="op">[</span>row <span class="op">*</span> Width <span class="op">+</span> col<span class="op">]</span> <span class="op">=</span> Pvalue<span class="op">;</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p>Matrix multiplication using multiple blocks by tiling P:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/book-figure-3-10.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.10</figcaption>
</figure>
</div>


</section>
</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>