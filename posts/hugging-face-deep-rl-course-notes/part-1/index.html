<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-05-05">
<meta name="description" content="Unit 1 introduces the basic concepts for reinforcement learning and covers how to train an agent for the classic lunar lander environment.">

<title>Christian Mills - Notes on The Hugging Face Deep RL Class Pt.1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on The Hugging Face Deep RL Class Pt.1">
<meta property="og:description" content="Unit 1 introduces the basic concepts for reinforcement learning and covers how to train an agent for the classic lunar lander environment.">
<meta property="og:image" content="christianjmills.com/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta property="og:image:height" content="295">
<meta property="og:image:width" content="300">
<meta name="twitter:title" content="Christian Mills - Notes on The Hugging Face Deep RL Class Pt.1">
<meta name="twitter:description" content="Unit 1 introduces the basic concepts for reinforcement learning and covers how to train an agent for the classic lunar lander environment.">
<meta name="twitter:image" content="christianjmills.com/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="295">
<meta name="twitter:image-width" content="300">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on The Hugging Face Deep RL Class Pt.1</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">reinforcement-learning</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Unit 1 introduces the basic concepts for reinforcement learning and covers how to train an agent for the classic lunar lander environment.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 5, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<ul>
<li><a href="#what-is-reinforcement-learning">What is Reinforcement Learning?</a></li>
<li><a href="#the-reinforcement-learning-framework">The Reinforcement Learning Framework</a></li>
<li><a href="#exploration-exploitation-tradeoff">Exploration-Exploitation Tradeoff</a></li>
<li><a href="#the-policy">The Policy</a></li>
<li><a href="#deep-reinforcement-learning">Deep Reinforcement Learning</a></li>
<li><a href="#lab">Lab</a></li>
<li><a href="#references">References</a></li>
</ul>
<section id="what-is-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2>
<ul>
<li>Reinforcement learning (RL) is a framework for solving control tasks where agents learn from the environment by interacting with it through trial and error and receiving rewards as unique feedback.</li>
</ul>
</section>
<section id="the-reinforcement-learning-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-reinforcement-learning-framework">The Reinforcement Learning Framework</h2>
<section id="the-rl-process" class="level3">
<h3 class="anchored" data-anchor-id="the-rl-process">The RL Process</h3>
<ul>
<li>The RL process is a loop that outputs a sequence of state <span class="math inline">\(S_{0}\)</span>, action <span class="math inline">\(A_{0}\)</span>, reward <span class="math inline">\(R_{1}\)</span>, and next state <span class="math inline">\(S_{1}\)</span>.</li>
</ul>
</section>
<section id="the-reward-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="the-reward-hypothesis">The Reward Hypothesis</h3>
<ul>
<li>The reward and next state result from taking the current action in the current state.</li>
<li>The goal is to maximize the expected cumulative reward, called the expected return.</li>
</ul>
</section>
<section id="markov-property" class="level3">
<h3 class="anchored" data-anchor-id="markov-property">Markov Property</h3>
<ul>
<li>The Markov property implies that agents only need the current state to decide what action to take and not the history of all the states and actions.</li>
</ul>
</section>
<section id="observationstates-space" class="level3">
<h3 class="anchored" data-anchor-id="observationstates-space">Observation/States Space</h3>
<ul>
<li>Observations/States are the information agents get from the environment.</li>
<li>The state is a complete description of the agent’s environment (e.g., a chessboard).</li>
<li>An observation is a partial description of the state (e.g., the current frame of a video game).</li>
</ul>
</section>
<section id="action-space" class="level3">
<h3 class="anchored" data-anchor-id="action-space">Action Space</h3>
<ul>
<li>The action space is the set of all possible actions in an environment.</li>
<li>Actions can be discrete (e.g., up, down, left, right) or continuous (e.g., steering angle).</li>
<li>Different RL algorithms are suited for discrete and continuous actions.</li>
</ul>
</section>
<section id="rewards-and-discounting" class="level3">
<h3 class="anchored" data-anchor-id="rewards-and-discounting">Rewards and discounting</h3>
<ul>
<li>The reward is the only feedback the agent receives for its actions.</li>
<li>Rewards that happen earlier in a session (e.g., at the beginning of the game) are more probable since they are more predictable than the long-term reward.</li>
<li>We can discount longer-term reward values that are less predictable.</li>
<li>We define a discount rate called gamma with a value between 0 and 1. The discount rate is typically 0.99 or 0.95.</li>
<li>The larger the gamma, the smaller the discount, meaning agents care more about long-term rewards.</li>
<li>We discount each reward by gamma to the exponent of the time step, so they are less predictable the further into the future.</li>
<li>We can write the cumulative reward at each time step <span class="math inline">\(t\)</span> as:</li>
</ul>
</section>
<section id="rtau-r_t1-r_t2-r_t3-r_t4-ldots" class="level3">
<h3 class="anchored" data-anchor-id="rtau-r_t1-r_t2-r_t3-r_t4-ldots"><span class="math display">\[R(\tau) = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + \ldots\]</span></h3>
</section>
<section id="rtau-suminfty_k0r_t-k-1" class="level3">
<h3 class="anchored" data-anchor-id="rtau-suminfty_k0r_t-k-1"><span class="math display">\[R(\tau) = \sum^{\infty}_{k=0}{r_{t} + k + 1}\]</span></h3>
<ul>
<li>Discounted cumulative expected reward:</li>
</ul>
</section>
<section id="rtau-r_t1-gamma-r_t2-gamma2r_t3-gamma3r_t4-ldots" class="level3">
<h3 class="anchored" data-anchor-id="rtau-r_t1-gamma-r_t2-gamma2r_t3-gamma3r_t4-ldots"><span class="math display">\[R(\tau) = r_{t+1} + \gamma r_{t+2} + \gamma^{2}r_{t+3} + \gamma^{3}r_{t+4} + \ldots\]</span></h3>
</section>
<section id="rtau-suminfty_k0gammak-r_t-k-1" class="level3">
<h3 class="anchored" data-anchor-id="rtau-suminfty_k0gammak-r_t-k-1"><span class="math display">\[R(\tau) = \sum^{\infty}_{k=0}{\gamma^k{} r_{t} + k + 1}\]</span></h3>
</section>
<section id="type-of-tasks" class="level3">
<h3 class="anchored" data-anchor-id="type-of-tasks">Type of tasks</h3>
<ul>
<li>A task is an instance of a Reinforcement Learning problem and is either episodic or continuous.</li>
</ul>
<section id="episodic-tasks" class="level4">
<h4 class="anchored" data-anchor-id="episodic-tasks">Episodic Tasks</h4>
<ul>
<li>Episodic tasks have starting points and ending points.</li>
<li>We can represent episodes as a list of states, actions, rewards, and new states.</li>
</ul>
</section>
<section id="continuous-tasks" class="level4">
<h4 class="anchored" data-anchor-id="continuous-tasks">Continuous Tasks</h4>
<ul>
<li>Continuous tasks have no terminal state, and the agent must learn to choose the best actions and simultaneously interact with the environment.</li>
</ul>
</section>
</section>
</section>
<section id="exploration-exploitation-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="exploration-exploitation-tradeoff">Exploration-Exploitation Tradeoff</h2>
<ul>
<li>We must balance gaining more information about the environment and exploiting known information to maximize reward (e.g., going with the usual restaurant or trying a new one).</li>
</ul>
</section>
<section id="the-policy" class="level2">
<h2 class="anchored" data-anchor-id="the-policy">The Policy</h2>
<ul>
<li><p>The policy is the function that tells the agent what action to take given the current state.</p></li>
<li><p>The goal is to find the optimal policy <span class="math inline">\(\pi\)</span> which maximizes the expected return.</p></li>
<li><p><span class="math inline">\(a = \pi(s)\)</span></p></li>
<li><p><span class="math inline">\(\pi\left( a \vert s \right) = P \left[ A \vert s \right]\)</span></p></li>
<li><p><span class="math inline">\(\text{policy} \left( \text{actions} \ \vert \ \text{state} \right) = \text{probability distribution over the set of actions given the current state}\)</span></p></li>
</ul>
<section id="policy-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="policy-based-methods">Policy-based Methods</h3>
<ul>
<li>Policy-based methods involve learning a policy function directly by teaching the agent which action to take in a given state.</li>
<li>A deterministic policy will always return the same action in a given state.</li>
<li>A stochastic policy outputs a probability distribution over actions.</li>
</ul>
</section>
<section id="value-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="value-based-methods">Value-based methods</h3>
<ul>
<li>Value-based methods teach the agent to learn which future state is more valuable.</li>
<li>Value-based methods involve training a value function that maps a state to the expected value of being in that state.</li>
<li>The value of a state is the expected discounted return the agent can get if it starts in that state and then acts according to the policy.</li>
</ul>
</section>
</section>
<section id="deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>
<ul>
<li>Deep reinforcement learning introduces deep neural networks to solve RL problems.</li>
</ul>
</section>
<section id="lab" class="level2">
<h2 class="anchored" data-anchor-id="lab">Lab</h2>
<ul>
<li><strong>Objective:</strong> Train a lander agent to land correctly, share it to the community, and experiment with different configurations.</li>
<li><a href="https://github.com/huggingface/deep-rl-class">Syllabus</a></li>
<li><a href="https://discord.gg/aYka4Yhff9">Discord server</a></li>
<li><a href="https://discord.gg/aYka4Yhff9">#study-group-unit1 discord channel</a></li>
<li>Environment: <a href="https://www.gymlibrary.ml/environments/box2d/lunar_lander/">LunarLander-v2</a></li>
<li>RL-Library: <a href="https://stable-baselines3.readthedocs.io/en/master/">Stable-Baselines3</a></li>
</ul>
<section id="prerequisites" class="level3">
<h3 class="anchored" data-anchor-id="prerequisites">Prerequisites</h3>
<ul>
<li><a href="https://github.com/huggingface/deep-rl-class/blob/main/unit1/README.md">Unit 1 README</a></li>
<li><a href="https://huggingface.co/blog/deep-rl-intro">An Introduction to Deep Reinforcement Learning</a></li>
</ul>
</section>
<section id="objectives" class="level3">
<h3 class="anchored" data-anchor-id="objectives">Objectives</h3>
<ul>
<li>Be able to use <strong>Gym</strong>, the environment library.</li>
<li>Be able to use <strong>Stable-Baselines3</strong>, the deep reinforcement learning library.</li>
<li>Be able to <strong>push your trained agent to the Hub</strong> with a nice video replay and an evaluation score.</li>
</ul>
</section>
<section id="set-the-gpu-google-colab" class="level3">
<h3 class="anchored" data-anchor-id="set-the-gpu-google-colab">Set the GPU (Google Colab)</h3>
<ul>
<li><code>Runtime &gt; Change Runtime type</code></li>
<li><code>Hardware Accelerator &gt; GPU</code></li>
</ul>
</section>
<section id="install-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="install-dependencies">Install dependencies</h3>
<p><strong>Install virtual screen libraries for rendering the environment</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>apt install python<span class="op">-</span>opengl</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>apt install ffmpeg</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>apt install xvfb</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install pyvirtualdisplay</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create and run a virual screen</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Virtual display</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyvirtualdisplay <span class="im">import</span> Display</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>virtual_display <span class="op">=</span> Display(visible<span class="op">=</span><span class="dv">0</span>, size<span class="op">=</span>(<span class="dv">1400</span>, <span class="dv">900</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>virtual_display.start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    &lt;pyvirtualdisplay.display.Display at 0x7f2df34855d0&gt;</code></pre>
<hr>
<section id="gymbox2d" class="level4">
<h4 class="anchored" data-anchor-id="gymbox2d">Gym[box2d]</h4>
<ul>
<li>Gym is a toolkit that contains test environments for developing and comparing reinforcement learning algorithms.</li>
<li><a href="https://www.gymlibrary.ml/environments/box2d/">Box2D</a> environments all involve toy games based around physics control, using <a href="https://box2d.org/">box2d</a>-based physics and PyGame-based rendering.</li>
<li><a href="https://github.com/openai/gym">GitHub Repository</a></li>
<li><a href="https://www.gymlibrary.ml/">Gym Documentation</a></li>
</ul>
</section>
<section id="stable-baselines" class="level4">
<h4 class="anchored" data-anchor-id="stable-baselines">Stable Baselines</h4>
<ul>
<li>The Stable Baselines3 library is a set of reliable implementations of reinforcement learning algorithms in PyTorch.</li>
<li><a href="https://github.com/DLR-RM/stable-baselines3">GitHub Repository</a></li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/index.html">Documentation</a></li>
</ul>
</section>
<section id="hugging-face-x-stable-baselines" class="level4">
<h4 class="anchored" data-anchor-id="hugging-face-x-stable-baselines">Hugging Face x Stable-baselines</h4>
<ul>
<li>Load and upload Stable-baseline3 models from the Hugging Face Hub.</li>
<li><a href="https://github.com/huggingface/huggingface_sb3">GitHub Repository</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install gym[box2d]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install stable<span class="op">-</span>baselines3[extra]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install huggingface_sb3</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install ale<span class="op">-</span>py<span class="op">==</span><span class="fl">0.7.4</span> <span class="co"># To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="import-the-packages" class="level3">
<h3 class="anchored" data-anchor-id="import-the-packages">Import the packages</h3>
<p>The Hugging Face Hub Hugging Face works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easilly collaborate with others.</p>
<p><a href="https://huggingface.co/models?pipeline_tag=reinforcement-learning&amp;sort=downloads">Hugging Face Hub Deep reinforcement Learning models</a></p>
<section id="load_from_hub" class="level4">
<h4 class="anchored" data-anchor-id="load_from_hub"><code>load_from_hub</code></h4>
<ul>
<li>Download a model from Hugging Face Hub.</li>
<li><a href="https://github.com/huggingface/huggingface_sb3/blob/23837ad2617c4288e1df71551ac2ef7f3eeee9d5/huggingface_sb3/load_from_hub.py#L6">Source Code</a></li>
</ul>
</section>
<section id="package_to_hub" class="level4">
<h4 class="anchored" data-anchor-id="package_to_hub"><code>package_to_hub</code></h4>
<ul>
<li>Evaluate a model, generate a demo video, and upload the model to Hugging Face Hub.</li>
<li><a href="https://github.com/huggingface/huggingface_sb3/blob/23837ad2617c4288e1df71551ac2ef7f3eeee9d5/huggingface_sb3/push_to_hub.py#L241">Source Code</a></li>
</ul>
</section>
<section id="push_to_hub" class="level4">
<h4 class="anchored" data-anchor-id="push_to_hub"><code>push_to_hub</code></h4>
<ul>
<li>Upload a model to Hugging Face Hub.</li>
<li><a href="https://github.com/huggingface/huggingface_sb3/blob/23837ad2617c4288e1df71551ac2ef7f3eeee9d5/huggingface_sb3/push_to_hub.py#L350">Source Code</a></li>
</ul>
</section>
<section id="notebook_login" class="level4">
<h4 class="anchored" data-anchor-id="notebook_login"><code>notebook_login</code></h4>
<ul>
<li>Display a widget to login to the HF website and store the token.</li>
<li><a href="https://github.com/huggingface/huggingface_hub/blob/7042df38a9839ac41efd60cf7f985d473c49d426/src/huggingface_hub/commands/user.py#L312">Source Code</a></li>
</ul>
</section>
<section id="ppo" class="level4">
<h4 class="anchored" data-anchor-id="ppo"><code>PPO</code></h4>
<ul>
<li>The <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization</a> algorithm</li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">Documentation</a></li>
</ul>
</section>
<section id="evaluate_policy" class="level4">
<h4 class="anchored" data-anchor-id="evaluate_policy"><code>evaluate_policy</code></h4>
<ul>
<li>Run a policy and return the average reward.</li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html#stable_baselines3.common.evaluation.evaluate_policy">Documentation</a></li>
</ul>
</section>
<section id="make_vec_env" class="level4">
<h4 class="anchored" data-anchor-id="make_vec_env"><code>make_vec_env</code></h4>
<ul>
<li>Create a wrapped, monitored vectorized environment (<a href="https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html?highlight=VecEnv#vecenv">VecEnv</a>).</li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/common/env_util.html#stable_baselines3.common.env_util.make_vec_env">Documentation</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_sb3 <span class="im">import</span> load_from_hub, package_to_hub, push_to_hub</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3 <span class="im">import</span> PPO</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.evaluation <span class="im">import</span> evaluate_policy</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.env_util <span class="im">import</span> make_vec_env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="understand-the-gym-api" class="level3">
<h3 class="anchored" data-anchor-id="understand-the-gym-api">Understand the Gym API</h3>
<ol type="1">
<li>Create our environment using <code>gym.make()</code></li>
<li>Reset the environment to its initial state with <code>observation = env.reset()</code></li>
<li>Get an action using our model</li>
<li>Perform the action using <code>env.step(action)</code>, which returns:</li>
</ol>
<ul>
<li><code>obsevation</code>: The new state (st+1)</li>
<li><code>reward</code>: The reward we get after executing the action</li>
<li><code>done</code>: Indicates if the episode terminated</li>
<li><code>info</code>: A dictionary that provides additional environment-specific information.</li>
</ul>
<ol start="5" type="1">
<li>Reset the environment to its initial state with <code>observation = env.reset()</code> at the end of each episode</li>
</ol>
</section>
<section id="create-the-lunarlander-environment-and-understand-how-it-works" class="level3">
<h3 class="anchored" data-anchor-id="create-the-lunarlander-environment-and-understand-how-it-works">Create the LunarLander environment and understand how it works</h3>
<section id="lunar-lander-environment" class="level4">
<h4 class="anchored" data-anchor-id="lunar-lander-environment">Lunar Lander Environment</h4>
<ul>
<li>This environment is a classic rocket trajectory optimization problem.</li>
<li>The agent needs to learn <strong>to adapt its speed and position(horizontal, vertical, and angular) to land correctly.</strong></li>
<li><a href="https://www.gymlibrary.ml/environments/box2d/lunar_lander/">Documentation</a></li>
</ul>
<table class="table">
<tbody>
<tr class="odd">
<td>Action Space</td>
<td>Discrete(4)</td>
</tr>
<tr class="even">
<td>Observation Space</td>
<td>(8,)</td>
</tr>
<tr class="odd">
<td>Observation High</td>
<td><code>[inf inf inf inf inf inf inf inf]</code></td>
</tr>
<tr class="even">
<td>Observation Low</td>
<td><code>[-inf -inf -inf -inf -inf -inf -inf -inf]</code></td>
</tr>
<tr class="odd">
<td>Import</td>
<td><code>gym.make("LunarLander-v2")</code></td>
</tr>
</tbody>
</table>
<p><strong>Create a <a href="https://www.gymlibrary.ml/environments/box2d/lunar_lander/">Lunar Lander</a> environment</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"LunarLander-v2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Reset the environment</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>observation <span class="op">=</span> env.reset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Take some random actions in the environment</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Take a random action</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  action <span class="op">=</span> env.action_space.sample()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"Action taken:"</span>, action)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Do this action in the environment and get</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># next_state, reward, done and info</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  observation, reward, done, info <span class="op">=</span> env.step(action)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If the game is done (in our case we land, crashed or timeout)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> done:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Reset the environment</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">"Environment is reset"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>      observation <span class="op">=</span> env.reset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Action taken: 0
    Action taken: 1
    Action taken: 0
    Action taken: 3
    Action taken: 0
    Action taken: 3
    Action taken: 1
    Action taken: 1
    Action taken: 0
    Action taken: 1
    Action taken: 0
    Action taken: 1
    Action taken: 0
    Action taken: 2
    Action taken: 1
    Action taken: 2
    Action taken: 3
    Action taken: 3
    Action taken: 3
    Action taken: 3</code></pre>
<hr>
<p><strong>Inspect the observation space</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We create a new environment</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"LunarLander-v2"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the environment</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>env.reset()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"_____OBSERVATION SPACE_____ </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Observation Space Shape"</span>, env.observation_space.shape)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample observation"</span>, env.observation_space.sample()) <span class="co"># Get a random observation</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    _____OBSERVATION SPACE_____ 
    
    Observation Space Shape (8,)
    Sample observation [ 1.9953048  -0.9302978   0.26271465 -1.406391    0.42527643 -0.07207114
      2.1984298   0.4171027 ]</code></pre>
<p><strong>Note:</strong> * The observation is a vector of size 8, where each value is a different piece of information about the lander. 1. Horizontal pad coordinate (x) 2. Vertical pad coordinate (y) 3. Horizontal speed (x) 4. Vertical speed (y) 5. Angle 6. Angular speed 7. If the left leg has contact point touched the land 8. If the right leg has contact point touched the land</p>
<hr>
<p><strong>Inspect the action space</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> _____ACTION SPACE_____ </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Action Space Shape"</span>, env.action_space.n)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Action Space Sample"</span>, env.action_space.sample()) <span class="co"># Take a random action</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>
     _____ACTION SPACE_____ 
    
    Action Space Shape 4
    Action Space Sample 1</code></pre>
<p><strong>Note:</strong> * The action space is discrete, with four available actions. 1. Do nothing. 2. Fire left orientation engine. 3. Fire the main engine. 4. Fire right orientation engine.</p>
<ul>
<li>Reward function details:
<ul>
<li>Moving from the top of the screen to the landing pad and zero speed is about 100~140 points.</li>
<li>Firing main engine is -0.3 each frame</li>
<li>Each leg ground contact is +10 points</li>
<li>Episode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)</li>
<li>The game is solved if your agent does 200 points.</li>
</ul></li>
</ul>
<hr>
<section id="vectorized-environment" class="level5">
<h5 class="anchored" data-anchor-id="vectorized-environment">Vectorized Environment</h5>
<ul>
<li>We can stack multiple independent environments into a single vector to get more diverse experiences during the training.</li>
</ul>
<p><strong>Stack 16 independent environments</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> make_vec_env(<span class="st">'LunarLander-v2'</span>, n_envs<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="create-the-model" class="level3">
<h3 class="anchored" data-anchor-id="create-the-model">Create the Model</h3>
<ul>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">PPO (aka Proximal Policy Optimization)</a> is a combination of:
<ul>
<li><em>Value-based reinforcement learning method</em>: learning an action-value function that will tell us what’s the <strong>most valuable action to take given a state and action</strong>.</li>
<li><em>Policy-based reinforcement learning method</em>: learning a policy that will <strong>gives us a probability distribution over actions</strong>.</li>
</ul></li>
</ul>
<section id="stable-baselines3-setup-steps" class="level5">
<h5 class="anchored" data-anchor-id="stable-baselines3-setup-steps">Stable-Baselines3 setup steps:</h5>
<ol type="1">
<li>You <strong>create your environment</strong> (in our case it was done above)</li>
<li>You define the <strong>model you want to use and instantiate this model</strong> <code>model = PPO("MlpPolicy")</code></li>
<li>You <strong>train the agent</strong> with <code>model.learn</code> and define the number of training timesteps</li>
</ol>
<p><strong>Sample Code:</strong></p>
<pre><code># Create environment
env = gym.make('LunarLander-v2')

# Instantiate the agent
model = PPO('MlpPolicy', env, verbose=1)
# Train the agent
model.learn(total_timesteps=int(2e5))</code></pre>
<hr>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Inspect default PPO arguments</strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>args <span class="op">=</span> inspect.getfullargspec(PPO).args</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>defaults <span class="op">=</span> inspect.getfullargspec(PPO).defaults</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>defaults <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span>(<span class="bu">len</span>(args)<span class="op">-</span><span class="bu">len</span>(defaults)) <span class="op">+</span> <span class="bu">list</span>(defaults)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>annotations <span class="op">=</span> inspect.getfullargspec(PPO).annotations.values()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>annotations <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span>(<span class="bu">len</span>(args)<span class="op">-</span><span class="bu">len</span>(annotations)) <span class="op">+</span> <span class="bu">list</span>(annotations)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>ppo_default_args <span class="op">=</span> {arg:[default, annotation] <span class="cf">for</span> arg,default,annotation <span class="kw">in</span> <span class="bu">zip</span>(args, defaults, annotations)}</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(ppo_default_args, index<span class="op">=</span>[<span class="st">"Default Value"</span>, <span class="st">"Annotation"</span>]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
Default Value
</th>
<th>
Annotation
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
self
</th>
<td>
None
</td>
<td>
None
</td>
</tr>
<tr>
<th>
policy
</th>
<td>
None
</td>
<td>
typing.Union[str, typing.Type[stable_baselines3.common.policies.ActorCriticPolicy]]
</td>
</tr>
<tr>
<th>
env
</th>
<td>
None
</td>
<td>
typing.Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str]
</td>
</tr>
<tr>
<th>
learning_rate
</th>
<td>
0.0003
</td>
<td>
typing.Union[float, typing.Callable[[float], float]]
</td>
</tr>
<tr>
<th>
n_steps
</th>
<td>
2048
</td>
<td>
&lt;class ‘int’&gt;
</td>
</tr>
<tr>
<th>
batch_size
</th>
<td>
64
</td>
<td>
&lt;class ‘int’&gt;
</td>
</tr>
<tr>
<th>
n_epochs
</th>
<td>
10
</td>
<td>
&lt;class ‘int’&gt;
</td>
</tr>
<tr>
<th>
gamma
</th>
<td>
0.99
</td>
<td>
&lt;class ‘float’&gt;
</td>
</tr>
<tr>
<th>
gae_lambda
</th>
<td>
0.95
</td>
<td>
&lt;class ‘float’&gt;
</td>
</tr>
<tr>
<th>
clip_range
</th>
<td>
0.2
</td>
<td>
typing.Union[float, typing.Callable[[float], float]]
</td>
</tr>
<tr>
<th>
clip_range_vf
</th>
<td>
None
</td>
<td>
typing.Union[NoneType, float, typing.Callable[[float], float]]
</td>
</tr>
<tr>
<th>
normalize_advantage
</th>
<td>
True
</td>
<td>
&lt;class ‘bool’&gt;
</td>
</tr>
<tr>
<th>
ent_coef
</th>
<td>
0.0
</td>
<td>
&lt;class ‘float’&gt;
</td>
</tr>
<tr>
<th>
vf_coef
</th>
<td>
0.5
</td>
<td>
&lt;class ‘float’&gt;
</td>
</tr>
<tr>
<th>
max_grad_norm
</th>
<td>
0.5
</td>
<td>
&lt;class ‘float’&gt;
</td>
</tr>
<tr>
<th>
use_sde
</th>
<td>
False
</td>
<td>
&lt;class ‘bool’&gt;
</td>
</tr>
<tr>
<th>
sde_sample_freq
</th>
<td>
-1
</td>
<td>
&lt;class ‘int’&gt;
</td>
</tr>
<tr>
<th>
target_kl
</th>
<td>
None
</td>
<td>
typing.Optional[float]
</td>
</tr>
<tr>
<th>
tensorboard_log
</th>
<td>
None
</td>
<td>
typing.Optional[str]
</td>
</tr>
<tr>
<th>
create_eval_env
</th>
<td>
False
</td>
<td>
&lt;class ‘bool’&gt;
</td>
</tr>
<tr>
<th>
policy_kwargs
</th>
<td>
None
</td>
<td>
typing.Optional[typing.Dict[str, typing.Any]]
</td>
</tr>
<tr>
<th>
verbose
</th>
<td>
0
</td>
<td>
&lt;class ‘int’&gt;
</td>
</tr>
<tr>
<th>
seed
</th>
<td>
None
</td>
<td>
typing.Optional[int]
</td>
</tr>
<tr>
<th>
device
</th>
<td>
auto
</td>
<td>
typing.Union[torch.device, str]
</td>
</tr>
<tr>
<th>
_init_setup_model
</th>
<td>
True
</td>
<td>
&lt;class ‘bool’&gt;
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Define a PPO MlpPolicy architecture</strong></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PPO(<span class="st">"MlpPolicy"</span>, env, verbose<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Using cuda device</code></pre>
<p><strong>Note:</strong> * We use a Multilayer Perceptron because the observations are vectors instead of images.</p>
<ul>
<li>Recommended Values:</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>Argument</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>n_steps</td>
<td>1024</td>
</tr>
<tr class="even">
<td>batch_size</td>
<td>64</td>
</tr>
<tr class="odd">
<td>n_epochs</td>
<td>4</td>
</tr>
<tr class="even">
<td>gamma</td>
<td>0.999</td>
</tr>
<tr class="odd">
<td>gae_lambda</td>
<td>0.98</td>
</tr>
<tr class="even">
<td>ent_coef</td>
<td>0.01</td>
</tr>
<tr class="odd">
<td>verbose</td>
<td>1</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="train-the-ppo-agent" class="level3">
<h3 class="anchored" data-anchor-id="train-the-ppo-agent">Train the PPO agent</h3>
<p><strong>Train the model</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model.learn(total_timesteps<span class="op">=</span><span class="bu">int</span>(<span class="dv">2000000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ---------------------------------
    | rollout/           |          |
    |    ep_len_mean     | 94.8     |
    |    ep_rew_mean     | -199     |
    | time/              |          |
    |    fps             | 2891     |
    |    iterations      | 1        |
    |    time_elapsed    | 11       |
    |    total_timesteps | 32768    |
    ---------------------------------
...
    ------------------------------------------
    | rollout/                |              |
    |    ep_len_mean          | 187          |
    |    ep_rew_mean          | 281          |
    | time/                   |              |
    |    fps                  | 593          |
    |    iterations           | 62           |
    |    time_elapsed         | 3421         |
    |    total_timesteps      | 2031616      |
    | train/                  |              |
    |    approx_kl            | 0.0047587324 |
    |    clip_fraction        | 0.0585       |
    |    clip_range           | 0.2          |
    |    entropy_loss         | -0.469       |
    |    explained_variance   | 0.986        |
    |    learning_rate        | 0.0003       |
    |    loss                 | 3.62         |
    |    n_updates            | 610          |
    |    policy_gradient_loss | -0.0007      |
    |    value_loss           | 11.5         |
    ------------------------------------------

    &lt;stable_baselines3.ppo.ppo.PPO at 0x7fcc807b8410&gt;</code></pre>
<hr>
</section>
<section id="evaluate-the-agent" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-the-agent">Evaluate the agent</h3>
<ul>
<li>We can evaluate the model’s performance using the <a href="https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html#stable_baselines3.common.evaluation.evaluate_policy"><code>evaluate_policy()</code></a> method.</li>
<li><a href="https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading">Example</a></li>
</ul>
<p><strong>Create a new environment for evaluation</strong></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>eval_env <span class="op">=</span> gym.make(<span class="st">'LunarLander-v2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Evaluate the model with 10 evaluation episodes and deterministic=True</strong></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>mean_reward, std_reward <span class="op">=</span> evaluate_policy(model, eval_env, n_eval_episodes<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Print the results</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"mean_reward=</span><span class="sc">{</span>mean_reward<span class="sc">:.2f}</span><span class="ss"> +/- </span><span class="sc">{</span>std_reward<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    mean_reward=78.15 +/- 94.84891574522395</code></pre>
<hr>
</section>
<section id="publish-our-trained-model-on-the-hub" class="level3">
<h3 class="anchored" data-anchor-id="publish-our-trained-model-on-the-hub">Publish our trained model on the Hub</h3>
<ul>
<li>We can use the <code>package_to_hub()</code> method to evaluate the model, record a replay, generate a model card, and push the model to the Hub in a single line of code.</li>
<li><a href="https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard"><strong>Leaderboard</strong></a></li>
<li>The <code>package_to_hub()</code> method returns a link to a Hub model repository such as https://huggingface.co/osanseviero/test_sb3.</li>
<li>Model repository features:
<ul>
<li>A video preview of your agent at the right.</li>
<li>Click “Files and versions” to see all the files in the repository.</li>
<li>Click “Use in stable-baselines3” to get a code snippet that shows how to load the model.</li>
<li>A model card (<code>README.md</code> file) which gives a description of the model</li>
</ul></li>
<li>Hugging Face Hub uses git-based repositories so we can update the model with new versions.</li>
</ul>
<p>Connect to Hugging Face Hub: 1. Create Hugging Face account https://huggingface.co/join 2. Create a new authentication token (https://huggingface.co/settings/tokens) <strong>with write role</strong> 3. Run the <code>notebook_login()</code> method.</p>
<p><strong>Log into Hugging Face account</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>notebook_login()</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>git config <span class="op">--</span><span class="kw">global</span> credential.helper store</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Login successful
    Your token has been saved to /root/.huggingface/token</code></pre>
<hr>
<p><code>package_to_hub</code> function arguments: - <code>model</code>: our trained model. - <code>model_name</code>: the name of the trained model that we defined in <code>model_save</code> - <code>model_architecture</code>: the model architecture we used (e.g., PPO) - <code>env_id</code>: the name of the environment, in our case <code>LunarLander-v2</code> - <code>eval_env</code>: the evaluation environment defined in eval_env - <code>repo_id</code>: the name of the Hugging Face Hub Repository that will be created/updated <code>(repo_id = {username}/{repo_name})</code> * <strong>Example format:</strong> {username}/{model_architecture}-{env_id} - <code>commit_message</code>: message of the commit</p>
<hr>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.vec_env <span class="im">import</span> DummyVecEnv</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_sb3 <span class="im">import</span> package_to_hub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Push the model to the Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the name of the environment</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>env_id <span class="op">=</span> <span class="st">"LunarLander-v2"</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the evaluation env</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>eval_env <span class="op">=</span> DummyVecEnv([<span class="kw">lambda</span>: gym.make(env_id)])</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model architecture we used</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>model_architecture <span class="op">=</span> <span class="st">"ppo"</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Define a repo_id</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>repo_id <span class="op">=</span> <span class="ss">f"cj-mills/</span><span class="sc">{</span>model_architecture<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>env_id<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_architecture<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>env_id<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co">## Define the commit message</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>commit_message <span class="op">=</span> <span class="ss">f"Upload </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> model with longer training session"</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="co"># method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>package_to_hub(model<span class="op">=</span>model, <span class="co"># Our trained model</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>               model_name<span class="op">=</span>model_name, <span class="co"># The name of our trained model </span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>               model_architecture<span class="op">=</span>model_architecture, <span class="co"># The model architecture we used: in our case PPO</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>               env_id<span class="op">=</span>env_id, <span class="co"># Name of the environment</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>               eval_env<span class="op">=</span>eval_env, <span class="co"># Evaluation Environment</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>               repo_id<span class="op">=</span>repo_id, <span class="co"># id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2</span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>               commit_message<span class="op">=</span>commit_message)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/ppo-LunarLander-v2'</code></pre>
<hr>
</section>
<section id="some-additional-challenges" class="level3">
<h3 class="anchored" data-anchor-id="some-additional-challenges">Some additional challenges</h3>
<ul>
<li>Train for more steps.</li>
<li>Try different <a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters">hyperparameters</a> of <code>PPO</code>.</li>
<li>Check the <a href="https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html">Stable-Baselines3 documentation</a> and try another model such as DQN.</li>
<li>Try using the <a href="https://www.gymlibrary.ml/environments/classic_control/cart_pole/">CartPole-v1</a>, <a href="https://www.gymlibrary.ml/environments/classic_control/mountain_car/">MountainCar-v0</a> or <a href="https://www.gymlibrary.ml/environments/box2d/car_racing/">CarRacing-v0</a> environments.</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://github.com/huggingface/deep-rl-class">The Hugging Face Deep Reinforcement Learning Class</a></li>
<li><a href="https://huggingface.co/blog/deep-rl-intro">An Introduction to Deep Reinforcement Learning</a></li>
</ul>
<!-- Cloudflare Web Analytics -->
<script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script>
<!-- End Cloudflare Web Analytics -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2022, Christian J. Mills
  </li>  
</ul>
      </div>
  </div>
</footer>



</body></html>