<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-08-29">
<meta name="description" content="In this talk, Hailey Schoelkopf from Eleuther AI provides an overview of the challenges in LLM evaluation, exploring different measurement techniques, highlighting reproducibility issues, and advocating for best practices like sharing evaluation code and using task-specific downstream evaluations.">

<title>Conference Talk 16: A Deep Dive on LLM Evaluation – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-d551e32f15e27e893f08ce3c93a41c1c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-10454ac70b1a46c3ffe242e9c1fedf28.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Conference Talk 16: A Deep Dive on LLM Evaluation – Christian Mills">
<meta property="og:description" content="In this talk, Hailey Schoelkopf from Eleuther AI provides an overview of the challenges in LLM evaluation, exploring different measurement techniques, highlighting reproducibility issues, and advocating for best practices like sharing evaluation code and using task-specific downstream evaluations.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Conference Talk 16: A Deep Dive on LLM Evaluation – Christian Mills">
<meta name="twitter:description" content="In this talk, Hailey Schoelkopf from Eleuther AI provides an overview of the challenges in LLM evaluation, exploring different measurement techniques, highlighting reproducibility issues, and advocating for best practices like sharing evaluation code and using task-specific downstream evaluations.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul>
  <li><a href="#about-the-speaker" id="toc-about-the-speaker" class="nav-link" data-scroll-target="#about-the-speaker">About the Speaker</a></li>
  <li><a href="#about-eleuther-ai" id="toc-about-eleuther-ai" class="nav-link" data-scroll-target="#about-eleuther-ai">About Eleuther AI:</a></li>
  <li><a href="#lm-evaluation-harness" id="toc-lm-evaluation-harness" class="nav-link" data-scroll-target="#lm-evaluation-harness">LM Evaluation Harness</a></li>
  </ul></li>
  <li><a href="#challenges-in-llm-evaluation" id="toc-challenges-in-llm-evaluation" class="nav-link" data-scroll-target="#challenges-in-llm-evaluation">Challenges in LLM Evaluation</a>
  <ul>
  <li><a href="#scoring-difficulties" id="toc-scoring-difficulties" class="nav-link" data-scroll-target="#scoring-difficulties">1. Scoring Difficulties</a></li>
  <li><a href="#reproducibility-issues" id="toc-reproducibility-issues" class="nav-link" data-scroll-target="#reproducibility-issues">2. Reproducibility Issues</a></li>
  </ul></li>
  <li><a href="#common-llm-evaluation-methods" id="toc-common-llm-evaluation-methods" class="nav-link" data-scroll-target="#common-llm-evaluation-methods">Common LLM Evaluation Methods</a>
  <ul>
  <li><a href="#log-likelihoods-assessing-the-probability-of-expected-outputs" id="toc-log-likelihoods-assessing-the-probability-of-expected-outputs" class="nav-link" data-scroll-target="#log-likelihoods-assessing-the-probability-of-expected-outputs">1. Log Likelihoods: Assessing the Probability of Expected Outputs</a></li>
  <li><a href="#perplexity-measuring-how-well-a-model-fits-a-data-distribution" id="toc-perplexity-measuring-how-well-a-model-fits-a-data-distribution" class="nav-link" data-scroll-target="#perplexity-measuring-how-well-a-model-fits-a-data-distribution">2. Perplexity: Measuring How Well a Model Fits a Data Distribution</a></li>
  <li><a href="#text-generation-evaluating-real-world-output-but-facing-scoring-challenges" id="toc-text-generation-evaluating-real-world-output-but-facing-scoring-challenges" class="nav-link" data-scroll-target="#text-generation-evaluating-real-world-output-but-facing-scoring-challenges">3. Text Generation: Evaluating Real-World Output but Facing Scoring Challenges</a></li>
  </ul></li>
  <li><a href="#the-need-for-reproducibility-and-best-practices" id="toc-the-need-for-reproducibility-and-best-practices" class="nav-link" data-scroll-target="#the-need-for-reproducibility-and-best-practices">The Need for Reproducibility and Best Practices</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul>
  <li><a href="#qa-highlights" id="toc-qa-highlights" class="nav-link" data-scroll-target="#qa-highlights">Q&amp;A Highlights:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Conference Talk 16: A Deep Dive on LLM Evaluation</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">llms</div>
  </div>
  </div>

<div>
  <div class="description">
    In this talk, <strong>Hailey Schoelkopf</strong> from <strong>Eleuther AI</strong> provides an overview of the challenges in LLM evaluation, exploring different measurement techniques, highlighting reproducibility issues, and advocating for best practices like sharing evaluation code and using task-specific downstream evaluations.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 29, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Slides">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Presentation Slides
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Slides:</strong> <a href="https://docs.google.com/presentation/d/1qTaDYqLCgxkUaTfxQkN1it4tx6_jixwv9ZtsbqQgE4U/">A Deep Dive on LM Evaluation</a></li>
</ul>
</div>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li><strong>Speaker:</strong> Hailey Schoelkopf, Research Scientist at Eleuther AI</li>
<li><strong>Topic:</strong> Deep dive into the challenges and best practices of Large Language Model (LLM) evaluation.</li>
</ul>
<section id="about-the-speaker" class="level3">
<h3 class="anchored" data-anchor-id="about-the-speaker">About the Speaker</h3>
<ul>
<li><strong>Hailey Schoelkopf:</strong>
<ul>
<li>Research Scientist at Eleuther AI.</li>
<li>Maintainer of the <strong>LM Evaluation Harness</strong>, a widely used open-source library for evaluating LLMs.</li>
</ul></li>
</ul>
</section>
<section id="about-eleuther-ai" class="level3">
<h3 class="anchored" data-anchor-id="about-eleuther-ai">About Eleuther AI:</h3>
<ul>
<li><strong>Website:</strong> <a href="https://eleuther.ai/">https://eleuther.ai/</a></li>
<li><strong>Project Page:</strong> <a href="https://www.eleuther.ai/projects/large-language-model-evaluation">Evaluating LLMs</a></li>
<li><strong>Non-profit research lab</strong> known for:
<ul>
<li>Releasing open-source LLMs like GPT-J and GPT-NeoX-20B.</li>
<li>Research on:
<ul>
<li>Model Interpretability</li>
<li>Datasets</li>
<li>Distributed Training</li>
<li>LLM Evaluation</li>
</ul></li>
<li>Building and maintaining tools for the open-source AI ecosystem.</li>
</ul></li>
</ul>
</section>
<section id="lm-evaluation-harness" class="level3">
<h3 class="anchored" data-anchor-id="lm-evaluation-harness">LM Evaluation Harness</h3>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/EleutherAI/lm-evaluation-harness">https://github.com/EleutherAI/lm-evaluation-harness</a></li>
<li><strong>Purpose:</strong>
<ul>
<li>Originally created to reproduce and track the evaluations from the GPT-3 paper.</li>
<li>Evolved into a comprehensive library for evaluating LLMs.</li>
</ul></li>
<li><strong>Usage:</strong>
<ul>
<li>Widely used by researchers and practitioners.</li>
<li>Powers the backend for the OpenLLM Leaderboard.</li>
</ul></li>
</ul>
</section>
</section>
<section id="challenges-in-llm-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-llm-evaluation">Challenges in LLM Evaluation</h2>
<section id="scoring-difficulties" class="level4">
<h4 class="anchored" data-anchor-id="scoring-difficulties">1. Scoring Difficulties</h4>
<ul>
<li><strong>Core Issue:</strong> Reliably evaluating the correctness of LLM responses in natural language.</li>
<li><strong>Challenges:</strong>
<ul>
<li><strong>Subjectivity of Language:</strong> What constitutes a “correct” response can be subjective and context-dependent.</li>
<li><strong>Hallucination:</strong> LLMs can generate plausible-sounding but incorrect information, making it challenging to determine accuracy based on surface-level analysis.</li>
<li><strong>Lack of Standardized Metrics:</strong> Absence of universally agreed-upon metrics for evaluating LLM performance across different tasks and domains.</li>
</ul></li>
</ul>
</section>
<section id="reproducibility-issues" class="level4">
<h4 class="anchored" data-anchor-id="reproducibility-issues">2. Reproducibility Issues</h4>
<ul>
<li><strong>Importance:</strong> Ensuring that evaluation results are consistent and replicable.</li>
<li><strong>Challenges:</strong>
<ul>
<li><strong>Sensitivity to Implementation Details:</strong> LLM performance can vary significantly based on seemingly minor differences in implementation, such as tokenization, prompt formatting, and hyperparameters.</li>
<li><strong>Lack of Transparency:</strong> Limited sharing of evaluation code and detailed methodologies makes it difficult for others to reproduce results.</li>
<li><strong>Data Set Variability:</strong> Differences in data set composition and quality can lead to inconsistent evaluations.</li>
</ul></li>
</ul>
</section>
</section>
<section id="common-llm-evaluation-methods" class="level2">
<h2 class="anchored" data-anchor-id="common-llm-evaluation-methods">Common LLM Evaluation Methods</h2>
<section id="log-likelihoods-assessing-the-probability-of-expected-outputs" class="level4">
<h4 class="anchored" data-anchor-id="log-likelihoods-assessing-the-probability-of-expected-outputs">1. Log Likelihoods: Assessing the Probability of Expected Outputs</h4>
<ul>
<li><p><strong>Background:</strong></p>
<ul>
<li>LLMs output a probability distribution over vocabulary for each possible next token.</li>
<li>This distribution represents the model’s confidence in different words following the given input.</li>
</ul></li>
<li><p><strong>How It Works:</strong></p>
<ul>
<li><p><strong>Input:</strong> A prompt (X) and a potential output (Y).</p></li>
<li><p><strong>Process:</strong> Calculate the probability of the model generating Y given X. This involves summing the log probabilities of each token in Y, conditioned on the preceding tokens in X and Y.</p>
<ul>
<li><p><span class="math display">\[
\log P(y|x) = \sum_{i=0}^{m-1} \log p(y_i | x, y_0, \ldots, y_{i-1}) = \sum_{i=0}^{m-1} l(n+i, y_{i})
\]</span></p></li>
<li><p>where <span class="math inline">\(\log p(y_i | x, y_0, \ldots, y_{i-1})\)</span> is the log probability of the <span class="math inline">\(i\)</span>-th target token conditioned on the full input <span class="math inline">\(x\)</span> and the preceding target tokens. (and where <span class="math inline">\(x, y_0, \ldots, y_{i-1}\)</span> denotes conditioning on only <span class="math inline">\(x\)</span>.).</p></li>
</ul></li>
<li><p><strong>Example:</strong> For the prompt “The cow jumped over the,” calculate the probability of the model generating “moon” versus other words.</p></li>
</ul></li>
<li><p><strong>Use Case: Multiple Choice Question Answering</strong></p>
<ul>
<li><strong>Advantages:</strong>
<ul>
<li>Computationally cheaper than generation-based evaluation.</li>
<li>Avoids issues with parsing errors in generated text.</li>
<li>Suitable for evaluating smaller LLMs or those in early training stages.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Limited real-world applicability compared to open-ended generation.</li>
<li>Doesn’t assess a model’s ability to formulate its own answers.</li>
<li>Cannot evaluate chain-of-thought reasoning.</li>
</ul></li>
</ul></li>
<li><p><strong>Challenges with Log Likelihoods and Perplexity:</strong></p>
<ul>
<li><strong>Tokenizer Sensitivity:</strong> Metrics are affected by the specific tokenizer used, making comparisons between models with different tokenizers difficult.
<ul>
<li><strong>Solution:</strong> Implement normalization techniques to account for tokenizer variations.</li>
</ul></li>
<li><strong>Limited Information:</strong> Log likelihoods only consider the probability of a given output, not its overall quality, coherence, or factual accuracy.</li>
</ul></li>
</ul>
</section>
<section id="perplexity-measuring-how-well-a-model-fits-a-data-distribution" class="level4">
<h4 class="anchored" data-anchor-id="perplexity-measuring-how-well-a-model-fits-a-data-distribution">2. Perplexity: Measuring How Well a Model Fits a Data Distribution</h4>
<ul>
<li><p><strong>Concept:</strong> Quantifies how well a language model predicts a given text, indicating its familiarity with the data distribution.</p></li>
<li><p><strong>Calculation:</strong> Based on the average per-token log probability of the text, with lower perplexity indicating a better fit to the data.</p>
<ul>
<li><span class="math display">\[
\text{PPL} = \exp \left( -\frac{1}{\sum_{j=1}^{|D|} N_j} \sum_{j=1}^{|D|} \sum_{i=1}^{N_j} \log P(y_{ji} | y_{j1}, \ldots, y_{ji-1}) \right)
\]</span></li>
</ul></li>
<li><p><strong>Use Case:</strong> Evaluating a model’s understanding of a specific text corpus (e.g., Wikipedia).</p></li>
<li><p><strong>Limitations:</strong></p>
<ul>
<li><strong>Domain Specificity:</strong> Perplexity on one dataset (e.g., Wikipedia) may not generalize to other domains or tasks.</li>
<li><strong>Limited Insight into Downstream Performance:</strong> A low perplexity doesn’t guarantee good performance in real-world applications like chatbots or question answering.</li>
</ul></li>
</ul>
</section>
<section id="text-generation-evaluating-real-world-output-but-facing-scoring-challenges" class="level4">
<h4 class="anchored" data-anchor-id="text-generation-evaluating-real-world-output-but-facing-scoring-challenges">3. Text Generation: Evaluating Real-World Output but Facing Scoring Challenges</h4>
<ul>
<li><strong>Importance:</strong> Crucial for assessing LLMs in tasks involving text generation (e.g., chatbots, story writing).</li>
<li><strong>Challenges:</strong>
<ul>
<li><strong>Scoring Free-Form Text:</strong> Determining the correctness and quality of generated text is difficult.
<ul>
<li>Simple heuristics (e.g., keyword matching) are unreliable and prone to gaming.</li>
<li>Human evaluation is expensive and time-consuming.</li>
<li>LLM-based judges introduce their own biases and limitations.</li>
</ul></li>
<li><strong>Sensitivity to Prompt Details:</strong> Minor variations in prompts (e.g., trailing whitespace) can drastically impact results, hindering reproducibility.
<ul>
<li><strong>Example:</strong> In code generation, a trailing tab in the prompt can create syntax errors for models that generate code with specific formatting, leading to artificially lower performance scores.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="the-need-for-reproducibility-and-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-reproducibility-and-best-practices">The Need for Reproducibility and Best Practices</h2>
<ul>
<li><strong>Reproducibility is Crucial:</strong> Ensuring that evaluation results can be independently verified is essential for:
<ul>
<li><strong>Fair Model Comparisons:</strong> Accurately assessing the relative performance of different LLMs.</li>
<li><strong>Meaningful Progress Tracking:</strong> Tracking improvements in model development and evaluation methods.</li>
</ul></li>
<li><strong>Challenges to Reproducibility:</strong>
<ul>
<li>Lack of standardized evaluation practices and metrics.</li>
<li>Incomplete reporting of evaluation details (e.g., prompts, code, evaluation settings).</li>
</ul></li>
<li><strong>Best Practices for Reproducible LLM Evaluation:</strong>
<ul>
<li><strong>Share Evaluation Code:</strong> Publicly release code used for evaluation to allow for scrutiny and replication of results.</li>
<li><strong>Detailed Reporting:</strong> Provide comprehensive information about evaluation procedures, including:
<ul>
<li>Specific prompts and instructions given to models.</li>
<li>Data preprocessing steps and evaluation datasets used.</li>
<li>Evaluation metrics and their calculation.</li>
</ul></li>
<li><strong>Use Standardized Evaluation Frameworks:</strong> Leverage libraries like the <code>lm-evaluation-harness</code> or other tools (Helm, OpenCompass) to promote consistency and reduce implementation discrepancies.
<ul>
<li><strong><code>lm-evaluation-harness</code>:</strong> <a href="https://github.com/EleutherAI/lm-evaluation-harness">GitHub Repository</a></li>
<li><strong><code>helm</code>:</strong> <a href="https://github.com/stanford-crfm/helm">GitHub Repository</a></li>
<li><strong><code>opencompass</code>:</strong> <a href="https://github.com/open-compass/opencompass">GitHub Repository</a></li>
</ul></li>
</ul></li>
<li><strong>Distinction Between Model Evals and Downstream Evals:</strong>
<ul>
<li><strong>Model Evals (e.g., MMLU benchmark):</strong> Measure general language understanding and capabilities across diverse tasks.</li>
<li><strong>Downstream Evals:</strong> Focus on performance in a specific application or domain (e.g., chatbot for customer support).</li>
<li>Prioritize Downstream Evals whenever possible for production settings, as they directly reflect real-world performance needs.</li>
<li><strong>OpenLLM Leaderboard:</strong> <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></li>
<li><strong>MMLU Benchmark:</strong> <a href="https://paperswithcode.com/dataset/mmlu">https://paperswithcode.com/dataset/mmlu</a></li>
<li><strong>HellaSwag Benchmark:</strong> <a href="https://paperswithcode.com/dataset/hellaswag">https://paperswithcode.com/dataset/hellaswag</a></li>
<li><strong>ARC Benchmark:</strong> <a href="https://paperswithcode.com/dataset/arc">https://paperswithcode.com/dataset/arc</a>
<ul>
<li>ARC focuses on generalization and multi-step reasoning, making it more challenging than benchmarks that rely heavily on memorization.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li><strong>Implementation Details Matter:</strong> LLMs are highly sensitive to minor variations in evaluation procedures.</li>
<li><strong>Transparency and Standardization are Key:</strong> Sharing code, detailed reporting, and using standardized frameworks are crucial for reproducible LLM evaluation.</li>
<li><strong>Prioritize Downstream Evaluations:</strong> Focus on evaluations that directly measure performance in your specific application context.</li>
<li><strong>For Further Exploration:</strong>
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2405.14782"><em>Lessons from the Trenches on Reproducible Evaluation of Language Models</em></a> (Eleuther AI)</li>
<li><strong>Library:</strong> <a href="https://github.com/EleutherAI/lm-evaluation-harness"><code>lm-evaluation-harness</code></a> (Eleuther AI)</li>
</ul></li>
</ul>
<section id="qa-highlights" class="level3">
<h3 class="anchored" data-anchor-id="qa-highlights">Q&amp;A Highlights:</h3>
<ul>
<li><strong>Dataset Quality:</strong> Errors or biases in benchmark datasets can significantly affect evaluation results and limit the usefulness of benchmarks.</li>
<li><strong>Overfitting to Evaluations:</strong> Repeatedly optimizing for a specific benchmark can lead to overfitting, where models excel on the benchmark but fail to generalize to other tasks or data.</li>
<li><strong>Measurement Validity:</strong> It’s essential to ensure that evaluation metrics accurately measure the desired aspects of LLM performance (e.g., factual accuracy, reasoning, coherence).</li>
<li><strong>LLMs as Judges:</strong>
<ul>
<li><strong>Benefits:</strong> LLMs can potentially automate the evaluation of tasks requiring nuanced understanding and reasoning, which are difficult to assess with simple heuristics.</li>
<li><strong>Considerations:</strong>
<ul>
<li><strong>Judge Model Selection:</strong> Carefully choose an LLM judge that possesses the necessary capabilities for the task being evaluated.</li>
<li><strong>Judge Model Limitations:</strong> Be aware of the judge model’s own biases and limitations, as these can influence the evaluation outcomes.</li>
</ul></li>
</ul></li>
<li><strong>Reliable Multiple-Choice Answers Without Additional Text:</strong>
<ul>
<li><strong>Structured Generation:</strong> Use techniques that constrain the model’s output to specific formats.</li>
<li><strong>System Prompts:</strong> Provide clear instructions to the model to only output the answer.</li>
<li><strong>Log Likelihoods:</strong> Rely on log likelihood-based multiple-choice evaluations if structured generation isn’t possible.</li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, an Applied AI Consultant and Educator.</p>
<p>Whether I’m writing an in-depth tutorial or sharing detailed notes, my goal is the same: to bring clarity to complex topics and find practical, valuable insights.</p>
<p>If you need a strategic partner who brings this level of depth and systematic thinking to your AI project, I’m here to help. Let’s talk about de-risking your roadmap and building a real-world solution.</p>
<p>Start the conversation with my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> or learn more <a href="../../../about.html">about my approach</a>.</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/christianjmills\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>