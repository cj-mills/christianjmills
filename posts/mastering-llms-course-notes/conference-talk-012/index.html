<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-07-24">
<meta name="description" content="In this talk, Mark Saroufim and Jane Xu, discuss techniques and tools for mitigating Out of Memory (OOM) errors in PyTorch, specifically when working with LLMs.">

<title>Conference Talk 12: Slaying OOMs with PyTorch FSDP and torchao – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-61f2d351c58b11e1d25c66c489878dfa.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-fb8cbff63e0d11b0ded76255c6f80362.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Conference Talk 12: Slaying OOMs with PyTorch FSDP and torchao – Christian Mills">
<meta property="og:description" content="In this talk, Mark Saroufim and Jane Xu, discuss techniques and tools for mitigating Out of Memory (OOM) errors in PyTorch, specifically when working with LLMs.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Conference Talk 12: Slaying OOMs with PyTorch FSDP and torchao – Christian Mills">
<meta name="twitter:description" content="In this talk, Mark Saroufim and Jane Xu, discuss techniques and tools for mitigating Out of Memory (OOM) errors in PyTorch, specifically when working with LLMs.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#understanding-model-memory" id="toc-understanding-model-memory" class="nav-link" data-scroll-target="#understanding-model-memory">Understanding Model Memory</a></li>
  <li><a href="#optimizing-memory-usage" id="toc-optimizing-memory-usage" class="nav-link" data-scroll-target="#optimizing-memory-usage">Optimizing Memory Usage</a>
  <ul>
  <li><a href="#optimizer-memory" id="toc-optimizer-memory" class="nav-link" data-scroll-target="#optimizer-memory">Optimizer Memory</a></li>
  <li><a href="#parameter-quantization" id="toc-parameter-quantization" class="nav-link" data-scroll-target="#parameter-quantization">Parameter Quantization</a></li>
  <li><a href="#gradient-optimization-with-lora" id="toc-gradient-optimization-with-lora" class="nav-link" data-scroll-target="#gradient-optimization-with-lora">Gradient Optimization with LoRA</a></li>
  <li><a href="#challenges-and-solutions-with-qlora" id="toc-challenges-and-solutions-with-qlora" class="nav-link" data-scroll-target="#challenges-and-solutions-with-qlora">Challenges and Solutions with QLoRA</a></li>
  </ul></li>
  <li><a href="#model-parallelism-with-fsdp" id="toc-model-parallelism-with-fsdp" class="nav-link" data-scroll-target="#model-parallelism-with-fsdp">Model Parallelism with FSDP</a>
  <ul>
  <li><a href="#data-parallism" id="toc-data-parallism" class="nav-link" data-scroll-target="#data-parallism">Data Parallism</a></li>
  <li><a href="#fsdp" id="toc-fsdp" class="nav-link" data-scroll-target="#fsdp">FSDP</a></li>
  <li><a href="#fsdp1-vs.-fsdp2" id="toc-fsdp1-vs.-fsdp2" class="nav-link" data-scroll-target="#fsdp1-vs.-fsdp2">FSDP1 vs.&nbsp;FSDP2</a></li>
  <li><a href="#cpu-offloading" id="toc-cpu-offloading" class="nav-link" data-scroll-target="#cpu-offloading">CPU Offloading</a></li>
  </ul></li>
  <li><a href="#benchmarking-fsdp2" id="toc-benchmarking-fsdp2" class="nav-link" data-scroll-target="#benchmarking-fsdp2">Benchmarking FSDP2</a>
  <ul>
  <li><a href="#benchmarking-plan" id="toc-benchmarking-plan" class="nav-link" data-scroll-target="#benchmarking-plan">Benchmarking Plan</a></li>
  <li><a href="#initial-benchmarking-and-discrepancies" id="toc-initial-benchmarking-and-discrepancies" class="nav-link" data-scroll-target="#initial-benchmarking-and-discrepancies">Initial Benchmarking and Discrepancies</a></li>
  <li><a href="#identifying-the-root-causes" id="toc-identifying-the-root-causes" class="nav-link" data-scroll-target="#identifying-the-root-causes">Identifying the Root Causes</a></li>
  <li><a href="#addressing-the-discrepancies-and-further-benchmarking" id="toc-addressing-the-discrepancies-and-further-benchmarking" class="nav-link" data-scroll-target="#addressing-the-discrepancies-and-further-benchmarking">Addressing the Discrepancies and Further Benchmarking</a></li>
  <li><a href="#deep-dive-into-performance-gaps" id="toc-deep-dive-into-performance-gaps" class="nav-link" data-scroll-target="#deep-dive-into-performance-gaps">Deep Dive into Performance Gaps</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Conference Talk 12: Slaying OOMs with PyTorch FSDP and torchao</h1>
  <div class="quarto-categories">
    <div class="quarto-category">notes</div>
    <div class="quarto-category">llms</div>
  </div>
  </div>

<div>
  <div class="description">
    In this talk, <strong>Mark Saroufim</strong> and <strong>Jane Xu</strong>, discuss techniques and tools for mitigating Out of Memory (OOM) errors in PyTorch, specifically when working with LLMs.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 24, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#understanding-model-memory">Understanding Model Memory</a></li>
<li><a href="#optimizing-memory-usage">Optimizing Memory Usage</a></li>
<li><a href="#model-parallelism-with-fsdp">Model Parallelism with FSDP</a></li>
<li><a href="#benchmarking-fsdp2">Benchmarking FSDP2</a></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Resources">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Presentation Resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://drive.google.com/drive/u/0/folders/1HmGNC4v4L5nXhtdDMVCpUBrme1ELp-2C">Slaying OOMs</a></li>
</ul>
</div>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>OOM errors are a common challenge when working with large PyTorch models.</li>
<li>Traditional solutions like reducing batch size or model size are limited in effectiveness.</li>
</ul>
</section>
<section id="understanding-model-memory" class="level2">
<h2 class="anchored" data-anchor-id="understanding-model-memory">Understanding Model Memory</h2>
<ul>
<li><strong>VRAM Constraint:</strong> Modern GPUs have limited VRAM (e.g., 24GB for 3090s and 4090s), leading to challenges when training large models.</li>
<li><strong>Model Memory Components:</strong>
<ul>
<li><strong>Parameters:</strong> Model weights, typically stored in FP16 (2 bytes per parameter). For a 7B parameter LLaMa model, this translates to 14GB.</li>
<li><strong>Gradients:</strong> Calculated during backpropagation, require the same storage size as parameters (another 14GB for LLaMa 7B).</li>
<li><strong>Optimizer State:</strong> Optimizers like Adam store additional information, often twice the size of parameters (28GB for LLaMa 7B).</li>
<li><strong>Activations:</strong> Intermediate outputs of model layers.
<ul>
<li>The size is harder to estimate and depends on factors like batch size and context length.</li>
<li>Activations tend to dominate memory usage at larger batch sizes and context lengths.</li>
</ul></li>
</ul></li>
<li><strong>Example:</strong> A full fine-tuning of a 7B parameter LLaMa model can easily exceed 56GB (14GB parameters + 14GB gradients + 28GB optimizer state), exceeding the VRAM capacity of most consumer GPUs.</li>
<li><strong>Forum Post:</strong> <a href="https://dev-discuss.pytorch.org/t/how-to-measure-memory-usage-from-your-model-without-running-it/2024/1">How to measure memory usage from your model without running it?</a></li>
</ul>
</section>
<section id="optimizing-memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-memory-usage">Optimizing Memory Usage</h2>
<section id="optimizer-memory" class="level3">
<h3 class="anchored" data-anchor-id="optimizer-memory">Optimizer Memory</h3>
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></li>
<li>While alternative optimizers like SGD might seem appealing due to their lower memory overhead, Adam remains dominant in practice due to its effectiveness.</li>
<li>Replacing Adam is challenging and might not yield significant memory savings.</li>
</ul>
</section>
<section id="parameter-quantization" class="level3">
<h3 class="anchored" data-anchor-id="parameter-quantization">Parameter Quantization</h3>
<ul>
<li><p><strong>4-bit Quantization:</strong> Reduces the precision of model parameters from FP16 to INT4 (half a byte per parameter).</p></li>
<li><p>This technique can significantly reduce memory footprint, for example, bringing the size of a 7B parameter LLaMa model down from 14GB to 3.5GB.</p></li>
<li><p><strong>Torch Compile:</strong> This tool can be leveraged to create efficient quantization kernels directly from Python code, eliminating the need for custom CUDA kernels.</p>
<ul>
<li><p>Decorate <code>quantize_tensor</code> and <code>dequantize_tensor</code> functions with <code>@torch.compile()</code></p></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"TORCH_LOGS"</span>] <span class="op">=</span> <span class="st">"output_code"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.compile</span>()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_tensor(x_fp32):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    absmax <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(x_fp32))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> <span class="fl">127.0</span> <span class="op">/</span> absmax</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    x_int8 <span class="op">=</span> torch.<span class="bu">round</span>(c <span class="op">*</span> x_fp32).to(torch.int8)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_int8, c</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.compile</span>()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dequantize_tensor(x_int8, c):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    x_fp32 <span class="op">=</span> x_int8.to(torch.float32) <span class="op">/</span> c</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_fp32</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>x_int8, c <span class="op">=</span> quantize_tensor(torch.randn(<span class="dv">10</span>, device<span class="op">=</span><span class="st">"cuda"</span>))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>x_fp32 <span class="op">=</span> dequantize_tensor(x_int8, c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div></li>
<li><p><strong>Docs:</strong> <a href="https://pytorch.org/docs/stable/generated/torch.compile.html">https://pytorch.org/docs/stable/generated/torch.compile.html</a></p></li>
<li><p><strong>Tutorial:</strong> <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">Introduction to <code>torch.compile</code></a></p></li>
</ul></li>
</ul>
</section>
<section id="gradient-optimization-with-lora" class="level3">
<h3 class="anchored" data-anchor-id="gradient-optimization-with-lora">Gradient Optimization with LoRA</h3>
<ul>
<li>Directly quantizing gradients to 4-bit negatively impacts convergence.</li>
<li><strong>Low-Rank Adaptation (LoRA):</strong> Circumvents this issue by training only a small subset of parameters (adapters) while keeping the majority frozen.</li>
<li><strong>QLoRA:</strong> This technique combines LoRA with parameter quantization to achieve significant memory savings without compromising accuracy.</li>
</ul>
</section>
<section id="challenges-and-solutions-with-qlora" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-solutions-with-qlora">Challenges and Solutions with QLoRA</h3>
<ul>
<li><p>QLoRA implementation can be complex, often requiring custom CUDA kernels (e.g., the <a href="https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/csrc/kernels.cu">original implementation</a> by <a href="https://timdettmers.com/about/">Tim Dettmers</a> consists of 4,000 lines of CUDA code).</p>
<ul>
<li>Weights aren’t in int4 but NF4 which is closer to a normal distribution</li>
<li>Can’t matrix multiply NF4 tensors
<ul>
<li>need to dequantize and matmul</li>
</ul></li>
<li>Can’t use the same max for everything otherwise you’re too sensitive to outliers</li>
<li>Quantization typically done in blocks with independent scales</li>
<li>QLoRA quantizes the scales (double quantization)</li>
</ul></li>
<li><p><strong>Simplified Implementation with Torch Compile:</strong> <a href="https://github.com/drisspg">Driss Guessous</a>, a PyTorch developer, implemented QLoRA in approximately 900 lines of Python code using Torch Compile.</p>
<ul>
<li><strong>Code:</strong> <a href="https://www.github.com/torchao/dtypes/nf4tensor.py">torchao/dtypes/nf4tensor.py</a></li>
</ul></li>
<li><p><strong>Tensor Subclasses:</strong> PyTorch’s tensor subclassing feature enables the creation of custom data types like NF4 (used in QLoRA), allowing for more efficient representation and manipulation of quantized tensors.</p>
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> F.linear(<span class="bu">input</span>, weight.to(<span class="bu">input</span>.dtype))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div></li>
<li><p><strong>Docs:</strong> <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.Tensor</code></a></p></li>
<li><p><strong><a href="https://github.com/albanD/subclass_zoo/">subclass_zoo</a>:</strong> Contains a number of examples of Tensor subclasses in PyTorch</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="model-parallelism-with-fsdp" class="level2">
<h2 class="anchored" data-anchor-id="model-parallelism-with-fsdp">Model Parallelism with FSDP</h2>
<section id="data-parallism" class="level3">
<h3 class="anchored" data-anchor-id="data-parallism">Data Parallism</h3>
<ul>
<li><strong>Data parallelism:</strong> Split batches across multipls devices and keep a copy of the gradients, model params, and optimizer state on each device</li>
<li>Data parallelism alone, while helpful, might not be sufficient for extremely large models.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/oom-with-data-parallelism.png" class="img-fluid figure-img"></p>
<figcaption>OOM With Data Parallelism - Slaying OOMs: Slide 19</figcaption>
</figure>
</div>
</section>
<section id="fsdp" class="level3">
<h3 class="anchored" data-anchor-id="fsdp">FSDP</h3>
<ul>
<li><strong>Fully Sharded Data Parallel (FSDP):</strong> Distributes model parameters (and by consequence the gradients, and optimizer states) across multiple GPUs, allowing for training models that exceed the memory capacity of a single device.
<ul>
<li>Memory corresponding to the layer getting processed will be freed when the layer is done.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fsdp-memory-step.png" class="img-fluid figure-img"></p>
<figcaption>FSDP Memory - Slaying OOMs: Slide 21</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Sharding:</strong> Dividing model components into smaller chunks (shards) that can be placed on different GPUs.</p></li>
<li><p><strong>All Gather Operations:</strong> Used to gather necessary data from different GPUs during model training.</p></li>
<li><p><strong>Layers</strong>:</p>
<ul>
<li>Every PyTorch <code>nn</code> module is a tree of more <code>nn</code> modules.</li>
<li>The user’s wrapping policy determines what gets treated as its own “layer”.</li>
<li>What you decided to wrap influences memory usage
<ul>
<li>Smaller blobs = less memory needs to be all-gathered at a time.</li>
</ul></li>
</ul></li>
<li><p><strong>CPU Offloading:</strong></p>
<ul>
<li><p>Can keep parameters on the CPU and move them to the GPU when computing forward + backward.</p></li>
<li><p>The optimizer update will be done on CPU, so the optim state lives there too.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fsdp-memory-step-cpu-offloading.png" class="img-fluid figure-img"></p>
<figcaption>FSDP CPU Offloading - Slaying OOMs: Slide 25</figcaption>
</figure>
</div></li>
</ul></li>
</ul>
</section>
<section id="fsdp1-vs.-fsdp2" class="level3">
<h3 class="anchored" data-anchor-id="fsdp1-vs.-fsdp2">FSDP1 vs.&nbsp;FSDP2</h3>
<ul>
<li><p><strong>Goal:</strong> Make all-gather efficient</p></li>
<li><p><strong>Constraint:</strong> <a href="https://developer.nvidia.com/nccl">NCCL</a> (NVIDIA Collective Communications Library) requires each GPU contribute same-size Tensors</p></li>
<li><p><strong>FSDP1:</strong></p>
<ul>
<li><p>Flattens and concatenates all tensors before sharding, potentially leading to type and metadata conflicts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/flatparam-fsdp-1.png" class="img-fluid figure-img"></p>
<figcaption>FlatParam FSDP - Slaying OOMs: Slide 28</figcaption>
</figure>
</div>
<ul>
<li><code>t1</code> and the first part of <code>t2</code> get combined into a single tensor</li>
<li>The second part of <code>t2</code> gets combined with <code>t3</code> into a single tensor</li>
<li><code>t2</code> gets split between GPUs</li>
<li>Forces <code>t1</code>, <code>t2</code>, and <code>t3</code> to all have the same d-type and other metadata</li>
</ul></li>
<li><p>Can lead to non-deterministic memory spikes, making memory management challenging.</p></li>
</ul></li>
<li><p><strong>FSDP2 (Per-Parameter FSDP):</strong></p>
<ul>
<li><p>Introduces distributed tensors (D-tensors) that allow for sharding individual tensors, preserving their original data types and metadata.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/per-parameter-fsdp-2.png" class="img-fluid figure-img"></p>
<figcaption>Per-Parameter FSDP - Slaying OOMs: Slide 30</figcaption>
</figure>
</div></li>
<li><p>Offers better memory determinism, ensuring more predictable and manageable memory usage.</p></li>
<li><p>Enables more flexible and efficient training scenarios, especially when combined with techniques like QLoRA.</p></li>
<li><p>Requires extra copies during all-gather compared to FSDP1</p></li>
<li><p><strong>Forum Post:</strong> <a href="https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486/1">FSDP &amp; CUDACachingAllocator: an outsider newb perspective</a></p></li>
</ul></li>
</ul>
</section>
<section id="cpu-offloading" class="level3">
<h3 class="anchored" data-anchor-id="cpu-offloading">CPU Offloading</h3>
<ul>
<li>Leverages CPU memory to store parameters and optimizer states, further reducing the memory load on GPUs.</li>
<li>Offloads the optimizer step to the CPU, allowing GPUs to focus on computationally intensive forward and backward passes.</li>
</ul>
</section>
</section>
<section id="benchmarking-fsdp2" class="level2">
<h2 class="anchored" data-anchor-id="benchmarking-fsdp2">Benchmarking FSDP2</h2>
<section id="benchmarking-plan" class="level3">
<h3 class="anchored" data-anchor-id="benchmarking-plan">Benchmarking Plan</h3>
<ul>
<li><strong>Goal:</strong> Run benchmarks, identify performance gaps between FSDP2 and the baseline (FSDP1 &amp; bnb), and explore ways to improve FSDP2’s speed.</li>
<li><strong>Hardware:</strong> Two NVIDIA 3090 GPUs (consumer-grade, 24GB VRAM each) acquired from Vast AI.</li>
<li><strong>Baseline:</strong> Answer.ai’s train.py with FSDP1 and bnb, using a batch size of 8.</li>
</ul>
</section>
<section id="initial-benchmarking-and-discrepancies" class="level3">
<h3 class="anchored" data-anchor-id="initial-benchmarking-and-discrepancies">Initial Benchmarking and Discrepancies</h3>
<ul>
<li><p><strong>Benchmarking Environments:</strong></p>
<ul>
<li><p><a href="https://github.com/AnswerDotAI/fsdp_qLoRA/blob/main/train.py">Answer.ai’s train.py</a> (command-line configuration)</p>
<ul>
<li><div class="sourceCode" id="cb3"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>  <span class="ex">python</span> train.py <span class="at">--model_name</span> meta-llama/Llama-2-7b-hf <span class="at">--batch_size</span> 8 <span class="at">--context_length</span> 2048 <span class="at">--train_type</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="ex">qLoRA</span> <span class="at">--use_gradient_checkpointing</span> True <span class="at">--reentrant_checkpointing</span> True <span class="at">--dataset</span> dummy <span class="at">--dataset_samples</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">48</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
<li><p><a href="https://github.com/pytorch/torchtune/blob/1fa1f04baf124c074dcd93831fa38c8b657af1e9/recipes/configs/dev/llama2/7B_qLoRA_fsdp2.yaml">TorchTune recipe</a> (YAML configuration)</p></li>
</ul></li>
<li><p><strong>Initial Findings:</strong></p>
<ul>
<li><table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>batch size</th>
<th>peak memory</th>
<th>runtime for a step</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>train.py</td>
<td>8</td>
<td>12.88 GiB</td>
<td>14.0s</td>
</tr>
<tr class="even">
<td>torchtune</td>
<td>8</td>
<td>12.60 GiB</td>
<td>16.5s</td>
</tr>
</tbody>
</table></li>
<li><p>FSDP2 showed better peak memory usage.</p></li>
<li><p>FSDP2 was slower in runtime.</p></li>
</ul></li>
<li><p><strong>Tracing Analysis (Perfetto):</strong></p>
<ul>
<li><strong>Blog Posts:</strong>
<ul>
<li><a href="https://pytorch.org/blog/understanding-gpu-memory-1/">Understanding GPU Memory 1: Visualizing All Allocations over Time</a></li>
<li><a href="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles</a></li>
</ul></li>
<li><strong>Traces:</strong>
<ul>
<li><strong>Google Drive:</strong> <a href="https://drive.google.com/file/d/1ObfUUySBwuaCSLMXRxFiM1w7XYMebxvB/view">answer.ai train.py</a></li>
<li><strong>Google Drive:</strong> <a href="https://drive.google.com/file/d/1BpdlZZ55746IHcifho2u2okJQ1ihr8dY/view">torchtune</a></li>
</ul></li>
<li><strong><a href="https://ui.perfetto.dev/">Perfetto</a>:</strong> A production-grade open-source stack for performance instrumentation and trace analysis</li>
<li><strong>Significant difference in the number of optimizer steps:</strong> The optimizer took much longer in FSDP2.</li>
<li><strong>Double the number of operations:</strong> Indicating FSDP2 might be training on more parameters than the baseline.</li>
</ul></li>
</ul>
</section>
<section id="identifying-the-root-causes" class="level3">
<h3 class="anchored" data-anchor-id="identifying-the-root-causes">Identifying the Root Causes</h3>
<ul>
<li><strong>Root Cause 1: Configuration Discrepancy</strong>
<ul>
<li>TorchTune recipe LoRA-fied to the output projection (adding two low rank adapters per layer), resulting in 64 additional parameters compared to train.py.</li>
</ul></li>
<li><strong>Root Cause 2: Tensor Subclass Dispatch Overhead</strong>
<ul>
<li>Distributed tensors (D-tensors) introduce overhead due to metadata unwrapping during kernel calls.</li>
<li>This overhead was amplified by the increased number of parameters, making the optimizer step significantly slower in FSDP2.</li>
</ul></li>
</ul>
</section>
<section id="addressing-the-discrepancies-and-further-benchmarking" class="level3">
<h3 class="anchored" data-anchor-id="addressing-the-discrepancies-and-further-benchmarking">Addressing the Discrepancies and Further Benchmarking</h3>
<ul>
<li><strong>Ensuring Apples-to-Apples Comparison:</strong>
<ul>
<li>Standardized the dataset, parameter count, and wrapping policy across benchmarks.</li>
</ul></li>
<li><strong>Results After Standardization:</strong>
<ul>
<li>Still slower runtime for FSDP2.</li>
<li>Significantly improved peak memory usage.</li>
</ul></li>
<li><strong>Further Analysis:</strong>
<ul>
<li>Focused on forward pass, backward pass, and optimizer as the main culprits for the remaining performance gap.</li>
</ul></li>
</ul>
</section>
<section id="deep-dive-into-performance-gaps" class="level3">
<h3 class="anchored" data-anchor-id="deep-dive-into-performance-gaps">Deep Dive into Performance Gaps</h3>
<ul>
<li><strong>Gap 1: Optimizer Step Slowdown</strong>
<ul>
<li><strong>Problem:</strong> D-tensor overhead resulted in a 3x slower optimizer step.</li>
<li><strong>Solution:</strong> Collaborated with Intel to develop a fused Adam optimizer kernel for single dispatch and vectorization, leading to an 8x speedup.</li>
</ul></li>
<li><strong>Gap 2: Larger All Gather Operations</strong>
<ul>
<li><strong>Problem 1:</strong> FSDP2 packed more data (scalars and quantization factors) into the all gather operation, unlike the baseline.</li>
<li><strong>Problem 2:</strong> Output projection wasn’t quantized in FSDP2 when opting out of LoRA, leading to a 4x larger size compared to the baseline.</li>
<li><strong>Solution:</strong>
<ul>
<li>Problem 1 is expected behavior.</li>
<li>Problem 2 will be addressed by TorchTune in future releases.</li>
</ul></li>
</ul></li>
<li><strong>Gap 3: Overhead from Dequantization</strong>
<ul>
<li><strong>Problem:</strong> FSDP2’s dequantization process from NF4 to BF16 for CUDA kernels was less efficient than the specialized implementation used in the baseline.</li>
<li><strong>Solution:</strong> Explore using Torch Compile, custom Torch kernels, or Triton kernels for faster dequantization.</li>
</ul></li>
<li><strong>Gap 4: Different Rope Algorithms</strong>
<ul>
<li><strong>Problem:</strong> TorchTune and the baseline employed different Rope algorithms, leading to different operations before the SDPA.</li>
<li><strong>Solution:</strong> TorchTune to offer a wider selection of Rope algorithms.</li>
</ul></li>
<li><strong>Gap 5: Overlapping Communication and Computation</strong>
<ul>
<li><strong>Problem:</strong> FSDP2’s stricter memory management exposed inefficiencies in overlapping CPU offloading with computation. The computation tasks were much smaller than the communication tasks, leading to idle time.</li>
<li><strong>Solution:</strong> Adjusted the wrapping policy to group layers differently, enabling better overlap and reducing idle time. This solution is only feasible with FSDP2 due to its ability to handle mixed precision within a layer.</li>
</ul></li>
</ul>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, a deep learning consultant specializing in practical AI implementations. I help clients leverage cutting-edge AI technologies to solve real-world problems.</p>
<p>Interested in working together? Fill out my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> form or learn more <a href="../../../about.html">about me</a>.</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2025 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>