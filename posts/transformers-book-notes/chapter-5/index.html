<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-08">
<meta name="description" content="Chapter 5 covers different methods for generating text with GPT-2.">

<title>Christian Mills - Notes on Transformers Book Ch. 5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on Transformers Book Ch. 5">
<meta property="og:description" content="Chapter 5 covers different methods for generating text with GPT-2.">
<meta property="og:image" content="christianjmills.com/posts/transformers-book-notes/chapter-5/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Notes on Transformers Book Ch. 5">
<meta name="twitter:description" content="Chapter 5 covers different methods for generating text with GPT-2.">
<meta name="twitter:image" content="christianjmills.com/posts/transformers-book-notes/chapter-5/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html" rel="" target="">
 <span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../services.html" rel="" target="">
 <span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com" rel="" target=""><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-challenge-with-generating-coherent-text" id="toc-the-challenge-with-generating-coherent-text" class="nav-link" data-scroll-target="#the-challenge-with-generating-coherent-text">The Challenge with Generating Coherent Text</a>
  <ul>
  <li><a href="#gpt-2-pretraining-process" id="toc-gpt-2-pretraining-process" class="nav-link" data-scroll-target="#gpt-2-pretraining-process">GPT-2 Pretraining Process</a></li>
  <li><a href="#plefty_1ldotsy_t-vert-x-right-prodn_t1plefty_t-vert-y_-tx-right" id="toc-plefty_1ldotsy_t-vert-x-right-prodn_t1plefty_t-vert-y_-tx-right" class="nav-link" data-scroll-target="#plefty_1ldotsy_t-vert-x-right-prodn_t1plefty_t-vert-y_-tx-right"><span class="math display">\[P\left(y_{1},\ldots,y_{t} \vert x \right) = \prod^{N}_{t=1}{P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}\]</span></a></li>
  <li><a href="#decoding" id="toc-decoding" class="nav-link" data-scroll-target="#decoding">Decoding</a></li>
  <li><a href="#plefty_t-w_i-vert-y_-tx-right-softmax-left-z_ti-right" id="toc-plefty_t-w_i-vert-y_-tx-right-softmax-left-z_ti-right" class="nav-link" data-scroll-target="#plefty_t-w_i-vert-y_-tx-right-softmax-left-z_ti-right"><span class="math display">\[P\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = softmax \left( z_{t,i} \right)\]</span></a></li>
  <li><a href="#haty-undersetyargmax-plefty-vert-x-right" id="toc-haty-undersetyargmax-plefty-vert-x-right" class="nav-link" data-scroll-target="#haty-undersetyargmax-plefty-vert-x-right"><span class="math display">\[\hat{y} = \underset{y}{argmax} P\left(y \vert x \right)\]</span></a></li>
  </ul></li>
  <li><a href="#greedy-search-decoding" id="toc-greedy-search-decoding" class="nav-link" data-scroll-target="#greedy-search-decoding">Greedy Search Decoding</a>
  <ul>
  <li><a href="#haty_t-undersetyargmax-plefty_t-vert-y_-tx-right" id="toc-haty_t-undersetyargmax-plefty_t-vert-y_-tx-right" class="nav-link" data-scroll-target="#haty_t-undersetyargmax-plefty_t-vert-y_-tx-right"><span class="math display">\[\hat{y}_{t} = \underset{y}{argmax} {P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}\]</span></a>
  <ul class="collapse">
  <li><a href="#generationmixin.generate" id="toc-generationmixin.generate" class="nav-link" data-scroll-target="#generationmixin.generate"><code>GenerationMixin.generate</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#beam-search-decoding" id="toc-beam-search-decoding" class="nav-link" data-scroll-target="#beam-search-decoding">Beam Search Decoding</a>
  <ul>
  <li><a href="#logplefty_1ldotsy_t-vert-x-right-sumn_t1logplefty_t-vert-y_-tx-right" id="toc-logplefty_1ldotsy_t-vert-x-right-sumn_t1logplefty_t-vert-y_-tx-right" class="nav-link" data-scroll-target="#logplefty_1ldotsy_t-vert-x-right-sumn_t1logplefty_t-vert-y_-tx-right"><span class="math display">\[\log{P\left(y_{1},\ldots,y_{t} \vert x \right)} = \sum^{N}_{t=1}{\log{P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}}\]</span></a>
  <ul class="collapse">
  <li><a href="#log_softmax" id="toc-log_softmax" class="nav-link" data-scroll-target="#log_softmax"><code>log_softmax</code></a></li>
  </ul></li>
  <li><a href="#textlogsoftmaxx_i-logleftfracexpx_i-sum_j-expx_j-right" id="toc-textlogsoftmaxx_i-logleftfracexpx_i-sum_j-expx_j-right" class="nav-link" data-scroll-target="#textlogsoftmaxx_i-logleftfracexpx_i-sum_j-expx_j-right"><span class="math display">\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i)}{ \sum_j \exp(x_j)} \right)\]</span></a></li>
  </ul></li>
  <li><a href="#sampling-methods" id="toc-sampling-methods" class="nav-link" data-scroll-target="#sampling-methods">Sampling Methods</a>
  <ul>
  <li><a href="#plefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracexpz_ti-sumv_j1-expz_tj" id="toc-plefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracexpz_ti-sumv_j1-expz_tj" class="nav-link" data-scroll-target="#plefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracexpz_ti-sumv_j1-expz_tj"><span class="math display">\[P\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = \text{softmax} \left( z_{t,i} \right) = \frac{\exp(z_{t,i})}{ \sum^{|V|}_{j=1} \exp(z_{t,j})}\]</span></a></li>
  <li><a href="#lefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracfracexpz_tit-sumv_j1-fracexpz_tjt" id="toc-lefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracfracexpz_tit-sumv_j1-fracexpz_tjt" class="nav-link" data-scroll-target="#lefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracfracexpz_tit-sumv_j1-fracexpz_tjt"><span class="math display">\[\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = \text{softmax} \left( z_{t,i} \right) = \frac{\frac{\exp(z_{t,i})}{T}}{ \sum^{|V|}_{j=1} \frac{\exp(z_{t,j}}{T})}\]</span></a></li>
  </ul></li>
  <li><a href="#top-k-and-nucleus-sampling" id="toc-top-k-and-nucleus-sampling" class="nav-link" data-scroll-target="#top-k-and-nucleus-sampling">Top-k and Nucleus Sampling</a>
  <ul>
  <li><a href="#top-k-sampling" id="toc-top-k-sampling" class="nav-link" data-scroll-target="#top-k-sampling">Top-k Sampling</a></li>
  </ul></li>
  <li><a href="#which-decoding-method-is-best" id="toc-which-decoding-method-is-best" class="nav-link" data-scroll-target="#which-decoding-method-is-best">Which Decoding Method Is Best?</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 5</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 5 covers different methods for generating text with GPT-2.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 8, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/transformers-book-notes.html"><strong>Natural Language Processing with Transformers</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-challenge-with-generating-coherent-text">The Challenge with Generating Coherent Text</a></li>
<li><a href="#greedy-search-decoding">Greedy Search Decoding</a></li>
<li><a href="#beam-search-decoding">Beam Search Decoding</a></li>
<li><a href="#sampling-methods">Sampling Methods</a></li>
<li><a href="#top-k-and-nucleus-sampling">Top-k and Nucleus Sampling</a></li>
<li><a href="#which-decoding-method-is-best">Which Decoding Method Is Best?</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>transformers.__version__, datasets.__version__, accelerate.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('4.11.3', '1.16.1', '0.5.1')</code></pre>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>Transformer-based language models like GPT-2 and GPT-3 can generate text almost indistinguishable from text written by humans.</li>
<li>Such models acquire a broad set of skills and pattern recognition abilities by learning to predict the next word in the text of millions of web pages.</li>
<li>We can activate these skills with different kinds of input prompts.</li>
<li>Language models are exposed to sequences of tasks during pretraining that we can adapt during inference.</li>
</ul>
</section>
<section id="the-challenge-with-generating-coherent-text" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge-with-generating-coherent-text">The Challenge with Generating Coherent Text</h2>
<ul>
<li>Converting a model’s probabilistic output to text requires a decoding method.</li>
<li>The decoding process is iterative and involves significantly more computing than passing inputs once through the forward pass of a model.</li>
<li>The quality and diversity of the generated text depend on the choice of decoding method and associated hyperparameters.</li>
</ul>
<section id="gpt-2-pretraining-process" class="level3">
<h3 class="anchored" data-anchor-id="gpt-2-pretraining-process">GPT-2 Pretraining Process</h3>
<ul>
<li>GPT-2 is pretrained to estimate the probability <span class="math inline">\(P\left(y \vert x \right)\)</span> of a sequence of tokens <span class="math inline">\(y = y_{1},y_{2},\ldots,y_{t}\)</span> occurring in the text <span class="math inline">\(x = x_{1},x_{2},\ldots,x_{k}\)</span>, given some initial prompt or context sequence.</li>
<li>It is common to use the chain rule of probability to factorize it as a product of conditional probabilities.</li>
</ul>
</section>
<section id="plefty_1ldotsy_t-vert-x-right-prodn_t1plefty_t-vert-y_-tx-right" class="level3">
<h3 class="anchored" data-anchor-id="plefty_1ldotsy_t-vert-x-right-prodn_t1plefty_t-vert-y_-tx-right"><span class="math display">\[P\left(y_{1},\ldots,y_{t} \vert x \right) = \prod^{N}_{t=1}{P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}\]</span></h3>
<blockquote class="blockquote">
<ul>
<li>where <span class="math inline">\(y_{ \ &lt; \ t}\)</span> is the shorthand notation for the sequence <span class="math inline">\(y_{1},\ldots,y_{t-1}\)</span></li>
</ul>
</blockquote>
<ul>
<li>We can adapt this token prediction task to generate sequences of arbitrary length by feeding the model a prompt.</li>
<li>We then iteratively add the next predicted token to the prompt and feed the new prompt to the model.</li>
<li>Some call this type of text generation conditional text generation since the output sequence depends on the choice of input prompt.</li>
</ul>
</section>
<section id="decoding" class="level3">
<h3 class="anchored" data-anchor-id="decoding">Decoding</h3>
<ul>
<li>A decoding method determines which token to select at each timestep.</li>
<li>The language model produces a logit <span class="math inline">\(z_{t,i}\)</span> per token in the vocabulary at each time step.</li>
<li>We can get the probability distribution over the next possible token <span class="math inline">\(w_{i}\)</span> by taking the softmax.</li>
</ul>
</section>
<section id="plefty_t-w_i-vert-y_-tx-right-softmax-left-z_ti-right" class="level3">
<h3 class="anchored" data-anchor-id="plefty_t-w_i-vert-y_-tx-right-softmax-left-z_ti-right"><span class="math display">\[P\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = softmax \left( z_{t,i} \right)\]</span></h3>
<ul>
<li>Most decoder methods search for the most likely overall sequence by picking a <span class="math inline">\(\hat{y}\)</span> such that</li>
</ul>
</section>
<section id="haty-undersetyargmax-plefty-vert-x-right" class="level3">
<h3 class="anchored" data-anchor-id="haty-undersetyargmax-plefty-vert-x-right"><span class="math display">\[\hat{y} = \underset{y}{argmax} P\left(y \vert x \right)\]</span></h3>
<ul>
<li>We use approximations for <span class="math inline">\(\hat{y}\)</span> instead of finding it directly.</li>
</ul>
</section>
</section>
<section id="greedy-search-decoding" class="level2">
<h2 class="anchored" data-anchor-id="greedy-search-decoding">Greedy Search Decoding</h2>
<ul>
<li>The simplest decoding method is to greedily select the token with the highest probability at each timestep.</li>
</ul>
<section id="haty_t-undersetyargmax-plefty_t-vert-y_-tx-right" class="level3">
<h3 class="anchored" data-anchor-id="haty_t-undersetyargmax-plefty_t-vert-y_-tx-right"><span class="math display">\[\hat{y}_{t} = \underset{y}{argmax} {P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}\]</span></h3>
<ul>
<li>Greedy search decoding tends to produce repetitive output sequences.</li>
<li>Greedy search can miss sequences whose overall probability is higher when low probability words precede high-probability words.</li>
<li>Greedy search is not suitable for text generation tasks that require diversity.</li>
<li>Greedy search is better suited for producing short sequences like arithmetic that require deterministic and factually correct output.</li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Load the 1.5-billion-parameter version of GPT-2 with a language modeling head</strong></p>
<p><strong>Note:</strong> The model takes up around 8GB of VRAM.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt2-xl"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Prepare Input</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"Transformers are the"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tokenizer.convert_ids_to_tokens(input_ids[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['Transform', 'ers', 'Ġare', 'Ġthe']</code></pre>
<hr>
<p><strong>Perform Greedy Search Decoding</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> []</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>choices_per_step <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        iteration <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        iteration[<span class="st">"Input"</span>] <span class="op">=</span> tokenizer.decode(input_ids[<span class="dv">0</span>])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(input_ids<span class="op">=</span>input_ids)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select logits of the first batch and the last token and apply softmax</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        next_token_logits <span class="op">=</span> output.logits[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        next_token_probs <span class="op">=</span> torch.softmax(next_token_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        sorted_ids <span class="op">=</span> torch.argsort(next_token_probs, dim<span class="op">=-</span><span class="dv">1</span>, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store tokens with highest probabilities</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> choice_idx <span class="kw">in</span> <span class="bu">range</span>(choices_per_step):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            token_id <span class="op">=</span> sorted_ids[choice_idx]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>            token_prob <span class="op">=</span> next_token_probs[token_id].cpu().numpy()</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>            token_choice <span class="op">=</span> (</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(token_id)<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> token_prob<span class="sc">:.2f}</span><span class="ss">%)"</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            iteration[<span class="ss">f"Choice </span><span class="sc">{</span>choice_idx<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> token_choice</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append predicted next token to input</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> torch.cat([input_ids, sorted_ids[<span class="va">None</span>, <span class="dv">0</span>, <span class="va">None</span>]], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        iterations.append(iteration)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(iterations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Input
</th>
<th>
Choice 1
</th>
<th>
Choice 2
</th>
<th>
Choice 3
</th>
<th>
Choice 4
</th>
<th>
Choice 5
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
Transformers are the
</td>
<td>
most (8.53%)
</td>
<td>
only (4.96%)
</td>
<td>
best (4.65%)
</td>
<td>
Transformers (4.37%)
</td>
<td>
ultimate (2.16%)
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Transformers are the most
</td>
<td>
popular (16.78%)
</td>
<td>
powerful (5.37%)
</td>
<td>
common (4.96%)
</td>
<td>
famous (3.72%)
</td>
<td>
successful (3.20%)
</td>
</tr>
<tr>
<th>
2
</th>
<td>
Transformers are the most popular
</td>
<td>
toy (10.63%)
</td>
<td>
toys (7.23%)
</td>
<td>
Transformers (6.60%)
</td>
<td>
of (5.46%)
</td>
<td>
and (3.76%)
</td>
</tr>
<tr>
<th>
3
</th>
<td>
Transformers are the most popular toy
</td>
<td>
line (34.38%)
</td>
<td>
in (18.20%)
</td>
<td>
of (11.71%)
</td>
<td>
brand (6.10%)
</td>
<td>
line (2.69%)
</td>
</tr>
<tr>
<th>
4
</th>
<td>
Transformers are the most popular toy line
</td>
<td>
in (46.28%)
</td>
<td>
of (15.09%)
</td>
<td>
, (4.94%)
</td>
<td>
on (4.40%)
</td>
<td>
ever (2.72%)
</td>
</tr>
<tr>
<th>
5
</th>
<td>
Transformers are the most popular toy line in
</td>
<td>
the (65.99%)
</td>
<td>
history (12.42%)
</td>
<td>
America (6.91%)
</td>
<td>
Japan (2.44%)
</td>
<td>
North (1.40%)
</td>
</tr>
<tr>
<th>
6
</th>
<td>
Transformers are the most popular toy line in the
</td>
<td>
world (69.26%)
</td>
<td>
United (4.55%)
</td>
<td>
history (4.29%)
</td>
<td>
US (4.23%)
</td>
<td>
U (2.30%)
</td>
</tr>
<tr>
<th>
7
</th>
<td>
Transformers are the most popular toy line in the world
</td>
<td>
, (39.73%)
</td>
<td>
. (30.64%)
</td>
<td>
and (9.87%)
</td>
<td>
with (2.32%)
</td>
<td>
today (1.74%)
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The generated sentence indicates that GPT-2 internalized some knowledge about the Transformers media franchise during pretraining.</p>
<hr>
<section id="generationmixin.generate" class="level4">
<h4 class="anchored" data-anchor-id="generationmixin.generate"><code>GenerationMixin.generate</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate">Documentation</a></li>
<li>Generates sequences of token ids for models with a language modeling head.</li>
<li><strong>greedy decoding:</strong>
<ul>
<li><code>num_beams=1</code> and <code>do_sample=False</code></li>
</ul></li>
<li><strong>multinomial decoding:</strong>
<ul>
<li><code>num_beams=1</code> and <code>do_sample=True</code></li>
</ul></li>
<li><strong>beam-search decoding:</strong>
<ul>
<li><code>num_beams&gt;1</code> and <code>do_sample=False</code></li>
</ul></li>
<li><strong>beam-search multinomial sampling:</strong>
<ul>
<li><code>num_beams&gt;1</code> and <code>do_sample=True</code></li>
</ul></li>
<li><strong>diverse beam-search decoding:</strong>
<ul>
<li><code>num_beams&gt;1</code> and `num_beam_groups&gt;1</li>
</ul></li>
<li><strong>constrained beam-search decoding:</strong>
<ul>
<li><code>constraints!=None</code> or <code>force_words_ids!=None</code></li>
</ul></li>
</ul>
<hr>
<p><strong>Perform Greedy Search Decoding with the <code>generate()</code> function</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model.generate(input_ids, max_new_tokens<span class="op">=</span>n_steps, do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Transformers are the most popular toy line in the world,</code></pre>
<hr>
<p><strong>Try to perform arithmetic with Greedy Search Decoding</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"""5 + 8 =&gt; 13 </span><span class="ch">\n</span><span class="st"> 7 + 2 =&gt; 9 </span><span class="ch">\n</span><span class="st"> 1 + 0 =&gt;"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>output_greedy <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                               do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_greedy[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    5 + 8 =&gt; 13 
     7 + 2 =&gt; 9 
     1 + 0 =&gt; 1 </code></pre>
<hr>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"""5 + 8 =&gt; 13 </span><span class="ch">\n</span><span class="st"> 7 + 2 =&gt; 9 </span><span class="ch">\n</span><span class="st"> 2 * 10 =&gt;"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>output_greedy <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                               do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_greedy[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    5 + 8 =&gt; 13 
     7 + 2 =&gt; 9 
     2 * 10 =&gt; 20 </code></pre>
<hr>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"""5 + 8 =&gt; 13 </span><span class="ch">\n</span><span class="st"> 7 + 2 =&gt; 9 </span><span class="ch">\n</span><span class="st"> 2 * 13 =&gt;"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>output_greedy <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                               do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_greedy[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    5 + 8 =&gt; 13 
     7 + 2 =&gt; 9 
     2 * 13 =&gt; 13 </code></pre>
<p><strong>Note:</strong> Not perfect.</p>
<hr>
<p><strong>Try to replicate the OpenAI Unicorn story with Greedy Search Decoding</strong></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"""In a shocking finding, scientist discovered </span><span class="ch">\</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="st">a herd of unicorns living in a remote, previously unexplored </span><span class="ch">\</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="st">valley, in the Andes Mountains. Even more surprising to the </span><span class="ch">\</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="st">researchers was the fact that the unicorns spoke perfect English.</span><span class="ch">\n\n</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>output_greedy <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                               do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_greedy[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.


​    
​    The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.


​    
​    The researchers were surprised to find that the unicorns were able</code></pre>
<p><strong>Note:</strong> The results demonstrate the repetitive output that is characteristic of greedy search decoding.</p>
</section>
</section>
</section>
<section id="beam-search-decoding" class="level2">
<h2 class="anchored" data-anchor-id="beam-search-decoding">Beam Search Decoding</h2>
<ul>
<li>Beam search keeps track of the <span class="math inline">\(top-b\)</span> most probable next tokens, where <span class="math inline">\(b\)</span> is the number of beams or partial hypotheses.</li>
<li>There is a tradeoff between output quality and speed when choosing the number of beams.</li>
<li>We choose the next set of beams by considering all possible next-token extensions of the existing ones and selecting the <span class="math inline">\(b\)</span> most likely extensions.</li>
<li>We repeat this process until we reach the maximum length or an EOS token.</li>
<li>We select the most likely sequence by ranking the <span class="math inline">\(b\)</span> beams according to their log probabilities.</li>
</ul>
<section id="logplefty_1ldotsy_t-vert-x-right-sumn_t1logplefty_t-vert-y_-tx-right" class="level3">
<h3 class="anchored" data-anchor-id="logplefty_1ldotsy_t-vert-x-right-sumn_t1logplefty_t-vert-y_-tx-right"><span class="math display">\[\log{P\left(y_{1},\ldots,y_{t} \vert x \right)} = \sum^{N}_{t=1}{\log{P\left(y_{t} \vert y_{ \ &lt; \ t},x \right)}}\]</span></h3>
<hr>
<p><strong>Note:</strong> We use the log probabilities to avoid numerical instability due to floating-point precision.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.5</span> <span class="op">**</span> <span class="dv">1024</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    5.562684646268003e-309</code></pre>
<hr>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>([np.log(<span class="fl">0.5</span>)] <span class="op">*</span> <span class="dv">1024</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    -709.7827128933695</code></pre>
<hr>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="log_softmax" class="level4">
<h4 class="anchored" data-anchor-id="log_softmax"><code>log_softmax</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html">Documentation</a></li>
<li>Mathematically equivalent to <code>log(softmax(x))</code></li>
</ul>
</section>
</section>
<section id="textlogsoftmaxx_i-logleftfracexpx_i-sum_j-expx_j-right" class="level3">
<h3 class="anchored" data-anchor-id="textlogsoftmaxx_i-logleftfracexpx_i-sum_j-expx_j-right"><span class="math display">\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i)}{ \sum_j \exp(x_j)} \right)\]</span></h3>
<hr>
<p><strong>Define a function to calculate the log probability of a single token</strong></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_probs_from_logits(logits, labels):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize the logits with softmax before taking the log</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    logp_label <span class="op">=</span> torch.gather(logp, <span class="dv">2</span>, labels.unsqueeze(<span class="dv">2</span>)).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logp_label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to calculate the log probabilities of a sequence</strong></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sequence_logprob(model, labels, input_len<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(labels)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        log_probs <span class="op">=</span> log_probs_from_logits(</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We dont need the last logit since we don't have a ground truth token for it</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We don't have a logit for the first token</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>            output.logits[:, :<span class="op">-</span><span class="dv">1</span>, :], labels[:, <span class="dv">1</span>:])</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sum the log probabilities for each token</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ignore the log probabilities of the input sequence</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        seq_log_prob <span class="op">=</span> torch.<span class="bu">sum</span>(log_probs[:, input_len:])</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seq_log_prob.cpu().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare the log probabilities texts generated by greedy and beam search</strong></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> sequence_logprob(model, output_greedy, input_len<span class="op">=</span><span class="bu">len</span>(input_ids[<span class="dv">0</span>]))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_greedy[<span class="dv">0</span>]))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">log-prob: </span><span class="sc">{</span>logp<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.


​    
​    The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.


​    
​    The researchers were surprised to find that the unicorns were able
​    
    log-prob: -87.43</code></pre>
<hr>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>output_beam <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, num_beams<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                             do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> sequence_logprob(model, output_beam, input_len<span class="op">=</span><span class="bu">len</span>(input_ids[<span class="dv">0</span>]))</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_beam[<span class="dv">0</span>]))</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">log-prob: </span><span class="sc">{</span>logp<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.


​    
​    The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English
​    
    log-prob: -55.23</code></pre>
<p><strong>Note:</strong> * A higher log probability is better. * Beam search still suffers from repetitive text. * We can impose an n-gram penalty that tracks which n-grams are already present in the output.</p>
<hr>
<p><strong>Test beam search with an n-gram penalty</strong></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>output_beam <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, num_beams<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>                             do_sample<span class="op">=</span><span class="va">False</span>, no_repeat_ngram_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>logp <span class="op">=</span> sequence_logprob(model, output_beam, input_len<span class="op">=</span><span class="bu">len</span>(input_ids[<span class="dv">0</span>]))</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_beam[<span class="dv">0</span>]))</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">log-prob: </span><span class="sc">{</span>logp<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.
​    
    According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were
    
    log-prob: -93.12</code></pre>
<p><strong>Note:</strong> * There are no repetitions, and the text remains coherent despite a lower log probability. * We can use beam search with an n-gram penalty to find a tradeoff between focusing on high-probability tokens while reducing repetitions.</p>
<hr>
</section>
</section>
<section id="sampling-methods" class="level2">
<h2 class="anchored" data-anchor-id="sampling-methods">Sampling Methods</h2>
<ul>
<li>The simplest sampling method is to randomly sample from the probability distribution of the model’s outputs over the entire vocabulary at each timestep.</li>
</ul>
<section id="plefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracexpz_ti-sumv_j1-expz_tj" class="level3">
<h3 class="anchored" data-anchor-id="plefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracexpz_ti-sumv_j1-expz_tj"><span class="math display">\[P\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = \text{softmax} \left( z_{t,i} \right) = \frac{\exp(z_{t,i})}{ \sum^{|V|}_{j=1} \exp(z_{t,j})}\]</span></h3>
<ul>
<li>where <span class="math inline">\(\vert V \vert\)</span> denotes the cardinality of the vocabulary</li>
<li>We can control the diversity of the output by adding a temperature parameter <span class="math inline">\(T\)</span> that rescales the logits before taking the softmax.</li>
</ul>
</section>
<section id="lefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracfracexpz_tit-sumv_j1-fracexpz_tjt" class="level3">
<h3 class="anchored" data-anchor-id="lefty_t-w_i-vert-y_-tx-right-textsoftmax-left-z_ti-right-fracfracexpz_tit-sumv_j1-fracexpz_tjt"><span class="math display">\[\left(y_{t} = w_{i} \vert y_{ \ &lt; \ t},x \right) = \text{softmax} \left( z_{t,i} \right) = \frac{\frac{\exp(z_{t,i})}{T}}{ \sum^{|V|}_{j=1} \frac{\exp(z_{t,j}}{T})}\]</span></h3>
<ul>
<li>We can tune the temperature parameter to control the shape of the probability distribution.</li>
<li>A <span class="math inline">\(T\)</span> value much less than <span class="math inline">\(1\)</span> suppresses the rare tokens.</li>
<li>A <span class="math inline">\(T\)</span> value much greater than <span class="math inline">\(1\)</span> causes each token to become equally likely.</li>
</ul>
<hr>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a softmax function with a temperature parameter</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(logits, T<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    e_x <span class="op">=</span> np.exp(logits <span class="op">/</span> T)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> e_x <span class="op">/</span> e_x.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot a distribution of randomly generated token probabilities for three selected temperatures</strong></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.exp(np.random.random(<span class="dv">1000</span>))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>sorted_logits <span class="op">=</span> np.sort(logits)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">1000</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> T <span class="kw">in</span> [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>]:</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    plt.step(x, softmax(sorted_logits, T), label<span class="op">=</span><span class="ss">f"T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Sorted token probabilities"</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_51_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Sample generated text with a high temperature</strong></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>output_temp <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, do_sample<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>                             temperature<span class="op">=</span><span class="fl">2.0</span>, top_k<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_temp[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    While the station aren protagonist receive Pengala nostalgiates tidbitRegarding Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop});b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter</code></pre>
<p><strong>Note:</strong> Sampling with a high temperature produces gibberish.</p>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Sample generated text with a low temperature</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>output_temp <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, do_sample<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>                             temperature<span class="op">=</span><span class="fl">0.5</span>, top_k<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_temp[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.


​    
​    The unicorns were living in a remote valley in the Andes mountains
​    
    'When we first heard the noise of the animals, we thought it was a lion or a tiger,' said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.


​    
​    'But when</code></pre>
<p><strong>Note:</strong> Sampling with a low temperature produces a much more coherent output.</p>
<hr>
</section>
</section>
<section id="top-k-and-nucleus-sampling" class="level2">
<h2 class="anchored" data-anchor-id="top-k-and-nucleus-sampling">Top-k and Nucleus Sampling</h2>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>input_txt <span class="op">=</span> <span class="st">"""In a shocking finding, scientist discovered </span><span class="ch">\</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="st">a herd of unicorns living in a remote, previously unexplored </span><span class="ch">\</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="st">valley, in the Andes Mountains. Even more surprising to the </span><span class="ch">\</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="st">researchers was the fact that the unicorns spoke perfect English.</span><span class="ch">\n\n</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_txt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"input_ids"</span>].to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Calculate the probability distribution of the model’s outputs at <span class="math inline">\(T=1\)</span></strong></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(input_ids<span class="op">=</span>input_ids)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    next_token_logits <span class="op">=</span> output.logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> F.softmax(next_token_logits, dim<span class="op">=-</span><span class="dv">1</span>).detach().cpu().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the cumulative probability distribution of the model’s outputs at <span class="math inline">\(T=1\)</span></strong></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="fl">3.5</span>))</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].hist(probs[<span class="dv">0</span>], bins<span class="op">=</span>np.logspace(<span class="op">-</span><span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">100</span>), color<span class="op">=</span><span class="st">"C0"</span>, edgecolor<span class="op">=</span><span class="st">"C0"</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xscale(<span class="st">"log"</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yscale(<span class="st">"log"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Probability distribution"</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Probability"</span>)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Count"</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(np.cumsum(np.sort(probs[<span class="dv">0</span>])[::<span class="op">-</span><span class="dv">1</span>]), color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlim([<span class="dv">0</span>, <span class="dv">10000</span>])</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylim([<span class="fl">0.75</span>, <span class="fl">1.01</span>])</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Cumulative probability"</span>)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Token (descending probability)"</span>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].minorticks_on()</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>top_k_label <span class="op">=</span> <span class="st">'top-k threshold (k=2000)'</span></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>top_p_label <span class="op">=</span> <span class="st">'nucleus threshold (p=0.95)'</span></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].vlines(x<span class="op">=</span><span class="dv">2000</span>, ymin<span class="op">=</span><span class="dv">0</span>, ymax<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'C0'</span>, label<span class="op">=</span>top_k_label)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hlines(y<span class="op">=</span><span class="fl">0.95</span>, xmin<span class="op">=</span><span class="dv">0</span>, xmax<span class="op">=</span><span class="dv">10000</span>, color<span class="op">=</span><span class="st">'C1'</span>, label<span class="op">=</span>top_p_label, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_69_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The histogram has a peak around 10^-8 and a second smaller peak around 10^-4, followed by a sharp drop. * The probability of picking the token with the highest likelihood is 1 in 10. * There are 50,257 tokens in GPT-2’s vocabulary. * The curved line in the Cumulative probability chart represents the probability of picking any of the preceding tokens. * There is a 1 in 100 chance of not picking any of the tokens that are not n the top 2000. * There is a significant chance of picking an unlikely token when sampling hundreds of times. * Picking such tokens can negatively impact the quality of the generated text.</p>
<hr>
<section id="top-k-sampling" class="level3">
<h3 class="anchored" data-anchor-id="top-k-sampling">Top-k Sampling</h3>
<ul>
<li>The idea behind top-k sampling is to avoid low-probability choices by only choosing from the k tokens with the highest probability.</li>
<li>We can find a good value for k by looking at some text quality metrics.</li>
</ul>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Generate text using the 50 tokens with the highest probability</strong></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>output_topk <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, do_sample<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>                             top_k<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_topk[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The wild unicorns roam the Andes Mountains in the region of Cajamarca, on the border with Argentina (Picture: Alamy/Ecole Nationale Supérieure d'Histoire Naturelle)
​    
    The researchers came across about 50 of the animals in the valley. They had lived in such a remote and isolated area at that location for nearly a thousand years that


### Nucleus (top-p) Sampling
* The idea behind nucleus sampling is to cut off the long tail of the distribution after reaching a certain probability mass in the selection.
* We order all tokens in descending order by probability and add one token after another from the top of the list until the sum of the probabilities of the selected tokens reaches the target mass.</code></pre>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Generate text using top-p sampling</strong></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>output_topp <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, do_sample<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>                             top_p<span class="op">=</span><span class="fl">0.90</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_topp[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.


​    
​    According to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.


​    
​    Since their migration, the animals have been adapting to</code></pre>
<p><strong>Note:</strong> Top-p sampling also produces a coherent story.</p>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Generate text using top-k and top-p sampling</strong></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>output_topp <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span>max_length, do_sample<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                             top_k<span class="op">=</span><span class="dv">50</span>, top_p<span class="op">=</span><span class="fl">0.90</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(output_topp[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.


​    
​    The scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.


​    
​    According to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended.


​    
​    Since their migration, the animals have been adapting to</code></pre>
<hr>
</section>
</section>
<section id="which-decoding-method-is-best" class="level2">
<h2 class="anchored" data-anchor-id="which-decoding-method-is-best">Which Decoding Method Is Best?</h2>
<ul>
<li>The best approach depends on the nature of the task.</li>
<li>Lower the temperature or use deterministic methods to perform a precise task like arithmetic or providing an answer to a specific question.</li>
<li>Switch to sampling methods and increase the temperature when you want the model to generate longer text and be more creative.</li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-4/">Notes on Transformers Book Ch. 4</a></p>
<p><strong>Next:</strong> <a href="../chapter-6/">Notes on Transformers Book Ch. 6</a></p>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2023, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>