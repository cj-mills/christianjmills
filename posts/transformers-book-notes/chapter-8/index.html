<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-14">
<meta name="description" content="Chapter 8 covers different methods to make transformer models more efficient in production.">

<title>Christian Mills - Notes on Transformers Book Ch. 8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on Transformers Book Ch. 8">
<meta property="og:description" content="Chapter 8 covers different methods to make transformer models more efficient in production.">
<meta property="og:image" content="christianjmills.com/posts/transformers-book-notes/chapter-8/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Notes on Transformers Book Ch. 8">
<meta name="twitter:description" content="Chapter 8 covers different methods to make transformer models more efficient in production.">
<meta name="twitter:image" content="christianjmills.com/posts/transformers-book-notes/chapter-8/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html" rel="" target="">
 <span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../services.html" rel="" target="">
 <span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com" rel="" target=""><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#making-transformers-efficient-in-production" id="toc-making-transformers-efficient-in-production" class="nav-link active" data-scroll-target="#making-transformers-efficient-in-production">Making Transformers Efficient in Production</a></li>
  <li><a href="#project-optimize-an-intent-detection-model" id="toc-project-optimize-an-intent-detection-model" class="nav-link" data-scroll-target="#project-optimize-an-intent-detection-model">Project: Optimize an Intent Detection Model</a>
  <ul>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The Model</a></li>
  <li><a href="#clinc150-dataset" id="toc-clinc150-dataset" class="nav-link" data-scroll-target="#clinc150-dataset">CLINC150 Dataset</a></li>
  </ul></li>
  <li><a href="#creating-a-performance-benchmark" id="toc-creating-a-performance-benchmark" class="nav-link" data-scroll-target="#creating-a-performance-benchmark">Creating a Performance Benchmark</a>
  <ul>
  <li><a href="#time.perf_counter" id="toc-time.perf_counter" class="nav-link" data-scroll-target="#time.perf_counter"><code>time.perf_counter</code></a></li>
  </ul></li>
  <li><a href="#making-models-smaller-via-knowledge-distillation" id="toc-making-models-smaller-via-knowledge-distillation" class="nav-link" data-scroll-target="#making-models-smaller-via-knowledge-distillation">Making Models Smaller via Knowledge Distillation</a>
  <ul>
  <li><a href="#knowledge-distillation-for-fine-tuning" id="toc-knowledge-distillation-for-fine-tuning" class="nav-link" data-scroll-target="#knowledge-distillation-for-fine-tuning">Knowledge Distillation for Fine-Tuning</a></li>
  <li><a href="#fracexp-left-z_ix-rightsum_jexp-left-z_ix-right" id="toc-fracexp-left-z_ix-rightsum_jexp-left-z_ix-right" class="nav-link" data-scroll-target="#fracexp-left-z_ix-rightsum_jexp-left-z_ix-right"><span class="math display">\[\frac{exp \left( z_{i}(x) \right)}{\sum_{j}{exp \left( z_{i}(x) \right)}}\]</span></a></li>
  <li><a href="#p_ix-fracexp-left-frac-z_ix-t-rightsum_jexp-left-frac-z_ix-t-right" id="toc-p_ix-fracexp-left-frac-z_ix-t-rightsum_jexp-left-frac-z_ix-t-right" class="nav-link" data-scroll-target="#p_ix-fracexp-left-frac-z_ix-t-rightsum_jexp-left-frac-z_ix-t-right"><span class="math display">\[p_{i}(x) = \frac{exp \left( \frac{ z_{i}(x) }{T} \right)}{\sum_{j}{exp \left( \frac{ z_{i}(x) }{T} \right)}}\]</span></a></li>
  <li><a href="#d_klpq-sum_ip_ixlogfracp_ixq_ix" id="toc-d_klpq-sum_ip_ixlogfracp_ixq_ix" class="nav-link" data-scroll-target="#d_klpq-sum_ip_ixlogfracp_ixq_ix"><span class="math display">\[D_{KL}(p,q) = \sum_{i}{p_{i}(x)\log{\frac{p_{i}(x)}{q_{i}(x)}}}\]</span></a></li>
  <li><a href="#l_kd-t2d_kl" id="toc-l_kd-t2d_kl" class="nav-link" data-scroll-target="#l_kd-t2d_kl"><span class="math display">\[L_{KD} = T^{2}D_{KL}\]</span></a></li>
  <li><a href="#l_student-alpha-l_ce-left-1---alpha-rightl_kd" id="toc-l_student-alpha-l_ce-left-1---alpha-rightl_kd" class="nav-link" data-scroll-target="#l_student-alpha-l_ce-left-1---alpha-rightl_kd"><span class="math display">\[L_{student} = \alpha L_{CE} \ + \left( 1 - \alpha \right)L_{KD}\]</span></a></li>
  <li><a href="#knowledge-distillation-for-pretraining" id="toc-knowledge-distillation-for-pretraining" class="nav-link" data-scroll-target="#knowledge-distillation-for-pretraining">Knowledge Distillation for Pretraining</a></li>
  <li><a href="#l_distilbert-alpha-l_mlm-beta-l_kd-y-loss_cos" id="toc-l_distilbert-alpha-l_mlm-beta-l_kd-y-loss_cos" class="nav-link" data-scroll-target="#l_distilbert-alpha-l_mlm-beta-l_kd-y-loss_cos"><span class="math display">\[L_{DistilBERT} = \alpha L_{mlm} \ + \ \beta L_{KD} \ + \ y \ Loss_{cos}\]</span></a></li>
  <li><a href="#creating-a-knowledge-distillation-trainer" id="toc-creating-a-knowledge-distillation-trainer" class="nav-link" data-scroll-target="#creating-a-knowledge-distillation-trainer">Creating a Knowledge Distillation Trainer</a>
  <ul class="collapse">
  <li><a href="#additions-to-the-base-trainer-class" id="toc-additions-to-the-base-trainer-class" class="nav-link" data-scroll-target="#additions-to-the-base-trainer-class">Additions to the base Trainer Class:</a></li>
  <li><a href="#nn.kldivloss" id="toc-nn.kldivloss" class="nav-link" data-scroll-target="#nn.kldivloss"><code>nn.KLDivLoss</code></a></li>
  </ul></li>
  <li><a href="#ly_textpred-y_texttrue-y_texttrue-cdot-log-fracy_texttruey_textpred-y_texttrue-cdot-log-y_texttrue---log-y_textpred" id="toc-ly_textpred-y_texttrue-y_texttrue-cdot-log-fracy_texttruey_textpred-y_texttrue-cdot-log-y_texttrue---log-y_textpred" class="nav-link" data-scroll-target="#ly_textpred-y_texttrue-y_texttrue-cdot-log-fracy_texttruey_textpred-y_texttrue-cdot-log-y_texttrue---log-y_textpred"><span class="math display">\[L(y_{\text{pred}},\ y_{\text{true}}) = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}} = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})\]</span></a></li>
  <li><a href="#choosing-a-good-student-initialization" id="toc-choosing-a-good-student-initialization" class="nav-link" data-scroll-target="#choosing-a-good-student-initialization">Choosing a Good Student Initialization</a></li>
  <li><a href="#finding-good-hyperparameters-with-optuna" id="toc-finding-good-hyperparameters-with-optuna" class="nav-link" data-scroll-target="#finding-good-hyperparameters-with-optuna">Finding Good Hyperparameters with Optuna</a>
  <ul class="collapse">
  <li><a href="#optuna.study.study" id="toc-optuna.study.study" class="nav-link" data-scroll-target="#optuna.study.study"><code>optuna.study.Study</code></a></li>
  <li><a href="#optuna.create_study" id="toc-optuna.create_study" class="nav-link" data-scroll-target="#optuna.create_study"><code>optuna.create_study()</code></a></li>
  <li><a href="#trainer.hyperparameter_search" id="toc-trainer.hyperparameter_search" class="nav-link" data-scroll-target="#trainer.hyperparameter_search"><code>Trainer.hyperparameter_search</code></a></li>
  <li><a href="#bestrun" id="toc-bestrun" class="nav-link" data-scroll-target="#bestrun"><code>BestRun</code></a></li>
  </ul></li>
  <li><a href="#benchmarking-our-distilled-model" id="toc-benchmarking-our-distilled-model" class="nav-link" data-scroll-target="#benchmarking-our-distilled-model">Benchmarking Our Distilled Model</a></li>
  </ul></li>
  <li><a href="#a-primer-on-floating-point-and-fixed-point-numbers" id="toc-a-primer-on-floating-point-and-fixed-point-numbers" class="nav-link" data-scroll-target="#a-primer-on-floating-point-and-fixed-point-numbers">A Primer on Floating-Point and Fixed-Point Numbers</a></li>
  <li><a href="#making-models-faster-with-quantization" id="toc-making-models-faster-with-quantization" class="nav-link" data-scroll-target="#making-models-faster-with-quantization">Making Models Faster with Quantization</a>
  <ul>
  <li><a href="#f-left-fracf_max---f_minq_max---q_min-rightq-z-sq-z" id="toc-f-left-fracf_max---f_minq_max---q_min-rightq-z-sq-z" class="nav-link" data-scroll-target="#f-left-fracf_max---f_minq_max---q_min-rightq-z-sq-z"><span class="math display">\[f = \left( \frac{f_{max} - f_{min}}{q_{max} - q_{min}} \right)(q-Z) = S(q-Z)\]</span></a>
  <ul class="collapse">
  <li><a href="#quantize_per_tensor" id="toc-quantize_per_tensor" class="nav-link" data-scroll-target="#quantize_per_tensor"><code>quantize_per_tensor</code></a></li>
  <li><a href="#qfunctional" id="toc-qfunctional" class="nav-link" data-scroll-target="#qfunctional"><code>QFunctional</code></a></li>
  </ul></li>
  <li><a href="#approaches-to-quantization" id="toc-approaches-to-quantization" class="nav-link" data-scroll-target="#approaches-to-quantization">Approaches to Quantization</a>
  <ul class="collapse">
  <li><a href="#dynamic-quantization" id="toc-dynamic-quantization" class="nav-link" data-scroll-target="#dynamic-quantization">Dynamic Quantization</a></li>
  <li><a href="#static-quantization" id="toc-static-quantization" class="nav-link" data-scroll-target="#static-quantization">Static Quantization</a></li>
  <li><a href="#quantization-aware-training" id="toc-quantization-aware-training" class="nav-link" data-scroll-target="#quantization-aware-training">Quantization-aware training</a></li>
  <li><a href="#what-to-choose" id="toc-what-to-choose" class="nav-link" data-scroll-target="#what-to-choose">What to choose</a></li>
  <li><a href="#quantize_dynamic" id="toc-quantize_dynamic" class="nav-link" data-scroll-target="#quantize_dynamic"><code>quantize_dynamic</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#optimizing-inference-with-onnx-and-the-onnx-runtime" id="toc-optimizing-inference-with-onnx-and-the-onnx-runtime" class="nav-link" data-scroll-target="#optimizing-inference-with-onnx-and-the-onnx-runtime">Optimizing Inference with ONNX and the ONNX Runtime</a>
  <ul>
  <li><a href="#other-accelerators" id="toc-other-accelerators" class="nav-link" data-scroll-target="#other-accelerators">Other Accelerators</a></li>
  <li><a href="#convert-model-to-onnx-format" id="toc-convert-model-to-onnx-format" class="nav-link" data-scroll-target="#convert-model-to-onnx-format">Convert model to ONNX format</a>
  <ul class="collapse">
  <li><a href="#openmp" id="toc-openmp" class="nav-link" data-scroll-target="#openmp">OpenMP</a></li>
  <li><a href="#transformers.convert_graph_to_onnx.convert" id="toc-transformers.convert_graph_to_onnx.convert" class="nav-link" data-scroll-target="#transformers.convert_graph_to_onnx.convert"><code>transformers.convert_graph_to_onnx.convert()</code></a></li>
  </ul></li>
  <li><a href="#create-custom-pipeline" id="toc-create-custom-pipeline" class="nav-link" data-scroll-target="#create-custom-pipeline">Create Custom Pipeline</a></li>
  </ul></li>
  <li><a href="#making-models-sparser-with-weight-pruning" id="toc-making-models-sparser-with-weight-pruning" class="nav-link" data-scroll-target="#making-models-sparser-with-weight-pruning">Making Models Sparser with Weight Pruning</a>
  <ul>
  <li><a href="#sparsity-in-deep-neural-networks" id="toc-sparsity-in-deep-neural-networks" class="nav-link" data-scroll-target="#sparsity-in-deep-neural-networks">Sparsity in Deep Neural Networks</a></li>
  <li><a href="#weight-pruning-methods" id="toc-weight-pruning-methods" class="nav-link" data-scroll-target="#weight-pruning-methods">Weight Pruning Methods</a></li>
  <li><a href="#top_ks_ij-1-text-if-s_ij-text-in-top-k-percent-else-0" id="toc-top_ks_ij-1-text-if-s_ij-text-in-top-k-percent-else-0" class="nav-link" data-scroll-target="#top_ks_ij-1-text-if-s_ij-text-in-top-k-percent-else-0"><span class="math display">\[Top_{k}(S)_{ij} = 1 \text{ if } S_{ij} \text{ in top k percent else } 0\]</span></a></li>
  <li><a href="#a_i-sum_kw_ikm_ikx_k" id="toc-a_i-sum_kw_ikm_ikx_k" class="nav-link" data-scroll-target="#a_i-sum_kw_ikm_ikx_k"><span class="math display">\[a_{i} = \sum_{k}{W_{ik}M_{ik}x_{k}}\]</span></a>
  <ul class="collapse">
  <li><a href="#questions-to-consider" id="toc-questions-to-consider" class="nav-link" data-scroll-target="#questions-to-consider">Questions to consider</a></li>
  <li><a href="#magnitude-pruning" id="toc-magnitude-pruning" class="nav-link" data-scroll-target="#magnitude-pruning">Magnitude pruning</a></li>
  </ul></li>
  <li><a href="#s_t-s_f-left-s_i---s_f-right-left-1---fract---t_0ndelta-t-right3-for-t-in-left-t_0t_0-delta-t-ldots-t_0-ndelta-t-right" id="toc-s_t-s_f-left-s_i---s_f-right-left-1---fract---t_0ndelta-t-right3-for-t-in-left-t_0t_0-delta-t-ldots-t_0-ndelta-t-right" class="nav-link" data-scroll-target="#s_t-s_f-left-s_i---s_f-right-left-1---fract---t_0ndelta-t-right3-for-t-in-left-t_0t_0-delta-t-ldots-t_0-ndelta-t-right"><span class="math display">\[s_{t} = s_{f} + \left( s_{i} - s_{f} \right) \left( 1 - \frac{t - t_{0}}{N\Delta t} \right)^{3} for t \in \left\{ t_{0},t_{0} + \Delta t, \ldots, t_{0} + N\Delta t \right\}\]</span></a>
  <ul class="collapse">
  <li><a href="#movement-pruning" id="toc-movement-pruning" class="nav-link" data-scroll-target="#movement-pruning">Movement pruning</a></li>
  </ul></li>
  <li><a href="#m-top_ks" id="toc-m-top_ks" class="nav-link" data-scroll-target="#m-top_ks"><span class="math display">\[M = Top_{k}(S)\]</span></a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 8</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 8 covers different methods to make transformer models more efficient in production.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 14, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/transformers-book-notes.html"><strong>Natural Language Processing with Transformers</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#making-transformers-efficient-in-production">Making Transformers Efficient in Production</a></li>
<li><a href="#project-optimize-an-intent-detection-model">Project: Optimize an Intent Detection Model</a></li>
<li><a href="#creating-a-performance-benchmark">Creating a Performance Benchmark</a></li>
<li><a href="#Making%20Models%20Smaller%20via%20Knowledge%20Distillation">Making Models Smaller via Knowledge Distillation</a></li>
<li><a href="#a-primer-on-floating-point-and-fixed-point-numbers">A Primer on Floating-Point and Fixed-Point Numbers</a></li>
<li><a href="#making-models-faster-with-quantization">Making Models Faster with Quantization</a></li>
<li><a href="#optimizing-inference-with-onnx-and-the-onnx-runtime">Optimizing Inference with ONNX and the ONNX Runtime</a></li>
<li><a href="#making-models-sparser-with-weight-pruning">Making Models Sparser with Weight Pruning</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>transformers.__version__, datasets.__version__, accelerate.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('4.18.0', '2.0.0', '0.5.1')</code></pre>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="making-transformers-efficient-in-production" class="level2">
<h2 class="anchored" data-anchor-id="making-transformers-efficient-in-production">Making Transformers Efficient in Production</h2>
<ul>
<li>A state-of-the-art model is not very useful if it is too slow or too large to meet an application’s business requirements.</li>
<li>Starting with a faster, more compact model often results in degraded performance.</li>
<li>Knowledge distillation, quantization, pruning, and graph optimization are complementary techniques that can speed up predictions and reduce the memory footprint of models.</li>
<li>We can combine some of these techniques to produce significant performance gains.</li>
<li><a href="https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/">Roblox: How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs</a>
<ul>
<li>Roblox improved the latency and throughput of their BERT classifier by over 30x by combining knowledge distillation and quantization.</li>
</ul></li>
</ul>
</section>
<section id="project-optimize-an-intent-detection-model" class="level2">
<h2 class="anchored" data-anchor-id="project-optimize-an-intent-detection-model">Project: Optimize an Intent Detection Model</h2>
<ul>
<li>The goal is to create a text-based assistant for a call center so customers can request their account balance and make bookings.</li>
<li>The assistant must be able to classify a wide variety of natural language text into a set of predefined intents.</li>
<li>The classifier must also handle out-of-scope queries and yield fallback responses when they do not belong to any predefined intents.</li>
</ul>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The Model</h3>
<ul>
<li>The baseline model is a fine-tuned BERT-base model that achieves 94% accuracy on the CLINC150 dataset.</li>
<li><a href="https://huggingface.co/transformersbook/bert-base-uncased-finetuned-clinc">Hugging Face Dataset Card</a></li>
</ul>
</section>
<section id="clinc150-dataset" class="level3">
<h3 class="anchored" data-anchor-id="clinc150-dataset">CLINC150 Dataset</h3>
<ul>
<li><a href="https://github.com/clinc/oos-eval/">Homepage</a></li>
<li><a href="https://huggingface.co/datasets/clinc_oos">HuggingFace Dataset Card</a></li>
<li>The CLINC150 dataset includes 22,500 in-scope queries across 150 intents and ten domains.</li>
<li>The dataset contains 1,200 out-of-scope queries that belong to an oos intent class.</li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Instantiate a text classification pipeline with the baseline model</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>bert_ckpt <span class="op">=</span> <span class="st">"transformersbook/bert-base-uncased-finetuned-clinc"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>bert_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Classify a sample query</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">"""Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="st">Paris and I need a 15 passenger van"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>pipe(query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [{'label': 'car_rental', 'score': 0.549003541469574}]</code></pre>
<p><strong>Note:</strong> The model correctly detects that the user wants to rent a vehicle.</p>
<hr>
</section>
</section>
<section id="creating-a-performance-benchmark" class="level2">
<h2 class="anchored" data-anchor-id="creating-a-performance-benchmark">Creating a Performance Benchmark</h2>
<ul>
<li>Deploying transformers in production involves a tradeoff between several constraints.</li>
<li>Business and product metrics are the most important to consider.</li>
<li>Model performance refers to how the model performs on a well-crafted test set representing production data.</li>
<li>Model performance is especially crucial when the cost of making errors is high or when performing inference on millions of examples and minor improvements translate to significant gains.</li>
<li>Latency refers to how fast the model delivers predictions. Latency is most important for real-time environments with lots of traffic.
<ul>
<li><a href="https://stackoverflow.blog/2020/04/09/the-unfriendly-robot-automatically-flagging-unwelcoming-comments/">The Unfriendly Robot: Automatically flagging unwelcoming comments</a></li>
</ul></li>
<li>Memory constraints play an important role in mobile and edge devices where we need to perform inference without access to a cloud server.</li>
<li>Failing to address these constraints can negatively impact the user experience.</li>
<li>Running expensive cloud servers that may only need to handle a few requests can lead to ballooning costs.</li>
</ul>
<p><strong>Define a benchmark that measures model performance, latency, and memory usage</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PerformanceBenchmark:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, pipeline, dataset, optim_type<span class="op">=</span><span class="st">"BERT baseline"</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pipeline <span class="op">=</span> pipeline</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dataset <span class="op">=</span> dataset</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optim_type <span class="op">=</span> optim_type</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_accuracy(<span class="va">self</span>):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll define this later</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_size(<span class="va">self</span>):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll define this later</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> time_pipeline(<span class="va">self</span>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll define this later</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run_benchmark(<span class="va">self</span>):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        metrics <span class="op">=</span> {}</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        metrics[<span class="va">self</span>.optim_type] <span class="op">=</span> <span class="va">self</span>.compute_size()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        metrics[<span class="va">self</span>.optim_type].update(<span class="va">self</span>.time_pipeline())</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        metrics[<span class="va">self</span>.optim_type].update(<span class="va">self</span>.compute_accuracy())</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> metrics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Load the CLINC150 Dataset</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>clinc <span class="op">=</span> load_dataset(<span class="st">"clinc_oos"</span>, <span class="st">"plus"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>clinc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['text', 'intent'],
            num_rows: 15250
        })
        validation: Dataset({
            features: ['text', 'intent'],
            num_rows: 3100
        })
        test: Dataset({
            features: ['text', 'intent'],
            num_rows: 5500
        })
    })</code></pre>
<p><strong>Note:</strong> * The <code>plus</code> configuration refers to the subset that contains the out-of-scope training examples. * Each example consists of a query in the text column and its corresponding intent.</p>
<hr>
<p><strong>View an example</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> clinc[<span class="st">"test"</span>][<span class="dv">42</span>]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'text': 'transfer $100 from my checking to saving account', 'intent': 133}</code></pre>
<hr>
<p><strong>Map intent ID to the corresponding string</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>intents <span class="op">=</span> clinc[<span class="st">"test"</span>].features[<span class="st">"intent"</span>]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>intents.int2str(sample[<span class="st">"intent"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'transfer'</code></pre>
<hr>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(intents._int2str)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; overflow-y:auto; height:600px">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
restaurant_reviews
</td>
</tr>
<tr>
<th>
1
</th>
<td>
nutrition_info
</td>
</tr>
<tr>
<th>
2
</th>
<td>
account_blocked
</td>
</tr>
<tr>
<th>
3
</th>
<td>
oil_change_how
</td>
</tr>
<tr>
<th>
4
</th>
<td>
time
</td>
</tr>
<tr>
<th>
5
</th>
<td>
weather
</td>
</tr>
<tr>
<th>
6
</th>
<td>
redeem_rewards
</td>
</tr>
<tr>
<th>
7
</th>
<td>
interest_rate
</td>
</tr>
<tr>
<th>
8
</th>
<td>
gas_type
</td>
</tr>
<tr>
<th>
9
</th>
<td>
accept_reservations
</td>
</tr>
<tr>
<th>
10
</th>
<td>
smart_home
</td>
</tr>
<tr>
<th>
11
</th>
<td>
user_name
</td>
</tr>
<tr>
<th>
12
</th>
<td>
report_lost_card
</td>
</tr>
<tr>
<th>
13
</th>
<td>
repeat
</td>
</tr>
<tr>
<th>
14
</th>
<td>
whisper_mode
</td>
</tr>
<tr>
<th>
15
</th>
<td>
what_are_your_hobbies
</td>
</tr>
<tr>
<th>
16
</th>
<td>
order
</td>
</tr>
<tr>
<th>
17
</th>
<td>
jump_start
</td>
</tr>
<tr>
<th>
18
</th>
<td>
schedule_meeting
</td>
</tr>
<tr>
<th>
19
</th>
<td>
meeting_schedule
</td>
</tr>
<tr>
<th>
20
</th>
<td>
freeze_account
</td>
</tr>
<tr>
<th>
21
</th>
<td>
what_song
</td>
</tr>
<tr>
<th>
22
</th>
<td>
meaning_of_life
</td>
</tr>
<tr>
<th>
23
</th>
<td>
restaurant_reservation
</td>
</tr>
<tr>
<th>
24
</th>
<td>
traffic
</td>
</tr>
<tr>
<th>
25
</th>
<td>
make_call
</td>
</tr>
<tr>
<th>
26
</th>
<td>
text
</td>
</tr>
<tr>
<th>
27
</th>
<td>
bill_balance
</td>
</tr>
<tr>
<th>
28
</th>
<td>
improve_credit_score
</td>
</tr>
<tr>
<th>
29
</th>
<td>
change_language
</td>
</tr>
<tr>
<th>
30
</th>
<td>
no
</td>
</tr>
<tr>
<th>
31
</th>
<td>
measurement_conversion
</td>
</tr>
<tr>
<th>
32
</th>
<td>
timer
</td>
</tr>
<tr>
<th>
33
</th>
<td>
flip_coin
</td>
</tr>
<tr>
<th>
34
</th>
<td>
do_you_have_pets
</td>
</tr>
<tr>
<th>
35
</th>
<td>
balance
</td>
</tr>
<tr>
<th>
36
</th>
<td>
tell_joke
</td>
</tr>
<tr>
<th>
37
</th>
<td>
last_maintenance
</td>
</tr>
<tr>
<th>
38
</th>
<td>
exchange_rate
</td>
</tr>
<tr>
<th>
39
</th>
<td>
uber
</td>
</tr>
<tr>
<th>
40
</th>
<td>
car_rental
</td>
</tr>
<tr>
<th>
41
</th>
<td>
credit_limit
</td>
</tr>
<tr>
<th>
42
</th>
<td>
oos
</td>
</tr>
<tr>
<th>
43
</th>
<td>
shopping_list
</td>
</tr>
<tr>
<th>
44
</th>
<td>
expiration_date
</td>
</tr>
<tr>
<th>
45
</th>
<td>
routing
</td>
</tr>
<tr>
<th>
46
</th>
<td>
meal_suggestion
</td>
</tr>
<tr>
<th>
47
</th>
<td>
tire_change
</td>
</tr>
<tr>
<th>
48
</th>
<td>
todo_list
</td>
</tr>
<tr>
<th>
49
</th>
<td>
card_declined
</td>
</tr>
<tr>
<th>
50
</th>
<td>
rewards_balance
</td>
</tr>
<tr>
<th>
51
</th>
<td>
change_accent
</td>
</tr>
<tr>
<th>
52
</th>
<td>
vaccines
</td>
</tr>
<tr>
<th>
53
</th>
<td>
reminder_update
</td>
</tr>
<tr>
<th>
54
</th>
<td>
food_last
</td>
</tr>
<tr>
<th>
55
</th>
<td>
change_ai_name
</td>
</tr>
<tr>
<th>
56
</th>
<td>
bill_due
</td>
</tr>
<tr>
<th>
57
</th>
<td>
who_do_you_work_for
</td>
</tr>
<tr>
<th>
58
</th>
<td>
share_location
</td>
</tr>
<tr>
<th>
59
</th>
<td>
international_visa
</td>
</tr>
<tr>
<th>
60
</th>
<td>
calendar
</td>
</tr>
<tr>
<th>
61
</th>
<td>
translate
</td>
</tr>
<tr>
<th>
62
</th>
<td>
carry_on
</td>
</tr>
<tr>
<th>
63
</th>
<td>
book_flight
</td>
</tr>
<tr>
<th>
64
</th>
<td>
insurance_change
</td>
</tr>
<tr>
<th>
65
</th>
<td>
todo_list_update
</td>
</tr>
<tr>
<th>
66
</th>
<td>
timezone
</td>
</tr>
<tr>
<th>
67
</th>
<td>
cancel_reservation
</td>
</tr>
<tr>
<th>
68
</th>
<td>
transactions
</td>
</tr>
<tr>
<th>
69
</th>
<td>
credit_score
</td>
</tr>
<tr>
<th>
70
</th>
<td>
report_fraud
</td>
</tr>
<tr>
<th>
71
</th>
<td>
spending_history
</td>
</tr>
<tr>
<th>
72
</th>
<td>
directions
</td>
</tr>
<tr>
<th>
73
</th>
<td>
spelling
</td>
</tr>
<tr>
<th>
74
</th>
<td>
insurance
</td>
</tr>
<tr>
<th>
75
</th>
<td>
what_is_your_name
</td>
</tr>
<tr>
<th>
76
</th>
<td>
reminder
</td>
</tr>
<tr>
<th>
77
</th>
<td>
where_are_you_from
</td>
</tr>
<tr>
<th>
78
</th>
<td>
distance
</td>
</tr>
<tr>
<th>
79
</th>
<td>
payday
</td>
</tr>
<tr>
<th>
80
</th>
<td>
flight_status
</td>
</tr>
<tr>
<th>
81
</th>
<td>
find_phone
</td>
</tr>
<tr>
<th>
82
</th>
<td>
greeting
</td>
</tr>
<tr>
<th>
83
</th>
<td>
alarm
</td>
</tr>
<tr>
<th>
84
</th>
<td>
order_status
</td>
</tr>
<tr>
<th>
85
</th>
<td>
confirm_reservation
</td>
</tr>
<tr>
<th>
86
</th>
<td>
cook_time
</td>
</tr>
<tr>
<th>
87
</th>
<td>
damaged_card
</td>
</tr>
<tr>
<th>
88
</th>
<td>
reset_settings
</td>
</tr>
<tr>
<th>
89
</th>
<td>
pin_change
</td>
</tr>
<tr>
<th>
90
</th>
<td>
replacement_card_duration
</td>
</tr>
<tr>
<th>
91
</th>
<td>
new_card
</td>
</tr>
<tr>
<th>
92
</th>
<td>
roll_dice
</td>
</tr>
<tr>
<th>
93
</th>
<td>
income
</td>
</tr>
<tr>
<th>
94
</th>
<td>
taxes
</td>
</tr>
<tr>
<th>
95
</th>
<td>
date
</td>
</tr>
<tr>
<th>
96
</th>
<td>
who_made_you
</td>
</tr>
<tr>
<th>
97
</th>
<td>
pto_request
</td>
</tr>
<tr>
<th>
98
</th>
<td>
tire_pressure
</td>
</tr>
<tr>
<th>
99
</th>
<td>
how_old_are_you
</td>
</tr>
<tr>
<th>
100
</th>
<td>
rollover_401k
</td>
</tr>
<tr>
<th>
101
</th>
<td>
pto_request_status
</td>
</tr>
<tr>
<th>
102
</th>
<td>
how_busy
</td>
</tr>
<tr>
<th>
103
</th>
<td>
application_status
</td>
</tr>
<tr>
<th>
104
</th>
<td>
recipe
</td>
</tr>
<tr>
<th>
105
</th>
<td>
calendar_update
</td>
</tr>
<tr>
<th>
106
</th>
<td>
play_music
</td>
</tr>
<tr>
<th>
107
</th>
<td>
yes
</td>
</tr>
<tr>
<th>
108
</th>
<td>
direct_deposit
</td>
</tr>
<tr>
<th>
109
</th>
<td>
credit_limit_change
</td>
</tr>
<tr>
<th>
110
</th>
<td>
gas
</td>
</tr>
<tr>
<th>
111
</th>
<td>
pay_bill
</td>
</tr>
<tr>
<th>
112
</th>
<td>
ingredients_list
</td>
</tr>
<tr>
<th>
113
</th>
<td>
lost_luggage
</td>
</tr>
<tr>
<th>
114
</th>
<td>
goodbye
</td>
</tr>
<tr>
<th>
115
</th>
<td>
what_can_i_ask_you
</td>
</tr>
<tr>
<th>
116
</th>
<td>
book_hotel
</td>
</tr>
<tr>
<th>
117
</th>
<td>
are_you_a_bot
</td>
</tr>
<tr>
<th>
118
</th>
<td>
next_song
</td>
</tr>
<tr>
<th>
119
</th>
<td>
change_speed
</td>
</tr>
<tr>
<th>
120
</th>
<td>
plug_type
</td>
</tr>
<tr>
<th>
121
</th>
<td>
maybe
</td>
</tr>
<tr>
<th>
122
</th>
<td>
w2
</td>
</tr>
<tr>
<th>
123
</th>
<td>
oil_change_when
</td>
</tr>
<tr>
<th>
124
</th>
<td>
thank_you
</td>
</tr>
<tr>
<th>
125
</th>
<td>
shopping_list_update
</td>
</tr>
<tr>
<th>
126
</th>
<td>
pto_balance
</td>
</tr>
<tr>
<th>
127
</th>
<td>
order_checks
</td>
</tr>
<tr>
<th>
128
</th>
<td>
travel_alert
</td>
</tr>
<tr>
<th>
129
</th>
<td>
fun_fact
</td>
</tr>
<tr>
<th>
130
</th>
<td>
sync_device
</td>
</tr>
<tr>
<th>
131
</th>
<td>
schedule_maintenance
</td>
</tr>
<tr>
<th>
132
</th>
<td>
apr
</td>
</tr>
<tr>
<th>
133
</th>
<td>
transfer
</td>
</tr>
<tr>
<th>
134
</th>
<td>
ingredient_substitution
</td>
</tr>
<tr>
<th>
135
</th>
<td>
calories
</td>
</tr>
<tr>
<th>
136
</th>
<td>
current_location
</td>
</tr>
<tr>
<th>
137
</th>
<td>
international_fees
</td>
</tr>
<tr>
<th>
138
</th>
<td>
calculator
</td>
</tr>
<tr>
<th>
139
</th>
<td>
definition
</td>
</tr>
<tr>
<th>
140
</th>
<td>
next_holiday
</td>
</tr>
<tr>
<th>
141
</th>
<td>
update_playlist
</td>
</tr>
<tr>
<th>
142
</th>
<td>
mpg
</td>
</tr>
<tr>
<th>
143
</th>
<td>
min_payment
</td>
</tr>
<tr>
<th>
144
</th>
<td>
change_user_name
</td>
</tr>
<tr>
<th>
145
</th>
<td>
restaurant_suggestion
</td>
</tr>
<tr>
<th>
146
</th>
<td>
travel_notification
</td>
</tr>
<tr>
<th>
147
</th>
<td>
cancel
</td>
</tr>
<tr>
<th>
148
</th>
<td>
pto_used
</td>
</tr>
<tr>
<th>
149
</th>
<td>
travel_suggestion
</td>
</tr>
<tr>
<th>
150
</th>
<td>
change_volume
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_metric </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Load the accuracy metric</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>accuracy_score <span class="op">=</span> load_metric(<span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Metric(name: "accuracy", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: """
    Args:
        predictions: Predicted labels, as returned by a model.
        references: Ground truth labels.
        normalize: If False, return the number of correctly classified samples.
            Otherwise, return the fraction of correctly classified samples.
        sample_weight: Sample weights.
    Returns:
        accuracy: Accuracy score.
    Examples:
    
        &gt;&gt;&gt; accuracy_metric = datasets.load_metric("accuracy")
        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])
        &gt;&gt;&gt; print(results)
        {'accuracy': 1.0}
    """, stored examples: 0)</code></pre>
<p><strong>Note:</strong> The accuracy metric expects the predictions and ground truth labels to be integers.</p>
<hr>
<p><strong>Implement the <code>PerformanceBenchmark.compute_accuracy()</code> method</strong></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_accuracy(<span class="va">self</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This overrides the PerformanceBenchmark.compute_accuracy() method"""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    preds, labels <span class="op">=</span> [], []</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Collect all the predictions and labels into lists</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> example <span class="kw">in</span> <span class="va">self</span>.dataset:</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> <span class="va">self</span>.pipeline(example[<span class="st">"text"</span>])[<span class="dv">0</span>][<span class="st">"label"</span>]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> example[<span class="st">"intent"</span>]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        preds.append(intents.str2int(pred))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        labels.append(label)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the accuracy for the predictions</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score.compute(predictions<span class="op">=</span>preds, references<span class="op">=</span>labels)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Accuracy on test set - </span><span class="sc">{</span>accuracy[<span class="st">'accuracy'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Override the PerformanceBenchmark.compute_accuracy() method</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>PerformanceBenchmark.compute_accuracy <span class="op">=</span> compute_accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compute the model size</strong></p>
<p><strong>Note:</strong> * We can compute the model size using the torch.save() function. * The <code>save()</code> function uses Python’s pickle module. * The recommended way to save a PyTorch model is by using its state_dict. * The state_dict is a Python dictionary that maps each layer in a model to its learnable parameters.</p>
<p><strong>Inspect the <code>state_dict</code> for the baseline model</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(pipe.model.state_dict().items())[<span class="dv">42</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('bert.encoder.layer.2.attention.self.value.weight',
     tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,
               4.6521e-03,  2.9844e-02],
             [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,
              -2.6890e-02, -2.1943e-02],
             [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,
               3.1152e-02, -9.7786e-03],
             ...,
             [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,
               1.1093e-02,  2.9703e-03],
             [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,
               6.7487e-03,  1.0511e-03],
             [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,
               2.3981e-02, -4.2880e-02]]))</code></pre>
<p><strong>Note:</strong> Each key-value pair corresponds to a specific layer and tensor in BERT.</p>
<hr>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Implement the <code>PerformanceBenchmark.compute_size()</code> method</strong></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_size(<span class="va">self</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This overrides the PerformanceBenchmark.compute_size() method"""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> <span class="va">self</span>.pipeline.model.state_dict()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    tmp_path <span class="op">=</span> Path(<span class="st">"model.pt"</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Temporarily save the model to disk</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    torch.save(state_dict, tmp_path)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate size in megabytes</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    size_mb <span class="op">=</span> Path(tmp_path).stat().st_size <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Delete temporary file</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    tmp_path.unlink()</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Model size (MB) - </span><span class="sc">{</span>size_mb<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"size_mb"</span>: size_mb}</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Override the PerformanceBenchmark.compute_size() method</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>PerformanceBenchmark.compute_size <span class="op">=</span> compute_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compute the model latency</strong></p>
<ul>
<li>For this application, latency refers to the time it takes to feed a text query to the pipeline and return the predicted intent from the model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> perf_counter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="time.perf_counter" class="level4">
<h4 class="anchored" data-anchor-id="time.perf_counter"><code>time.perf_counter</code></h4>
<ul>
<li><a href="https://docs.python.org/3/library/time.html#time.perf_counter">Documentation</a></li>
<li>Get the value in fractional seconds of a clock with the highest available resolution to measure a short duration.</li>
</ul>
<hr>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(perf_counter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Help on built-in function perf_counter in module time:
    
    perf_counter(...)
        perf_counter() -&gt; float
        
        Performance counter for benchmarking.</code></pre>
<hr>
<p><strong>Test the latency of the baseline model</strong></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> perf_counter()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> pipe(query)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    latency <span class="op">=</span> perf_counter() <span class="op">-</span> start_time</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Latency (ms) - </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> latency<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Latency (ms) - 29.646
    Latency (ms) - 28.035
    Latency (ms) - 27.233</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>There is a notable spread in the latencies, so we should collect the latencies over many runs to calculate the mean and standard deviation.</li>
<li>The latency depends on the query length, and it is good practice to benchmark using queries the models are likely to encounter in production.</li>
</ul>
<hr>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Implement the <code>PerformanceBenchmark.time_pipeline()</code> method</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> time_pipeline(<span class="va">self</span>, query<span class="op">=</span><span class="st">"What is the pin number for my account?"</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This overrides the PerformanceBenchmark.time_pipeline() method"""</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    latencies <span class="op">=</span> []</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warmup</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="va">self</span>.pipeline(query)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Timed run</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        start_time <span class="op">=</span> perf_counter()</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> <span class="va">self</span>.pipeline(query)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        latency <span class="op">=</span> perf_counter() <span class="op">-</span> start_time</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        latencies.append(latency)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute run statistics</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    time_avg_ms <span class="op">=</span> <span class="dv">1000</span> <span class="op">*</span> np.mean(latencies)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    time_std_ms <span class="op">=</span> <span class="dv">1000</span> <span class="op">*</span> np.std(latencies)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Average latency (ms) - </span><span class="sc">{</span>time_avg_ms<span class="sc">:.2f}</span><span class="ss"> +\- </span><span class="sc">{</span>time_std_ms<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"time_avg_ms"</span>: time_avg_ms, <span class="st">"time_std_ms"</span>: time_std_ms}</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Override the PerformanceBenchmark.time_pipeline() method</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>PerformanceBenchmark.time_pipeline <span class="op">=</span> time_pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark the baseline model</strong></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> PerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>])</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>perf_metrics <span class="op">=</span> pb.run_benchmark()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 418.16
    Average latency (ms) - 24.46 +\- 1.20
    Accuracy on test set - 0.867</code></pre>
<hr>
</section>
</section>
<section id="making-models-smaller-via-knowledge-distillation" class="level2">
<h2 class="anchored" data-anchor-id="making-models-smaller-via-knowledge-distillation">Making Models Smaller via Knowledge Distillation</h2>
<ul>
<li>Knowledge distillation is a general-purpose method for training a smaller student model to mimic the behavior of a slower but better-performing teacher.</li>
<li><a href="https://dl.acm.org/doi/10.1145/1150402.1150464">Model compression</a>
<ul>
<li>This paper introduced the concept of knowledge distillation in 2006 in the context of ensemble models.</li>
</ul></li>
<li><a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a>
<ul>
<li>This paper generalized knowledge distillation to deep neural networks and applied it to image classification and automatic speech recognition.</li>
</ul></li>
<li>The current trend is to pre-train language models with ever-increasing parameters counts.
<ul>
<li><a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></li>
</ul></li>
<li>Knowledge distillation is a popular strategy to compress huge pretrained models and make them more suitable for building practical applications.</li>
</ul>
<section id="knowledge-distillation-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-distillation-for-fine-tuning">Knowledge Distillation for Fine-Tuning</h3>
<ul>
<li>Knowledge distillation for supervised tasks like fine-tuning involves augmenting the ground truth labels with a distribution of “soft probabilities” from the teacher, providing complementary information for the student.</li>
<li>If the teacher assigns high probabilities to multiple intents, they might lie close to each other in the feature space.</li>
<li>The goal is to train the student to distill some of this “dark knowledge” learned by the teacher.</li>
<li>This “dark knowledge” is not available from the labels alone.</li>
<li>We feed an input sequence <span class="math inline">\(x\)</span> to the teacher to generate a vector of logits <span class="math inline">\(z(x) = \left[ z_{1}(x),\ldots,z_{N}(x) \right]\)</span> and convert these logits into probabilities using the softmax function.</li>
</ul>
</section>
<section id="fracexp-left-z_ix-rightsum_jexp-left-z_ix-right" class="level3">
<h3 class="anchored" data-anchor-id="fracexp-left-z_ix-rightsum_jexp-left-z_ix-right"><span class="math display">\[\frac{exp \left( z_{i}(x) \right)}{\sum_{j}{exp \left( z_{i}(x) \right)}}\]</span></h3>
<ul>
<li>The teacher will often assign a high probability to one class, with all other class probabilities close to zero, providing little additional information beyond the ground truth labels.</li>
<li>We can “soften” the probabilities by scaling the logits with a temperature hyperparameter <span class="math inline">\(T\)</span> before applying the softmax.</li>
</ul>
</section>
<section id="p_ix-fracexp-left-frac-z_ix-t-rightsum_jexp-left-frac-z_ix-t-right" class="level3">
<h3 class="anchored" data-anchor-id="p_ix-fracexp-left-frac-z_ix-t-rightsum_jexp-left-frac-z_ix-t-right"><span class="math display">\[p_{i}(x) = \frac{exp \left( \frac{ z_{i}(x) }{T} \right)}{\sum_{j}{exp \left( \frac{ z_{i}(x) }{T} \right)}}\]</span></h3>
<ul>
<li>Higher temperature values produce a softer probability distribution over the classes and reveal much more information about the decision boundary learned by the teacher for each example.
<ul>
<li>When T = 1, we get the original softmax distribution.</li>
</ul></li>
<li>We can use the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> to measure the difference between the teacher’s probability distribution and the student’s probability distribution.</li>
</ul>
</section>
<section id="d_klpq-sum_ip_ixlogfracp_ixq_ix" class="level3">
<h3 class="anchored" data-anchor-id="d_klpq-sum_ip_ixlogfracp_ixq_ix"><span class="math display">\[D_{KL}(p,q) = \sum_{i}{p_{i}(x)\log{\frac{p_{i}(x)}{q_{i}(x)}}}\]</span></h3>
<ul>
<li>With the KL divergence, we can calculate how much is lost when we approximate the probability distribution of the teacher with the student.</li>
<li><strong>Kowledge Distillation Loss:</strong></li>
</ul>
</section>
<section id="l_kd-t2d_kl" class="level3">
<h3 class="anchored" data-anchor-id="l_kd-t2d_kl"><span class="math display">\[L_{KD} = T^{2}D_{KL}\]</span></h3>
<ul>
<li><p><span class="math inline">\(T_{2}\)</span> is the normalization factor to account for the magnitude of the gradients produced by the soft labels scaling as <span class="math inline">\(1/T^{2}\)</span>.</p></li>
<li><p>For classification tasks, the student loss is a weighted average of the distillation loss with the usual cross-entropy loss <span class="math inline">\(L_{CE}\)</span> of the ground truth labels.</p></li>
</ul>
</section>
<section id="l_student-alpha-l_ce-left-1---alpha-rightl_kd" class="level3">
<h3 class="anchored" data-anchor-id="l_student-alpha-l_ce-left-1---alpha-rightl_kd"><span class="math display">\[L_{student} = \alpha L_{CE} \ + \left( 1 - \alpha \right)L_{KD}\]</span></h3>
<ul>
<li><span class="math inline">\(\alpha\)</span> is a hyperparameter that controls the relative strength of each loss.</li>
</ul>
</section>
<section id="knowledge-distillation-for-pretraining" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-distillation-for-pretraining">Knowledge Distillation for Pretraining</h3>
<ul>
<li>We can use knowledge distillation during pretraining to create a general-purpose student that we subsequently fine-tune on downstream tasks.</li>
<li>The teacher is a pretrained language model like BERT, which transfers its knowledge about masked-language modeling to the student.</li>
<li>For DistilBERT, we augment the masked language modeling loss <span class="math inline">\(L_{mlm}\)</span> with a term from knowledge distillation and a cosine embedding loss <span class="math inline">\(L_{cos} = 1 \ - \ \cos \left( h_{s},h_{t} \right)\)</span> to align the directions of the hidden state vectors between the teacher and student.<br>
</li>
</ul>
</section>
<section id="l_distilbert-alpha-l_mlm-beta-l_kd-y-loss_cos" class="level3">
<h3 class="anchored" data-anchor-id="l_distilbert-alpha-l_mlm-beta-l_kd-y-loss_cos"><span class="math display">\[L_{DistilBERT} = \alpha L_{mlm} \ + \ \beta L_{KD} \ + \ y \ Loss_{cos}\]</span></h3>
</section>
<section id="creating-a-knowledge-distillation-trainer" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-knowledge-distillation-trainer">Creating a Knowledge Distillation Trainer</h3>
<ul>
<li>We can augment the cross-entropy loss with an <span class="math inline">\(L_{KD}\)</span> term by creating a custom trainer.</li>
</ul>
<section id="additions-to-the-base-trainer-class" class="level4">
<h4 class="anchored" data-anchor-id="additions-to-the-base-trainer-class">Additions to the base Trainer Class:</h4>
<ul>
<li>The new hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(T\)</span>.</li>
<li>The fine-tuned teacher model</li>
<li>A new loss function that combines the the cross-entropy loss with the knowledge distillation loss</li>
</ul>
<hr>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a new TrainingArguments subclass with the new hyperparameters</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DistillationTrainingArguments(TrainingArguments):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, alpha<span class="op">=</span><span class="fl">0.5</span>, temperature<span class="op">=</span><span class="fl">2.0</span>, <span class="op">**</span>kwargs):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.temperature <span class="op">=</span> temperature</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="nn.kldivloss" class="level4">
<h4 class="anchored" data-anchor-id="nn.kldivloss"><code>nn.KLDivLoss</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html">Documentation</a></li>
<li>Compute the Kullback-Leibler divergence loss.</li>
</ul>
</section>
</section>
<section id="ly_textpred-y_texttrue-y_texttrue-cdot-log-fracy_texttruey_textpred-y_texttrue-cdot-log-y_texttrue---log-y_textpred" class="level3">
<h3 class="anchored" data-anchor-id="ly_textpred-y_texttrue-y_texttrue-cdot-log-fracy_texttruey_textpred-y_texttrue-cdot-log-y_texttrue---log-y_textpred"><span class="math display">\[L(y_{\text{pred}},\ y_{\text{true}}) = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}} = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})\]</span></h3>
<ul>
<li><p>where <span class="math inline">\(y_{\text{pred}}\)</span> is the input and <span class="math inline">\(y_{\text{true}}\)</span> is the target</p></li>
<li><p>The inputs need to be in the form of log probabilities.</p></li>
<li><p>The labels need to be in the form of normal probabilities.</p></li>
</ul>
<p><strong>Create a new Trainer subclass and override the <code>compute_loss()</code> method</strong></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DistillationTrainer(Trainer):</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, teacher_model<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.teacher_model <span class="op">=</span> teacher_model</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_loss(<span class="va">self</span>, model, inputs, return_outputs<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        outputs_stu <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract cross-entropy loss and logits from student</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        loss_ce <span class="op">=</span> outputs_stu.loss</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        logits_stu <span class="op">=</span> outputs_stu.logits</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract logits from teacher</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>            outputs_tea <span class="op">=</span> <span class="va">self</span>.teacher_model(<span class="op">**</span>inputs)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>            logits_tea <span class="op">=</span> outputs_tea.logits</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Soften probabilities and compute distillation loss</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        loss_fct <span class="op">=</span> nn.KLDivLoss(reduction<span class="op">=</span><span class="st">"batchmean"</span>)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        loss_kd <span class="op">=</span> <span class="va">self</span>.args.temperature <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> loss_fct(</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>            F.log_softmax(logits_stu <span class="op">/</span> <span class="va">self</span>.args.temperature, dim<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>            F.softmax(logits_tea <span class="op">/</span> <span class="va">self</span>.args.temperature, dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return weighted student loss</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.args.alpha <span class="op">*</span> loss_ce <span class="op">+</span> (<span class="fl">1.</span> <span class="op">-</span> <span class="va">self</span>.args.alpha) <span class="op">*</span> loss_kd</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (loss, outputs_stu) <span class="cf">if</span> return_outputs <span class="cf">else</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The <code>reduction="batchmean"</code> argument in <code>nn.KVDivLoss()</code> specifies that we average the losses over the batch dimension.</p>
<hr>
</section>
<section id="choosing-a-good-student-initialization" class="level3">
<h3 class="anchored" data-anchor-id="choosing-a-good-student-initialization">Choosing a Good Student Initialization</h3>
<ul>
<li>The student model should be smaller to reduce the latency and memory footprint.</li>
<li><a href="https://arxiv.org/abs/2010.13382">FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</a>
<ul>
<li>Knowledge distillation tends to work best when the teacher and student are of the same model type.</li>
<li>Different model types like BERT and RoBERTa can have incompatible output embedding spaces, hindering the student’s ability to mimic the teacher.</li>
</ul></li>
<li>DistilBERT is a compatible student model for the BERT baseline model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Load the tokenizer for the DistilBERT student model</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>student_ckpt <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>student_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(student_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Tokenize and encode the queries</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_text(batch):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> student_tokenizer(batch[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>clinc_enc <span class="op">=</span> clinc.<span class="bu">map</span>(tokenize_text, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>[<span class="st">"text"</span>])</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>clinc_enc <span class="op">=</span> clinc_enc.rename_column(<span class="st">"intent"</span>, <span class="st">"labels"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> * We no longer need the text column. * The trainer looks for a column called labels when fine-tuning for classification tasks. * We can override this default with the label_names argument of the TrainingArguments object.</p>
<hr>
<p><strong>Disable Tokenizers Parallelism</strong></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env TOKENIZERS_PARALLELISM<span class="op">=</span>false</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    env: TOKENIZERS_PARALLELISM=false</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Log into Hugging Face account</strong></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>notebook_login()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Login successful
    Your token has been saved to /home/innom-dt/.huggingface/token</code></pre>
<hr>
<p><strong>Define the metrics to track during training</strong></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(pred):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    predictions, labels <span class="op">=</span> pred</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the most confident class predictions</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compare the predictions to the ground truth label</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy_score.compute(predictions<span class="op">=</span>predictions, references<span class="op">=</span>labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define the training arguments</strong></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">48</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>finetuned_ckpt <span class="op">=</span> <span class="st">"distilbert-base-uncased-finetuned-clinc"</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>student_training_args <span class="op">=</span> DistillationTrainingArguments(</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>finetuned_ckpt, evaluation_strategy <span class="op">=</span> <span class="st">"epoch"</span>, </span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">5</span>, learning_rate<span class="op">=</span><span class="fl">2e-5</span>, </span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>batch_size, </span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>batch_size, alpha<span class="op">=</span><span class="dv">1</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>, </span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">True</span>, fp16<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Starting with <span class="math inline">\(\alpha=1\)</span> to see how well the student performs without any signal from the teacher.</p>
<hr>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>student_training_args.logging_steps <span class="op">=</span> <span class="bu">len</span>(clinc_enc[<span class="st">'train'</span>]) <span class="op">//</span> batch_size</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>student_training_args.disable_tqdm <span class="op">=</span> <span class="va">False</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>student_training_args.save_steps <span class="op">=</span> <span class="fl">1e9</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>student_training_args.log_level <span class="op">=</span> <span class="dv">40</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Provide the student model with the mappings between each intent and label ID</strong></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> pipe.model.config.id2label</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>label2id <span class="op">=</span> pipe.model.config.label2id</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a custom model configuration from the student</strong></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> intents.num_classes</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>student_config <span class="op">=</span> (AutoConfig.from_pretrained(student_ckpt, num_labels<span class="op">=</span>num_labels, </span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>                                             id2label<span class="op">=</span>id2label, label2id<span class="op">=</span>label2id))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Use a CUDA GPU is available</strong></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to initialize the student model with a sequence classification head</strong></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> student_init():</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (AutoModelForSequenceClassification</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>            .from_pretrained(student_ckpt, config<span class="op">=</span>student_config).to(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize the teacher model with a sequence classification head</strong></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>teacher_ckpt <span class="op">=</span> <span class="st">"transformersbook/bert-base-uncased-finetuned-clinc"</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>teacher_model <span class="op">=</span> (AutoModelForSequenceClassification</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>                 .from_pretrained(teacher_ckpt, num_labels<span class="op">=</span>num_labels)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>                 .to(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize the custom trainer</strong></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>distilbert_trainer <span class="op">=</span> DistillationTrainer(model_init<span class="op">=</span>student_init,</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    teacher_model<span class="op">=</span>teacher_model, args<span class="op">=</span>student_training_args,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>clinc_enc[<span class="st">'train'</span>], eval_dataset<span class="op">=</span>clinc_enc[<span class="st">'validation'</span>],</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span>compute_metrics, tokenizer<span class="op">=</span>student_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/distilbert-base-uncased-finetuned-clinc is already a clone of https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-clinc. Make sure you pull the latest changes with `repo.git_pull()`.</code></pre>
<hr>
<p><strong>Note:</strong> Had to add the following workaround.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>old_collator <span class="op">=</span> distilbert_trainer.data_collator</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>distilbert_trainer.data_collator <span class="op">=</span> <span class="kw">lambda</span> data: <span class="bu">dict</span>(old_collator(data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the model</strong></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>distilbert_trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


Accuracy






1


4.293800


3.290489


0.740968




2


2.634600


1.883282


0.832581




3


1.555400


1.165018


0.892581




4


1.018900


0.863598


0.910968




5


0.802800


0.779555


0.916129





</div>
<pre class="text"><code>    TrainOutput(global_step=1590, training_loss=2.0571008596780165, metrics={'train_runtime': 62.8736, 'train_samples_per_second': 1212.75, 'train_steps_per_second': 25.289, 'total_flos': 413896353421488.0, 'train_loss': 2.0571008596780165, 'epoch': 5.0})</code></pre>
<p><strong>Note:</strong> The student achieves a validation accuracy of nearly 92% compared to the teacher’s 94% accuracy.</p>
<hr>
<p><strong>Push the trained model to Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>distilbert_trainer.push_to_hub(<span class="st">"Training completed!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-clinc/commit/028b8f56cb944e1c7e1b8f4f6265c5beeddef127'</code></pre>
<hr>
<p><strong>Load the fine-tuned student model into a text classification pipeline</strong></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>finetuned_ckpt <span class="op">=</span> <span class="st">"cj-mills/distilbert-base-uncased-finetuned-clinc"</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>finetuned_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark the student model</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>optim_type <span class="op">=</span> <span class="st">"DistilBERT"</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> PerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>], optim_type<span class="op">=</span>optim_type)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>perf_metrics.update(pb.run_benchmark())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 255.89
    Average latency (ms) - 12.44 +\- 0.43
    Accuracy on test set - 0.857</code></pre>
<hr>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare the student performance metrics to the baseline model</strong></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co">u'</span><span class="ch">\u25CC</span><span class="co">'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    '◌'</code></pre>
<hr>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_metrics(perf_metrics, current_optim_type):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame.from_dict(perf_metrics, orient<span class="op">=</span><span class="st">'index'</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> df.index:</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        df_opt <span class="op">=</span> df.loc[idx]</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add a dashed circle around the current optimization type</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">==</span> current_optim_type:</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>            plt.scatter(df_opt[<span class="st">"time_avg_ms"</span>], df_opt[<span class="st">"accuracy"</span>] <span class="op">*</span> <span class="dv">100</span>, </span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>                        alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span>df_opt[<span class="st">"size_mb"</span>], label<span class="op">=</span>idx, </span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>                        marker<span class="op">=</span><span class="st">'$</span><span class="ch">\u25CC</span><span class="st">$'</span>)</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>            plt.scatter(df_opt[<span class="st">"time_avg_ms"</span>], df_opt[<span class="st">"accuracy"</span>] <span class="op">*</span> <span class="dv">100</span>, </span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>                        s<span class="op">=</span>df_opt[<span class="st">"size_mb"</span>], label<span class="op">=</span>idx, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    legend <span class="op">=</span> plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> handle <span class="kw">in</span> legend.legendHandles:</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>        handle.set_sizes([<span class="dv">20</span>])</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="dv">80</span>,<span class="dv">90</span>)</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the slowest model to define the x-axis range</span></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>    xlim <span class="op">=</span> <span class="bu">int</span>(perf_metrics[<span class="st">"BERT baseline"</span>][<span class="st">"time_avg_ms"</span>] <span class="op">+</span> <span class="dv">3</span>)</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="dv">1</span>, xlim)</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Accuracy (%)"</span>)</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Average latency (ms)"</span>)</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a>plot_metrics(perf_metrics, optim_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_108_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The student is twice as fast and nearly as accurate.</p>
<hr>
</section>
<section id="finding-good-hyperparameters-with-optuna" class="level3">
<h3 class="anchored" data-anchor-id="finding-good-hyperparameters-with-optuna">Finding Good Hyperparameters with Optuna</h3>
<ul>
<li><a href="https://arxiv.org/abs/1907.10902">Optuna: A Next-generation Hyperparameter Optimization Framework</a></li>
<li>Optuna formulates the hyperparameter search problem in terms of an objective function optimized through multiple trials.</li>
</ul>
<hr>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>The Rosenbrock “banana function” of two variables</strong></p>
<ul>
<li>The Rosenbrock function is a famous test case for optimization.</li>
<li>Finding the valley is easy, but converging to the global minimum is not.</li>
</ul>
<hr>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span><span class="op">-</span>x)<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">100</span><span class="op">*</span>(y<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the banana function</strong></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">250</span>), np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">250</span>))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X,Y)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="dv">1</span>], [<span class="dv">1</span>], <span class="st">'x'</span>, mew<span class="op">=</span><span class="dv">3</span>, markersize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>ax.contourf(X, Y, Z, np.logspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">30</span>), cmap<span class="op">=</span><span class="st">'viridis'</span>, extend<span class="op">=</span><span class="st">"both"</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">1.3</span>, <span class="fl">1.3</span>)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">0.9</span>, <span class="fl">1.7</span>)</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_115_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> In Optuna, we can find the minimum of the <span class="math inline">\(f(x,y)\)</span> function by defining an <code>objective()</code> function that returns the value of the <span class="math inline">\(f(x,y)\)</span>.</p>
<hr>
<p><strong>Define an objective function for the Rosenbrock function</strong></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> trial.suggest_float(<span class="st">"x"</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> trial.suggest_float(<span class="st">"y"</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span> <span class="op">-</span> x) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span> <span class="op">*</span> (y <span class="op">-</span> x <span class="op">**</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> * The trial.suggest_float object specifies the parameter ranges to sample from uniformly. * Optuna collects multiple trials as a <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna-study-study">Study</a>.</p>
<hr>
<div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="optuna.study.study" class="level4">
<h4 class="anchored" data-anchor-id="optuna.study.study"><code>optuna.study.Study</code></h4>
<ul>
<li><a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna-study-study">Documentation</a></li>
<li>A study corresponds to a set of trials for an optimization task.</li>
<li>A study object provides interfaces to run a new <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial"><code>Trial</code></a>.</li>
</ul>
</section>
<section id="optuna.create_study" class="level4">
<h4 class="anchored" data-anchor-id="optuna.create_study"><code>optuna.create_study()</code></h4>
<ul>
<li>Create a new Study object.</li>
</ul>
<hr>
<p><strong>Find the best hyperparameters for the Rosenbrock function</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study()</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>study.best_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'x': 0.9569346059991378, 'y': 0.920346631232987}</code></pre>
<hr>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">250</span>), np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">250</span>))</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X,Y)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>ax.plot([study.best_params[<span class="st">'x'</span>]], study.best_params[<span class="st">'y'</span>], <span class="st">'x'</span>, mew<span class="op">=</span><span class="dv">3</span>, markersize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>ax.contourf(X, Y, Z, np.logspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">30</span>), cmap<span class="op">=</span><span class="st">'viridis'</span>, extend<span class="op">=</span><span class="st">"both"</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">1.3</span>, <span class="fl">1.3</span>)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="fl">0.9</span>, <span class="fl">1.7</span>)</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_125_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> Optuna managed to find values for x and y that are reasonably close to the global minimum.</p>
<hr>
<p><strong>Define the hyperparameter space for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(T\)</span></strong></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hp_space(trial):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"num_train_epochs"</span>: trial.suggest_int(<span class="st">"num_train_epochs"</span>, <span class="dv">5</span>, <span class="dv">10</span>),</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"alpha"</span>: trial.suggest_float(<span class="st">"alpha"</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"temperature"</span>: trial.suggest_int(<span class="st">"temperature"</span>, <span class="dv">2</span>, <span class="dv">20</span>)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="trainer.hyperparameter_search" class="level4">
<h4 class="anchored" data-anchor-id="trainer.hyperparameter_search"><code>Trainer.hyperparameter_search</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.hyperparameter_search">Documentation</a></li>
<li>Launch an hyperparameter search using <a href="https://optuna.org/">optuna</a> or <a href="https://www.ray.io/ray-tune">Ray Tune</a> or <a href="https://sigopt.com/">SigOpt</a>.</li>
</ul>
<p><strong>Run the hyperparameter search</strong></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>best_run <span class="op">=</span> distilbert_trainer.hyperparameter_search(</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    n_trials<span class="op">=</span><span class="dv">20</span>, direction<span class="op">=</span><span class="st">"maximize"</span>, hp_space<span class="op">=</span>hp_space)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(best_run)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    transformers.trainer_utils.BestRun</code></pre>
<hr>
</section>
<section id="bestrun" class="level4">
<h4 class="anchored" data-anchor-id="bestrun"><code>BestRun</code></h4>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/442dc4564578584c5fbc9c22e08f91fcbf9da570/src/transformers/trainer_utils.py#L159">Source Code</a></li>
<li>Stores the best run found by a hyperparameter search</li>
</ul>
<hr>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>best_run.hyperparameters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'num_train_epochs': 10, 'alpha': 0.9901751316785802, 'temperature': 5}</code></pre>
<hr>
<p><strong>Update the training arguments with the new hyperparameter values</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k,v <span class="kw">in</span> best_run.hyperparameters.items():</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">setattr</span>(student_training_args, k, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a new repository to store our distilled model</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>distilled_ckpt <span class="op">=</span> <span class="st">"distilbert-base-uncased-distilled-clinc"</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>student_training_args.output_dir <span class="op">=</span> distilled_ckpt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a new Trainer with optimal parameters</strong></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>distil_trainer <span class="op">=</span> DistillationTrainer(model_init<span class="op">=</span>student_init,</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    teacher_model<span class="op">=</span>teacher_model, args<span class="op">=</span>student_training_args,</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>clinc_enc[<span class="st">'train'</span>], eval_dataset<span class="op">=</span>clinc_enc[<span class="st">'validation'</span>],</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span>compute_metrics, tokenizer<span class="op">=</span>student_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    /media/innom-dt/Samsung_T3/Projects/Current_Projects/nlp-with-transformers-book/notebooks/distilbert-base-uncased-distilled-clinc is already a clone of https://huggingface.co/cj-mills/distilbert-base-uncased-distilled-clinc. Make sure you pull the latest changes with `repo.git_pull()`.</code></pre>
<hr>
<p><strong>Note:</strong> Had to add the following workaround.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>old_collator <span class="op">=</span> distil_trainer.data_collator</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>distil_trainer.data_collator <span class="op">=</span> <span class="kw">lambda</span> data: <span class="bu">dict</span>(old_collator(data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the model</strong></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>distil_trainer.train()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table>
<tbody><tr style="text-align: left;">
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
Accuracy
</th>
</tr>

</tbody><tbody>
<tr>
<td>
1
</td>
<td>
4.224600
</td>
<td>
3.158392
</td>
<td>
0.754516
</td>
</tr>
<tr>
<td>
2
</td>
<td>
2.403300
</td>
<td>
1.565648
</td>
<td>
0.865161
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.168400
</td>
<td>
0.779509
</td>
<td>
0.916129
</td>
</tr>
<tr>
<td>
4
</td>
<td>
0.569300
</td>
<td>
0.465274
</td>
<td>
0.932903
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.304200
</td>
<td>
0.341210
</td>
<td>
0.940645
</td>
</tr>
<tr>
<td>
6
</td>
<td>
0.179400
</td>
<td>
0.291207
</td>
<td>
0.940323
</td>
</tr>
<tr>
<td>
7
</td>
<td>
0.118400
</td>
<td>
0.265375
</td>
<td>
0.946129
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.087300
</td>
<td>
0.255724
</td>
<td>
0.943871
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.071900
</td>
<td>
0.254949
</td>
<td>
0.946452
</td>
</tr>
<tr>
<td>
10
</td>
<td>
0.064600
</td>
<td>
0.252466
</td>
<td>
0.946774
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The student achieved over 94% accuracy despite having almost half the number of parameters of the teacher model.</p>
<hr>
<p><strong>Push the trained model to Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>distil_trainer.push_to_hub(<span class="st">"Training complete"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/distilbert-base-uncased-distilled-clinc/commit/e4cee3ec87d5415df7ca130dfe1e75446de03b26'</code></pre>
<hr>
</section>
</section>
<section id="benchmarking-our-distilled-model" class="level3">
<h3 class="anchored" data-anchor-id="benchmarking-our-distilled-model">Benchmarking Our Distilled Model</h3>
<p><strong>Create a new text classification pipeline using the latest student model</strong></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>distilled_ckpt <span class="op">=</span> <span class="st">"cj-mills/distilbert-base-uncased-distilled-clinc"</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>distilled_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark the latest student</strong></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>optim_type <span class="op">=</span> <span class="st">"Distillation"</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> PerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>], optim_type<span class="op">=</span>optim_type)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>perf_metrics.update(pb.run_benchmark())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 255.89
    Average latency (ms) - 12.37 +\- 0.35
    Accuracy on test set - 0.887</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(perf_metrics, optim_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_153_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The distillation student exceeds the baseline model performance. * The teacher model was likely not fine-tuned as systematically as the student.</p>
<hr>
</section>
</section>
<section id="a-primer-on-floating-point-and-fixed-point-numbers" class="level2">
<h2 class="anchored" data-anchor-id="a-primer-on-floating-point-and-fixed-point-numbers">A Primer on Floating-Point and Fixed-Point Numbers</h2>
<ul>
<li>Most transformers pre-train and fine-tune using FP32 or a mix of FP16 and FP32.</li>
<li>These floating-point data types provide the precision needed to accommodate the very different ranges of weights, activations, and gradients.</li>
<li>A floating-point number like FP32 represents a sequence of 32 bits grouped in terms of a sign, exponent, and significand.</li>
<li>The sign determines whether the number is positive or negative.</li>
<li>The significand corresponds to the number of significant digits, scaled using the exponent in some fixed base (usually 2 for binary or 10 for decimal).</li>
<li>We can represent a wide range of real numbers through the exponent.</li>
<li>The decimal or binary point can go anywhere relative to the significant digits (hence the name “floating-point”).</li>
<li>We can reduce the precision of the data types after training without impacting the accuracy too much.</li>
<li>It is common to use a fixed-point format for the low-precision data types that represent real numbers as B-bit integers scaled by a common factor for all variables of the same data type.
<ul>
<li>We can represent the floating-point number <span class="math inline">\(137.035\)</span> as the integer <span class="math inline">\(137,035\)</span> scaled by <span class="math inline">\(1/1000\)</span>.</li>
<li>We control the range and precision of a fixed-point number by adjusting the scaling factor.</li>
</ul></li>
</ul>
</section>
<section id="making-models-faster-with-quantization" class="level2">
<h2 class="anchored" data-anchor-id="making-models-faster-with-quantization">Making Models Faster with Quantization</h2>
<ul>
<li>Quantization makes computation more efficient by representing the weights and activations with low-precision data types like an 8-bit integer (INT8) instead of the usual 32-bit floating-point (FP32).</li>
<li>Reducing the number of bits means the model requires less memory, and operations like matrix multiplication are much faster with integer arithmetic.</li>
<li>We can quantize models with little to no impact on accuracy.</li>
<li>We “discretize” the floating-point values <span class="math inline">\(f\)</span> in each tensor by mapping their range <span class="math inline">\(\left[ f_{max}, f_{min} \right]\)</span> into a smaller one <span class="math inline">\(\left[ q_{max}, q_{min} \right]\)</span> of fixed-point numbers <span class="math inline">\(q\)</span> and linearly distributing all tensor values in between.</li>
</ul>
<section id="f-left-fracf_max---f_minq_max---q_min-rightq-z-sq-z" class="level3">
<h3 class="anchored" data-anchor-id="f-left-fracf_max---f_minq_max---q_min-rightq-z-sq-z"><span class="math display">\[f = \left( \frac{f_{max} - f_{min}}{q_{max} - q_{min}} \right)(q-Z) = S(q-Z)\]</span></h3>
<ul>
<li><p>where <span class="math inline">\(S\)</span> is a positive floatin-point number and the constant <span class="math inline">\(Z\)</span> has the same type as <span class="math inline">\(q\)</span> and is called the zero point becaue it corresponds to the quentized value of the floating-point value <span class="math inline">\(f=0\)</span></p></li>
<li><p>The map needs to be affine (<span class="math inline">\(y=Ax+b\)</span>) to get back floating-point numbers when we dequantize the fixed-point ones.</p></li>
<li><p>Transformers and other deep neural networks are prime candidates for quantization because the weights and activations tend to take values in relatively small ranges.</p></li>
</ul>
<p><strong>Plot the frequency distribution of values for a single attention weight matrix</strong></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> pipe.model.state_dict()</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> state_dict[<span class="st">"distilbert.transformer.layer.0.attention.out_lin.weight"</span>]</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>plt.hist(weights.flatten().numpy(), bins<span class="op">=</span><span class="dv">250</span>, <span class="bu">range</span><span class="op">=</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.3</span>), edgecolor<span class="op">=</span><span class="st">"C0"</span>)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_158_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The weight values fall in the range <span class="math inline">\(\left[ -0.1, 0.1 \right]\)</span> around zero.</p>
<hr>
<p><strong>Calculate the fixed-point scaling value</strong></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>zero_point <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> (weights.<span class="bu">max</span>() <span class="op">-</span> weights.<span class="bu">min</span>()) <span class="op">/</span> (<span class="dv">127</span> <span class="op">-</span> (<span class="op">-</span><span class="dv">128</span>))</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>scale</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    tensor(0.0053)</code></pre>
<p><strong>Note:</strong> * The range of possible values for the integers is <span class="math inline">\(\left[ q_{max}, q_{min} \right] = \left[ -128, 127 \right]\)</span> * The zero point coincides with the zero of FP32.</p>
<hr>
<p><strong>Quantize a single weight matrix</strong></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>(weights <span class="op">/</span> scale <span class="op">+</span> zero_point).clamp(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).<span class="bu">round</span>().char()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    tensor([[ -5,  -7,   0,  ...,  -6,  -4,   8],
            [  9,   2,   1,  ...,  -4,   7,   0],
            [ -9,  -6,   5,  ...,   1,   5,  -4],
            ...,
            [  5,   0,  12,  ...,   0,   6,  -1],
            [  0,  -2, -12,  ...,  11,  -7, -13],
            [-13,  -1,  -9,  ...,   8,   2,  -2]], dtype=torch.int8)</code></pre>
<hr>
<div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> quantize_per_tensor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="quantize_per_tensor" class="level4">
<h4 class="anchored" data-anchor-id="quantize_per_tensor"><code>quantize_per_tensor</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html">Documentation</a></li>
<li>Convert a float tensor to a quantized tensor with a given scale and zero point.</li>
</ul>
<p><strong>Quantize a single weight matrix using PyTorch</strong></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> torch.qint8</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>quantized_weights <span class="op">=</span> quantize_per_tensor(weights, scale, zero_point, dtype)</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>quantized_weights.int_repr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    tensor([[ -5,  -7,   0,  ...,  -6,  -4,   8],
            [  9,   2,   1,  ...,  -4,   7,   0],
            [ -9,  -6,   5,  ...,   1,   5,  -4],
            ...,
            [  5,   0,  12,  ...,   0,   6,  -1],
            [  0,  -2, -12,  ...,  11,  -7, -13],
            [-13,  -1,  -9,  ...,   8,   2,  -2]], dtype=torch.int8)
</code></pre>
<hr>
<div class="sourceCode" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.axes_grid1.inset_locator <span class="im">import</span> zoomed_inset_axes,mark_inset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the effect of quantization on a transformer’s weights</strong></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create histogram</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>ax.hist(quantized_weights.dequantize().flatten().numpy(), </span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>         bins<span class="op">=</span><span class="dv">250</span>, <span class="bu">range</span><span class="op">=</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.3</span>), edgecolor<span class="op">=</span><span class="st">"C0"</span>)<span class="op">;</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create zoom inset</span></span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>axins <span class="op">=</span> zoomed_inset_axes(ax, <span class="dv">5</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>axins.hist(quantized_weights.dequantize().flatten().numpy(), </span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>         bins<span class="op">=</span><span class="dv">250</span>, <span class="bu">range</span><span class="op">=</span>(<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.3</span>))<span class="op">;</span></span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>x1, x2, y1, y2 <span class="op">=</span> <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="dv">500</span>, <span class="dv">2500</span></span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>axins.set_xlim(x1, x2)</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>axins.set_ylim(y1, y2)</span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>axins.axes.xaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>axins.axes.yaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a>mark_inset(ax, axins, loc1<span class="op">=</span><span class="dv">2</span>, loc2<span class="op">=</span><span class="dv">4</span>, fc<span class="op">=</span><span class="st">"none"</span>, ec<span class="op">=</span><span class="st">"0.5"</span>)</span>
<span id="cb109-15"><a href="#cb109-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_171_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<hr>
<p><strong>Time how long matrix multiplication takes with FP32.</strong></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit </span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">@</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    1.03 ms ± 2.14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</code></pre>
<hr>
<div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.quantized <span class="im">import</span> QFunctional</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="qfunctional" class="level4">
<h4 class="anchored" data-anchor-id="qfunctional"><code>QFunctional</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.quantized.QFunctional.html?highlight=qfunctional#torch.nn.quantized.QFunctional">Documentation</a></li>
<li>A wrapper class for quantized operations</li>
</ul>
<hr>
<div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>q_fn <span class="op">=</span> QFunctional()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Time how long matrix multiplication takes with INT8.</strong></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>q_fn.mul(quantized_weights, quantized_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    23.5 µs ± 179 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)</code></pre>
<p><strong>Note:</strong> Using the INT8 tensors is significantly faster.</p>
<hr>
<div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Calculate the difference in storage requirements</strong></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>sys.getsizeof(weights.storage()) <span class="op">/</span> sys.getsizeof(quantized_weights.storage())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    3.999755879241598</code></pre>
<p><strong>Note:</strong> * Quantization reduces memory storage requirements by up to a factor of four (32/8=4). * The actual compression rate for an entire model depends on which layers are quantized.</p>
<hr>
</section>
</section>
<section id="approaches-to-quantization" class="level3">
<h3 class="anchored" data-anchor-id="approaches-to-quantization">Approaches to Quantization</h3>
<ul>
<li>Changing the precision for all computations in the model introduces tiny but compounding disturbances in the model’s computational graph, which affect the model’s performance.</li>
<li>There are three main approaches for quantizing deep neural networks.</li>
</ul>
<section id="dynamic-quantization" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-quantization">Dynamic Quantization</h4>
<ul>
<li>Dynamic quantization converts the weights and activations to INT8 after training completes.</li>
<li>Dynamic quantization happens on the fly, and we still read and write to memory the activations in floating-point format.</li>
<li>The conversion between integer and floating-point can be a performance bottleneck.</li>
</ul>
</section>
<section id="static-quantization" class="level4">
<h4 class="anchored" data-anchor-id="static-quantization">Static Quantization</h4>
<ul>
<li>Static quantization precomputes the quantization scheme by observing the activation patterns on a representative sample of the data ahead of inference time.</li>
<li>Static quantization enables us to skip the conversion between INT8 and FP32 values and speeds up the computations.</li>
<li>Static quantization requires access to an adequate data sample and introduces an additional step in the pipeline.</li>
<li>Static quantization does not address the discrepancy between the precision during training and inference, leading to a performance drop in the model’s metrics.</li>
</ul>
</section>
<section id="quantization-aware-training" class="level4">
<h4 class="anchored" data-anchor-id="quantization-aware-training">Quantization-aware training</h4>
<ul>
<li>Quantization-aware training simulates quantization during training by “fake” quantization of FP32 values.</li>
<li>We round the FP32 values to mimic the effect of quantization during the forward and backward passes.</li>
<li>Quantization-aware training improves performance in terms of model metrics over static and dynamic quantization.</li>
</ul>
</section>
<section id="what-to-choose" class="level4">
<h4 class="anchored" data-anchor-id="what-to-choose">What to choose</h4>
<ul>
<li>Dynamic quantization is the best approach for transformers as the main bottleneck for running inference is the compute and memory bandwidth associated with the enormous numbers of weights.</li>
<li>The limiting factor for smaller compute vision models is the memory bandwidth of the activations, making static quantization or quantization-aware training the best approach.</li>
</ul>
<hr>
<div class="sourceCode" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.quantization <span class="im">import</span> quantize_dynamic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="quantize_dynamic" class="level4">
<h4 class="anchored" data-anchor-id="quantize_dynamic"><code>quantize_dynamic</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.quantization.quantize_dynamic.html?highlight=quantize_dynamic#torch.quantization.quantize_dynamic">Documentation</a></li>
<li>Quantize the weights of a floating-point model.</li>
</ul>
<p><strong>Quantize the distilled student model</strong></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"cj-mills/distilbert-base-uncased-distilled-clinc"</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> (AutoModelForSequenceClassification</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>         .from_pretrained(model_ckpt).to(<span class="st">"cpu"</span>))</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>model_quantized <span class="op">=</span> quantize_dynamic(model, {nn.Linear}, dtype<span class="op">=</span>torch.qint8)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark the quantized model</strong></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>model_quantized, </span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>                tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>optim_type <span class="op">=</span> <span class="st">"Distillation + quantization"</span></span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> PerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>], optim_type<span class="op">=</span>optim_type)</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>perf_metrics.update(pb.run_benchmark())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 132.40
    Average latency (ms) - 5.33 +\- 0.14
    Accuracy on test set - 0.892</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(perf_metrics, optim_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_191_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The quantized model is nearly half the size of the distilled model and gained a slight accuracy boost.</p>
<hr>
</section>
</section>
</section>
<section id="optimizing-inference-with-onnx-and-the-onnx-runtime" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-inference-with-onnx-and-the-onnx-runtime">Optimizing Inference with ONNX and the ONNX Runtime</h2>
<ul>
<li>ONNX is an open standard that defines a common set of operators and a common file format to represent deep learning models in different frameworks.</li>
<li>These operators are the building blocks for constructing a computational graph (often called an intermediate representation) for exported models.</li>
<li>A computational graph represents the flow of data through the neural network.</li>
<li>The standardized operators and data types make it easy to switch between frameworks.</li>
<li><a href="https://onnxruntime.ai/">ONNX Runtime</a> provides tools to optimize the ONNX graph through techniques like operator fusion and constant folding and defines an interface to execution providers that allow you to run the model on different types of hardware.</li>
<li>A fused operator involves merging one operator (usually an activation function) into another, so they execute as a single step.</li>
<li>Constant folding refers to evaluating constant expressions at compile time instead of runtime.</li>
</ul>
<section id="other-accelerators" class="level3">
<h3 class="anchored" data-anchor-id="other-accelerators">Other Accelerators</h3>
<ul>
<li><a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a></li>
<li><a href="https://tvm.apache.org/docs/index.html">Apache TVM</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">Intel OpenVINO Toolkit</a></li>
</ul>
</section>
<section id="convert-model-to-onnx-format" class="level3">
<h3 class="anchored" data-anchor-id="convert-model-to-onnx-format">Convert model to ONNX format</h3>
<hr>
<div class="sourceCode" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> psutil <span class="im">import</span> cpu_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="openmp" class="level4">
<h4 class="anchored" data-anchor-id="openmp">OpenMP</h4>
<ul>
<li><a href="https://www.openmp.org/">Homepage</a></li>
<li>The OpenMP API supports multi-platform shared-memory parallel programming in C/C++ and Fortran.</li>
</ul>
<p><strong>Set OpenMP environment variables</strong></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"OMP_NUM_THREADS"</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>cpu_count()<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"OMP_WAIT_POLICY"</span>] <span class="op">=</span> <span class="st">"ACTIVE"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> * The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel computations in the ONNX runtime. * OMP_WAIT_POLICY=ACTIVE specifies that waiting threads should be active.</p>
<hr>
<div class="sourceCode" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.convert_graph_to_onnx <span class="im">import</span> convert</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="transformers.convert_graph_to_onnx.convert" class="level4">
<h4 class="anchored" data-anchor-id="transformers.convert_graph_to_onnx.convert"><code>transformers.convert_graph_to_onnx.convert()</code></h4>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/442dc4564578584c5fbc9c22e08f91fcbf9da570/src/transformers/convert_graph_to_onnx.py#L351">Source Code</a></li>
<li>Convert a pipeline object to the ONNX Intermediate Representation (IR) format</li>
<li>The Hugging Face Transformers library provides a function called convert_graph_to_onnx.convert() that simplifies the process by taking the following steps:
<ol type="1">
<li>Initialize the model as a Pipeline.</li>
<li>Run placeholder inputs through the pipeline so that ONNX can record the computational graph.</li>
<li>Define dynamic axes to handle dynamic sequence lengths.</li>
<li>Save the graph with network parameters.</li>
</ol></li>
</ul>
<p><strong>Convert the distilled model to ONNX format using a text classification pipeline</strong></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"cj-mills/distilbert-base-uncased-distilled-clinc"</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>onnx_model_path <span class="op">=</span> Path(<span class="st">"onnx/model.onnx"</span>)</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>convert(framework<span class="op">=</span><span class="st">"pt"</span>, model<span class="op">=</span>model_ckpt, tokenizer<span class="op">=</span>tokenizer, </span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>        output<span class="op">=</span>onnx_model_path, opset<span class="op">=</span><span class="dv">12</span>, pipeline_name<span class="op">=</span><span class="st">"text-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ONNX opset version set to: 12
    Loading pipeline (model: cj-mills/distilbert-base-uncased-distilled-clinc, tokenizer: PreTrainedTokenizerFast(name_or_path='cj-mills/distilbert-base-uncased-distilled-clinc', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))


    /home/innom-dt/miniconda3/envs/transformer-book/lib/python3.9/site-packages/transformers/convert_graph_to_onnx.py:378: FutureWarning: The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of Transformers
      warnings.warn(


    Creating folder onnx
    Using framework PyTorch: 1.11.0
    Found input input_ids with shape: {0: 'batch', 1: 'sequence'}
    Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}
    Found output output_0 with shape: {0: 'batch'}
    Ensuring inputs are in correct order
    head_mask is not present in the generated input list.
    Generated inputs order: ['input_ids', 'attention_mask']</code></pre>
<hr>
<div class="sourceCode" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> onnxruntime <span class="im">import</span> (GraphOptimizationLevel, InferenceSession, </span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>                         SessionOptions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to create an InferenceSession</strong></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model_for_provider(model_path, provider<span class="op">=</span><span class="st">"CPUExecutionProvider"</span>): </span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>    options <span class="op">=</span> SessionOptions()</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>    options.intra_op_num_threads <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>    options.graph_optimization_level <span class="op">=</span> GraphOptimizationLevel.ORT_ENABLE_ALL</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>    session <span class="op">=</span> InferenceSession(<span class="bu">str</span>(model_path), options, providers<span class="op">=</span>[provider])</span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>    session.disable_fallback()</span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> session</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create an inference session using the exported model</strong></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>onnx_model <span class="op">=</span> create_model_for_provider(onnx_model_path)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="co"># onnx_model = create_model_for_provider(onnx_model_path, provider="CUDAExecutionProvider")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Get the class logits from the ONNX model</strong></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> clinc_enc[<span class="st">"test"</span>][:<span class="dv">1</span>]</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> inputs[<span class="st">"labels"</span>]</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>logits_onnx <span class="op">=</span> onnx_model.run(<span class="va">None</span>, inputs)[<span class="dv">0</span>]</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>logits_onnx.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (1, 151)</code></pre>
<hr>
<p><strong>Get the most confident prediction</strong></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>np.argmax(logits_onnx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    61</code></pre>
<hr>
<p><strong>Compare prediction to ground truth label</strong></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>clinc_enc[<span class="st">"test"</span>][<span class="dv">0</span>][<span class="st">"labels"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>61</code></pre>
<p><strong>Note:</strong> The model prediction matches the ground truth.</p>
<hr>
</section>
</section>
<section id="create-custom-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="create-custom-pipeline">Create Custom Pipeline</h3>
<ul>
<li>The ONNX model is not compatible with the text classification pipeline so we need to mimic the pipeline’s core behavior.</li>
</ul>
<hr>
<div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> softmax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a custom pipeline class</strong></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OnnxPipeline:</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, tokenizer):</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, query):</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>        model_inputs <span class="op">=</span> <span class="va">self</span>.tokenizer(query, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>        inputs_onnx <span class="op">=</span> {k: v.cpu().detach().numpy() </span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a>                       <span class="cf">for</span> k, v <span class="kw">in</span> model_inputs.items()}</span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.model.run(<span class="va">None</span>, inputs_onnx)[<span class="dv">0</span>][<span class="dv">0</span>, :]</span>
<span id="cb139-11"><a href="#cb139-11" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> softmax(logits)</span>
<span id="cb139-12"><a href="#cb139-12" aria-hidden="true" tabindex="-1"></a>        pred_idx <span class="op">=</span> np.argmax(probs).item()</span>
<span id="cb139-13"><a href="#cb139-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [{<span class="st">"label"</span>: intents.int2str(pred_idx), <span class="st">"score"</span>: probs[pred_idx]}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test the custom pipeline</strong></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> OnnxPipeline(onnx_model, tokenizer)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>pipe(query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [{'label': 'car_rental', 'score': 0.9709836}]</code></pre>
<hr>
<p><strong>Define a performance benchmark class for ONNX models</strong></p>
<div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OnnxPerformanceBenchmark(PerformanceBenchmark):</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, model_path, <span class="op">**</span>kwargs):</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model_path <span class="op">=</span> model_path</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Override the PerformanceBenchmark.compute_size() method</span></span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_size(<span class="va">self</span>):</span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>        size_mb <span class="op">=</span> Path(<span class="va">self</span>.model_path).stat().st_size <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Model size (MB) - </span><span class="sc">{</span>size_mb<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb142-10"><a href="#cb142-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"size_mb"</span>: size_mb}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark the ONNX Model</strong></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>optim_type <span class="op">=</span> <span class="st">"Distillation + ORT"</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> OnnxPerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>], optim_type,</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>                              model_path<span class="op">=</span><span class="st">"onnx/model.onnx"</span>)</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>perf_metrics.update(pb.run_benchmark())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 255.90
    Average latency (ms) - 10.42 +\- 0.29
    Accuracy on test set - 0.887</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(perf_metrics, optim_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_226_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> Converting the distilled model to ONNX format decreased latency.</p>
<hr>
<div class="sourceCode" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> onnxruntime.quantization <span class="im">import</span> quantize_dynamic, QuantType</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(quantize_dynamic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Help on function quantize_dynamic in module onnxruntime.quantization.quantize:
    
    quantize_dynamic(model_input: pathlib.Path, model_output: pathlib.Path, op_types_to_quantize=[], per_channel=False, reduce_range=False, weight_type=&lt;QuantType.QInt8: 0&gt;, nodes_to_quantize=[], nodes_to_exclude=[], optimize_model=True, use_external_data_format=False, extra_options={})
            Given an onnx model, create a quantized onnx model and save it into a file
        :param model_input: file path of model to quantize
        :param model_output: file path of quantized model
        :param op_types_to_quantize: specify the types of operators to quantize, like ['Conv'] to quantize Conv only. It quantizes all supported operators by default
        :param per_channel: quantize weights per channel
        :param reduce_range: quantize weights with 7-bits. It may improve the accuracy for some models running on non-VNNI machine, especially for per-channel mode
        :param nbits: number of bits to represent quantized data. Currently only supporting 8-bit types
        :param activation_type: quantization data type of activation. Please refer to https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection
        :param weight_type: quantization data type of weight. Please refer to https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection
        :param nodes_to_quantize:
            List of nodes names to quantize. When this list is not None only the nodes in this list
            are quantized.
            example:
            [
                'Conv__224',
                'Conv__252'
            ]
        :param nodes_to_exclude:
            List of nodes names to exclude. The nodes in this list will be excluded from quantization
            when it is not None.
        :parma use_external_data_format: option used for large size (&gt;2GB) model. Set to False by default.
            :param extra_options:
            key value pair dictionary for various options in different case. Current used:
                extra.Sigmoid.nnapi = True/False  (Default is False)
                ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).
                WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).
                EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.
                                              Dyanmic mode currently is supported. Will support more in future.
                DisableShapeInference = True/False : in dynamic quantize mode, shape inference is not must have
                                                     and if it cause some issue, you could disable it.
                ForceQuantizeNoInputCheck = True/False : By default, some latent operators like maxpool, transpose, do not quantize
                                                         if their input is not quantized already. Setting to True to force such operator
                                                         always quantize input and so generate quantized output. Also the True behavior
                                                         could be disabled per node using the nodes_to_exclude.
                MatMulConstBOnly = True/False: Default is True for dynamic mode. If enabled, only MatMul with const B will be quantized.</code></pre>
<hr>
<p><strong>Quantize the ONNX model</strong></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>model_input <span class="op">=</span> <span class="st">"onnx/model.onnx"</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>model_output <span class="op">=</span> <span class="st">"onnx/model.quant.onnx"</span></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>quantize_dynamic(model_input, model_output, weight_type<span class="op">=</span>QuantType.QInt8)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Benchmark Quantized ONNX Model</strong></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>onnx_quantized_model <span class="op">=</span> create_model_for_provider(model_output)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> OnnxPipeline(onnx_quantized_model, tokenizer)</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>optim_type <span class="op">=</span> <span class="st">"Distillation + ORT (quantized)"</span></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>pb <span class="op">=</span> OnnxPerformanceBenchmark(pipe, clinc[<span class="st">"test"</span>], optim_type, </span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>                              model_path<span class="op">=</span>model_output)</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>perf_metrics.update(pb.run_benchmark())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Model size (MB) - 64.22
    Average latency (ms) - 3.39 +\- 0.25
    Accuracy on test set - 0.893</code></pre>
<div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(perf_metrics, optim_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_234_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The quantized ONNX model further reduced latency and improved accuracy compared to the quantized PyTorch model. * PyTorch only optimizes the <code>nn.Linear</code> modules while ONNX also quantized the embedding layer.</p>
<hr>
</section>
</section>
<section id="making-models-sparser-with-weight-pruning" class="level2">
<h2 class="anchored" data-anchor-id="making-models-sparser-with-weight-pruning">Making Models Sparser with Weight Pruning</h2>
<ul>
<li><a href="https://github.com/huggingface/nn_pruning">Neural Networks Block Movement Pruning</a>
<ul>
<li>A Hugging Face library for pruning a model while finetuning or training.</li>
</ul></li>
<li>Applications that run on mobile and edge devices can have significant memory constraints.</li>
<li>Weight pruning gradually removes weight connections (and potentially neurons) during training such that the model becomes progressively sparser.</li>
<li>The resulting pruned model has fewer nonzero parameters, which we can store in a compact sparse matrix format.</li>
<li>We can combine pruning with quantization to obtain further compression.</li>
</ul>
<section id="sparsity-in-deep-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="sparsity-in-deep-neural-networks">Sparsity in Deep Neural Networks</h3>
</section>
<section id="weight-pruning-methods" class="level3">
<h3 class="anchored" data-anchor-id="weight-pruning-methods">Weight Pruning Methods</h3>
<ul>
<li>Most weight pruning methods calculate a matrix <span class="math inline">\(S\)</span> of importance scores and select the top <span class="math inline">\(k\)</span> percent of weights by importance.</li>
</ul>
</section>
<section id="top_ks_ij-1-text-if-s_ij-text-in-top-k-percent-else-0" class="level3">
<h3 class="anchored" data-anchor-id="top_ks_ij-1-text-if-s_ij-text-in-top-k-percent-else-0"><span class="math display">\[Top_{k}(S)_{ij} = 1 \text{ if } S_{ij} \text{ in top k percent else } 0\]</span></h3>
<ul>
<li><span class="math inline">\(k\)</span> acts as a new hyperparameter to control the amount of sparsity in the model.</li>
<li>Lower values of k correspond to sparser matrices.</li>
<li>We can use these scores to define a mask matrix <span class="math inline">\(M\)</span> that masks weights <span class="math inline">\(W_{ik}\)</span> during the forward pass with some input and effectively creates a sparse network of activations <span class="math inline">\(a_{i}\)</span>.</li>
</ul>
</section>
<section id="a_i-sum_kw_ikm_ikx_k" class="level3">
<h3 class="anchored" data-anchor-id="a_i-sum_kw_ikm_ikx_k"><span class="math display">\[a_{i} = \sum_{k}{W_{ik}M_{ik}x_{k}}\]</span></h3>
<section id="questions-to-consider" class="level4">
<h4 class="anchored" data-anchor-id="questions-to-consider">Questions to consider</h4>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html">Second order derivatives for network pruning: Optimal Brain Surgeon</a></p></li>
<li><p>Which weights should be pruned?</p></li>
<li><p>How should the remaining weights be adjuststed for best performance?</p></li>
<li><p>How can such network pruning be done in a computationally efficient way?</p></li>
</ul>
</section>
<section id="magnitude-pruning" class="level4">
<h4 class="anchored" data-anchor-id="magnitude-pruning">Magnitude pruning</h4>
<ul>
<li>Magnitude pruning calculates the scores according to the magnitude of the weights <span class="math display">\[S = \left( \left \vert W_{ij} \right \vert \right)_{1 \ \le \ j, j \ \le \ n}\]</span> and then derives the masks <span class="math display">\[M = Top_{k}(S)\]</span>.</li>
<li>It is common to apply magnitude iteratively by first training the model to learn which connections are important and pruning weights of least importance.
<ul>
<li><a href="https://arxiv.org/abs/1506.02626">Learning both Weights and Connections for Efficient Neural Networks</a></li>
</ul></li>
<li>It is generally better to gradually increase the initial sparsity <span class="math inline">\(s_{i}\)</span> to a final value <span class="math inline">\(s_{f}\)</span> after <span class="math inline">\(N\)</span> steps.
<ul>
<li><a href="https://arxiv.org/abs/1710.01878">To prune, or not to prune: exploring the efficacy of pruning for model compression</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="s_t-s_f-left-s_i---s_f-right-left-1---fract---t_0ndelta-t-right3-for-t-in-left-t_0t_0-delta-t-ldots-t_0-ndelta-t-right" class="level3">
<h3 class="anchored" data-anchor-id="s_t-s_f-left-s_i---s_f-right-left-1---fract---t_0ndelta-t-right3-for-t-in-left-t_0t_0-delta-t-ldots-t_0-ndelta-t-right"><span class="math display">\[s_{t} = s_{f} + \left( s_{i} - s_{f} \right) \left( 1 - \frac{t - t_{0}}{N\Delta t} \right)^{3} for t \in \left\{ t_{0},t_{0} + \Delta t, \ldots, t_{0} + N\Delta t \right\}\]</span></h3>
<ul>
<li><p>The idea is to update the binary masks <span class="math inline">\(M\)</span> every <span class="math inline">\(\Delta t\)</span> step to allow masked weights to reactivate during training and recover from any potential accuracy losses introduced by the pruning process.</p></li>
<li><p>The cubic factor implies the rate of pruning is highest in the early phases and gradually tapers off.</p></li>
<li><p>Magnitude pruning works for purely supervised learning, where the importance of each weight directly relates to the task at hand.</p></li>
<li><p>In transfer learning, the pretraining phase determines the importance of the weights, and magnitude pruning can remove connections needed for the fine-tuning task.</p></li>
</ul>
<p><strong>Plot the cubic sparsity scheduler used for pruning</strong></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _sparsity(t, t_0<span class="op">=</span><span class="dv">0</span>, dt<span class="op">=</span><span class="dv">1</span>, s_i<span class="op">=</span><span class="dv">0</span>, s_f<span class="op">=</span><span class="fl">0.9</span>, N<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s_f <span class="op">+</span> (s_i <span class="op">-</span> s_f) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> (t <span class="op">-</span> t_0) <span class="op">/</span> (N <span class="op">*</span> dt))<span class="op">**</span><span class="dv">3</span></span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">100</span>,<span class="dv">100</span>)</span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [_sparsity(t) <span class="cf">for</span> t <span class="kw">in</span> steps]</span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a>ax.plot(steps, values)</span>
<span id="cb153-9"><a href="#cb153-9" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb153-10"><a href="#cb153-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>,<span class="dv">100</span>)</span>
<span id="cb153-11"><a href="#cb153-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Pruning step"</span>)</span>
<span id="cb153-12"><a href="#cb153-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Sparsity"</span>)</span>
<span id="cb153-13"><a href="#cb153-13" aria-hidden="true" tabindex="-1"></a>plt.grid(linestyle<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb153-14"><a href="#cb153-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_240_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<hr>
<section id="movement-pruning" class="level4">
<h4 class="anchored" data-anchor-id="movement-pruning">Movement pruning</h4>
<ul>
<li><a href="https://arxiv.org/abs/2005.07683">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a></li>
<li>Movement pruning gradually removes weights during fine-tuning such that the model becomes progressively sparser.</li>
<li>We derive both the weights and scores through gradient descent during fine-tuning, meaning we also track the loss <span class="math inline">\(L\)</span> for the scores <span class="math inline">\(S_{ij}\)</span> in the backward pass.</li>
<li>We can then use the learned scores to generate the binary mask.</li>
</ul>
</section>
</section>
<section id="m-top_ks" class="level3">
<h3 class="anchored" data-anchor-id="m-top_ks"><span class="math display">\[M = Top_{k}(S)\]</span></h3>
<ul>
<li>The weights moving the most from zero are the most important ones to keep.</li>
<li>There is also a soft version of movement pruning where we use a global threshold <span class="math inline">\(\tau\)</span> to define the binary mask: <span class="math inline">\(M = \left( S \gt \tau \right)\)</span>.</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-7/">Notes on Transformers Book Ch. 7</a></p>
<p><strong>Next:</strong> <a href="../chapter-9/">Notes on Transformers Book Ch. 9</a></p>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2023, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>