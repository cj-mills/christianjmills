<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.336">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-25">
<meta name="description" content="Chapter 10 covers how to train a GPT-like model to generate Python source code from scratch.">

<title>Christian Mills - Notes on Transformers Book Ch. 10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on Transformers Book Ch. 10">
<meta property="og:description" content="Chapter 10 covers how to train a GPT-like model to generate Python source code from scratch.">
<meta property="og:image" content="christianjmills.com/posts/transformers-book-notes/chapter-10/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Notes on Transformers Book Ch. 10">
<meta name="twitter:description" content="Chapter 10 covers how to train a GPT-like model to generate Python source code from scratch.">
<meta name="twitter:image" content="christianjmills.com/posts/transformers-book-notes/chapter-10/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com" rel="" target=""><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#training-transformers-from-scratch" id="toc-training-transformers-from-scratch" class="nav-link active" data-scroll-target="#training-transformers-from-scratch">Training Transformers from Scratch</a></li>
  <li><a href="#project-python-source-code-generator" id="toc-project-python-source-code-generator" class="nav-link" data-scroll-target="#project-python-source-code-generator">Project: Python Source Code Generator</a>
  <ul class="collapse">
  <li><a href="#existing-ai-code-completion-products" id="toc-existing-ai-code-completion-products" class="nav-link" data-scroll-target="#existing-ai-code-completion-products">Existing AI Code Completion Products</a></li>
  <li><a href="#codeparrot" id="toc-codeparrot" class="nav-link" data-scroll-target="#codeparrot">CodeParrot</a></li>
  </ul></li>
  <li><a href="#large-datasets-and-where-to-find-them" id="toc-large-datasets-and-where-to-find-them" class="nav-link" data-scroll-target="#large-datasets-and-where-to-find-them">Large Datasets and Where to Find Them</a>
  <ul class="collapse">
  <li><a href="#challenges-of-building-a-large-scale-corpus" id="toc-challenges-of-building-a-large-scale-corpus" class="nav-link" data-scroll-target="#challenges-of-building-a-large-scale-corpus">Challenges of Building a Large-Scale Corpus</a></li>
  <li><a href="#compare-text-generations-from-gpt-and-gpt-2" id="toc-compare-text-generations-from-gpt-and-gpt-2" class="nav-link" data-scroll-target="#compare-text-generations-from-gpt-and-gpt-2">Compare text generations from GPT and GPT-2</a></li>
  <li><a href="#building-a-custom-code-dataset" id="toc-building-a-custom-code-dataset" class="nav-link" data-scroll-target="#building-a-custom-code-dataset">Building a Custom Code Dataset</a></li>
  <li><a href="#to-filter-the-noise-or-not" id="toc-to-filter-the-noise-or-not" class="nav-link" data-scroll-target="#to-filter-the-noise-or-not">To Filter the Noise or Not?</a></li>
  <li><a href="#working-with-large-datasets" id="toc-working-with-large-datasets" class="nav-link" data-scroll-target="#working-with-large-datasets">Working with Large Datasets</a></li>
  <li><a href="#adding-datasets-to-the-hugging-face-hub" id="toc-adding-datasets-to-the-hugging-face-hub" class="nav-link" data-scroll-target="#adding-datasets-to-the-hugging-face-hub">Adding Datasets to the Hugging Face Hub</a></li>
  </ul></li>
  <li><a href="#building-a-tokenizer" id="toc-building-a-tokenizer" class="nav-link" data-scroll-target="#building-a-tokenizer">Building a Tokenizer</a>
  <ul class="collapse">
  <li><a href="#the-tokenizer-model" id="toc-the-tokenizer-model" class="nav-link" data-scroll-target="#the-tokenizer-model">The Tokenizer Model</a></li>
  <li><a href="#measuring-tokenizer-performance" id="toc-measuring-tokenizer-performance" class="nav-link" data-scroll-target="#measuring-tokenizer-performance">Measuring Tokenizer Performance</a></li>
  <li><a href="#a-tokenizer-for-python" id="toc-a-tokenizer-for-python" class="nav-link" data-scroll-target="#a-tokenizer-for-python">A Tokenizer for Python</a></li>
  <li><a href="#training-a-tokenizer" id="toc-training-a-tokenizer" class="nav-link" data-scroll-target="#training-a-tokenizer">Training a Tokenizer</a></li>
  <li><a href="#saving-a-custom-tokenizer-on-the-hub" id="toc-saving-a-custom-tokenizer-on-the-hub" class="nav-link" data-scroll-target="#saving-a-custom-tokenizer-on-the-hub">Saving a Custom Tokenizer on the Hub</a></li>
  </ul></li>
  <li><a href="#training-a-model-from-scratch" id="toc-training-a-model-from-scratch" class="nav-link" data-scroll-target="#training-a-model-from-scratch">Training a Model from Scratch</a>
  <ul class="collapse">
  <li><a href="#a-tale-of-pretraining-objectives" id="toc-a-tale-of-pretraining-objectives" class="nav-link" data-scroll-target="#a-tale-of-pretraining-objectives">A Tale of Pretraining Objectives</a></li>
  <li><a href="#initializing-the-model" id="toc-initializing-the-model" class="nav-link" data-scroll-target="#initializing-the-model">Initializing the Model</a></li>
  <li><a href="#implementing-the-dataloader" id="toc-implementing-the-dataloader" class="nav-link" data-scroll-target="#implementing-the-dataloader">Implementing the Dataloader</a></li>
  <li><a href="#defining-the-training-loop" id="toc-defining-the-training-loop" class="nav-link" data-scroll-target="#defining-the-training-loop">Defining the Training Loop</a></li>
  <li><a href="#the-training-run" id="toc-the-training-run" class="nav-link" data-scroll-target="#the-training-run">The Training Run</a></li>
  </ul></li>
  <li><a href="#results-and-analysis" id="toc-results-and-analysis" class="nav-link" data-scroll-target="#results-and-analysis">Results and Analysis</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 10</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 10 covers how to train a GPT-like model to generate Python source code from scratch.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 25, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<ul>
<li><a href="#training-transformers-from-scratch">Training Transformers from Scratch</a></li>
<li><a href="#project-python-source-code-generator">Project: Python Source Code Generator</a></li>
<li><a href="#large-datasets-and-where-to-find-them">Large Datasets and Where to Find Them</a></li>
<li><a href="#building-a-tokenizer">Building a Tokenizer</a></li>
<li><a href="#training-a-model-from-scratch">Training a Model from Scratch</a></li>
<li><a href="#results-and-analysis">Results and Analysis</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>transformers.__version__, datasets.__version__, accelerate.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('4.18.0', '2.1.0', '0.5.1')</code></pre>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="training-transformers-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="training-transformers-from-scratch">Training Transformers from Scratch</h2>
<ul>
<li>Efficiently training large models from scratch requires special tools for distributed training.</li>
</ul>
</section>
<section id="project-python-source-code-generator" class="level2">
<h2 class="anchored" data-anchor-id="project-python-source-code-generator">Project: Python Source Code Generator</h2>
<ul>
<li>The goal is to train a GPT-like model to generate Python source code.</li>
</ul>
<section id="existing-ai-code-completion-products" class="level3">
<h3 class="anchored" data-anchor-id="existing-ai-code-completion-products">Existing AI Code Completion Products</h3>
<ul>
<li><a href="https://copilot.github.com/">GitHub Copilot</a></li>
<li><a href="https://www.tabnine.com/">TabNine</a></li>
<li><a href="https://www.kite.com/">Kite</a></li>
</ul>
</section>
<section id="codeparrot" class="level3">
<h3 class="anchored" data-anchor-id="codeparrot">CodeParrot</h3>
<ul>
<li><a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot">GitHub Repository</a></li>
<li>CodeParrot is a GPT-2 model trained from scratch on Python code.</li>
</ul>
</section>
</section>
<section id="large-datasets-and-where-to-find-them" class="level2">
<h2 class="anchored" data-anchor-id="large-datasets-and-where-to-find-them">Large Datasets and Where to Find Them</h2>
<ul>
<li>Many domains often have large amounts of data available such as legal documents, biomedical databases, and programming codebases.</li>
<li>Large datasets can usually only be labeled using heuristics or accompanying metadata.</li>
<li>We can still use large unlabeled datasets to fine-tune language models for domain adaptation.</li>
<li>Using a pretrained model forces you to use the model’s corresponding tokenizer.</li>
<li>Using a tokenizer trained on a corpus from a different domain is typically suboptimal.</li>
</ul>
<section id="challenges-of-building-a-large-scale-corpus" class="level3">
<h3 class="anchored" data-anchor-id="challenges-of-building-a-large-scale-corpus">Challenges of Building a Large-Scale Corpus</h3>
<ul>
<li>The model will inherit any defects in the pretraining corpus.</li>
<li>It becomes more difficult to control or fully understand the contents of a dataset the larger it gets.</li>
<li>Most exceedingly large datasets are not handcrafted.</li>
<li>Creating large-scale datasets typically requires using data generated as a side effect of other activities.</li>
<li>The high degree of automation used to create large-scale datasets means there is limited control over the content and the method to create them.</li>
<li>There is an increased risk of training a model on lower-quality and biased data.</li>
<li>A significant portion of the C4 corpus used to train T5 is machine-translated rather than human-translated.</li>
<li>The stopword filtering in C4 disproportionately removed African-American English from the corpus.</li>
<li>It is challenging to find a middle ground between including too much explicit content and erasing all mention of sexuality or gender.</li>
<li>Common words like “sex” are absent from C4.</li>
<li>There are many copyright violations in the Bookcorpus dataset used to train BERT.</li>
<li>Bookcorpus also contains genre-skew toward “romance” novels.</li>
<li><a href="https://arxiv.org/abs/1506.06724">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</a></li>
<li><a href="https://arxiv.org/abs/2105.05241">Addressing “Documentation Debt” in Machine Learning Research: A Retrospective Datasheet for BookCorpus</a></li>
</ul>
</section>
<section id="compare-text-generations-from-gpt-and-gpt-2" class="level3">
<h3 class="anchored" data-anchor-id="compare-text-generations-from-gpt-and-gpt-2">Compare text generations from GPT and GPT-2</h3>
<ul>
<li>The original GPT model trained predominately on BookCorpus.</li>
<li>GPT-2 trained on web pages, blogs, and news articles linked from Reddit.</li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline, set_seed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialze text generation pipelines with the original GPT and GPT-2</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>generation_gpt <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"openai-gpt"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>generation_gpt2 <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The main difference between the two models is the pretraining dataset.</p>
<hr>
<p><strong>Compare the model sizes</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_size(model):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(t.numel() <span class="cf">for</span> t <span class="kw">in</span> model.parameters())</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT  size: </span><span class="sc">{</span>model_size(generation_gpt.model)<span class="op">/</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.1f}</span><span class="ss">M parameters"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT2 size: </span><span class="sc">{</span>model_size(generation_gpt2.model)<span class="op">/</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.1f}</span><span class="ss">M parameters"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    GPT  size: 116.5M parameters
    GPT2 size: 124.4M parameters</code></pre>
<p><strong>Note:</strong> The original GPT model is approximately the same size as the smallest GPT-2 variant.</p>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to generate text using a prompt</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> enum_pipeline_ouputs(pipe, prompt, num_return_sequences):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> pipe(prompt, num_return_sequences<span class="op">=</span>num_return_sequences,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>               clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(<span class="ss">f"</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">."</span> <span class="op">+</span> s[<span class="st">"generated_text"</span>] <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare the output of the two models</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">When they came back"</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPT completions:</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> enum_pipeline_ouputs(generation_gpt, prompt, <span class="dv">3</span>))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">""</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GPT-2 completions:</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> enum_pipeline_ouputs(generation_gpt2, prompt, <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    GPT completions:
    1.
    When they came back. 
     " we need all we can get, " jason said once they had settled into the back of the truck without anyone stopping them. " after getting out here, it 'll be up to us what to find. for now
    2.
    When they came back. 
     his gaze swept over her body. he 'd dressed her, too, in the borrowed clothes that she 'd worn for the journey. 
     " i thought it would be easier to just leave you there. " a woman like
    3.
    When they came back to the house and she was sitting there with the little boy. 
     " don't be afraid, " he told her. she nodded slowly, her eyes wide. she was so lost in whatever she discovered that tom knew her mistake
    
    GPT-2 completions:
    1.
    When they came back we had a big dinner and the other guys went to see what their opinion was on her. I did an hour and they were happy with it.
    2.
    When they came back to this island there had been another massacre, but he could not help but feel pity for the helpless victim who had been left to die, and that they had failed that day. And so was very, very grateful indeed.
    3.
    When they came back to our house after the morning, I asked if she was sure. She said, "Nope." The two kids were gone that morning. I thought they were back to being a good friend.
    
    When Dost</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The text generated with the original GPT model has a distinctive romance skew.</li>
<li>GPT-2 generates more neutral text containing blog-like or adventure-related elements.</li>
<li>A model reflects the language bias and over or underrepresentation of populations of the dataset used to train it.</li>
<li>We need to consider the model’s biases concerning the target audience.</li>
<li><a href="https://arxiv.org/abs/2010.13561">Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure</a></li>
</ul>
<hr>
</section>
<section id="building-a-custom-code-dataset" class="level3">
<h3 class="anchored" data-anchor-id="building-a-custom-code-dataset">Building a Custom Code Dataset</h3>
<ul>
<li>We can obtain a pretraining corpus of Python code from GitHub repositories.</li>
<li>We can access GitHub repositories via the <a href="https://docs.github.com/en/rest/guides/getting-started-with-the-rest-api">GitHub REST API</a> or public dataset inventories like <a href="https://console.cloud.google.com/marketplace/product/github/github-repos?pli=1&amp;project=majestic-vault-303101">Google BigQuery</a>.</li>
<li>The GitHub REST API is rate limited but provides access to additional attributes like star and downstream usage information.</li>
<li>The <a href="https://libraries.io/">Libraries.io</a> service monitors open source packages.</li>
</ul>
<section id="bigquery-public-data.github_repos.contents-table" class="level4">
<h4 class="anchored" data-anchor-id="bigquery-public-data.github_repos.contents-table"><code>bigquery-public-data.github_repos.contents</code> table</h4>
<ul>
<li>The <a href="https://console.cloud.google.com/bigquery?project=bigquery-public-data&amp;page=table&amp;t=contents&amp;d=github_repos&amp;p=bigquery-public-data&amp;redirect_from_classic=true&amp;ws=!1m5!1m4!4m3!1sbigquery-public-data!2sgithub_repos!3scontents"><code>bigquery-public-data.github_repos.contents</code> table</a> contains copies of all ASCII files less than 10MB in size.</li>
</ul>
</section>
<section id="codesearchnet-corpus" class="level4">
<h4 class="anchored" data-anchor-id="codesearchnet-corpus">CodeSearchNet corpus</h4>
<ul>
<li>The CodeSearchNet corpus contains 2 million comment-code pairs from open-source libraries hosted on GitHub.</li>
<li>It contains code and documentation for several programming languages.</li>
<li><a href="https://huggingface.co/datasets/code_search_net">Hugging Face Dataset Card</a></li>
</ul>
</section>
<section id="creating-a-dataset-with-google-bigquery" class="level4">
<h4 class="anchored" data-anchor-id="creating-a-dataset-with-google-bigquery">Creating a dataset with Google BigQuery</h4>
<ul>
<li><a href="https://arxiv.org/abs/2006.03511">Unsupervised Translation of Programming Languages</a></li>
</ul>
<p><strong>Steps to export Python files</strong></p>
<ol type="1">
<li>Create a Google Cloud account.</li>
<li>Create a Google BigQuery project under your account.</li>
<li>Create a dataset inside the project.</li>
<li>Create a table in the dataset to store the results of the SQL request.</li>
<li>Prepare the following SQL query and specify a destination table</li>
</ol>
<div class="sourceCode" id="cb12"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    f.repo_name, f.path, c.copies, c.<span class="kw">size</span>, c.content, l.license</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    `bigquery<span class="op">-</span><span class="kw">public</span><span class="op">-</span><span class="kw">data</span>.github_repos.files` <span class="kw">AS</span> f</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">JOIN</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    `bigquery<span class="op">-</span><span class="kw">public</span><span class="op">-</span><span class="kw">data</span>.github_repos.contents` <span class="kw">AS</span> c</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="kw">ON</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    f.<span class="kw">id</span> <span class="op">=</span> c.<span class="kw">id</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">JOIN</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    `bigquery<span class="op">-</span><span class="kw">public</span><span class="op">-</span><span class="kw">data</span>.github_repos.licenses` <span class="kw">as</span> l</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">ON</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    f.repo_name <span class="op">=</span> l.repo_name</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">NOT</span> c.binary</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">AND</span> ((F.path <span class="kw">LIKE</span> <span class="st">'%.py'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="kw">AND</span> (c.<span class="kw">size</span> <span class="kw">BETWEEN</span> <span class="dv">1024</span> <span class="kw">and</span> <span class="dv">1048575</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="6" type="1">
<li>Run the query</li>
</ol>
<p><strong>Note:</strong> Encoutered the following error when attempting to run the query</p>
<pre class="text"><code>Quota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas </code></pre>
<ul>
<li>The above command processes about 2.6TB of data to extract 26.8 million files.</li>
<li>The resulting dataset contains about 50 GB of compressed JSON files.</li>
<li>The dataset is about 200GB when uncompressed.</li>
<li>Each JSON file contains source code from Python files.</li>
<li>The query filters empty files like <code>__init__.py</code> files and files larger than 1MB.</li>
<li>The query includes the licenses for the files so we can filter the training data later on.</li>
</ul>
<p><strong>Steps to download results from Google Cloud</strong></p>
<ol type="1">
<li>Export results to Google Cloud
<ol type="a">
<li>Create a bucket and a folder in Google Cloud Storage (GCS).</li>
<li>Export your table to this bucket by selecting Export &gt; Export to GCS, with a JSON export format and gzip compression.</li>
</ol></li>
<li>Download the bucket to your local machine using <a href="https://cloud.google.com/storage/docs/gsutil">gsutil</a></li>
</ol>
<ol type="a">
<li>Install gsutil with pip install gsutil.
<ol start="2" type="a">
<li>Configure gsutil with your Google account: gsutil config.</li>
<li>Copy your bucket on your machine:</li>
</ol>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">gsutil</span> <span class="at">-m</span> <span class="at">-o</span> <span class="st">"GSUtil:parallel_process_count=1"</span> cp <span class="at">-r</span> gs://<span class="op">&lt;</span>name_of_bucket<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
<p><strong>Alternative: Download the dataset from Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://huggingface.co/datasets/transformersbook/codeparrot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="to-filter-the-noise-or-not" class="level3">
<h3 class="anchored" data-anchor-id="to-filter-the-noise-or-not">To Filter the Noise or Not?</h3>
<ul>
<li>Data preparation is crucial, and we should clean the dataset as much as possible.</li>
<li>The quality of code in GitHub repositories varies greatly.</li>
<li>Having some noise in the training dataset makes our code generation system robust to noisy inputs at inference time but also makes predictions more random.</li>
<li>The intended use case and whole-system integration determine whether you want more or less noisy data and add pre and post-filtering operations.</li>
</ul>
<section id="potential-steps-to-clean-dataset" class="level4">
<h4 class="anchored" data-anchor-id="potential-steps-to-clean-dataset">Potential steps to clean dataset</h4>
<ul>
<li>Filter code based on stars or usage information.</li>
<li>Code with more stars or higher usage is more likely to be higher quality.</li>
<li>Remove duplicated code samples.</li>
<li>Consider copyright information.</li>
<li>Investigate the language used in the documentation, comments, or docstrings.</li>
<li>Remove personal identifying information such as passwords or keys.</li>
</ul>
</section>
</section>
<section id="working-with-large-datasets" class="level3">
<h3 class="anchored" data-anchor-id="working-with-large-datasets">Working with Large Datasets</h3>
<ul>
<li>Working with large datasets requires additional considerations regarding disk space and RAM usage.</li>
<li>It is common for datasets to be larger than the available RAM.</li>
<li>The Hugging Face Datasets library provides memory mapping and streaming functionality to address RAM and disk space limitations.</li>
</ul>
<section id="memory-mapping" class="level4">
<h4 class="anchored" data-anchor-id="memory-mapping">Memory mapping</h4>
<ul>
<li>Hugging Face Datasets uses a mechanism for zero-copy and zero-overhead memory mapping.</li>
<li>The mechanism caches each dataset in a file that directly reflects the content in RAM.</li>
<li>Hugging Face Datasets opens a read-only pointer to this file and uses it as a substitute for RAM.</li>
</ul>
<hr>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, DownloadConfig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Decompress and load the downloaded dataset from the local folder</strong></p>
<blockquote class="blockquote">
<p><strong>Note:</strong> The following code block assumes that you have downloaded the BigQuery dataset to a folder called <code>codeparrot</code>. We suggest skipping this step since it will unpack the compressed files and require ~180GB of disk space. This code is just for demonstration purposes and you can just continue below with the streamed dataset which will not consume that much disk space.</p>
</blockquote>
<hr>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>download_config <span class="op">=</span> DownloadConfig(delete_extracted<span class="op">=</span><span class="va">True</span>, cache_dir<span class="op">=</span><span class="st">"/mnt/980SSD/Datasets/codeparrot-cache"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"/mnt/980SSD/Datasets/codeparrot"</span>, cache_dir<span class="op">=</span><span class="st">"/mnt/980SSD/Datasets/codeparrot-cache"</span>, split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                       download_config<span class="op">=</span>download_config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Dataset json downloaded and prepared to /mnt/980SSD/Datasets/codeparrot-cache/json/codeparrot-43fc192cc9f62326/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The <code>delete_extracted=True</code> argument deletes the extracted files to free up disk space.</li>
<li>The Hugging Face Datasets library extracted and read the compressed JSON files by loading them in a single optimized cache file.</li>
</ul>
<hr>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> psutil, os</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Check the size of the cached dataset</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of python files code in dataset : </span><span class="sc">{</span><span class="bu">len</span>(dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ds_size <span class="op">=</span> <span class="bu">sum</span>(os.stat(f[<span class="st">"filename"</span>]).st_size <span class="cf">for</span> f <span class="kw">in</span> dataset.cache_files)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># os.stat.st_size is expressed in bytes, so we convert to GB</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset size (cache file) : </span><span class="sc">{</span>ds_size <span class="op">/</span> <span class="dv">2</span><span class="op">**</span><span class="dv">30</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Process.memory_info is expressed in bytes, so we convert to MB</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RAM used: </span><span class="sc">{</span>psutil<span class="sc">.</span>Process(os.getpid())<span class="sc">.</span>memory_info()<span class="sc">.</span>rss <span class="op">&gt;&gt;</span> <span class="dv">20</span><span class="sc">}</span><span class="ss"> MB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Number of python files code in dataset : 18695559
    Dataset size (cache file) : 183.68 GB
    RAM used: 4359 MB</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The dataset is much larger than the available RAM, but we can still load and access it.</li>
<li>NLP data is typically lightweight to load compared to the model processing computations.</li>
<li>The zero-copy/zero-overhead format uses Apache Arrow under the hood for efficiency.</li>
</ul>
<hr>
</section>
<section id="streaming" class="level4">
<h4 class="anchored" data-anchor-id="streaming">Streaming</h4>
<ul>
<li>Some datasets are too large to fit in most hard drives.</li>
<li>The Hugging Face Datasets library supports streaming many compressed and uncompressed file formats that we can read line-by-line.</li>
<li>Hugging Face Datasets opens and reads compressed JSON files on the fly in streaming mode.</li>
<li>Streamed datasets are of the type <a href="https://huggingface.co/docs/datasets/v2.1.0/en/package_reference/main_classes#datasets.IterableDataset"><code>IterableDataset</code></a>.</li>
<li>We cannot access random elements and need to read them in order.</li>
<li>Methods like <code>shuffle()</code> operate by fetching a buffer of examples and shuffling within this buffer.</li>
<li>The samples of a streamed dataset are identical to those of a nonstreamed dataset.</li>
<li>Streamed datasets do not generate a cache file on the drive or require significant RAM.</li>
<li>Individual batches load into memory as requested, reducing the memory footprint.</li>
<li>We can also stream remote datasets from the Hugging Face Hub, allowing us to use arbitrarily large datasets on small servers.</li>
</ul>
<hr>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>streamed_dataset <span class="op">=</span> load_dataset(<span class="st">"/mnt/980SSD/Datasets/codeparrot"</span>, split<span class="op">=</span><span class="st">"train"</span>, streaming<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    AttributeError: '_io.BufferedReader' object has no attribute 'loc'</code></pre>
<hr>
<p><strong>Iterate through the streamed dataset</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>iterator <span class="op">=</span> <span class="bu">iter</span>(streamed_dataset)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="dv">0</span>] <span class="op">==</span> <span class="bu">next</span>(iterator))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="dv">1</span>] <span class="op">==</span> <span class="bu">next</span>(iterator))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Stream a remote dataset</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>remote_dataset <span class="op">=</span> load_dataset(<span class="st">'transformersbook/codeparrot'</span>, split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                              streaming<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="adding-datasets-to-the-hugging-face-hub" class="level3">
<h3 class="anchored" data-anchor-id="adding-datasets-to-the-hugging-face-hub">Adding Datasets to the Hugging Face Hub</h3>
<ul>
<li>Pushing our dataset to the Hugging Face Hub allows us to access it from a training server and share it with the community.</li>
</ul>
<section id="command-line-steps" class="level4">
<h4 class="anchored" data-anchor-id="command-line-steps">Command Line Steps</h4>
<ol type="1">
<li>Log into Hugging Face account</li>
</ol>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Create a new dataset repository on the Hub for the training split</li>
</ol>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> repo create <span class="at">--type</span> dataset codeparrot-train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>Create a new dataset repository on the Hub for the validation split</li>
</ol>
<div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> repo create <span class="at">--type</span> dataset codeparrot-valid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="4" type="1">
<li>Clone the training repository</li>
</ol>
<div class="sourceCode" id="cb29"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> repo create <span class="at">--type</span> dataset codeparrot-train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="5" type="1">
<li>Clone the validation repository</li>
</ol>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> repo create <span class="at">--type</span> dataset codeparrot-valid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="6" type="1">
<li>Copy all but the last GitHub file to the as the training set</li>
</ol>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> codeparrot-train</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> ../codeparrot/<span class="pp">*</span>.json.gz .</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span> ./file-000000000183.json.gz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="7" type="1">
<li>Commit the files and push them to the Hub</li>
</ol>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> add .</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">"Adding dataset files"</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> push</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="8" type="1">
<li>Repeat the process for the validation set</li>
</ol>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ../codeparrot-valid</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> ../codeparrot/file-000000000183.json.gz</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mv</span> ./file-000000000183.json.gz ./file-000000000183_validation.json.gz</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> add .</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">"Adding dataset files"</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> push</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>It is good practice to add README cards that explain how the datasets were created and provide as much helpful information as possible.</li>
<li>A well-documented dataset is more likely to be valuable to other people, including the future you.</li>
<li><a href="https://github.com/huggingface/datasets/blob/master/templates/README_guide.md">Hugging Face Dataset Card Creation Guide</a></li>
</ul>
</section>
</section>
</section>
<section id="building-a-tokenizer" class="level2">
<h2 class="anchored" data-anchor-id="building-a-tokenizer">Building a Tokenizer</h2>
<ul>
<li>It is crucial to stick with the same preprocessing design choices used during the pretraining process when using a pretrained model.</li>
<li>Using a tokenizer prepared for another dataset when training a new model can be suboptimal.
<ul>
<li>The T5 tokenizer uses extensive stopword filtering and is unaware of some common English words like “sex.”</li>
<li>The CamemBERT tokenizer is only trained on French text and is unaware of common English words such as “being.”</li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tok_list(tokenizer, string):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokenizer(string, add_special_tokens<span class="op">=</span><span class="va">False</span>)[<span class="st">"input_ids"</span>]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [tokenizer.decode(tok) <span class="cf">for</span> tok <span class="kw">in</span> input_ids]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Initialize tokenizers using the pretrained T5 and CamemBERT model vocabularies</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>tokenizer_T5 <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"t5-base"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>tokenizer_camembert <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"camembert-base"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test the limitations of the T5 and CamemBERT tokenizers</strong></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'T5 tokens for "sex": </span><span class="sc">{</span>tok_list(tokenizer_T5,<span class="st">"sex"</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'CamemBERT tokens for "being": </span><span class="sc">{</span>tok_list(tokenizer_camembert,<span class="st">"being"</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    T5 tokens for "sex": ['', 's', 'ex']
    CamemBERT tokens for "being": ['be', 'ing']</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>Splitting such short and common words into subparts is often inefficient as it increases the sequence length of the model.</li>
<li>It is essential to consider the domain and the preprocessing of the dataset used to train a tokenizer.</li>
<li>The tokenizer and model can encode bias from the dataset that impacts the downstream behavior of the model.</li>
</ul>
<hr>
<section id="the-tokenizer-model" class="level3">
<h3 class="anchored" data-anchor-id="the-tokenizer-model">The Tokenizer Model</h3>
<ul>
<li>Training a tokenizer is a way to create an optimal mapping from a string of text to a list of integers that the model can ingest.</li>
<li>The optimal string-to-integer conversion involves a vocabulary consisting of a list of atomic strings and an associated method to convert, normalize, cut, or map a text string into a list of indices with this vocabulary.</li>
<li>The list of indices is the input for the neural network.</li>
<li>The tokenizer processing pipeline involves normalization, pre-tokenization, the tokenizer model, and postprocessing.</li>
<li>The tokenizer model trains on a corpus.</li>
<li>Several subword tokenization algorithms are available, such as BPE, WordPiece, and Unigram.</li>
<li>BPE starts from a list of single characters and creates a vocabulary by progressively creating new tokens formed by merging the most frequently co-occurring basic units and adding them to the list.</li>
<li>This process continues until we reach the predefined vocabulary size.</li>
<li>Unigram initializes its base vocabulary with all the words in the corpus and potential subwords and progressively removes or splits the less helpful tokens until it reaches the target vocab size.</li>
<li>The impact of the chosen tokenization algorithm on downstream performance varies based on the task.</li>
<li>It is difficult to identify if one algorithm is better than the others.</li>
<li>Both BPE and Unigram perform reasonably well in most cases.</li>
</ul>
</section>
<section id="measuring-tokenizer-performance" class="level3">
<h3 class="anchored" data-anchor-id="measuring-tokenizer-performance">Measuring Tokenizer Performance</h3>
<ul>
<li>It is challenging to measure a tokenizer’s optimality and performance in practice.</li>
<li>Subword fertility calculates the average number of subwords produced per tokenized word.</li>
<li>The proportion of continued words refers to the amount of tokenized words in a corpus split into at least two subtokens.</li>
<li>Coverage metrics track information like the proportion of unknown words or rarely used tokens in a tokenized corpus.</li>
<li>We often estimate the robustness to misspelling or noise and model performance on such out-of-domain examples.</li>
<li>These measures provide different views on tokenizer performance.</li>
<li>However, they tend to ignore the interaction of the tokenizer with the model.</li>
<li>The best way to evaluate tokenizers is using the downstream performance of the model.</li>
</ul>
</section>
<section id="a-tokenizer-for-python" class="level3">
<h3 class="anchored" data-anchor-id="a-tokenizer-for-python">A Tokenizer for Python</h3>
<ul>
<li>Using a natural language pre-tokenizer for Python code might be suboptimal.</li>
<li>Indentation has semantic meaning in Python code.</li>
<li>Splitting on all whitespaces and removing them would remove valuable indentation information.</li>
<li>Line breaks are not meaningful in Python code, and we can remove them without issue.</li>
<li>Underscores can be part of single variable names and would not to use for splitting text.</li>
<li>Byte-level tokenizers preserve spaces and might be a good candidate for tokenizing code.</li>
<li>Python has a built-in tokenize module that splits Python code strings into meaningful units.
<ul>
<li>This approach is slow since it is Python-based and limited by the Python global interpreter lock (GIL).</li>
</ul></li>
<li>Most tokenizers provided by the Hugging Face Tokenizers library are in Rust and many orders of magnitude faster to train and use.</li>
</ul>
<hr>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test the byte-level GPT-2 tokenizer on Python code</strong></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>python_code <span class="op">=</span> <span class="vs">r"""def say_hello():</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="vs">    print("Hello, World!")</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="vs"># Print it</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="vs">say_hello()</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="vs">"""</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>python_code</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'def say_hello():\n    print("Hello, World!")\n# Print it\nsay_hello()\n'</code></pre>
<hr>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenizer(python_code).tokens()).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
<th>
25
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
Ġsay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
():
</td>
<td>
Ċ
</td>
<td>
Ġ
</td>
<td>
Ġ
</td>
<td>
Ġ
</td>
<td>
Ġprint
</td>
<td>
(“
</td>
<td>
Hello
</td>
<td>
,
</td>
<td>
ĠWorld
</td>
<td>
!“
</td>
<td>
)
</td>
<td>
Ċ
</td>
<td>
#
</td>
<td>
ĠPrint
</td>
<td>
Ġit
</td>
<td>
Ċ
</td>
<td>
say
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
()
</td>
<td>
Ċ
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Inspect the normalization step</strong></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.backend_tokenizer.normalizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    None</code></pre>
<p><strong>Note:</strong> The GPT-2 tokenizer does not use normalization and works directly on raw Unicode inputs.</p>
<hr>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Inspect the pre-tokenization step</strong></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
(0, 3)
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Ġsay
</td>
<td>
(3, 7)
</td>
</tr>
<tr>
<th>
2
</th>
<td>
_
</td>
<td>
(7, 8)
</td>
</tr>
<tr>
<th>
3
</th>
<td>
hello
</td>
<td>
(8, 13)
</td>
</tr>
<tr>
<th>
4
</th>
<td>
():
</td>
<td>
(13, 16)
</td>
</tr>
<tr>
<th>
5
</th>
<td>
ĊĠĠĠ
</td>
<td>
(16, 20)
</td>
</tr>
<tr>
<th>
6
</th>
<td>
Ġprint
</td>
<td>
(20, 26)
</td>
</tr>
<tr>
<th>
7
</th>
<td>
(“
</td>
<td>
(26, 28)
</td>
</tr>
<tr>
<th>
8
</th>
<td>
Hello
</td>
<td>
(28, 33)
</td>
</tr>
<tr>
<th>
9
</th>
<td>
,
</td>
<td>
(33, 34)
</td>
</tr>
<tr>
<th>
10
</th>
<td>
ĠWorld
</td>
<td>
(34, 40)
</td>
</tr>
<tr>
<th>
11
</th>
<td>
!“)
</td>
<td>
(40, 43)
</td>
</tr>
<tr>
<th>
12
</th>
<td>
Ċ
</td>
<td>
(43, 44)
</td>
</tr>
<tr>
<th>
13
</th>
<td>
#
</td>
<td>
(44, 45)
</td>
</tr>
<tr>
<th>
14
</th>
<td>
ĠPrint
</td>
<td>
(45, 51)
</td>
</tr>
<tr>
<th>
15
</th>
<td>
Ġit
</td>
<td>
(51, 54)
</td>
</tr>
<tr>
<th>
16
</th>
<td>
Ċ
</td>
<td>
(54, 55)
</td>
</tr>
<tr>
<th>
17
</th>
<td>
say
</td>
<td>
(55, 58)
</td>
</tr>
<tr>
<th>
18
</th>
<td>
_
</td>
<td>
(58, 59)
</td>
</tr>
<tr>
<th>
19
</th>
<td>
hello
</td>
<td>
(59, 64)
</td>
</tr>
<tr>
<th>
20
</th>
<td>
()
</td>
<td>
(64, 66)
</td>
</tr>
<tr>
<th>
21
</th>
<td>
Ċ
</td>
<td>
(66, 67)
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>Hugging Face Tokenizers provides an offset tracking feature for switching between strings and tokens.</li>
<li>Hugging Face Tokenizers tracks all operations on the input string so that it is possible to know what part of the input string corresponds to a token after tokenization.</li>
<li>The numbers in the above output indicate where each token originates in the original string.</li>
<li>The word “hello” corresponds to the characters 8 to 13 in the original string.</li>
<li>Each Unicode character is composed of between 1 and 4 bytes.</li>
<li>There are 143,859 Unicode characters and 256 elements in the byte alphabet.</li>
<li>We can express each Unicode character as a sequence of bytes.</li>
<li>We can have a model using an alphabet of only 256 words and process any Unicode string.</li>
</ul>
<hr>
<p><strong>Check the representations of some Unicode characters</strong></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>a, e <span class="op">=</span> <span class="st">u"a"</span>, <span class="st">u"€"</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>byte <span class="op">=</span> <span class="bu">ord</span>(a.encode(<span class="st">"utf-8"</span>))</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'`</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">` is encoded as `</span><span class="sc">{</span>a<span class="sc">.</span>encode(<span class="st">"utf-8"</span>)<span class="sc">}</span><span class="ss">` with a single byte: </span><span class="sc">{</span>byte<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>byte <span class="op">=</span> [<span class="bu">ord</span>(<span class="bu">chr</span>(i)) <span class="cf">for</span> i <span class="kw">in</span> e.encode(<span class="st">"utf-8"</span>)]</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'`</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">` is encoded as `</span><span class="sc">{</span>e<span class="sc">.</span>encode(<span class="st">"utf-8"</span>)<span class="sc">}</span><span class="ss">` with three bytes: </span><span class="sc">{</span>byte<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    `a` is encoded as `b'a'` with a single byte: 97
    `€` is encoded as `b'\xe2\x82\xac'` with three bytes: [226, 130, 172]</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>Building our vocabulary from the 143,859 Unicode characters would make the model’s embedding layer extremely large.</li>
<li>Using only the 256 byte-values as the vocabulary would result in longer input sequences.
<ul>
<li><a href="https://arxiv.org/abs/2105.13626">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a>
<ul>
<li>The ByT5 paper provides a details study of the overhead from using byte values for our vocabulary.</li>
</ul></li>
</ul></li>
<li>The BPE algorithm constructs a medium-sized vocabulary by extending the 256 byte-values with the most common combinations of bytes.</li>
<li>The name, Byte-Pair Encoding, comes from a data compression technique proposed by Philip Gage in 1994, which operated on bytes.
<ul>
<li><a href="https://thesai.org/Publications/ViewPaper?Volume=3&amp;Issue=8&amp;Code=IJACSA&amp;SerialNo=3">A New Algorithm for Data Compression Optimization</a></li>
</ul></li>
<li>Standard BPE algorithms in NLP typically operate on Unicode strings rather than bytes.
<ul>
<li>A recent type of BPE that works specifically on bytes is called byte-level BPE.</li>
</ul></li>
<li>The BPE algorithms are designed to work with clean Unicode strings as inputs, not bytes, and expect regular ASCII characters in the inputs without spaces or control characters.</li>
<li>Many Unicode control characters correspond to the 256 first bytes.</li>
<li>The GPT-2 tokenizer maps all the 256 input bytes to printable Unicode characters, which the BPE algorithms can digest.</li>
</ul>
<hr>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.models.gpt2.tokenization_gpt2 <span class="im">import</span> bytes_to_unicode</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Inspect the GPT-2 mapping of bytes to Unicode characters</strong></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>byte_to_unicode_map <span class="op">=</span> bytes_to_unicode()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>unicode_to_byte_map <span class="op">=</span> <span class="bu">dict</span>((v, k) <span class="cf">for</span> k, v <span class="kw">in</span> byte_to_unicode_map.items())</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>base_vocab <span class="op">=</span> <span class="bu">list</span>(unicode_to_byte_map.keys())</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Size of our base vocabulary: </span><span class="sc">{</span><span class="bu">len</span>(base_vocab)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'First element: `</span><span class="sc">{</span>base_vocab[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">`, last element: `</span><span class="sc">{</span>base_vocab[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">`'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Size of our base vocabulary: 256
First element: `!`, last element: `Ń`</code></pre>
<hr>
<p><strong>Examples of character mappings in BPE</strong></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>byte_to_unicode_map <span class="op">=</span> bytes_to_unicode()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>unicode_to_byte_map <span class="op">=</span> <span class="bu">dict</span>((v, k) <span class="cf">for</span> k, v <span class="kw">in</span> byte_to_unicode_map.items())</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>base_vocab <span class="op">=</span> <span class="bu">list</span>(unicode_to_byte_map.keys())</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'Regular characters'</span>, <span class="st">'`a` and `?`'</span>, <span class="ss">f'</span><span class="sc">{</span><span class="bu">ord</span>(<span class="st">"a"</span>)<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span><span class="bu">ord</span>(<span class="st">"?"</span>)<span class="sc">}</span><span class="ss">'</span> , <span class="ss">f'`</span><span class="sc">{</span>byte_to_unicode_map[<span class="bu">ord</span>(<span class="st">"a"</span>)]<span class="sc">}</span><span class="ss">` and `</span><span class="sc">{</span>byte_to_unicode_map[<span class="bu">ord</span>(<span class="st">"?"</span>)]<span class="sc">}</span><span class="ss">`'</span>],</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'Nonprintable control character (carriage return)'</span>, <span class="st">'`U+000D`'</span>, <span class="ss">f'13'</span>, <span class="ss">f'`</span><span class="sc">{</span>byte_to_unicode_map[<span class="dv">13</span>]<span class="sc">}</span><span class="ss">`'</span>],</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'A space'</span>, <span class="st">'` `'</span>, <span class="ss">f'</span><span class="sc">{</span><span class="bu">ord</span>(<span class="st">" "</span>)<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'`</span><span class="sc">{</span>byte_to_unicode_map[<span class="bu">ord</span>(<span class="st">" "</span>)]<span class="sc">}</span><span class="ss">`'</span>],</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'A nonbreakable space'</span>, <span class="st">'`</span><span class="ch">\\</span><span class="st">xa0`'</span>, <span class="st">'160'</span>, <span class="ss">f'`</span><span class="sc">{</span>byte_to_unicode_map[<span class="bu">ord</span>(<span class="bu">chr</span>(<span class="dv">160</span>))]<span class="sc">}</span><span class="ss">`'</span>],</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'A newline character'</span>, <span class="st">'`</span><span class="ch">\\</span><span class="st">n`'</span>, <span class="st">'10'</span>, <span class="ss">f'`</span><span class="sc">{</span>byte_to_unicode_map[<span class="bu">ord</span>(<span class="bu">chr</span>(<span class="dv">10</span>))]<span class="sc">}</span><span class="ss">`'</span>],</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(examples, columns <span class="op">=</span> [<span class="st">'Description'</span>, <span class="st">'Character'</span>, <span class="st">'Bytes'</span>, <span class="st">'Mapped bytes'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Description
</th>
<th>
Character
</th>
<th>
Bytes
</th>
<th>
Mapped bytes
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
Regular characters
</td>
<td>
<code>a</code> and <code>?</code>
</td>
<td>
97 and 63
</td>
<td>
<code>a</code> and <code>?</code>
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Nonprintable control character (carriage return)
</td>
<td>
<code>U+000D</code>
</td>
<td>
13
</td>
<td>
<code>č</code>
</td>
</tr>
<tr>
<th>
2
</th>
<td>
A space
</td>
<td>
<code></code>
</td>
<td>
32
</td>
<td>
<code>Ġ</code>
</td>
</tr>
<tr>
<th>
3
</th>
<td>
A nonbreakable space
</td>
<td>
<code>\xa0</code>
</td>
<td>
160
</td>
<td>
<code>ł</code>
</td>
</tr>
<tr>
<th>
4
</th>
<td>
A newline character
</td>
<td>
<code>\n</code>
</td>
<td>
10
</td>
<td>
<code>Ċ</code>
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Inspect the pre-tokenization step again</strong></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
(0, 3)
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Ġsay
</td>
<td>
(3, 7)
</td>
</tr>
<tr>
<th>
2
</th>
<td>
_
</td>
<td>
(7, 8)
</td>
</tr>
<tr>
<th>
3
</th>
<td>
hello
</td>
<td>
(8, 13)
</td>
</tr>
<tr>
<th>
4
</th>
<td>
():
</td>
<td>
(13, 16)
</td>
</tr>
<tr>
<th>
5
</th>
<td>
ĊĠĠĠ
</td>
<td>
(16, 20)
</td>
</tr>
<tr>
<th>
6
</th>
<td>
Ġprint
</td>
<td>
(20, 26)
</td>
</tr>
<tr>
<th>
7
</th>
<td>
(“
</td>
<td>
(26, 28)
</td>
</tr>
<tr>
<th>
8
</th>
<td>
Hello
</td>
<td>
(28, 33)
</td>
</tr>
<tr>
<th>
9
</th>
<td>
,
</td>
<td>
(33, 34)
</td>
</tr>
<tr>
<th>
10
</th>
<td>
ĠWorld
</td>
<td>
(34, 40)
</td>
</tr>
<tr>
<th>
11
</th>
<td>
!“)
</td>
<td>
(40, 43)
</td>
</tr>
<tr>
<th>
12
</th>
<td>
Ċ
</td>
<td>
(43, 44)
</td>
</tr>
<tr>
<th>
13
</th>
<td>
#
</td>
<td>
(44, 45)
</td>
</tr>
<tr>
<th>
14
</th>
<td>
ĠPrint
</td>
<td>
(45, 51)
</td>
</tr>
<tr>
<th>
15
</th>
<td>
Ġit
</td>
<td>
(51, 54)
</td>
</tr>
<tr>
<th>
16
</th>
<td>
Ċ
</td>
<td>
(54, 55)
</td>
</tr>
<tr>
<th>
17
</th>
<td>
say
</td>
<td>
(55, 58)
</td>
</tr>
<tr>
<th>
18
</th>
<td>
_
</td>
<td>
(58, 59)
</td>
</tr>
<tr>
<th>
19
</th>
<td>
hello
</td>
<td>
(59, 64)
</td>
</tr>
<tr>
<th>
20
</th>
<td>
()
</td>
<td>
(64, 66)
</td>
</tr>
<tr>
<th>
21
</th>
<td>
Ċ
</td>
<td>
(66, 67)
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>Consecutive spaces count as a single word.</li>
<li>Each space preceding a word is attached to and considered part of the following word.</li>
</ul>
<hr>
<p><strong>Check the size of the GPT-2 vocabulary</strong></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size of the vocabulary: </span><span class="sc">{</span><span class="bu">len</span>(tokenizer)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Size of the vocabulary: 50257</code></pre>
<p><strong>Note:</strong> The GPT-2 vocabulary consists of the base vocabulary with the 256 values of the bytes, 50,000 additional tokens created by repeatedly merging the most commonly occurring tokens, and a special character to represent document boundaries.</p>
<hr>
<p><strong>Run the GPT-2 tokenizer pipeline again</strong></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenizer(python_code).tokens()).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
<th>
25
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
Ġsay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
():
</td>
<td>
Ċ
</td>
<td>
Ġ
</td>
<td>
Ġ
</td>
<td>
Ġ
</td>
<td>
Ġprint
</td>
<td>
(“
</td>
<td>
Hello
</td>
<td>
,
</td>
<td>
ĠWorld
</td>
<td>
!“
</td>
<td>
)
</td>
<td>
Ċ
</td>
<td>
#
</td>
<td>
ĠPrint
</td>
<td>
Ġit
</td>
<td>
Ċ
</td>
<td>
say
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
()
</td>
<td>
Ċ
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The tokenizer keeps most of the words but splits indentations into several consecutive spaces.</li>
<li>The training corpus for the tokenizer mostly contained text where consecutive spaces are rare.</li>
<li>The BPE model does not include a specific token for indentation, meaning it is not well suited for Python code.</li>
</ul>
<hr>
</section>
<section id="training-a-tokenizer" class="level3">
<h3 class="anchored" data-anchor-id="training-a-tokenizer">Training a Tokenizer</h3>
<ul>
<li>A tokenizer learns which letter combinations are the most frequent in a target corpus.</li>
<li>The corpus does not need to be very large, just representative of the target domain.</li>
<li>We can train a tokenizer on a target corpus using the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.train_new_from_iterator"><code>tokenizer.train_new_from_iterator()</code></a> method.</li>
<li>We need to specify a target vocab size and prepare an iterator to supply lists of input strings.</li>
<li>The tokenizer might store unusual character sequences depending on the vocab size and the exact texts in the corpus.</li>
</ul>
<p><strong>Check the longest words in the GPT-2 tokenizer vocabulary</strong></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">sorted</span>(tokenizer.vocab.items(), key<span class="op">=</span><span class="kw">lambda</span> x: <span class="bu">len</span>(x[<span class="dv">0</span>]), reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([<span class="ss">f'</span><span class="sc">{</span>tokenizer<span class="sc">.</span>convert_tokens_to_string(t)<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> t, _ <span class="kw">in</span> tokens[:<span class="dv">8</span>]]).style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table id="T_72966">
<thead>
</thead>
<tbody>
<tr>
<th id="T_72966_level0_row0" class="row_heading level0 row0">
0
</th>
<td id="T_72966_row0_col0" class="data row0 col0">
ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ
</td>
</tr>
<tr>
<th id="T_72966_level0_row1" class="row_heading level0 row1">
1
</th>
<td id="T_72966_row1_col0" class="data row1 col0">
=================================================================
</td>
</tr>
<tr>
<th id="T_72966_level0_row2" class="row_heading level0 row2">
2
</th>
<td id="T_72966_row2_col0" class="data row2 col0">
—————————————————————-
</td>
</tr>
<tr>
<th id="T_72966_level0_row3" class="row_heading level0 row3">
3
</th>
<td id="T_72966_row3_col0" class="data row3 col0">
================================================================
</td>
</tr>
<tr>
<th id="T_72966_level0_row4" class="row_heading level0 row4">
4
</th>
<td id="T_72966_row4_col0" class="data row4 col0">
________________________________________________________________
</td>
</tr>
<tr>
<th id="T_72966_level0_row5" class="row_heading level0 row5">
5
</th>
<td id="T_72966_row5_col0" class="data row5 col0">
—————————————————————-
</td>
</tr>
<tr>
<th id="T_72966_level0_row6" class="row_heading level0 row6">
6
</th>
<td id="T_72966_row6_col0" class="data row6 col0">
ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ
</td>
</tr>
<tr>
<th id="T_72966_level0_row7" class="row_heading level0 row7">
7
</th>
<td id="T_72966_row7_col0" class="data row7 col0">
……………………………………………………….
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> These tokens look like separator lines used on forums.</p>
<hr>
<p><strong>Check the least frequent words</strong></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">sorted</span>(tokenizer.vocab.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([<span class="ss">f'</span><span class="sc">{</span>tokenizer<span class="sc">.</span>convert_tokens_to_string(t)<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> t, _ <span class="kw">in</span> tokens[:<span class="dv">12</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
&lt;|endoftext|&gt;
</td>
</tr>
<tr>
<th>
1
</th>
<td>
gazed
</td>
</tr>
<tr>
<th>
2
</th>
<td>
informants
</td>
</tr>
<tr>
<th>
3
</th>
<td>
Collider
</td>
</tr>
<tr>
<th>
4
</th>
<td>
regress
</td>
</tr>
<tr>
<th>
5
</th>
<td>
ominated
</td>
</tr>
<tr>
<th>
6
</th>
<td>
amplification
</td>
</tr>
<tr>
<th>
7
</th>
<td>
Compar
</td>
</tr>
<tr>
<th>
8
</th>
<td>
….”
</td>
</tr>
<tr>
<th>
9
</th>
<td>
(/
</td>
</tr>
<tr>
<th>
10
</th>
<td>
Commission
</td>
</tr>
<tr>
<th>
11
</th>
<td>
Hitman
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The <code>&lt;|endoftext|&gt;</code> token specifies the end of a text sequence and is not from the training corpus.</li>
<li>The model has to learn an associated word embedding for each token.</li>
<li>This tokenizer embeds some highly time and space-specific knowledge of the world by granting these words separate tokens.</li>
<li>Overly specific tokens can indicate the target vocab size is too large or that the corpus contains peculiar tokens.</li>
<li>We don’t want the embedding matrix to contain too many noisy words.</li>
</ul>
<hr>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train a fresh tokenizer on 100,000 documents</strong></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">'transformersbook/codeparrot-train'</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(dataset_name, split<span class="op">=</span><span class="st">"train"</span>, streaming<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>iter_dataset <span class="op">=</span> <span class="bu">iter</span>(dataset)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_iterator(batch_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">0</span>, length, batch_size)):</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> [<span class="bu">next</span>(iter_dataset)[<span class="st">'content'</span>] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(batch_size)]</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>new_tokenizer <span class="op">=</span> tokenizer.train_new_from_iterator(batch_iterator(), </span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>                                                  vocab_size<span class="op">=</span><span class="dv">12500</span>,</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>                                                  initial_alphabet<span class="op">=</span>base_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Examine the first tokens added by the BPE algorithm</strong></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">sorted</span>(new_tokenizer.vocab.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([<span class="ss">f'</span><span class="sc">{</span>tokenizer<span class="sc">.</span>convert_tokens_to_string(t)<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> t, _ <span class="kw">in</span> tokens[<span class="dv">257</span>:<span class="dv">280</span>]]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
se
</td>
<td>
in
</td>
<td>
</td>
<td>
re
</td>
<td>
on
</td>
<td>
te
</td>
<td>

</td>
<td>

</td>
<td>
or
</td>
<td>
st
</td>
<td>
de
</td>
<td>

</td>
<td>
th
</td>
<td>
le
</td>
<td>
=
</td>
<td>
lf
</td>
<td>
self
</td>
<td>
me
</td>
<td>
al
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>There are various standard levels of indentation and whitespace tokens and short common Python keywords.</li>
<li>The BPE algorithm is working as intended.</li>
</ul>
<hr>
<p><strong>Examine the last tokens added by the BPE algorithm</strong></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([<span class="ss">f'</span><span class="sc">{</span>new_tokenizer<span class="sc">.</span>convert_tokens_to_string(t)<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> t,_ <span class="kw">in</span> tokens[<span class="op">-</span><span class="dv">12</span>:]]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
capt
</td>
<td>
embedded
</td>
<td>
regarding
</td>
<td>
Bundle
</td>
<td>
355
</td>
<td>
recv
</td>
<td>
dmp
</td>
<td>
vault
</td>
<td>
Mongo
</td>
<td>
possibly
</td>
<td>
implementation
</td>
<td>
Matches
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>There are still some relatively common words like the <a href="https://docs.python.org/3/library/socket.html#socket.socket.recv"><code>recv</code></a> method.</li>
<li>There are also some more noisy words potentially from comments.</li>
</ul>
<hr>
<p><strong>Test the custom tokenizer on the sample code</strong></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(new_tokenizer(python_code).tokens()).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
Ġs
</td>
<td>
ay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
():
</td>
<td>
ĊĠĠĠ
</td>
<td>
Ġprint
</td>
<td>
(“
</td>
<td>
Hello
</td>
<td>
,
</td>
<td>
ĠWor
</td>
<td>
ld
</td>
<td>
!“)
</td>
<td>
Ċ
</td>
<td>
#
</td>
<td>
ĠPrint
</td>
<td>
Ġit
</td>
<td>
Ċ
</td>
<td>
s
</td>
<td>
ay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
()
</td>
<td>
Ċ
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The tokenize splits common English words like “World” and “say.”</p>
<hr>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keyword</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="keyword" class="level4">
<h4 class="anchored" data-anchor-id="keyword"><code>keyword</code></h4>
<ul>
<li><a href="https://docs.python.org/3/library/keyword.html">Documentation</a></li>
<li>Determine if a string is a <a href="https://docs.python.org/3/reference/lexical_analysis.html#keywords">keyword</a> or <a href="https://docs.python.org/3/reference/lexical_analysis.html#soft-keywords">soft keyword</a>.</li>
</ul>
<p><strong>Check if all the Python reserved words are in the vocabulary</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'There are in total </span><span class="sc">{</span><span class="bu">len</span>(keyword.kwlist)<span class="sc">}</span><span class="ss"> Python keywords.'</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> keyw <span class="kw">in</span> keyword.kwlist:</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keyw <span class="kw">not</span> <span class="kw">in</span> new_tokenizer.vocab:</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'No, keyword `</span><span class="sc">{</span>keyw<span class="sc">}</span><span class="ss">` is not in the vocabulary'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    There are in total 36 Python keywords.
    No, keyword `__peg_parser__` is not in the vocabulary
    No, keyword `await` is not in the vocabulary
    No, keyword `finally` is not in the vocabulary
    No, keyword `nonlocal` is not in the vocabulary</code></pre>
<p><strong>Note:</strong> Several frequent keywords like “finally” are not in the vocabulary.</p>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train a tokenizer using a larger target vocab size and dataset sample</strong></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>new_tokenizer_larger <span class="op">=</span> tokenizer.train_new_from_iterator(batch_iterator(),</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">32768</span>, initial_alphabet<span class="op">=</span>base_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Check the last tokens added</strong></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">sorted</span>(new_tokenizer_larger.vocab.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>],</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>                reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([<span class="ss">f'</span><span class="sc">{</span>tokenizer<span class="sc">.</span>convert_tokens_to_string(t)<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> t, _ <span class="kw">in</span> tokens[<span class="op">-</span><span class="dv">12</span>:]]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
组
</td>
<td>
typically
</td>
<td>
ARGIN
</td>
<td>
Termination
</td>
<td>
StaticText
</td>
<td>
interesting
</td>
<td>
Circular
</td>
<td>
combinatorics
</td>
<td>
)([
</td>
<td>
969
</td>
<td>
EAR
</td>
<td>
Gap
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The group of least-frequent tokens does not contain any Python keywords.</p>
<hr>
<p><strong>Test the new tokenizer on the sample code</strong></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(new_tokenizer_larger(python_code).tokens()).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
Ġsay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
():
</td>
<td>
ĊĠĠĠ
</td>
<td>
Ġprint
</td>
<td>
(“
</td>
<td>
Hello
</td>
<td>
,
</td>
<td>
ĠWorld
</td>
<td>
!“)
</td>
<td>
Ċ
</td>
<td>
#
</td>
<td>
ĠPrint
</td>
<td>
Ġit
</td>
<td>
Ċ
</td>
<td>
say
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
()
</td>
<td>
Ċ
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The new tokenizer keeps the indents in the vocabulary and does not split common English words.</p>
<hr>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> keyw <span class="kw">in</span> keyword.kwlist:</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keyw <span class="kw">not</span> <span class="kw">in</span> new_tokenizer_larger.vocab:</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'No, keyword `</span><span class="sc">{</span>keyw<span class="sc">}</span><span class="ss">` is not in the vocabulary'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    No, keyword `__peg_parser__` is not in the vocabulary
    No, keyword `nonlocal` is not in the vocabulary</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The new tokenizer vocabulary is still missing a couple of rare Python keywords, neither of which are relevant for most Python code.</li>
<li>The <code>__peg_parser__</code> keyword is an easter egg for the new <a href="https://peps.python.org/pep-0617/">PEG parser</a> and <a href="https://bugs.python.org/issue40939">will not be in Python 3.10</a>.</li>
<li>The <code>nonlocal</code> keyword causes listed identifiers to refer to previously bound variables in the nearest enclosing scope, excluding globals.</li>
<li>The new tokenizer is more efficient than the standard GPT-2 tokenizer as it uses fewer tokens to encode a given code sample.</li>
</ul>
<hr>
<p><strong>Disable Tokenizers Parallelism</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env TOKENIZERS_PARALLELISM<span class="op">=</span>false</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    env: TOKENIZERS_PARALLELISM=false</code></pre>
<hr>
</section>
</section>
<section id="saving-a-custom-tokenizer-on-the-hub" class="level3">
<h3 class="anchored" data-anchor-id="saving-a-custom-tokenizer-on-the-hub">Saving a Custom Tokenizer on the Hub</h3>
<p><strong>Log into Hugging Face account</strong></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>notebook_login()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Login successful
    Your token has been saved to /home/innom-dt/.huggingface/token</code></pre>
<hr>
<p><strong>Push custom tokenizer to Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"codeparrot"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># org = "transformersbook"</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>new_tokenizer_larger.push_to_hub(model_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/codeparrot/commit/97c7905ef55cb4139e88f9b9d17225c372fc8f55'</code></pre>
<hr>
<p><strong>Load the custom tokenizer from the Hub repository</strong></p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reloaded_tokenizer = AutoTokenizer.from_pretrained(org + "/" + model_ckpt)</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>reloaded_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(reloaded_tokenizer(python_code).tokens()).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
def
</td>
<td>
Ġsay
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
():
</td>
<td>
ĊĠĠĠ
</td>
<td>
Ġprint
</td>
<td>
(“
</td>
<td>
Hello
</td>
<td>
,
</td>
<td>
ĠWorld
</td>
<td>
!“)
</td>
<td>
Ċ
</td>
<td>
#
</td>
<td>
ĠPrint
</td>
<td>
Ġit
</td>
<td>
Ċ
</td>
<td>
say
</td>
<td>
_
</td>
<td>
hello
</td>
<td>
()
</td>
<td>
Ċ
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Push the smaller tokenizer to Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>new_tokenizer.push_to_hub(model_ckpt<span class="op">+</span> <span class="st">"-small-vocabulary"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/codeparrot-small-vocabulary/commit/b4efe8c9692ce772175b97b01cffc9f1924ae706'</code></pre>
<hr>
</section>
</section>
<section id="training-a-model-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="training-a-model-from-scratch">Training a Model from Scratch</h2>
<ul>
<li><a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot">CodeParrot Trainng Script and Instructions</a></li>
</ul>
<section id="a-tale-of-pretraining-objectives" class="level3">
<h3 class="anchored" data-anchor-id="a-tale-of-pretraining-objectives">A Tale of Pretraining Objectives</h3>
<ul>
<li>The large-scale pretraining corpus allows us to tackle several downstream tasks.</li>
<li>The selected task will influence which pretraining objective we choose.</li>
</ul>
<section id="causal-language-modeling" class="level4">
<h4 class="anchored" data-anchor-id="causal-language-modeling">Causal language modeling</h4>
<ul>
<li>Causal language modeling is a self-supervised approach that does not require annotations.</li>
<li>Code autocompletion is a directly related downstream task.</li>
<li>We can provide a model with the beginning of a code sample and have it generate possible completions.</li>
<li>A decoder-only architecture like the GPT family is usually best suited for this task.</li>
</ul>
</section>
<section id="masked-language-modeling" class="level4">
<h4 class="anchored" data-anchor-id="masked-language-modeling">Masked language modeling</h4>
<ul>
<li>Masked language modeling (also called denoising) is a self-supervised training objective.</li>
<li>We can provide a model with a noisy code sample (e.g., by replacing a code instruction with a random or masked word) and have it reconstruct the original clean sequence.</li>
<li>Masked language modeling is not directly related to a downstream task like autocompletion, but it is a practical pretraining objective for learning general representations.</li>
<li>We can combine masked language modeling with fine-tuning the model on a downstream task.</li>
<li>Encoder architectures are best suited to this pretraining objective.</li>
</ul>
</section>
<section id="sequence-to-sequence-training" class="level4">
<h4 class="anchored" data-anchor-id="sequence-to-sequence-training">Sequence-to-sequence training</h4>
<ul>
<li>Sequence-to-sequence training is a supervised learning objective where one category serves as input while another serves as labels.</li>
<li>We can use a heuristic like regular expressions to separate comments or docstrings from code and build a large-scale annotated dataset of code-comment pairs.</li>
<li>We can then use this dataset to train a model to transcript comments in code or vice versa.</li>
<li>Document generation from code and code generation from comments are directly-related downstream tasks.</li>
<li>Encoder decoder architectures are best suited to sequence-to-sequence objectives.</li>
</ul>
</section>
</section>
<section id="initializing-the-model" class="level3">
<h3 class="anchored" data-anchor-id="initializing-the-model">Initializing the Model</h3>
<blockquote class="blockquote">
<p><strong>NOTE</strong>: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you use the small checkpoint by replacing the configuration with <code>config = AutoConfig.from_pretrained("gpt2", vocab_size=len(tokenizer))</code>.</p>
</blockquote>
<hr>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig, AutoModelForCausalLM, AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Instantiate a tokenizer using the custom checkpoint</strong></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Start with the hyperparameters for training the 1.5 billion-parameter GPT-2 variant</strong></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AutoConfig.from_pretrained(<span class="st">"gpt2-xl"</span>, vocab_size<span class="op">=</span><span class="bu">len</span>(tokenizer))</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    GPT2Config {
      "_name_or_path": "gpt2-xl",
      "activation_function": "gelu_new",
      "architectures": [
        "GPT2LMHeadModel"
      ],
      "attn_pdrop": 0.1,
      "bos_token_id": 50256,
      "embd_pdrop": 0.1,
      "eos_token_id": 50256,
      "initializer_range": 0.02,
      "layer_norm_epsilon": 1e-05,
      "model_type": "gpt2",
      "n_ctx": 1024,
      "n_embd": 1600,
      "n_head": 25,
      "n_inner": null,
      "n_layer": 48,
      "n_positions": 1024,
      "output_past": true,
      "reorder_and_upcast_attn": false,
      "resid_pdrop": 0.1,
      "scale_attn_by_inverse_layer_idx": false,
      "scale_attn_weights": true,
      "summary_activation": null,
      "summary_first_dropout": 0.1,
      "summary_proj_to_labels": true,
      "summary_type": "cls_index",
      "summary_use_proj": true,
      "task_specific_params": {
        "text-generation": {
          "do_sample": true,
          "max_length": 50
        }
      },
      "transformers_version": "4.18.0",
      "use_cache": true,
      "vocab_size": 32768
    }</code></pre>
<hr>
<p><strong>Free unoccupied cached memory</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize a GPT-2 XL model using the custom tokenizer</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_config(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Check the model size</strong></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'GPT-2 (xl) size: </span><span class="sc">{</span>model_size(model)<span class="op">/</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.1f}</span><span class="ss">M parameters'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    GPT-2 (xl) size: 1529.6M parameters</code></pre>
<p><strong>Note:</strong> Large models are generally more efficient to train as long as the dataset is reasonably large.</p>
<hr>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>git lfs install</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Updated Git hooks.
    Git LFS initialized.</code></pre>
<hr>
<p><strong>Save the newly initialized model to the Hub</strong></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="st">"models/"</span> <span class="op">+</span> model_ckpt<span class="op">+</span><span class="st">"-large"</span>, push_to_hub<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    OSError: EOF
    error: failed to push some refs to 'https://user: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@huggingface.co/cj-mills/codeparrot-large'</code></pre>
<hr>
<p><strong>Initialize a smaller GPT-2 variant using the custom tokenizer</strong></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>config_small <span class="op">=</span> AutoConfig.from_pretrained(<span class="st">"gpt2"</span>, vocab_size<span class="op">=</span><span class="bu">len</span>(tokenizer))</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>model_small <span class="op">=</span> AutoModelForCausalLM.from_config(config_small)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Check smaller model size</strong></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'GPT-2 size: </span><span class="sc">{</span>model_size(model_small)<span class="op">/</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.1f}</span><span class="ss">M parameters'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>GPT-2 size: 111.0M parameters</code></pre>
<hr>
<p><strong>Push the smaller model to the Hub</strong></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>model_small.save_pretrained(<span class="st">"models/"</span> <span class="op">+</span> model_ckpt <span class="op">+</span> <span class="st">"-small"</span>, push_to_hub<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="implementing-the-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-dataloader">Implementing the Dataloader</h3>
<ul>
<li>We want to supply our model with sequences that fill its context length for maximal efficiency.</li>
<li>Some code examples might be shorter or longer than the 1,024 token context length.</li>
<li>We can concatenate several examples to create a long sequence using the EOS token as a separator.</li>
<li>We then split this sequence into equally sized chunks that fill the context length.</li>
</ul>
<div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>input_characters <span class="op">=</span> number_of_sequences <span class="op">*</span> sequence_length <span class="op">*</span> characters_per_token</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><code>input_characters</code>: the number of characters in the string input to the tokenizer</li>
<li><code>number_of_seqeunces</code>: the number of (truncated) sequences returned by the tokenizer</li>
<li><code>sequence_length</code>: the number of tokens per sequence returned by the tokenizer</li>
<li><code>characters_per_token</code>: the average number of characters per output token that we first need to estimate</li>
</ul>
<p><strong>Estimate the average character length per token</strong></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>examples, total_characters, total_tokens <span class="op">=</span> <span class="dv">500</span>, <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">'transformersbook/codeparrot-train'</span>, split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>                       streaming<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, example <span class="kw">in</span> tqdm(<span class="bu">zip</span>(<span class="bu">range</span>(examples), <span class="bu">iter</span>(dataset)), total<span class="op">=</span>examples):</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    total_characters <span class="op">+=</span> <span class="bu">len</span>(example[<span class="st">'content'</span>])</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">+=</span> <span class="bu">len</span>(tokenizer(example[<span class="st">'content'</span>]).tokens())</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>characters_per_token <span class="op">=</span> total_characters <span class="op">/</span> total_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(characters_per_token)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>3.621530410894045</code></pre>
<p><strong>Note:</strong> We’ll round this to <span class="math inline">\(3.6\)</span>.</p>
<hr>
<div class="sourceCode" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> IterableDataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define an IterableDataset class for preparing constant-length inputs</strong></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConstantLengthDataset(IterableDataset):</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, dataset, seq_length<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>                 num_of_sequences<span class="op">=</span><span class="dv">1024</span>, chars_per_token<span class="op">=</span><span class="fl">3.6</span>):</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.concat_token_id <span class="op">=</span> tokenizer.eos_token_id</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dataset <span class="op">=</span> dataset</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.seq_length <span class="op">=</span> seq_length</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_characters <span class="op">=</span> num_of_sequences <span class="op">*</span> seq_length <span class="op">*</span> chars_per_token</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>        iterator <span class="op">=</span> <span class="bu">iter</span>(<span class="va">self</span>.dataset)</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>        more_examples <span class="op">=</span> <span class="va">True</span></span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> more_examples:</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">buffer</span>, buffer_len <span class="op">=</span> [], <span class="dv">0</span></span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Check if the buffer is full</span></span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> buffer_len <span class="op">&gt;=</span> <span class="va">self</span>.input_characters:</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a>                    m<span class="op">=</span><span class="ss">f"Buffer full: </span><span class="sc">{</span>buffer_len<span class="sc">}</span><span class="ss">&gt;=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>input_characters<span class="sc">:.0f}</span><span class="ss">"</span></span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(m)</span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Try to add the next code sample to the buffer</span></span>
<span id="cb105-23"><a href="#cb105-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb105-24"><a href="#cb105-24" aria-hidden="true" tabindex="-1"></a>                    m<span class="op">=</span><span class="ss">f"Fill buffer: </span><span class="sc">{</span>buffer_len<span class="sc">}</span><span class="ss">&lt;</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>input_characters<span class="sc">:.0f}</span><span class="ss">"</span></span>
<span id="cb105-25"><a href="#cb105-25" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(m)</span>
<span id="cb105-26"><a href="#cb105-26" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">buffer</span>.append(<span class="bu">next</span>(iterator)[<span class="st">"content"</span>])</span>
<span id="cb105-27"><a href="#cb105-27" aria-hidden="true" tabindex="-1"></a>                    buffer_len <span class="op">+=</span> <span class="bu">len</span>(<span class="bu">buffer</span>[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb105-28"><a href="#cb105-28" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Reset iterator</span></span>
<span id="cb105-29"><a href="#cb105-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">StopIteration</span>:</span>
<span id="cb105-30"><a href="#cb105-30" aria-hidden="true" tabindex="-1"></a>                    iterator <span class="op">=</span> <span class="bu">iter</span>(<span class="va">self</span>.dataset)</span>
<span id="cb105-31"><a href="#cb105-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb105-32"><a href="#cb105-32" aria-hidden="true" tabindex="-1"></a>            all_token_ids <span class="op">=</span> []</span>
<span id="cb105-33"><a href="#cb105-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Tokenize the code samples in the buffer</span></span>
<span id="cb105-34"><a href="#cb105-34" aria-hidden="true" tabindex="-1"></a>            tokenized_inputs <span class="op">=</span> <span class="va">self</span>.tokenizer(<span class="bu">buffer</span>, truncation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb105-35"><a href="#cb105-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Concatenate the tokenized code samples</span></span>
<span id="cb105-36"><a href="#cb105-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tokenized_input <span class="kw">in</span> tokenized_inputs[<span class="st">'input_ids'</span>]:</span>
<span id="cb105-37"><a href="#cb105-37" aria-hidden="true" tabindex="-1"></a>                all_token_ids.extend(tokenized_input <span class="op">+</span> [<span class="va">self</span>.concat_token_id])</span>
<span id="cb105-38"><a href="#cb105-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Split the sequence into equally sized chunks</span></span>
<span id="cb105-39"><a href="#cb105-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(all_token_ids), <span class="va">self</span>.seq_length):</span>
<span id="cb105-40"><a href="#cb105-40" aria-hidden="true" tabindex="-1"></a>                input_ids <span class="op">=</span> all_token_ids[i : i <span class="op">+</span> <span class="va">self</span>.seq_length]</span>
<span id="cb105-41"><a href="#cb105-41" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(input_ids) <span class="op">==</span> <span class="va">self</span>.seq_length:</span>
<span id="cb105-42"><a href="#cb105-42" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">yield</span> torch.tensor(input_ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> We don’t need attention masks here since all sequences precisely fill the context length of 1024 tokens.</p>
<hr>
<p><strong>Prepare the constant-length dataset</strong></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>shuffled_dataset <span class="op">=</span> dataset.shuffle(buffer_size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>constant_length_dataset <span class="op">=</span> ConstantLengthDataset(tokenizer, shuffled_dataset,</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>                                                num_of_sequences<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> We can’t shuffle iterable datasets as a whole, so we need to use a buffer instead.</p>
<hr>
<p><strong>Verify the dataset yields equal length chunks</strong></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>dataset_iterator <span class="op">=</span> <span class="bu">iter</span>(constant_length_dataset)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> [<span class="bu">len</span>(b) <span class="cf">for</span> _, b <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">5</span>), dataset_iterator)]</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lengths of the sequences: </span><span class="sc">{</span>lengths<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Fill buffer: 0&lt;36864
    Fill buffer: 4344&lt;36864
    Fill buffer: 5460&lt;36864
    Fill buffer: 7467&lt;36864
    Fill buffer: 13812&lt;36864
    Fill buffer: 16142&lt;36864
    Fill buffer: 17571&lt;36864
    Fill buffer: 25693&lt;36864
    Fill buffer: 27359&lt;36864
    Fill buffer: 28903&lt;36864
    Fill buffer: 32076&lt;36864
    Buffer full: 49996&gt;=36864
    Lengths of the sequences: [1024, 1024, 1024, 1024, 1024]</code></pre>
<hr>
</section>
<section id="defining-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-training-loop">Defining the Training Loop</h3>
<ul>
<li>Even modern GPUs can’t train a model at GPT-2 scale in a reasonable time.</li>
<li>We need to use data parallelism to utilize several GPUs for training.</li>
<li>The Hugging Face Accelerate library makes distributed training and changing the underlying hardware for training easier.</li>
<li>Hugging Face Accelerate provides an API to make training scripts run with mixed precision and in any distributed setting.</li>
<li>The same code can run seamlessly on your local machine for debugging and a beefy training cluster for a final training run.</li>
</ul>
<hr>
<div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> argparse <span class="im">import</span> Namespace</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define the hyperparameters</strong></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Commented parameters correspond to the small model</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {<span class="st">"train_batch_size"</span>: <span class="dv">2</span>, <span class="co"># 12</span></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>          <span class="st">"valid_batch_size"</span>: <span class="dv">2</span>, <span class="co"># 12</span></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>          <span class="st">"weight_decay"</span>: <span class="fl">0.1</span>,</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">"shuffle_buffer"</span>: <span class="dv">1000</span>,</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>          <span class="st">"learning_rate"</span>: <span class="fl">2e-4</span>, <span class="co"># 5e-4</span></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a>          <span class="st">"lr_scheduler_type"</span>: <span class="st">"cosine"</span>,</span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>          <span class="st">"num_warmup_steps"</span>: <span class="dv">750</span>, <span class="co"># 2000</span></span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>          <span class="st">"gradient_accumulation_steps"</span>: <span class="dv">16</span>, <span class="co"># 1</span></span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a>          <span class="st">"max_train_steps"</span>: <span class="dv">50000</span>, <span class="co"># 150000</span></span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a>          <span class="st">"max_eval_steps"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a>          <span class="st">"seq_length"</span>: <span class="dv">1024</span>,</span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a>          <span class="st">"seed"</span>: <span class="dv">1</span>,</span>
<span id="cb110-14"><a href="#cb110-14" aria-hidden="true" tabindex="-1"></a>          <span class="st">"save_checkpoint_steps"</span>: <span class="dv">50000</span>} <span class="co"># 15000</span></span>
<span id="cb110-15"><a href="#cb110-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-16"><a href="#cb110-16" aria-hidden="true" tabindex="-1"></a>args <span class="op">=</span> Namespace(<span class="op">**</span>config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> wandb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="logging.getlogger" class="level4">
<h4 class="anchored" data-anchor-id="logging.getlogger"><code>logging.getLogger()</code></h4>
<ul>
<li><a href="https://docs.python.org/3/library/logging.html#logging.getLogger">Documentation</a></li>
<li>Create a <a href="https://docs.python.org/3/library/logging.html#logger-objects">Logger object</a>.</li>
</ul>
</section>
<section id="torch.utils.tensorboard.writer.summarywriter" class="level4">
<h4 class="anchored" data-anchor-id="torch.utils.tensorboard.writer.summarywriter"><code>torch.utils.tensorboard.writer.SummaryWriter</code></h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter">Documentation</a></li>
<li>Write entries directly to event files for <a href="https://github.com/tensorflow/tensorboard">TensorBoard</a></li>
</ul>
</section>
<section id="wandb" class="level4">
<h4 class="anchored" data-anchor-id="wandb"><code>wandb</code></h4>
<ul>
<li><a href="https://github.com/wandb/client">GitHub Repository</a></li>
<li><a href="https://docs.wandb.ai/">Documentation</a></li>
<li>A tool for visualizing and tracking machine learning experiements.</li>
</ul>
<hr>
<p><strong>Define a method to initialize the loggers for the training process</strong></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_logging(project_name):</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>    logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>    logging.basicConfig(</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">format</span><span class="op">=</span><span class="st">"</span><span class="sc">%(asctime)s</span><span class="st"> - </span><span class="sc">%(levelname)s</span><span class="st"> - </span><span class="sc">%(name)s</span><span class="st"> - </span><span class="sc">%(message)s</span><span class="st">"</span>,</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>        datefmt<span class="op">=</span><span class="st">"%m/</span><span class="sc">%d</span><span class="st">/%Y %H:%M:%S"</span>, level<span class="op">=</span>logging.INFO, handlers<span class="op">=</span>[</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>        logging.FileHandler(<span class="ss">f"log/debug_</span><span class="sc">{</span>accelerator<span class="sc">.</span>process_index<span class="sc">}</span><span class="ss">.log"</span>),</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>        logging.StreamHandler()])</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> accelerator.is_main_process: <span class="co"># We only want to set up logging once</span></span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>        wandb.init(project<span class="op">=</span>project_name, config<span class="op">=</span>args)</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>        run_name <span class="op">=</span> wandb.run.name</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>        tb_writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>        tb_writer.add_hparams(<span class="bu">vars</span>(args), {<span class="st">'0'</span>: <span class="dv">0</span>})</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging.INFO)</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>        datasets.utils.logging.set_verbosity_debug()</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a>        transformers.utils.logging.set_verbosity_info()</span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb112-17"><a href="#cb112-17" aria-hidden="true" tabindex="-1"></a>        tb_writer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb112-18"><a href="#cb112-18" aria-hidden="true" tabindex="-1"></a>        run_name <span class="op">=</span> <span class="st">''</span></span>
<span id="cb112-19"><a href="#cb112-19" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging.ERROR)</span>
<span id="cb112-20"><a href="#cb112-20" aria-hidden="true" tabindex="-1"></a>        datasets.utils.logging.set_verbosity_error()</span>
<span id="cb112-21"><a href="#cb112-21" aria-hidden="true" tabindex="-1"></a>        transformers.utils.logging.set_verbosity_error()</span>
<span id="cb112-22"><a href="#cb112-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logger, tb_writer, run_name</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong></p>
<ul>
<li>Each worker gets a unique <code>accelerator.process_index</code>, which we use with the FileHandler to write the logs of each worker to an individual file.</li>
<li>We’ll use the unique <code>run_name</code> to name our experiment branch on the Hub.</li>
</ul>
<hr>
<p><strong>Define function to log metrics with TensorBoard and Weights and Biases</strong></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_metrics(step, metrics):</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>metrics<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> accelerator.is_main_process:</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>        wandb.log(metrics)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>        [tb_writer.add_scalar(k, v, step) <span class="cf">for</span> k, v <span class="kw">in</span> metrics.items()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataloader <span class="im">import</span> DataLoader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to create dataloaders for the training and validation sets</strong></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_dataloaders(dataset_name):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> load_dataset(dataset_name<span class="op">+</span><span class="st">'-train'</span>, split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>                              streaming<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> train_data.shuffle(buffer_size<span class="op">=</span>args.shuffle_buffer,</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>                                    seed<span class="op">=</span>args.seed)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>    valid_data <span class="op">=</span> load_dataset(dataset_name<span class="op">+</span><span class="st">'-valid'</span>, split<span class="op">=</span><span class="st">"validation"</span>,</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>                              streaming<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> ConstantLengthDataset(tokenizer, train_data,</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>                                          seq_length<span class="op">=</span>args.seq_length)</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>    valid_dataset <span class="op">=</span> ConstantLengthDataset(tokenizer, valid_data,</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a>                                          seq_length<span class="op">=</span>args.seq_length)</span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>    train_dataloader<span class="op">=</span>DataLoader(train_dataset, batch_size<span class="op">=</span>args.train_batch_size)</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>    eval_dataloader<span class="op">=</span>DataLoader(valid_dataset, batch_size<span class="op">=</span>args.valid_batch_size)</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_dataloader, eval_dataloader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Hugging Face Accelerate takes care of distributing batches to each worker.</p>
<hr>
<p><strong>Define a helper function to differentiate the parameters that should receive weight decay</strong></p>
<ul>
<li>Biases and LayerNorm weights are generally not subject to weight decay.</li>
</ul>
<div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_grouped_params(model, no_decay<span class="op">=</span>[<span class="st">"bias"</span>, <span class="st">"LayerNorm.weight"</span>]):</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>    params_with_wd, params_without_wd <span class="op">=</span> [], []</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n, p <span class="kw">in</span> model.named_parameters():</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(nd <span class="kw">in</span> n <span class="cf">for</span> nd <span class="kw">in</span> no_decay):</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>            params_without_wd.append(p)</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>            params_with_wd.append(p)</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [{<span class="st">'params'</span>: params_with_wd, <span class="st">'weight_decay'</span>: args.weight_decay},</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'params'</span>: params_without_wd, <span class="st">'weight_decay'</span>: <span class="fl">0.0</span>}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to evaluate the model on the validation set</strong></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate():</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(eval_dataloader):</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(batch, labels<span class="op">=</span>batch)</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs.loss.repeat(args.valid_batch_size)</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>        losses.append(accelerator.gather(loss))</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> args.max_eval_steps <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> step <span class="op">&gt;=</span> args.max_eval_steps: <span class="cf">break</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.mean(torch.cat(losses))</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>        perplexity <span class="op">=</span> torch.exp(loss)</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">OverflowError</span>:</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>        perplexity <span class="op">=</span> torch.tensor(<span class="bu">float</span>(<span class="st">"inf"</span>))</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item(), perplexity.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong></p>
<ul>
<li>The perplexity measures how well the model’s output probability distributions predict the targeted tokens.</li>
<li>A lower perplexity corresponds to better performance.</li>
<li>We compute the perplexity by exponentiating the cross-entropy loss from the model’s output.</li>
</ul>
<hr>
<p><strong>Training session</strong></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset random seed</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>set_seed(args.seed)</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Accelerator</span></span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>accelerator <span class="op">=</span> Accelerator()</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>samples_per_step <span class="op">=</span> accelerator.state.num_processes <span class="op">*</span> args.train_batch_size</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Logging</span></span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>logger, tb_writer, run_name <span class="op">=</span> setup_logging(project_name.split(<span class="st">"/"</span>)[<span class="dv">1</span>])</span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a>logger.info(accelerator.state)</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and tokenizer</span></span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> accelerator.is_main_process:</span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check out a new branch for the current run</span></span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a>    hf_repo <span class="op">=</span> Repository(<span class="st">"./"</span>, clone_from<span class="op">=</span>project_name, revision<span class="op">=</span>run_name)</span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">"./"</span>, gradient_checkpointing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"./"</span>)</span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset and dataloader</span></span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a>train_dataloader, eval_dataloader <span class="op">=</span> create_dataloaders(dataset_name)</span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-22"><a href="#cb118-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the optimizer and learning rate scheduler</span></span>
<span id="cb118-23"><a href="#cb118-23" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(get_grouped_params(model), lr<span class="op">=</span>args.learning_rate)</span>
<span id="cb118-24"><a href="#cb118-24" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> get_scheduler(name<span class="op">=</span>args.lr_scheduler_type, optimizer<span class="op">=</span>optimizer,</span>
<span id="cb118-25"><a href="#cb118-25" aria-hidden="true" tabindex="-1"></a>                             num_warmup_steps<span class="op">=</span>args.num_warmup_steps,</span>
<span id="cb118-26"><a href="#cb118-26" aria-hidden="true" tabindex="-1"></a>                             num_training_steps<span class="op">=</span>args.max_train_steps,)</span>
<span id="cb118-27"><a href="#cb118-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lr():</span>
<span id="cb118-28"><a href="#cb118-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>]</span>
<span id="cb118-29"><a href="#cb118-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-30"><a href="#cb118-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare everything with our `accelerator` (order of args is not important)</span></span>
<span id="cb118-31"><a href="#cb118-31" aria-hidden="true" tabindex="-1"></a>model, optimizer, train_dataloader, eval_dataloader <span class="op">=</span> accelerator.prepare(</span>
<span id="cb118-32"><a href="#cb118-32" aria-hidden="true" tabindex="-1"></a>    model, optimizer, train_dataloader, eval_dataloader)</span>
<span id="cb118-33"><a href="#cb118-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-34"><a href="#cb118-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model</span></span>
<span id="cb118-35"><a href="#cb118-35" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb118-36"><a href="#cb118-36" aria-hidden="true" tabindex="-1"></a>completed_steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb118-37"><a href="#cb118-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader, start<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb118-38"><a href="#cb118-38" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch, labels<span class="op">=</span>batch).loss</span>
<span id="cb118-39"><a href="#cb118-39" aria-hidden="true" tabindex="-1"></a>    log_metrics(step, {<span class="st">'lr'</span>: get_lr(), <span class="st">'samples'</span>: step<span class="op">*</span>samples_per_step,</span>
<span id="cb118-40"><a href="#cb118-40" aria-hidden="true" tabindex="-1"></a>                       <span class="st">'steps'</span>: completed_steps, <span class="st">'loss/train'</span>: loss.item()})</span>
<span id="cb118-41"><a href="#cb118-41" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss <span class="op">/</span> args.gradient_accumulation_steps</span>
<span id="cb118-42"><a href="#cb118-42" aria-hidden="true" tabindex="-1"></a>    accelerator.backward(loss)</span>
<span id="cb118-43"><a href="#cb118-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use gradient accumulation to imitate larger batch sizes</span></span>
<span id="cb118-44"><a href="#cb118-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> args.gradient_accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb118-45"><a href="#cb118-45" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb118-46"><a href="#cb118-46" aria-hidden="true" tabindex="-1"></a>        lr_scheduler.step()</span>
<span id="cb118-47"><a href="#cb118-47" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb118-48"><a href="#cb118-48" aria-hidden="true" tabindex="-1"></a>        completed_steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb118-49"><a href="#cb118-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> args.save_checkpoint_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb118-50"><a href="#cb118-50" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="st">'Evaluating and saving model checkpoint'</span>)</span>
<span id="cb118-51"><a href="#cb118-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate the model every time we save a new checkpoint</span></span>
<span id="cb118-52"><a href="#cb118-52" aria-hidden="true" tabindex="-1"></a>        eval_loss, perplexity <span class="op">=</span> evaluate()</span>
<span id="cb118-53"><a href="#cb118-53" aria-hidden="true" tabindex="-1"></a>        log_metrics(step, {<span class="st">'loss/eval'</span>: eval_loss, <span class="st">'perplexity'</span>: perplexity})</span>
<span id="cb118-54"><a href="#cb118-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Synchronize the model before storing the latest checkpoint</span></span>
<span id="cb118-55"><a href="#cb118-55" aria-hidden="true" tabindex="-1"></a>        accelerator.wait_for_everyone()</span>
<span id="cb118-56"><a href="#cb118-56" aria-hidden="true" tabindex="-1"></a>        unwrapped_model <span class="op">=</span> accelerator.unwrap_model(model)</span>
<span id="cb118-57"><a href="#cb118-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accelerator.is_main_process:</span>
<span id="cb118-58"><a href="#cb118-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Save the latest checkpoint to disk</span></span>
<span id="cb118-59"><a href="#cb118-59" aria-hidden="true" tabindex="-1"></a>            unwrapped_model.save_pretrained(<span class="st">"./"</span>)</span>
<span id="cb118-60"><a href="#cb118-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Push the latest checkpoint to the Hub</span></span>
<span id="cb118-61"><a href="#cb118-61" aria-hidden="true" tabindex="-1"></a>            hf_repo.push_to_hub(commit_message<span class="op">=</span><span class="ss">f'step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb118-62"><a href="#cb118-62" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb118-63"><a href="#cb118-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> completed_steps <span class="op">&gt;=</span> args.max_train_steps:</span>
<span id="cb118-64"><a href="#cb118-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb118-65"><a href="#cb118-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-66"><a href="#cb118-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate and save the last checkpoint</span></span>
<span id="cb118-67"><a href="#cb118-67" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="st">'Evaluating and saving model after training'</span>)</span>
<span id="cb118-68"><a href="#cb118-68" aria-hidden="true" tabindex="-1"></a>eval_loss, perplexity <span class="op">=</span> evaluate()</span>
<span id="cb118-69"><a href="#cb118-69" aria-hidden="true" tabindex="-1"></a>log_metrics(step, {<span class="st">'loss/eval'</span>: eval_loss, <span class="st">'perplexity'</span>: perplexity})</span>
<span id="cb118-70"><a href="#cb118-70" aria-hidden="true" tabindex="-1"></a>accelerator.wait_for_everyone()</span>
<span id="cb118-71"><a href="#cb118-71" aria-hidden="true" tabindex="-1"></a>unwrapped_model <span class="op">=</span> accelerator.unwrap_model(model)</span>
<span id="cb118-72"><a href="#cb118-72" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> accelerator.is_main_process:</span>
<span id="cb118-73"><a href="#cb118-73" aria-hidden="true" tabindex="-1"></a>    unwrapped_model.save_pretrained(<span class="st">"./"</span>)</span>
<span id="cb118-74"><a href="#cb118-74" aria-hidden="true" tabindex="-1"></a>    hf_repo.push_to_hub(commit_message<span class="op">=</span><span class="ss">f'final model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong></p>
<ul>
<li>here are several approaches to distributed training depending on the model size and volume of data.</li>
<li>Hugging Face Accelerate uses <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DataDistributedParalellism (DDP)</a>.</li>
<li>DDP allows you to train models faster with larger batch sizes that wouldn’t fit into any single GPU.</li>
<li>Hugging Face Accelerate prepares batches of data and sends them to the workers.</li>
<li>Each worker consists of a GPU and calculates the loss and their respective accumulated gradients from forward and backward passes with a local copy of the model.</li>
<li>We average the gradients from each node with a <code>reduce</code> pattern and send the average back to each worker.</li>
<li>We apply the gradients using the optimizer on each node to avoid transferring copies of the large models between nodes.</li>
<li>We repeat the process after updating the models for each worker.</li>
<li>DDP requires that the model fits on a single GPU.</li>
<li><a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">Fitting larger networks into memory.</a></li>
<li><a href="https://huggingface.co/docs/transformers/main/en/parallelism">Model Paralellism</a></li>
</ul>
<hr>
</section>
</section>
<section id="the-training-run" class="level3">
<h3 class="anchored" data-anchor-id="the-training-run">The Training Run</h3>
<ul>
<li>We can save the training steps to a script and push them to a repository on the Hub.</li>
<li>We can then execute the training script on a training server using the <code>accelerate launch</code> command.</li>
</ul>
<div class="sourceCode" id="cb119"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://huggingface.co/transformerbook/codeparrot</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> codeparrot</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb</span> login</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> config</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> launch codparrot_training.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>The <code>accelerate config</code> command guides you through setting up the infrastructure.</li>
<li>Hugging Face uses <a href="https://cloud.google.com/blog/products/compute/announcing-google-cloud-a2-vm-family-based-on-nvidia-a100-gpu"><code>a2-megagpu-16g</code></a> instances on Google Cloud for experiments (<a href="https://www.economize.cloud/gcp/pricing/a2/a2-megagpu-16g">pricing</a>).</li>
<li><a href="https://spltech.co.uk/reducing-90-in-costs-with-spot-vms-for-machine-learning-on-google-kubernetes-engine-in-gcp/">Reducing 90% in costs with Spot VMs for Machine Learning on Google Kubernetes Engine in GCP</a></li>
</ul>
<p><strong>Configuration used to train CodeParrot models</strong></p>
<table class="table">
<thead>
<tr class="header">
<th>Setting</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Compute Environment?</td>
<td>multi-gpu</td>
</tr>
<tr class="even">
<td>How many machines?</td>
<td>1</td>
</tr>
<tr class="odd">
<td>DeepSpeed?</td>
<td>No</td>
</tr>
<tr class="even">
<td>How many processes?</td>
<td>16</td>
</tr>
<tr class="odd">
<td>Use FP16?</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<ul>
<li>Running the training script with the above settings takes about 24 hours for the small model and seven days for the large model.</li>
<li>Test the code on smaller infrastructure before using expensive cloud instances.</li>
<li>We can merge the experiment branch back into the main one after training completes.</li>
</ul>
<div class="sourceCode" id="cb120"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> checkout main</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> merge <span class="op">&lt;</span>RUN_NAME<span class="op">&gt;</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> push</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="results-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h2>
<ul>
<li>The training loss and validation perplexity should go down continuously during training.</li>
<li>The large model converges with fewer processed tokens, but training takes longer overall.</li>
<li>Qualitative analysis involves looking at concrete examples and trying to better understand in which cases the model succeeds and fails.</li>
<li>Quantitative analysis involves evaluating model performance statistically on a large set of test cases.</li>
</ul>
<hr>
<div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline, set_seed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Wrap the small model in a text generation pipeline</strong></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">'transformersbook/codeparrot-small'</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_ckpt, device<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> set_seed </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to extract the first code block from the model output</strong></p>
<div class="sourceCode" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> first_block(string):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> re.split(<span class="st">'</span><span class="ch">\n</span><span class="st">class|</span><span class="ch">\n</span><span class="st">def|</span><span class="ch">\n</span><span class="st">#|</span><span class="ch">\n</span><span class="st">@|</span><span class="ch">\n</span><span class="st">print|</span><span class="ch">\n</span><span class="st">if'</span>, string)[<span class="dv">0</span>].rstrip()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to print out generated code completions</strong></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> complete_code(pipe, prompt, max_length<span class="op">=</span><span class="dv">64</span>, num_completions<span class="op">=</span><span class="dv">4</span>, seed<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    set_seed(seed)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    gen_kwargs <span class="op">=</span> {<span class="st">"temperature"</span>:<span class="fl">0.4</span>, <span class="st">"top_p"</span>:<span class="fl">0.95</span>, <span class="st">"top_k"</span>:<span class="dv">0</span>, <span class="st">"num_beams"</span>:<span class="dv">1</span>,</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"do_sample"</span>:<span class="va">True</span>,}</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>    code_gens <span class="op">=</span> generation(prompt, num_return_sequences<span class="op">=</span>num_completions, </span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>                            max_length<span class="op">=</span>max_length, <span class="op">**</span>gen_kwargs)</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>    code_strings <span class="op">=</span> []</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> code_gen <span class="kw">in</span> code_gens:</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>        generated_code <span class="op">=</span> first_block(code_gen[<span class="st">'generated_text'</span>][<span class="bu">len</span>(prompt):])</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>        code_strings.append(generated_code)</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>((<span class="st">'</span><span class="ch">\n</span><span class="st">'</span><span class="op">+</span><span class="st">'='</span><span class="op">*</span><span class="dv">80</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>).join(code_strings))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test the model on a simple task</strong></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'''def area_of_rectangle(a: float, b: float):</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="st">    """Return the area of the rectangle."""'''</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>complete_code(generation, prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>
        return math.sqrt(a * b)
    ================================================================================
    
        return a * b / 2.0
    ================================================================================
    
        return a * b
    ================================================================================
    
        return a * b / 2.0</code></pre>
<p><strong>Note:</strong> The generated outputs look convincing, but not all of them are correct.</p>
<hr>
<p><strong>Test the model on a more complex task</strong></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'''def get_urls_from_html(html):</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="st">    """Get all embedded URLs in a HTML string."""'''</span></span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>complete_code(generation, prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>
        if not html:
            return []
        return [url for url in re.findall(r'&lt;a href="(/[^/]+/[^"]+?)"&gt;', html)]
    ================================================================================
    
        return [url for url in re.findall(r'&lt;a href="(.*?)"', html)
                if url]
    ================================================================================
    
        return [url for url in re.findall(r'&lt;a href="(.*?)"', html)]
    ================================================================================
    
        return re.findall(r'&lt;a href="([^"]+)"&gt;', html)</code></pre>
<p><strong>Note:</strong> The second attempt is not quite right, but the other three generations are correct.</p>
<hr>
<p><strong>Test the generated code</strong></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_urls_from_html(html):</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [url <span class="cf">for</span> url <span class="kw">in</span> re.findall(<span class="vs">r'&lt;a href="(.*?)"'</span>, html) <span class="cf">if</span> url]</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(get_urls_from_html(requests.get(<span class="st">'https://hf.co/'</span>).text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
https://huggingface.co/bigscience/tr11-176B-ml-logs
</td>
</tr>
<tr>
<th>
1
</th>
<td>
https://github.com/huggingface/transformers
</td>
</tr>
<tr>
<th>
2
</th>
<td>
/join
</td>
</tr>
<tr>
<th>
3
</th>
<td>
/tasks
</td>
</tr>
<tr>
<th>
4
</th>
<td>
https://huggingface.co/transformers
</td>
</tr>
<tr>
<th>
5
</th>
<td>
/inference-api
</td>
</tr>
<tr>
<th>
6
</th>
<td>
/distilbert-base-uncased
</td>
</tr>
<tr>
<th>
7
</th>
<td>
/dbmdz/bert-large-cased-finetuned-conll03-english
</td>
</tr>
<tr>
<th>
8
</th>
<td>
https://bigscience.huggingface.co/
</td>
</tr>
<tr>
<th>
9
</th>
<td>
https://bigscience.huggingface.co/blog/t0
</td>
</tr>
<tr>
<th>
10
</th>
<td>
https://medium.com/huggingface/distilbert-8cf3380435b5
</td>
</tr>
<tr>
<th>
11
</th>
<td>
https://arxiv.org/abs/1811.06031
</td>
</tr>
<tr>
<th>
12
</th>
<td>
https://arxiv.org/abs/1803.10631
</td>
</tr>
<tr>
<th>
13
</th>
<td>
/coref
</td>
</tr>
<tr>
<th>
14
</th>
<td>
https://transformer.huggingface.co/
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The URLs starting with <code>https</code> are external pages, while the others are subpages of the main website.</p>
<hr>
<p><strong>Wrap the large model in a text generation pipeline</strong></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">'transformersbook/codeparrot'</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_ckpt, device<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Try to translate a function from pure Python to NumPy using the large model</strong></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'''# a function in native python:</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="st">def mean(a):</span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="st">    return sum(a)/len(a)</span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="st"># the same function using numpy:</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="st">import numpy as np</span></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="st">def mean(a):'''</span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>complete_code(generation, prompt, max_length<span class="op">=</span><span class="dv">64</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>
        return np.mean(a)
    ================================================================================
    
        return sum(a)/len(a)
    ================================================================================
    
        return np.mean(a)
    ================================================================================
    
        return sum(a)/len(a)</code></pre>
<p><strong>Note:</strong> It worked.</p>
<hr>
<p><strong>Try building a Scilit-learn model</strong></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'''X = np.random.randn(100, 100)</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="st">y = np.random.randint(0, 1, 100)</span></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="st"># fit random forest classifier with 20 estimators'''</span></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>complete_code(generation, prompt, max_length<span class="op">=</span><span class="dv">96</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>
    reg = DummyRegressor()
    
    forest = RandomForestClassifier(n_estimators=20)
    
    forest.fit(X, y)
    ================================================================================
    
    clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')
    clf.fit(X, y)
    ================================================================================
    
    clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)
    clf.fit(X, y)
    ================================================================================
    
    clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)
    clf.fit(X, y)</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The second attempt used an <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">extra-trees classifier</a>, but the other three generated what we asked.</li>
<li>The BLEU score is not well suited for measuring the quality of generated code as it would punish a generation that deviates from the reference naming.</li>
<li>The success of a program does not depend on the naming scheme as long as it is consistent.</li>
<li>We can use traditional software development methods like unit tests to measure the quality of generated code.</li>
<li>OpenAI evaluated Codex models by running several code generations for coding tasks through some unit tests and calculating the fraction that passes the tests.</li>
<li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
</ul>
<hr>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-9/">Notes on Transformers Book Ch. 9</a></p>
<p><strong>Next:</strong> <a href="../chapter-11/">Notes on Transformers Book Ch. 11</a></p>
<!-- Cloudflare Web Analytics -->
<script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script>
<!-- End Cloudflare Web Analytics -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2023, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>