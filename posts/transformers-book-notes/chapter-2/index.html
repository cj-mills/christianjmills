<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-01">
<meta name="description" content="Chapter 2 covers training a model to classify emotions expressed in Twitter messages.">

<title>Notes on Transformers Book Ch. 2 – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Notes on Transformers Book Ch. 2 – Christian Mills">
<meta property="og:description" content="Chapter 2 covers training a model to classify emotions expressed in Twitter messages.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Notes on Transformers Book Ch. 2 – Christian Mills">
<meta name="twitter:description" content="Chapter 2 covers training a model to classify emotions expressed in Twitter messages.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#project-analyze-product-sentiment-on-twitter" id="toc-project-analyze-product-sentiment-on-twitter" class="nav-link active" data-scroll-target="#project-analyze-product-sentiment-on-twitter">Project: Analyze Product Sentiment on Twitter</a>
  <ul>
  <li><a href="#hugging-face-project-pipeline" id="toc-hugging-face-project-pipeline" class="nav-link" data-scroll-target="#hugging-face-project-pipeline">Hugging Face Project Pipeline:</a></li>
  </ul></li>
  <li><a href="#the-dataset" id="toc-the-dataset" class="nav-link" data-scroll-target="#the-dataset">The Dataset</a>
  <ul>
  <li><a href="#a-first-look-at-hugging-face-datasets" id="toc-a-first-look-at-hugging-face-datasets" class="nav-link" data-scroll-target="#a-first-look-at-hugging-face-datasets">A First Look at Hugging Face Datasets</a>
  <ul class="collapse">
  <li><a href="#load_dataset" id="toc-load_dataset" class="nav-link" data-scroll-target="#load_dataset"><code>load_dataset</code></a></li>
  <li><a href="#methods-to-load-common-data-formats" id="toc-methods-to-load-common-data-formats" class="nav-link" data-scroll-target="#methods-to-load-common-data-formats">Methods to Load Common Data Formats</a></li>
  <li><a href="#automated-process" id="toc-automated-process" class="nav-link" data-scroll-target="#automated-process">Automated Process</a></li>
  <li><a href="#manual-process---local" id="toc-manual-process---local" class="nav-link" data-scroll-target="#manual-process---local">Manual Process - Local</a></li>
  <li><a href="#manual-process---remote" id="toc-manual-process---remote" class="nav-link" data-scroll-target="#manual-process---remote">Manual Process - Remote</a></li>
  <li><a href="#datasetdict" id="toc-datasetdict" class="nav-link" data-scroll-target="#datasetdict"><code>DatasetDict</code></a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset"><code>Dataset</code></a></li>
  <li><a href="#classlabel" id="toc-classlabel" class="nav-link" data-scroll-target="#classlabel"><code>ClassLabel</code></a></li>
  <li><a href="#value" id="toc-value" class="nav-link" data-scroll-target="#value"><code>Value</code></a></li>
  </ul></li>
  <li><a href="#from-datasets-to-dataframes" id="toc-from-datasets-to-dataframes" class="nav-link" data-scroll-target="#from-datasets-to-dataframes">From Datasets to DataFrames</a>
  <ul class="collapse">
  <li><a href="#datasetdict.set_format" id="toc-datasetdict.set_format" class="nav-link" data-scroll-target="#datasetdict.set_format"><code>DatasetDict.set_format</code></a></li>
  <li><a href="#dataset.set_format" id="toc-dataset.set_format" class="nav-link" data-scroll-target="#dataset.set_format"><code>Dataset.set_format</code></a></li>
  <li><a href="#classlabel.int2str" id="toc-classlabel.int2str" class="nav-link" data-scroll-target="#classlabel.int2str"><code>ClassLabel.int2str</code></a></li>
  </ul></li>
  <li><a href="#looking-at-the-class-distribution" id="toc-looking-at-the-class-distribution" class="nav-link" data-scroll-target="#looking-at-the-class-distribution">Looking at the Class Distribution</a>
  <ul class="collapse">
  <li><a href="#methods-to-deal-with-imbalanced-data" id="toc-methods-to-deal-with-imbalanced-data" class="nav-link" data-scroll-target="#methods-to-deal-with-imbalanced-data">Methods to Deal with Imbalanced Data</a></li>
  <li><a href="#imbalanced-learn" id="toc-imbalanced-learn" class="nav-link" data-scroll-target="#imbalanced-learn">imbalanced-learn</a></li>
  </ul></li>
  <li><a href="#how-long-are-our-tweets" id="toc-how-long-are-our-tweets" class="nav-link" data-scroll-target="#how-long-are-our-tweets">How Long Are Our Tweets?</a>
  <ul class="collapse">
  <li><a href="#datasetdict.reset_format" id="toc-datasetdict.reset_format" class="nav-link" data-scroll-target="#datasetdict.reset_format"><code>DatasetDict.reset_format()</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#from-text-to-tokens" id="toc-from-text-to-tokens" class="nav-link" data-scroll-target="#from-text-to-tokens">From Text to Tokens</a>
  <ul>
  <li><a href="#character-tokenization" id="toc-character-tokenization" class="nav-link" data-scroll-target="#character-tokenization">Character Tokenization</a></li>
  <li><a href="#numericalization" id="toc-numericalization" class="nav-link" data-scroll-target="#numericalization">Numericalization</a></li>
  <li><a href="#one-hot-encoding" id="toc-one-hot-encoding" class="nav-link" data-scroll-target="#one-hot-encoding">One-hot Encoding</a>
  <ul class="collapse">
  <li><a href="#pytorch-one_hot" id="toc-pytorch-one_hot" class="nav-link" data-scroll-target="#pytorch-one_hot">PyTorch <code>one_hot</code>:</a></li>
  </ul></li>
  <li><a href="#word-tokenization" id="toc-word-tokenization" class="nav-link" data-scroll-target="#word-tokenization">Word Tokenization</a></li>
  <li><a href="#subword-tokenization" id="toc-subword-tokenization" class="nav-link" data-scroll-target="#subword-tokenization">Subword Tokenization</a>
  <ul class="collapse">
  <li><a href="#autotokenizer" id="toc-autotokenizer" class="nav-link" data-scroll-target="#autotokenizer"><code>AutoTokenizer</code></a></li>
  <li><a href="#distilberttokenizerfast" id="toc-distilberttokenizerfast" class="nav-link" data-scroll-target="#distilberttokenizerfast"><code>DistilBertTokenizerFast</code></a></li>
  <li><a href="#convert_ids_to_tokens" id="toc-convert_ids_to_tokens" class="nav-link" data-scroll-target="#convert_ids_to_tokens"><code>convert_ids_to_tokens</code></a></li>
  </ul></li>
  <li><a href="#tokenizing-the-whole-dataset" id="toc-tokenizing-the-whole-dataset" class="nav-link" data-scroll-target="#tokenizing-the-whole-dataset">Tokenizing the Whole Dataset</a>
  <ul class="collapse">
  <li><a href="#datasetdict.map" id="toc-datasetdict.map" class="nav-link" data-scroll-target="#datasetdict.map"><code>DatasetDict.map</code></a></li>
  <li><a href="#dataset.map" id="toc-dataset.map" class="nav-link" data-scroll-target="#dataset.map"><code>Dataset.map</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#training-a-text-classifier" id="toc-training-a-text-classifier" class="nav-link" data-scroll-target="#training-a-text-classifier">Training a Text Classifier</a>
  <ul>
  <li><a href="#architecture-of-an-encoder-based-classifier" id="toc-architecture-of-an-encoder-based-classifier" class="nav-link" data-scroll-target="#architecture-of-an-encoder-based-classifier">Architecture of an Encoder-Based Classifier</a></li>
  <li><a href="#methods-to-train-a-text-classifier" id="toc-methods-to-train-a-text-classifier" class="nav-link" data-scroll-target="#methods-to-train-a-text-classifier">Methods to Train a Text Classifier</a>
  <ul class="collapse">
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction">Feature Extraction</a></li>
  <li><a href="#fine-tuning" id="toc-fine-tuning" class="nav-link" data-scroll-target="#fine-tuning">Fine-tuning</a></li>
  </ul></li>
  <li><a href="#transformers-as-feature-extractors" id="toc-transformers-as-feature-extractors" class="nav-link" data-scroll-target="#transformers-as-feature-extractors">Transformers as Feature Extractors</a>
  <ul class="collapse">
  <li><a href="#using-pretrained-models" id="toc-using-pretrained-models" class="nav-link" data-scroll-target="#using-pretrained-models">Using pretrained models</a></li>
  <li><a href="#automodel.from_pretrained" id="toc-automodel.from_pretrained" class="nav-link" data-scroll-target="#automodel.from_pretrained"><code>AutoModel.from_pretrained</code></a></li>
  <li><a href="#extracting-the-last-hidden-states" id="toc-extracting-the-last-hidden-states" class="nav-link" data-scroll-target="#extracting-the-last-hidden-states">Extracting the last hidden states</a></li>
  <li><a href="#creating-a-feature-matrix" id="toc-creating-a-feature-matrix" class="nav-link" data-scroll-target="#creating-a-feature-matrix">Creating a feature matrix</a></li>
  <li><a href="#visualizing-the-training-set" id="toc-visualizing-the-training-set" class="nav-link" data-scroll-target="#visualizing-the-training-set">Visualizing the training set</a></li>
  <li><a href="#umap-uniform-manifold-approximation-and-projection-for-dimension-reduction" id="toc-umap-uniform-manifold-approximation-and-projection-for-dimension-reduction" class="nav-link" data-scroll-target="#umap-uniform-manifold-approximation-and-projection-for-dimension-reduction">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
  <li><a href="#scit-kit-learn-minmaxscaler" id="toc-scit-kit-learn-minmaxscaler" class="nav-link" data-scroll-target="#scit-kit-learn-minmaxscaler">Scit-Kit Learn MinMaxScaler</a></li>
  <li><a href="#matplotlib.pyplot.hexbin" id="toc-matplotlib.pyplot.hexbin" class="nav-link" data-scroll-target="#matplotlib.pyplot.hexbin"><code>matplotlib.pyplot.hexbin</code></a></li>
  <li><a href="#training-a-simple-classifier" id="toc-training-a-simple-classifier" class="nav-link" data-scroll-target="#training-a-simple-classifier">Training a simple classifier</a></li>
  <li><a href="#sklearn.linear_model.logisticregression" id="toc-sklearn.linear_model.logisticregression" class="nav-link" data-scroll-target="#sklearn.linear_model.logisticregression"><code>sklearn.linear_model.LogisticRegression</code></a></li>
  <li><a href="#sklearn.dummy.dummyclassifier" id="toc-sklearn.dummy.dummyclassifier" class="nav-link" data-scroll-target="#sklearn.dummy.dummyclassifier"><code>sklearn.dummy.DummyClassifier</code></a></li>
  <li><a href="#sklearn.metrics.confusionmatrixdisplay" id="toc-sklearn.metrics.confusionmatrixdisplay" class="nav-link" data-scroll-target="#sklearn.metrics.confusionmatrixdisplay"><code>sklearn.metrics.ConfusionMatrixDisplay</code></a></li>
  <li><a href="#sklearn.metrics.confusion_matrix" id="toc-sklearn.metrics.confusion_matrix" class="nav-link" data-scroll-target="#sklearn.metrics.confusion_matrix"><code>sklearn.metrics.confusion_matrix</code></a></li>
  </ul></li>
  <li><a href="#fine-tuning-transformers" id="toc-fine-tuning-transformers" class="nav-link" data-scroll-target="#fine-tuning-transformers">Fine-Tuning Transformers</a>
  <ul class="collapse">
  <li><a href="#loading-a-pretrained-model" id="toc-loading-a-pretrained-model" class="nav-link" data-scroll-target="#loading-a-pretrained-model">Loading a pretrained model</a></li>
  <li><a href="#automodelforsequenceclassification.from_pretrained" id="toc-automodelforsequenceclassification.from_pretrained" class="nav-link" data-scroll-target="#automodelforsequenceclassification.from_pretrained"><code>AutoModelForSequenceClassification.from_pretrained</code></a></li>
  <li><a href="#distilbertforsequenceclassification" id="toc-distilbertforsequenceclassification" class="nav-link" data-scroll-target="#distilbertforsequenceclassification"><code>DistilBertForSequenceClassification</code></a></li>
  <li><a href="#defining-the-performance-metrics" id="toc-defining-the-performance-metrics" class="nav-link" data-scroll-target="#defining-the-performance-metrics">Defining the performance metrics</a></li>
  <li><a href="#sklearn.metrics.f1_score" id="toc-sklearn.metrics.f1_score" class="nav-link" data-scroll-target="#sklearn.metrics.f1_score"><code>sklearn.metrics.f1_score</code></a></li>
  <li><a href="#sklearn.metrics.accuracy_score" id="toc-sklearn.metrics.accuracy_score" class="nav-link" data-scroll-target="#sklearn.metrics.accuracy_score"><code>sklearn.metrics.accuracy_score</code></a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">Training the model</a></li>
  <li><a href="#trainingarguments" id="toc-trainingarguments" class="nav-link" data-scroll-target="#trainingarguments"><code>TrainingArguments</code></a></li>
  <li><a href="#trainer" id="toc-trainer" class="nav-link" data-scroll-target="#trainer"><code>Trainer</code></a></li>
  <li><a href="#error-analysis" id="toc-error-analysis" class="nav-link" data-scroll-target="#error-analysis">Error analysis</a></li>
  <li><a href="#saving-and-sharing-the-model" id="toc-saving-and-sharing-the-model" class="nav-link" data-scroll-target="#saving-and-sharing-the-model">Saving and sharing the model</a></li>
  <li><a href="#trainer.push_to_hub" id="toc-trainer.push_to_hub" class="nav-link" data-scroll-target="#trainer.push_to_hub"><code>Trainer.push_to_hub</code></a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">Inference</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul>
  <li><a href="#nlp-challenges" id="toc-nlp-challenges" class="nav-link" data-scroll-target="#nlp-challenges">NLP Challenges</a>
  <ul class="collapse">
  <li><a href="#moving-a-model-to-production" id="toc-moving-a-model-to-production" class="nav-link" data-scroll-target="#moving-a-model-to-production">Moving a model to production</a></li>
  <li><a href="#increasing-inference-speed" id="toc-increasing-inference-speed" class="nav-link" data-scroll-target="#increasing-inference-speed">Increasing Inference Speed</a></li>
  <li><a href="#applying-a-model-to-other-tasks" id="toc-applying-a-model-to-other-tasks" class="nav-link" data-scroll-target="#applying-a-model-to-other-tasks">Applying a Model to other tasks</a></li>
  <li><a href="#using-non-english-text" id="toc-using-non-english-text" class="nav-link" data-scroll-target="#using-non-english-text">Using Non-English Text</a></li>
  <li><a href="#working-with-little-labeled-data" id="toc-working-with-little-labeled-data" class="nav-link" data-scroll-target="#working-with-little-labeled-data">Working with little labeled data</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 2 covers training a model to classify emotions expressed in Twitter messages.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 1, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/transformers-book-notes.html"><strong>Natural Language Processing with Transformers</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#project-analyze-product-sentiment-on-twitter">Project: Analyze Product Sentiment on Twitter</a></li>
<li><a href="#the-dataset">The Dataset</a></li>
<li><a href="#from-text-to-tokens">From Text to Tokens</a></li>
<li><a href="#training-a-text-classifier">Training a Text Classifier</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>transformers.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    '4.11.3'</code></pre>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>,<span class="va">None</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="project-analyze-product-sentiment-on-twitter" class="level2">
<h2 class="anchored" data-anchor-id="project-analyze-product-sentiment-on-twitter">Project: Analyze Product Sentiment on Twitter</h2>
<ul>
<li>Sentiment analysis involves classifying the feelings or opinions expressed in a given text.</li>
<li>The goal is to build a system that automatically classifies emotions expressed in Twitter messages about a product.</li>
<li>A model will take a single tweet as input and assign one of the possible labels.</li>
<li>Possible labels include anger, fear, joy, love, sadness, and surprise.</li>
<li>The project will use a variant of BERT called DistilBERT.</li>
<li><a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>
<ul>
<li>DistilBERT achieves comparable accuracy to BERT while being significantly more efficient in size and speed.</li>
<li>DistilBERT was created in 2019 by researchers at Hugging Face.</li>
</ul></li>
</ul>
<section id="hugging-face-project-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face-project-pipeline">Hugging Face Project Pipeline:</h3>
<ol type="1">
<li>Load and process datasets using the <a href="https://huggingface.co/docs/datasets/index">Datasets</a> library.</li>
<li>Tokenize input texts using the <a href="https://huggingface.co/docs/tokenizers/python/latest/">Tokenizers</a> library.</li>
<li>Load, train, and run models using the <a href="https://huggingface.co/docs/transformers/index">Transformers</a> library.</li>
<li>Load metrics and evaluate models using the <a href="https://huggingface.co/docs/datasets/index">Datasets</a> library.</li>
</ol>
</section>
</section>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The Dataset</h2>
<ul>
<li><a href="https://aclanthology.org/D18-1404/">CARER: Contextualized Affect Representations for Emotion Recognition</a>
<ul>
<li>The authors of the paper created an emotion dataset of English twitter messages.</li>
<li>The emotion dataset contains messages that express anger, fear, joy, love, sadness, or surprise.</li>
<li>Emoticons present in the tweets determine the initial labels.</li>
<li>A graph-based algorithm then uses these initial labels to construct contextualized, pattern-based emotion features.</li>
<li>Word embeddings help further enrich these features.</li>
</ul></li>
<li><a href="https://github.com/dair-ai/emotion_dataset">GitHub Repository</a></li>
<li><a href="https://huggingface.co/datasets/emotion">Hugging Face Dataset Card</a></li>
</ul>
<section id="a-first-look-at-hugging-face-datasets" class="level3">
<h3 class="anchored" data-anchor-id="a-first-look-at-hugging-face-datasets">A First Look at Hugging Face Datasets</h3>
<ul>
<li><a href="https://github.com/huggingface/datasets">GitHub Repository</a></li>
<li><a href="https://huggingface.co/docs/datasets/index">Documentation</a></li>
<li>Hugging Face Datasets is based on <a href="https://arrow.apache.org/">Apache Arrow</a>.
<ul>
<li>Apache Arrow defines a typed columnar format that is more memory efficient than native Python.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> list_datasets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>print_source(list_datasets, exclude_doc<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def list_datasets(with_community_datasets=True, with_details=False):
        datasets = huggingface_hub.list_datasets(full=with_details)
        if not with_community_datasets:
            datasets = [dataset for dataset in datasets if '/' not in dataset.id]
        if not with_details:
            datasets = [dataset.id for dataset in datasets]
        return datasets</code></pre>
<hr>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of all the datasets scripts available on the Hugging Face Hub</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>all_datasets <span class="op">=</span> list_datasets()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"There are </span><span class="sc">{</span><span class="bu">len</span>(all_datasets)<span class="sc">}</span><span class="ss"> datasets currently available on the Hub"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The first 10 are: </span><span class="sc">{</span>all_datasets[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    There are 3896 datasets currently available on the Hub
    The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']</code></pre>
<hr>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="load_dataset" class="level4">
<h4 class="anchored" data-anchor-id="load_dataset"><code>load_dataset</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/loading_methods#datasets.load_dataset">Documentation</a></li>
<li>This method downloads and imports the loading script for the specified dataset.</li>
<li>The script defines the citation, info, and format of the dataset, the URL to the original data files, and the code to load examples from the original files.</li>
<li>The script downloads the dataset files and caches them in typed <a href="https://arrow.apache.org/">Apache Arrow</a> tables.</li>
<li>Several loading scripts are available to handle local and remote datasets.</li>
</ul>
</section>
<section id="methods-to-load-common-data-formats" class="level4">
<h4 class="anchored" data-anchor-id="methods-to-load-common-data-formats">Methods to Load Common Data Formats</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 18%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Data format</th>
<th style="text-align: center;">Loading script</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">CSV</td>
<td style="text-align: center;"><code>csv</code></td>
<td><code>load_dataset("csv", data_files="my_file.csv")</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">Text</td>
<td style="text-align: center;"><code>text</code></td>
<td><code>load_dataset("text", data_files="my_file.txt")</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">JSON</td>
<td style="text-align: center;"><code>json</code></td>
<td><code>load_dataset("json", data_files="my_file.jsonl")</code></td>
</tr>
</tbody>
</table>
<hr>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>print_source(load_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def load_dataset(path: str, name: Optional[str]=None, data_dir: Optional[
        str]=None, data_files: Optional[Union[str, Sequence[str], Mapping[str,
        Union[str, Sequence[str]]]]]=None, split: Optional[Union[str, Split]]=
        None, cache_dir: Optional[str]=None, features: Optional[Features]=None,
        download_config: Optional[DownloadConfig]=None, download_mode: Optional
        [GenerateMode]=None, ignore_verifications: bool=False, keep_in_memory:
        Optional[bool]=None, save_infos: bool=False, revision: Optional[Union[
        str, Version]]=None, use_auth_token: Optional[Union[bool, str]]=None,
        task: Optional[Union[str, TaskTemplate]]=None, streaming: bool=False,
        script_version='deprecated', **config_kwargs) -&gt;Union[DatasetDict,
        Dataset, IterableDatasetDict, IterableDataset]:
        if script_version != 'deprecated':
            warnings.warn(
                "'script_version' was renamed to 'revision' in version 1.13 and will be removed in 1.15."
                , FutureWarning)
            revision = script_version
        ignore_verifications = ignore_verifications or save_infos
        builder_instance = load_dataset_builder(path=path, name=name, data_dir=
            data_dir, data_files=data_files, cache_dir=cache_dir, features=
            features, download_config=download_config, download_mode=
            download_mode, revision=revision, use_auth_token=use_auth_token, **
            config_kwargs)
        if streaming:
            extend_module_for_streaming(builder_instance.__module__,
                use_auth_token=use_auth_token)
            if not builder_instance.__module__.startswith('datasets.'):
                for imports in get_imports(inspect.getfile(builder_instance.
                    __class__)):
                    if imports[0] == 'internal':
                        internal_import_name = imports[1]
                        internal_module_name = '.'.join(builder_instance.
                            __module__.split('.')[:-1] + [internal_import_name])
                        extend_module_for_streaming(internal_module_name,
                            use_auth_token=use_auth_token)
            return builder_instance.as_streaming_dataset(split=split,
                use_auth_token=use_auth_token)
        try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES
        builder_instance.download_and_prepare(download_config=download_config,
            download_mode=download_mode, ignore_verifications=
            ignore_verifications, try_from_hf_gcs=try_from_hf_gcs,
            use_auth_token=use_auth_token)
        keep_in_memory = (keep_in_memory if keep_in_memory is not None else
            is_small_dataset(builder_instance.info.dataset_size))
        ds = builder_instance.as_dataset(split=split, ignore_verifications=
            ignore_verifications, in_memory=keep_in_memory)
        if task is not None:
            ds = ds.prepare_for_task(task)
        if save_infos:
            builder_instance._save_infos()
        return ds</code></pre>
</section>
<section id="automated-process" class="level4">
<h4 class="anchored" data-anchor-id="automated-process">Automated Process</h4>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download dataset from Hub</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>emotions <span class="op">=</span> load_dataset(<span class="st">"emotion"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(<span class="bu">list</span>(emotions.cache_files.items()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
train
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-train.arrow’}]
</td>
</tr>
<tr>
<th>
1
</th>
<td>
validation
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-validation.arrow’}]
</td>
</tr>
<tr>
<th>
2
</th>
<td>
test
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/emotion-test.arrow’}]
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="manual-process---local" class="level4">
<h4 class="anchored" data-anchor-id="manual-process---local">Manual Process - Local</h4>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the download URLs</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>urls <span class="op">=</span> <span class="bu">list</span>(emotions[<span class="st">'train'</span>].info.download_checksums.keys())</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>urls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1',
     'https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1',
     'https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1']</code></pre>
<hr>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download each dataset to current directory</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> url <span class="kw">in</span> urls:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove url parameters</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> url.split(<span class="st">'?'</span>)[<span class="dv">0</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run the wget shell command in the jupyter notebook</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>wget $url</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    --2022-04-01 11:59:26--  https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt
    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212
    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: /s/raw/1pzkadrvffbqw6o/train.txt [following]
    --2022-04-01 11:59:26--  https://www.dropbox.com/s/raw/1pzkadrvffbqw6o/train.txt
    Reusing existing connection to www.dropbox.com:443.
    HTTP request sent, awaiting response... 302 Found
    Location: https://ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com/cd/0/inline/BimHlqS8EcLnVO8-ErygeREvWupg-stxp_BKxrhhRBD8zXEOdQ5P-ssnFHFhv63Jx0wos3YwmuzmYs4Ex3iGW6lF430Y2yc4Y-ro00V20otuMPHh1I7x6YnZWmMe_xQOeM_-RNv_CbVeXC2wxDFZxE-TWzFuwjHo-RUy7RcwlYWMng/file# [following]
    --2022-04-01 11:59:26--  https://ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com/cd/0/inline/BimHlqS8EcLnVO8-ErygeREvWupg-stxp_BKxrhhRBD8zXEOdQ5P-ssnFHFhv63Jx0wos3YwmuzmYs4Ex3iGW6lF430Y2yc4Y-ro00V20otuMPHh1I7x6YnZWmMe_xQOeM_-RNv_CbVeXC2wxDFZxE-TWzFuwjHo-RUy7RcwlYWMng/file
    Resolving ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com (ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f
    Connecting to ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com (ucd8bd8ccbe834141eed1bd4fe3f.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 1658616 (1.6M) [text/plain]
    Saving to: ‘train.txt.8’
    
    train.txt.8         100%[===================&gt;]   1.58M  --.-KB/s    in 0.1s    
    
    2022-04-01 11:59:27 (12.6 MB/s) - ‘train.txt.8’ saved [1658616/1658616]
    
    --2022-04-01 11:59:27--  https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt
    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212
    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: /s/raw/2mzialpsgf9k5l3/val.txt [following]
    --2022-04-01 11:59:27--  https://www.dropbox.com/s/raw/2mzialpsgf9k5l3/val.txt
    Reusing existing connection to www.dropbox.com:443.
    HTTP request sent, awaiting response... 302 Found
    Location: https://ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com/cd/0/inline/BinpUxjuQUPZKSAw9nVygw-6QF-JqzCuvRo2N8QqZPM8-Aqp5PxM0tHDJ3zclYqIKMhc_9_ORaLBDtdxeknAqfm_e3E0QJIYPA4tUpTQ7h31LAD_sc__6kyvioIZzjK61S5MlbTyM3YUMq3gPYMRH9_XE5gYrjnC1pddo3lRgrcUrg/file# [following]
    --2022-04-01 11:59:27--  https://ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com/cd/0/inline/BinpUxjuQUPZKSAw9nVygw-6QF-JqzCuvRo2N8QqZPM8-Aqp5PxM0tHDJ3zclYqIKMhc_9_ORaLBDtdxeknAqfm_e3E0QJIYPA4tUpTQ7h31LAD_sc__6kyvioIZzjK61S5MlbTyM3YUMq3gPYMRH9_XE5gYrjnC1pddo3lRgrcUrg/file
    Resolving ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com (ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f
    Connecting to ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com (ucd7c254cf6c0298b8fdea83c996.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 204240 (199K) [text/plain]
    Saving to: ‘val.txt.8’
    
    val.txt.8           100%[===================&gt;] 199.45K  --.-KB/s    in 0.09s   
    
    2022-04-01 11:59:28 (2.23 MB/s) - ‘val.txt.8’ saved [204240/204240]
    
    --2022-04-01 11:59:28--  https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt
    Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6017:18::a27d:212
    Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: /s/raw/ikkqxfdbdec3fuj/test.txt [following]
    --2022-04-01 11:59:28--  https://www.dropbox.com/s/raw/ikkqxfdbdec3fuj/test.txt
    Reusing existing connection to www.dropbox.com:443.
    HTTP request sent, awaiting response... 302 Found
    Location: https://uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com/cd/0/inline/BileG7vM49CD4NPqfWBG0td8OcodftXS6fihHcq6NCZrPE8Xn9puhgIP1mCk-KXlQnwxW_3WTCdvFmmavZXbvU5qj_mu4PoCB4quNit8j4vVynpa3QWMxcPTiHfQB8UgZaKz319rr67HSjySTKFR1xvmTxTwZIsB0Ixss_Bem8ixQg/file# [following]
    --2022-04-01 11:59:28--  https://uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com/cd/0/inline/BileG7vM49CD4NPqfWBG0td8OcodftXS6fihHcq6NCZrPE8Xn9puhgIP1mCk-KXlQnwxW_3WTCdvFmmavZXbvU5qj_mu4PoCB4quNit8j4vVynpa3QWMxcPTiHfQB8UgZaKz319rr67HSjySTKFR1xvmTxTwZIsB0Ixss_Bem8ixQg/file
    Resolving uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com (uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6017:15::a27d:20f
    Connecting to uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com (uc6a6ed094f33148a8d600d0bd94.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 206760 (202K) [text/plain]
    Saving to: ‘test.txt.8’
    
    test.txt.8          100%[===================&gt;] 201.91K  --.-KB/s    in 0.07s   
    
    2022-04-01 11:59:29 (2.95 MB/s) - ‘test.txt.8’ saved [206760/206760]</code></pre>
<hr>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>head <span class="op">-</span><span class="dv">5</span> train.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    i didnt feel humiliated;sadness
    i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness
    im grabbing a minute to post i feel greedy wrong;anger
    i am ever feeling nostalgic about the fireplace i will know that it is still on the property;love
    i am feeling grouchy;anger</code></pre>
<hr>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>dataset_names <span class="op">=</span> [<span class="st">'train'</span>, <span class="st">'validation'</span>, <span class="st">'test'</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>file_names <span class="op">=</span> [url.split(<span class="st">'?'</span>)[<span class="dv">0</span>].split(<span class="st">'/'</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> url <span class="kw">in</span> urls]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>data_files<span class="op">=</span>{name:<span class="bu">file</span> <span class="cf">for</span> name,<span class="bu">file</span> <span class="kw">in</span> <span class="bu">zip</span>(dataset_names, file_names)}</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>data_files</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'train': 'train.txt', 'validation': 'val.txt', 'test': 'test.txt'}</code></pre>
<hr>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>emotions_local <span class="op">=</span> load_dataset(<span class="st">"csv"</span>, data_files<span class="op">=</span>data_files, sep<span class="op">=</span><span class="st">";"</span>, names<span class="op">=</span>[<span class="st">"text"</span>, <span class="st">"label"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>emotions_local</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['text', 'label'],
            num_rows: 16000
        })
        validation: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
        test: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
    })</code></pre>
<hr>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(<span class="bu">list</span>(emotions_local.cache_files.items()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
train
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-train.arrow’}]
</td>
</tr>
<tr>
<th>
1
</th>
<td>
validation
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-validation.arrow’}]
</td>
</tr>
<tr>
<th>
2
</th>
<td>
test
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-88fded83f2f02d15/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-test.arrow’}]
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="manual-process---remote" class="level4">
<h4 class="anchored" data-anchor-id="manual-process---remote">Manual Process - Remote</h4>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>data_files <span class="op">=</span> {name:url <span class="cf">for</span> name,url <span class="kw">in</span> <span class="bu">zip</span>(dataset_names,urls)}</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>data_files</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'train': 'https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1',
     'validation': 'https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1',
     'test': 'https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1'}</code></pre>
<hr>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>emotions_remote <span class="op">=</span> load_dataset(<span class="st">"csv"</span>, data_files<span class="op">=</span>data_files, sep<span class="op">=</span><span class="st">";"</span>, names<span class="op">=</span>[<span class="st">"text"</span>, <span class="st">"label"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>emotions_remote</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['text', 'label'],
            num_rows: 16000
        })
        validation: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
        test: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
    })</code></pre>
<hr>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(<span class="bu">list</span>(emotions_remote.cache_files.items()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
train
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-train.arrow’}]
</td>
</tr>
<tr>
<th>
1
</th>
<td>
validation
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-validation.arrow’}]
</td>
</tr>
<tr>
<th>
2
</th>
<td>
test
</td>
<td>
[{‘filename’: ‘/home/innom-dt/.cache/huggingface/datasets/csv/default-6e495c0980795f6b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/csv-test.arrow’}]
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="datasetdict" class="level4">
<h4 class="anchored" data-anchor-id="datasetdict"><code>DatasetDict</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.DatasetDict">Documentation</a></li>
<li>A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)</li>
</ul>
<hr>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>emotions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['text', 'label'],
            num_rows: 16000
        })
        validation: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
        test: Dataset({
            features: ['text', 'label'],
            num_rows: 2000
        })
    })</code></pre>
<p><strong>Note:</strong> The data is already split into training, validation, and test sets.</p>
</section>
<section id="dataset" class="level4">
<h4 class="anchored" data-anchor-id="dataset"><code>Dataset</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset">Documentation</a></li>
<li>The base class datasets.Dataset implements a Dataset backed by an Apache Arrow table.</li>
<li>Behaves like an ordinary Python array or list.</li>
</ul>
<hr>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> emotions[<span class="st">"train"</span>]</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>train_ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Dataset({
        features: ['text', 'label'],
        num_rows: 16000
    })</code></pre>
<hr>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    16000</code></pre>
<hr>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>train_ds[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'text': 'i didnt feel humiliated', 'label': 0}</code></pre>
<hr>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>train_ds.column_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['text', 'label']</code></pre>
<hr>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the data types used for text and labels.</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_ds.features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}</code></pre>
<hr>
</section>
<section id="classlabel" class="level4">
<h4 class="anchored" data-anchor-id="classlabel"><code>ClassLabel</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.ClassLabel">Documentation</a></li>
<li>Feature type for integer class labels.</li>
<li>This class provides methods to convert integer labels to strings and strings to integer labels.</li>
</ul>
<hr>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>datasets.ClassLabel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    datasets.features.features.ClassLabel</code></pre>
<hr>
</section>
<section id="value" class="level4">
<h4 class="anchored" data-anchor-id="value"><code>Value</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Value">Documentation</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>datasets.Value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    datasets.features.features.Value</code></pre>
<hr>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.Value, <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @dataclass
    class Value:
        """
        The Value dtypes are as follows:
    
        null
        bool
        int8
        int16
        int32
        int64
        uint8
        uint16
        uint32
        uint64
        float16
        float32 (alias float)
        float64 (alias double)
        timestamp[(s|ms|us|ns)]
        timestamp[(s|ms|us|ns), tz=(tzstring)]
        binary
        large_binary
        string
        large_string
        """
        dtype: str
        id: Optional[str] = None
        pa_type: ClassVar[Any] = None
        _type: str = field(default='Value', init=False, repr=False)
    
        def __post_init__(self):
            if self.dtype == 'double':
                self.dtype = 'float64'
            if self.dtype == 'float':
                self.dtype = 'float32'
            self.pa_type = string_to_arrow(self.dtype)
    
        def __call__(self):
            return self.pa_type
    
        def encode_example(self, value):
            if pa.types.is_boolean(self.pa_type):
                return bool(value)
            elif pa.types.is_integer(self.pa_type):
                return int(value)
            elif pa.types.is_floating(self.pa_type):
                return float(value)
            elif pa.types.is_string(self.pa_type):
                return str(value)
            else:
                return value</code></pre>
</section>
</section>
<section id="from-datasets-to-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="from-datasets-to-dataframes">From Datasets to DataFrames</h3>
<ul>
<li>Hugging Face Datasets provides a <code>set_format</code> method to convert Datasets objects to Pandas DataFrames.</li>
<li>The underlying data format is still an Arrow table.</li>
</ul>
<section id="datasetdict.set_format" class="level4">
<h4 class="anchored" data-anchor-id="datasetdict.set_format"><code>DatasetDict.set_format</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.DatasetDict.set_format">Documentation</a></li>
<li>Set the format for every Dataset object in the dictionary.</li>
</ul>
<hr>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.DatasetDict.set_format, exclude_doc<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def set_format(self, type: Optional[str]=None, columns: Optional[List]=None,
        output_all_columns: bool=False, **format_kwargs):
        self._check_values_type()
        for dataset in self.values():
            dataset.set_format(type=type, columns=columns, output_all_columns=
                output_all_columns, **format_kwargs)</code></pre>
</section>
<section id="dataset.set_format" class="level4">
<h4 class="anchored" data-anchor-id="dataset.set_format"><code>Dataset.set_format</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.set_format">Documentation</a></li>
<li>Set the <a href="https://docs.python.org/3/reference/datamodel.html#object.__getitem__"><code>__getitem__</code></a> return format.
<ul>
<li><code>None</code> (Python object), <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>pandas</code>, <code>arrow</code></li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.Dataset.set_format, exclude_doc<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @fingerprint_transform(inplace=True)
    def set_format(self, type: Optional[str]=None, columns: Optional[List]=None,
        output_all_columns: bool=False, **format_kwargs):
        format_kwargs.update(format_kwargs.pop('format_kwargs', {}))
        type = get_format_type_from_alias(type)
        _ = get_formatter(type, features=self.features, **format_kwargs)
        if isinstance(columns, str):
            columns = [columns]
        if isinstance(columns, tuple):
            columns = list(columns)
        if columns is not None and any(col not in self._data.column_names for
            col in columns):
            raise ValueError(
                f'Columns {list(filter(lambda col: col not in self._data.column_names, columns))} not in the dataset. Current columns in the dataset: {self._data.column_names}'
                )
        if columns is not None:
            columns = columns.copy()
        self._format_type = type
        self._format_kwargs = format_kwargs
        self._format_columns = columns
        self._output_all_columns = output_all_columns
        logger.debug(
            'Set __getitem__(key) output type to %s for %s columns  (when key is int or slice) and %s output other (un-formatted) columns.'
            , 'python objects' if type is None else type, 'no' if columns is
            None else str(columns), 'do' if output_all_columns else "don't")</code></pre>
<hr>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>emotions.set_format(<span class="bu">type</span><span class="op">=</span><span class="st">"pandas"</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> emotions[<span class="st">"train"</span>][:]</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
text
</th>
<th>
label
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
i didnt feel humiliated
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake
</td>
<td>
0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
im grabbing a minute to post i feel greedy wrong
</td>
<td>
3
</td>
</tr>
<tr>
<th>
3
</th>
<td>
i am ever feeling nostalgic about the fireplace i will know that it is still on the property
</td>
<td>
2
</td>
</tr>
<tr>
<th>
4
</th>
<td>
i am feeling grouchy
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="classlabel.int2str" class="level4">
<h4 class="anchored" data-anchor-id="classlabel.int2str"><code>ClassLabel.int2str</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.ClassLabel.int2str">Documentation</a></li>
<li>Convert an integer label to the corresponding class name string.</li>
</ul>
<hr>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.ClassLabel.int2str)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def int2str(self, values: Union[int, Iterable]):
        assert isinstance(values, int) or isinstance(values, Iterable
            ), f'Values {values} should be an integer or an Iterable (list, numpy array, pytorch, tensorflow tensors)'
        return_list = True
        if isinstance(values, int):
            values = [values]
            return_list = False
        for v in values:
            if not 0 &lt;= v &lt; self.num_classes:
                raise ValueError(f'Invalid integer class label {v:d}')
        if self._int2str:
            output = [self._int2str[int(v)] for v in values]
        else:
            output = [str(v) for v in values]
        return output if return_list else output[0]</code></pre>
<hr>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the corresponding emotion name</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> label_int2str(row):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emotions[<span class="st">"train"</span>].features[<span class="st">"label"</span>].int2str(row)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a new column with the corresponding emotion name</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"emotion"</span>] <span class="op">=</span> df[<span class="st">"label"</span>].<span class="bu">apply</span>(label_int2str)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
text
</th>
<th>
label
</th>
<th>
emotion
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
i didnt feel humiliated
</td>
<td>
0
</td>
<td>
sadness
</td>
</tr>
<tr>
<th>
1
</th>
<td>
i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake
</td>
<td>
0
</td>
<td>
sadness
</td>
</tr>
<tr>
<th>
2
</th>
<td>
im grabbing a minute to post i feel greedy wrong
</td>
<td>
3
</td>
<td>
anger
</td>
</tr>
<tr>
<th>
3
</th>
<td>
i am ever feeling nostalgic about the fireplace i will know that it is still on the property
</td>
<td>
2
</td>
<td>
love
</td>
</tr>
<tr>
<th>
4
</th>
<td>
i am feeling grouchy
</td>
<td>
3
</td>
<td>
anger
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="looking-at-the-class-distribution" class="level3">
<h3 class="anchored" data-anchor-id="looking-at-the-class-distribution">Looking at the Class Distribution</h3>
<ul>
<li><a href="https://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a>
<ul>
<li>The first step to training a neural network involves thoroughly inspecting the data.</li>
<li>Understand the distribution of the training examples and look for patterns.</li>
</ul></li>
<li>Datasets with skewed class distribution might require a different treatment regarding the training loss and evaluation metrics.</li>
</ul>
<hr>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase the figure size</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">6</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a horizontal bar chart</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"emotion"</span>].value_counts(ascending<span class="op">=</span><span class="va">True</span>).plot.barh()</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frequency of Classes"</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the figure size</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> plt.rcParamsDefault[<span class="st">"figure.figsize"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_54_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> Messages expressing joy and sadness are about 5-10 times more common than messages expressing love and surprise.</p>
<section id="methods-to-deal-with-imbalanced-data" class="level4">
<h4 class="anchored" data-anchor-id="methods-to-deal-with-imbalanced-data">Methods to Deal with Imbalanced Data</h4>
<ul>
<li>Randomly oversample the minority class.</li>
<li>Randomly undersample the majority class.</li>
<li>Gather more labeled data from the underrepresented classes.</li>
</ul>
</section>
<section id="imbalanced-learn" class="level4">
<h4 class="anchored" data-anchor-id="imbalanced-learn">imbalanced-learn</h4>
<ul>
<li><a href="https://imbalanced-learn.org/stable/">Documentation</a></li>
<li>This library extends scikit-learn and provides tools for dealing with imbalanced classes.</li>
</ul>
</section>
</section>
<section id="how-long-are-our-tweets" class="level3">
<h3 class="anchored" data-anchor-id="how-long-are-our-tweets">How Long Are Our Tweets?</h3>
<ul>
<li>Transformer models have a maximum input sequence length called the maximum context size.</li>
<li>The maximum context size for DistilBERT is 512 tokens, which is roughly equivalent to a few paragraphs of text.</li>
<li>We need to <a href="https://huggingface.co/docs/transformers/preprocessing#truncation">truncate</a> pieces of text that do not fit in a model’s context size, which might remove crucial information.</li>
<li>We can approximate the number of tokens per twee for each emotion by looking at the distribution of words per tweet.</li>
</ul>
<hr>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase the figure size</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">7</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new column containing the number of words for each tweet</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"Words Per Tweet"</span>] <span class="op">=</span> df[<span class="st">"text"</span>].<span class="bu">str</span>.split().<span class="bu">apply</span>(<span class="bu">len</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a box plot</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>df.boxplot(<span class="st">"Words Per Tweet"</span>, by<span class="op">=</span><span class="st">"emotion"</span>, grid<span class="op">=</span><span class="va">False</span>, showfliers<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">""</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">""</span>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the figure size</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> plt.rcParamsDefault[<span class="st">"figure.figsize"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_58_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> Most tweets are between 15 and 20 words long, with a max length of around 50 words.</p>
<section id="datasetdict.reset_format" class="level4">
<h4 class="anchored" data-anchor-id="datasetdict.reset_format"><code>DatasetDict.reset_format()</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.DatasetDict.reset_format">Documentation</a></li>
<li>return format to python objects for all datasets in the dictionary</li>
<li>calls <code>set_format</code> with the default arguments</li>
</ul>
<hr>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>datasets.DatasetDict.reset_format</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    &lt;function datasets.dataset_dict.DatasetDict.reset_format(self)&gt;</code></pre>
<hr>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.DatasetDict.reset_format)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def reset_format(self):
        self._check_values_type()
        for dataset in self.values():
            dataset.set_format()</code></pre>
<hr>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.Dataset.reset_format)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def reset_format(self):
        self.set_format()</code></pre>
<hr>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>emotions.reset_format()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="from-text-to-tokens" class="level2">
<h2 class="anchored" data-anchor-id="from-text-to-tokens">From Text to Tokens</h2>
<ul>
<li>Transformer models cannot receive raw strings as input.</li>
<li>Text first needs to be tokenized and encoded as numerical vectors.</li>
<li>The three main tokenization strategies are character tokenization, word tokenization, and subword tokenization.</li>
</ul>
<p><strong>IMPORTANT:</strong> Use the same tokenizer when training, fine-tuning, and performing inference with a given model.</p>
<section id="character-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="character-tokenization">Character Tokenization</h3>
<ul>
<li>Character-based tokenizers split the text into single characters.</li>
<li>Character tokenization results in a smaller vocabulary and much fewer out-of-vocabulary tokens.</li>
<li>It also results in a much higher number of tokens for a given input sequence.</li>
<li>A character-based representation is less meaningful compared to using words.</li>
<li>The model needs to learn linguistic structures like words from the data, making training more expensive.</li>
<li>Most projects do not use character tokenization.</li>
</ul>
<hr>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenizing text is a core task of NLP."</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>tokenized_text <span class="op">=</span> <span class="bu">list</span>(text)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenized_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']</code></pre>
</section>
<section id="numericalization" class="level3">
<h3 class="anchored" data-anchor-id="numericalization">Numericalization</h3>
<ul>
<li>Models can only process numbers, so we need to encode tokens as numerical data.</li>
<li>A simple encoding method is to convert each unique token to a unique integer.</li>
</ul>
<hr>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map each unique token to a unique integer</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>token2idx <span class="op">=</span> {ch: idx <span class="cf">for</span> idx, ch <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(<span class="bu">set</span>(tokenized_text)))}</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(token2idx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}</code></pre>
<hr>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode the tokenized text</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> [token2idx[token] <span class="cf">for</span> token <span class="kw">in</span> tokenized_text]</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(input_ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]</code></pre>
</section>
<section id="one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encoding">One-hot Encoding</h3>
<ul>
<li>It is common to encode categorical variables as one-hot vectors, where a single entry has the value 1, and every other entry has the value 0.</li>
<li>One-hot encoding can help prevent the model from learning undesired relationships like fictitious ordering between names.</li>
</ul>
<hr>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample categorical data</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>categorical_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"Name"</span>: [<span class="st">"Bumblebee"</span>, <span class="st">"Optimus Prime"</span>, <span class="st">"Megatron"</span>], <span class="st">"Label ID"</span>: [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>]})</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>categorical_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Name
</th>
<th>
Label ID
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
Bumblebee
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Optimus Prime
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
Megatron
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> A model might interpret the order of values in the <code>Label ID</code> column as significant.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create one-hot vectors for each unique value in the Name column</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>pd.get_dummies(categorical_df[<span class="st">"Name"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Bumblebee
</th>
<th>
Megatron
</th>
<th>
Optimus Prime
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> We can use this approach to prevent the model from learning similar undesired relationships in the <code>input_ids</code> list.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="pytorch-one_hot" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-one_hot">PyTorch <code>one_hot</code>:</h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html">Documentation</a></li>
<li>Generate one-hot encodings for a tensor with a specified number of classes</li>
</ul>
<hr>
<div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(input_ids), <span class="bu">len</span>(token2idx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (38, 20)</code></pre>
<hr>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert input_ids list to a tensor</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor(input_ids)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate one-hot encodings</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>one_hot_encodings <span class="op">=</span> F.one_hot(input_ids, num_classes<span class="op">=</span><span class="bu">len</span>(token2idx))</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>one_hot_encodings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    torch.Size([38, 20])</code></pre>
<p><strong>Note:</strong> Make sure to set <code>num_classes</code> to the vocabulary size.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token: </span><span class="sc">{</span>tokenized_text[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor index: </span><span class="sc">{</span>input_ids[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"One-hot: </span><span class="sc">{</span>one_hot_encodings[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Token: T
    Tensor index: 5
    One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</code></pre>
</section>
</section>
<section id="word-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="word-tokenization">Word Tokenization</h3>
<ul>
<li>Word-based tokenizers split the text into words and map each word to an integer.</li>
<li>Word tokenization creates less work for the model as it does not need to learn such linguistic structures from the data.</li>
<li>There is a loss of meaning across very similar words.</li>
<li>User-defined rules tell the tokenizer how to split the raw text into words.</li>
<li>A simple tokenization method is to split text using whitespace.</li>
<li>More sophisticated word tokenizers have additional rules to handle punctuation.</li>
<li>Other methods involve <a href="https://www.youtube.com/watch?v=tG3pUwmGjsc&amp;t=621s">stemming or lemmatization</a> to normalize words to their stem.
<ul>
<li>Example: “great”, “greater”, and “greatest” all become “great”</li>
<li>These methods come at the expense of losing some information in the text.</li>
</ul></li>
<li>Word tokenization results in a bigger vocabulary size, which requires more model parameters.
<ul>
<li>The first layer for a model that compressed the input vectors for a vocabulary with one million unique words to one thousand dimensional vectors would contain one billion weights.</li>
</ul></li>
<li>A popular method to limit the vocabulary size involves only adding the 100,000 most common words in the corpus.
<ul>
<li>Words that are not part of the vocabulary are classified as unknown and mapped to a shared token.</li>
<li>This approach risks losing potentially important information related to rare words.</li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>tokenized_text <span class="op">=</span> text.split()</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenized_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']</code></pre>
</section>
<section id="subword-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="subword-tokenization">Subword Tokenization</h3>
<ul>
<li>Subword tokenization algorithms decompose rare words into meaningful subwords while keeping the most frequently used words as unique entities.</li>
<li>Subword tokenization algorithms can identify start-of-word tokens.</li>
<li>Most state-of-the-art English models use subword-tokenization.</li>
<li>Subword tokenization helps keep vocabulary size and input length manageable by sharing information across different words.</li>
<li>There are several tokenization methods used in NLP.
<ul>
<li>The tokenizers for both the BERT and DistilBERT models use <a href="https://paperswithcode.com/method/wordpiece">WordPiece</a>.</li>
<li>GPT-2 uses <a href="https://paperswithcode.com/method/bpe">Byte Pair Encoding</a>.</li>
<li>Several multilingual models use <a href="https://paperswithcode.com/method/sentencepiece">SentencePiece</a> or <a href="https://paperswithcode.com/method/unigram-segmentation">Unigram Segmentation</a>.</li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="autotokenizer" class="level4">
<h4 class="anchored" data-anchor-id="autotokenizer"><code>AutoTokenizer</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">Documentation</a></li>
<li>Quickly load the tokenizer associated with a pretrained model.</li>
<li>AutoTokenizer belongs to a set of <a href="https://huggingface.co/docs/transformers/model_doc/auto">auto classes</a> that automatically retrieve the model’s configuration, pretrained weights, or vocabulary from the name of a checkpoint.</li>
</ul>
<hr>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    transformers.models.auto.tokenization_auto.AutoTokenizer</code></pre>
<hr>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Hugging Face automatically caches the parameters of the pretrained tokenizer after the first download.</p>
<hr>
<div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast</code></pre>
</section>
<section id="distilberttokenizerfast" class="level4">
<h4 class="anchored" data-anchor-id="distilberttokenizerfast"><code>DistilBertTokenizerFast</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizerFast">Documentation</a></li>
<li>Construct a “fast” DistilBERT tokenizer that runs end-to-end tokenization, including punctuation and WordPiece.</li>
</ul>
<hr>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>tokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})</code></pre>
<hr>
<div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>tokenizer.init_kwargs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'do_lower_case': True,
     'unk_token': '[UNK]',
     'sep_token': '[SEP]',
     'pad_token': '[PAD]',
     'cls_token': '[CLS]',
     'mask_token': '[MASK]',
     'tokenize_chinese_chars': True,
     'strip_accents': None,
     'model_max_length': 512,
     'special_tokens_map_file': None,
     'name_or_path': 'distilbert-base-uncased'}</code></pre>
<hr>
<div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Tokenizing text is a core task of NLP.</code></pre>
<hr>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map the raw text content to unique integers</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>encoded_text <span class="op">=</span> tokenizer(text)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(encoded_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</code></pre>
</section>
<section id="convert_ids_to_tokens" class="level4">
<h4 class="anchored" data-anchor-id="convert_ids_to_tokens"><code>convert_ids_to_tokens</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.convert_ids_to_tokens">Documentation</a></li>
<li>Convert a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.</li>
</ul>
<hr>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>print_source(tokenizer.convert_ids_to_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def convert_ids_to_tokens(self, ids: Union[int, List[int]],
        skip_special_tokens: bool=False) -&gt;Union[str, List[str]]:
        if isinstance(ids, int):
            return self._tokenizer.id_to_token(ids)
        tokens = []
        for index in ids:
            index = int(index)
            if skip_special_tokens and index in self.all_special_ids:
                continue
            tokens.append(self._tokenizer.id_to_token(index))
        return tokens</code></pre>
<hr>
<div class="sourceCode" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert integer ids to tokens</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(encoded_text.input_ids)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']</code></pre>
<p><strong>Note:</strong> 1. The <code>[CLS]</code> and <code>[SEP]</code> tokens indicate the start and end of a sequence respectively. 2. All the tokens are lower case. 3. The words “tokenizing” and “NLP” have been decomposed into subwords since they are rare. 4. The <code>##</code> prefix indicates the preceding string was not whitespace.</p>
<hr>
<div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert tokens to plain text</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.convert_tokens_to_string(tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [CLS] tokenizing text is a core task of nlp. [SEP]</code></pre>
<hr>
<div class="sourceCode" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>print_source(tokenizer.convert_tokens_to_string)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def convert_tokens_to_string(self, tokens: List[str]) -&gt;str:
        return self.backend_tokenizer.decoder.decode(tokens)</code></pre>
<hr>
<div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>tokenizer.vocab_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    30522</code></pre>
<hr>
<div class="sourceCode" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>tokenizer.model_max_length</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    512</code></pre>
<hr>
<div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>tokenizer.model_input_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['input_ids', 'attention_mask']</code></pre>
</section>
</section>
<section id="tokenizing-the-whole-dataset" class="level3">
<h3 class="anchored" data-anchor-id="tokenizing-the-whole-dataset">Tokenizing the Whole Dataset</h3>
<ul>
<li>We need to define a processing function to tokenize training examples.</li>
</ul>
<hr>
<div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(batch):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the tokenizer to a batch of examples</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(batch[<span class="st">"text"</span>],</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>                     <span class="co"># Pad examples with zeros to the longest one in the batch</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>                     padding<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>                     <span class="co"># Truncate the examples to the model's maximum context size</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>                     truncation<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenize(emotions[<span class="st">"train"</span>][:<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    {'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}</code></pre>
<p><strong>Note:</strong> 1. The zeros have a corresponding <code>[PAD]</code> token in the vocabulary. 2. The attention mask allows the model to ignore padded parts of the input.</p>
<hr>
<div class="sourceCode" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>tokens2ids <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(tokenizer.all_special_tokens, tokenizer.all_special_ids))</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="bu">sorted</span>(tokens2ids, key<span class="op">=</span><span class="kw">lambda</span> x : x[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data, columns<span class="op">=</span>[<span class="st">"Special Token"</span>, <span class="st">"Special Token ID"</span>])</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>df.T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Special Token
</th>
<td>
[PAD]
</td>
<td>
[UNK]
</td>
<td>
[CLS]
</td>
<td>
[SEP]
</td>
<td>
[MASK]
</td>
</tr>
<tr>
<th>
Special Token ID
</th>
<td>
0
</td>
<td>
100
</td>
<td>
101
</td>
<td>
102
</td>
<td>
103
</td>
</tr>
</tbody>
</table>
</div>
<section id="datasetdict.map" class="level4">
<h4 class="anchored" data-anchor-id="datasetdict.map"><code>DatasetDict.map</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.DatasetDict.map">Documentation</a></li>
<li>Apply a function to all the elements in the tables for all datasets in the dictionary.</li>
</ul>
<hr>
<div class="sourceCode" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>print_source(datasets.DatasetDict.<span class="bu">map</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def map(self, function, with_indices: bool=False, input_columns: Optional[
        Union[str, List[str]]]=None, batched: bool=False, batch_size: Optional[
        int]=1000, remove_columns: Optional[Union[str, List[str]]]=None,
        keep_in_memory: bool=False, load_from_cache_file: bool=True,
        cache_file_names: Optional[Dict[str, Optional[str]]]=None,
        writer_batch_size: Optional[int]=1000, features: Optional[Features]=
        None, disable_nullable: bool=False, fn_kwargs: Optional[dict]=None,
        num_proc: Optional[int]=None, desc: Optional[str]=None) -&gt;'DatasetDict':
        self._check_values_type()
        if cache_file_names is None:
            cache_file_names = {k: None for k in self}
        return DatasetDict({k: dataset.map(function=function, with_indices=
            with_indices, input_columns=input_columns, batched=batched,
            batch_size=batch_size, remove_columns=remove_columns,
            keep_in_memory=keep_in_memory, load_from_cache_file=
            load_from_cache_file, cache_file_name=cache_file_names[k],
            writer_batch_size=writer_batch_size, features=features,
            disable_nullable=disable_nullable, fn_kwargs=fn_kwargs, num_proc=
            num_proc, desc=desc) for k, dataset in self.items()})</code></pre>
</section>
<section id="dataset.map" class="level4">
<h4 class="anchored" data-anchor-id="dataset.map"><code>Dataset.map</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.map">Documentation</a></li>
<li>Apply a function to all the examples in the table and update the table.</li>
</ul>
<hr>
<div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the processing function across all splits in the corpus in batches</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>emotions_encoded <span class="op">=</span> emotions.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> * The map function applies the processing function to the entire dataset as a single batch when the batch size is None. * This approach ensures the input tensors and attention masks have the same shape globally.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(emotions_encoded[<span class="st">"train"</span>].column_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['attention_mask', 'input_ids', 'label', 'text']</code></pre>
</section>
</section>
</section>
<section id="training-a-text-classifier" class="level2">
<h2 class="anchored" data-anchor-id="training-a-text-classifier">Training a Text Classifier</h2>
<ul>
<li>Models like DistilBERT are pretrained to predict masked words in a sequence, and we need to modify them for text classification.</li>
<li>We can combine the body of the pretrained model with a custom classification head.</li>
</ul>
<section id="architecture-of-an-encoder-based-classifier" class="level3">
<h3 class="anchored" data-anchor-id="architecture-of-an-encoder-based-classifier">Architecture of an Encoder-Based Classifier</h3>
<ol type="1">
<li>Tokenize the text and represent it as one-hot vectors called token encodings.
<ul>
<li>The size of the tokenized vocabulary determines the dimensions of the token encodings and usually consists of 20 thousand to 200 thousand unique tokens.</li>
</ul></li>
<li>Convert the token encodings to token embeddings, which are vectors living in the lowe-dimensional space.</li>
<li>Pass the token embeddings through the encoder block layers, which yield a hidden state for each input token.</li>
<li>Replace the pretrained language modeling layer with a classification layer.</li>
</ol>
<p><strong>Note:</strong> PyTorch skips the step of creating one-hot vectors because multiplying a matrix with a one-hot vector is the same as selecting a column with the token ID from the matrix.</p>
</section>
<section id="methods-to-train-a-text-classifier" class="level3">
<h3 class="anchored" data-anchor-id="methods-to-train-a-text-classifier">Methods to Train a Text Classifier</h3>
<section id="feature-extraction" class="level4">
<h4 class="anchored" data-anchor-id="feature-extraction">Feature Extraction</h4>
<ul>
<li>Use the hidden states as features and train the classifier on them without modifying the pretrained model.</li>
</ul>
</section>
<section id="fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="fine-tuning">Fine-tuning</h4>
<ul>
<li>Train the whole model end-to-end, which also updates the parameters of the pretrained model.</li>
</ul>
</section>
</section>
<section id="transformers-as-feature-extractors" class="level3">
<h3 class="anchored" data-anchor-id="transformers-as-feature-extractors">Transformers as Feature Extractors</h3>
<ul>
<li>This method is well-suited for quickly training a small or shallow model.</li>
<li>The model could be a neural classification layer or a method that does not rely on gradients like random forests.</li>
<li>Using the transformer as a feature extractor is especially useful when GPUs are unavailable since the hidden states only need to be precomputed once.</li>
</ul>
<section id="using-pretrained-models" class="level4">
<h4 class="anchored" data-anchor-id="using-pretrained-models">Using pretrained models</h4>
<hr>
<div class="sourceCode" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="automodel.from_pretrained" class="level4">
<h4 class="anchored" data-anchor-id="automodel.from_pretrained"><code>AutoModel.from_pretrained</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained">Documentation</a></li>
<li>Instantiate one of the base model classes of the library from a pretrained model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>print_source(AutoModel.from_pretrained)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        trust_remote_code = kwargs.pop('trust_remote_code', False)
        kwargs['_from_auto'] = True
        if not isinstance(config, PretrainedConfig):
            config, kwargs = AutoConfig.from_pretrained(
                pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs)
        if hasattr(config, 'auto_map') and cls.__name__ in config.auto_map:
            if not trust_remote_code:
                raise ValueError(
                    f'Loading {pretrained_model_name_or_path} requires you to execute the modeling file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.'
                    )
            if kwargs.get('revision', None) is None:
                logger.warn(
                    'Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.'
                    )
            class_ref = config.auto_map[cls.__name__]
            module_file, class_name = class_ref.split('.')
            model_class = get_class_from_dynamic_module(
                pretrained_model_name_or_path, module_file + '.py', class_name,
                **kwargs)
            return model_class.from_pretrained(pretrained_model_name_or_path, *
                model_args, config=config, **kwargs)
        elif type(config) in cls._model_mapping.keys():
            model_class = _get_model_class(config, cls._model_mapping)
            return model_class.from_pretrained(pretrained_model_name_or_path, *
                model_args, config=config, **kwargs)
        raise ValueError(
            f"""Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.
    Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}."""
            )</code></pre>
<hr>
<div class="sourceCode" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a CUDA GPU if available</span></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate a pretrained DistilBertModel</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_ckpt).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (1): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (2): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (3): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (4): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (5): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )</code></pre>
</section>
<section id="extracting-the-last-hidden-states" class="level4">
<h4 class="anchored" data-anchor-id="extracting-the-last-hidden-states">Extracting the last hidden states</h4>
<hr>
<div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode some sample text</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"this is a test"</span></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input tensor shape: </span><span class="sc">{</span>inputs[<span class="st">'input_ids'</span>]<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Input tensor shape: torch.Size([1, 6])</code></pre>
<hr>
<div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Move the input tensors to the same device as the model</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {k:v.to(device) <span class="cf">for</span> k,v <span class="kw">in</span> inputs.items()}</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the last hidden states for the sample text</span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],
             [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],
             [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],
             [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],
             [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],
             [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],
           device='cuda:0'), hidden_states=None, attentions=None)</code></pre>
<p><strong>Note:</strong> The hidden state tensor has the shape <code>[batch_size, n_tokens, hidden_dim]</code>.</p>
<hr>
<div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>outputs.last_hidden_state.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    torch.Size([1, 6, 768])</code></pre>
<hr>
<div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>outputs.last_hidden_state[:,<span class="dv">0</span>].size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    torch.Size([1, 768])</code></pre>
<p><strong>Note:</strong> * For classification tasks, it is common to use the hidden state associated with the <code>[CLS]</code> token as the input feature. * Since the <code>[CLS]</code> token appears at the start of each sequence, we can extract it by accessing the associated index of the hidden state tensor.</p>
<hr>
<div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_hidden_states(batch):</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Place model inputs on the GPU</span></span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> {k:v.to(device) <span class="cf">for</span> k,v <span class="kw">in</span> batch.items() </span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> k <span class="kw">in</span> tokenizer.model_input_names}</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Disable automatic calculation of the gradient</span></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract last hidden states</span></span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a>        last_hidden_state <span class="op">=</span> model(<span class="op">**</span>inputs).last_hidden_state</span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return vector for [CLS] token</span></span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"hidden_state"</span>: last_hidden_state[:,<span class="dv">0</span>].cpu().numpy()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The <code>map()</code> method requires the processing function to return Python or NumPy objects.</p>
<hr>
<div class="sourceCode" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>emotions_encoded.set_format(<span class="st">"torch"</span>, columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"label"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the hidden states for every token in the dataset</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>emotions_hidden <span class="op">=</span> emotions_encoded.<span class="bu">map</span>(extract_hidden_states, batched<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>emotions_hidden[<span class="st">"train"</span>].column_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['attention_mask', 'hidden_state', 'input_ids', 'label', 'text']</code></pre>
</section>
<section id="creating-a-feature-matrix" class="level4">
<h4 class="anchored" data-anchor-id="creating-a-feature-matrix">Creating a feature matrix</h4>
<ul>
<li>We can use the hidden states as input features and the labels as targets.</li>
</ul>
<hr>
<div class="sourceCode" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the input and target data for the training and validation sets</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array(emotions_hidden[<span class="st">"train"</span>][<span class="st">"hidden_state"</span>])</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>X_valid <span class="op">=</span> np.array(emotions_hidden[<span class="st">"validation"</span>][<span class="st">"hidden_state"</span>])</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array(emotions_hidden[<span class="st">"train"</span>][<span class="st">"label"</span>])</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>y_valid <span class="op">=</span> np.array(emotions_hidden[<span class="st">"validation"</span>][<span class="st">"label"</span>])</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>X_train.shape, X_valid.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ((16000, 768), (2000, 768))</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>X_train[<span class="dv">0</span>].size, y_train[<span class="dv">0</span>].size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (768, 1)</code></pre>
</section>
<section id="visualizing-the-training-set" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-the-training-set">Visualizing the training set</h4>
<hr>
<div class="sourceCode" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> umap <span class="im">import</span> UMAP</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="umap-uniform-manifold-approximation-and-projection-for-dimension-reduction" class="level4">
<h4 class="anchored" data-anchor-id="umap-uniform-manifold-approximation-and-projection-for-dimension-reduction"><a href="https://arxiv.org/abs/1802.03426">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></h4>
<ul>
<li><a href="https://umap-learn.readthedocs.io/en/latest/">Documentation</a></li>
<li>UMAP is a dimension reduction technique that can be useful for visualization as a drop-in replacement for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>.</li>
<li>We can use the UMAP algorithm to scale the 768-dimensional vectors down to a 2-dimensional representation.</li>
<li>UMAP works best with feature values scaled to <code>[0,1]</code>.</li>
</ul>
</section>
<section id="scit-kit-learn-minmaxscaler" class="level4">
<h4 class="anchored" data-anchor-id="scit-kit-learn-minmaxscaler">Scit-Kit Learn MinMaxScaler</h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">Documentation</a></li>
<li>Transform features by scaling each to a given range.</li>
</ul>
<hr>
<div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features to [0,1] range</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> MinMaxScaler().fit_transform(X_train)</span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and fit UMAP</span></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>mapper <span class="op">=</span> UMAP(n_components<span class="op">=</span><span class="dv">2</span>, metric<span class="op">=</span><span class="st">"cosine"</span>).fit(X_scaled)</span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame of 2D embeddings</span></span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>df_emb <span class="op">=</span> pd.DataFrame(mapper.embedding_, columns<span class="op">=</span>[<span class="st">"X"</span>, <span class="st">"Y"</span>])</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>df_emb[<span class="st">"label"</span>] <span class="op">=</span> y_train</span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a>df_emb.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
X
</th>
<th>
Y
</th>
<th>
label
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
4.345317
</td>
<td>
6.545871
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-2.770711
</td>
<td>
5.816418
</td>
<td>
0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
5.433491
</td>
<td>
3.048345
</td>
<td>
3
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-2.210309
</td>
<td>
3.550199
</td>
<td>
2
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-3.055895
</td>
<td>
3.723285
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The UMAP algorithm has compressed the hidden state vectors from 768 dimensions to 2 dimensions.</p>
</section>
<section id="matplotlib.pyplot.hexbin" class="level4">
<h4 class="anchored" data-anchor-id="matplotlib.pyplot.hexbin"><code>matplotlib.pyplot.hexbin</code></h4>
<ul>
<li><a href="https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.hexbin.html">Documentation</a></li>
<li>Make a 2D hexagonal binning plot of points x, y.</li>
</ul>
<hr>
<div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">10</span>))</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Collapse the array into one dimension</span></span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>cmaps <span class="op">=</span> [<span class="st">"Greys"</span>, <span class="st">"Blues"</span>, <span class="st">"Oranges"</span>, <span class="st">"Reds"</span>, <span class="st">"Purples"</span>, <span class="st">"Greens"</span>]</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> emotions[<span class="st">"train"</span>].features[<span class="st">"label"</span>].names</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (label, cmap) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(labels, cmaps)):</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a>    df_emb_sub <span class="op">=</span> df_emb.query(<span class="ss">f"label == </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a>    axes[i].hexbin(df_emb_sub[<span class="st">"X"</span>], df_emb_sub[<span class="st">"Y"</span>], cmap<span class="op">=</span>cmap, gridsize<span class="op">=</span><span class="dv">20</span>, linewidths<span class="op">=</span>(<span class="dv">0</span>,))</span>
<span id="cb150-10"><a href="#cb150-10" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(label)</span>
<span id="cb150-11"><a href="#cb150-11" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xticks([]), axes[i].set_yticks([])</span>
<span id="cb150-12"><a href="#cb150-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-13"><a href="#cb150-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the padding between and around subplots.</span></span>
<span id="cb150-14"><a href="#cb150-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb150-15"><a href="#cb150-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_151_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The negative feelings such as sadness, anger, and fear occupy similar regions in the hidden state with slightly varying distributions. * The positive emotions, joy, and love, are well separated from the negative emotions and share a similar space. * Surprise is scattered all over the place. * The model did not train to know the difference between these emotions. It learned them implicitly by guessing masked words in the training corpus. * Just because some categories overlap when projected onto a lower-dimensional space does not mean they are not separable in the original space.</p>
</section>
<section id="training-a-simple-classifier" class="level4">
<h4 class="anchored" data-anchor-id="training-a-simple-classifier">Training a simple classifier</h4>
<ul>
<li>We can use the hidden states to train a simple logistic regression model.</li>
<li>This type of model is trains quickly and does not require a GPU.</li>
</ul>
<hr>
<div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sklearn.linear_model.logisticregression" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.linear_model.logisticregression"><code>sklearn.linear_model.LogisticRegression</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Documentation</a></li>
<li>Logistic Regression classifier</li>
</ul>
<hr>
<div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase `max_iter` to guarantee convergence </span></span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>lr_clf <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">3000</span>)</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>lr_clf.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    LogisticRegression(max_iter=3000)</code></pre>
<hr>
<div class="sourceCode" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>lr_clf.score(X_valid, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    0.6335</code></pre>
<p><strong>Note:</strong> The model performs well, considering the training data is unbalanced.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.dummy <span class="im">import</span> DummyClassifier</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sklearn.dummy.dummyclassifier" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.dummy.dummyclassifier"><code>sklearn.dummy.DummyClassifier</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html">Documentation</a></li>
<li>A DummyClassifier makes predictions using a predefined strategy and ignores the input features.</li>
<li>The classifier serves as a simple baseline to compare against other more complex classifiers.</li>
</ul>
<hr>
<div class="sourceCode" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the DummyClassifier to always select the most frequent class</span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>dummy_clf <span class="op">=</span> DummyClassifier(strategy<span class="op">=</span><span class="st">"most_frequent"</span>)</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>dummy_clf.fit(X_train, y_train)</span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>dummy_clf.score(X_valid, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    0.352</code></pre>
<p><strong>Note:</strong> The simple logistic regression classifier performs significantly better than a model that always selects the most frequent class.</p>
<hr>
<div class="sourceCode" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay, confusion_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sklearn.metrics.confusionmatrixdisplay" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.metrics.confusionmatrixdisplay"><code>sklearn.metrics.ConfusionMatrixDisplay</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html">Documentation</a></li>
<li>Create a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> visualization</li>
</ul>
</section>
<section id="sklearn.metrics.confusion_matrix" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.metrics.confusion_matrix"><code>sklearn.metrics.confusion_matrix</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">Documentation</a></li>
<li>Compute confusion matrix to evaluate the accuracy of a classification.</li>
</ul>
<hr>
<div class="sourceCode" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_confusion_matrix(y_preds, y_true, labels):</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_true, y_preds, normalize<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>labels)</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>    disp.plot(cmap<span class="op">=</span><span class="st">"Blues"</span>, values_format<span class="op">=</span><span class="st">".2f"</span>, ax<span class="op">=</span>ax, colorbar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Normalized confusion matrix"</span>)</span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> lr_clf.predict(X_valid)</span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(y_preds, y_valid, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_165_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * Anger and fear are most often confused with sadness. * Love and surprise are frequently mistaken for joy.</p>
</section>
</section>
<section id="fine-tuning-transformers" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-transformers">Fine-Tuning Transformers</h3>
<ul>
<li>Fine-tuning results in superior performance than feature extraction but requires more computational resources such as GPUs.</li>
<li>Fine-tuning involves training the hidden states, so the classification head needs to be differentiable.</li>
<li>Training the hidden states that serve as input to the classifier helps avoid the problem of working with data that may not be well suited for the classification task.</li>
</ul>
<section id="loading-a-pretrained-model" class="level4">
<h4 class="anchored" data-anchor-id="loading-a-pretrained-model">Loading a pretrained model</h4>
<hr>
<div class="sourceCode" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="automodelforsequenceclassification.from_pretrained" class="level4">
<h4 class="anchored" data-anchor-id="automodelforsequenceclassification.from_pretrained"><code>AutoModelForSequenceClassification.from_pretrained</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification.from_pretrained">Documentation</a></li>
<li>Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the number of labels (i.e. the number of emotions)</span></span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> (AutoModelForSequenceClassification</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>         .from_pretrained(model_ckpt, num_labels<span class="op">=</span>num_labels)</span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>         .to(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The classifier head is randomly initialized.</p>
<hr>
<div class="sourceCode" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification</code></pre>
</section>
<section id="distilbertforsequenceclassification" class="level4">
<h4 class="anchored" data-anchor-id="distilbertforsequenceclassification"><code>DistilBertForSequenceClassification</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">Documentation</a></li>
<li>DistilBert Model transformer with a sequence classification head on top</li>
</ul>
<hr>
<div class="sourceCode" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> child <span class="kw">in</span> model.named_children(): <span class="bu">print</span>(child[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    distilbert
    pre_classifier
    classifier
    dropout</code></pre>
<hr>
<div class="sourceCode" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.named_children())[<span class="op">-</span><span class="dv">3</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [('pre_classifier', Linear(in_features=768, out_features=768, bias=True)),
     ('classifier', Linear(in_features=768, out_features=6, bias=True)),
     ('dropout', Dropout(p=0.2, inplace=False))]</code></pre>
</section>
<section id="defining-the-performance-metrics" class="level4">
<h4 class="anchored" data-anchor-id="defining-the-performance-metrics">Defining the performance metrics</h4>
<ul>
<li>We need to define a function to compute metrics for the trainer so we can monitor performance during training.</li>
<li>The function receives an EvalPrediction object containing predictions and label_ids attributes and returns a dictionary that maps each metric’s name to its value.</li>
</ul>
<hr>
<div class="sourceCode" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sklearn.metrics.f1_score" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.metrics.f1_score"><code>sklearn.metrics.f1_score</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">Documentation</a></li>
<li>Compute the <a href="https://www.educative.io/edpresso/what-is-the-f1-score"><span class="math inline">\(F_{1}\)</span>-score</a></li>
</ul>
</section>
<section id="sklearn.metrics.accuracy_score" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.metrics.accuracy_score"><code>sklearn.metrics.accuracy_score</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">Documentation</a></li>
<li>Compute the classification accuracy score.</li>
</ul>
<hr>
<div class="sourceCode" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(pred):</span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> pred.label_ids</span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> pred.predictions.argmax(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(labels, preds, average<span class="op">=</span><span class="st">"weighted"</span>)</span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_score(labels, preds)</span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"accuracy"</span>: acc, <span class="st">"f1"</span>: f1}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-the-model" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model">Training the model</h4>
<ul>
<li>We can use the <a href="https://huggingface.co/docs/huggingface_hub/index">Hugging Face Hub API</a> to push our fine-tuned model to our account on the Hub and share it with the community.</li>
</ul>
<hr>
<div class="sourceCode" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>inspect.getdoc(notebook_login)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'Displays a widget to login to the HF website and store the token.'</code></pre>
<hr>
<div class="sourceCode" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>print_source(notebook_login)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def notebook_login():
        try:
            import ipywidgets.widgets as widgets
            from IPython.display import clear_output, display
        except ImportError:
            raise ImportError(
                'The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywdidgets` module: `pip install ipywidgets`.'
                )
        box_layout = widgets.Layout(display='flex', flex_flow='column',
            align_items='center', width='50%')
        token_widget = widgets.Password(description='Token:')
        token_finish_button = widgets.Button(description='Login')
        switch_button = widgets.Button(description='Use password')
        login_token_widget = widgets.VBox([widgets.HTML(
            NOTEBOOK_LOGIN_TOKEN_HTML_START), token_widget, token_finish_button,
            widgets.HTML(NOTEBOOK_LOGIN_TOKEN_HTML_END), switch_button], layout
            =box_layout)
        display(login_token_widget)
        input_widget = widgets.Text(description='Username:')
        password_widget = widgets.Password(description='Password:')
        password_finish_button = widgets.Button(description='Login')
        login_password_widget = widgets.VBox([widgets.HTML(value=
            NOTEBOOK_LOGIN_PASSWORD_HTML), widgets.HBox([input_widget,
            password_widget]), password_finish_button], layout=box_layout)
    
        def login_token_event(t):
            token_widget.value = ''
            clear_output()
            _login(HfApi(), token=token)
        token_finish_button.on_click(login_token_event)
    
        def login_password_event(t):
            password = password_widget.value
            password_widget.value = ''
            clear_output()
            _login(HfApi(), username=username, password=password)
        password_finish_button.on_click(login_password_event)
    
        def switch_event(t):
            display(login_password_widget)
        switch_button.on_click(switch_event)</code></pre>
<hr>
<div class="sourceCode" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>notebook_login()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb177"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>git config <span class="op">--</span><span class="kw">global</span> credential.helper store</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The equivalent terminal command is <code>huggingface-cli login</code>.</p>
<hr>
<div class="sourceCode" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="trainingarguments" class="level4">
<h4 class="anchored" data-anchor-id="trainingarguments"><code>TrainingArguments</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">Documentation</a></li>
<li>The TrainingArguments class provides fine-grained control over the arguments related to the training loop.</li>
</ul>
<hr>
<div class="sourceCode" id="cb179"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(inspect.signature(TrainingArguments).parameters).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
<th>
25
</th>
<th>
26
</th>
<th>
27
</th>
<th>
28
</th>
<th>
29
</th>
<th>
30
</th>
<th>
31
</th>
<th>
32
</th>
<th>
33
</th>
<th>
34
</th>
<th>
35
</th>
<th>
36
</th>
<th>
37
</th>
<th>
38
</th>
<th>
39
</th>
<th>
40
</th>
<th>
41
</th>
<th>
42
</th>
<th>
43
</th>
<th>
44
</th>
<th>
45
</th>
<th>
46
</th>
<th>
47
</th>
<th>
48
</th>
<th>
49
</th>
<th>
50
</th>
<th>
51
</th>
<th>
52
</th>
<th>
53
</th>
<th>
54
</th>
<th>
55
</th>
<th>
56
</th>
<th>
57
</th>
<th>
58
</th>
<th>
59
</th>
<th>
60
</th>
<th>
61
</th>
<th>
62
</th>
<th>
63
</th>
<th>
64
</th>
<th>
65
</th>
<th>
66
</th>
<th>
67
</th>
<th>
68
</th>
<th>
69
</th>
<th>
70
</th>
<th>
71
</th>
<th>
72
</th>
<th>
73
</th>
<th>
74
</th>
<th>
75
</th>
<th>
76
</th>
<th>
77
</th>
<th>
78
</th>
<th>
79
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
output_dir
</td>
<td>
overwrite_output_dir
</td>
<td>
do_train
</td>
<td>
do_eval
</td>
<td>
do_predict
</td>
<td>
evaluation_strategy
</td>
<td>
prediction_loss_only
</td>
<td>
per_device_train_batch_size
</td>
<td>
per_device_eval_batch_size
</td>
<td>
per_gpu_train_batch_size
</td>
<td>
per_gpu_eval_batch_size
</td>
<td>
gradient_accumulation_steps
</td>
<td>
eval_accumulation_steps
</td>
<td>
learning_rate
</td>
<td>
weight_decay
</td>
<td>
adam_beta1
</td>
<td>
adam_beta2
</td>
<td>
adam_epsilon
</td>
<td>
max_grad_norm
</td>
<td>
num_train_epochs
</td>
<td>
max_steps
</td>
<td>
lr_scheduler_type
</td>
<td>
warmup_ratio
</td>
<td>
warmup_steps
</td>
<td>
log_level
</td>
<td>
log_level_replica
</td>
<td>
log_on_each_node
</td>
<td>
logging_dir
</td>
<td>
logging_strategy
</td>
<td>
logging_first_step
</td>
<td>
logging_steps
</td>
<td>
logging_nan_inf_filter
</td>
<td>
save_strategy
</td>
<td>
save_steps
</td>
<td>
save_total_limit
</td>
<td>
save_on_each_node
</td>
<td>
no_cuda
</td>
<td>
seed
</td>
<td>
fp16
</td>
<td>
fp16_opt_level
</td>
<td>
fp16_backend
</td>
<td>
fp16_full_eval
</td>
<td>
local_rank
</td>
<td>
xpu_backend
</td>
<td>
tpu_num_cores
</td>
<td>
tpu_metrics_debug
</td>
<td>
debug
</td>
<td>
dataloader_drop_last
</td>
<td>
eval_steps
</td>
<td>
dataloader_num_workers
</td>
<td>
past_index
</td>
<td>
run_name
</td>
<td>
disable_tqdm
</td>
<td>
remove_unused_columns
</td>
<td>
label_names
</td>
<td>
load_best_model_at_end
</td>
<td>
metric_for_best_model
</td>
<td>
greater_is_better
</td>
<td>
ignore_data_skip
</td>
<td>
sharded_ddp
</td>
<td>
deepspeed
</td>
<td>
label_smoothing_factor
</td>
<td>
adafactor
</td>
<td>
group_by_length
</td>
<td>
length_column_name
</td>
<td>
report_to
</td>
<td>
ddp_find_unused_parameters
</td>
<td>
dataloader_pin_memory
</td>
<td>
skip_memory_metrics
</td>
<td>
use_legacy_prediction_loop
</td>
<td>
push_to_hub
</td>
<td>
resume_from_checkpoint
</td>
<td>
hub_model_id
</td>
<td>
hub_strategy
</td>
<td>
hub_token
</td>
<td>
gradient_checkpointing
</td>
<td>
push_to_hub_model_id
</td>
<td>
push_to_hub_organization
</td>
<td>
push_to_hub_token
</td>
<td>
mp_parameters
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>logging_steps <span class="op">=</span> <span class="bu">len</span>(emotions_encoded[<span class="st">"train"</span>]) <span class="op">//</span> batch_size</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_ckpt<span class="sc">}</span><span class="ss">-finetuned-emotion"</span></span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(output_dir<span class="op">=</span>model_name,</span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>                                  num_train_epochs<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a>                                  learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a>                                  per_device_train_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a>                                  per_device_eval_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a>                                  weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>                                  evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb180-11"><a href="#cb180-11" aria-hidden="true" tabindex="-1"></a>                                  disable_tqdm<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb180-12"><a href="#cb180-12" aria-hidden="true" tabindex="-1"></a>                                  logging_steps<span class="op">=</span>logging_steps,</span>
<span id="cb180-13"><a href="#cb180-13" aria-hidden="true" tabindex="-1"></a>                                  push_to_hub<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb180-14"><a href="#cb180-14" aria-hidden="true" tabindex="-1"></a>                                  log_level<span class="op">=</span><span class="st">"error"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>training_args.output_dir</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'distilbert-base-uncased-finetuned-emotion'</code></pre>
<hr>
<div class="sourceCode" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="trainer" class="level4">
<h4 class="anchored" data-anchor-id="trainer"><code>Trainer</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Documentation</a></li>
<li>The Trainer class provides a simple, feature-complete training and eval loop for PyTorch, optimized for Hugging Face Transformers.</li>
</ul>
<p><strong>Note:</strong> Install <a href="https://git-lfs.github.com/">Git-LFS</a> before running the following code cell.</p>
<hr>
<div class="sourceCode" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model<span class="op">=</span>model, args<span class="op">=</span>training_args, </span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a>                  compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a>                  train_dataset<span class="op">=</span>emotions_encoded[<span class="st">"train"</span>],</span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a>                  eval_dataset<span class="op">=</span>emotions_encoded[<span class="st">"validation"</span>],</span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a>                  tokenizer<span class="op">=</span>tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Had to add the following <a href="https://github.com/nlp-with-transformers/notebooks/issues/31#issuecomment-1073017664">workaround</a></p>
<hr>
<div class="sourceCode" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>old_collator <span class="op">=</span> trainer.data_collator</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>trainer.data_collator <span class="op">=</span> <span class="kw">lambda</span> data: <span class="bu">dict</span>(old_collator(data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>trainer.train()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;
    &lt;thead&gt;
        &lt;tr style="text-align: left;"&gt;
            &lt;th&gt;Epoch&lt;/th&gt;
            &lt;th&gt;Training Loss&lt;/th&gt;
            &lt;th&gt;Validation Loss&lt;/th&gt;
            &lt;th&gt;Accuracy&lt;/th&gt;
            &lt;th&gt;F1&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;1&lt;/td&gt;
            &lt;td&gt;0.044200&lt;/td&gt;
            &lt;td&gt;0.239172&lt;/td&gt;
            &lt;td&gt;0.926000&lt;/td&gt;
            &lt;td&gt;0.926452&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;2&lt;/td&gt;
            &lt;td&gt;0.046300&lt;/td&gt;
            &lt;td&gt;0.220463&lt;/td&gt;
            &lt;td&gt;0.936000&lt;/td&gt;
            &lt;td&gt;0.936133&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;</code></pre>
</div>
<hr>
<div class="sourceCode" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>preds_output <span class="op">=</span> trainer.predict(emotions_encoded[<span class="st">"validation"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> The predict method returns a PredictionOutput object, which contains arrays of predictions and label_ids, along with the user-defined metrics.</p>
<hr>
<div class="sourceCode" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(preds_output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>transformers.trainer_utils.PredictionOutput</code></pre>
<hr>
<div class="sourceCode" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>preds_output.metrics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'test_loss': 0.22046349942684174,
 'test_accuracy': 0.936,
 'test_f1': 0.9361334972007946,
 'test_runtime': 1.522,
 'test_samples_per_second': 1314.038,
 'test_steps_per_second': 21.025}</code></pre>
<p><strong>Note:</strong> The fine-tuned model performs significantly better than the feature-based logistic regression classifier.</p>
<hr>
<div class="sourceCode" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted labels</span></span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> np.argmax(preds_output.predictions, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb194"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(y_preds, y_valid, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_207_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>This one is much closer to the ideal diagonal confusion matrix than the confusion matrix for the logistic regression model.</li>
<li>The love category is still often confused with joy.</li>
<li>Surprise is frequently confused for joy or fear.</li>
</ul>
</section>
<section id="error-analysis" class="level4">
<h4 class="anchored" data-anchor-id="error-analysis">Error analysis</h4>
<ul>
<li>A simple error-analysis technique involves sorting the validation samples by the model loss.
<ul>
<li>This approach allows us to find and correct mislabeled data.</li>
</ul></li>
<li>Inspecting the model’s weakest predictions can help identify quirks of the dataset.
<ul>
<li>Cleaning the data or injecting similar examples can make the model more robust.</li>
</ul></li>
<li>We can significantly improve model performance by refining the dataset without obtaining more data or using a larger model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> cross_entropy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb196"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass_with_label(batch):</span>
<span id="cb196-2"><a href="#cb196-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Place all input tensors on the same device as the model</span></span>
<span id="cb196-3"><a href="#cb196-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> {k:v.to(device) <span class="cf">for</span> k,v <span class="kw">in</span> batch.items() </span>
<span id="cb196-4"><a href="#cb196-4" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> k <span class="kw">in</span> tokenizer.model_input_names}</span>
<span id="cb196-5"><a href="#cb196-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-6"><a href="#cb196-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb196-7"><a href="#cb196-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb196-8"><a href="#cb196-8" aria-hidden="true" tabindex="-1"></a>        pred_label <span class="op">=</span> torch.argmax(output.logits, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb196-9"><a href="#cb196-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the cross entropy loss between the prediction and the target</span></span>
<span id="cb196-10"><a href="#cb196-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> cross_entropy(output.logits, batch[<span class="st">"label"</span>].to(device), </span>
<span id="cb196-11"><a href="#cb196-11" aria-hidden="true" tabindex="-1"></a>                             reduction<span class="op">=</span><span class="st">"none"</span>)</span>
<span id="cb196-12"><a href="#cb196-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-13"><a href="#cb196-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Place outputs on CPU for compatibility with other dataset columns   </span></span>
<span id="cb196-14"><a href="#cb196-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"loss"</span>: loss.cpu().numpy(), </span>
<span id="cb196-15"><a href="#cb196-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"predicted_label"</span>: pred_label.cpu().numpy()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert our dataset back to PyTorch tensors</span></span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a>emotions_encoded.set_format(<span class="st">"torch"</span>, columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>, <span class="st">"label"</span>])</span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute loss values</span></span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a>emotions_encoded[<span class="st">"validation"</span>] <span class="op">=</span> emotions_encoded[<span class="st">"validation"</span>].<span class="bu">map</span>(</span>
<span id="cb197-5"><a href="#cb197-5" aria-hidden="true" tabindex="-1"></a>    forward_pass_with_label, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb198"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with the texts, losses, and predicted/true labels</span></span>
<span id="cb198-2"><a href="#cb198-2" aria-hidden="true" tabindex="-1"></a>emotions_encoded.set_format(<span class="st">"pandas"</span>)</span>
<span id="cb198-3"><a href="#cb198-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [<span class="st">"text"</span>, <span class="st">"label"</span>, <span class="st">"predicted_label"</span>, <span class="st">"loss"</span>]</span>
<span id="cb198-4"><a href="#cb198-4" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> emotions_encoded[<span class="st">"validation"</span>][:][cols]</span>
<span id="cb198-5"><a href="#cb198-5" aria-hidden="true" tabindex="-1"></a>df_test[<span class="st">"label"</span>] <span class="op">=</span> df_test[<span class="st">"label"</span>].<span class="bu">apply</span>(label_int2str)</span>
<span id="cb198-6"><a href="#cb198-6" aria-hidden="true" tabindex="-1"></a>df_test[<span class="st">"predicted_label"</span>] <span class="op">=</span> (df_test[<span class="st">"predicted_label"</span>]</span>
<span id="cb198-7"><a href="#cb198-7" aria-hidden="true" tabindex="-1"></a>                              .<span class="bu">apply</span>(label_int2str))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the validation samples by the model loss</span></span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>df_test.sort_values(<span class="st">"loss"</span>, ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
text
</th>
<th>
label
</th>
<th>
predicted_label
</th>
<th>
loss
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
318
</th>
<td>
i felt ashamed of these feelings and was scared because i knew that something wrong with me and thought i might be gay
</td>
<td>
fear
</td>
<td>
sadness
</td>
<td>
8.869118
</td>
</tr>
<tr>
<th>
1509
</th>
<td>
i guess this is a memoir so it feels like that should be fine too except i dont know something about such a deep amount of self absorption made me feel uncomfortable
</td>
<td>
joy
</td>
<td>
fear
</td>
<td>
8.770837
</td>
</tr>
<tr>
<th>
1950
</th>
<td>
i as representative of everything thats wrong with corporate america and feel that sending him to washington is a ludicrous idea
</td>
<td>
surprise
</td>
<td>
sadness
</td>
<td>
8.217673
</td>
</tr>
<tr>
<th>
882
</th>
<td>
i feel badly about reneging on my commitment to bring donuts to the faithful at holy family catholic church in columbus ohio
</td>
<td>
love
</td>
<td>
sadness
</td>
<td>
8.134083
</td>
</tr>
<tr>
<th>
1757
</th>
<td>
i feel like there s a reason to buy another tom petty record
</td>
<td>
anger
</td>
<td>
joy
</td>
<td>
7.790391
</td>
</tr>
<tr>
<th>
1111
</th>
<td>
im lazy my characters fall into categories of smug and or blas people and their foils people who feel inconvenienced by smug and or blas people
</td>
<td>
joy
</td>
<td>
fear
</td>
<td>
7.778357
</td>
</tr>
<tr>
<th>
1500
</th>
<td>
i guess we would naturally feel a sense of loneliness even the people who said unkind things to you might be missed
</td>
<td>
anger
</td>
<td>
sadness
</td>
<td>
7.741042
</td>
</tr>
<tr>
<th>
1919
</th>
<td>
i should admit when consuming alcohol myself in small amounts i feel much less inhibited ideas come to me more easily and i can write with greater ease
</td>
<td>
fear
</td>
<td>
sadness
</td>
<td>
7.342785
</td>
</tr>
<tr>
<th>
415
</th>
<td>
im kind of embarrassed about feeling that way though because my moms training was such a wonderfully defining part of my own life and i loved and still love
</td>
<td>
love
</td>
<td>
sadness
</td>
<td>
7.320217
</td>
</tr>
<tr>
<th>
1801
</th>
<td>
i feel that he was being overshadowed by the supporting characters
</td>
<td>
love
</td>
<td>
sadness
</td>
<td>
6.833299
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The model made some incorrect predictions.</li>
<li>Some examples seem mislabeled or do not fit into one of the six emotion classes.</li>
<li>Joy, in particular, seems to be mislabeled several times.</li>
</ul>
<hr>
<div class="sourceCode" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a>df_test.sort_values(<span class="st">"loss"</span>, ascending<span class="op">=</span><span class="va">True</span>).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
text
</th>
<th>
label
</th>
<th>
predicted_label
</th>
<th>
loss
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
702
</th>
<td>
i only find out that they are looking and feeling complacent just before a match started and i have no other way to find out except through the assistant manager
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000212
</td>
</tr>
<tr>
<th>
1205
</th>
<td>
i log on feeling vaguely sociable and after a short amount of time im all socialised out
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000214
</td>
</tr>
<tr>
<th>
1607
</th>
<td>
i feel incredibly mellow and spacey
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000214
</td>
</tr>
<tr>
<th>
452
</th>
<td>
i manage to complete the lap not too far behind the front runners and am feeling pretty jubilant until i realise that this is just the warm up
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000215
</td>
</tr>
<tr>
<th>
400
</th>
<td>
i are just relaxing together and i feel ecstatic and blissfully happy because i know he loves me and i love him
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000217
</td>
</tr>
<tr>
<th>
911
</th>
<td>
i feel in love with a cute little maltese
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000218
</td>
</tr>
<tr>
<th>
1567
</th>
<td>
i feel wonderful shayla admitted
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000220
</td>
</tr>
<tr>
<th>
1198
</th>
<td>
i feel like i should also mention that there was some content that i wasnt thrilled with either
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000220
</td>
</tr>
<tr>
<th>
1951
</th>
<td>
i can finish even if i have to eat and feel satisfied bellmont cabinets before it leaves bellmont cabinets a wipe out on the spot it is not necessary to wipe out for when you o
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000221
</td>
</tr>
<tr>
<th>
293
</th>
<td>
i am sure she makes all waiting couples feel this way but we left feeling like she is pulling for us and she will be so thrilled when it all works out
</td>
<td>
joy
</td>
<td>
joy
</td>
<td>
0.000222
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="saving-and-sharing-the-model" class="level4">
<h4 class="anchored" data-anchor-id="saving-and-sharing-the-model">Saving and sharing the model</h4>
<ul>
<li>Everyone can share and download pretrained and fine-tuned models via the Hugging Face Hub.</li>
</ul>
</section>
<section id="trainer.push_to_hub" class="level4">
<h4 class="anchored" data-anchor-id="trainer.push_to_hub"><code>Trainer.push_to_hub</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub">Documentation</a></li>
<li>Upload the trainer model and tokenizer to the Hugging Face Model Hub.</li>
</ul>
<hr>
<div class="sourceCode" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a>trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"Training completed!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/distilbert-base-uncased-finetuned-emotion/commit/5ca5827ba0121e07c8056a8592398e73beca3f17'</code></pre>
</section>
<section id="inference" class="level4">
<h4 class="anchored" data-anchor-id="inference">Inference</h4>
<ul>
<li>We can now perform inference using the fine-tuned model from our Hub repository.</li>
</ul>
<hr>
<div class="sourceCode" id="cb203"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb204"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Change `transformersbook` to your Hub username</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"cj-mills/distilbert-base-uncased-finetuned-emotion"</span></span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>model_id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>custom_tweet <span class="op">=</span> <span class="st">"I saw a movie today and it was really good."</span></span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> classifier(custom_tweet, return_all_scores<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb206"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a>preds_df <span class="op">=</span> pd.DataFrame(preds[<span class="dv">0</span>])</span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a>plt.bar(labels, <span class="dv">100</span> <span class="op">*</span> preds_df[<span class="st">"score"</span>], color<span class="op">=</span><span class="st">'C0'</span>)</span>
<span id="cb206-3"><a href="#cb206-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'"</span><span class="sc">{</span>custom_tweet<span class="sc">}</span><span class="ss">"'</span>)</span>
<span id="cb206-4"><a href="#cb206-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Class probability (%)"</span>)</span>
<span id="cb206-5"><a href="#cb206-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_224_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb207"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a>custom_tweet <span class="op">=</span> <span class="st">"I saw a movie today and it was garbage!."</span></span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> classifier(custom_tweet, return_all_scores<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>preds_df <span class="op">=</span> pd.DataFrame(preds[<span class="dv">0</span>])</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>plt.bar(labels, <span class="dv">100</span> <span class="op">*</span> preds_df[<span class="st">"score"</span>], color<span class="op">=</span><span class="st">'C0'</span>)</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'"</span><span class="sc">{</span>custom_tweet<span class="sc">}</span><span class="ss">"'</span>)</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Class probability (%)"</span>)</span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_226_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb209"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a>custom_tweet <span class="op">=</span> <span class="st">"I saw a movie today and it was really weird."</span></span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> classifier(custom_tweet, return_all_scores<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>preds_df <span class="op">=</span> pd.DataFrame(preds[<span class="dv">0</span>])</span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>plt.bar(labels, <span class="dv">100</span> <span class="op">*</span> preds_df[<span class="st">"score"</span>], color<span class="op">=</span><span class="st">'C0'</span>)</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'"</span><span class="sc">{</span>custom_tweet<span class="sc">}</span><span class="ss">"'</span>)</span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Class probability (%)"</span>)</span>
<span id="cb210-5"><a href="#cb210-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_228_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<section id="nlp-challenges" class="level3">
<h3 class="anchored" data-anchor-id="nlp-challenges">NLP Challenges</h3>
<section id="moving-a-model-to-production" class="level4">
<h4 class="anchored" data-anchor-id="moving-a-model-to-production">Moving a model to production</h4>
<ul>
<li>Hugging Face creates an inference endpoint automatically when you push a model to the Hub.</li>
<li><a href="https://api-inference.huggingface.co/docs/python/html/index.html">Hugging Face Accelerated Inference API</a></li>
</ul>
</section>
<section id="increasing-inference-speed" class="level4">
<h4 class="anchored" data-anchor-id="increasing-inference-speed">Increasing Inference Speed</h4>
<ul>
<li>The process used to create the more efficient DistilBERT model is called knowledge distillation.</li>
</ul>
</section>
<section id="applying-a-model-to-other-tasks" class="level4">
<h4 class="anchored" data-anchor-id="applying-a-model-to-other-tasks">Applying a Model to other tasks</h4>
<ul>
<li>Transformers are exceedingly versatile.</li>
</ul>
</section>
<section id="using-non-english-text" class="level4">
<h4 class="anchored" data-anchor-id="using-non-english-text">Using Non-English Text</h4>
<ul>
<li>Multilingual transformers are available.</li>
</ul>
</section>
<section id="working-with-little-labeled-data" class="level4">
<h4 class="anchored" data-anchor-id="working-with-little-labeled-data">Working with little labeled data</h4>
<ul>
<li>Fine-tuning might not be an option when little labeled training data is available.</li>
</ul>
</section>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-1/">Notes on Transformers Book Ch. 1</a></p>
<p><strong>Next:</strong> <a href="../chapter-3/">Notes on Transformers Book Ch. 3</a></p>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>I’m Christian Mills, a deep learning consultant specializing in computer vision and practical AI implementations.</li>
<li>I help clients leverage cutting-edge AI technologies to solve real-world problems.</li>
<li>Learn more <a href="../../../about.html">about me</a> or reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a> to discuss your project.</li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>