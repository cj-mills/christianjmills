<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-07">
<meta name="description" content="Chapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.">

<title>Notes on Transformers Book Ch. 4 – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Notes on Transformers Book Ch. 4 – Christian Mills">
<meta property="og:description" content="Chapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Notes on Transformers Book Ch. 4 – Christian Mills">
<meta name="twitter:description" content="Chapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#project-multilingual-named-entity-recognition" id="toc-project-multilingual-named-entity-recognition" class="nav-link" data-scroll-target="#project-multilingual-named-entity-recognition">Project: Multilingual Named Entity Recognition</a></li>
  <li><a href="#the-dataset" id="toc-the-dataset" class="nav-link" data-scroll-target="#the-dataset">The Dataset</a>
  <ul>
  <li><a href="#wikiann-a.k.a-pan-x" id="toc-wikiann-a.k.a-pan-x" class="nav-link" data-scroll-target="#wikiann-a.k.a-pan-x">WikiAnn (a.k.a PAN-X)</a>
  <ul class="collapse">
  <li><a href="#get_dataset_config_names" id="toc-get_dataset_config_names" class="nav-link" data-scroll-target="#get_dataset_config_names"><code>get_dataset_config_names</code></a></li>
  <li><a href="#xtreme-hugging-face-dataset-card" id="toc-xtreme-hugging-face-dataset-card" class="nav-link" data-scroll-target="#xtreme-hugging-face-dataset-card"><code>xtreme</code> Hugging Face Dataset Card</a></li>
  <li><a href="#dataset.shuffle" id="toc-dataset.shuffle" class="nav-link" data-scroll-target="#dataset.shuffle"><code>Dataset.shuffle</code></a></li>
  <li><a href="#dataset.select" id="toc-dataset.select" class="nav-link" data-scroll-target="#dataset.select"><code>Dataset.select</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#multilingual-transformers" id="toc-multilingual-transformers" class="nav-link" data-scroll-target="#multilingual-transformers">Multilingual Transformers</a>
  <ul>
  <li><a href="#evaluation-methods" id="toc-evaluation-methods" class="nav-link" data-scroll-target="#evaluation-methods">Evaluation Methods</a></li>
  </ul></li>
  <li><a href="#a-closer-look-at-tokenization" id="toc-a-closer-look-at-tokenization" class="nav-link" data-scroll-target="#a-closer-look-at-tokenization">A Closer Look at Tokenization</a>
  <ul>
  <li><a href="#the-tokenizer-pipeline" id="toc-the-tokenizer-pipeline" class="nav-link" data-scroll-target="#the-tokenizer-pipeline">The Tokenizer Pipeline</a>
  <ul class="collapse">
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization">1. Normalization</a></li>
  <li><a href="#pretokenization" id="toc-pretokenization" class="nav-link" data-scroll-target="#pretokenization">2. Pretokenization</a></li>
  <li><a href="#tokenizer-model" id="toc-tokenizer-model" class="nav-link" data-scroll-target="#tokenizer-model">3. Tokenizer Model</a></li>
  <li><a href="#postprocessing" id="toc-postprocessing" class="nav-link" data-scroll-target="#postprocessing">4. Postprocessing</a></li>
  </ul></li>
  <li><a href="#the-sentencepiece-tokenizer" id="toc-the-sentencepiece-tokenizer" class="nav-link" data-scroll-target="#the-sentencepiece-tokenizer">The SentencePiece Tokenizer</a></li>
  </ul></li>
  <li><a href="#transformers-for-named-entity-recognition" id="toc-transformers-for-named-entity-recognition" class="nav-link" data-scroll-target="#transformers-for-named-entity-recognition">Transformers for Named Entity Recognition</a></li>
  <li><a href="#the-anatomy-of-the-transformers-model-class" id="toc-the-anatomy-of-the-transformers-model-class" class="nav-link" data-scroll-target="#the-anatomy-of-the-transformers-model-class">The Anatomy of the Transformers Model Class</a>
  <ul>
  <li><a href="#bodies-and-heads" id="toc-bodies-and-heads" class="nav-link" data-scroll-target="#bodies-and-heads">Bodies and Heads</a></li>
  <li><a href="#creating-a-custom-model-for-token-classification" id="toc-creating-a-custom-model-for-token-classification" class="nav-link" data-scroll-target="#creating-a-custom-model-for-token-classification">Creating a Custom Model for Token Classification</a>
  <ul class="collapse">
  <li><a href="#tokenclassifieroutput" id="toc-tokenclassifieroutput" class="nav-link" data-scroll-target="#tokenclassifieroutput"><code>TokenClassifierOutput</code></a></li>
  <li><a href="#modeloutput" id="toc-modeloutput" class="nav-link" data-scroll-target="#modeloutput"><code>ModelOutput</code></a></li>
  <li><a href="#robertamodel" id="toc-robertamodel" class="nav-link" data-scroll-target="#robertamodel"><code>RobertaModel</code></a></li>
  <li><a href="#robertapretrainedmodel" id="toc-robertapretrainedmodel" class="nav-link" data-scroll-target="#robertapretrainedmodel"><code>RobertaPreTrainedModel</code></a></li>
  </ul></li>
  <li><a href="#loading-a-custom-model" id="toc-loading-a-custom-model" class="nav-link" data-scroll-target="#loading-a-custom-model">Loading a Custom Model</a></li>
  </ul></li>
  <li><a href="#tokenizing-texts-for-ner" id="toc-tokenizing-texts-for-ner" class="nav-link" data-scroll-target="#tokenizing-texts-for-ner">Tokenizing Texts for NER</a>
  <ul>
  <li><a href="#batchencoding.word_ids.word_ids" id="toc-batchencoding.word_ids.word_ids" class="nav-link" data-scroll-target="#batchencoding.word_ids.word_ids"><code>BatchEncoding.word_ids.word_ids</code></a></li>
  </ul></li>
  <li><a href="#performance-measures" id="toc-performance-measures" class="nav-link" data-scroll-target="#performance-measures">Performance Measures</a>
  <ul>
  <li><a href="#seqval" id="toc-seqval" class="nav-link" data-scroll-target="#seqval">seqval</a>
  <ul class="collapse">
  <li><a href="#classification_report" id="toc-classification_report" class="nav-link" data-scroll-target="#classification_report"><code>classification_report</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#fine-tuning-xlm-roberta" id="toc-fine-tuning-xlm-roberta" class="nav-link" data-scroll-target="#fine-tuning-xlm-roberta">Fine-Tuning XLM-RoBERTa</a>
  <ul>
  <li><a href="#datacollatorfortokenclassification" id="toc-datacollatorfortokenclassification" class="nav-link" data-scroll-target="#datacollatorfortokenclassification"><code>DataCollatorForTokenClassification</code></a></li>
  </ul></li>
  <li><a href="#error-analysis" id="toc-error-analysis" class="nav-link" data-scroll-target="#error-analysis">Error Analysis</a>
  <ul>
  <li><a href="#failure-modes" id="toc-failure-modes" class="nav-link" data-scroll-target="#failure-modes">Failure Modes</a></li>
  </ul></li>
  <li><a href="#cross-lingual-transfer" id="toc-cross-lingual-transfer" class="nav-link" data-scroll-target="#cross-lingual-transfer">Cross-Lingual Transfer</a>
  <ul>
  <li><a href="#when-does-zero-shot-transfer-make-sense" id="toc-when-does-zero-shot-transfer-make-sense" class="nav-link" data-scroll-target="#when-does-zero-shot-transfer-make-sense">When Does Zero-Shot Transfer Make Sense?</a></li>
  <li><a href="#fine-tuning-on-multiple-languages-at-once" id="toc-fine-tuning-on-multiple-languages-at-once" class="nav-link" data-scroll-target="#fine-tuning-on-multiple-languages-at-once">Fine-Tuning on Multiple Languages at Once</a>
  <ul class="collapse">
  <li><a href="#concatenate_datasets" id="toc-concatenate_datasets" class="nav-link" data-scroll-target="#concatenate_datasets"><code>concatenate_datasets</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 4</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 4 covers fine-tuning a multilingual transformer model to perform named entity recognition.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 7, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/transformers-book-notes.html"><strong>Natural Language Processing with Transformers</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#project-multilingual-named-entity-recognition">Project: Multilingual Named Entity Recognition</a></li>
<li><a href="#the-dataset">The Dataset</a></li>
<li><a href="#multilingual-transformers">Multilingual Transformers</a></li>
<li><a href="#a-closer-look%20at-tokenization">A Closer Look at Tokenization</a></li>
<li><a href="#transformers-for-named-entity-recognition">Transformers for Named Entity Recognition</a></li>
<li><a href="#the-anatomy-of-the-transformers-model-class">The Anatomy of the Transformers Model Class</a></li>
<li><a href="#tokenizing-texts-for-ner">Tokenizing Texts for NER</a></li>
<li><a href="#performance-measures">Performance Measures</a></li>
<li><a href="#fine-tuning-xlm-roberta">Fine-Tuning XLM-RoBERTa</a></li>
<li><a href="#error-analysis">Error Analysis</a></li>
<li><a href="#cross-lingual-transfer">Cross-Lingual Transfer</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>transformers.__version__, datasets.__version__, accelerate.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('4.11.3', '1.16.1', '0.5.1')</code></pre>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>Non-English pretrained models typically exist only for languages like German, Russian, or Mandarin, where plenty of web text is available for pretraining.</li>
<li>Avoid maintaining multiple monolingual models in production when possible.</li>
<li>Transformer models pretrained on large corpora across many languages can perform zero-shot cross-lingual transfer.
<ul>
<li>We can fine-tune a model using one language and apply it to others without further training.</li>
</ul></li>
<li>Multilingual transformers are well-suited for situations where a speaker alternates between two or more languages in the context of a single conversation.</li>
</ul>
</section>
<section id="project-multilingual-named-entity-recognition" class="level2">
<h2 class="anchored" data-anchor-id="project-multilingual-named-entity-recognition">Project: Multilingual Named Entity Recognition</h2>
<ul>
<li>The goal is to fine-tine the transformer model <a href="https://arxiv.org/abs/1911.02116">XLM-RoBERTa</a> to perform named entity recognition for a customer in Switzerland, where there are <a href="https://en.wikipedia.org/wiki/Languages_of_Switzerland">four national languages</a>.
<ul>
<li>We will use German, French, Italian, and English as the four languages.</li>
</ul></li>
<li>Named entity recognition involves extracting real-world objects like products, places, and people from a piece of text.
<ul>
<li>Some potential NER applications include gaining insights from company documents, augmenting the quality of search engines, and building a structured database from a corpus.</li>
</ul></li>
</ul>
</section>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The Dataset</h2>
<section id="wikiann-a.k.a-pan-x" class="level3">
<h3 class="anchored" data-anchor-id="wikiann-a.k.a-pan-x">WikiAnn (a.k.a PAN-X)</h3>
<ul>
<li>WikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages.</li>
<li>Each article has annotations for location, person, and organization tags in the <a href="https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)">IOB2</a> format.
<ul>
<li>The IOB2 format indicates the beginning of an entity with a <code>B-</code> prefix, consecutive tags belonging to the same entity with an <code>I-</code> prefix, and tokens that do not belong to any entity with an <code>O</code> tag.</li>
</ul></li>
<li>WikiANN is a subset of the <a href="https://arxiv.org/abs/2003.11080">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</a> benchmark.</li>
<li><a href="https://aclanthology.org/P17-1178/">Cross-lingual Name Tagging and Linking for 282 Languages</a></li>
<li><a href="https://huggingface.co/datasets/wikiann">Hugging Face Dataset Card</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An example of a sequence annotated with named entities in IOB2 format</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>toks <span class="op">=</span> <span class="st">"Jeff Dean is a computer scientist at Google in California"</span>.split()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>lbls <span class="op">=</span> [<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-ORG"</span>, <span class="st">"O"</span>, <span class="st">"B-LOC"</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(data<span class="op">=</span>[toks, lbls], index<span class="op">=</span>[<span class="st">'Tokens'</span>, <span class="st">'Tags'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
Jeff
</td>
<td>
Dean
</td>
<td>
is
</td>
<td>
a
</td>
<td>
computer
</td>
<td>
scientist
</td>
<td>
at
</td>
<td>
Google
</td>
<td>
in
</td>
<td>
California
</td>
</tr>
<tr>
<th>
Tags
</th>
<td>
B-PER
</td>
<td>
I-PER
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-ORG
</td>
<td>
O
</td>
<td>
B-LOC
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> get_dataset_config_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="get_dataset_config_names" class="level4">
<h4 class="anchored" data-anchor-id="get_dataset_config_names"><code>get_dataset_config_names</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/loading_methods#datasets.get_dataset_config_names">Documentation</a></li>
<li>Get the list of available configuration names for a particular dataset.</li>
</ul>
<hr>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>print_source(get_dataset_config_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def get_dataset_config_names(path: str, revision: Optional[Union[str,
        Version]]=None, download_config: Optional[DownloadConfig]=None,
        download_mode: Optional[GenerateMode]=None, force_local_path: Optional[
        str]=None, dynamic_modules_path: Optional[str]=None, data_files:
        Optional[Union[Dict, List, str]]=None, **download_kwargs):
        dataset_module = dataset_module_factory(path, revision=revision,
            download_config=download_config, download_mode=download_mode,
            force_local_path=force_local_path, dynamic_modules_path=
            dynamic_modules_path, data_files=data_files, **download_kwargs)
        builder_cls = import_main_class(dataset_module.module_path)
        return list(builder_cls.builder_configs.keys()) or [dataset_module.
            builder_kwargs.get('name', 'default')]</code></pre>
<hr>
</section>
<section id="xtreme-hugging-face-dataset-card" class="level4">
<h4 class="anchored" data-anchor-id="xtreme-hugging-face-dataset-card"><a href="https://huggingface.co/datasets/xtreme"><code>xtreme</code> Hugging Face Dataset Card</a></h4>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the names of the subsets for the XTREME dataset</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>xtreme_subsets <span class="op">=</span> get_dataset_config_names(<span class="st">"xtreme"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XTREME has </span><span class="sc">{</span><span class="bu">len</span>(xtreme_subsets)<span class="sc">}</span><span class="ss"> configurations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    XTREME has 183 configurations</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(xtreme_subsets).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>).hide(axis<span class="op">=</span><span class="st">'index'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table id="T_28e13">
<thead>
</thead>
<tbody>
<tr>
<td id="T_28e13_row0_col0" class="data row0 col0">
XNLI
</td>
<td id="T_28e13_row0_col1" class="data row0 col1">
tydiqa
</td>
<td id="T_28e13_row0_col2" class="data row0 col2">
SQuAD
</td>
<td id="T_28e13_row0_col3" class="data row0 col3">
PAN-X.af
</td>
<td id="T_28e13_row0_col4" class="data row0 col4">
PAN-X.ar
</td>
<td id="T_28e13_row0_col5" class="data row0 col5">
PAN-X.bg
</td>
<td id="T_28e13_row0_col6" class="data row0 col6">
PAN-X.bn
</td>
<td id="T_28e13_row0_col7" class="data row0 col7">
PAN-X.de
</td>
<td id="T_28e13_row0_col8" class="data row0 col8">
PAN-X.el
</td>
<td id="T_28e13_row0_col9" class="data row0 col9">
PAN-X.en
</td>
<td id="T_28e13_row0_col10" class="data row0 col10">
PAN-X.es
</td>
<td id="T_28e13_row0_col11" class="data row0 col11">
PAN-X.et
</td>
<td id="T_28e13_row0_col12" class="data row0 col12">
PAN-X.eu
</td>
<td id="T_28e13_row0_col13" class="data row0 col13">
PAN-X.fa
</td>
<td id="T_28e13_row0_col14" class="data row0 col14">
PAN-X.fi
</td>
<td id="T_28e13_row0_col15" class="data row0 col15">
PAN-X.fr
</td>
<td id="T_28e13_row0_col16" class="data row0 col16">
PAN-X.he
</td>
<td id="T_28e13_row0_col17" class="data row0 col17">
PAN-X.hi
</td>
<td id="T_28e13_row0_col18" class="data row0 col18">
PAN-X.hu
</td>
<td id="T_28e13_row0_col19" class="data row0 col19">
PAN-X.id
</td>
<td id="T_28e13_row0_col20" class="data row0 col20">
PAN-X.it
</td>
<td id="T_28e13_row0_col21" class="data row0 col21">
PAN-X.ja
</td>
<td id="T_28e13_row0_col22" class="data row0 col22">
PAN-X.jv
</td>
<td id="T_28e13_row0_col23" class="data row0 col23">
PAN-X.ka
</td>
<td id="T_28e13_row0_col24" class="data row0 col24">
PAN-X.kk
</td>
<td id="T_28e13_row0_col25" class="data row0 col25">
PAN-X.ko
</td>
<td id="T_28e13_row0_col26" class="data row0 col26">
PAN-X.ml
</td>
<td id="T_28e13_row0_col27" class="data row0 col27">
PAN-X.mr
</td>
<td id="T_28e13_row0_col28" class="data row0 col28">
PAN-X.ms
</td>
<td id="T_28e13_row0_col29" class="data row0 col29">
PAN-X.my
</td>
<td id="T_28e13_row0_col30" class="data row0 col30">
PAN-X.nl
</td>
<td id="T_28e13_row0_col31" class="data row0 col31">
PAN-X.pt
</td>
<td id="T_28e13_row0_col32" class="data row0 col32">
PAN-X.ru
</td>
<td id="T_28e13_row0_col33" class="data row0 col33">
PAN-X.sw
</td>
<td id="T_28e13_row0_col34" class="data row0 col34">
PAN-X.ta
</td>
<td id="T_28e13_row0_col35" class="data row0 col35">
PAN-X.te
</td>
<td id="T_28e13_row0_col36" class="data row0 col36">
PAN-X.th
</td>
<td id="T_28e13_row0_col37" class="data row0 col37">
PAN-X.tl
</td>
<td id="T_28e13_row0_col38" class="data row0 col38">
PAN-X.tr
</td>
<td id="T_28e13_row0_col39" class="data row0 col39">
PAN-X.ur
</td>
<td id="T_28e13_row0_col40" class="data row0 col40">
PAN-X.vi
</td>
<td id="T_28e13_row0_col41" class="data row0 col41">
PAN-X.yo
</td>
<td id="T_28e13_row0_col42" class="data row0 col42">
PAN-X.zh
</td>
<td id="T_28e13_row0_col43" class="data row0 col43">
MLQA.ar.ar
</td>
<td id="T_28e13_row0_col44" class="data row0 col44">
MLQA.ar.de
</td>
<td id="T_28e13_row0_col45" class="data row0 col45">
MLQA.ar.vi
</td>
<td id="T_28e13_row0_col46" class="data row0 col46">
MLQA.ar.zh
</td>
<td id="T_28e13_row0_col47" class="data row0 col47">
MLQA.ar.en
</td>
<td id="T_28e13_row0_col48" class="data row0 col48">
MLQA.ar.es
</td>
<td id="T_28e13_row0_col49" class="data row0 col49">
MLQA.ar.hi
</td>
<td id="T_28e13_row0_col50" class="data row0 col50">
MLQA.de.ar
</td>
<td id="T_28e13_row0_col51" class="data row0 col51">
MLQA.de.de
</td>
<td id="T_28e13_row0_col52" class="data row0 col52">
MLQA.de.vi
</td>
<td id="T_28e13_row0_col53" class="data row0 col53">
MLQA.de.zh
</td>
<td id="T_28e13_row0_col54" class="data row0 col54">
MLQA.de.en
</td>
<td id="T_28e13_row0_col55" class="data row0 col55">
MLQA.de.es
</td>
<td id="T_28e13_row0_col56" class="data row0 col56">
MLQA.de.hi
</td>
<td id="T_28e13_row0_col57" class="data row0 col57">
MLQA.vi.ar
</td>
<td id="T_28e13_row0_col58" class="data row0 col58">
MLQA.vi.de
</td>
<td id="T_28e13_row0_col59" class="data row0 col59">
MLQA.vi.vi
</td>
<td id="T_28e13_row0_col60" class="data row0 col60">
MLQA.vi.zh
</td>
<td id="T_28e13_row0_col61" class="data row0 col61">
MLQA.vi.en
</td>
<td id="T_28e13_row0_col62" class="data row0 col62">
MLQA.vi.es
</td>
<td id="T_28e13_row0_col63" class="data row0 col63">
MLQA.vi.hi
</td>
<td id="T_28e13_row0_col64" class="data row0 col64">
MLQA.zh.ar
</td>
<td id="T_28e13_row0_col65" class="data row0 col65">
MLQA.zh.de
</td>
<td id="T_28e13_row0_col66" class="data row0 col66">
MLQA.zh.vi
</td>
<td id="T_28e13_row0_col67" class="data row0 col67">
MLQA.zh.zh
</td>
<td id="T_28e13_row0_col68" class="data row0 col68">
MLQA.zh.en
</td>
<td id="T_28e13_row0_col69" class="data row0 col69">
MLQA.zh.es
</td>
<td id="T_28e13_row0_col70" class="data row0 col70">
MLQA.zh.hi
</td>
<td id="T_28e13_row0_col71" class="data row0 col71">
MLQA.en.ar
</td>
<td id="T_28e13_row0_col72" class="data row0 col72">
MLQA.en.de
</td>
<td id="T_28e13_row0_col73" class="data row0 col73">
MLQA.en.vi
</td>
<td id="T_28e13_row0_col74" class="data row0 col74">
MLQA.en.zh
</td>
<td id="T_28e13_row0_col75" class="data row0 col75">
MLQA.en.en
</td>
<td id="T_28e13_row0_col76" class="data row0 col76">
MLQA.en.es
</td>
<td id="T_28e13_row0_col77" class="data row0 col77">
MLQA.en.hi
</td>
<td id="T_28e13_row0_col78" class="data row0 col78">
MLQA.es.ar
</td>
<td id="T_28e13_row0_col79" class="data row0 col79">
MLQA.es.de
</td>
<td id="T_28e13_row0_col80" class="data row0 col80">
MLQA.es.vi
</td>
<td id="T_28e13_row0_col81" class="data row0 col81">
MLQA.es.zh
</td>
<td id="T_28e13_row0_col82" class="data row0 col82">
MLQA.es.en
</td>
<td id="T_28e13_row0_col83" class="data row0 col83">
MLQA.es.es
</td>
<td id="T_28e13_row0_col84" class="data row0 col84">
MLQA.es.hi
</td>
<td id="T_28e13_row0_col85" class="data row0 col85">
MLQA.hi.ar
</td>
<td id="T_28e13_row0_col86" class="data row0 col86">
MLQA.hi.de
</td>
<td id="T_28e13_row0_col87" class="data row0 col87">
MLQA.hi.vi
</td>
<td id="T_28e13_row0_col88" class="data row0 col88">
MLQA.hi.zh
</td>
<td id="T_28e13_row0_col89" class="data row0 col89">
MLQA.hi.en
</td>
<td id="T_28e13_row0_col90" class="data row0 col90">
MLQA.hi.es
</td>
<td id="T_28e13_row0_col91" class="data row0 col91">
MLQA.hi.hi
</td>
<td id="T_28e13_row0_col92" class="data row0 col92">
XQuAD.ar
</td>
<td id="T_28e13_row0_col93" class="data row0 col93">
XQuAD.de
</td>
<td id="T_28e13_row0_col94" class="data row0 col94">
XQuAD.vi
</td>
<td id="T_28e13_row0_col95" class="data row0 col95">
XQuAD.zh
</td>
<td id="T_28e13_row0_col96" class="data row0 col96">
XQuAD.en
</td>
<td id="T_28e13_row0_col97" class="data row0 col97">
XQuAD.es
</td>
<td id="T_28e13_row0_col98" class="data row0 col98">
XQuAD.hi
</td>
<td id="T_28e13_row0_col99" class="data row0 col99">
XQuAD.el
</td>
<td id="T_28e13_row0_col100" class="data row0 col100">
XQuAD.ru
</td>
<td id="T_28e13_row0_col101" class="data row0 col101">
XQuAD.th
</td>
<td id="T_28e13_row0_col102" class="data row0 col102">
XQuAD.tr
</td>
<td id="T_28e13_row0_col103" class="data row0 col103">
bucc18.de
</td>
<td id="T_28e13_row0_col104" class="data row0 col104">
bucc18.fr
</td>
<td id="T_28e13_row0_col105" class="data row0 col105">
bucc18.zh
</td>
<td id="T_28e13_row0_col106" class="data row0 col106">
bucc18.ru
</td>
<td id="T_28e13_row0_col107" class="data row0 col107">
PAWS-X.de
</td>
<td id="T_28e13_row0_col108" class="data row0 col108">
PAWS-X.en
</td>
<td id="T_28e13_row0_col109" class="data row0 col109">
PAWS-X.es
</td>
<td id="T_28e13_row0_col110" class="data row0 col110">
PAWS-X.fr
</td>
<td id="T_28e13_row0_col111" class="data row0 col111">
PAWS-X.ja
</td>
<td id="T_28e13_row0_col112" class="data row0 col112">
PAWS-X.ko
</td>
<td id="T_28e13_row0_col113" class="data row0 col113">
PAWS-X.zh
</td>
<td id="T_28e13_row0_col114" class="data row0 col114">
tatoeba.afr
</td>
<td id="T_28e13_row0_col115" class="data row0 col115">
tatoeba.ara
</td>
<td id="T_28e13_row0_col116" class="data row0 col116">
tatoeba.ben
</td>
<td id="T_28e13_row0_col117" class="data row0 col117">
tatoeba.bul
</td>
<td id="T_28e13_row0_col118" class="data row0 col118">
tatoeba.deu
</td>
<td id="T_28e13_row0_col119" class="data row0 col119">
tatoeba.cmn
</td>
<td id="T_28e13_row0_col120" class="data row0 col120">
tatoeba.ell
</td>
<td id="T_28e13_row0_col121" class="data row0 col121">
tatoeba.est
</td>
<td id="T_28e13_row0_col122" class="data row0 col122">
tatoeba.eus
</td>
<td id="T_28e13_row0_col123" class="data row0 col123">
tatoeba.fin
</td>
<td id="T_28e13_row0_col124" class="data row0 col124">
tatoeba.fra
</td>
<td id="T_28e13_row0_col125" class="data row0 col125">
tatoeba.heb
</td>
<td id="T_28e13_row0_col126" class="data row0 col126">
tatoeba.hin
</td>
<td id="T_28e13_row0_col127" class="data row0 col127">
tatoeba.hun
</td>
<td id="T_28e13_row0_col128" class="data row0 col128">
tatoeba.ind
</td>
<td id="T_28e13_row0_col129" class="data row0 col129">
tatoeba.ita
</td>
<td id="T_28e13_row0_col130" class="data row0 col130">
tatoeba.jav
</td>
<td id="T_28e13_row0_col131" class="data row0 col131">
tatoeba.jpn
</td>
<td id="T_28e13_row0_col132" class="data row0 col132">
tatoeba.kat
</td>
<td id="T_28e13_row0_col133" class="data row0 col133">
tatoeba.kaz
</td>
<td id="T_28e13_row0_col134" class="data row0 col134">
tatoeba.kor
</td>
<td id="T_28e13_row0_col135" class="data row0 col135">
tatoeba.mal
</td>
<td id="T_28e13_row0_col136" class="data row0 col136">
tatoeba.mar
</td>
<td id="T_28e13_row0_col137" class="data row0 col137">
tatoeba.nld
</td>
<td id="T_28e13_row0_col138" class="data row0 col138">
tatoeba.pes
</td>
<td id="T_28e13_row0_col139" class="data row0 col139">
tatoeba.por
</td>
<td id="T_28e13_row0_col140" class="data row0 col140">
tatoeba.rus
</td>
<td id="T_28e13_row0_col141" class="data row0 col141">
tatoeba.spa
</td>
<td id="T_28e13_row0_col142" class="data row0 col142">
tatoeba.swh
</td>
<td id="T_28e13_row0_col143" class="data row0 col143">
tatoeba.tam
</td>
<td id="T_28e13_row0_col144" class="data row0 col144">
tatoeba.tel
</td>
<td id="T_28e13_row0_col145" class="data row0 col145">
tatoeba.tgl
</td>
<td id="T_28e13_row0_col146" class="data row0 col146">
tatoeba.tha
</td>
<td id="T_28e13_row0_col147" class="data row0 col147">
tatoeba.tur
</td>
<td id="T_28e13_row0_col148" class="data row0 col148">
tatoeba.urd
</td>
<td id="T_28e13_row0_col149" class="data row0 col149">
tatoeba.vie
</td>
<td id="T_28e13_row0_col150" class="data row0 col150">
udpos.Afrikaans
</td>
<td id="T_28e13_row0_col151" class="data row0 col151">
udpos.Arabic
</td>
<td id="T_28e13_row0_col152" class="data row0 col152">
udpos.Basque
</td>
<td id="T_28e13_row0_col153" class="data row0 col153">
udpos.Bulgarian
</td>
<td id="T_28e13_row0_col154" class="data row0 col154">
udpos.Dutch
</td>
<td id="T_28e13_row0_col155" class="data row0 col155">
udpos.English
</td>
<td id="T_28e13_row0_col156" class="data row0 col156">
udpos.Estonian
</td>
<td id="T_28e13_row0_col157" class="data row0 col157">
udpos.Finnish
</td>
<td id="T_28e13_row0_col158" class="data row0 col158">
udpos.French
</td>
<td id="T_28e13_row0_col159" class="data row0 col159">
udpos.German
</td>
<td id="T_28e13_row0_col160" class="data row0 col160">
udpos.Greek
</td>
<td id="T_28e13_row0_col161" class="data row0 col161">
udpos.Hebrew
</td>
<td id="T_28e13_row0_col162" class="data row0 col162">
udpos.Hindi
</td>
<td id="T_28e13_row0_col163" class="data row0 col163">
udpos.Hungarian
</td>
<td id="T_28e13_row0_col164" class="data row0 col164">
udpos.Indonesian
</td>
<td id="T_28e13_row0_col165" class="data row0 col165">
udpos.Italian
</td>
<td id="T_28e13_row0_col166" class="data row0 col166">
udpos.Japanese
</td>
<td id="T_28e13_row0_col167" class="data row0 col167">
udpos.Kazakh
</td>
<td id="T_28e13_row0_col168" class="data row0 col168">
udpos.Korean
</td>
<td id="T_28e13_row0_col169" class="data row0 col169">
udpos.Chinese
</td>
<td id="T_28e13_row0_col170" class="data row0 col170">
udpos.Marathi
</td>
<td id="T_28e13_row0_col171" class="data row0 col171">
udpos.Persian
</td>
<td id="T_28e13_row0_col172" class="data row0 col172">
udpos.Portuguese
</td>
<td id="T_28e13_row0_col173" class="data row0 col173">
udpos.Russian
</td>
<td id="T_28e13_row0_col174" class="data row0 col174">
udpos.Spanish
</td>
<td id="T_28e13_row0_col175" class="data row0 col175">
udpos.Tagalog
</td>
<td id="T_28e13_row0_col176" class="data row0 col176">
udpos.Tamil
</td>
<td id="T_28e13_row0_col177" class="data row0 col177">
udpos.Telugu
</td>
<td id="T_28e13_row0_col178" class="data row0 col178">
udpos.Thai
</td>
<td id="T_28e13_row0_col179" class="data row0 col179">
udpos.Turkish
</td>
<td id="T_28e13_row0_col180" class="data row0 col180">
udpos.Urdu
</td>
<td id="T_28e13_row0_col181" class="data row0 col181">
udpos.Vietnamese
</td>
<td id="T_28e13_row0_col182" class="data row0 col182">
udpos.Yoruba
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> We are only interested in the PAN-X subsets for this project.</p>
<hr>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Look for configuration names containing 'PAN'</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>panx_subsets <span class="op">=</span> [s <span class="cf">for</span> s <span class="kw">in</span> xtreme_subsets <span class="cf">if</span> s.startswith(<span class="st">"PAN"</span>)]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(panx_subsets), panx_subsets[:<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (40, ['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg'])</code></pre>
<p><strong>Note:</strong> * There are 40 PAN-X subsets. * Each subset has a two-letter suffix indicating the <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">ISO 639-1</a> language code. * German (de) * French (fr) * Italian (it) * English (en)</p>
<hr>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> DatasetDict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the desired language codes</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>langs <span class="op">=</span> [<span class="st">"de"</span>, <span class="st">"fr"</span>, <span class="st">"it"</span>, <span class="st">"en"</span>]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the percentage each language should contribute to the total dataset</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>fracs <span class="op">=</span> [<span class="fl">0.629</span>, <span class="fl">0.229</span>, <span class="fl">0.084</span>, <span class="fl">0.059</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> * These percentages represent the spoken proportions for each language in Switzerland. * This language imbalance simulates the common situation where acquiring labeled examples in a minority language is cost-prohibitive.</p>
<hr>
</section>
<section id="dataset.shuffle" class="level4">
<h4 class="anchored" data-anchor-id="dataset.shuffle"><code>Dataset.shuffle</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.shuffle">Documentation</a></li>
<li>Create a new dataset with shuffled rows.</li>
</ul>
</section>
<section id="dataset.select" class="level4">
<h4 class="anchored" data-anchor-id="dataset.select"><code>Dataset.select</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.select">Documentation</a></li>
<li>Create a new dataset with rows selected following the list/array of indices.<br>
</li>
</ul>
<hr>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Return a DatasetDict if a key doesn't exist</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>panx_ch <span class="op">=</span> defaultdict(DatasetDict)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lang, frac <span class="kw">in</span> <span class="bu">zip</span>(langs, fracs):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load monolingual corpus</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    ds <span class="op">=</span> load_dataset(<span class="st">"xtreme"</span>, name<span class="op">=</span><span class="ss">f"PAN-X.</span><span class="sc">{</span>lang<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shuffle and downsample each split according to spoken proportion</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> split <span class="kw">in</span> ds:</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        panx_ch[lang][split] <span class="op">=</span> (</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>            ds[split]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Shuffle the dataset split rows</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            .shuffle(seed<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select subset of dataset split</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            .select(<span class="bu">range</span>(<span class="bu">int</span>(frac <span class="op">*</span> ds[split].num_rows))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({lang: [panx_ch[lang][<span class="st">"train"</span>].num_rows] <span class="cf">for</span> lang <span class="kw">in</span> langs}, index<span class="op">=</span>[<span class="st">"Number of training examples"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
de
</th>
<th>
fr
</th>
<th>
it
</th>
<th>
en
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Number of training examples
</th>
<td>
12580
</td>
<td>
4580
</td>
<td>
1680
</td>
<td>
1180
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">sum</span>([panx_ch[lang][<span class="st">'train'</span>].num_rows <span class="cf">for</span> lang <span class="kw">in</span> langs])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    20020</code></pre>
<hr>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    {lang: [panx_ch[lang][<span class="st">"train"</span>].num_rows, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'</span><span class="sc">{</span>panx_ch[lang][<span class="st">"train"</span>]<span class="sc">.</span>num_rows<span class="op">/</span>train_size<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>] <span class="cf">for</span> lang <span class="kw">in</span> langs</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    }, index<span class="op">=</span>[<span class="st">"Number of training examples"</span>, <span class="st">"Proportion of Dataset"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
de
</th>
<th>
fr
</th>
<th>
it
</th>
<th>
en
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Number of training examples
</th>
<td>
12580
</td>
<td>
4580
</td>
<td>
1680
</td>
<td>
1180
</td>
</tr>
<tr>
<th>
Proportion of Dataset
</th>
<td>
62.84%
</td>
<td>
22.88%
</td>
<td>
8.39%
</td>
<td>
5.89%
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lang <span class="kw">in</span> langs: <span class="bu">print</span>(panx_ch[lang][<span class="st">"train"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Dataset({
        features: ['tokens', 'ner_tags', 'langs'],
        num_rows: 12580
    })
    Dataset({
        features: ['tokens', 'ner_tags', 'langs'],
        num_rows: 4580
    })
    Dataset({
        features: ['tokens', 'ner_tags', 'langs'],
        num_rows: 1680
    })
    Dataset({
        features: ['tokens', 'ner_tags', 'langs'],
        num_rows: 1180
    })</code></pre>
<hr>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>element <span class="op">=</span> panx_ch[<span class="st">"de"</span>][<span class="st">"train"</span>][<span class="dv">0</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(element).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
2.000
</td>
<td>
Einwohnern
</td>
<td>
an
</td>
<td>
der
</td>
<td>
Danziger
</td>
<td>
Bucht
</td>
<td>
in
</td>
<td>
der
</td>
<td>
polnischen
</td>
<td>
Woiwodschaft
</td>
<td>
Pommern
</td>
<td>
.
</td>
</tr>
<tr>
<th>
ner_tags
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
</tr>
<tr>
<th>
langs
</th>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The German text translates to “2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania.”
<ul>
<li>Gdansk Bay is a bay in the Baltic Sea.</li>
<li>The word “voivodeship” corresponds to a state in Poland.</li>
</ul></li>
<li>The ner_tags column corresponds to the mapping of each entity to a class ID.</li>
<li>The Dataset object has a “features” attribute that specifies the underlying data types associated with each column.</li>
</ul>
<hr>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>tags <span class="op">=</span> panx_ch[<span class="st">"de"</span>][<span class="st">"train"</span>].features[<span class="st">"ner_tags"</span>].feature</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>tags</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None)</code></pre>
<hr>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>tags.names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']</code></pre>
<hr>
<p><strong>Map the class IDs to the corresponding tag names</strong></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_tag_names(batch):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"ner_tags_str"</span>: [tags.int2str(idx) <span class="cf">for</span> idx <span class="kw">in</span> batch[<span class="st">"ner_tags"</span>]]}</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>panx_de <span class="op">=</span> panx_ch[<span class="st">"de"</span>].<span class="bu">map</span>(create_tag_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(panx_de[<span class="st">"train"</span>][<span class="dv">0</span>]).reindex(columns<span class="op">=</span>[<span class="st">"tokens"</span>, <span class="st">"ner_tags_str"</span>,<span class="st">"ner_tags"</span>,<span class="st">"langs"</span>]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
2.000
</td>
<td>
Einwohnern
</td>
<td>
an
</td>
<td>
der
</td>
<td>
Danziger
</td>
<td>
Bucht
</td>
<td>
in
</td>
<td>
der
</td>
<td>
polnischen
</td>
<td>
Woiwodschaft
</td>
<td>
Pommern
</td>
<td>
.
</td>
</tr>
<tr>
<th>
ner_tags_str
</th>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
I-LOC
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
B-LOC
</td>
<td>
I-LOC
</td>
<td>
O
</td>
</tr>
<tr>
<th>
ner_tags
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
</tr>
<tr>
<th>
langs
</th>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
<td>
de
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Calculate the frequencies of each entity across each split</strong></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>split2freqs <span class="op">=</span> defaultdict(Counter)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> split, dataset <span class="kw">in</span> panx_de.items():</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> dataset[<span class="st">"ner_tags_str"</span>]:</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tag <span class="kw">in</span> row:</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> tag.startswith(<span class="st">"B"</span>):</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>                tag_type <span class="op">=</span> tag.split(<span class="st">"-"</span>)[<span class="dv">1</span>]</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>                split2freqs[split][tag_type] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>pd.DataFrame.from_dict(split2freqs, orient<span class="op">=</span><span class="st">"index"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
ORG
</th>
<th>
LOC
</th>
<th>
PER
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
validation
</th>
<td>
2683
</td>
<td>
3172
</td>
<td>
2893
</td>
</tr>
<tr>
<th>
test
</th>
<td>
2573
</td>
<td>
3180
</td>
<td>
3071
</td>
</tr>
<tr>
<th>
train
</th>
<td>
5366
</td>
<td>
6186
</td>
<td>
5810
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The distributions of the entity frequencies are roughly the same for each split.</p>
<hr>
</section>
</section>
</section>
<section id="multilingual-transformers" class="level2">
<h2 class="anchored" data-anchor-id="multilingual-transformers">Multilingual Transformers</h2>
<ul>
<li>Multilingual transformers use a corpus consisting of documents in many languages for pretraining.
<ul>
<li>The models do not receive any explicit information to differentiate among languages.</li>
</ul></li>
<li>The resulting linguistic representations generalize well across languages for many downstream tasks.</li>
<li>Many use the CoNLL-2002 and CoNLL-2003 datasets as benchmarks to measure the progress of cross-lingual transfer for named entity recognition for English, Dutch, Spanish, and German.</li>
</ul>
<section id="evaluation-methods" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-methods">Evaluation Methods</h3>
<ol type="1">
<li><strong>en:</strong> Fine-tune using the English training data and then evaluate the model on each language’s test set.</li>
<li><strong>each:</strong> Fine-tune and evaluate using monolingual test data to measure per-language performance.</li>
<li><strong>all:</strong> Fine-tune using all the training data to evaluate the model on each language’s test set.</li>
</ol>
</section>
</section>
<section id="a-closer-look-at-tokenization" class="level2">
<h2 class="anchored" data-anchor-id="a-closer-look-at-tokenization">A Closer Look at Tokenization</h2>
<ul>
<li>XLM-RoBERTa uses the <a href="https://paperswithcode.com/method/sentencepiece">SentencePiece</a> subword tokenizer instead of the WordPiece tokenizer.</li>
</ul>
<hr>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>bert_model_name <span class="op">=</span> <span class="st">"bert-base-cased"</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>xlmr_model_name <span class="op">=</span> <span class="st">"xlm-roberta-base"</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>bert_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(bert_model_name)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>xlmr_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(xlmr_model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Compare the WordPiece and SentencePiece tokenizers</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Jack Sparrow loves New York!"</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>bert_tokens <span class="op">=</span> bert_tokenizer(text).tokens()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>xlmr_tokens <span class="op">=</span> xlmr_tokenizer(text).tokens()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([bert_tokens, xlmr_tokens], index<span class="op">=</span>[<span class="st">"BERT"</span>, <span class="st">"XLM-R"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
BERT
</th>
<td>
[CLS]
</td>
<td>
Jack
</td>
<td>
Spa
</td>
<td>
##rrow
</td>
<td>
loves
</td>
<td>
New
</td>
<td>
York
</td>
<td>
!
</td>
<td>
[SEP]
</td>
<td>
None
</td>
</tr>
<tr>
<th>
XLM-R
</th>
<td>
&lt;s&gt;
</td>
<td>
_Jack
</td>
<td>
_Spar
</td>
<td>
row
</td>
<td>
_love
</td>
<td>
s
</td>
<td>
_New
</td>
<td>
_York
</td>
<td>
!
</td>
<td>
&lt;/s&gt;
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> SentencePiece uses <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> to indicate the start and end sequences.</p>
<hr>
<section id="the-tokenizer-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="the-tokenizer-pipeline">The Tokenizer Pipeline</h3>
<section id="normalization" class="level4">
<h4 class="anchored" data-anchor-id="normalization">1. Normalization</h4>
<ul>
<li>The normalization step includes the operations to clean up raw text, such as stripping whitespace and removing accented characters.</li>
<li><a href="https://unicode.org/reports/tr15/">Unicode normalization schemes</a> replace the various ways to write the same character with standard forms.
<ul>
<li>Unicode normalization is particularly effective when working with multilingual corpora.</li>
</ul></li>
<li>Lowercasing can help reduce the vocabulary size when the model only accepts and uses lowercase characters.</li>
</ul>
</section>
<section id="pretokenization" class="level4">
<h4 class="anchored" data-anchor-id="pretokenization">2. Pretokenization</h4>
<ul>
<li>The pre-tokenization step splits a text into smaller objects, and the final tokens will be subunits of these objects.</li>
<li>Some languages might require language-specific pre-tokenization methods.</li>
</ul>
</section>
<section id="tokenizer-model" class="level4">
<h4 class="anchored" data-anchor-id="tokenizer-model">3. Tokenizer Model</h4>
<ul>
<li>The tokenizer model analyzes the training corpus to find the most commonly occurring groups of characters, which become the vocab.</li>
</ul>
</section>
<section id="postprocessing" class="level4">
<h4 class="anchored" data-anchor-id="postprocessing">4. Postprocessing</h4>
<ul>
<li>The postprocessing step applies some additional transformations, such as adding special characters to the start or end of an input sequence.</li>
</ul>
</section>
</section>
<section id="the-sentencepiece-tokenizer" class="level3">
<h3 class="anchored" data-anchor-id="the-sentencepiece-tokenizer">The SentencePiece Tokenizer</h3>
<ul>
<li>The SentencePiece tokenizer builds on the Unigram subword segmentation algorithm and encodes each input text as a sequence of Unicode characters.</li>
<li>SentencePiece supports the <a href="https://aclanthology.org/P16-1162/">byte-pair-encoding (BPE)</a> algorithm and the <a href="https://arxiv.org/abs/1804.10959">unigram language model</a>.</li>
<li>SentencePiece replaces whitespace with the Unicode symbol <code>U+2581</code> for <code>_</code>.</li>
</ul>
<hr>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">""</span>.join(xlmr_tokens).replace(<span class="st">u"</span><span class="ch">\u2581</span><span class="st">"</span>, <span class="st">" "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    '&lt;s&gt; Jack Sparrow loves New York!&lt;/s&gt;'</code></pre>
<hr>
</section>
</section>
<section id="transformers-for-named-entity-recognition" class="level2">
<h2 class="anchored" data-anchor-id="transformers-for-named-entity-recognition">Transformers for Named Entity Recognition</h2>
<ul>
<li>For text classification, BERT uses the <code>[CLS]</code> token to represent an entire sequence of text.</li>
<li>For named entity recognition, BERT feeds the representation of each input token through the same fully connected layer to output the entity of each one.
<ul>
<li>We can assign the entity label to the first subword of a word and ignore the rest.</li>
</ul></li>
</ul>
</section>
<section id="the-anatomy-of-the-transformers-model-class" class="level2">
<h2 class="anchored" data-anchor-id="the-anatomy-of-the-transformers-model-class">The Anatomy of the Transformers Model Class</h2>
<ul>
<li>The Hugging Face Transformers library has dedicated classes for each architecture and task.</li>
<li>We can extend existing models for specific use cases with little overhead.</li>
</ul>
<section id="bodies-and-heads" class="level3">
<h3 class="anchored" data-anchor-id="bodies-and-heads">Bodies and Heads</h3>
<ul>
<li>Hugging Face Transformers splits model architectures into a body and head</li>
<li>The body is task-agnostic, and the model head is unique to a specific downstream task.</li>
</ul>
</section>
<section id="creating-a-custom-model-for-token-classification" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-custom-model-for-token-classification">Creating a Custom Model for Token Classification</h3>
<hr>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> XLMRobertaConfig</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.modeling_outputs <span class="im">import</span> TokenClassifierOutput</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.models.roberta.modeling_roberta <span class="im">import</span> RobertaModel</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.models.roberta.modeling_roberta <span class="im">import</span> RobertaPreTrainedModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="tokenclassifieroutput" class="level4">
<h4 class="anchored" data-anchor-id="tokenclassifieroutput"><code>TokenClassifierOutput</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput">Documentation</a></li>
<li>A base class for outputs of token classification models.</li>
</ul>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>print_source(TokenClassifierOutput)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @dataclass
    class TokenClassifierOutput(ModelOutput):
        loss: Optional[torch.FloatTensor] = None
        logits: torch.FloatTensor = None
        hidden_states: Optional[Tuple[torch.FloatTensor]] = None
        attentions: Optional[Tuple[torch.FloatTensor]] = None</code></pre>
<hr>
</section>
<section id="modeloutput" class="level4">
<h4 class="anchored" data-anchor-id="modeloutput"><code>ModelOutput</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">Documentation</a></li>
<li>The base class for all model outputs.</li>
</ul>
</section>
<section id="robertamodel" class="level4">
<h4 class="anchored" data-anchor-id="robertamodel"><code>RobertaModel</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel">Documentation</a></li>
<li>A bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.</li>
</ul>
</section>
<section id="robertapretrainedmodel" class="level4">
<h4 class="anchored" data-anchor-id="robertapretrainedmodel"><code>RobertaPreTrainedModel</code></h4>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/febe42b5daf4b416f4613e9d7f68617ee983bb40/src/transformers/models/roberta/modeling_roberta.py#L585">Source Code</a></li>
<li>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</li>
</ul>
<hr>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>XLMRobertaConfig()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    XLMRobertaConfig {
      "attention_probs_dropout_prob": 0.1,
      "bos_token_id": 0,
      "classifier_dropout": null,
      "eos_token_id": 2,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "xlm-roberta",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "pad_token_id": 1,
      "position_embedding_type": "absolute",
      "transformers_version": "4.11.3",
      "type_vocab_size": 2,
      "use_cache": true,
      "vocab_size": 30522
    }</code></pre>
<hr>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> XLMRobertaForTokenClassification(RobertaPreTrainedModel):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the standard XLM-RoBERTa settings.</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    config_class <span class="op">=</span> XLMRobertaConfig</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(config)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_labels <span class="op">=</span> config.num_labels</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load model body</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set add_pooling_layer to False to get all hidden states in the output</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.roberta <span class="op">=</span> RobertaModel(config, add_pooling_layer<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set up token classification head</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.hidden_dropout_prob)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(config.hidden_size, config.num_labels)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load and initialize weights</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids<span class="op">=</span><span class="va">None</span>, attention_mask<span class="op">=</span><span class="va">None</span>, token_type_ids<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>                labels<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use model body to get encoder representations</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.roberta(input_ids, attention_mask<span class="op">=</span>attention_mask,</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>                               token_type_ids<span class="op">=</span>token_type_ids, <span class="op">**</span>kwargs)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply classifier to encoder representation</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        sequence_output <span class="op">=</span> <span class="va">self</span>.dropout(outputs[<span class="dv">0</span>])</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(sequence_output)</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate losses</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>            loss_fct <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fct(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_labels), labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return model output object</span></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> TokenClassifierOutput(loss<span class="op">=</span>loss, logits<span class="op">=</span>logits, </span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>                                     hidden_states<span class="op">=</span>outputs.hidden_states, </span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>                                     attentions<span class="op">=</span>outputs.attentions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="loading-a-custom-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-a-custom-model">Loading a Custom Model</h3>
<ul>
<li>We need to provide the tags for labeling each entity and mappings to convert between tags and IDs</li>
</ul>
<p><strong>Define the mappings to convert between tags and index IDs</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>index2tag <span class="op">=</span> {idx: tag <span class="cf">for</span> idx, tag <span class="kw">in</span> <span class="bu">enumerate</span>(tags.names)}</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>tag2index <span class="op">=</span> {tag: idx <span class="cf">for</span> idx, tag <span class="kw">in</span> <span class="bu">enumerate</span>(tags.names)}</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>index2tag, tag2index</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ({0: 'O',
      1: 'B-PER',
      2: 'I-PER',
      3: 'B-ORG',
      4: 'I-ORG',
      5: 'B-LOC',
      6: 'I-LOC'},
     {'O': 0,
      'B-PER': 1,
      'I-PER': 2,
      'B-ORG': 3,
      'I-ORG': 4,
      'B-LOC': 5,
      'I-LOC': 6})</code></pre>
<hr>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Override the default parameters stored in XLMRobertaConfig</strong></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>xlmr_config <span class="op">=</span> AutoConfig.from_pretrained(xlmr_model_name, </span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>                                         num_labels<span class="op">=</span>tags.num_classes,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>                                         id2label<span class="op">=</span>index2tag, label2id<span class="op">=</span>tag2index)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>xlmr_config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    XLMRobertaConfig {
      "architectures": [
        "XLMRobertaForMaskedLM"
      ],
      "attention_probs_dropout_prob": 0.1,
      "bos_token_id": 0,
      "classifier_dropout": null,
      "eos_token_id": 2,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "id2label": {
        "0": "O",
        "1": "B-PER",
        "2": "I-PER",
        "3": "B-ORG",
        "4": "I-ORG",
        "5": "B-LOC",
        "6": "I-LOC"
      },
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "label2id": {
        "B-LOC": 5,
        "B-ORG": 3,
        "B-PER": 1,
        "I-LOC": 6,
        "I-ORG": 4,
        "I-PER": 2,
        "O": 0
      },
      "layer_norm_eps": 1e-05,
      "max_position_embeddings": 514,
      "model_type": "xlm-roberta",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "output_past": true,
      "pad_token_id": 1,
      "position_embedding_type": "absolute",
      "transformers_version": "4.11.3",
      "type_vocab_size": 1,
      "use_cache": true,
      "vocab_size": 250002
    }</code></pre>
<hr>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Load a pretrained XLM-RoBERTa model with the custom classification head and configuration parameters</strong></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>xlmr_model <span class="op">=</span> (XLMRobertaForTokenClassification</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>              .from_pretrained(xlmr_model_name, config<span class="op">=</span>xlmr_config)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>              .to(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Encode some sample text</strong></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'Jack Sparrow loves New York!'</code></pre>
<hr>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> xlmr_tokenizer.encode(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([xlmr_tokens, input_ids[<span class="dv">0</span>].numpy()], index<span class="op">=</span>[<span class="st">"Tokens"</span>, <span class="st">"Input IDs"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_Jack
</td>
<td>
_Spar
</td>
<td>
row
</td>
<td>
_love
</td>
<td>
s
</td>
<td>
_New
</td>
<td>
_York
</td>
<td>
!
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Input IDs
</th>
<td>
0
</td>
<td>
21763
</td>
<td>
37456
</td>
<td>
15555
</td>
<td>
5161
</td>
<td>
7
</td>
<td>
2356
</td>
<td>
5753
</td>
<td>
38
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>Test model predictions with the untrained classifier</strong></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> xlmr_model(input_ids.to(device)).logits</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> torch.argmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of tokens in sequence: </span><span class="sc">{</span><span class="bu">len</span>(xlmr_tokens)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of outputs: </span><span class="sc">{</span>outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Number of tokens in sequence: 10
    Shape of outputs: torch.Size([1, 10, 7])</code></pre>
<p><strong>Note:</strong> The logits have the shape <code>[batch_size, num_tokens, num_tags]</code>.</p>
<hr>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> [tags.names[p] <span class="cf">for</span> p <span class="kw">in</span> predictions[<span class="dv">0</span>].cpu().numpy()]</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([xlmr_tokens, preds], index<span class="op">=</span>[<span class="st">"Tokens"</span>, <span class="st">"Tags"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_Jack
</td>
<td>
_Spar
</td>
<td>
row
</td>
<td>
_love
</td>
<td>
s
</td>
<td>
_New
</td>
<td>
_York
</td>
<td>
!
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Tags
</th>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The output is useless as the weights are still randomly initialized.</p>
<hr>
<p><strong>Wrap the prediction steps in a helper function</strong></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tag_text(text, tags, model, tokenizer):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get tokens with special characters</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer(text).tokens()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode the sequence into IDs</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> xlmr_tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids.to(device)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions as distribution over 7 possible classes</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(input_ids)[<span class="dv">0</span>]</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take argmax to get most likely class per token</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> torch.argmax(outputs, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to DataFrame</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> [tags.names[p] <span class="cf">for</span> p <span class="kw">in</span> predictions[<span class="dv">0</span>].cpu().numpy()]</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame([tokens, preds], index<span class="op">=</span>[<span class="st">"Tokens"</span>, <span class="st">"Tags"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="tokenizing-texts-for-ner" class="level2">
<h2 class="anchored" data-anchor-id="tokenizing-texts-for-ner">Tokenizing Texts for NER</h2>
<p><strong>Collect the words and tags as ordinary lists</strong></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>de_example <span class="op">=</span> panx_de[<span class="st">'train'</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>words, labels <span class="op">=</span> de_example[<span class="st">"tokens"</span>], de_example[<span class="st">"ner_tags"</span>]</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([words,labels], index<span class="op">=</span>[<span class="st">"words"</span>, <span class="st">"labels"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
words
</th>
<td>
2.000
</td>
<td>
Einwohnern
</td>
<td>
an
</td>
<td>
der
</td>
<td>
Danziger
</td>
<td>
Bucht
</td>
<td>
in
</td>
<td>
der
</td>
<td>
polnischen
</td>
<td>
Woiwodschaft
</td>
<td>
Pommern
</td>
<td>
.
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
5
</td>
<td>
6
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>Tokenize each word</strong></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>tokenized_input <span class="op">=</span> xlmr_tokenizer(de_example[<span class="st">"tokens"</span>], is_split_into_words<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenized_input.values(), index<span class="op">=</span>tokenized_input.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
input_ids
</th>
<td>
0
</td>
<td>
70101
</td>
<td>
176581
</td>
<td>
19
</td>
<td>
142
</td>
<td>
122
</td>
<td>
2290
</td>
<td>
708
</td>
<td>
1505
</td>
<td>
18363
</td>
<td>
18
</td>
<td>
23
</td>
<td>
122
</td>
<td>
127474
</td>
<td>
15439
</td>
<td>
13787
</td>
<td>
14
</td>
<td>
15263
</td>
<td>
18917
</td>
<td>
663
</td>
<td>
6947
</td>
<td>
19
</td>
<td>
6
</td>
<td>
5
</td>
<td>
2
</td>
</tr>
<tr>
<th>
attention_mask
</th>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The <code>is_split_into_words</code> argument tells the tokenizer the input sequence is a list of separated words.</p>
<hr>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[<span class="st">"input_ids"</span>])</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokens, columns<span class="op">=</span>[<span class="st">"tokens"</span>]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_2.000
</td>
<td>
_Einwohner
</td>
<td>
n
</td>
<td>
_an
</td>
<td>
_der
</td>
<td>
_Dan
</td>
<td>
zi
</td>
<td>
ger
</td>
<td>
_Buch
</td>
<td>
t
</td>
<td>
_in
</td>
<td>
_der
</td>
<td>
_polni
</td>
<td>
schen
</td>
<td>
_Wo
</td>
<td>
i
</td>
<td>
wod
</td>
<td>
schaft
</td>
<td>
_Po
</td>
<td>
mmer
</td>
<td>
n
</td>
<td>
_
</td>
<td>
.
</td>
<td>
&lt;/s&gt;
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> We can use the <code>word_ids()</code> function to mask the subword representations after the first subword.</p>
<hr>
<section id="batchencoding.word_ids.word_ids" class="level4">
<h4 class="anchored" data-anchor-id="batchencoding.word_ids.word_ids"><code>BatchEncoding.word_ids.word_ids</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding.word_ids">Documentation</a></li>
<li>Get a list indicating the word corresponding to each token.</li>
</ul>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>word_ids <span class="op">=</span> tokenized_input.word_ids()</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([tokens, word_ids], index<span class="op">=</span>[<span class="st">"Tokens"</span>, <span class="st">"Word IDs"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_2.000
</td>
<td>
_Einwohner
</td>
<td>
n
</td>
<td>
_an
</td>
<td>
_der
</td>
<td>
_Dan
</td>
<td>
zi
</td>
<td>
ger
</td>
<td>
_Buch
</td>
<td>
t
</td>
<td>
_in
</td>
<td>
_der
</td>
<td>
_polni
</td>
<td>
schen
</td>
<td>
_Wo
</td>
<td>
i
</td>
<td>
wod
</td>
<td>
schaft
</td>
<td>
_Po
</td>
<td>
mmer
</td>
<td>
n
</td>
<td>
_
</td>
<td>
.
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Word IDs
</th>
<td>
None
</td>
<td>
0
</td>
<td>
1
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
5
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
<td>
8
</td>
<td>
8
</td>
<td>
9
</td>
<td>
9
</td>
<td>
9
</td>
<td>
9
</td>
<td>
10
</td>
<td>
10
</td>
<td>
10
</td>
<td>
11
</td>
<td>
11
</td>
<td>
None
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> tokens map to <code>None</code> as they are not words from the original text.</p>
<hr>
<p><strong>Set -100 as the label for the start and end tokens and masked subwords</strong></p>
<ul>
<li>The PyTorch cross-entropy loss class has an attribute called <code>ignore_index</code> whose value is -100.</li>
</ul>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>previous_word_idx <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>label_ids <span class="op">=</span> []</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word_idx <span class="kw">in</span> word_ids:</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word_idx <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> word_idx <span class="op">==</span> previous_word_idx:</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        label_ids.append(<span class="op">-</span><span class="dv">100</span>)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> word_idx <span class="op">!=</span> previous_word_idx:</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        label_ids.append(labels[word_idx])</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    previous_word_idx <span class="op">=</span> word_idx</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [index2tag[l] <span class="cf">if</span> l <span class="op">!=</span> <span class="op">-</span><span class="dv">100</span> <span class="cf">else</span> <span class="st">"IGN"</span> <span class="cf">for</span> l <span class="kw">in</span> label_ids]</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> [<span class="st">"Tokens"</span>, <span class="st">"Word IDs"</span>, <span class="st">"Label IDs"</span>, <span class="st">"Labels"</span>]</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([tokens, word_ids, label_ids, labels], index<span class="op">=</span>index)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
<th>
19
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_2.000
</td>
<td>
_Einwohner
</td>
<td>
n
</td>
<td>
_an
</td>
<td>
_der
</td>
<td>
_Dan
</td>
<td>
zi
</td>
<td>
ger
</td>
<td>
_Buch
</td>
<td>
t
</td>
<td>
_in
</td>
<td>
_der
</td>
<td>
_polni
</td>
<td>
schen
</td>
<td>
_Wo
</td>
<td>
i
</td>
<td>
wod
</td>
<td>
schaft
</td>
<td>
_Po
</td>
<td>
mmer
</td>
<td>
n
</td>
<td>
_
</td>
<td>
.
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Word IDs
</th>
<td>
None
</td>
<td>
0
</td>
<td>
1
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
5
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
<td>
8
</td>
<td>
8
</td>
<td>
9
</td>
<td>
9
</td>
<td>
9
</td>
<td>
9
</td>
<td>
10
</td>
<td>
10
</td>
<td>
10
</td>
<td>
11
</td>
<td>
11
</td>
<td>
None
</td>
</tr>
<tr>
<th>
Label IDs
</th>
<td>
-100
</td>
<td>
0
</td>
<td>
0
</td>
<td>
-100
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
6
</td>
<td>
-100
</td>
<td>
0
</td>
<td>
0
</td>
<td>
5
</td>
<td>
-100
</td>
<td>
5
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
6
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
0
</td>
<td>
-100
</td>
<td>
-100
</td>
</tr>
<tr>
<th>
Labels
</th>
<td>
IGN
</td>
<td>
O
</td>
<td>
O
</td>
<td>
IGN
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
I-LOC
</td>
<td>
IGN
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
IGN
</td>
<td>
B-LOC
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
I-LOC
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
O
</td>
<td>
IGN
</td>
<td>
IGN
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>Wrap the tokenization and label alignment steps into a single function</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_and_align_labels(examples):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    tokenized_inputs <span class="op">=</span> xlmr_tokenizer(examples[<span class="st">"tokens"</span>], truncation<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>                                      is_split_into_words<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, label <span class="kw">in</span> <span class="bu">enumerate</span>(examples[<span class="st">"ner_tags"</span>]):</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>        word_ids <span class="op">=</span> tokenized_inputs.word_ids(batch_index<span class="op">=</span>idx)</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>        previous_word_idx <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        label_ids <span class="op">=</span> []</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word_idx <span class="kw">in</span> word_ids:</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word_idx <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> word_idx <span class="op">==</span> previous_word_idx:</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>                label_ids.append(<span class="op">-</span><span class="dv">100</span>)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>                label_ids.append(label[word_idx])</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>            previous_word_idx <span class="op">=</span> word_idx</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>        labels.append(label_ids)</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>    tokenized_inputs[<span class="st">"labels"</span>] <span class="op">=</span> labels</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a mapping function to encode the dataset</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_panx_dataset(corpus):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> corpus.<span class="bu">map</span>(tokenize_and_align_labels, batched<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>                      remove_columns<span class="op">=</span>[<span class="st">'langs'</span>, <span class="st">'ner_tags'</span>, <span class="st">'tokens'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>panx_ch[<span class="st">"de"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        validation: Dataset({
            features: ['tokens', 'ner_tags', 'langs'],
            num_rows: 6290
        })
        test: Dataset({
            features: ['tokens', 'ner_tags', 'langs'],
            num_rows: 6290
        })
        train: Dataset({
            features: ['tokens', 'ner_tags', 'langs'],
            num_rows: 12580
        })
    })</code></pre>
<hr>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>panx_de_encoded <span class="op">=</span> encode_panx_dataset(panx_ch[<span class="st">"de"</span>])</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>panx_de_encoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        validation: Dataset({
            features: ['attention_mask', 'input_ids', 'labels'],
            num_rows: 6290
        })
        test: Dataset({
            features: ['attention_mask', 'input_ids', 'labels'],
            num_rows: 6290
        })
        train: Dataset({
            features: ['attention_mask', 'input_ids', 'labels'],
            num_rows: 12580
        })
    })</code></pre>
<hr>
</section>
</section>
<section id="performance-measures" class="level2">
<h2 class="anchored" data-anchor-id="performance-measures">Performance Measures</h2>
<ul>
<li>Standard performance measures for NER tasks include <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">precision</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">recall</a>, and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1-score</a>.</li>
<li>The model needs to correctly predict all words of an entity for a prediction to count as correct.</li>
</ul>
<section id="seqval" class="level3">
<h3 class="anchored" data-anchor-id="seqval">seqval</h3>
<ul>
<li><a href="https://github.com/chakki-works/seqeval">GitHub Repository</a></li>
<li>A Python framework for sequence labeling evaluation</li>
</ul>
<hr>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="classification_report" class="level4">
<h4 class="anchored" data-anchor-id="classification_report"><code>classification_report</code></h4>
<ul>
<li><a href="https://github.com/chakki-works/seqeval/blob/2921931184a98aff0dbbda5ff943214fe50a7847/seqeval/metrics/sequence_labeling.py#L613">Source Code</a></li>
<li>Build a text report showing the main classification metrics for a sequence of targets and predictions.</li>
<li>The function expects targets and predictions as lists of lists.</li>
</ul>
<hr>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [[<span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-MISC"</span>, <span class="st">"I-MISC"</span>, <span class="st">"I-MISC"</span>, <span class="st">"O"</span>],[<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>]]</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [[<span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-MISC"</span>, <span class="st">"I-MISC"</span>, <span class="st">"I-MISC"</span>, <span class="st">"I-MISC"</span>, <span class="st">"O"</span>],[<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>]]</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>                  precision    recall  f1-score   support
    
            MISC       0.00      0.00      0.00         1
             PER       1.00      1.00      1.00         1
    
       micro avg       0.50      0.50      0.50         2
       macro avg       0.50      0.50      0.50         2
    weighted avg       0.50      0.50      0.50         2</code></pre>
<hr>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Format predictions and target labels for seqval</strong></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> align_predictions(predictions, label_ids):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len <span class="op">=</span> preds.shape</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    labels_list, preds_list <span class="op">=</span> [], []</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>        example_labels, example_preds <span class="op">=</span> [], []</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> seq_idx <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Ignore label IDs = -100</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> label_ids[batch_idx, seq_idx] <span class="op">!=</span> <span class="op">-</span><span class="dv">100</span>:</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>                example_preds.append(index2tag[preds[batch_idx][seq_idx]])</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>        labels_list.append(example_labels)</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>        preds_list.append(example_preds)</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> preds_list, labels_list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
</section>
<section id="fine-tuning-xlm-roberta" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-xlm-roberta">Fine-Tuning XLM-RoBERTa</h2>
<p><strong>Define training attributes</strong></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>logging_steps <span class="op">=</span> <span class="bu">len</span>(panx_de_encoded[<span class="st">"train"</span>]) <span class="op">//</span> batch_size</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>xlmr_model_name<span class="sc">}</span><span class="ss">-finetuned-panx-de"</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>model_name, log_level<span class="op">=</span><span class="st">"error"</span>, num_train_epochs<span class="op">=</span>num_epochs, </span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>batch_size, </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>batch_size, evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>, </span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="fl">1e6</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>, disable_tqdm<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span>logging_steps, push_to_hub<span class="op">=</span><span class="va">True</span>, fp16<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Set <code>save_steps</code> to a large number to disable checkpointing.</p>
<hr>
<p><strong>Log into Hugging Face account</strong></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>notebook_login()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
    To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
    Login successful
    Your token has been saved to /home/innom-dt/.huggingface/token
    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
    To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
<hr>
<p><strong>Compute the <span class="math inline">\(f_{1}\)</span>-score on the validation set</strong></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> f1_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(eval_pred):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    y_pred, y_true <span class="op">=</span> align_predictions(eval_pred.predictions, </span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>                                       eval_pred.label_ids)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"f1"</span>: f1_score(y_true, y_pred)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a collator to pad each input sequence to the highest sequence length in a batch</strong></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DataCollatorForTokenClassification</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="datacollatorfortokenclassification" class="level4">
<h4 class="anchored" data-anchor-id="datacollatorfortokenclassification"><code>DataCollatorForTokenClassification</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForTokenClassification">Documentation</a></li>
<li>Create a data collator that will dynamically pad inputs and labels.</li>
</ul>
<hr>
<div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>print_source(DataCollatorForTokenClassification.torch_call)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    def torch_call(self, features):
        label_name = 'label' if 'label' in features[0].keys() else 'labels'
        labels = [feature[label_name] for feature in features
            ] if label_name in features[0].keys() else None
        batch = self.tokenizer.pad(features, padding=self.padding, max_length=
            self.max_length, pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors='pt' if labels is None else None)
        if labels is None:
            return batch
        sequence_length = torch.tensor(batch['input_ids']).shape[1]
        padding_side = self.tokenizer.padding_side
        if padding_side == 'right':
            batch[label_name] = [(list(label) + [self.label_pad_token_id] * (
                sequence_length - len(label))) for label in labels]
        else:
            batch[label_name] = [([self.label_pad_token_id] * (sequence_length -
                len(label)) + list(label)) for label in labels]
        batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}
        return batch</code></pre>
<hr>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>DataCollatorForTokenClassification.label_pad_token_id</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    -100</code></pre>
<p><strong>Note:</strong> * We need to pad the labels as they are also sequences. * The collator pads label sequences with the value -100, so the PyTorch loss function ignores them.</p>
<hr>
<div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForTokenClassification(xlmr_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a helper function to initialize a new model for a training session</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_init():</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (XLMRobertaForTokenClassification</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>            .from_pretrained(xlmr_model_name, config<span class="op">=</span>xlmr_config)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>            .to(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Disable Tokenizers Parallelism</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env TOKENIZERS_PARALLELISM<span class="op">=</span>false</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    env: TOKENIZERS_PARALLELISM=false</code></pre>
<hr>
<p><strong>Initialize the Trainer object</strong></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_init<span class="op">=</span>model_init, args<span class="op">=</span>training_args, </span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>                  data_collator<span class="op">=</span>data_collator, compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>                  train_dataset<span class="op">=</span>panx_de_encoded[<span class="st">"train"</span>],</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>                  eval_dataset<span class="op">=</span>panx_de_encoded[<span class="st">"validation"</span>], </span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>                  tokenizer<span class="op">=</span>xlmr_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Run the training loop and push the final model to the Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"Training completed!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


0.326400


0.162317


0.813909




2


0.136000


0.133068


0.845137




3


0.096000


0.131872


0.857581




</div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-de/commit/1ebdc3c9051a980588be5a495ad96896f330932c'</code></pre>
<hr>
<p><strong>How to manually display the training log</strong></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>trainer.state.log_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    [{'loss': 0.3264,
      'learning_rate': 3.3671742808798645e-05,
      'epoch': 0.99,
      'step': 196},
     {'eval_loss': 0.1623172014951706,
      'eval_f1': 0.8139089269612262,
      'eval_runtime': 7.0145,
      'eval_samples_per_second': 896.714,
      'eval_steps_per_second': 14.114,
      'epoch': 1.0,
      'step': 197},
     {'loss': 0.136,
      'learning_rate': 1.7174280879864637e-05,
      'epoch': 1.99,
      'step': 392},
     {'eval_loss': 0.1330675333738327,
      'eval_f1': 0.8451372416130125,
      'eval_runtime': 6.8702,
      'eval_samples_per_second': 915.543,
      'eval_steps_per_second': 14.41,
      'epoch': 2.0,
      'step': 394},
     {'loss': 0.096,
      'learning_rate': 6.76818950930626e-07,
      'epoch': 2.98,
      'step': 588},
     {'eval_loss': 0.13187244534492493,
      'eval_f1': 0.8575809199318569,
      'eval_runtime': 6.8965,
      'eval_samples_per_second': 912.061,
      'eval_steps_per_second': 14.355,
      'epoch': 3.0,
      'step': 591},
     {'train_runtime': 95.0424,
      'train_samples_per_second': 397.086,
      'train_steps_per_second': 6.218,
      'total_flos': 1039360955930616.0,
      'train_loss': 0.18559023183211054,
      'epoch': 3.0,
      'step': 591}]</code></pre>
<hr>
<div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(trainer.state.log_history)[[<span class="st">'epoch'</span>,<span class="st">'loss'</span> ,<span class="st">'eval_loss'</span>, <span class="st">'eval_f1'</span>]]</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.rename(columns<span class="op">=</span>{<span class="st">"epoch"</span>:<span class="st">"Epoch"</span>,<span class="st">"loss"</span>: <span class="st">"Training Loss"</span>, <span class="st">"eval_loss"</span>: <span class="st">"Validation Loss"</span>, <span class="st">"eval_f1"</span>:<span class="st">"F1"</span>})</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Epoch'</span>] <span class="op">=</span> df[<span class="st">"Epoch"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">round</span>(x))</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Training Loss'</span>] <span class="op">=</span> df[<span class="st">"Training Loss"</span>].ffill()</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>df[[<span class="st">'Validation Loss'</span>, <span class="st">'F1'</span>]] <span class="op">=</span> df[[<span class="st">'Validation Loss'</span>, <span class="st">'F1'</span>]].bfill().ffill()</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>df.drop_duplicates()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
F1
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0.3264
</td>
<td>
0.162317
</td>
<td>
0.813909
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2
</td>
<td>
0.1360
</td>
<td>
0.133068
</td>
<td>
0.845137
</td>
</tr>
<tr>
<th>
4
</th>
<td>
3
</td>
<td>
0.0960
</td>
<td>
0.131872
</td>
<td>
0.857581
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>Test the model on some sample text</strong></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>text_de <span class="op">=</span> <span class="st">"Jeff Dean ist ein Informatiker bei Google in Kalifornien"</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>tag_text(text_de, tags, trainer.model, xlmr_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_Jeff
</td>
<td>
_De
</td>
<td>
an
</td>
<td>
_ist
</td>
<td>
_ein
</td>
<td>
_Informati
</td>
<td>
ker
</td>
<td>
_bei
</td>
<td>
_Google
</td>
<td>
_in
</td>
<td>
_Kaliforni
</td>
<td>
en
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Tags
</th>
<td>
O
</td>
<td>
B-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-ORG
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
I-LOC
</td>
<td>
O
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The fine-tuned model correctly identifies the entities in the sample text.</p>
<hr>
</section>
</section>
<section id="error-analysis" class="level2">
<h2 class="anchored" data-anchor-id="error-analysis">Error Analysis</h2>
<ul>
<li>Error analysis is an effective tool to understand a model’s strengths and weaknesses.</li>
<li>Looking at the errors can yield helpful insights and reveal bugs that would be hard to spot by only looking at the code.</li>
<li>There are several failure modes where a model might appear to perform well but have serious flaws.</li>
</ul>
<section id="failure-modes" class="level3">
<h3 class="anchored" data-anchor-id="failure-modes">Failure Modes</h3>
<ul>
<li>We might accidentally mask too many tokens and some labels, resulting in a promising loss drop.</li>
<li>The metrics function might have a bug.</li>
<li>We might include the zero class, skewing the accuracy and <span class="math inline">\(F_{1}\)</span>-score.</li>
</ul>
<hr>
<p><strong>Define a function that returns the loss and predicted labels for a single batch</strong></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> cross_entropy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass_with_label(batch):</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert dict of lists to list of dicts suitable for data collator</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> [<span class="bu">dict</span>(<span class="bu">zip</span>(batch, t)) <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">zip</span>(<span class="op">*</span>batch.values())]</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pad inputs and labels and put all tensors on device</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> data_collator(features)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> batch[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> batch[<span class="st">"attention_mask"</span>].to(device)</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> batch[<span class="st">"labels"</span>].to(device)</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass data through model  </span></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> trainer.model(input_ids, attention_mask)</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Logit.size: [batch_size, sequence_length, classes]</span></span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict class with largest logit value on classes axis</span></span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>        predicted_label <span class="op">=</span> torch.argmax(output.logits, axis<span class="op">=-</span><span class="dv">1</span>).cpu().numpy()</span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate loss per token after flattening batch dimension with view</span></span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> cross_entropy(output.logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">7</span>), </span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a>                         labels.view(<span class="op">-</span><span class="dv">1</span>), reduction<span class="op">=</span><span class="st">"none"</span>)</span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unflatten batch dimension and convert to numpy array</span></span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss.view(<span class="bu">len</span>(input_ids), <span class="op">-</span><span class="dv">1</span>).cpu().numpy()</span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-21"><a href="#cb101-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"loss"</span>:loss, <span class="st">"predicted_label"</span>: predicted_label}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Get the loss and predictions for the validation set</strong></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>valid_set <span class="op">=</span> panx_de_encoded[<span class="st">"validation"</span>]</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>valid_set <span class="op">=</span> valid_set.<span class="bu">map</span>(forward_pass_with_label, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> valid_set.to_pandas()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>index2tag[<span class="op">-</span><span class="dv">100</span>] <span class="op">=</span> <span class="st">"IGN"</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Map IDs to tokens</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"input_tokens"</span>] <span class="op">=</span> df[<span class="st">"input_ids"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: xlmr_tokenizer.convert_ids_to_tokens(x))</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Map predicted label index to tag</span></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"predicted_label"</span>] <span class="op">=</span> df[<span class="st">"predicted_label"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: [index2tag[i] <span class="cf">for</span> i <span class="kw">in</span> x])</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Map target label index to tag</span></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"labels"</span>] <span class="op">=</span> df[<span class="st">"labels"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: [index2tag[i] <span class="cf">for</span> i <span class="kw">in</span> x])</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove padding for the loss field</span></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'loss'</span>] <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'loss'</span>][:<span class="bu">len</span>(x[<span class="st">'input_ids'</span>])], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove padding for the predicted label field</span></span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'predicted_label'</span>] <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'predicted_label'</span>][:<span class="bu">len</span>(x[<span class="st">'input_ids'</span>])], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">1</span>).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
attention_mask
</th>
<td>
[1, 1, 1, 1, 1, 1, 1]
</td>
</tr>
<tr>
<th>
input_ids
</th>
<td>
[0, 10699, 11, 15, 16104, 1388, 2]
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]
</td>
</tr>
<tr>
<th>
loss
</th>
<td>
[0.0, 0.03210718, 0.0, 0.05737416, 0.0494957, 0.062034503, 0.0]
</td>
</tr>
<tr>
<th>
predicted_label
</th>
<td>
[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]
</td>
</tr>
<tr>
<th>
input_tokens
</th>
<td>
[&lt;s&gt;, <em>Ham, a, </em>(, <em>Unternehmen, </em>), &lt;/s&gt;]
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform each element of a list-like to a row</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>df_tokens <span class="op">=</span> df.<span class="bu">apply</span>(pd.Series.explode)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the tokens labeled with 'IGN'</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>df_tokens <span class="op">=</span> df_tokens.query(<span class="st">"labels != 'IGN'"</span>)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Round loss values to two decimal places</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>df_tokens[<span class="st">"loss"</span>] <span class="op">=</span> df_tokens[<span class="st">"loss"</span>].astype(<span class="bu">float</span>).<span class="bu">round</span>(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>df_tokens.head(<span class="dv">7</span>).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table id="T_65e8d">
<thead>
</thead>
<tbody>
<tr>
<th id="T_65e8d_level0_row0" class="row_heading level0 row0">
attention_mask
</th>
<td id="T_65e8d_row0_col0" class="data row0 col0">
1
</td>
<td id="T_65e8d_row0_col1" class="data row0 col1">
1
</td>
<td id="T_65e8d_row0_col2" class="data row0 col2">
1
</td>
<td id="T_65e8d_row0_col3" class="data row0 col3">
1
</td>
<td id="T_65e8d_row0_col4" class="data row0 col4">
1
</td>
<td id="T_65e8d_row0_col5" class="data row0 col5">
1
</td>
<td id="T_65e8d_row0_col6" class="data row0 col6">
1
</td>
</tr>
<tr>
<th id="T_65e8d_level0_row1" class="row_heading level0 row1">
input_ids
</th>
<td id="T_65e8d_row1_col0" class="data row1 col0">
10699
</td>
<td id="T_65e8d_row1_col1" class="data row1 col1">
15
</td>
<td id="T_65e8d_row1_col2" class="data row1 col2">
16104
</td>
<td id="T_65e8d_row1_col3" class="data row1 col3">
1388
</td>
<td id="T_65e8d_row1_col4" class="data row1 col4">
56530
</td>
<td id="T_65e8d_row1_col5" class="data row1 col5">
83982
</td>
<td id="T_65e8d_row1_col6" class="data row1 col6">
10
</td>
</tr>
<tr>
<th id="T_65e8d_level0_row2" class="row_heading level0 row2">
labels
</th>
<td id="T_65e8d_row2_col0" class="data row2 col0">
B-ORG
</td>
<td id="T_65e8d_row2_col1" class="data row2 col1">
I-ORG
</td>
<td id="T_65e8d_row2_col2" class="data row2 col2">
I-ORG
</td>
<td id="T_65e8d_row2_col3" class="data row2 col3">
I-ORG
</td>
<td id="T_65e8d_row2_col4" class="data row2 col4">
O
</td>
<td id="T_65e8d_row2_col5" class="data row2 col5">
B-ORG
</td>
<td id="T_65e8d_row2_col6" class="data row2 col6">
I-ORG
</td>
</tr>
<tr>
<th id="T_65e8d_level0_row3" class="row_heading level0 row3">
loss
</th>
<td id="T_65e8d_row3_col0" class="data row3 col0">
0.030000
</td>
<td id="T_65e8d_row3_col1" class="data row3 col1">
0.060000
</td>
<td id="T_65e8d_row3_col2" class="data row3 col2">
0.050000
</td>
<td id="T_65e8d_row3_col3" class="data row3 col3">
0.060000
</td>
<td id="T_65e8d_row3_col4" class="data row3 col4">
0.000000
</td>
<td id="T_65e8d_row3_col5" class="data row3 col5">
0.600000
</td>
<td id="T_65e8d_row3_col6" class="data row3 col6">
0.380000
</td>
</tr>
<tr>
<th id="T_65e8d_level0_row4" class="row_heading level0 row4">
predicted_label
</th>
<td id="T_65e8d_row4_col0" class="data row4 col0">
B-ORG
</td>
<td id="T_65e8d_row4_col1" class="data row4 col1">
I-ORG
</td>
<td id="T_65e8d_row4_col2" class="data row4 col2">
I-ORG
</td>
<td id="T_65e8d_row4_col3" class="data row4 col3">
I-ORG
</td>
<td id="T_65e8d_row4_col4" class="data row4 col4">
O
</td>
<td id="T_65e8d_row4_col5" class="data row4 col5">
B-ORG
</td>
<td id="T_65e8d_row4_col6" class="data row4 col6">
I-ORG
</td>
</tr>
<tr>
<th id="T_65e8d_level0_row5" class="row_heading level0 row5">
input_tokens
</th>
<td id="T_65e8d_row5_col0" class="data row5 col0">
_Ham
</td>
<td id="T_65e8d_row5_col1" class="data row5 col1">
_(
</td>
<td id="T_65e8d_row5_col2" class="data row5 col2">
_Unternehmen
</td>
<td id="T_65e8d_row5_col3" class="data row5 col3">
_)
</td>
<td id="T_65e8d_row5_col4" class="data row5 col4">
_WE
</td>
<td id="T_65e8d_row5_col5" class="data row5 col5">
_Luz
</td>
<td id="T_65e8d_row5_col6" class="data row5 col6">
_a
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Group data by the input tokens</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>    df_tokens.groupby(<span class="st">"input_tokens"</span>)[[<span class="st">"loss"</span>]]</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aggregate the losses for each token</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>    .agg([<span class="st">"count"</span>, <span class="st">"mean"</span>, <span class="st">"sum"</span>])</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get rid of multi-level columns</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>    .droplevel(level<span class="op">=</span><span class="dv">0</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort values with the highest losses first</span></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>    .sort_values(by<span class="op">=</span><span class="st">"sum"</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    .reset_index()</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">10</span>)</span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>    .T</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
input_tokens
</th>
<td>
_
</td>
<td>
_in
</td>
<td>
_von
</td>
<td>
_der
</td>
<td>
_/
</td>
<td>
_und
</td>
<td>
_(
</td>
<td>
_)
</td>
<td>
_’’
</td>
<td>
_A
</td>
</tr>
<tr>
<th>
count
</th>
<td>
6066
</td>
<td>
989
</td>
<td>
808
</td>
<td>
1388
</td>
<td>
163
</td>
<td>
1171
</td>
<td>
246
</td>
<td>
246
</td>
<td>
2898
</td>
<td>
125
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
0.03
</td>
<td>
0.11
</td>
<td>
0.14
</td>
<td>
0.07
</td>
<td>
0.51
</td>
<td>
0.07
</td>
<td>
0.28
</td>
<td>
0.27
</td>
<td>
0.02
</td>
<td>
0.47
</td>
</tr>
<tr>
<th>
sum
</th>
<td>
187.46
</td>
<td>
110.59
</td>
<td>
110.46
</td>
<td>
100.7
</td>
<td>
83.81
</td>
<td>
83.13
</td>
<td>
69.48
</td>
<td>
67.49
</td>
<td>
59.03
</td>
<td>
58.63
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The whitespace token has the highest total loss since it is the most common token.</li>
<li>The whitespace token has a low mean loss, indicating the model does not struggle to classify it.</li>
<li>Words like “in,” “von,” “der,” and “und” often appear together with named entities and are sometimes part of them.</li>
<li>It is rare to have parentheses, slashes, and capital letters at the beginning of words, but those have a relatively high average loss.</li>
</ul>
<hr>
<div class="sourceCode" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Group data by the label IDs</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    df_tokens.groupby(<span class="st">"labels"</span>)[[<span class="st">"loss"</span>]] </span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>    .agg([<span class="st">"count"</span>, <span class="st">"mean"</span>, <span class="st">"sum"</span>])</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>    .droplevel(level<span class="op">=</span><span class="dv">0</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>    .sort_values(by<span class="op">=</span><span class="st">"mean"</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>    .reset_index()</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>    .T</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
labels
</th>
<td>
B-ORG
</td>
<td>
I-LOC
</td>
<td>
I-ORG
</td>
<td>
B-LOC
</td>
<td>
B-PER
</td>
<td>
I-PER
</td>
<td>
O
</td>
</tr>
<tr>
<th>
count
</th>
<td>
2683
</td>
<td>
1462
</td>
<td>
3820
</td>
<td>
3172
</td>
<td>
2893
</td>
<td>
4139
</td>
<td>
43648
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
0.59
</td>
<td>
0.59
</td>
<td>
0.42
</td>
<td>
0.34
</td>
<td>
0.3
</td>
<td>
0.18
</td>
<td>
0.03
</td>
</tr>
<tr>
<th>
sum
</th>
<td>
1582.79
</td>
<td>
857.5
</td>
<td>
1598.29
</td>
<td>
1073.82
</td>
<td>
861.09
</td>
<td>
727.88
</td>
<td>
1419.61
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> B-ORG has the highest average loss, meaning the model struggles to find the beginning of organization entities.</p>
<hr>
<p><strong>Plot a confusion matrix of the token classification</strong></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay, confusion_matrix</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_confusion_matrix(y_preds, y_true, labels):</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_true, y_preds, normalize<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>labels)</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>    disp.plot(cmap<span class="op">=</span><span class="st">"Blues"</span>, values_format<span class="op">=</span><span class="st">".2f"</span>, ax<span class="op">=</span>ax, colorbar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Normalized confusion matrix"</span>)</span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(df_tokens[<span class="st">"labels"</span>], df_tokens[<span class="st">"predicted_label"</span>],</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>                      tags.names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_155_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The model often confuses the beginning subword (<code>B-ORG</code>) of an organizational entity with the subsequent subwords (<code>I-ORG</code>).</p>
<hr>
<p><strong>Examine token sequences with high losses</strong></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_samples(df):</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over DataFrame rows</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>        labels, preds, tokens, losses <span class="op">=</span> [], [], [], []</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, mask <span class="kw">in</span> <span class="bu">enumerate</span>(row[<span class="st">"attention_mask"</span>]):</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> {<span class="dv">0</span>, <span class="bu">len</span>(row[<span class="st">"attention_mask"</span>])}:</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>                labels.append(row[<span class="st">"labels"</span>][i])</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>                preds.append(row[<span class="st">"predicted_label"</span>][i])</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>                tokens.append(row[<span class="st">"input_tokens"</span>][i])</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>                losses.append(<span class="ss">f"</span><span class="sc">{</span>row[<span class="st">'loss'</span>][i]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>        df_tmp <span class="op">=</span> pd.DataFrame({<span class="st">"tokens"</span>: tokens, <span class="st">"labels"</span>: labels, </span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>                               <span class="st">"preds"</span>: preds, <span class="st">"losses"</span>: losses}).T</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> df_tmp</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"total_loss"</span>] <span class="op">=</span> df[<span class="st">"loss"</span>].<span class="bu">apply</span>(<span class="bu">sum</span>)</span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>df_tmp <span class="op">=</span> df.sort_values(by<span class="op">=</span><span class="st">"total_loss"</span>, ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">3</span>)</span>
<span id="cb112-17"><a href="#cb112-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-18"><a href="#cb112-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample <span class="kw">in</span> get_samples(df_tmp):</span>
<span id="cb112-19"><a href="#cb112-19" aria-hidden="true" tabindex="-1"></a>    display(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
<th>
18
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
_’
</td>
<td>
_’’
</td>
<td>
_Τ
</td>
<td>
Κ
</td>
<td>
_’’
</td>
<td>
_’
</td>
<td>
_’
</td>
<td>
_’’
</td>
<td>
_T
</td>
<td>
_’’
</td>
<td>
_’
</td>
<td>
ri
</td>
<td>
_’’
</td>
<td>
_’
</td>
<td>
k
</td>
<td>
_’’
</td>
<td>
_’
</td>
<td>
ala
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
IGN
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
I-LOC
</td>
<td>
I-LOC
</td>
<td>
I-LOC
</td>
<td>
I-LOC
</td>
<td>
IGN
</td>
<td>
I-LOC
</td>
<td>
I-LOC
</td>
<td>
IGN
</td>
<td>
I-LOC
</td>
<td>
I-LOC
</td>
<td>
IGN
</td>
<td>
IGN
</td>
</tr>
<tr>
<th>
preds
</th>
<td>
O
</td>
<td>
O
</td>
<td>
B-ORG
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
</tr>
<tr>
<th>
losses
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
2.42
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
9.83
</td>
<td>
9.15
</td>
<td>
7.60
</td>
<td>
6.55
</td>
<td>
6.66
</td>
<td>
0.00
</td>
<td>
5.83
</td>
<td>
6.83
</td>
<td>
0.00
</td>
<td>
7.26
</td>
<td>
7.44
</td>
<td>
0.00
</td>
<td>
0.00
</td>
</tr>
</tbody>
</table>
</div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
<th>
15
</th>
<th>
16
</th>
<th>
17
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
_’’
</td>
<td>
8
</td>
<td>
.
</td>
<td>
_Juli
</td>
<td>
_’’
</td>
<td>
_:
</td>
<td>
_Protest
</td>
<td>
camp
</td>
<td>
_auf
</td>
<td>
_dem
</td>
<td>
_Gelände
</td>
<td>
_der
</td>
<td>
_Republika
</td>
<td>
n
</td>
<td>
ischen
</td>
<td>
_Gar
</td>
<td>
de
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
B-ORG
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
IGN
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
IGN
</td>
<td>
IGN
</td>
<td>
I-ORG
</td>
<td>
IGN
</td>
<td>
IGN
</td>
</tr>
<tr>
<th>
preds
</th>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
O
</td>
</tr>
<tr>
<th>
losses
</th>
<td>
8.37
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
4.67
</td>
<td>
9.00
</td>
<td>
8.87
</td>
<td>
6.17
</td>
<td>
0.00
</td>
<td>
7.98
</td>
<td>
8.33
</td>
<td>
7.00
</td>
<td>
4.32
</td>
<td>
2.61
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.01
</td>
<td>
0.00
</td>
<td>
0.00
</td>
</tr>
</tbody>
</table>
</div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
<th>
14
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokens
</th>
<td>
_United
</td>
<td>
_Nations
</td>
<td>
_Multi
</td>
<td>
dimensional
</td>
<td>
_Integra
</td>
<td>
ted
</td>
<td>
_Stabil
</td>
<td>
ization
</td>
<td>
_Mission
</td>
<td>
_in
</td>
<td>
_the
</td>
<td>
_Central
</td>
<td>
_African
</td>
<td>
_Republic
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
B-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
IGN
</td>
<td>
I-PER
</td>
<td>
IGN
</td>
<td>
I-PER
</td>
<td>
IGN
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
IGN
</td>
</tr>
<tr>
<th>
preds
</th>
<td>
B-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
<td>
I-ORG
</td>
</tr>
<tr>
<th>
losses
</th>
<td>
5.46
</td>
<td>
5.36
</td>
<td>
5.51
</td>
<td>
0.00
</td>
<td>
5.53
</td>
<td>
0.00
</td>
<td>
5.46
</td>
<td>
0.00
</td>
<td>
5.06
</td>
<td>
5.22
</td>
<td>
5.62
</td>
<td>
5.71
</td>
<td>
5.36
</td>
<td>
5.09
</td>
<td>
0.00
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The PAN-X dataset used an imperfect automated process to apply annotations, resulting in some labeling issues.
<ul>
<li>The United Nations and the Central African Republic are organizations, not people.</li>
<li>The date “8. Juli” (July 8th) also has an incorrect label.</li>
</ul></li>
</ul>
<hr>
<p><strong>Examine sequences with an opening parenthesis</strong></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co">u"</span><span class="ch">\u2581</span><span class="co">("</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    '_('</code></pre>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
</colgroup>
<tbody>
<tr class="odd">
<td>```python df_tmp = df.loc[df[“input_tokens”].apply(lambda x: u”581(” in x)].head(2) for sample in get_samples(df_tmp): display(sample) ````</td>
</tr>
<tr class="even">
<td>* We generally don’t include the parentheses and their contents as part of the named entity, but the automated annotation process does. * Some parentheses contain a geographic specification. * We might want to disconnect this information from the original location in the annotations. * The dataset consists of Wikipedia articles in different languages, and the article titles often contain an explanation in parentheses. * We need to know about such characteristics in our datasets when rolling out models to production. * We can use these insights to clean up the dataset and retrain the model.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="cross-lingual-transfer" class="level2">
<h2 class="anchored" data-anchor-id="cross-lingual-transfer">Cross-Lingual Transfer</h2>
<p><strong>Create a helper function to evaluate the model on different datasets</strong></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_f1_score(trainer, dataset):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trainer.predict(dataset).metrics[<span class="st">"test_f1"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Examine the German model’s performance on the German test set</strong></p>
<div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>f1_scores[<span class="st">"de"</span>][<span class="st">"de"</span>] <span class="op">=</span> get_f1_score(trainer, panx_de_encoded[<span class="st">"test"</span>])</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score of [de] model on [de] dataset: </span><span class="sc">{</span>f1_scores[<span class="st">'de'</span>][<span class="st">'de'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    F1-score of [de] model on [de] dataset: 0.859</code></pre>
<p><strong>Test the German model’s performance on French text</strong></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>text_fr <span class="op">=</span> <span class="st">"Jeff Dean est informaticien chez Google en Californie"</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
11
</th>
<th>
12
</th>
<th>
13
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Tokens
</th>
<td>
&lt;s&gt;
</td>
<td>
_Jeff
</td>
<td>
_De
</td>
<td>
an
</td>
<td>
_est
</td>
<td>
_informatic
</td>
<td>
ien
</td>
<td>
_chez
</td>
<td>
_Google
</td>
<td>
_en
</td>
<td>
_Cali
</td>
<td>
for
</td>
<td>
nie
</td>
<td>
&lt;/s&gt;
</td>
</tr>
<tr>
<th>
Tags
</th>
<td>
O
</td>
<td>
B-PER
</td>
<td>
I-PER
</td>
<td>
I-PER
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
O
</td>
<td>
B-ORG
</td>
<td>
O
</td>
<td>
B-LOC
</td>
<td>
B-LOC
</td>
<td>
I-LOC
</td>
<td>
O
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The model correctly labeled the French translation of “Kalifornien” as a location.</p>
<hr>
<p><strong>Define a helper function to encode a dataset and generate a classification report</strong></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_lang_performance(lang, trainer):</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    panx_ds <span class="op">=</span> encode_panx_dataset(panx_ch[lang])</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_f1_score(trainer, panx_ds[<span class="st">"test"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Evaluate the German model’s performance on the French test set</strong></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>f1_scores[<span class="st">"de"</span>][<span class="st">"fr"</span>] <span class="op">=</span> evaluate_lang_performance(<span class="st">"fr"</span>, trainer)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score of [de] model on [fr] dataset: </span><span class="sc">{</span>f1_scores[<span class="st">'de'</span>][<span class="st">'fr'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    F1-score of [de] model on [fr] dataset: 0.708</code></pre>
<p><strong>Note:</strong> The German model still performs relatively well despite not training on a single labeled French example.</p>
<hr>
<p><strong>Evaluate the German model’s performance on the Italian test set</strong></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>f1_scores[<span class="st">"de"</span>][<span class="st">"it"</span>] <span class="op">=</span> evaluate_lang_performance(<span class="st">"it"</span>, trainer)</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score of [de] model on [it] dataset: </span><span class="sc">{</span>f1_scores[<span class="st">'de'</span>][<span class="st">'it'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    F1-score of [de] model on [it] dataset: 0.691</code></pre>
<hr>
<p><strong>Evaluate the German model’s performance on the English test set</strong></p>
<div class="sourceCode" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide_output</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>f1_scores[<span class="st">"de"</span>][<span class="st">"en"</span>] <span class="op">=</span> evaluate_lang_performance(<span class="st">"en"</span>, trainer)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score of [de] model on [en] dataset: </span><span class="sc">{</span>f1_scores[<span class="st">'de'</span>][<span class="st">'en'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    F1-score of [de] model on [en] dataset: 0.596</code></pre>
<p><strong>Note:</strong> The model performs worse on the English dataset despite being closer to German than French.</p>
<hr>
<section id="when-does-zero-shot-transfer-make-sense" class="level3">
<h3 class="anchored" data-anchor-id="when-does-zero-shot-transfer-make-sense">When Does Zero-Shot Transfer Make Sense?</h3>
<ul>
<li>We can determine at which point zero-shot cross-lingual transfer is superior to fine-tuning on a monolingual corpus by fine-tuning the model on training sets of increasing size.</li>
</ul>
<p><strong>Define a function to train a model on a downsampled dataset</strong></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_on_subset(dataset, num_samples):</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Downsample the training set to the target number of samples</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> dataset[<span class="st">"train"</span>].shuffle(seed<span class="op">=</span><span class="dv">42</span>).select(<span class="bu">range</span>(num_samples))</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>    valid_ds <span class="op">=</span> dataset[<span class="st">"validation"</span>]</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>    test_ds <span class="op">=</span> dataset[<span class="st">"test"</span>]</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a>    training_args.logging_steps <span class="op">=</span> <span class="bu">len</span>(train_ds) <span class="op">//</span> batch_size</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Traing the model on the downsampled dataset</span></span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> Trainer(model_init<span class="op">=</span>model_init, args<span class="op">=</span>training_args,</span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a>        data_collator<span class="op">=</span>data_collator, compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a>        train_dataset<span class="op">=</span>train_ds, eval_dataset<span class="op">=</span>valid_ds, tokenizer<span class="op">=</span>xlmr_tokenizer)</span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>    trainer.train()</span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> training_args.push_to_hub:</span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a>        trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"Training completed!"</span>)</span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the performance metrics</span></span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a>    f1_score <span class="op">=</span> get_f1_score(trainer, test_ds)</span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame.from_dict(</span>
<span id="cb126-17"><a href="#cb126-17" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"num_samples"</span>: [<span class="bu">len</span>(train_ds)], <span class="st">"f1_score"</span>: [f1_score]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Encode the French Dataset</strong></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>panx_fr_encoded <span class="op">=</span> encode_panx_dataset(panx_ch[<span class="st">"fr"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the model on 250 French samples</strong></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>training_args.push_to_hub <span class="op">=</span> <span class="va">False</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>metrics_df <span class="op">=</span> train_on_subset(panx_fr_encoded, <span class="dv">250</span>)</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>metrics_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


2.360800


2.210924


0.109819




2


2.192800


1.484458


0.032251




3


1.480200


1.368229


0.008093




</div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
num_samples
</th>
<th>
f1_score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
250
</td>
<td>
0.007832
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The French model significantly underperforms the German model when using only 250 examples.</p>
<hr>
<p><strong>Train the model on an increasing number of French samples</strong></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_samples <span class="kw">in</span> [<span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">4000</span>]:</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>    metrics_df <span class="op">=</span> metrics_df.append(</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>        train_on_subset(panx_fr_encoded, num_samples), ignore_index<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


2.204900


1.488627


0.023411




2


1.465700


1.249257


0.144914




3


1.217400


1.093112


0.161066




</div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


1.800700


1.176858


0.175238




2


1.016500


0.698511


0.578441




3


0.669500


0.540801


0.639231




</div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
F1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
1.413700
</td>
<td>
0.686913
</td>
<td>
0.531559
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.526900
</td>
<td>
0.386696
</td>
<td>
0.741683
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.318900
</td>
<td>
0.352989
</td>
<td>
0.771843
</td>
</tr>
</tbody>
</table>
</div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
F1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.895500
</td>
<td>
0.371288
</td>
<td>
0.757611
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.324200
</td>
<td>
0.327193
</td>
<td>
0.777248
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.243800
</td>
<td>
0.284226
</td>
<td>
0.822527
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>ax.axhline(f1_scores[<span class="st">"de"</span>][<span class="st">"fr"</span>], ls<span class="op">=</span><span class="st">"--"</span>, color<span class="op">=</span><span class="st">"r"</span>)</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>metrics_df.set_index(<span class="st">"num_samples"</span>).plot(ax<span class="op">=</span>ax)</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"Zero-shot from de"</span>, <span class="st">"Fine-tuned on fr"</span>], loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Training Samples"</span>)</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1 Score"</span>)</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_192_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The zero-shot transfer model remains competitive until about 1500 training examples. * Getting domain experts to label hundreds (let alone thousands) of documents can be costly, especially for NER.</p>
<hr>
</section>
<section id="fine-tuning-on-multiple-languages-at-once" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-on-multiple-languages-at-once">Fine-Tuning on Multiple Languages at Once</h3>
<ul>
<li>We can mitigate the performance drop from zero-shot cross-lingual transfer by fine-tuning with multiple languages at once.</li>
</ul>
<hr>
<div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> concatenate_datasets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="concatenate_datasets" class="level4">
<h4 class="anchored" data-anchor-id="concatenate_datasets"><code>concatenate_datasets</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.concatenate_datasets">Documentation</a></li>
<li>Convert a list of Dataset objects with the same schema into a single Dataset.</li>
</ul>
<hr>
<p><strong>Define a function to combine a list of datasets using based on split names</strong></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> concatenate_splits(corpora):</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>    multi_corpus <span class="op">=</span> DatasetDict()</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> split <span class="kw">in</span> corpora[<span class="dv">0</span>].keys():</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a>        multi_corpus[split] <span class="op">=</span> concatenate_datasets(</span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a>            [corpus[split] <span class="cf">for</span> corpus <span class="kw">in</span> corpora]).shuffle(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> multi_corpus</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Combine the German and French datasets</strong></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>panx_de_fr_encoded <span class="op">=</span> concatenate_splits([panx_de_encoded, panx_fr_encoded])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Update training attributes</strong></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>training_args.logging_steps <span class="op">=</span> <span class="bu">len</span>(panx_de_fr_encoded[<span class="st">"train"</span>]) <span class="op">//</span> batch_size</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>training_args.push_to_hub <span class="op">=</span> <span class="va">True</span></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>training_args.output_dir <span class="op">=</span> <span class="st">"xlm-roberta-base-finetuned-panx-de-fr"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the model on the combined dataset</strong></p>
<div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_init<span class="op">=</span>model_init, args<span class="op">=</span>training_args,</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator, compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>xlmr_tokenizer, train_dataset<span class="op">=</span>panx_de_fr_encoded[<span class="st">"train"</span>],</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>panx_de_fr_encoded[<span class="st">"validation"</span>])</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
Epoch
</th>
<th>
Training Loss
</th>
<th>
Validation Loss
</th>
<th>
F1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.371800
</td>
<td>
0.176133
</td>
<td>
0.822272
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.153500
</td>
<td>
0.160763
</td>
<td>
0.840360
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.107400
</td>
<td>
0.157969
</td>
<td>
0.854692
</td>
</tr>
</tbody>
</table>
</div>
<pre><code>TrainOutput(global_step=807, training_loss=0.2103209033368393, metrics={'train_runtime': 129.7072, 'train_samples_per_second': 396.894, 'train_steps_per_second': 6.222, 'total_flos': 1399867154966784.0, 'train_loss': 0.2103209033368393, 'epoch': 3.0})</code></pre>
<hr>
<div class="sourceCode" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"Training completed!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-de-fr/commit/e93b59a0d16dc03a657342fd9bf31413af9aebc1'</code></pre>
<hr>
<div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lang <span class="kw">in</span> langs:</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> evaluate_lang_performance(lang, trainer)</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"F1-score of [de-fr] model on [</span><span class="sc">{</span>lang<span class="sc">}</span><span class="ss">] dataset: </span><span class="sc">{</span>f1<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    F1-score of [de-fr] model on [de] dataset: 0.862
    F1-score of [de-fr] model on [fr] dataset: 0.848
    F1-score of [de-fr] model on [it] dataset: 0.793
    F1-score of [de-fr] model on [en] dataset: 0.688</code></pre>
<p><strong>Note:</strong> The model now performs much better on the French split, and it even improved on the Italian and English sets.</p>
<hr>
<p><strong>Test the performance from fine-tuning on each language separately</strong></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>corpora <span class="op">=</span> [panx_de_encoded]</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Exclude German from iteration</span></span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lang <span class="kw">in</span> langs[<span class="dv">1</span>:]:</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>    training_args.output_dir <span class="op">=</span> <span class="ss">f"xlm-roberta-base-finetuned-panx-</span><span class="sc">{</span>lang<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fine-tune on monolingual corpus</span></span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>    ds_encoded <span class="op">=</span> encode_panx_dataset(panx_ch[lang])</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> train_on_subset(ds_encoded, ds_encoded[<span class="st">"train"</span>].num_rows)</span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Collect F1-scores in common dict</span></span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>    f1_scores[lang][lang] <span class="op">=</span> metrics[<span class="st">"f1_score"</span>][<span class="dv">0</span>]</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add monolingual corpus to list of corpora to concatenate</span></span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a>    corpora.append(ds_encoded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


0.854100


0.352915


0.782609




2


0.306900


0.280733


0.815359




3


0.226200


0.271911


0.829342




</div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


1.454800


0.652213


0.545667




2


0.521400


0.347615


0.740443




3


0.318600


0.292827


0.773021




</div>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


1.711900


1.000937


0.226577




2


0.891000


0.640531


0.528084




3


0.602300


0.508421


0.579369




</div>
<hr>
<p><strong>Test the performance from multilingual learning on all the corpora</strong></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>corpora_encoded <span class="op">=</span> concatenate_splits(corpora)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>training_args.logging_steps <span class="op">=</span> <span class="bu">len</span>(corpora_encoded[<span class="st">"train"</span>]) <span class="op">//</span> batch_size</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>training_args.output_dir <span class="op">=</span> <span class="st">"xlm-roberta-base-finetuned-panx-all"</span></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(model_init<span class="op">=</span>model_init, args<span class="op">=</span>training_args,</span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator, compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>xlmr_tokenizer, train_dataset<span class="op">=</span>corpora_encoded[<span class="st">"train"</span>],</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>corpora_encoded[<span class="st">"validation"</span>])</span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a>trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"Training completed!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Cloning https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-all into local empty directory.</code></pre>
<div style="overflow-x:auto;">
<pre><code>&lt;table border="1" class="dataframe"&gt;</code></pre>



Epoch


Training Loss


Validation Loss


F1






1


0.370100


0.200005


0.805385




2


0.162900


0.168012


0.837781




3


0.115600


0.167403


0.847745




</div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/xlm-roberta-base-finetuned-panx-all/commit/f01950fd63b31959f5c3d520125366485fb375b6'</code></pre>
<hr>
<p><strong>Generate predictions on the test set for each language</strong></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, lang <span class="kw">in</span> <span class="bu">enumerate</span>(langs):</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>    f1_scores[<span class="st">"all"</span>][lang] <span class="op">=</span> get_f1_score(trainer, corpora[idx][<span class="st">"test"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>scores_data <span class="op">=</span> {<span class="st">"de"</span>: f1_scores[<span class="st">"de"</span>],</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>               <span class="st">"each"</span>: {lang: f1_scores[lang][lang] <span class="cf">for</span> lang <span class="kw">in</span> langs},</span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">"all"</span>: f1_scores[<span class="st">"all"</span>]}</span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a>f1_scores_df <span class="op">=</span> pd.DataFrame(scores_data).T.<span class="bu">round</span>(<span class="dv">4</span>)</span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a>f1_scores_df.rename_axis(index<span class="op">=</span><span class="st">"Fine-tune on"</span>, columns<span class="op">=</span><span class="st">"Evaluated on"</span>,</span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a>                         inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a>f1_scores_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
Evaluated on
</th>
<th>
de
</th>
<th>
fr
</th>
<th>
it
</th>
<th>
en
</th>
</tr>
<tr>
<th>
Fine-tune on
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
de
</th>
<td>
0.8590
</td>
<td>
0.7079
</td>
<td>
0.6910
</td>
<td>
0.5962
</td>
</tr>
<tr>
<th>
each
</th>
<td>
0.8590
</td>
<td>
0.8321
</td>
<td>
0.7696
</td>
<td>
0.5962
</td>
</tr>
<tr>
<th>
all
</th>
<td>
0.8592
</td>
<td>
0.8568
</td>
<td>
0.8646
</td>
<td>
0.7678
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>Multilingual learning can provide significant performance gains.</li>
<li>You should generally focus your attention on cross-lingual transfer within language families.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-3/">Notes on Transformers Book Ch. 3</a></p>
<p><strong>Next:</strong> <a href="../chapter-5/">Notes on Transformers Book Ch. 5</a></p>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m Christian Mills, a deep learning consultant specializing in practical AI implementations. I help clients leverage cutting-edge AI technologies to solve real-world problems.</p>
<p>Interested in working together? Fill out my <a href="https://docs.google.com/forms/d/e/1FAIpQLScKDKPJF9Be47LA3nrEDXTVpzH2UMLz8SzHMHM9hWT5qlvjkw/viewform?usp=sf_link">Quick AI Project Assessment</a> form or learn more <a href="../../../about.html">about me</a>.</p>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>