<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.336">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-04-22">
<meta name="description" content="Chapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.">

<title>Christian Mills - Notes on Transformers Book Ch. 9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Notes on Transformers Book Ch. 9">
<meta property="og:description" content="Chapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.">
<meta property="og:image" content="christianjmills.com/posts/transformers-book-notes/chapter-9/images/logo.png">
<meta property="og:site-name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Notes on Transformers Book Ch. 9">
<meta name="twitter:description" content="Chapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.">
<meta name="twitter:image" content="christianjmills.com/posts/transformers-book-notes/chapter-9/images/logo.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com" rel="" target=""><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dealing-with-few-to-no-labels" id="toc-dealing-with-few-to-no-labels" class="nav-link active" data-scroll-target="#dealing-with-few-to-no-labels">Dealing with Few to No Labels</a></li>
  <li><a href="#project-build-a-github-issues-tagger" id="toc-project-build-a-github-issues-tagger" class="nav-link" data-scroll-target="#project-build-a-github-issues-tagger">Project: Build a GitHub Issues Tagger</a>
  <ul class="collapse">
  <li><a href="#getting-the-data" id="toc-getting-the-data" class="nav-link" data-scroll-target="#getting-the-data">Getting the Data</a></li>
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data">Preparing the Data</a></li>
  <li><a href="#creating-training-sets" id="toc-creating-training-sets" class="nav-link" data-scroll-target="#creating-training-sets">Creating Training Sets</a></li>
  <li><a href="#creating-training-slices" id="toc-creating-training-slices" class="nav-link" data-scroll-target="#creating-training-slices">Creating Training Slices</a></li>
  </ul></li>
  <li><a href="#implementing-a-naive-bayesline" id="toc-implementing-a-naive-bayesline" class="nav-link" data-scroll-target="#implementing-a-naive-bayesline">Implementing a Naive Bayesline</a></li>
  <li><a href="#working-with-no-labeled-data" id="toc-working-with-no-labeled-data" class="nav-link" data-scroll-target="#working-with-no-labeled-data">Working with No Labeled Data</a>
  <ul class="collapse">
  <li><a href="#zero-shot-fill-mask-prediction" id="toc-zero-shot-fill-mask-prediction" class="nav-link" data-scroll-target="#zero-shot-fill-mask-prediction">Zero-shot Fill-Mask Prediction</a></li>
  <li><a href="#text-entailment" id="toc-text-entailment" class="nav-link" data-scroll-target="#text-entailment">Text Entailment</a></li>
  </ul></li>
  <li><a href="#working-with-a-few-labels" id="toc-working-with-a-few-labels" class="nav-link" data-scroll-target="#working-with-a-few-labels">Working with a Few Labels</a>
  <ul class="collapse">
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data Augmentation</a></li>
  <li><a href="#using-embeddings-as-a-lookup-table" id="toc-using-embeddings-as-a-lookup-table" class="nav-link" data-scroll-target="#using-embeddings-as-a-lookup-table">Using Embeddings as a Lookup Table</a></li>
  <li><a href="#efficient-similarity-search-with-faiss" id="toc-efficient-similarity-search-with-faiss" class="nav-link" data-scroll-target="#efficient-similarity-search-with-faiss">Efficient Similarity Search with FAISS</a></li>
  <li><a href="#fine-tuning-a-vanilla-transformer" id="toc-fine-tuning-a-vanilla-transformer" class="nav-link" data-scroll-target="#fine-tuning-a-vanilla-transformer">Fine-Tuning a Vanilla Transformer</a></li>
  <li><a href="#in-context-and-few-shot-learning-with-prompts" id="toc-in-context-and-few-shot-learning-with-prompts" class="nav-link" data-scroll-target="#in-context-and-few-shot-learning-with-prompts">In-Context and Few-Shot Learning with Prompts</a></li>
  </ul></li>
  <li><a href="#leveraging-unlabeled-data" id="toc-leveraging-unlabeled-data" class="nav-link" data-scroll-target="#leveraging-unlabeled-data">Leveraging Unlabeled Data</a>
  <ul class="collapse">
  <li><a href="#fine-tuning-a-language-model" id="toc-fine-tuning-a-language-model" class="nav-link" data-scroll-target="#fine-tuning-a-language-model">Fine-Tuning a Language Model</a></li>
  <li><a href="#fine-tuning-a-classifier" id="toc-fine-tuning-a-classifier" class="nav-link" data-scroll-target="#fine-tuning-a-classifier">Fine-Tuning a Classifier</a></li>
  <li><a href="#advanced-methods" id="toc-advanced-methods" class="nav-link" data-scroll-target="#advanced-methods">Advanced Methods</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Transformers Book Ch. 9</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">huggingface</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">notes</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 9 covers how to deal with few to no labels by training a model that automatically tags GitHub issues for the Hugging Face Transformers library.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 22, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<ul>
<li><a href="#dealing-with-few-to-no-labels">Dealing with Few to No Labels</a></li>
<li><a href="#project-build-a-github-issues-tagger">Project: Build a GitHub Issues Tagger</a></li>
<li><a href="#implementing-a-naive-bayesline">Implementing a Naive Bayesline</a></li>
<li><a href="#working-with-no-labeled-data">Working with No Labeled Data</a></li>
<li><a href="#working-with-a-few-labels">Working with a Few Labels</a></li>
<li><a href="#leveraging-unlabeled-data">Leveraging Unlabeled Data</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only print error messages</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>transformers.logging.set_verbosity_error()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>datasets.logging.set_verbosity_error()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>transformers.__version__, datasets.__version__, accelerate.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ('4.18.0', '2.1.0', '0.5.1')</code></pre>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://astor.readthedocs.io/en/latest/</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> astor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj, exclude_doc<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get source code</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> inspect.getsource(obj)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any common leading whitespace from every line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    cleaned_source <span class="op">=</span> textwrap.dedent(source)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the source into an AST node.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    parsed <span class="op">=</span> ast.parse(cleaned_source)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> node <span class="kw">in</span> ast.walk(parsed):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Skip any nodes that are not class or function definitions</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exclude_doc <span class="kw">and</span> <span class="bu">len</span>(node.body) <span class="op">&gt;</span> <span class="dv">1</span>: node.body <span class="op">=</span> node.body[<span class="dv">1</span>:]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(astor.to_source(parsed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="dealing-with-few-to-no-labels" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-few-to-no-labels">Dealing with Few to No Labels</h2>
<ul>
<li>We often have little to no labeled data when starting a new project.</li>
<li>Non-pretrained models do not perform well with little data.</li>
<li>Annotating additional training examples is time-consuming and expensive.</li>
<li>There are several methods for dealing with few to no labels.</li>
<li>Zero-shot learning often sets a strong baseline when there is no labeled data.</li>
<li>Standard fine-tuning works well when there is a lot of labeled data.</li>
<li>We can fine-tune a language model on a large corpus of unlabeled data before training a classifier on a small number of labeled examples.</li>
<li>More sophisticated methods for training with unlabeled data include Unsupervised Data Augmentation and Uncertainty-aware self-training.
<ul>
<li><a href="https://arxiv.org/abs/2006.15315">Uncertainty-aware Self-training for Text Classification with Few Labels</a></li>
</ul></li>
<li>We can use few-shot learning when we only have a small number of labeled examples and no unlabeled data.</li>
<li>We can also use the embeddings from a pretrained language model to perform lookups with a nearest-neighbor search.</li>
</ul>
</section>
<section id="project-build-a-github-issues-tagger" class="level2">
<h2 class="anchored" data-anchor-id="project-build-a-github-issues-tagger">Project: Build a GitHub Issues Tagger</h2>
<ul>
<li>Many support teams use issue trackers like <a href="https://www.atlassian.com/software/jira">Jira</a> or <a href="https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues">GitHub</a> to assist users by tagging issues with metadata based on the issue’s description.</li>
<li>Tags can define the issue type, the product causing the problem, or which team is responsible for handling the reported issue.</li>
<li>Automating issue tagging can significantly improve productivity and enables the support teams to focus on helping users.</li>
<li>The goal is to train a model that automatically tags <a href="https://github.com/huggingface/transformers/issues">GitHub issues</a> for the Hugging Face Transformers library.</li>
<li><a href="https://github.com/huggingface/transformers/issues/9931">GitHub issues</a> contain a title, a description, and a set of tags/labels that characterize them.</li>
<li>The model will take a title and description as input and predict one or more labels (i.e., multilabel classification).</li>
</ul>
<section id="getting-the-data" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-data">Getting the Data</h3>
<ul>
<li>We can use the <a href="https://docs.github.com/en/rest">GitHub REST API</a> to poll the <a href="https://docs.github.com/en/rest/reference/issues#list-repository-issues">Issues endpoint</a>.</li>
<li>The Issues endpoint returns a list of JSON objects.</li>
<li>Each JSON object includes whether it is open or closed, who opened the issue, the title, the body, and the labels.</li>
<li>The GitHub REST API treats pull requests as issues.</li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to download issues for a GitHub project to a <code>.jsonl</code> file</strong></p>
<ul>
<li>We need to download the issues in batches to avoid exceeding GitHub’s limit on the number of requests per hour.</li>
</ul>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fetch_issues(owner<span class="op">=</span><span class="st">"huggingface"</span>, repo<span class="op">=</span><span class="st">"transformers"</span>, num_issues<span class="op">=</span><span class="dv">10_000</span>, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                 rate_limit<span class="op">=</span><span class="dv">5_000</span>):    </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    all_issues <span class="op">=</span> []</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Max number of issues we can request per page</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    per_page <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Number of requests we need to make</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    num_pages <span class="op">=</span> math.ceil(num_issues <span class="op">/</span> per_page)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    base_url <span class="op">=</span> <span class="st">"https://api.github.com/repos"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> page <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_pages)):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Query with state=all to get both open and closed issues</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="ss">f"issues?page=</span><span class="sc">{</span>page<span class="sc">}</span><span class="ss">&amp;per_page=</span><span class="sc">{</span>per_page<span class="sc">}</span><span class="ss">&amp;state=all"</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample: https://api.github.com/repos/huggingface/transformers/issues?page=0&amp;per_page=100&amp;state=all</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        issues <span class="op">=</span> requests.get(<span class="ss">f"</span><span class="sc">{</span>base_url<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>owner<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>repo<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>query<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        batch.extend(issues.json())</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(batch) <span class="op">&gt;</span> rate_limit <span class="kw">and</span> <span class="bu">len</span>(all_issues) <span class="op">&lt;</span> num_issues:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            all_issues.extend(batch)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> [] <span class="co"># Flush batch for next time period</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Reached GitHub rate limit. Sleeping for one hour ..."</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            time.sleep(<span class="dv">60</span> <span class="op">*</span> <span class="dv">60</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    all_issues.extend(batch)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame.from_records(all_issues)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    df.to_json(<span class="ss">f"github-issues-</span><span class="sc">{</span>repo<span class="sc">}</span><span class="ss">.jsonl"</span>, orient<span class="op">=</span><span class="st">"records"</span>, lines<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> It takes a while to fetch all the issues.</p>
<hr>
<p><strong>Download the GitHub Issues</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fetch_issues()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="preparing-the-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-the-data">Preparing the Data</h3>
<hr>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="va">None</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>pd.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    '1.4.2'</code></pre>
<hr>
<p><strong>Import the dataset</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>dataset_url <span class="op">=</span> <span class="st">"https://git.io/nlp-with-transformers"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>df_issues <span class="op">=</span> pd.read_json(dataset_url, lines<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"DataFrame shape: </span><span class="sc">{</span>df_issues<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>DataFrame shape: (9930, 26)</code></pre>
<hr>
<p><strong>Inspect a single GitHub issue</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert Series to DataFrame</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>df_issues.loc[<span class="dv">2</span>].to_frame()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
2
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/11044
</td>
</tr>
<tr>
<th>
repository_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers
</td>
</tr>
<tr>
<th>
labels_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/11044/labels{/name}
</td>
</tr>
<tr>
<th>
comments_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/11044/comments
</td>
</tr>
<tr>
<th>
events_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/11044/events
</td>
</tr>
<tr>
<th>
html_url
</th>
<td>
https://github.com/huggingface/transformers/issues/11044
</td>
</tr>
<tr>
<th>
id
</th>
<td>
849529761
</td>
</tr>
<tr>
<th>
node_id
</th>
<td>
MDU6SXNzdWU4NDk1Mjk3NjE=
</td>
</tr>
<tr>
<th>
number
</th>
<td>
11044
</td>
</tr>
<tr>
<th>
title
</th>
<td>
[DeepSpeed] ZeRO stage 3 integration: getting started and issues
</td>
</tr>
<tr>
<th>
user
</th>
<td>
{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
[{‘id’: 2659267025, ‘node_id’: ‘MDU6TGFiZWwyNjU5MjY3MDI1’, ‘url’: ‘https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed’, ‘name’: ‘DeepSpeed’, ‘color’: ‘4D34F7’, ‘default’: False, ‘description’: ’’}]
</td>
</tr>
<tr>
<th>
state
</th>
<td>
open
</td>
</tr>
<tr>
<th>
locked
</th>
<td>
False
</td>
</tr>
<tr>
<th>
assignee
</th>
<td>
{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
</tr>
<tr>
<th>
assignees
</th>
<td>
[{‘login’: ‘stas00’, ‘id’: 10676103, ‘node_id’: ‘MDQ6VXNlcjEwNjc2MTAz’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/10676103?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/stas00’, ‘html_url’: ‘https://github.com/stas00’, ‘followers_url’: ‘https://api.github.com/users/stas00/followers’, ‘following_url’: ‘https://api.github.com/users/stas00/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/stas00/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/stas00/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/stas00/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/stas00/orgs’, ‘repos_url’: ‘https://api.github.com/users/stas00/repos’, ‘events_url’: ‘https://api.github.com/users/stas00/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/stas00/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]
</td>
</tr>
<tr>
<th>
milestone
</th>
<td>
NaN
</td>
</tr>
<tr>
<th>
comments
</th>
<td>
0
</td>
</tr>
<tr>
<th>
created_at
</th>
<td>
2021-04-02 23:40:42
</td>
</tr>
<tr>
<th>
updated_at
</th>
<td>
2021-04-03 00:00:18
</td>
</tr>
<tr>
<th>
closed_at
</th>
<td>
NaT
</td>
</tr>
<tr>
<th>
author_association
</th>
<td>
COLLABORATOR
</td>
</tr>
<tr>
<th>
active_lock_reason
</th>
<td>
None
</td>
</tr>
<tr>
<th>
body
</th>
<td>
<strong>[This is not yet alive, preparing for the release, so please ignore for now]</strong>DeepSpeed ZeRO-3 has been integrated into HF <code>transformers</code>. I tried to write tests for a wide range of situations I’m sure I’ve missed some scenarios so if you run into any problems please file a separate issue. I’m going to use this issue to track progress on individual ZeRO3 issues.# Why would you want ZeRO-3a few words, while ZeRO-2 was very limited scability-wise - if <code>model.half()</code> couldn’t fit onto a single gpu, adding more gpus won’t have helped so if you had a 24GB GPU you couldn’t train a model larger than about 5B params.with ZeRO-3 the model weights are partitioned across multiple GPUs plus offloaded to CPU, the upper limit on model size has increased by about 2 orders of magnitude. That is ZeRO-3 allows you to scale to huge models with Trillions of parameters assuming you have enough GPUs and general RAM to support this. ZeRO-3 can benefit a lot from general RAM if you have it. If not that’s OK too. ZeRO-3 combines all your GPUs memory and general RAM into a vast pool of memory.you don’t have many GPUs but just a single one but have a lot of general RAM ZeRO-3 will allow you to fit larger models.course, if you run in an environment like the free google colab, while you can use run Deepspeed there, you get so little general RAM it’s very hard to make something out of nothing. Some users (or some sessions) one gets 12GB of RAM which is impossible to work with - you want at least 24GB instances. Setting is up might be tricky too, please see this notebook for an example:://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb# Getting startedthe latest deepspeed version:<code>\r\npip install deepspeed\r\n</code>will want to be on a transformers master branch, if you want to run a quick test:<code>\r\ngit clone https://github.com/huggingface/transformers\r\ncd transformers\r\nBS=4; PYTHONPATH=src USE_TF=0 deepspeed examples/seq2seq/run_translation.py \\r\n--model_name_or_path t5-small --output_dir /tmp/zero3 --overwrite_output_dir --max_train_samples 64 \\r\n--max_val_samples 64 --max_source_length 128 --max_target_length 128 --val_max_target_length 128 \\r\n--do_train --num_train_epochs 1 --per_device_train_batch_size $BS --per_device_eval_batch_size $BS \\r\n--learning_rate 3e-3 --warmup_steps 500 --predict_with_generate --logging_steps 0 --save_steps 0 \\r\n--eval_steps 1 --group_by_length  --adafactor --dataset_name wmt16 --dataset_config ro-en --source_lang en \\r\n--target_lang ro --source_prefix "translate English to Romanian: " \\r\n--deepspeed examples/tests/deepspeed/ds_config_zero3.json\r\n</code>will find a very detailed configuration here: https://huggingface.co/transformers/master/main_classes/trainer.html#deepspeednew config file will look like this:<code>json\r\n{\r\n    "fp16": {\r\n        "enabled": true,\r\n        "loss_scale": 0,\r\n        "loss_scale_window": 1000,\r\n        "initial_scale_power": 16,\r\n        "hysteresis": 2,\r\n        "min_loss_scale": 1\r\n    },\r\n\r\n    "zero_optimization": {\r\n        "stage": 3,\r\n        "cpu_offload": true,\r\n        "cpu_offload_params": true,\r\n        "cpu_offload_use_pin_memory" : true,\r\n        "overlap_comm": true,\r\n        "contiguous_gradients": true,\r\n        "stage3_max_live_parameters": 1e9,\r\n        "stage3_max_reuse_distance": 1e9,\r\n        "stage3_prefetch_bucket_size": 0.94e6,\r\n        "stage3_param_persistence_threshold": 1e4,\r\n        "reduce_bucket_size": 1e6,\r\n        "prefetch_bucket_size": 3e6,\r\n        "sub_group_size": 1e14,\r\n        "stage3_gather_fp16_weights_on_model_save": true\r\n    },\r\n\r\n    "optimizer": {\r\n        "type": "AdamW",\r\n        "params": {\r\n            "lr": 3e-5,\r\n            "betas": [0.8, 0.999],\r\n            "eps": 1e-8,\r\n            "weight_decay": 3e-7\r\n        }\r\n    },\r\n\r\n    "scheduler": {\r\n        "type": "WarmupLR",\r\n        "params": {\r\n            "warmup_min_lr": 0,\r\n            "warmup_max_lr": 3e-5,\r\n            "warmup_num_steps": 500\r\n        }\r\n    },\r\n\r\n    "steps_per_print": 2000,\r\n    "wall_clock_breakdown": false\r\n}\r\n\r\n</code>if you were already using ZeRO-2 it’s only the <code>zero_optimization</code> stage that has changed.of the biggest nuances of ZeRO-3 is that the model weights aren’t inside <code>model.state_dict</code>, as they are spread out through multiple gpus. The Trainer has been modified to support this but you will notice a slow model saving - as it has to consolidate weights from all the gpus. I’m planning to do more performance improvements in the future PRs, but for now let’s focus on making things work.# Issues / Questionsyou have any general questions or something is unclear/missing in the docs please don’t hesitate to ask in this thread. But for any bugs or problems please open a new Issue and tag me there. You don’t need to tag anybody else. Thank you!
</td>
</tr>
<tr>
<th>
performed_via_github_app
</th>
<td>
NaN
</td>
</tr>
<tr>
<th>
pull_request
</th>
<td>
None
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> The labels column contains the tags.</p>
<hr>
<p><strong>Inspect the labels column</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(df_issues.loc[<span class="dv">2</span>][<span class="st">'labels'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
id
</th>
<th>
node_id
</th>
<th>
url
</th>
<th>
name
</th>
<th>
color
</th>
<th>
default
</th>
<th>
description
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
2659267025
</td>
<td>
MDU6TGFiZWwyNjU5MjY3MDI1
</td>
<td>
https://api.github.com/repos/huggingface/transformers/labels/DeepSpeed
</td>
<td>
DeepSpeed
</td>
<td>
4D34F7
</td>
<td>
False
</td>
<td>
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Extract the tags names from the labels column</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"labels"</span>] <span class="op">=</span> (df_issues[<span class="st">"labels"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: [meta[<span class="st">"name"</span>] <span class="cf">for</span> meta <span class="kw">in</span> x]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df_issues[[<span class="st">"labels"</span>]].head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
labels
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
[]
</td>
</tr>
<tr>
<th>
1
</th>
<td>
[]
</td>
</tr>
<tr>
<th>
2
</th>
<td>
[DeepSpeed]
</td>
</tr>
<tr>
<th>
3
</th>
<td>
[]
</td>
</tr>
<tr>
<th>
4
</th>
<td>
[]
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Get the number of labels per issue</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"labels"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x : <span class="bu">len</span>(x)).value_counts().to_frame().T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
labels
</th>
<td>
6440
</td>
<td>
3057
</td>
<td>
305
</td>
<td>
100
</td>
<td>
25
</td>
<td>
3
</td>
</tr>
</tbody>

</table>
</div>
<p><strong>Note:</strong> Most GitHub issues have zero or one label, and very few have more than one label.</p>
<hr>
<p><strong>View the only three issues with five tags</strong></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>df_issues[df_issues[<span class="st">'labels'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x) <span class="op">==</span> <span class="dv">5</span>)].T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
6005
</th>
<th>
7541
</th>
<th>
8266
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/5057
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/3513
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/2787
</td>
</tr>
<tr>
<th>
repository_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers
</td>
<td>
https://api.github.com/repos/huggingface/transformers
</td>
<td>
https://api.github.com/repos/huggingface/transformers
</td>
</tr>
<tr>
<th>
labels_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/5057/labels{/name}
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/3513/labels{/name}
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/2787/labels{/name}
</td>
</tr>
<tr>
<th>
comments_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/5057/comments
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/3513/comments
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/2787/comments
</td>
</tr>
<tr>
<th>
events_url
</th>
<td>
https://api.github.com/repos/huggingface/transformers/issues/5057/events
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/3513/events
</td>
<td>
https://api.github.com/repos/huggingface/transformers/issues/2787/events
</td>
</tr>
<tr>
<th>
html_url
</th>
<td>
https://github.com/huggingface/transformers/issues/5057
</td>
<td>
https://github.com/huggingface/transformers/issues/3513
</td>
<td>
https://github.com/huggingface/transformers/issues/2787
</td>
</tr>
<tr>
<th>
id
</th>
<td>
639635502
</td>
<td>
589781536
</td>
<td>
562124488
</td>
</tr>
<tr>
<th>
node_id
</th>
<td>
MDU6SXNzdWU2Mzk2MzU1MDI=
</td>
<td>
MDU6SXNzdWU1ODk3ODE1MzY=
</td>
<td>
MDU6SXNzdWU1NjIxMjQ0ODg=
</td>
</tr>
<tr>
<th>
number
</th>
<td>
5057
</td>
<td>
3513
</td>
<td>
2787
</td>
</tr>
<tr>
<th>
title
</th>
<td>
Examples tests improvements
</td>
<td>
Adding mbart-large-cc25
</td>
<td>
Distillation code loss functions
</td>
</tr>
<tr>
<th>
user
</th>
<td>
{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
<td>
{‘login’: ‘maksym-del’, ‘id’: 8141935, ‘node_id’: ‘MDQ6VXNlcjgxNDE5MzU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/8141935?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/maksym-del’, ‘html_url’: ‘https://github.com/maksym-del’, ‘followers_url’: ‘https://api.github.com/users/maksym-del/followers’, ‘following_url’: ‘https://api.github.com/users/maksym-del/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/maksym-del/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/maksym-del/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/maksym-del/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/maksym-del/orgs’, ‘repos_url’: ‘https://api.github.com/users/maksym-del/repos’, ‘events_url’: ‘https://api.github.com/users/maksym-del/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/maksym-del/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
<td>
{‘login’: ‘snaik2016’, ‘id’: 18183245, ‘node_id’: ‘MDQ6VXNlcjE4MTgzMjQ1’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/18183245?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/snaik2016’, ‘html_url’: ‘https://github.com/snaik2016’, ‘followers_url’: ‘https://api.github.com/users/snaik2016/followers’, ‘following_url’: ‘https://api.github.com/users/snaik2016/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/snaik2016/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/snaik2016/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/snaik2016/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/snaik2016/orgs’, ‘repos_url’: ‘https://api.github.com/users/snaik2016/repos’, ‘events_url’: ‘https://api.github.com/users/snaik2016/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/snaik2016/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
[Examples, Good First Issue, Help wanted, cleanup, wontfix]
</td>
<td>
[Documentation, Help wanted, New model, seq2seq, translation]
</td>
<td>
[Core: Modeling, Distillation, PyTorch, Usage, wontfix]
</td>
</tr>
<tr>
<th>
state
</th>
<td>
closed
</td>
<td>
closed
</td>
<td>
closed
</td>
</tr>
<tr>
<th>
locked
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
</tr>
<tr>
<th>
assignee
</th>
<td>
{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
<td>
{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}
</td>
<td>
None
</td>
</tr>
<tr>
<th>
assignees
</th>
<td>
[{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]
</td>
<td>
[{‘login’: ‘sshleifer’, ‘id’: 6045025, ‘node_id’: ‘MDQ6VXNlcjYwNDUwMjU=’, ‘avatar_url’: ‘https://avatars.githubusercontent.com/u/6045025?v=4’, ‘gravatar_id’: ’‘, ’url’: ‘https://api.github.com/users/sshleifer’, ‘html_url’: ‘https://github.com/sshleifer’, ‘followers_url’: ‘https://api.github.com/users/sshleifer/followers’, ‘following_url’: ‘https://api.github.com/users/sshleifer/following{/other_user}’, ‘gists_url’: ‘https://api.github.com/users/sshleifer/gists{/gist_id}’, ‘starred_url’: ‘https://api.github.com/users/sshleifer/starred{/owner}{/repo}’, ‘subscriptions_url’: ‘https://api.github.com/users/sshleifer/subscriptions’, ‘organizations_url’: ‘https://api.github.com/users/sshleifer/orgs’, ‘repos_url’: ‘https://api.github.com/users/sshleifer/repos’, ‘events_url’: ‘https://api.github.com/users/sshleifer/events{/privacy}’, ‘received_events_url’: ‘https://api.github.com/users/sshleifer/received_events’, ‘type’: ‘User’, ‘site_admin’: False}]
</td>
<td>
[]
</td>
</tr>
<tr>
<th>
milestone
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
comments
</th>
<td>
12
</td>
<td>
8
</td>
<td>
4
</td>
</tr>
<tr>
<th>
created_at
</th>
<td>
2020-06-16 12:45:32
</td>
<td>
2020-03-29 12:32:30
</td>
<td>
2020-02-09 05:21:33
</td>
</tr>
<tr>
<th>
updated_at
</th>
<td>
2020-10-04 01:14:08
</td>
<td>
2020-07-07 17:23:01
</td>
<td>
2020-04-19 22:29:10
</td>
</tr>
<tr>
<th>
closed_at
</th>
<td>
2020-10-04 01:14:08
</td>
<td>
2020-07-07 17:23:01
</td>
<td>
2020-04-19 22:29:10
</td>
</tr>
<tr>
<th>
author_association
</th>
<td>
MEMBER
</td>
<td>
CONTRIBUTOR
</td>
<td>
NONE
</td>
</tr>
<tr>
<th>
active_lock_reason
</th>
<td>
None
</td>
<td>
None
</td>
<td>
None
</td>
</tr>
<tr>
<th>
body
</th>
<td>
There are a few things about the <code>examples/</code> tests that are suboptimal:. They never use cuda or fp16, even if they are available.. The <code>@slow</code> decorator used in the main tests is not importable, so there are no <span class="citation" data-cites="slow">@slow</span> tests.. <code>test_run_glue</code> uses distilbert-case-cased. It should use a smaller model, one of the <code>tiny</code> family <a href="https://huggingface.co/models?search=sshleifer/tiny">here</a> or a new tiny model.. There is no test coverage for TPU.help on any of these fronts would be much appreciated!
</td>
<td>
# 🌟 New model additionBART model implemented in fairseq introduced by FAIR## Model descriptionissue is to request adding mBART model existing as a part of fairseq lib. (https://github.com/pytorch/fairseq/tree/master/examples/mbart)(https://arxiv.org/abs/2001.08210)pretrained BART checkpoint.&lt;!– Important information –&gt;model code follows the original BART model code which is already a part of <code>transformers</code> repo. However, it introduces a couple more features like multilingual denoising and translation from pretrained BART. ## Open source status- [x] <em>the model implementation is available: (give details)</em>(https://github.com/pytorch/fairseq/commit/5e79322b3a4a9e9a11525377d3dda7ac520b921c) PR shows the main pieces that were added to the fairseq to make mBART work considering BART which is already existing in the codebase. However, a few additional mBART commits were added afterward.- [x] <em>the model weights are available: (give details)</em>(https://github.com/pytorch/fairseq/tree/master/examples/mbart#pre-trained-models)- [x] <em>who are the authors: (mention them, if possible by <span class="citation" data-cites="gh-username">@gh-username</span>)</em>AI Research (<span class="citation" data-cites="MultiPath">@MultiPath</span>)
</td>
<td>
# ❓ Questions &amp; Helpcompute cross entropy loss from the hard labels in distillation code?self.alpha_clm &gt; 0.0:shift_logits = s_logits[…, :-1, :].contiguous()shift_labels = lm_labels[…, 1:].contiguous()loss_clm = self.lm_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))loss += self.alpha_clm * loss_clmmodel outputs loss when passed with the labels.&lt;!– The GitHub issue tracker is primarly intended for bugs, feature requests,new models and benchmarks, and migration questions. For all other questions,we direct you to Stack Overflow (SO) where a whole community of PyTorch andTensorflow enthusiast can help you out. Make sure to tag your question with theright deep learning framework as well as the huggingface-transformers tag: https://stackoverflow.com/questions/tagged/huggingface-transformers If your question wasn’t answered after a period of time on Stack Overflow, youcan always open a question on GitHub. You should then link to the SO question that you posted.–&gt;## Details&lt;!– Description of your issue –&gt;&lt;!– You should first ask your question on SO, and only ifyou didn’t get an answer ask it here on GitHub. –&gt;*A link to original question on Stack Overflow**:
</td>
</tr>
<tr>
<th>
performed_via_github_app
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
pull_request
</th>
<td>
None
</td>
<td>
None
</td>
<td>
None
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<section id="pandas.dataframe.explode" class="level4">
<h4 class="anchored" data-anchor-id="pandas.dataframe.explode"><code>pandas.DataFrame.explode</code></h4>
<ul>
<li><a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html">Documentation</a></li>
<li>Transform each element of a list-like into a row, replicating index values.</li>
</ul>
<p><strong>Get the 20 most frequent labels in the dataset</strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>df_counts <span class="op">=</span> df_issues[<span class="st">"labels"</span>].explode().value_counts()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of labels: </span><span class="sc">{</span><span class="bu">len</span>(df_counts)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the top-20 label categories</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>df_counts.to_frame().head(<span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Number of labels: 65</code></pre>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
labels
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
wontfix
</th>
<td>
2284
</td>
</tr>
<tr>
<th>
model card
</th>
<td>
649
</td>
</tr>
<tr>
<th>
Core: Tokenization
</th>
<td>
106
</td>
</tr>
<tr>
<th>
New model
</th>
<td>
98
</td>
</tr>
<tr>
<th>
Core: Modeling
</th>
<td>
64
</td>
</tr>
<tr>
<th>
Help wanted
</th>
<td>
52
</td>
</tr>
<tr>
<th>
Good First Issue
</th>
<td>
50
</td>
</tr>
<tr>
<th>
Usage
</th>
<td>
46
</td>
</tr>
<tr>
<th>
Core: Pipeline
</th>
<td>
42
</td>
</tr>
<tr>
<th>
Feature request
</th>
<td>
41
</td>
</tr>
<tr>
<th>
TensorFlow
</th>
<td>
41
</td>
</tr>
<tr>
<th>
Tests
</th>
<td>
40
</td>
</tr>
<tr>
<th>
PyTorch
</th>
<td>
37
</td>
</tr>
<tr>
<th>
DeepSpeed
</th>
<td>
33
</td>
</tr>
<tr>
<th>
seq2seq
</th>
<td>
32
</td>
</tr>
<tr>
<th>
Should Fix
</th>
<td>
30
</td>
</tr>
<tr>
<th>
marian
</th>
<td>
29
</td>
</tr>
<tr>
<th>
Discussion
</th>
<td>
28
</td>
</tr>
<tr>
<th>
Documentation
</th>
<td>
28
</td>
</tr>
<tr>
<th>
Examples
</th>
<td>
24
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>df_counts[:<span class="dv">2</span>].<span class="bu">sum</span>() <span class="op">/</span> df_counts.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    0.7185203331700147</code></pre>
<p><strong>Note:</strong> * There are 65 unique tags (i.e., classes) in the dataset. * The dataset is highly imbalanced, with the two most common classes accounting for more than 70% of the dataset. * Some labels (e.g., “Good First” or “Help Wanted”) are potentially too difficult to predict from the issue’s description, while others (e.g., “model card”) might only require simple rules to classify.</p>
<hr>
<p><strong>Filter the dataset to a subset of labels</strong></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>label_map <span class="op">=</span> {<span class="st">"Core: Tokenization"</span>: <span class="st">"tokenization"</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">"New model"</span>: <span class="st">"new model"</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>             <span class="st">"Core: Modeling"</span>: <span class="st">"model training"</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>             <span class="st">"Usage"</span>: <span class="st">"usage"</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>             <span class="st">"Core: Pipeline"</span>: <span class="st">"pipeline"</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>             <span class="st">"TensorFlow"</span>: <span class="st">"tensorflow or tf"</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>             <span class="st">"PyTorch"</span>: <span class="st">"pytorch"</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>             <span class="st">"Examples"</span>: <span class="st">"examples"</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>             <span class="st">"Documentation"</span>: <span class="st">"documentation"</span>}</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> filter_labels(x):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [label_map[label] <span class="cf">for</span> label <span class="kw">in</span> x <span class="cf">if</span> label <span class="kw">in</span> label_map]</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"labels"</span>] <span class="op">=</span> df_issues[<span class="st">"labels"</span>].<span class="bu">apply</span>(filter_labels)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> <span class="bu">list</span>(label_map.values())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Check the distribution of the filtered dataset</strong></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>df_counts <span class="op">=</span> df_issues[<span class="st">"labels"</span>].explode().value_counts()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>df_counts.to_frame().T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
tokenization
</th>
<th>
new model
</th>
<th>
model training
</th>
<th>
usage
</th>
<th>
pipeline
</th>
<th>
tensorflow or tf
</th>
<th>
pytorch
</th>
<th>
documentation
</th>
<th>
examples
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
labels
</th>
<td>
106
</td>
<td>
98
</td>
<td>
64
</td>
<td>
46
</td>
<td>
42
</td>
<td>
41
</td>
<td>
37
</td>
<td>
28
</td>
<td>
24
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>df_counts[:<span class="dv">2</span>].<span class="bu">sum</span>() <span class="op">/</span> df_counts.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    0.41975308641975306</code></pre>
<p><strong>Note:</strong> The filtered dataset is more balanced, with the two most common classes accounting for less than 42% of the dataset.</p>
<hr>
<p><strong>Create a new column to indicate whether an issue is unlabeled</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"split"</span>] <span class="op">=</span> <span class="st">"unlabeled"</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> df_issues[<span class="st">"labels"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x)) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>df_issues.loc[mask, <span class="st">"split"</span>] <span class="op">=</span> <span class="st">"labeled"</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"split"</span>].value_counts().to_frame()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
split
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
unlabeled
</th>
<td>
9489
</td>
</tr>
<tr>
<th>
labeled
</th>
<td>
441
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"split"</span>].value_counts()[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(df_issues)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    0.9555891238670695</code></pre>
<p><strong>Note:</strong> Over 95% of issues are unlabeled.</p>
<hr>
<p><strong>Inspect a labeled example</strong></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> column <span class="kw">in</span> [<span class="st">"title"</span>, <span class="st">"body"</span>, <span class="st">"labels"</span>]:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>column<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>df_issues[column]<span class="sc">.</span>iloc[<span class="dv">26</span>][:<span class="dv">500</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    title: Add new CANINE model
    
    body: # 🌟 New model addition
    
    ## Model description
    
    Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting:
    
    &gt; Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually en
    
    labels: ['new model']</code></pre>
<p><strong>Note:</strong> * This GitHub issue is proposing a new model architecture. * Both the title and description contain helpful information for the label classifier.</p>
<hr>
<p><strong>Concatenate the title and description for each issue into a new column</strong></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>df_issues[<span class="st">"text"</span>] <span class="op">=</span> (df_issues.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">"title"</span>] <span class="op">+</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span> <span class="op">+</span> x[<span class="st">"body"</span>], axis<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Remove any duplicate rows based on the <code>text</code> column values</strong></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>len_before <span class="op">=</span> <span class="bu">len</span>(df_issues)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>df_issues <span class="op">=</span> df_issues.drop_duplicates(subset<span class="op">=</span><span class="st">"text"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Removed </span><span class="sc">{</span>(len_before<span class="op">-</span><span class="bu">len</span>(df_issues))<span class="op">/</span>len_before<span class="sc">:.2%}</span><span class="ss"> duplicates."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Removed 1.88% duplicates.</code></pre>
<hr>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the number of words per issue</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>(df_issues[<span class="st">"text"</span>].<span class="bu">str</span>.split().<span class="bu">apply</span>(<span class="bu">len</span>).hist(bins<span class="op">=</span>np.linspace(<span class="dv">0</span>, <span class="dv">750</span>, <span class="dv">50</span>), grid<span class="op">=</span><span class="va">False</span>, edgecolor<span class="op">=</span><span class="st">"C0"</span>))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Words per issue"</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of words"</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Number of issues"</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_52_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> * The text for most issues is short, but some have more than 500 words. * Issues with error messages and code snippets are often longer. * Most of the examples should fit into the typical context size of 512 tokens.</p>
<hr>
</section>
</section>
<section id="creating-training-sets" class="level3">
<h3 class="anchored" data-anchor-id="creating-training-sets">Creating Training Sets</h3>
<ul>
<li>There is no guaranteed balance for all labels when splitting the dataset.</li>
<li>We can use the scikit-multilearn library to approximate a balanced split.</li>
</ul>
<section id="scikit-multilearn-library" class="level4">
<h4 class="anchored" data-anchor-id="scikit-multilearn-library">scikit-multilearn library</h4>
<ul>
<li><a href="https://scikit.ml/">Homepage</a></li>
<li>A multi-label classification library built on top of the scikit-learn ecosystem.</li>
</ul>
<hr>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MultiLabelBinarizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="multilabelbinarizer" class="level4">
<h4 class="anchored" data-anchor-id="multilabelbinarizer"><code>MultiLabelBinarizer</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html">Documentation</a></li>
<li>Transform between iterable of iterables and a multilabel format.</li>
<li>Takes a list of names and creates a vector with zeros for absent labels and ones for present labels.</li>
</ul>
<hr>
<p><strong>Create a MultiLabelBinarizer to learn the mapping from label to ID</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mlb <span class="op">=</span> MultiLabelBinarizer()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mlb.fit([all_labels])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    MultiLabelBinarizer()</code></pre>
<hr>
<p><strong>Check the label mappings</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>mlb.transform([[<span class="st">"tokenization"</span>, <span class="st">"new model"</span>], [<span class="st">"pytorch"</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    array([[0, 0, 0, 1, 0, 0, 0, 1, 0],
           [0, 0, 0, 0, 0, 1, 0, 0, 0]])</code></pre>
<hr>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(mlb.transform([[label] <span class="cf">for</span> label <span class="kw">in</span> all_labels]).T, columns<span class="op">=</span>all_labels).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
tokenization
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
<tr>
<th>
new model
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
model training
</th>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
usage
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
pipeline
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
tensorflow or tf
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
pytorch
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
examples
</th>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
documentation
</th>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skmultilearn.model_selection <span class="im">import</span> iterative_train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="iterative_train_test_split" class="level4">
<h4 class="anchored" data-anchor-id="iterative_train_test_split"><code>iterative_train_test_split</code></h4>
<ul>
<li><a href="http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html">Documentation</a></li>
</ul>
<p><strong>Define a function to iteratively generate a balanced train/test split</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> balanced_split(df, test_size<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.expand_dims(np.arange(<span class="bu">len</span>(df)), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> mlb.transform(df[<span class="st">"labels"</span>])</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    ind_train, _, ind_test, _ <span class="op">=</span> iterative_train_test_split(ind, labels, </span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>                                                           test_size)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df.iloc[ind_train[:, <span class="dv">0</span>]], df.iloc[ind_test[:,<span class="dv">0</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Split the data into supervised and unsupervised datasets</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>df_clean <span class="op">=</span> df_issues[[<span class="st">"text"</span>, <span class="st">"labels"</span>, <span class="st">"split"</span>]].reset_index(drop<span class="op">=</span><span class="va">True</span>).copy()</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>df_unsup <span class="op">=</span> df_clean.loc[df_clean[<span class="st">"split"</span>] <span class="op">==</span> <span class="st">"unlabeled"</span>, [<span class="st">"text"</span>, <span class="st">"labels"</span>]]</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>df_sup <span class="op">=</span> df_clean.loc[df_clean[<span class="st">"split"</span>] <span class="op">==</span> <span class="st">"labeled"</span>, [<span class="st">"text"</span>, <span class="st">"labels"</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create balanced training, validation, and test sets</strong></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>df_train, df_tmp <span class="op">=</span> balanced_split(df_sup, test_size<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>df_valid, df_test <span class="op">=</span> balanced_split(df_tmp, test_size<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset, DatasetDict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="dataset.from_pandas" class="level4">
<h4 class="anchored" data-anchor-id="dataset.from_pandas"><code>Dataset.from_pandas</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.from_pandas">Documentation</a></li>
<li>Convert pandas.DataFrame to a pyarrow.Table to create a Dataset.</li>
</ul>
<hr>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>print_source(Dataset.from_pandas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @classmethod
    def from_pandas(cls, df: pd.DataFrame, features: Optional[Features]=None,
        info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None,
        preserve_index: Optional[bool]=None) -&gt;'Dataset':
        if info is not None and features is not None and info.features != features:
            raise ValueError(
                f"""Features specified in `features` and `info.features` can't be different:
    {features}
    {info.features}"""
                )
        features = (features if features is not None else info.features if info
             is not None else None)
        if info is None:
            info = DatasetInfo()
        info.features = features
        table = InMemoryTable.from_pandas(df=df, preserve_index=preserve_index,
            schema=features.arrow_schema if features is not None else None)
        return cls(table, info=info, split=split)</code></pre>
<hr>
<p><strong>Initialize a DatasetDict with the dataset splits</strong></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> DatasetDict({</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"train"</span>: Dataset.from_pandas(df_train.reset_index(drop<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"valid"</span>: Dataset.from_pandas(df_valid.reset_index(drop<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"test"</span>: Dataset.from_pandas(df_test.reset_index(drop<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"unsup"</span>: Dataset.from_pandas(df_unsup.reset_index(drop<span class="op">=</span><span class="va">True</span>))})</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['text', 'labels'],
            num_rows: 223
        })
        valid: Dataset({
            features: ['text', 'labels'],
            num_rows: 106
        })
        test: Dataset({
            features: ['text', 'labels'],
            num_rows: 111
        })
        unsup: Dataset({
            features: ['text', 'labels'],
            num_rows: 9303
        })
    })</code></pre>
<hr>
</section>
</section>
<section id="creating-training-slices" class="level3">
<h3 class="anchored" data-anchor-id="creating-training-slices">Creating Training Slices</h3>
<p><strong>Create training slices with different numbers of samples</strong></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>all_indices <span class="op">=</span> np.expand_dims(<span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(ds[<span class="st">"train"</span>]))), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>indices_pool <span class="op">=</span> all_indices</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> mlb.transform(ds[<span class="st">"train"</span>][<span class="st">"labels"</span>])</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>train_samples <span class="op">=</span> [<span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>]</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>train_slices, last_k <span class="op">=</span> [], <span class="dv">0</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, k <span class="kw">in</span> <span class="bu">enumerate</span>(train_samples):</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split off samples necessary to fill the gap to the next split size</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    indices_pool, labels, new_slice, _ <span class="op">=</span> iterative_train_test_split(</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        indices_pool, labels, (k<span class="op">-</span>last_k)<span class="op">/</span><span class="bu">len</span>(labels))</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    last_k <span class="op">=</span> k</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">==</span><span class="dv">0</span>: train_slices.append(new_slice)</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: train_slices.append(np.concatenate((train_slices[<span class="op">-</span><span class="dv">1</span>], new_slice)))</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Add full dataset as last slice</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>train_slices.append(all_indices), train_samples.append(<span class="bu">len</span>(ds[<span class="st">"train"</span>]))</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>train_slices <span class="op">=</span> [np.squeeze(train_slice) <span class="cf">for</span> train_slice <span class="kw">in</span> train_slices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> It is not always possible to find a balanced split with a given split size.</p>
<hr>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target split sizes:"</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_samples)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Actual split sizes:"</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> train_slices])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Target split sizes:
    [8, 16, 32, 64, 128, 223]
    Actual split sizes:
    [10, 19, 36, 68, 134, 223]</code></pre>
<hr>
</section>
</section>
<section id="implementing-a-naive-bayesline" class="level2">
<h2 class="anchored" data-anchor-id="implementing-a-naive-bayesline">Implementing a Naive Bayesline</h2>
<ul>
<li>A baseline based on regular expressions, handcrafted rules, or a simple model might work well enough to solve a given problem.
<ul>
<li>These are generally easier to deploy and maintain than transformer models.</li>
</ul></li>
<li>Baseline models provide quick sanity checks when exploring more complex models.
<ul>
<li>A more complex model like BERT should perform better than a simple logistic regression classifier on the same dataset.</li>
</ul></li>
<li>A Naive Bayes Classifier is a great baseline model for text classification as it is simple, quick to train, and reasonably robust to changes in input.</li>
</ul>
<p><strong>Create a new ids column with the multilabel vectors for each training sample</strong></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_labels(batch):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">"label_ids"</span>] <span class="op">=</span> mlb.transform(batch[<span class="st">"labels"</span>])</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> ds.<span class="bu">map</span>(prepare_labels, batched<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>ds[<span class="st">'train'</span>][:<span class="dv">5</span>][<span class="st">'labels'</span>], ds[<span class="st">'train'</span>][:<span class="dv">5</span>][<span class="st">'label_ids'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    ([['new model'], ['new model'], ['new model'], ['new model'], ['examples']],
     [[0, 0, 0, 1, 0, 0, 0, 0, 0],
      [0, 0, 0, 1, 0, 0, 0, 0, 0],
      [0, 0, 0, 1, 0, 0, 0, 0, 0],
      [0, 0, 0, 1, 0, 0, 0, 0, 0],
      [0, 1, 0, 0, 0, 0, 0, 0, 0]])</code></pre>
<hr>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create dictionaries to store micro and macro <span class="math inline">\(F_{1}\)</span>-scores</strong></p>
<ul>
<li>The micro <span class="math inline">\(F_{1}\)</span>-score tracks performance for on the frequent labels</li>
<li>The macro <span class="math inline">\(F_{1}\)</span>-score tracks performance on all the labels regardless of frequency</li>
</ul>
<hr>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>macro_scores, micro_scores <span class="op">=</span> defaultdict(<span class="bu">list</span>), defaultdict(<span class="bu">list</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skmultilearn.problem_transform <span class="im">import</span> BinaryRelevance</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="sklearn.naive_bayes.multinomialnb" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.naive_bayes.multinomialnb"><code>sklearn.naive_bayes.MultinomialNB</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">Documentation</a></li>
<li>Create a Naive Bayes classifier for multinomial models.</li>
</ul>
</section>
<section id="sklearn.metrics._classification.classification_report" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.metrics._classification.classification_report"><code>sklearn.metrics._classification.classification_report</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html">Documentation</a></li>
<li>Build a text report showing the main classification metrics.</li>
</ul>
</section>
<section id="skmultilearn.problem_transform.br.binaryrelevance" class="level4">
<h4 class="anchored" data-anchor-id="skmultilearn.problem_transform.br.binaryrelevance"><code>skmultilearn.problem_transform.br.BinaryRelevance</code></h4>
<ul>
<li><a href="https://scikit.ml/api/skmultilearn.problem_transform.br.html#skmultilearn.problem_transform.BinaryRelevance">Documentation</a></li>
<li>Treat each label as a separate single-class classification problem</li>
</ul>
</section>
<section id="sklearn.feature_extraction.text.countvectorizer" class="level4">
<h4 class="anchored" data-anchor-id="sklearn.feature_extraction.text.countvectorizer"><code>sklearn.feature_extraction.text.CountVectorizer</code></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">Documentation</a></li>
<li>Create a vector where each entry corresponds to the frequency with which a token appeared in the text.</li>
<li>Count vectorization is a bag-of-words approach since all information on the order of the words is lost.</li>
</ul>
<hr>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># count_vect = CountVectorizer()</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pd.DataFrame(count_vect.fit_transform(ds['train'].select(train_slices[0])["text"]).toarray())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train a baseline Naive Bayes Classifier for each of the training slices</strong></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get training slice and test data</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    ds_train_sample <span class="op">=</span> ds[<span class="st">"train"</span>].select(train_slice)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> np.array(ds_train_sample[<span class="st">"label_ids"</span>])</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.array(ds[<span class="st">"test"</span>][<span class="st">"label_ids"</span>])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use a simple count vectorizer to encode our texts as token counts</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    count_vect <span class="op">=</span> CountVectorizer()</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    X_train_counts <span class="op">=</span> count_vect.fit_transform(ds_train_sample[<span class="st">"text"</span>])</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    X_test_counts <span class="op">=</span> count_vect.transform(ds[<span class="st">"test"</span>][<span class="st">"text"</span>])</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create and train our model!</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> BinaryRelevance(classifier<span class="op">=</span>MultinomialNB())</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>    classifier.fit(X_train_counts, y_train)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate predictions and evaluate</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    y_pred_test <span class="op">=</span> classifier.predict(X_test_counts)</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>    clf_report <span class="op">=</span> classification_report(</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>        y_test, y_pred_test, target_names<span class="op">=</span>mlb.classes_, zero_division<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>        output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store metrics</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">"Naive Bayes"</span>].append(clf_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>])</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">"Naive Bayes"</span>].append(clf_report[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the performance of the baseline classifiers</strong></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    fig, (ax0, ax1) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> run <span class="kw">in</span> micro_scores.keys():</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> run <span class="op">==</span> current_model:</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>            ax0.plot(sample_sizes, micro_scores[run], label<span class="op">=</span>run, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>            ax1.plot(sample_sizes, macro_scores[run], label<span class="op">=</span>run, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>            ax0.plot(sample_sizes, micro_scores[run], label<span class="op">=</span>run, </span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>                     linestyle<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>            ax1.plot(sample_sizes, macro_scores[run], label<span class="op">=</span>run, </span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>                     linestyle<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    ax0.set_title(<span class="st">"Micro F1 scores"</span>)</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">"Macro F1 scores"</span>)</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>    ax0.set_ylabel(<span class="st">"Test set F1 score"</span>)</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>    ax0.legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ax <span class="kw">in</span> [ax0, ax1]:</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">"Number of training samples"</span>)</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>        ax.set_xscale(<span class="st">"log"</span>)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>        ax.set_xticks(sample_sizes)</span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>        ax.set_xticklabels(sample_sizes)</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>        ax.minorticks_off()</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Naive Bayes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_96_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The number of samples is on a logarithmic scale.</li>
<li>The micro and macro F1 scores improve as the number of training samples increases.</li>
<li>The results are slightly noisy since each slice can have a different class distribution.</li>
</ul>
<hr>
</section>
</section>
<section id="working-with-no-labeled-data" class="level2">
<h2 class="anchored" data-anchor-id="working-with-no-labeled-data">Working with No Labeled Data</h2>
<ul>
<li>Zero-shot classification is suitable when there is no labeled data at all.</li>
<li>Zero-shot classification uses a pretrained model without additional fine-tuning on a task-specific corpus.</li>
</ul>
<section id="zero-shot-fill-mask-prediction" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-fill-mask-prediction">Zero-shot Fill-Mask Prediction</h3>
<ul>
<li><p>The pretrained model needs to be aware of the topic in the context to predict a missing token.</p></li>
<li><p>We can trick the model into classifying a document by providing a sentence like: &gt; “This section was about the topic [MASK].”</p></li>
<li><p>The model should give a reasonable suggestion for the document’s topic.</p></li>
<li><p>Credit: <a href="https://joeddav.github.io/">Joe Davison</a></p></li>
</ul>
<hr>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a Masked Language Modeling pipeline</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-uncased"</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>pipe, <span class="bu">type</span>(pipe.model), pipe.device</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (&lt;transformers.pipelines.fill_mask.FillMaskPipeline at 0x7fd190bcd3a0&gt;,
     transformers.models.bert.modeling_bert.BertForMaskedLM,
     device(type='cpu'))</code></pre>
<hr>
<section id="transformers.pipelines.fill_mask.fillmaskpipeline" class="level4">
<h4 class="anchored" data-anchor-id="transformers.pipelines.fill_mask.fillmaskpipeline"><code>transformers.pipelines.fill_mask.FillMaskPipeline</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.FillMaskPipeline">Documentation</a></li>
<li>Create a masked language modeling prediction pipeline</li>
</ul>
<p><strong>Predict the topic for a movie about animals base on its description</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>movie_desc <span class="op">=</span> <span class="st">"The main characters of the movie madacascar </span><span class="ch">\</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="st">are a lion, a zebra, a giraffe, and a hippo. "</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"The movie is about [MASK]."</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(movie_desc <span class="op">+</span> prompt)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> element <span class="kw">in</span> output:</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token </span><span class="sc">{</span>element[<span class="st">'token_str'</span>]<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="sc">{</span>element[<span class="st">'score'</span>]<span class="sc">:.3f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Token animals:  0.103%
    Token lions:    0.066%
    Token birds:    0.025%
    Token love: 0.015%
    Token hunting:  0.013%</code></pre>
<p><strong>Note:</strong> The model successfully performs zero-shot classification and only predicts tokens related to animals.</p>
<hr>
<p><strong>Check the probability that the movie description is about specific topics</strong></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(movie_desc <span class="op">+</span> prompt, targets<span class="op">=</span>[<span class="st">"animals"</span>, <span class="st">"cars"</span>])</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> element <span class="kw">in</span> output:</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token </span><span class="sc">{</span>element[<span class="st">'token_str'</span>]<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="sc">{</span>element[<span class="st">'score'</span>]<span class="sc">:.3f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Token animals:  0.103%
    Token cars: 0.001%</code></pre>
<p><strong>Note:</strong> The model is confident the movie is not about cars.</p>
<hr>
<p><strong>Predict the topic for a movie about cars based on its description</strong></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>movie_desc <span class="op">=</span> <span class="st">"In the movie transformers aliens </span><span class="ch">\</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="st">can morph into a wide range of vehicles."</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(movie_desc <span class="op">+</span> prompt)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> element <span class="kw">in</span> output:</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token </span><span class="sc">{</span>element[<span class="st">'token_str'</span>]<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="sc">{</span>element[<span class="st">'score'</span>]<span class="sc">:.3f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Token aliens:   0.221%
    Token cars: 0.139%
    Token robots:   0.099%
    Token vehicles: 0.074%
    Token transformers: 0.059%</code></pre>
<hr>
<p><strong>Check the probability that the movie description is about specific topics</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>movie_desc <span class="op">=</span> <span class="st">"In the movie transformers aliens </span><span class="ch">\</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="st">can morph into a wide range of vehicles."</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(movie_desc <span class="op">+</span> prompt, targets<span class="op">=</span>[<span class="st">"animals"</span>, <span class="st">"cars"</span>])</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> element <span class="kw">in</span> output:</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token </span><span class="sc">{</span>element[<span class="st">'token_str'</span>]<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="sc">{</span>element[<span class="st">'score'</span>]<span class="sc">:.3f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Token cars: 0.139%
    Token animals:  0.006%</code></pre>
<hr>
</section>
</section>
<section id="text-entailment" class="level3">
<h3 class="anchored" data-anchor-id="text-entailment">Text Entailment</h3>
<ul>
<li>The model determines whether two text passages are likely to follow or contradict each other.</li>
<li><a href="https://arxiv.org/abs/1704.05426">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</a></li>
<li><a href="https://arxiv.org/abs/1809.05053">XNLI: Evaluating Cross-lingual Sentence Representations</a></li>
<li>Typical datasets for text entailment tasks include the Multi-Genere NLI Corpus (MNLI) and the Cross-Lingual NLI Corpus (XNLI).</li>
<li>Each sample in the dataset contains a premise, a hypothesis, and a label.</li>
<li>The label can be either <code>entailment</code>, <code>neutral</code>, or <code>contradiction</code>.
<ul>
<li>The <code>entailment</code> label indicates the hypothesis text is necessarily correct under the premise.</li>
<li>The <code>contradiction</code> label indicates the hypothesis is necessarily false or inappropriate under the premise.</li>
<li>The <code>neutral</code> label indicates the hypothesis is unrelated to the premise.</li>
</ul></li>
</ul>
<table class="table">
<colgroup>
<col style="width: 35%">
<col style="width: 48%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Premise</th>
<th>Hypothesis</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>His favorite color is blue.</td>
<td>He is into heavy metal</td>
<td>neutral</td>
</tr>
<tr class="even">
<td>She finds the joke hilarious.</td>
<td>She thinkg the joke is not funny at all.</td>
<td>contradiction</td>
</tr>
<tr class="odd">
<td>The house was recently built.</td>
<td>The house is new.</td>
<td>entailment</td>
</tr>
</tbody>
</table>
<section id="zero-shot-classification-with-text-entailment" class="level4">
<h4 class="anchored" data-anchor-id="zero-shot-classification-with-text-entailment">Zero-shot classification with Text Entailment</h4>
<ul>
<li><p>We can use a model trained on the MNLI dataset to build a classifier without needing any labels.</p></li>
<li><p>We treat the input text as a premise and formulate a hypothesis as: &gt; “This example is about {label}.”</p></li>
<li><p>We insert the class name for the label.</p></li>
<li><p>The resulting entailment score indicates how likely the premise is about the topic.</p></li>
<li><p>We need to test different classes sequentially, meaning we need to execute a forward pass for each test.</p></li>
<li><p>The choice of label names can significantly impact prediction accuracy.</p></li>
<li><p>Choosing labels with a semantic meaning is generally the best approach.</p></li>
</ul>
<hr>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Create a zero-shot classification pipeline</strong></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU if available</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"zero-shot-classification"</span>, device<span class="op">=</span><span class="dv">0</span>, fp16<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>pipe, <span class="bu">type</span>(pipe.model), pipe.device</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(&lt;transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline at 0x7fd189242d00&gt;, transformers.models.bart.modeling_bart.BartForSequenceClassification, device(type='cuda', index=0))</code></pre>
<hr>
<p><strong>Get the default hypothesis template</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>inspect.signature(pipe.preprocess).parameters[<span class="st">'hypothesis_template'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    &lt;Parameter "hypothesis_template='This example is {}.'"&gt;</code></pre>
</section>
<section id="transformers.pipelines.zero_shot_classification.zeroshotclassificationpipeline" class="level4">
<h4 class="anchored" data-anchor-id="transformers.pipelines.zero_shot_classification.zeroshotclassificationpipeline"><code>transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline">Documentation</a></li>
<li>Create a Natural-Language-Inference (NLI)-based zero-shot classification pipeline.</li>
<li>The pipeline takes any combination of sequences and labels.</li>
<li>The pipeline poses each combination as a premise-hypothesis pair and passes them to the pretrained model.</li>
</ul>
<hr>
<div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>pd.Series(ds[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">'text'</span>]).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>).hide(axis<span class="op">=</span><span class="st">'rows'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(ds[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">'text'</span>].split(<span class="st">' '</span>)), <span class="bu">len</span>(pipe.tokenizer(ds[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">'text'</span>])[<span class="st">'input_ids'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    (244, 562)</code></pre>
<hr>
<div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(ds[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">'text'</span>].split(<span class="st">' '</span>)).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> pipe.tokenizer(ds[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">'text'</span>])[<span class="st">'input_ids'</span>]</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(pipe.tokenizer.convert_ids_to_tokens(input_ids)).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test each possible label using text entailment</strong></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> ds[<span class="st">"train"</span>][<span class="dv">0</span>]</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels: </span><span class="sc">{</span>sample[<span class="st">'labels'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(sample[<span class="st">"text"</span>], candidate_labels<span class="op">=</span>all_labels, multi_label<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output[<span class="st">"sequence"</span>][:<span class="dv">400</span>])</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Predictions:"</span>)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(output[<span class="st">"labels"</span>], output[<span class="st">"scores"</span>]):</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Labels: ['new model']
    Add new CANINE model
    
    # 🌟 New model addition
    
    ## Model description
    
    Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting:
    
    &gt; Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokeni
    
    Predictions:
    new model, 0.98
    tensorflow or tf, 0.37
    examples, 0.34
    usage, 0.30
    pytorch, 0.25
    documentation, 0.25
    model training, 0.24
    tokenization, 0.17
    pipeline, 0.16</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The model is confident the text is about a new model, but it also produces relatively high scores for labels not found in the text.</li>
<li>The highly technical domain of the text is very different from the original text distribution in the MNLI dataset.</li>
<li>We can feed input with code to the model since we use a subword tokenizer.</li>
<li>Tokenization might be inefficient with code since only a tiny fraction of the pretraining dataset contains code snippets.</li>
<li>Code blocks can contain important information, such as frameworks used in the code.</li>
</ul>
<hr>
<p><strong>Define a function to feed a single example through the zero-shot pipeline</strong></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> zero_shot_pipeline(example):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> pipe(example[<span class="st">"text"</span>], all_labels, multi_label<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    example[<span class="st">"predicted_labels"</span>] <span class="op">=</span> output[<span class="st">"labels"</span>]</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    example[<span class="st">"scores"</span>] <span class="op">=</span> output[<span class="st">"scores"</span>]</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> example</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Feed the whole validation set through the pipeline</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>ds_zero_shot <span class="op">=</span> ds[<span class="st">"valid"</span>].<span class="bu">map</span>(zero_shot_pipeline)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> We can determine which labels to assign to each example using a minimum threshold value or selecting the top-k predictions.</p>
<hr>
<p><strong>Define a function to determine which set of labels to assign to each example using either a threshold value or top-k value</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_preds(example, threshold<span class="op">=</span><span class="va">None</span>, topk<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> []</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> threshold:</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(example[<span class="st">"predicted_labels"</span>], example[<span class="st">"scores"</span>]):</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> score <span class="op">&gt;=</span> threshold:</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>                preds.append(label)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> topk:</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(topk):</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>            preds.append(example[<span class="st">"predicted_labels"</span>][i])</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Set either `threshold` or `topk`."</span>)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"pred_label_ids"</span>: <span class="bu">list</span>(np.squeeze(mlb.transform([preds])))}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function that returns the scikit-learn classification report</strong></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_clf_report(ds):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> np.array(ds[<span class="st">"label_ids"</span>])</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.array(ds[<span class="st">"pred_label_ids"</span>])</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classification_report(</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>        y_true, y_pred, target_names<span class="op">=</span>mlb.classes_, zero_division<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>        output_dict<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test using top-k values to select labels</strong></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>macros, micros <span class="op">=</span> [], []</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>topks <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topk <span class="kw">in</span> topks:</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    ds_zero_shot <span class="op">=</span> ds_zero_shot.<span class="bu">map</span>(get_preds, batched<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>                                    fn_kwargs<span class="op">=</span>{<span class="st">'topk'</span>: topk})</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>    clf_report <span class="op">=</span> get_clf_report(ds_zero_shot)</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>    micros.append(clf_report[<span class="st">'micro avg'</span>][<span class="st">'f1-score'</span>])</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>    macros.append(clf_report[<span class="st">'macro avg'</span>][<span class="st">'f1-score'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>plt.plot(topks, micros, label<span class="op">=</span><span class="st">'Micro F1'</span>)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>plt.plot(topks, macros, label<span class="op">=</span><span class="st">'Macro F1'</span>)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Top-k"</span>)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1-score"</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_140_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> We obtain the best results using only the highest score per example (i.e., top-1), given most examples only have one label.</p>
<hr>
<p><strong>Test using a threshold value to select labels</strong></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>macros, micros <span class="op">=</span> [], []</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>thresholds <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> threshold <span class="kw">in</span> thresholds:</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    ds_zero_shot <span class="op">=</span> ds_zero_shot.<span class="bu">map</span>(get_preds,</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>                                    fn_kwargs<span class="op">=</span>{<span class="st">"threshold"</span>: threshold})</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    clf_report <span class="op">=</span> get_clf_report(ds_zero_shot)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    micros.append(clf_report[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>])</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    macros.append(clf_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, micros, label<span class="op">=</span><span class="st">"Micro F1"</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>plt.plot(thresholds, macros, label<span class="op">=</span><span class="st">"Macro F1"</span>)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Threshold"</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1-score"</span>)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_144_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The threshold approach performs slightly worse than the top-1 approach.</p>
<hr>
<div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>best_t, best_micro <span class="op">=</span> thresholds[np.argmax(micros)], np.<span class="bu">max</span>(micros)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Best threshold (micro): </span><span class="sc">{</span>best_t<span class="sc">}</span><span class="ss"> with F1-score </span><span class="sc">{</span>best_micro<span class="sc">:.2f}</span><span class="ss">.'</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>best_t, best_macro <span class="op">=</span> thresholds[np.argmax(macros)], np.<span class="bu">max</span>(macros)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Best threshold (micro): </span><span class="sc">{</span>best_t<span class="sc">}</span><span class="ss"> with F1-score </span><span class="sc">{</span>best_macro<span class="sc">:.2f}</span><span class="ss">.'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Best threshold (micro): 0.75 with F1-score 0.46.
    Best threshold (micro): 0.72 with F1-score 0.42.</code></pre>
<p><strong>Note:</strong> A threshold value of around 0.8 provides the best tradeoff between precision and recall.</p>
<hr>
<p><strong>Compare the zero-shot classifier to the baseline Naive Bayes model</strong></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>ds_zero_shot <span class="op">=</span> ds[<span class="st">'test'</span>].<span class="bu">map</span>(zero_shot_pipeline)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>ds_zero_shot <span class="op">=</span> ds_zero_shot.<span class="bu">map</span>(get_preds, fn_kwargs<span class="op">=</span>{<span class="st">'topk'</span>: <span class="dv">1</span>})</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>clf_report <span class="op">=</span> get_clf_report(ds_zero_shot)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">'Zero Shot'</span>].append(clf_report[<span class="st">'macro avg'</span>][<span class="st">'f1-score'</span>])</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">'Zero Shot'</span>].append(clf_report[<span class="st">'micro avg'</span>][<span class="st">'f1-score'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Zero Shot"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_150_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The zero-shot pipeline outperforms the baseline when using less than 60 labeled examples universally outperforms the baseline when considering both micro and macro F1 scores.</li>
<li>The baseline model performs better on the more common classes when using more than 60 examples.</li>
<li>The zero-shot classification pipeline is sensitive to the names of labels and might perform better when using different or several names in parallel and aggregating them.</li>
<li>Using a different <code>hypothesis_template</code> might improve performance.</li>
</ul>
</section>
</section>
</section>
<section id="working-with-a-few-labels" class="level2">
<h2 class="anchored" data-anchor-id="working-with-a-few-labels">Working with a Few Labels</h2>
<ul>
<li>There are often a few labeled examples available, at least, for most NLP projects.</li>
<li>The labels might come directly from a client, a cross-company team, from hand annotating a few examples.</li>
</ul>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<ul>
<li>We can use data augmentation to generate new training examples from existing ones. Perturbing words or characters can completely change the meaning. Noise introduced by data augmentation is less likely to change the meaning when the text is more than a few sentences.</li>
</ul>
<section id="back-translation" class="level4">
<h4 class="anchored" data-anchor-id="back-translation">Back Translation</h4>
<ul>
<li>Back translation involves translating the original text into one or more target languages and translating it back to the source language.</li>
<li>Back translation works best for high-resource languages or corpora that don’t contain too many domain-specific words.</li>
<li>We can implement back translation models using machine translation models like <a href="https://huggingface.co/facebook/m2m100_1.2B">MSM100</a>.</li>
</ul>
</section>
<section id="token-perturbations" class="level4">
<h4 class="anchored" data-anchor-id="token-perturbations">Token Perturbations</h4>
<ul>
<li>Token perturbations involve randomly choosing and performing simple transformations like synonym replacement, word insertion, swap, or deletion.</li>
<li>Libraries like <a href="https://github.com/makcedward/nlpaug">NlpAug</a> and <a href="https://github.com/QData/TextAttack">TextAttack</a> provide various recipes for token perturbations.</li>
<li><a href="https://arxiv.org/abs/1901.11196">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</a></li>
</ul>
<p><br></p>
<table class="table">
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Augmentation</th>
<th>Sentence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>None</td>
<td>Even if you defeat me Megatron, others will rise to defeat your tyranny</td>
</tr>
<tr class="even">
<td>Synonym replace</td>
<td>Even if you kill me, Megatron, others will prove to defeat your tyranny</td>
</tr>
<tr class="odd">
<td>Random Insert</td>
<td>Even if you defeat me Megatron, others humanity will rise to defeat your tyranny</td>
</tr>
<tr class="even">
<td>Random Swap</td>
<td>You even if defeat me Megatron, others will rise defeat to tyranny your</td>
</tr>
<tr class="odd">
<td>Random delete</td>
<td>Even if you me Megatron, others to defeat tyranny</td>
</tr>
<tr class="even">
<td>Back translate (German)</td>
<td>Even if you defeat me, other will rise up to defeat your tyranny</td>
</tr>
</tbody>
</table>
<p><strong>Disable Tokenizers Parallelism</strong></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>env TOKENIZERS_PARALLELISM<span class="op">=</span>false</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    env: TOKENIZERS_PARALLELISM=false</code></pre>
<hr>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> set_seed</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nlpaug.augmenter.word <span class="im">as</span> naw</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nlpaug.augmenter.char <span class="im">as</span> nac</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nlpaug.augmenter.sentence <span class="im">as</span> nas</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nlpaug.flow <span class="im">as</span> nafc</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Download a perceptron model for tagging words and the Wordnet corpora</strong></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'averaged_perceptron_tagger'</span>)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'wordnet'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    True</code></pre>
<hr>
<div class="sourceCode" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'omw-1.4'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    True</code></pre>
<hr>
<div class="sourceCode" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls <span class="op">~/</span>nltk_data<span class="op">/</span>corpora<span class="op">/</span>wordnet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    adj.exc       cntlist.rev  data.noun  index.adv    index.verb  noun.exc
    adv.exc       data.adj     data.verb  index.noun   lexnames    README
    citation.bib  data.adv     index.adj  index.sense  LICENSE     verb.exc</code></pre>
<hr>
<div class="sourceCode" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>head <span class="op">-</span><span class="dv">5</span> <span class="op">~/</span>nltk_data<span class="op">/</span>corpora<span class="op">/</span>wordnet<span class="op">/</span>noun.exc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    aardwolves aardwolf
    abaci abacus
    aboideaux aboideau
    aboiteaux aboiteau
    abscissae abscissa</code></pre>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define original text</strong></p>
<div class="sourceCode" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Even if you defeat me Megatron, others will rise to defeat your tyranny"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize augmentation dictionary</strong></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>augs <span class="op">=</span> {}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="nlpaug.augmenter.word.synonym.synonymaug" class="level4">
<h4 class="anchored" data-anchor-id="nlpaug.augmenter.word.synonym.synonymaug"><code>nlpaug.augmenter.word.synonym.SynonymAug</code></h4>
<ul>
<li><a href="https://nlpaug.readthedocs.io/en/latest/augmenter/word/synonym.html#nlpaug.augmenter.word.synonym.SynonymAug">Documentation</a></li>
<li>Create an augmenter that leverages semantic meaning to substitute words.</li>
</ul>
<p><strong>Add Synonym Replacement Augmentation using the WordNet corpora</strong></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"synonym_replace"</span>] <span class="op">=</span> naw.SynonymAug(aug_src<span class="op">=</span><span class="st">'wordnet'</span>)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"synonym_replace"</span>].augment(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'Even if you kill me Megatron, others will prove to defeat your tyranny'</code></pre>
<hr>
</section>
<section id="nlpaug.augmenter.word.context_word_embs.contextualwordembsaug" class="level4">
<h4 class="anchored" data-anchor-id="nlpaug.augmenter.word.context_word_embs.contextualwordembsaug"><code>nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug</code></h4>
<ul>
<li><a href="https://nlpaug.readthedocs.io/en/latest/augmenter/word/context_word_embs.html#nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug">Documentation</a></li>
<li>Create an augmenter that finds the top n similar words using contextual word embeddings</li>
</ul>
<p><strong>Add Random Insert Augmentation using the contextual word embeddings of DistilBERT</strong></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_insert"</span>] <span class="op">=</span> naw.ContextualWordEmbsAug(model_path<span class="op">=</span><span class="st">"distilbert-base-uncased"</span>, </span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>                                device<span class="op">=</span><span class="st">"cpu"</span>, action<span class="op">=</span><span class="st">"insert"</span>, aug_max<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_insert"</span>].augment(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'even if you defeat me megatron, others humanity will rise to defeat your tyranny'</code></pre>
<hr>
</section>
<section id="nlpaug.augmenter.word.random.randomwordaug" class="level4">
<h4 class="anchored" data-anchor-id="nlpaug.augmenter.word.random.randomwordaug"><code>nlpaug.augmenter.word.random.RandomWordAug</code></h4>
<ul>
<li><a href="https://nlpaug.readthedocs.io/en/latest/augmenter/word/random.html#nlpaug.augmenter.word.random.RandomWordAug">Documentation</a></li>
<li>Randomly apply substitute, swap, delete or crop augmentation
<ul>
<li>The default augmentation is to delete words.</li>
</ul></li>
</ul>
<p><strong>Randomly swap words</strong></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_swap"</span>] <span class="op">=</span> naw.RandomWordAug(action<span class="op">=</span><span class="st">"swap"</span>)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_swap"</span>].augment(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'You even if defeat me Megatron, others will rise defeat to tyranny your'</code></pre>
<hr>
<p><strong>Randomly delete words</strong></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_delete"</span>] <span class="op">=</span> naw.RandomWordAug()</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"random_delete"</span>].augment(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'Even if you me Megatron, others to defeat tyranny'</code></pre>
<hr>
</section>
<section id="nlpaug.augmenter.word.back_translation.backtranslationaug" class="level4">
<h4 class="anchored" data-anchor-id="nlpaug.augmenter.word.back_translation.backtranslationaug"><code>nlpaug.augmenter.word.back_translation.BackTranslationAug</code></h4>
<ul>
<li><a href="https://nlpaug.readthedocs.io/en/latest/augmenter/word/back_translation.html#nlpaug.augmenter.word.back_translation.BackTranslationAug">Documentation</a></li>
<li>Use two translation models to apply back translation</li>
</ul>
<hr>
<div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"bt_en_de"</span>] <span class="op">=</span> naw.BackTranslationAug(</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>    from_model_name<span class="op">=</span><span class="st">'facebook/wmt19-en-de'</span>, </span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>    to_model_name<span class="op">=</span><span class="st">'facebook/wmt19-de-en'</span></span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>augs[<span class="st">"bt_en_de"</span>].augment(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'Even if you defeat me, others will rise up to defeat your tyranny'</code></pre>
<hr>
<div class="sourceCode" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k,v <span class="kw">in</span> augs.items():</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">.</span>augment(text)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny
    synonym_replace: Even if you defeat me Megatron, others will go up to vote out your tyranny
    
    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny
    random_insert: even if you defeat me megatron, others will rise to defeat of your tyranny
    
    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny
    random_swap: If even you defeat me Megatron, others will rise to defeat tyranny your
    
    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny
    random_delete: If you me Megatron, will to defeat your tyranny
    
    Original text: Even if you defeat me Megatron, others will rise to defeat your tyranny
    bt_en_de: Even if you defeat me, others will rise up to defeat your tyranny</code></pre>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Add Random Synonym Replacement using the contextual word embeddings of DistilBERT</strong></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>aug <span class="op">=</span> naw.ContextualWordEmbsAug(model_path<span class="op">=</span><span class="st">"distilbert-base-uncased"</span>, </span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>                                device<span class="op">=</span><span class="st">"cpu"</span>, action<span class="op">=</span><span class="st">"substitute"</span>)</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Transformers are the most popular toys"</span></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Augmented text: </span><span class="sc">{</span>aug<span class="sc">.</span>augment(text)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Original text: Transformers are the most popular toys
    Augmented text: transformers'the most popular toys</code></pre>
<hr>
<p><strong>Define a function to apply synonym replacement augmentation to a batch</strong></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augment_text(batch, transformations_per_example<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>    text_aug, label_ids <span class="op">=</span> [], []</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text, labels <span class="kw">in</span> <span class="bu">zip</span>(batch[<span class="st">"text"</span>], batch[<span class="st">"label_ids"</span>]):</span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>        text_aug <span class="op">+=</span> [text]</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>        label_ids <span class="op">+=</span> [labels]</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(transformations_per_example):</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>            text_aug <span class="op">+=</span> [aug.augment(text)]</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>            label_ids <span class="op">+=</span> [labels]</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"text"</span>: text_aug, <span class="st">"label_ids"</span>: label_ids}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the baseline Naive Bayes Classifier using synonym replacement augmentation</strong></p>
<div class="sourceCode" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get training slice and test data</span></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>    ds_train_sample <span class="op">=</span> ds[<span class="st">"train"</span>].select(train_slice)</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten augmentations and align labels!</span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>    ds_train_aug <span class="op">=</span> (ds_train_sample.<span class="bu">map</span>(</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>        augment_text, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>ds_train_sample.column_names)</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>                    .shuffle(seed<span class="op">=</span><span class="dv">42</span>))</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> np.array(ds_train_aug[<span class="st">"label_ids"</span>])</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.array(ds[<span class="st">"test"</span>][<span class="st">"label_ids"</span>])</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use a simple count vectorizer to encode our texts as token counts</span></span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>    count_vect <span class="op">=</span> CountVectorizer()</span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a>    X_train_counts <span class="op">=</span> count_vect.fit_transform(ds_train_aug[<span class="st">"text"</span>])</span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a>    X_test_counts <span class="op">=</span> count_vect.transform(ds[<span class="st">"test"</span>][<span class="st">"text"</span>])</span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create and train our model!</span></span>
<span id="cb129-15"><a href="#cb129-15" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> BinaryRelevance(classifier<span class="op">=</span>MultinomialNB())</span>
<span id="cb129-16"><a href="#cb129-16" aria-hidden="true" tabindex="-1"></a>    classifier.fit(X_train_counts, y_train)</span>
<span id="cb129-17"><a href="#cb129-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate predictions and evaluate</span></span>
<span id="cb129-18"><a href="#cb129-18" aria-hidden="true" tabindex="-1"></a>    y_pred_test <span class="op">=</span> classifier.predict(X_test_counts)</span>
<span id="cb129-19"><a href="#cb129-19" aria-hidden="true" tabindex="-1"></a>    clf_report <span class="op">=</span> classification_report(</span>
<span id="cb129-20"><a href="#cb129-20" aria-hidden="true" tabindex="-1"></a>        y_test, y_pred_test, target_names<span class="op">=</span>mlb.classes_, zero_division<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb129-21"><a href="#cb129-21" aria-hidden="true" tabindex="-1"></a>        output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb129-22"><a href="#cb129-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store metrics</span></span>
<span id="cb129-23"><a href="#cb129-23" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">"Naive Bayes + Aug"</span>].append(clf_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>])</span>
<span id="cb129-24"><a href="#cb129-24" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">"Naive Bayes + Aug"</span>].append(clf_report[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare the results</strong></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Naive Bayes + Aug"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_191_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>A small amount of data augmentation improves the F1 score of the Naive Bayes Classifier.</li>
<li>The Naive Bayes Classifier overtakes the zero-shot pipeline for the macro F1 score at around 220 training samples.</li>
</ul>
<hr>
</section>
</section>
<section id="using-embeddings-as-a-lookup-table" class="level3">
<h3 class="anchored" data-anchor-id="using-embeddings-as-a-lookup-table">Using Embeddings as a Lookup Table</h3>
<ul>
<li><p>Large language models like GPT-3 are excellent at solving tasks with limited data because they learn representations of text that encode information across many dimensions.</p></li>
<li><p>We can use embeddings of large language models to develop a semantic search engine, find similar documents or comments, or classify text.</p></li>
<li><p>This approach does not require fine-tuning models to leverage the few labeled data points.</p></li>
<li><p>The embeddings should ideally be from a pretrained on a similar domain to the target dataset.</p></li>
<li><p><a href="https://beta.openai.com/docs/api-reference/classifications">OpenAI API Classification Endopint</a></p></li>
</ul>
<section id="steps-to-classify-text-using-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="steps-to-classify-text-using-embeddings">Steps to classify text using embeddings:</h4>
<ol type="1">
<li>Use the language model to embed all labeled texts.</li>
<li>Perform a nearest-neighbor search over the stored embeddings.</li>
<li>Aggregate the labels of the nearest neighbors to get a prediction.</li>
</ol>
<ul>
<li>We need to embed new text we want to classify and assign labels based on the labels of its nearest neighbors.</li>
<li>It is crucial to calibrate the number of neighbors for the nearest-neighbors search.
<ul>
<li>Using too few neighbors might result in noisy predictions.</li>
<li>Using too many neighbors might mix neighboring groups.</li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Instantiate a tokenizer and model using a GPT-2 checkpoint trained on Python code</strong></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"miguelvictor/python-gpt2-large"</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> Transformer models like GPT-2 return one embedding vector per token, and we want a single embedding for the entire output.</p>
<hr>
<p><strong>Define a function to create a single-vector representation for model output using average pooling</strong></p>
<ul>
<li>We don’t want to include padding tokens in the average.</li>
</ul>
<hr>
<div class="sourceCode" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_pooling(model_output, attention_mask):</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the token embeddings</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>    token_embeddings <span class="op">=</span> model_output[<span class="dv">0</span>]</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the attention mask</span></span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>    input_mask_expanded <span class="op">=</span> (attention_mask</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>                           .unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>                           .expand(token_embeddings.size())</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>                           .<span class="bu">float</span>())</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sum the embeddings, but ignore masked tokens</span></span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>    sum_embeddings <span class="op">=</span> torch.<span class="bu">sum</span>(token_embeddings <span class="op">*</span> input_mask_expanded, <span class="dv">1</span>)</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>    sum_mask <span class="op">=</span> torch.clamp(input_mask_expanded.<span class="bu">sum</span>(<span class="dv">1</span>), <span class="bu">min</span><span class="op">=</span><span class="fl">1e-9</span>)</span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the average as a single vector</span></span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sum_embeddings <span class="op">/</span> sum_mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to embed sample text</strong></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_text(examples):</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(examples[<span class="st">"text"</span>], padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>                       max_length<span class="op">=</span><span class="dv">128</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>        model_output <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>    pooled_embeds <span class="op">=</span> mean_pooling(model_output, inputs[<span class="st">"attention_mask"</span>])</span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"embedding"</span>: pooled_embeds.cpu().numpy()}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Use the end-of-string token as the padding token since GPT-style models don’t have one</strong></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Get the embeddings for each split</strong></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>embs_train <span class="op">=</span> ds[<span class="st">"train"</span>].<span class="bu">map</span>(embed_text, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>embs_valid <span class="op">=</span> ds[<span class="st">"valid"</span>].<span class="bu">map</span>(embed_text, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>embs_test <span class="op">=</span> ds[<span class="st">"test"</span>].<span class="bu">map</span>(embed_text, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="dataset.add_faiss_index" class="level4">
<h4 class="anchored" data-anchor-id="dataset.add_faiss_index"><code>Dataset.add_faiss_index</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.add_faiss_index">Documentation</a></li>
<li>Add a dense index using FAISS for fast retrieval.</li>
<li>FAISS is a library for efficient similarity search of dense vectors.
<ul>
<li><a href="https://github.com/facebookresearch/faiss">GitHub Repository</a></li>
<li><a href="https://arxiv.org/abs/1702.08734">Billion-scale similarity search with GPUs</a></li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> faiss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Create a FAISS index using the embeddings for the train split</strong></p>
<div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>embs_train.add_faiss_index(<span class="st">"embedding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Dataset({
        features: ['text', 'labels', 'label_ids', 'embedding'],
        num_rows: 223
    })</code></pre>
<hr>
</section>
<section id="datasets.search.indexablemixin.get_nearest_examples" class="level4">
<h4 class="anchored" data-anchor-id="datasets.search.indexablemixin.get_nearest_examples"><code>datasets.search.IndexableMixin.get_nearest_examples</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples">Documentation</a></li>
<li>Find the nearest examples in the dataset to the query.</li>
</ul>
<p><strong>Perform a nearest-neighbor lookup</strong></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>i, k <span class="op">=</span> <span class="dv">0</span>, <span class="dv">3</span> <span class="co"># Select the first query and 3 nearest neighbors</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>rn, nl <span class="op">=</span> <span class="st">"</span><span class="ch">\r\n\r\n</span><span class="st">"</span>, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="co"># Used to remove newlines in text for compact display</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span>  np.array(embs_valid[i][<span class="st">"embedding"</span>], dtype<span class="op">=</span>np.float32)</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>scores, samples <span class="op">=</span> embs_train.get_nearest_examples(<span class="st">"embedding"</span>, query, k<span class="op">=</span>k)</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"QUERY LABELS: </span><span class="sc">{</span>embs_valid[i][<span class="st">'labels'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"QUERY TEXT:</span><span class="ch">\n</span><span class="sc">{</span>embs_valid[i][<span class="st">'text'</span>][:<span class="dv">200</span>]<span class="sc">.</span>replace(rn, nl)<span class="sc">}</span><span class="ss"> [...]</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Retrieved documents:"</span>)</span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> score, label, text <span class="kw">in</span> <span class="bu">zip</span>(scores, samples[<span class="st">"labels"</span>], samples[<span class="st">"text"</span>]):</span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"TEXT:</span><span class="ch">\n</span><span class="sc">{</span>text[:<span class="dv">200</span>]<span class="sc">.</span>replace(rn, nl)<span class="sc">}</span><span class="ss"> [...]"</span>)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"SCORE: </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"LABELS: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    QUERY LABELS: ['new model']
    QUERY TEXT:
    Implementing efficient self attention in T5
    
    # 🌟 New model addition
    My teammates and I (including @ice-americano) would like to use efficient self attention methods such as Linformer, Performer and [...]
    
    ==================================================
    Retrieved documents:
    ==================================================
    TEXT:
    Add Linformer model
    
    # 🌟 New model addition
    ## Model description
    ### Linformer: Self-Attention with Linear Complexity
    Paper published June 9th on ArXiv: https://arxiv.org/abs/2006.04768
    La [...]
    SCORE: 54.92
    LABELS: ['new model']
    ==================================================
    TEXT:
    Add FAVOR+ / Performer attention
    
    # 🌟 FAVOR+ / Performer attention addition
    Are there any plans to add this new attention approximation block to Transformers library?
    ## Model description
    The n [...]
    SCORE: 57.90
    LABELS: ['new model']
    ==================================================
    TEXT:
    Implement DeLighT: Very Deep and Light-weight Transformers
    
    # 🌟 New model addition
    ## Model description
    DeLight, that delivers similar or better performance than transformer-based models with sign [...]
    SCORE: 60.12
    LABELS: ['new model']</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>The three retrieved documents all have the same labels as they should.</li>
<li>The query and the retrieved documents all relate to adding new and efficient transformer models.</li>
</ul>
<hr>
<p><strong>Define a function that returns the sample predictions using a label occurrence threshold</strong></p>
<div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sample_preds(sample, m):</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.<span class="bu">sum</span>(sample[<span class="st">"label_ids"</span>], axis<span class="op">=</span><span class="dv">0</span>) <span class="op">&gt;=</span> m).astype(<span class="bu">int</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="datasets.search.indexablemixin.get_nearest_examples_batch" class="level4">
<h4 class="anchored" data-anchor-id="datasets.search.indexablemixin.get_nearest_examples_batch"><code>datasets.search.IndexableMixin.get_nearest_examples_batch</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples_batch">Documentation</a></li>
</ul>
<p><strong>Define a function to test different k and threshold values for nearest-neighbor search</strong></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_best_k_m(ds_train, valid_queries, valid_labels, max_k<span class="op">=</span><span class="dv">17</span>):</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>    max_k <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(ds_train), max_k)</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>    perf_micro <span class="op">=</span> np.zeros((max_k, max_k))</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>    perf_macro <span class="op">=</span> np.zeros((max_k, max_k))</span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_k):</span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>            _, samples <span class="op">=</span> ds_train.get_nearest_examples_batch(<span class="st">"embedding"</span>,</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a>                                                             valid_queries, k<span class="op">=</span>k)</span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> np.array([get_sample_preds(s, m) <span class="cf">for</span> s <span class="kw">in</span> samples])</span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a>            clf_report <span class="op">=</span> classification_report(valid_labels, y_pred,</span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a>                target_names<span class="op">=</span>mlb.classes_, zero_division<span class="op">=</span><span class="dv">0</span>, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a>            perf_micro[k, m] <span class="op">=</span> clf_report[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>]</span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a>            perf_macro[k, m] <span class="op">=</span> clf_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>]</span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> perf_micro, perf_macro</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test different k and threshold values</strong></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>valid_labels <span class="op">=</span> np.array(embs_valid[<span class="st">"label_ids"</span>])</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>valid_queries <span class="op">=</span> np.array(embs_valid[<span class="st">"embedding"</span>], dtype<span class="op">=</span>np.float32)</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>perf_micro, perf_macro <span class="op">=</span> find_best_k_m(embs_train, valid_queries, valid_labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Plot the results</strong></p>
<div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>fig, (ax0, ax1) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="fl">3.5</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>ax0.imshow(perf_micro)</span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>ax1.imshow(perf_macro)</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>ax0.set_title(<span class="st">"micro scores"</span>)</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>ax0.set_ylabel(<span class="st">"k"</span>)</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">"macro scores"</span>)</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> [ax0, ax1]:</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim([<span class="fl">0.5</span>, <span class="dv">17</span> <span class="op">-</span> <span class="fl">0.5</span>])</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim([<span class="dv">17</span> <span class="op">-</span> <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"m"</span>)</span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_222_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>Choosing a threshold value <span class="math inline">\(m\)</span> that is too large or small for a given <span class="math inline">\(k\)</span> value yields suboptimal results.</li>
<li>A ratio of approximately <span class="math inline">\(m/k = 1/3\)</span> achieves the best results.</li>
</ul>
<hr>
<p><strong>Find the best k and threshold values</strong></p>
<div class="sourceCode" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>k, m <span class="op">=</span> np.unravel_index(perf_micro.argmax(), perf_micro.shape)</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best k: </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">, best m: </span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Best k: 15, best m: 5</code></pre>
<p><strong>Note:</strong> We get the best performance when we retrieve the 15 nearest neighbors and then assign the labels that occurred at least five times.</p>
<hr>
<p><strong>Evaluate the embedding lookup performance using different training slices</strong></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the FAISS index</span></span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>embs_train.drop_index(<span class="st">"embedding"</span>)</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> np.array(embs_test[<span class="st">"label_ids"</span>])</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>test_queries <span class="op">=</span> np.array(embs_test[<span class="st">"embedding"</span>], dtype<span class="op">=</span>np.float32)</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a FAISS index from training slice </span></span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a>    embs_train_tmp <span class="op">=</span> embs_train.select(train_slice)</span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>    embs_train_tmp.add_faiss_index(<span class="st">"embedding"</span>)</span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get best k, m values with validation set</span></span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a>    perf_micro, _ <span class="op">=</span> find_best_k_m(embs_train_tmp, valid_queries, valid_labels)</span>
<span id="cb148-12"><a href="#cb148-12" aria-hidden="true" tabindex="-1"></a>    k, m <span class="op">=</span> np.unravel_index(perf_micro.argmax(), perf_micro.shape)</span>
<span id="cb148-13"><a href="#cb148-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions on test set</span></span>
<span id="cb148-14"><a href="#cb148-14" aria-hidden="true" tabindex="-1"></a>    _, samples <span class="op">=</span> embs_train_tmp.get_nearest_examples_batch(<span class="st">"embedding"</span>,</span>
<span id="cb148-15"><a href="#cb148-15" aria-hidden="true" tabindex="-1"></a>                                                           test_queries,</span>
<span id="cb148-16"><a href="#cb148-16" aria-hidden="true" tabindex="-1"></a>                                                           k<span class="op">=</span><span class="bu">int</span>(k))</span>
<span id="cb148-17"><a href="#cb148-17" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.array([get_sample_preds(s, m) <span class="cf">for</span> s <span class="kw">in</span> samples])</span>
<span id="cb148-18"><a href="#cb148-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate predictions</span></span>
<span id="cb148-19"><a href="#cb148-19" aria-hidden="true" tabindex="-1"></a>    clf_report <span class="op">=</span> classification_report(test_labels, y_pred,</span>
<span id="cb148-20"><a href="#cb148-20" aria-hidden="true" tabindex="-1"></a>        target_names<span class="op">=</span>mlb.classes_, zero_division<span class="op">=</span><span class="dv">0</span>, output_dict<span class="op">=</span><span class="va">True</span>,)</span>
<span id="cb148-21"><a href="#cb148-21" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">"Embedding"</span>].append(clf_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>])</span>
<span id="cb148-22"><a href="#cb148-22" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">"Embedding"</span>].append(clf_report[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare performance to previous methods</strong></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Embedding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_230_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong></p>
<ul>
<li>The embedding lookup is competitive on the micro F1 scores while only having two “learnable” parameters, k and m, but performs slightly worse on the macro scores.</li>
<li>The method that works best in practice strongly depends on the domain.</li>
<li>The zero-shot pipeline might work much better on tasks closer to the pretraining domain.</li>
<li>The embeddings quality depends on the model and the original training data.</li>
</ul>
<hr>
</section>
</section>
<section id="efficient-similarity-search-with-faiss" class="level3">
<h3 class="anchored" data-anchor-id="efficient-similarity-search-with-faiss">Efficient Similarity Search with FAISS</h3>
<ul>
<li>We usually speed up text search by creating an inverted index that maps terms to documents.</li>
<li>An inverted index works just like an index at the end of a book, where each word maps to the pages/documents it occurs in.</li>
<li>We can quickly find which documents the search terms appear in when performing a query.</li>
<li>An inverted index works well with discrete objects such as words but does not work with continuous ones like vectors.</li>
<li>We need to look for similar matches instead of exact matches since each document likely has a unique vector.</li>
<li>FAISS avoids comparing the query vector to every vector in the database with several tricks.</li>
<li>FAISS speeds up the comparison process by applying k-means clustering to the dataset, grouping the embeddings by similarity.</li>
<li>We get a centroid vector for each group, which is the average of all group members.</li>
<li>We can then search across the k centroids for the one that is most similar to our query and then search within the corresponding group.</li>
<li>This approach reduces the number of comparisons from n to <span class="math inline">\(k + \frac{n}{k}\)</span>.
<ul>
<li>The minimum k value is <span class="math inline">\(k = \sqrt{n}\)</span>.</li>
</ul></li>
<li>FAISS also provides a GPU-enabled version for increased speed and several options to compress vectors with advanced quantization schemes.</li>
<li><a href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index">Guidelines to choose an index</a></li>
</ul>
</section>
<section id="fine-tuning-a-vanilla-transformer" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-a-vanilla-transformer">Fine-Tuning a Vanilla Transformer</h3>
<ul>
<li>We can fine-tune a pretrained transformer model when we have labeled data.</li>
<li>Starting with a pretrained BERT-like model is often a good idea.</li>
<li>The target corpus should not be too different from the pretraining corpus.</li>
<li>The Hugging Face hub has many models pretrained on different corpora.</li>
</ul>
<hr>
<div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (AutoTokenizer, AutoConfig,</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>                          AutoModelForSequenceClassification)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Load the pretrained tokenizer for the standard BERT checkpoint</strong></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"bert-base-uncased"</span></span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_ckpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Tokenize the dataset</strong></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(batch):</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(batch[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>ds_enc <span class="op">=</span> ds.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>ds_enc <span class="op">=</span> ds_enc.remove_columns([<span class="st">'labels'</span>, <span class="st">'text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Change the <code>label_ids</code> column data type to float for the multilabel loss function</strong></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>ds_enc.set_format(<span class="st">"torch"</span>)</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>ds_enc <span class="op">=</span> ds_enc.<span class="bu">map</span>(<span class="kw">lambda</span> x: {<span class="st">"label_ids_f"</span>: x[<span class="st">"label_ids"</span>].to(torch.<span class="bu">float</span>)},</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a>                    remove_columns<span class="op">=</span>[<span class="st">"label_ids"</span>])</span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>ds_enc <span class="op">=</span> ds_enc.rename_column(<span class="st">"label_ids_f"</span>, <span class="st">"label_ids"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Keep the best model based on the micro <span class="math inline">\(F_{1}\)</span>-score</strong></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>training_args_fine_tune <span class="op">=</span> TrainingArguments(</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./results"</span>, num_train_epochs<span class="op">=</span><span class="dv">20</span>, learning_rate<span class="op">=</span><span class="fl">3e-5</span>,</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">'constant'</span>, per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">32</span>, weight_decay<span class="op">=</span><span class="fl">0.0</span>, </span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a>    evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>, save_strategy<span class="op">=</span><span class="st">"epoch"</span>,logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>, metric_for_best_model<span class="op">=</span><span class="st">'micro f1'</span>,</span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">1</span>, log_level<span class="op">=</span><span class="st">'error'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> expit <span class="im">as</span> sigmoid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Define a function to compute the <span class="math inline">\(F_{1}\)</span>-scores</strong></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(pred):</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> pred.label_ids</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize the model predictions</span></span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> sigmoid(pred.predictions)</span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> (y_pred<span class="op">&gt;</span><span class="fl">0.5</span>).astype(<span class="bu">float</span>)</span>
<span id="cb157-6"><a href="#cb157-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb157-7"><a href="#cb157-7" aria-hidden="true" tabindex="-1"></a>    clf_dict <span class="op">=</span> classification_report(y_true, y_pred, target_names<span class="op">=</span>all_labels,</span>
<span id="cb157-8"><a href="#cb157-8" aria-hidden="true" tabindex="-1"></a>                                     zero_division<span class="op">=</span><span class="dv">0</span>, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb157-9"><a href="#cb157-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"micro f1"</span>: clf_dict[<span class="st">"micro avg"</span>][<span class="st">"f1-score"</span>],</span>
<span id="cb157-10"><a href="#cb157-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"macro f1"</span>: clf_dict[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>]}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Intitialize a BertConfig for Multi-label classification</strong></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_ckpt)</span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>config.num_labels <span class="op">=</span> <span class="bu">len</span>(all_labels)</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>config.problem_type <span class="op">=</span> <span class="st">"multi_label_classification"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    BertConfig {
      "_name_or_path": "bert-base-uncased",
      "architectures": [
        "BertForMaskedLM"
      ],
      "attention_probs_dropout_prob": 0.1,
      "classifier_dropout": null,
      "gradient_checkpointing": false,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1",
        "2": "LABEL_2",
        "3": "LABEL_3",
        "4": "LABEL_4",
        "5": "LABEL_5",
        "6": "LABEL_6",
        "7": "LABEL_7",
        "8": "LABEL_8"
      },
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1,
        "LABEL_2": 2,
        "LABEL_3": 3,
        "LABEL_4": 4,
        "LABEL_5": 5,
        "LABEL_6": 6,
        "LABEL_7": 7,
        "LABEL_8": 8
      },
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "bert",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "pad_token_id": 0,
      "position_embedding_type": "absolute",
      "problem_type": "multi_label_classification",
      "transformers_version": "4.18.0",
      "type_vocab_size": 2,
      "use_cache": true,
      "vocab_size": 30522
    }</code></pre>
<hr>
<p><strong>Train a classifier from scratch for each training slice</strong></p>
<div class="sourceCode" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_ckpt,</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>                                                               config<span class="op">=</span>config).to(device)</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> Trainer(</span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>training_args_fine_tune,</span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a>        compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a>        train_dataset<span class="op">=</span>ds_enc[<span class="st">"train"</span>].select(train_slice),</span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a>        eval_dataset<span class="op">=</span>ds_enc[<span class="st">"valid"</span>])</span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb161-13"><a href="#cb161-13" aria-hidden="true" tabindex="-1"></a>    old_collator <span class="op">=</span> trainer.data_collator</span>
<span id="cb161-14"><a href="#cb161-14" aria-hidden="true" tabindex="-1"></a>    trainer.data_collator <span class="op">=</span> <span class="kw">lambda</span> data: <span class="bu">dict</span>(old_collator(data))</span>
<span id="cb161-15"><a href="#cb161-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-16"><a href="#cb161-16" aria-hidden="true" tabindex="-1"></a>    trainer.train()</span>
<span id="cb161-17"><a href="#cb161-17" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> trainer.predict(ds_enc[<span class="st">"test"</span>])</span>
<span id="cb161-18"><a href="#cb161-18" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> compute_metrics(pred)</span>
<span id="cb161-19"><a href="#cb161-19" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">"Fine-tune (vanilla)"</span>].append(metrics[<span class="st">"macro f1"</span>])</span>
<span id="cb161-20"><a href="#cb161-20" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">"Fine-tune (vanilla)"</span>].append(metrics[<span class="st">"micro f1"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Compare the results to previous methods</strong></p>
<div class="sourceCode" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Fine-tune (vanilla)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_253_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The fine-tuned model is competitive when we have at least 64 training examples.</p>
<hr>
</section>
<section id="in-context-and-few-shot-learning-with-prompts" class="level3">
<h3 class="anchored" data-anchor-id="in-context-and-few-shot-learning-with-prompts">In-Context and Few-Shot Learning with Prompts</h3>
<ul>
<li>The GPT-3 paper found that large-language models can effectively learn from examples presented in a prompt.
<ul>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
</ul></li>
<li>The larger the model, the better it is at using in-context examples.</li>
<li>An alternative to using labeled data is to create examples of the prompts and desired predictions and continue training the language model on those examples.
<ul>
<li>ADAPET beats GPT-3 on a wide variety of tasks using this approach.
<ul>
<li><a href="https://arxiv.org/abs/2103.11955">Improving and Simplifying Pattern Exploiting Training</a></li>
</ul></li>
<li>Researchers at Hugging Face found that this approach can be more data-efficient than fine-tuning a custom head.
<ul>
<li><a href="https://arxiv.org/abs/2103.08493">How Many Data Points is a Prompt Worth?</a></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="leveraging-unlabeled-data" class="level2">
<h2 class="anchored" data-anchor-id="leveraging-unlabeled-data">Leveraging Unlabeled Data</h2>
<ul>
<li>Domain adaptation involves continuing the pretraining process of predicting masked words using unlabeled data from the target domain.</li>
<li>We can then reuse the adapted model for many use cases.</li>
<li>Domain adaptation can help boost model performance with unlabeled data and little effort.</li>
</ul>
<section id="fine-tuning-a-language-model" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-a-language-model">Fine-Tuning a Language Model</h3>
<ul>
<li>We need to mask the <code>[CLS]</code> and <code>[SEP]</code> tokens from the loss, so we don’t train the model to predict them when doing masked language modeling.</li>
<li>We can get a mask when tokenizing by setting <code>return_special_tokens_mask=True</code>.</li>
<li>We can use a <a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling">specialized data collator</a> to prepare the elements in a batch for masked language modeling on the fly.</li>
<li>The data collator needs to mask tokens in the input sequence and have the target tokens in the outputs.</li>
</ul>
<p><strong>Define a function to tokenize the text and get the special token mask information</strong></p>
<div class="sourceCode" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(batch):</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(batch[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a>                     max_length<span class="op">=</span><span class="dv">128</span>, return_special_tokens_mask<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Retokenize the text</strong></p>
<div class="sourceCode" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>ds_mlm <span class="op">=</span> ds.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a>ds_mlm <span class="op">=</span> ds_mlm.remove_columns([<span class="st">"labels"</span>, <span class="st">"text"</span>, <span class="st">"label_ids"</span>])</span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>ds_mlm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    DatasetDict({
        train: Dataset({
            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],
            num_rows: 223
        })
        valid: Dataset({
            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],
            num_rows: 106
        })
        test: Dataset({
            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],
            num_rows: 111
        })
        unsup: Dataset({
            features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],
            num_rows: 9303
        })
    })</code></pre>
<hr>
<div class="sourceCode" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(ds_mlm[<span class="st">'train'</span>][<span class="st">'special_tokens_mask'</span>][<span class="dv">0</span>]).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>).hide(axis<span class="op">=</span><span class="st">'rows'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table id="T_b3216">
<thead>
</thead>
<tbody>
<tr>
<td id="T_b3216_row0_col0" class="data row0 col0">
1
</td>
<td id="T_b3216_row0_col1" class="data row0 col1">
0
</td>
<td id="T_b3216_row0_col2" class="data row0 col2">
0
</td>
<td id="T_b3216_row0_col3" class="data row0 col3">
0
</td>
<td id="T_b3216_row0_col4" class="data row0 col4">
0
</td>
<td id="T_b3216_row0_col5" class="data row0 col5">
0
</td>
<td id="T_b3216_row0_col6" class="data row0 col6">
0
</td>
<td id="T_b3216_row0_col7" class="data row0 col7">
0
</td>
<td id="T_b3216_row0_col8" class="data row0 col8">
0
</td>
<td id="T_b3216_row0_col9" class="data row0 col9">
0
</td>
<td id="T_b3216_row0_col10" class="data row0 col10">
0
</td>
<td id="T_b3216_row0_col11" class="data row0 col11">
0
</td>
<td id="T_b3216_row0_col12" class="data row0 col12">
0
</td>
<td id="T_b3216_row0_col13" class="data row0 col13">
0
</td>
<td id="T_b3216_row0_col14" class="data row0 col14">
0
</td>
<td id="T_b3216_row0_col15" class="data row0 col15">
0
</td>
<td id="T_b3216_row0_col16" class="data row0 col16">
0
</td>
<td id="T_b3216_row0_col17" class="data row0 col17">
0
</td>
<td id="T_b3216_row0_col18" class="data row0 col18">
0
</td>
<td id="T_b3216_row0_col19" class="data row0 col19">
0
</td>
<td id="T_b3216_row0_col20" class="data row0 col20">
0
</td>
<td id="T_b3216_row0_col21" class="data row0 col21">
0
</td>
<td id="T_b3216_row0_col22" class="data row0 col22">
0
</td>
<td id="T_b3216_row0_col23" class="data row0 col23">
0
</td>
<td id="T_b3216_row0_col24" class="data row0 col24">
0
</td>
<td id="T_b3216_row0_col25" class="data row0 col25">
0
</td>
<td id="T_b3216_row0_col26" class="data row0 col26">
0
</td>
<td id="T_b3216_row0_col27" class="data row0 col27">
0
</td>
<td id="T_b3216_row0_col28" class="data row0 col28">
0
</td>
<td id="T_b3216_row0_col29" class="data row0 col29">
0
</td>
<td id="T_b3216_row0_col30" class="data row0 col30">
0
</td>
<td id="T_b3216_row0_col31" class="data row0 col31">
0
</td>
<td id="T_b3216_row0_col32" class="data row0 col32">
0
</td>
<td id="T_b3216_row0_col33" class="data row0 col33">
0
</td>
<td id="T_b3216_row0_col34" class="data row0 col34">
0
</td>
<td id="T_b3216_row0_col35" class="data row0 col35">
0
</td>
<td id="T_b3216_row0_col36" class="data row0 col36">
0
</td>
<td id="T_b3216_row0_col37" class="data row0 col37">
0
</td>
<td id="T_b3216_row0_col38" class="data row0 col38">
0
</td>
<td id="T_b3216_row0_col39" class="data row0 col39">
0
</td>
<td id="T_b3216_row0_col40" class="data row0 col40">
0
</td>
<td id="T_b3216_row0_col41" class="data row0 col41">
0
</td>
<td id="T_b3216_row0_col42" class="data row0 col42">
0
</td>
<td id="T_b3216_row0_col43" class="data row0 col43">
0
</td>
<td id="T_b3216_row0_col44" class="data row0 col44">
0
</td>
<td id="T_b3216_row0_col45" class="data row0 col45">
0
</td>
<td id="T_b3216_row0_col46" class="data row0 col46">
0
</td>
<td id="T_b3216_row0_col47" class="data row0 col47">
0
</td>
<td id="T_b3216_row0_col48" class="data row0 col48">
0
</td>
<td id="T_b3216_row0_col49" class="data row0 col49">
0
</td>
<td id="T_b3216_row0_col50" class="data row0 col50">
0
</td>
<td id="T_b3216_row0_col51" class="data row0 col51">
0
</td>
<td id="T_b3216_row0_col52" class="data row0 col52">
0
</td>
<td id="T_b3216_row0_col53" class="data row0 col53">
0
</td>
<td id="T_b3216_row0_col54" class="data row0 col54">
0
</td>
<td id="T_b3216_row0_col55" class="data row0 col55">
0
</td>
<td id="T_b3216_row0_col56" class="data row0 col56">
0
</td>
<td id="T_b3216_row0_col57" class="data row0 col57">
0
</td>
<td id="T_b3216_row0_col58" class="data row0 col58">
0
</td>
<td id="T_b3216_row0_col59" class="data row0 col59">
0
</td>
<td id="T_b3216_row0_col60" class="data row0 col60">
0
</td>
<td id="T_b3216_row0_col61" class="data row0 col61">
0
</td>
<td id="T_b3216_row0_col62" class="data row0 col62">
0
</td>
<td id="T_b3216_row0_col63" class="data row0 col63">
0
</td>
<td id="T_b3216_row0_col64" class="data row0 col64">
0
</td>
<td id="T_b3216_row0_col65" class="data row0 col65">
0
</td>
<td id="T_b3216_row0_col66" class="data row0 col66">
0
</td>
<td id="T_b3216_row0_col67" class="data row0 col67">
0
</td>
<td id="T_b3216_row0_col68" class="data row0 col68">
0
</td>
<td id="T_b3216_row0_col69" class="data row0 col69">
0
</td>
<td id="T_b3216_row0_col70" class="data row0 col70">
0
</td>
<td id="T_b3216_row0_col71" class="data row0 col71">
0
</td>
<td id="T_b3216_row0_col72" class="data row0 col72">
0
</td>
<td id="T_b3216_row0_col73" class="data row0 col73">
0
</td>
<td id="T_b3216_row0_col74" class="data row0 col74">
0
</td>
<td id="T_b3216_row0_col75" class="data row0 col75">
0
</td>
<td id="T_b3216_row0_col76" class="data row0 col76">
0
</td>
<td id="T_b3216_row0_col77" class="data row0 col77">
0
</td>
<td id="T_b3216_row0_col78" class="data row0 col78">
0
</td>
<td id="T_b3216_row0_col79" class="data row0 col79">
0
</td>
<td id="T_b3216_row0_col80" class="data row0 col80">
0
</td>
<td id="T_b3216_row0_col81" class="data row0 col81">
0
</td>
<td id="T_b3216_row0_col82" class="data row0 col82">
0
</td>
<td id="T_b3216_row0_col83" class="data row0 col83">
0
</td>
<td id="T_b3216_row0_col84" class="data row0 col84">
0
</td>
<td id="T_b3216_row0_col85" class="data row0 col85">
0
</td>
<td id="T_b3216_row0_col86" class="data row0 col86">
0
</td>
<td id="T_b3216_row0_col87" class="data row0 col87">
0
</td>
<td id="T_b3216_row0_col88" class="data row0 col88">
0
</td>
<td id="T_b3216_row0_col89" class="data row0 col89">
0
</td>
<td id="T_b3216_row0_col90" class="data row0 col90">
0
</td>
<td id="T_b3216_row0_col91" class="data row0 col91">
0
</td>
<td id="T_b3216_row0_col92" class="data row0 col92">
0
</td>
<td id="T_b3216_row0_col93" class="data row0 col93">
0
</td>
<td id="T_b3216_row0_col94" class="data row0 col94">
0
</td>
<td id="T_b3216_row0_col95" class="data row0 col95">
0
</td>
<td id="T_b3216_row0_col96" class="data row0 col96">
0
</td>
<td id="T_b3216_row0_col97" class="data row0 col97">
0
</td>
<td id="T_b3216_row0_col98" class="data row0 col98">
0
</td>
<td id="T_b3216_row0_col99" class="data row0 col99">
0
</td>
<td id="T_b3216_row0_col100" class="data row0 col100">
0
</td>
<td id="T_b3216_row0_col101" class="data row0 col101">
0
</td>
<td id="T_b3216_row0_col102" class="data row0 col102">
0
</td>
<td id="T_b3216_row0_col103" class="data row0 col103">
0
</td>
<td id="T_b3216_row0_col104" class="data row0 col104">
0
</td>
<td id="T_b3216_row0_col105" class="data row0 col105">
0
</td>
<td id="T_b3216_row0_col106" class="data row0 col106">
0
</td>
<td id="T_b3216_row0_col107" class="data row0 col107">
0
</td>
<td id="T_b3216_row0_col108" class="data row0 col108">
0
</td>
<td id="T_b3216_row0_col109" class="data row0 col109">
0
</td>
<td id="T_b3216_row0_col110" class="data row0 col110">
0
</td>
<td id="T_b3216_row0_col111" class="data row0 col111">
0
</td>
<td id="T_b3216_row0_col112" class="data row0 col112">
0
</td>
<td id="T_b3216_row0_col113" class="data row0 col113">
0
</td>
<td id="T_b3216_row0_col114" class="data row0 col114">
0
</td>
<td id="T_b3216_row0_col115" class="data row0 col115">
0
</td>
<td id="T_b3216_row0_col116" class="data row0 col116">
0
</td>
<td id="T_b3216_row0_col117" class="data row0 col117">
0
</td>
<td id="T_b3216_row0_col118" class="data row0 col118">
0
</td>
<td id="T_b3216_row0_col119" class="data row0 col119">
0
</td>
<td id="T_b3216_row0_col120" class="data row0 col120">
0
</td>
<td id="T_b3216_row0_col121" class="data row0 col121">
0
</td>
<td id="T_b3216_row0_col122" class="data row0 col122">
0
</td>
<td id="T_b3216_row0_col123" class="data row0 col123">
0
</td>
<td id="T_b3216_row0_col124" class="data row0 col124">
0
</td>
<td id="T_b3216_row0_col125" class="data row0 col125">
0
</td>
<td id="T_b3216_row0_col126" class="data row0 col126">
0
</td>
<td id="T_b3216_row0_col127" class="data row0 col127">
1
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<div class="sourceCode" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenizer.convert_ids_to_tokens(ds_mlm[<span class="st">'train'</span>][<span class="st">'input_ids'</span>][<span class="dv">0</span>])).T.style.hide(axis<span class="op">=</span><span class="st">'columns'</span>).hide(axis<span class="op">=</span><span class="st">'rows'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table id="T_4b0e1">
<thead>
</thead>
<tbody>
<tr>
<td id="T_4b0e1_row0_col0" class="data row0 col0">
[CLS]
</td>
<td id="T_4b0e1_row0_col1" class="data row0 col1">
add
</td>
<td id="T_4b0e1_row0_col2" class="data row0 col2">
new
</td>
<td id="T_4b0e1_row0_col3" class="data row0 col3">
canine
</td>
<td id="T_4b0e1_row0_col4" class="data row0 col4">
model
</td>
<td id="T_4b0e1_row0_col5" class="data row0 col5">
#
</td>
<td id="T_4b0e1_row0_col6" class="data row0 col6">
[UNK]
</td>
<td id="T_4b0e1_row0_col7" class="data row0 col7">
new
</td>
<td id="T_4b0e1_row0_col8" class="data row0 col8">
model
</td>
<td id="T_4b0e1_row0_col9" class="data row0 col9">
addition
</td>
<td id="T_4b0e1_row0_col10" class="data row0 col10">
#
</td>
<td id="T_4b0e1_row0_col11" class="data row0 col11">
#
</td>
<td id="T_4b0e1_row0_col12" class="data row0 col12">
model
</td>
<td id="T_4b0e1_row0_col13" class="data row0 col13">
description
</td>
<td id="T_4b0e1_row0_col14" class="data row0 col14">
google
</td>
<td id="T_4b0e1_row0_col15" class="data row0 col15">
recently
</td>
<td id="T_4b0e1_row0_col16" class="data row0 col16">
proposed
</td>
<td id="T_4b0e1_row0_col17" class="data row0 col17">
a
</td>
<td id="T_4b0e1_row0_col18" class="data row0 col18">
new
</td>
<td id="T_4b0e1_row0_col19" class="data row0 col19">
*
</td>
<td id="T_4b0e1_row0_col20" class="data row0 col20">
*
</td>
<td id="T_4b0e1_row0_col21" class="data row0 col21">
c
</td>
<td id="T_4b0e1_row0_col22" class="data row0 col22">
*
</td>
<td id="T_4b0e1_row0_col23" class="data row0 col23">
*
</td>
<td id="T_4b0e1_row0_col24" class="data row0 col24">
hara
</td>
<td id="T_4b0e1_row0_col25" class="data row0 col25">
##cter
</td>
<td id="T_4b0e1_row0_col26" class="data row0 col26">
*
</td>
<td id="T_4b0e1_row0_col27" class="data row0 col27">
*
</td>
<td id="T_4b0e1_row0_col28" class="data row0 col28">
a
</td>
<td id="T_4b0e1_row0_col29" class="data row0 col29">
*
</td>
<td id="T_4b0e1_row0_col30" class="data row0 col30">
*
</td>
<td id="T_4b0e1_row0_col31" class="data row0 col31">
rc
</td>
<td id="T_4b0e1_row0_col32" class="data row0 col32">
##hit
</td>
<td id="T_4b0e1_row0_col33" class="data row0 col33">
##ect
</td>
<td id="T_4b0e1_row0_col34" class="data row0 col34">
##ure
</td>
<td id="T_4b0e1_row0_col35" class="data row0 col35">
with
</td>
<td id="T_4b0e1_row0_col36" class="data row0 col36">
*
</td>
<td id="T_4b0e1_row0_col37" class="data row0 col37">
*
</td>
<td id="T_4b0e1_row0_col38" class="data row0 col38">
n
</td>
<td id="T_4b0e1_row0_col39" class="data row0 col39">
*
</td>
<td id="T_4b0e1_row0_col40" class="data row0 col40">
*
</td>
<td id="T_4b0e1_row0_col41" class="data row0 col41">
o
</td>
<td id="T_4b0e1_row0_col42" class="data row0 col42">
token
</td>
<td id="T_4b0e1_row0_col43" class="data row0 col43">
##ization
</td>
<td id="T_4b0e1_row0_col44" class="data row0 col44">
*
</td>
<td id="T_4b0e1_row0_col45" class="data row0 col45">
*
</td>
<td id="T_4b0e1_row0_col46" class="data row0 col46">
i
</td>
<td id="T_4b0e1_row0_col47" class="data row0 col47">
*
</td>
<td id="T_4b0e1_row0_col48" class="data row0 col48">
*
</td>
<td id="T_4b0e1_row0_col49" class="data row0 col49">
n
</td>
<td id="T_4b0e1_row0_col50" class="data row0 col50">
*
</td>
<td id="T_4b0e1_row0_col51" class="data row0 col51">
*
</td>
<td id="T_4b0e1_row0_col52" class="data row0 col52">
n
</td>
<td id="T_4b0e1_row0_col53" class="data row0 col53">
*
</td>
<td id="T_4b0e1_row0_col54" class="data row0 col54">
*
</td>
<td id="T_4b0e1_row0_col55" class="data row0 col55">
eu
</td>
<td id="T_4b0e1_row0_col56" class="data row0 col56">
##ral
</td>
<td id="T_4b0e1_row0_col57" class="data row0 col57">
*
</td>
<td id="T_4b0e1_row0_col58" class="data row0 col58">
*
</td>
<td id="T_4b0e1_row0_col59" class="data row0 col59">
e
</td>
<td id="T_4b0e1_row0_col60" class="data row0 col60">
*
</td>
<td id="T_4b0e1_row0_col61" class="data row0 col61">
*
</td>
<td id="T_4b0e1_row0_col62" class="data row0 col62">
nc
</td>
<td id="T_4b0e1_row0_col63" class="data row0 col63">
##oder
</td>
<td id="T_4b0e1_row0_col64" class="data row0 col64">
##s
</td>
<td id="T_4b0e1_row0_col65" class="data row0 col65">
architecture
</td>
<td id="T_4b0e1_row0_col66" class="data row0 col66">
(
</td>
<td id="T_4b0e1_row0_col67" class="data row0 col67">
canine
</td>
<td id="T_4b0e1_row0_col68" class="data row0 col68">
)
</td>
<td id="T_4b0e1_row0_col69" class="data row0 col69">
.
</td>
<td id="T_4b0e1_row0_col70" class="data row0 col70">
not
</td>
<td id="T_4b0e1_row0_col71" class="data row0 col71">
only
</td>
<td id="T_4b0e1_row0_col72" class="data row0 col72">
the
</td>
<td id="T_4b0e1_row0_col73" class="data row0 col73">
title
</td>
<td id="T_4b0e1_row0_col74" class="data row0 col74">
is
</td>
<td id="T_4b0e1_row0_col75" class="data row0 col75">
exciting
</td>
<td id="T_4b0e1_row0_col76" class="data row0 col76">
:
</td>
<td id="T_4b0e1_row0_col77" class="data row0 col77">
<blockquote class="blockquote">
</blockquote></td>
<td id="T_4b0e1_row0_col78" class="data row0 col78">
pipeline
</td>
<td id="T_4b0e1_row0_col79" class="data row0 col79">
##d
</td>
<td id="T_4b0e1_row0_col80" class="data row0 col80">
nl
</td>
<td id="T_4b0e1_row0_col81" class="data row0 col81">
##p
</td>
<td id="T_4b0e1_row0_col82" class="data row0 col82">
systems
</td>
<td id="T_4b0e1_row0_col83" class="data row0 col83">
have
</td>
<td id="T_4b0e1_row0_col84" class="data row0 col84">
largely
</td>
<td id="T_4b0e1_row0_col85" class="data row0 col85">
been
</td>
<td id="T_4b0e1_row0_col86" class="data row0 col86">
superseded
</td>
<td id="T_4b0e1_row0_col87" class="data row0 col87">
by
</td>
<td id="T_4b0e1_row0_col88" class="data row0 col88">
end
</td>
<td id="T_4b0e1_row0_col89" class="data row0 col89">
-
</td>
<td id="T_4b0e1_row0_col90" class="data row0 col90">
to
</td>
<td id="T_4b0e1_row0_col91" class="data row0 col91">
-
</td>
<td id="T_4b0e1_row0_col92" class="data row0 col92">
end
</td>
<td id="T_4b0e1_row0_col93" class="data row0 col93">
neural
</td>
<td id="T_4b0e1_row0_col94" class="data row0 col94">
modeling
</td>
<td id="T_4b0e1_row0_col95" class="data row0 col95">
,
</td>
<td id="T_4b0e1_row0_col96" class="data row0 col96">
yet
</td>
<td id="T_4b0e1_row0_col97" class="data row0 col97">
nearly
</td>
<td id="T_4b0e1_row0_col98" class="data row0 col98">
all
</td>
<td id="T_4b0e1_row0_col99" class="data row0 col99">
commonly
</td>
<td id="T_4b0e1_row0_col100" class="data row0 col100">
-
</td>
<td id="T_4b0e1_row0_col101" class="data row0 col101">
used
</td>
<td id="T_4b0e1_row0_col102" class="data row0 col102">
models
</td>
<td id="T_4b0e1_row0_col103" class="data row0 col103">
still
</td>
<td id="T_4b0e1_row0_col104" class="data row0 col104">
require
</td>
<td id="T_4b0e1_row0_col105" class="data row0 col105">
an
</td>
<td id="T_4b0e1_row0_col106" class="data row0 col106">
explicit
</td>
<td id="T_4b0e1_row0_col107" class="data row0 col107">
token
</td>
<td id="T_4b0e1_row0_col108" class="data row0 col108">
##ization
</td>
<td id="T_4b0e1_row0_col109" class="data row0 col109">
step
</td>
<td id="T_4b0e1_row0_col110" class="data row0 col110">
.
</td>
<td id="T_4b0e1_row0_col111" class="data row0 col111">
while
</td>
<td id="T_4b0e1_row0_col112" class="data row0 col112">
recent
</td>
<td id="T_4b0e1_row0_col113" class="data row0 col113">
token
</td>
<td id="T_4b0e1_row0_col114" class="data row0 col114">
##ization
</td>
<td id="T_4b0e1_row0_col115" class="data row0 col115">
approaches
</td>
<td id="T_4b0e1_row0_col116" class="data row0 col116">
based
</td>
<td id="T_4b0e1_row0_col117" class="data row0 col117">
on
</td>
<td id="T_4b0e1_row0_col118" class="data row0 col118">
data
</td>
<td id="T_4b0e1_row0_col119" class="data row0 col119">
-
</td>
<td id="T_4b0e1_row0_col120" class="data row0 col120">
derived
</td>
<td id="T_4b0e1_row0_col121" class="data row0 col121">
sub
</td>
<td id="T_4b0e1_row0_col122" class="data row0 col122">
##word
</td>
<td id="T_4b0e1_row0_col123" class="data row0 col123">
lexi
</td>
<td id="T_4b0e1_row0_col124" class="data row0 col124">
##con
</td>
<td id="T_4b0e1_row0_col125" class="data row0 col125">
##s
</td>
<td id="T_4b0e1_row0_col126" class="data row0 col126">
are
</td>
<td id="T_4b0e1_row0_col127" class="data row0 col127">
[SEP]
</td>
</tr>
</tbody>

</table>
</div>
<hr>

<div class="sourceCode" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DataCollatorForLanguageModeling, set_seed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<section id="transformers.data.data_collator.datacollatorforlanguagemodeling" class="level4">
<h4 class="anchored" data-anchor-id="transformers.data.data_collator.datacollatorforlanguagemodeling"><code>transformers.data.data_collator.DataCollatorForLanguageModeling</code></h4>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling">Documentation</a></li>
<li>Create a data collator for language modeling.</li>
</ul>
<p><strong>Initialize the data collator</strong></p>
<div class="sourceCode" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>                                                mlm_probability<span class="op">=</span><span class="fl">0.15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Reset random seed</strong></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Test the data collator</strong></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Switch the return formats of the tokenizer and data collator to NumPy</span></span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>data_collator.return_tensors <span class="op">=</span> <span class="st">"np"</span></span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"Transformers are awesome!"</span>, return_tensors<span class="op">=</span><span class="st">"np"</span>)</span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> data_collator([{<span class="st">"input_ids"</span>: inputs[<span class="st">"input_ids"</span>][<span class="dv">0</span>]}])</span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Original tokens"</span>: tokenizer.convert_ids_to_tokens(inputs[<span class="st">"input_ids"</span>][<span class="dv">0</span>]),</span>
<span id="cb171-9"><a href="#cb171-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Masked tokens"</span>: tokenizer.convert_ids_to_tokens(outputs[<span class="st">"input_ids"</span>][<span class="dv">0</span>]),</span>
<span id="cb171-10"><a href="#cb171-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Original input_ids"</span>: inputs[<span class="st">"input_ids"</span>][<span class="dv">0</span>],</span>
<span id="cb171-11"><a href="#cb171-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Masked input_ids"</span>: outputs[<span class="st">"input_ids"</span>][<span class="dv">0</span>],</span>
<span id="cb171-12"><a href="#cb171-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Labels"</span>: outputs[<span class="st">"labels"</span>][<span class="dv">0</span>]}).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">

<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Original tokens
</th>
<td>
[CLS]
</td>
<td>
transformers
</td>
<td>
are
</td>
<td>
awesome
</td>
<td>
!
</td>
<td>
[SEP]
</td>
</tr>
<tr>
<th>
Masked tokens
</th>
<td>
[CLS]
</td>
<td>
transformers
</td>
<td>
are
</td>
<td>
awesome
</td>
<td>
[MASK]
</td>
<td>
[SEP]
</td>
</tr>
<tr>
<th>
Original input_ids
</th>
<td>
101
</td>
<td>
19081
</td>
<td>
2024
</td>
<td>
12476
</td>
<td>
999
</td>
<td>
102
</td>
</tr>
<tr>
<th>
Masked input_ids
</th>
<td>
101
</td>
<td>
19081
</td>
<td>
2024
</td>
<td>
12476
</td>
<td>
103
</td>
<td>
102
</td>
</tr>
<tr>
<th>
Labels
</th>
<td>
-100
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
-100
</td>
<td>
999
</td>
<td>
-100
</td>
</tr>
</tbody>

</table>
</div>
<hr>
<p><strong>Switch the return formats of the data collator to PyTorch</strong></p>
<div class="sourceCode" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>data_collator.return_tensors <span class="op">=</span> <span class="st">"pt"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForMaskedLM</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize the training arguments</strong></p>
<div class="sourceCode" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>    output_dir <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_ckpt<span class="sc">}</span><span class="ss">-issues-128"</span>, per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>    logging_strategy<span class="op">=</span><span class="st">"epoch"</span>, evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>, save_strategy<span class="op">=</span><span class="st">"no"</span>,</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">16</span>, push_to_hub<span class="op">=</span><span class="va">True</span>, log_level<span class="op">=</span><span class="st">"error"</span>, report_to<span class="op">=</span><span class="st">"none"</span>, fp16<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Initialize the Trainer</strong></p>
<div class="sourceCode" id="cb175"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>AutoModelForMaskedLM.from_pretrained(<span class="st">"bert-base-uncased"</span>).to(device),</span>
<span id="cb175-3"><a href="#cb175-3" aria-hidden="true" tabindex="-1"></a>        tokenizer<span class="op">=</span>tokenizer, args<span class="op">=</span>training_args, data_collator<span class="op">=</span>data_collator,</span>
<span id="cb175-4"><a href="#cb175-4" aria-hidden="true" tabindex="-1"></a>        train_dataset<span class="op">=</span>ds_mlm[<span class="st">"unsup"</span>], eval_dataset<span class="op">=</span>ds_mlm[<span class="st">"train"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Train the model</strong></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    TrainOutput(global_step=4656, training_loss=1.2827145487991805, metrics={'train_runtime': 561.7109, 'train_samples_per_second': 264.99, 'train_steps_per_second': 8.289, 'train_loss': 1.2827145487991805, 'epoch': 16.0})</code></pre>
<hr>
<p><strong>Push the trained model to Hugging Face Hub</strong></p>
<div class="sourceCode" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>trainer.push_to_hub(<span class="st">"Training complete!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    'https://huggingface.co/cj-mills/bert-base-uncased-issues-128/commit/c7ba31377378cb7fb9412e9524aaf76eed6956b7'</code></pre>
<hr>
<p><strong>Inspect the training and validation losses from the training session</strong></p>
<div class="sourceCode" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>df_log <span class="op">=</span> pd.DataFrame(trainer.state.log_history)</span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop missing values</span></span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a>(df_log.dropna(subset<span class="op">=</span>[<span class="st">"eval_loss"</span>]).reset_index()[<span class="st">"eval_loss"</span>]</span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a> .plot(label<span class="op">=</span><span class="st">"Validation"</span>))</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a>df_log.dropna(subset<span class="op">=</span>[<span class="st">"loss"</span>]).reset_index()[<span class="st">"loss"</span>].plot(label<span class="op">=</span><span class="st">"Train"</span>)</span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>)</span>
<span id="cb180-11"><a href="#cb180-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_284_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> Both the training and validation loss decreased significantly.</p>
<hr>
<p><strong>Free unoccupied cached memory</strong></p>
<div class="sourceCode" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="fine-tuning-a-classifier" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-a-classifier">Fine-Tuning a Classifier</h3>
<p><strong>Initialize a BertConfig for multi-label classification with the custom checkpoint</strong></p>
<div class="sourceCode" id="cb182"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="st">"bert-base-uncased"</span></span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a>model_ckpt <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>model_ckpt<span class="sc">}</span><span class="ss">-issues-128'</span></span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_ckpt)</span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a>config.num_labels <span class="op">=</span> <span class="bu">len</span>(all_labels)</span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a>config.problem_type <span class="op">=</span> <span class="st">"multi_label_classification"</span></span>
<span id="cb182-6"><a href="#cb182-6" aria-hidden="true" tabindex="-1"></a>config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    BertConfig {
      "_name_or_path": "bert-base-uncased-issues-128",
      "architectures": [
        "BertForMaskedLM"
      ],
      "attention_probs_dropout_prob": 0.1,
      "classifier_dropout": null,
      "gradient_checkpointing": false,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1",
        "2": "LABEL_2",
        "3": "LABEL_3",
        "4": "LABEL_4",
        "5": "LABEL_5",
        "6": "LABEL_6",
        "7": "LABEL_7",
        "8": "LABEL_8"
      },
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1,
        "LABEL_2": 2,
        "LABEL_3": 3,
        "LABEL_4": 4,
        "LABEL_5": 5,
        "LABEL_6": 6,
        "LABEL_7": 7,
        "LABEL_8": 8
      },
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "bert",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "pad_token_id": 0,
      "position_embedding_type": "absolute",
      "problem_type": "multi_label_classification",
      "torch_dtype": "float32",
      "transformers_version": "4.18.0",
      "type_vocab_size": 2,
      "use_cache": true,
      "vocab_size": 30522
    }</code></pre>
<hr>
<p><strong>Fine-tune the classifier on each training slice</strong></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_slice <span class="kw">in</span> train_slices:</span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_ckpt,</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a>                                                               config<span class="op">=</span>config).to(device)</span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> Trainer(</span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb184-6"><a href="#cb184-6" aria-hidden="true" tabindex="-1"></a>        tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb184-7"><a href="#cb184-7" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>training_args_fine_tune,</span>
<span id="cb184-8"><a href="#cb184-8" aria-hidden="true" tabindex="-1"></a>        compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb184-9"><a href="#cb184-9" aria-hidden="true" tabindex="-1"></a>        train_dataset<span class="op">=</span>ds_enc[<span class="st">"train"</span>].select(train_slice),</span>
<span id="cb184-10"><a href="#cb184-10" aria-hidden="true" tabindex="-1"></a>        eval_dataset<span class="op">=</span>ds_enc[<span class="st">"valid"</span>],</span>
<span id="cb184-11"><a href="#cb184-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb184-12"><a href="#cb184-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-13"><a href="#cb184-13" aria-hidden="true" tabindex="-1"></a>    trainer.train()</span>
<span id="cb184-14"><a href="#cb184-14" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> trainer.predict(ds_enc[<span class="st">'test'</span>])</span>
<span id="cb184-15"><a href="#cb184-15" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> compute_metrics(pred)</span>
<span id="cb184-16"><a href="#cb184-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># DA refers to domain adaptation</span></span>
<span id="cb184-17"><a href="#cb184-17" aria-hidden="true" tabindex="-1"></a>    macro_scores[<span class="st">'Fine-tune (DA)'</span>].append(metrics[<span class="st">'macro f1'</span>])</span>
<span id="cb184-18"><a href="#cb184-18" aria-hidden="true" tabindex="-1"></a>    micro_scores[<span class="st">'Fine-tune (DA)'</span>].append(metrics[<span class="st">'micro f1'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>plot_metrics(micro_scores, macro_scores, train_samples, <span class="st">"Fine-tune (DA)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_293_0.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><strong>Note:</strong> The fine-tuned classifier performs better than the vanilla BERT, especially in the low-data domain.</p>
<hr>
</section>
<section id="advanced-methods" class="level3">
<h3 class="anchored" data-anchor-id="advanced-methods">Advanced Methods</h3>
<section id="unsupervised-data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="unsupervised-data-augmentation">Unsupervised data augmentation</h4>
<ul>
<li>Unsupervised data augmentation (UDA) works off the idea that a model’s predictions should be consistent for an unlabeled example and a slightly distorted one.</li>
<li>We can introduce distortions using standard data augmentation strategies.</li>
<li>We enforce consistency by minimizing the KL divergence between the predictions of the original and distorted examples.</li>
<li>We incorporate the consistency requirement by augmenting the cross-entropy loss with an additional term from the unlabeled examples.</li>
<li>The model trains on the labeled data with the standard supervised approach, but we constrain the model to make consistent predictions on the unlabeled data.</li>
<li>BERT models trained with UDA on a handful of labeled examples get similar performance to models trained on thousands of examples.</li>
<li>UDA requires a data augmentation pipeline to generate distorted examples.</li>
<li>Training takes much longer since it requires multiple forward passes to generate the predicted distributions on the unlabeled and augmented examples.</li>
</ul>
</section>
<section id="uncertainty-aware-self-training" class="level4">
<h4 class="anchored" data-anchor-id="uncertainty-aware-self-training">Uncertainty-aware self-training</h4>
<ul>
<li>Uncertainty-aware self-training (UST) involves training a teacher model on labeled data and using that model to create pseudo-labels for unlabeled data.</li>
<li>A student model then trains on the pseudo-labeled data.</li>
<li>We get an uncertainty measure of the student model’s predictions by feeding it the same input several times with dropout turned on.</li>
<li>The variance of the predictions gives a proxy for the certainty of the model on a specific sample.</li>
<li>We then sample the pseudo-labels using Bayesian Active Learning by Disagreement (BALD).</li>
<li>The teacher continuously gets better at creating pseudo-labels, improving the student’s performance.</li>
<li>UST gets within a few percentages of models trained on datasets with thousands of labeled samples and beats UDA on several datasets.</li>
<li><a href="https://arxiv.org/abs/2006.15315">Uncertainty-aware Self-training for Text Classification with Few Labels</a></li>
</ul>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>Set up an evaluation pipeline to test different approaches for dealing with little to no labeled data.</li>
<li>Build a validation and test set early on.</li>
<li>There are tradeoffs between more complex approaches like UDA and UST and getting more data.</li>
<li>It might make more sense to create a small high-quality dataset rather than engineering a very complex method to compensate for the lack of data.</li>
<li>Annotating a few hundred examples usually takes a couple of hours to a few days.</li>
<li>There are many annotation tools available to speed up labeling new data.</li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://transformersbook.com/">Natural Language Processing with Transformers Book</a></li>
<li><a href="https://github.com/nlp-with-transformers/notebooks">The Transformers book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-8/">Notes on Transformers Book Ch. 8</a></p>
<p><strong>Next:</strong> <a href="../chapter-10/">Notes on Transformers Book Ch. 10</a></p>
<!-- Cloudflare Web Analytics -->
<script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script>
<!-- End Cloudflare Web Analytics -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2023, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>