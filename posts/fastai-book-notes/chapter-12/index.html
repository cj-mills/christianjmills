<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2022-03-29">
<meta name="description" content="Chapter 12 covers building and training an LSTM from scratch.">

<title>Notes on fastai Book Ch. 12 – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Notes on fastai Book Ch. 12 – Christian Mills">
<meta property="og:description" content="Chapter 12 covers building and training an LSTM from scratch.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Notes on fastai Book Ch. 12 – Christian Mills">
<meta name="twitter:description" content="Chapter 12 covers building and training an LSTM from scratch.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-language-model-from-scratch" id="toc-a-language-model-from-scratch" class="nav-link active" data-scroll-target="#a-language-model-from-scratch">A Language Model from Scratch</a></li>
  <li><a href="#the-data" id="toc-the-data" class="nav-link" data-scroll-target="#the-data">The Data</a>
  <ul>
  <li><a href="#fastai-human-numbers-dataset" id="toc-fastai-human-numbers-dataset" class="nav-link" data-scroll-target="#fastai-human-numbers-dataset">fastai Human Numbers Dataset</a></li>
  </ul></li>
  <li><a href="#our-first-language-model-from-scratch" id="toc-our-first-language-model-from-scratch" class="nav-link" data-scroll-target="#our-first-language-model-from-scratch">Our First Language Model from Scratch</a>
  <ul>
  <li><a href="#our-language-model-in-pytorch" id="toc-our-language-model-in-pytorch" class="nav-link" data-scroll-target="#our-language-model-in-pytorch">Our Language Model in PyTorch</a></li>
  <li><a href="#our-first-recurrent-neural-network-a.k.a-a-looping-network" id="toc-our-first-recurrent-neural-network-a.k.a-a-looping-network" class="nav-link" data-scroll-target="#our-first-recurrent-neural-network-a.k.a-a-looping-network">Our First Recurrent Neural Network (a.k.a A Looping Network)</a>
  <ul class="collapse">
  <li><a href="#hidden-state" id="toc-hidden-state" class="nav-link" data-scroll-target="#hidden-state">Hidden State:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#improving-the-rnn" id="toc-improving-the-rnn" class="nav-link" data-scroll-target="#improving-the-rnn">Improving the RNN</a>
  <ul>
  <li><a href="#maintaining-the-state-of-an-rnn" id="toc-maintaining-the-state-of-an-rnn" class="nav-link" data-scroll-target="#maintaining-the-state-of-an-rnn">Maintaining the State of an RNN</a>
  <ul class="collapse">
  <li><a href="#backpropogation-through-time-bptt" id="toc-backpropogation-through-time-bptt" class="nav-link" data-scroll-target="#backpropogation-through-time-bptt">Backpropogation Through Time (BPTT)</a></li>
  <li><a href="#fastai-callbacks" id="toc-fastai-callbacks" class="nav-link" data-scroll-target="#fastai-callbacks">fastai Callbacks</a></li>
  </ul></li>
  <li><a href="#creating-more-signal" id="toc-creating-more-signal" class="nav-link" data-scroll-target="#creating-more-signal">Creating More Signal</a></li>
  </ul></li>
  <li><a href="#multilayer-rnns" id="toc-multilayer-rnns" class="nav-link" data-scroll-target="#multilayer-rnns">Multilayer RNNs</a>
  <ul>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The Model</a></li>
  <li><a href="#exploding-or-disappearing-activations" id="toc-exploding-or-disappearing-activations" class="nav-link" data-scroll-target="#exploding-or-disappearing-activations">Exploding or Disappearing Activations</a></li>
  </ul></li>
  <li><a href="#lstm" id="toc-lstm" class="nav-link" data-scroll-target="#lstm">LSTM</a>
  <ul>
  <li><a href="#building-an-lstm-from-scratch" id="toc-building-an-lstm-from-scratch" class="nav-link" data-scroll-target="#building-an-lstm-from-scratch">Building an LSTM from Scratch</a>
  <ul class="collapse">
  <li><a href="#torch-chunk" id="toc-torch-chunk" class="nav-link" data-scroll-target="#torch-chunk">torch chunk</a></li>
  </ul></li>
  <li><a href="#training-a-language-model-using-lstms" id="toc-training-a-language-model-using-lstms" class="nav-link" data-scroll-target="#training-a-language-model-using-lstms">Training a Language Model Using LSTMs</a></li>
  </ul></li>
  <li><a href="#regularizing-an-lstm" id="toc-regularizing-an-lstm" class="nav-link" data-scroll-target="#regularizing-an-lstm">Regularizing an LSTM</a>
  <ul>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a></li>
  <li><a href="#activation-regularization-and-temporal-activation-regularization" id="toc-activation-regularization-and-temporal-activation-regularization" class="nav-link" data-scroll-target="#activation-regularization-and-temporal-activation-regularization">Activation Regularization and Temporal Activation Regularization</a></li>
  <li><a href="#training-a-weight-tied-regularized-lstm" id="toc-training-a-weight-tied-regularized-lstm" class="nav-link" data-scroll-target="#training-a-weight-tied-regularized-lstm">Training a Weight-Tied Regularized LSTM</a>
  <ul class="collapse">
  <li><a href="#weight-tying" id="toc-weight-tying" class="nav-link" data-scroll-target="#weight-tying">Weight Tying</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on fastai Book Ch. 12</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ai</div>
    <div class="quarto-category">fastai</div>
    <div class="quarto-category">notes</div>
    <div class="quarto-category">pytorch</div>
  </div>
  </div>

<div>
  <div class="description">
    Chapter 12 covers building and training an LSTM from scratch.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 29, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/fastai-book-notes.html"><strong>Deep Learning for Coders with fastai &amp; PyTorch</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#the-data">The Data</a></li>
<li><a href="#our-first-language-model-from-scratch">Our First Language Model from Scratch</a></li>
<li><a href="#improving-the-rnn">Improving the RNN</a></li>
<li><a href="#multilayer-rnns">Multilayer RNNs</a></li>
<li><a href="#lstm">LSTM</a></li>
<li><a href="#regularizing-an-lstm">Regularizing an LSTM</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install -Uqq fastbook</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_source(obj):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> inspect.getsource(obj).split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(line)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="a-language-model-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="a-language-model-from-scratch">A Language Model from Scratch</h2>
</section>
<section id="the-data" class="level2">
<h2 class="anchored" data-anchor-id="the-data">The Data</h2>
<ul>
<li>try to think of the simplest useable dataset when starting on a new problem</li>
<li>the starter dataset should allow you to quickly and easily try out methods and interpret the results</li>
<li>one of the most common practical mistakes is failing to use appropriate datasets at appropriate times during the analysis process
<ul>
<li>most people tend to start with datasets that are too big and too complicated</li>
</ul></li>
</ul>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.text.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="fastai-human-numbers-dataset" class="level4">
<h4 class="anchored" data-anchor-id="fastai-human-numbers-dataset">fastai Human Numbers Dataset</h4>
<ul>
<li>A synthetic dataset consisting of human number counts in text such as one, two, three, four..</li>
<li>Useful for experimenting with Language Models</li>
</ul>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>URLs.HUMAN_NUMBERS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>'https://s3.amazonaws.com/fast-ai-sample/human_numbers.tgz'</code></pre>
<hr>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.HUMAN_NUMBERS)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>path</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Path('/home/innom-dt/.fastai/data/human_numbers')</code></pre>
<hr>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>path.ls()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#2) [Path('/home/innom-dt/.fastai/data/human_numbers/train.txt'),Path('/home/innom-dt/.fastai/data/human_numbers/valid.txt')]</code></pre>
<hr>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>train_file <span class="op">=</span> path.ls()[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>cat $train_file <span class="op">|</span> head <span class="op">-</span><span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>one 
two 
three 
four 
five 
cat: write error: Broken pipe</code></pre>
<hr>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>valid_file <span class="op">=</span> path.ls()[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>cat $valid_file <span class="op">|</span> head <span class="op">-</span><span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>eight thousand one 
eight thousand two 
eight thousand three 
eight thousand four 
eight thousand five 
cat: write error: Broken pipe</code></pre>
<hr>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>lines <span class="op">=</span> L()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the training and validation sets into a single List</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path<span class="op">/</span><span class="st">'train.txt'</span>) <span class="im">as</span> f: lines <span class="op">+=</span> L(<span class="op">*</span>f.readlines())</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path<span class="op">/</span><span class="st">'valid.txt'</span>) <span class="im">as</span> f: lines <span class="op">+=</span> L(<span class="op">*</span>f.readlines())</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>lines</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#9998) ['one \n','two \n','three \n','four \n','five \n','six \n','seven \n','eight \n','nine \n','ten \n'...]</code></pre>
<hr>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the '\n' new line characters and separate the words with a '.'</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">' . '</span>.join([l.strip() <span class="cf">for</span> l <span class="kw">in</span> lines])</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>text[:<span class="dv">100</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'</code></pre>
<hr>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the words into a list</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> text.split(<span class="st">' '</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']</code></pre>
<hr>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate unique vocab</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> L(<span class="op">*</span>tokens).unique()</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>vocab</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]</code></pre>
<hr>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(<span class="bu">list</span>(vocab))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
one
</td>
</tr>
<tr>
<th>
1
</th>
<td>
.
</td>
</tr>
<tr>
<th>
2
</th>
<td>
two
</td>
</tr>
<tr>
<th>
3
</th>
<td>
three
</td>
</tr>
<tr>
<th>
4
</th>
<td>
four
</td>
</tr>
<tr>
<th>
5
</th>
<td>
five
</td>
</tr>
<tr>
<th>
6
</th>
<td>
six
</td>
</tr>
<tr>
<th>
7
</th>
<td>
seven
</td>
</tr>
<tr>
<th>
8
</th>
<td>
eight
</td>
</tr>
<tr>
<th>
9
</th>
<td>
nine
</td>
</tr>
<tr>
<th>
10
</th>
<td>
ten
</td>
</tr>
<tr>
<th>
11
</th>
<td>
eleven
</td>
</tr>
<tr>
<th>
12
</th>
<td>
twelve
</td>
</tr>
<tr>
<th>
13
</th>
<td>
thirteen
</td>
</tr>
<tr>
<th>
14
</th>
<td>
fourteen
</td>
</tr>
<tr>
<th>
15
</th>
<td>
fifteen
</td>
</tr>
<tr>
<th>
16
</th>
<td>
sixteen
</td>
</tr>
<tr>
<th>
17
</th>
<td>
seventeen
</td>
</tr>
<tr>
<th>
18
</th>
<td>
eighteen
</td>
</tr>
<tr>
<th>
19
</th>
<td>
nineteen
</td>
</tr>
<tr>
<th>
20
</th>
<td>
twenty
</td>
</tr>
<tr>
<th>
21
</th>
<td>
thirty
</td>
</tr>
<tr>
<th>
22
</th>
<td>
forty
</td>
</tr>
<tr>
<th>
23
</th>
<td>
fifty
</td>
</tr>
<tr>
<th>
24
</th>
<td>
sixty
</td>
</tr>
<tr>
<th>
25
</th>
<td>
seventy
</td>
</tr>
<tr>
<th>
26
</th>
<td>
eighty
</td>
</tr>
<tr>
<th>
27
</th>
<td>
ninety
</td>
</tr>
<tr>
<th>
28
</th>
<td>
hundred
</td>
</tr>
<tr>
<th>
29
</th>
<td>
thousand
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map words to their vocab indices</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>word2idx <span class="op">=</span> {w:i <span class="cf">for</span> i,w <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Numericalize dataset</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>nums <span class="op">=</span> L(word2idx[i] <span class="cf">for</span> i <span class="kw">in</span> tokens)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>nums</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#63095) [0,1,2,1,3,1,4,1,5,1...]</code></pre>
</section>
</section>
<section id="our-first-language-model-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="our-first-language-model-from-scratch">Our First Language Model from Scratch</h2>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of (input, target) tuples</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># input: the previous three words</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target: the next word</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>L((tokens[i:i<span class="op">+</span><span class="dv">3</span>], tokens[i<span class="op">+</span><span class="dv">3</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(tokens)<span class="op">-</span><span class="dv">4</span>,<span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]</code></pre>
<hr>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create a list of (input, target) tuples</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># input: a tensor containing the numericalized forms of previous three words</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target: the numericalized form of the next word</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>seqs <span class="op">=</span> L((tensor(nums[i:i<span class="op">+</span><span class="dv">3</span>]), nums[i<span class="op">+</span><span class="dv">3</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(nums)<span class="op">-</span><span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>seqs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]</code></pre>
<hr>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>DataLoaders.from_dsets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>&lt;bound method DataLoaders.from_dsets of &lt;class 'fastai.data.core.DataLoaders'&gt;&gt;</code></pre>
<hr>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>print_source(DataLoaders.from_dsets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    @classmethod
    def from_dsets(cls, *ds, path='.',  bs=64, device=None, dl_type=TfmdDL, **kwargs):
        default = (True,) + (False,) * (len(ds)-1)
        defaults = {'shuffle': default, 'drop_last': default}
        tfms = {k:tuple(Pipeline(kwargs[k]) for i in range_of(ds)) for k in _batch_tfms if k in kwargs}
        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items() if k not in _batch_tfms}, tfms)
        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]
        return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device)</code></pre>
<hr>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data between train and valid 80/20</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>dls.one_batch()[<span class="dv">0</span>].shape, dls.one_batch()[<span class="dv">1</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(torch.Size([64, 3]), torch.Size([64]))</code></pre>
<hr>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>dls.one_batch()[<span class="dv">0</span>][<span class="dv">0</span>], dls.one_batch()[<span class="dv">1</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(tensor([0, 1, 2]), tensor(1))</code></pre>
<section id="our-language-model-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="our-language-model-in-pytorch">Our Language Model in PyTorch</h3>
<ul>
<li>Every word is interpreted in the information context of any words preceding it</li>
</ul>
<hr>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel1(Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input to hidden</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hidden to hidden</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)     </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hidden to output</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First input word</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass embedding for first word to first linear layer</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.i_h(x[:,<span class="dv">0</span>])))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second input word</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add embedding for second word to previous output</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,<span class="dv">1</span>])</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass to first linear layer</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Third input word</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add embeddingfor third word to previous output</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,<span class="dv">2</span>])</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass to first linear layer</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass output to second linear layer</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel1(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>F.cross_entropy, </span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">4</span>, <span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
1.813438
</td>
<td>
1.944979
</td>
<td>
0.466603
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
1
</td>
<td>
1.405106
</td>
<td>
1.702907
</td>
<td>
0.474447
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.427549
</td>
<td>
1.650981
</td>
<td>
0.489898
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.380016
</td>
<td>
1.685956
</td>
<td>
0.470882
</td>
<td>
00:01
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>range_of</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>&lt;function fastcore.basics.range_of(a, b=None, step=None)&gt;</code></pre>
<hr>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>print_source(range_of)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>def range_of(a, b=None, step=None):
    "All indices of collection `a`, if `a` is a collection, otherwise `range`"
    if is_coll(a): a = len(a)
    return list(range(a,b,step) if step is not None else range(a,b) if b is not None else range(a))</code></pre>
<hr>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the number of occurrences of each unique vocab item in the validation set</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>n,counts <span class="op">=</span> <span class="dv">0</span>,torch.zeros(<span class="bu">len</span>(vocab))</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>n, counts</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x,y <span class="kw">in</span> dls.valid:</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> y.shape[<span class="dv">0</span>]</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep track of </span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> range_of(vocab): counts[i] <span class="op">+=</span> (y<span class="op">==</span>i).<span class="bu">long</span>().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the index for the most common token in the validation set</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.argmax(counts)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the most common index</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>(idx, </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the corresponding word for the index</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>vocab[idx.item()], </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the likelihood of randomly picking the most common word</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>counts[idx].item()<span class="op">/</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(tensor(29), 'thousand', 0.15165200855716662)</code></pre>
<p><strong>Note:</strong> This indicates the model is performing much better than picking a word at random.</p>
</section>
<section id="our-first-recurrent-neural-network-a.k.a-a-looping-network" class="level3">
<h3 class="anchored" data-anchor-id="our-first-recurrent-neural-network-a.k.a-a-looping-network">Our First Recurrent Neural Network (a.k.a A Looping Network)</h3>
<ul>
<li>replace the hardcoded forward function in the LMModel1 with a for loop</li>
</ul>
<section id="hidden-state" class="level4">
<h4 class="anchored" data-anchor-id="hidden-state">Hidden State:</h4>
<ul>
<li>the activations that are updated at each step of a recurrent neural network</li>
</ul>
<hr>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel2(Module):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)  </span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)     </span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the hidden state</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the hidden state</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel2(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>F.cross_entropy, </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">4</span>, <span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
1.790029
</td>
<td>
1.993387
</td>
<td>
0.463038
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
1
</td>
<td>
1.389832
</td>
<td>
1.836371
</td>
<td>
0.466603
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.422808
</td>
<td>
1.669952
</td>
<td>
0.487045
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.380381
</td>
<td>
1.681706
</td>
<td>
0.459472
</td>
<td>
00:01
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="improving-the-rnn" class="level2">
<h2 class="anchored" data-anchor-id="improving-the-rnn">Improving the RNN</h2>
<ul>
<li>the above LMModel2 version resets the hidden state for every new input sequence
<ul>
<li>throwing away all the information we have about the sentences we have seen so far</li>
</ul></li>
<li>the above LMModel2 version only tries to predict the fourth word</li>
</ul>
<section id="maintaining-the-state-of-an-rnn" class="level3">
<h3 class="anchored" data-anchor-id="maintaining-the-state-of-an-rnn">Maintaining the State of an RNN</h3>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel3(Module):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)  </span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)     </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Maintain the same hidden state across input sequences</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.h))</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.h_o(<span class="va">self</span>.h)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Detach the hidden state from the pytorch computation graph</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h.detach()</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="backpropogation-through-time-bptt" class="level4">
<h4 class="anchored" data-anchor-id="backpropogation-through-time-bptt">Backpropogation Through Time (BPTT)</h4>
<ul>
<li>Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way</li>
<li>usually use Truncated BPTT which detaches the history of computation steps in the hidden state every few time steps.</li>
</ul>
<hr>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(seqs)<span class="op">//</span>bs</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>m,bs,<span class="bu">len</span>(seqs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(328, 64, 21031)</code></pre>
<hr>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> group_chunks(ds, bs):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the number of groups</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="bu">len</span>(ds) <span class="op">//</span> bs</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize new dataset container</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    new_ds <span class="op">=</span> L()</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Group dataset into chunks</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m): new_ds <span class="op">+=</span> L(ds[i <span class="op">+</span> m<span class="op">*</span>j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(bs))</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split dataset 80/20 into training and validation</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    group_chunks(seqs[:cut], bs), </span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    group_chunks(seqs[cut:], bs), </span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    bs<span class="op">=</span>bs, </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Drop the last batch that does not have the shape of bs</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    drop_last<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>ModelResetter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>fastai.callback.rnn.ModelResetter</code></pre>
<hr>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>print_source(ModelResetter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>@docs
class ModelResetter(Callback):
    "`Callback` that resets the model at each validation/training step"
    def before_train(self):    self.model.reset()
    def before_validate(self): self.model.reset()
    def after_fit(self):       self.model.reset()
    _docs = dict(before_train="Reset the model before training",
                 before_validate="Reset the model before validation",
                 after_fit="Reset the model after fitting")</code></pre>
</section>
<section id="fastai-callbacks" class="level4">
<h4 class="anchored" data-anchor-id="fastai-callbacks">fastai Callbacks</h4>
<ul>
<li><a href="https://docs.fast.ai/callback.core.html#Callback">Documentation</a></li>
<li><code>after_create:</code> called after the Learner is created</li>
<li><code>before_fit:</code> called before starting training or inference, ideal for initial setup.</li>
<li><code>before_epoch:</code> called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.</li>
<li><code>before_train:</code> called at the beginning of the training part of an epoch.</li>
<li><code>before_batch:</code> called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).</li>
<li><code>after_pred:</code> called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss.</li>
<li><code>after_loss:</code> called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).</li>
<li><code>before_backward:</code> called after the loss has been computed, but only in training mode (i.e.&nbsp;when the backward pass will be used)</li>
<li><code>before_step:</code> called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).</li>
<li><code>after_step:</code> called after the step and before the gradients are zeroed.</li>
<li><code>after_batch:</code> called at the end of a batch, for any clean-up before the next one.</li>
<li><code>after_train:</code> called at the end of the training phase of an epoch.</li>
<li><code>before_validate:</code> called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.</li>
<li><code>after_validate:</code> called at the end of the validation part of an epoch.</li>
<li><code>after_epoch:</code> called at the end of an epoch, for any clean-up before the next one.</li>
<li><code>after_fit:</code> called at the end of training, for final clean-up.</li>
</ul>
<hr>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>Callback</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>fastai.callback.core.Callback</code></pre>
<hr>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>print_source(Callback)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>@funcs_kwargs(as_method=True)
class Callback(Stateful,GetAttr):
    "Basic class handling tweaks of the training loop by changing a `Learner` in various events"
    order,_default,learn,run,run_train,run_valid = 0,'learn',None,True,True,True
    _methods = _events

    def __init__(self, **kwargs): assert not kwargs, f'Passed unknown events: {kwargs}'
    def __repr__(self): return type(self).__name__

    def __call__(self, event_name):
        "Call `self.{event_name}` if it's defined"
        _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or
               (self.run_valid and not getattr(self, 'training', False)))
        res = None
        if self.run and _run: res = getattr(self, event_name, noop)()
        if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit
        return res

    def __setattr__(self, name, value):
        if hasattr(self.learn,name):
            warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")
        super().__setattr__(name, value)

    @property
    def name(self):
        "Name of the `Callback`, camel-cased and with '*Callback*' removed"
        return class2attr(self, 'Callback')</code></pre>
<hr>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>                LMModel3(<span class="bu">len</span>(vocab), <span class="dv">64</span>), </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, </span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>                <span class="co"># reset the model at the beginning of each epoch and before each validation phase</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>                cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">10</span>, <span class="fl">3e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
1.695570
</td>
<td>
1.837262
</td>
<td>
0.474519
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
1
</td>
<td>
1.316114
</td>
<td>
1.939660
</td>
<td>
0.366346
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.102734
</td>
<td>
1.578932
</td>
<td>
0.469471
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.017313
</td>
<td>
1.470766
</td>
<td>
0.552163
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
4
</td>
<td>
0.961458
</td>
<td>
1.568437
</td>
<td>
0.551923
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.920572
</td>
<td>
1.632755
</td>
<td>
0.574519
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
6
</td>
<td>
0.932616
</td>
<td>
1.634864
</td>
<td>
0.588221
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
7
</td>
<td>
0.848161
</td>
<td>
1.668468
</td>
<td>
0.587500
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.802442
</td>
<td>
1.698610
</td>
<td>
0.591827
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.794550
</td>
<td>
1.716233
</td>
<td>
0.594952
</td>
<td>
00:01
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="creating-more-signal" class="level3">
<h3 class="anchored" data-anchor-id="creating-more-signal">Creating More Signal</h3>
<ul>
<li>we can increase the amount of signal for updating the model weights by predicting the next word after every single word, rather than every three words</li>
</ul>
<hr>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the sequence length</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>sl <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the dependent variable to include each of the words </span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co"># that follow each of the words in the independent variable</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>seqs <span class="op">=</span> L((tensor(nums[i:i<span class="op">+</span>sl]), tensor(nums[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>sl<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(nums)<span class="op">-</span>sl<span class="op">-</span><span class="dv">1</span>,sl))</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the split for the training and validation set</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>                             group_chunks(seqs[cut:], bs),</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>                             bs<span class="op">=</span>bs, drop_last<span class="op">=</span><span class="va">True</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>[L(vocab[o] <span class="cf">for</span> o <span class="kw">in</span> s) <span class="cf">for</span> s <span class="kw">in</span> seqs[<span class="dv">0</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>[(#16) ['one','.','two','.','three','.','four','.','five','.'...],
 (#16) ['.','two','.','three','.','four','.','five','.','six'...]]</code></pre>
<hr>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel4(Module):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)  </span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)     </span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>        outs <span class="op">=</span> []</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(sl):</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.h))</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store the output for each word in the current sequence</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>            outs.append(<span class="va">self</span>.h_o(<span class="va">self</span>.h))</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h.detach()</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># stack the output for each word in the current sequence</span></span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack(outs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define custom loss function that flattens the output before calculating cross entropy</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_func(inp, targ):</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.cross_entropy(inp.view(<span class="op">-</span><span class="dv">1</span>, <span class="bu">len</span>(vocab)), targ.view(<span class="op">-</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel4(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>loss_func,</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">3e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
3.229987
</td>
<td>
3.069768
</td>
<td>
0.249756
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
1
</td>
<td>
2.291759
</td>
<td>
1.903835
</td>
<td>
0.468018
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.719411
</td>
<td>
1.769336
</td>
<td>
0.469157
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.444394
</td>
<td>
1.729377
</td>
<td>
0.459554
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
4
</td>
<td>
1.273674
</td>
<td>
1.625678
</td>
<td>
0.531169
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
5
</td>
<td>
1.141202
</td>
<td>
1.762818
</td>
<td>
0.545898
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
6
</td>
<td>
1.037926
</td>
<td>
1.575556
</td>
<td>
0.573812
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
7
</td>
<td>
0.939284
</td>
<td>
1.470020
</td>
<td>
0.614095
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.858596
</td>
<td>
1.532887
</td>
<td>
0.628255
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.784250
</td>
<td>
1.495697
</td>
<td>
0.655843
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
10
</td>
<td>
0.739764
</td>
<td>
1.539676
</td>
<td>
0.666423
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
11
</td>
<td>
0.693413
</td>
<td>
1.550242
</td>
<td>
0.662191
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
12
</td>
<td>
0.661127
</td>
<td>
1.519285
</td>
<td>
0.680908
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
13
</td>
<td>
0.635551
</td>
<td>
1.523878
</td>
<td>
0.676921
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
14
</td>
<td>
0.621697
</td>
<td>
1.531653
</td>
<td>
0.684408
</td>
<td>
00:00
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="multilayer-rnns" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-rnns">Multilayer RNNs</h2>
<ul>
<li>pass the activations from one RNN into another RNN</li>
</ul>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The Model</h3>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel5(Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> torch.zeros(n_layers, bs, n_hidden)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        res,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h.detach()</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(res)</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>LMModel5(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>LMModel5(
  (i_h): Embedding(30, 64)
  (rnn): RNN(64, 64, num_layers=2, batch_first=True)
  (h_o): Linear(in_features=64, out_features=30, bias=True)
)</code></pre>
<hr>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel5(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>), </span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(), </span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">3e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
3.070420
</td>
<td>
2.586252
</td>
<td>
0.460775
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
1
</td>
<td>
2.154392
</td>
<td>
1.760734
</td>
<td>
0.471680
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.709090
</td>
<td>
1.851027
</td>
<td>
0.327311
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.523287
</td>
<td>
1.790196
</td>
<td>
0.412028
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
4
</td>
<td>
1.364664
</td>
<td>
1.816422
</td>
<td>
0.468262
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
5
</td>
<td>
1.247051
</td>
<td>
1.796951
</td>
<td>
0.493001
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
6
</td>
<td>
1.156087
</td>
<td>
1.907447
</td>
<td>
0.489095
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
7
</td>
<td>
1.073325
</td>
<td>
2.014389
</td>
<td>
0.499268
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.995001
</td>
<td>
2.056770
</td>
<td>
0.501139
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.927453
</td>
<td>
2.080244
</td>
<td>
0.503743
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
10
</td>
<td>
0.874861
</td>
<td>
2.084781
</td>
<td>
0.502441
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
11
</td>
<td>
0.837194
</td>
<td>
2.102611
</td>
<td>
0.514974
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
12
</td>
<td>
0.812340
</td>
<td>
2.111124
</td>
<td>
0.512126
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
13
</td>
<td>
0.797198
</td>
<td>
2.110253
</td>
<td>
0.513346
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
14
</td>
<td>
0.789102
</td>
<td>
2.108808
</td>
<td>
0.513997
</td>
<td>
00:00
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The multi-layer RNN performs worse than the single-layer RNN</p>
</section>
<section id="exploding-or-disappearing-activations" class="level3">
<h3 class="anchored" data-anchor-id="exploding-or-disappearing-activations">Exploding or Disappearing Activations</h3>
<ul>
<li><p>deeper models are more difficult to train</p>
<ul>
<li>performing matrix multiplication so many times can cause numbers to get extremely big or extremely small</li>
<li>floating point numbers get less accurate the further away they get from zero</li>
</ul></li>
<li><p><a href="https://www.volkerschatz.com/science/float.html">What you never wanted to know about floating point but will be forced to find out</a></p></li>
<li><p>Two types of layers are frequently used to avoid exploding activations in RNNs</p>
<ol type="1">
<li>Gated Recurrent Units (GRUs)</li>
<li>Long short-term memory (LSTM)</li>
</ol></li>
</ul>
</section>
</section>
<section id="lstm" class="level2">
<h2 class="anchored" data-anchor-id="lstm">LSTM</h2>
<ul>
<li>introduced in 1997 by <a href="https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber">Jürgen Schmidhuber</a> and <a href="https://en.wikipedia.org/wiki/Sepp_Hochreiter">Sepp Hochreiter</a></li>
<li>Normal RNNs are realy bad at retaining memory of what happened much earlier in a sentence</li>
<li>LSTMs maintain two hidden states to address this
<ul>
<li>cell state: responsible for keeping long short-term memory</li>
<li>hidden state: focuses on the next token to predict</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/17/The_LSTM_Cell.svg" class="img-fluid figure-img"></p>
<figcaption>lstm-cell</figcaption>
</figure>
</div>
<ul>
<li><span class="math inline">\(x_{t}\)</span> the current input</li>
<li><span class="math inline">\((h_{t-1})\)</span>: the previous hidden state</li>
<li><span class="math inline">\((c_{t-1})\)</span>: the previous hidden state</li>
<li><span class="math inline">\(\sigma\)</span>: sigmoid function</li>
<li><span class="math inline">\(tanh\)</span>: a sigmoid function rescaled to the range <span class="math inline">\([-1,1]\)</span></li>
<li><span class="math inline">\(tanh(x) = \frac{e^{x}+e^{-x}}{e^{x}-e^{-x}} = 2\sigma(2x)-1\)</span></li>
<li>four neural nets (orange) called gates (left to right):
<ol type="1">
<li><strong>forget gate:</strong> a linear layer followed by a sigmoid (i.e.&nbsp;output will be scalars [0,1])
<ul>
<li>multipy output by cell state to determine which information to keep</li>
<li>gives the LSTM the ability to forget things about its long-term state</li>
</ul></li>
<li><strong>input gate:</strong> works with the third gate (<code>tanh</code>) to update the cell state
<ul>
<li>decided which elements of the cell state to updates (values close to 1)</li>
</ul></li>
<li><strong>cell gate:</strong> determines what the updated values are for the cell state</li>
<li><strong>output gate:</strong> determines which information from the cell state to use to generate the new hidden state</li>
</ol></li>
</ul>
<hr>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">*</span>torch.sigmoid(<span class="dv">2</span><span class="op">*</span>tensor(<span class="fl">0.5</span>)) <span class="op">-</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor(0.4621)</code></pre>
<hr>
<div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>torch.tanh(tensor(<span class="fl">0.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor(0.4621)</code></pre>
<section id="building-an-lstm-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="building-an-lstm-from-scratch">Building an LSTM from Scratch</h3>
<div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCell(Module):</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ni, nh):</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.forget_gate <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_gate  <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cell_gate   <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_gate <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, state):</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        h,c <span class="op">=</span> state</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.cat([h, <span class="bu">input</span>], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>        forget <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.forget_gate(h))</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> c <span class="op">*</span> forget</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>        inp <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.input_gate(h))</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        cell <span class="op">=</span> torch.tanh(<span class="va">self</span>.cell_gate(h))</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> c <span class="op">+</span> inp <span class="op">*</span> cell</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.output_gate(h))</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> out <span class="op">*</span> torch.tanh(c)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h, (h,c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note:</strong> It is better for performance reasons to do one big matrix multiplication than four smaller ones</p>
<ul>
<li>launch the special fast kernel on the GPU only once</li>
<li>give the GPU more work to do in parallel</li>
</ul>
<hr>
<div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCell(Module):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ni, nh):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ih <span class="op">=</span> nn.Linear(ni,<span class="dv">4</span><span class="op">*</span>nh)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hh <span class="op">=</span> nn.Linear(nh,<span class="dv">4</span><span class="op">*</span>nh)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, state):</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>        h,c <span class="op">=</span> state</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One big multiplication for all the gates is better than 4 smaller ones</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        gates <span class="op">=</span> (<span class="va">self</span>.ih(<span class="bu">input</span>) <span class="op">+</span> <span class="va">self</span>.hh(h)).chunk(<span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        ingate,forgetgate,outgate <span class="op">=</span> <span class="bu">map</span>(torch.sigmoid, gates[:<span class="dv">3</span>])</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>        cellgate <span class="op">=</span> gates[<span class="dv">3</span>].tanh()</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> (forgetgate<span class="op">*</span>c) <span class="op">+</span> (ingate<span class="op">*</span>cellgate)</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> outgate <span class="op">*</span> c.tanh()</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h, (h,c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="torch-chunk" class="level4">
<h4 class="anchored" data-anchor-id="torch-chunk">torch chunk</h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.chunk.html">Documentation</a></li>
</ul>
<hr>
<div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(torch.chunk)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Help on built-in function chunk:

chunk(...)
    chunk(input, chunks, dim=0) -&gt; List of Tensors
    
    Attempts to split a tensor into the specified number of chunks. Each chunk is a view of
    the input tensor.
        
    .. note::
        This function may return less then the specified number of chunks!
    
    .. seealso::
    
        :func:`torch.tensor_split` a function that always returns exactly the specified number of chunks
    
    If the tensor size along the given dimesion :attr:`dim` is divisible by :attr:`chunks`,
    all returned chunks will be the same size.
    If the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`chunks`,
    all returned chunks will be the same size, except the last one.
    If such division is not possible, this function may return less
    than the specified number of chunks.
    
    Arguments:
        input (Tensor): the tensor to split
        chunks (int): number of chunks to return
        dim (int): dimension along which to split the tensor
    
    Example::
        &gt;&gt;&gt; torch.arange(11).chunk(6)
        (tensor([0, 1]),
         tensor([2, 3]),
         tensor([4, 5]),
         tensor([6, 7]),
         tensor([8, 9]),
         tensor([10]))
        &gt;&gt;&gt; torch.arange(12).chunk(6)
        (tensor([0, 1]),
         tensor([2, 3]),
         tensor([4, 5]),
         tensor([6, 7]),
         tensor([8, 9]),
         tensor([10, 11]))
        &gt;&gt;&gt; torch.arange(13).chunk(6)
        (tensor([0, 1, 2]),
         tensor([3, 4, 5]),
         tensor([6, 7, 8]),
         tensor([ 9, 10, 11]),
         tensor([12]))</code></pre>
<hr>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">0</span>,<span class="dv">10</span>)<span class="op">;</span> t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</code></pre>
<hr>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>t.chunk(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))</code></pre>
</section>
</section>
<section id="training-a-language-model-using-lstms" class="level3">
<h3 class="anchored" data-anchor-id="training-a-language-model-using-lstms">Training a Language Model Using LSTMs</h3>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel6(Module):</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers):</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [torch.zeros(n_layers, bs, n_hidden) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>        res,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [h_.detach() <span class="cf">for</span> h_ <span class="kw">in</span> h]</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(res)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): </span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.h: h.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using a two-layer LSTM</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel6(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>), </span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(), </span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>learn.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>LMModel6(
  (i_h): Embedding(30, 64)
  (rnn): LSTM(64, 64, num_layers=2, batch_first=True)
  (h_o): Linear(in_features=64, out_features=30, bias=True)
)</code></pre>
<hr>
<div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
3.013088
</td>
<td>
2.705310
</td>
<td>
0.417074
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
1
</td>
<td>
2.215323
</td>
<td>
1.904673
</td>
<td>
0.406657
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
2
</td>
<td>
1.622977
</td>
<td>
1.772446
</td>
<td>
0.438232
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
3
</td>
<td>
1.319893
</td>
<td>
1.853711
</td>
<td>
0.519613
</td>
<td>
00:00
</td>
</tr>
<tr>
<td>
4
</td>
<td>
1.096065
</td>
<td>
1.868788
</td>
<td>
0.554118
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.872888
</td>
<td>
1.679482
</td>
<td>
0.609375
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
6
</td>
<td>
0.590291
</td>
<td>
1.355017
</td>
<td>
0.661458
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
7
</td>
<td>
0.385917
</td>
<td>
1.319989
</td>
<td>
0.667887
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.284691
</td>
<td>
1.221118
</td>
<td>
0.689290
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.228731
</td>
<td>
1.181922
</td>
<td>
0.730632
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
10
</td>
<td>
0.172228
</td>
<td>
1.250237
</td>
<td>
0.727946
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
11
</td>
<td>
0.124468
</td>
<td>
1.155407
</td>
<td>
0.754720
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
12
</td>
<td>
0.090831
</td>
<td>
1.183195
</td>
<td>
0.749674
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
13
</td>
<td>
0.071399
</td>
<td>
1.179867
</td>
<td>
0.750081
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
14
</td>
<td>
0.061995
</td>
<td>
1.168421
</td>
<td>
0.753499
</td>
<td>
00:01
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> We were able to use a higher learning rate and achieve a much higher accuracy than the multi-layer RNN. <strong>Note:</strong> There is still some overfitting.</p>
</section>
</section>
<section id="regularizing-an-lstm" class="level2">
<h2 class="anchored" data-anchor-id="regularizing-an-lstm">Regularizing an LSTM</h2>
<ul>
<li><a href="https://arxiv.org/abs/1708.02182">Regularizing and Optimizing LSTM Language Models</a>
<ul>
<li>used an LSTM with dropout, activation regularization, and temporal activation regularization to beat state-of-the-art results that previously required much more complicated models</li>
<li>called the combination an <strong>AWD-LSTM</strong></li>
</ul></li>
</ul>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">Dropout</h3>
<ul>
<li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
<li><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li>randomly change some activations to zero at training time</li>
<li>makes activations more noisy</li>
<li>makes sure all neurons actively work toward the output</li>
<li>makes the model more robust</li>
<li>need to rescale activations after applying dropout
<ul>
<li>divide activations by <span class="math inline">\(1-p\)</span> where p is the probability to keep an activation</li>
</ul></li>
<li>using dropout before passing the output of our LSTM to the final layer will help reduce overfitting</li>
<li>make sure to turn off dropout during inference</li>
</ul>
<hr>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(Module):</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p): <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.training: <span class="cf">return</span> x</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> x.new(<span class="op">*</span>x.shape).bernoulli_(<span class="dv">1</span><span class="op">-</span>p)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> mask.div_(<span class="dv">1</span><span class="op">-</span>p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(torch.bernoulli)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Help on built-in function bernoulli:

bernoulli(...)
    bernoulli(input, *, generator=None, out=None) -&gt; Tensor
    
    Draws binary random numbers (0 or 1) from a Bernoulli distribution.
    
    The :attr:`input` tensor should be a tensor containing probabilities
    to be used for drawing the binary random number.
    Hence, all values in :attr:`input` have to be in the range:
    :math:`0 \leq \text{input}_i \leq 1`.
    
    The :math:`\text{i}^{th}` element of the output tensor will draw a
    value :math:`1` according to the :math:`\text{i}^{th}` probability value given
    in :attr:`input`.
    
    .. math::
        \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})
    
    The returned :attr:`out` tensor only has values 0 or 1 and is of the same
    shape as :attr:`input`.
    
    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating
    point ``dtype``.
    
    Args:
        input (Tensor): the input tensor of probability values for the Bernoulli distribution
    
    Keyword args:
        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
        out (Tensor, optional): the output tensor.
    
    Example::
    
        &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
        &gt;&gt;&gt; a
        tensor([[ 0.1737,  0.0950,  0.3609],
                [ 0.7148,  0.0289,  0.2676],
                [ 0.9456,  0.8937,  0.7202]])
        &gt;&gt;&gt; torch.bernoulli(a)
        tensor([[ 1.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 1.,  1.,  1.]])
    
        &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing "1" is 1
        &gt;&gt;&gt; torch.bernoulli(a)
        tensor([[ 1.,  1.,  1.],
                [ 1.,  1.,  1.],
                [ 1.,  1.,  1.]])
        &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing "1" is 0
        &gt;&gt;&gt; torch.bernoulli(a)
        tensor([[ 0.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 0.,  0.,  0.]])</code></pre>
</section>
<section id="activation-regularization-and-temporal-activation-regularization" class="level3">
<h3 class="anchored" data-anchor-id="activation-regularization-and-temporal-activation-regularization">Activation Regularization and Temporal Activation Regularization</h3>
<ul>
<li>both are similar to weight decay (AR)</li>
<li>activation regularization: try to make the final activations produced by the LSTM as small as possible
<ul>
<li><code>loss += alpha * activations.pow(2).mean()</code></li>
<li>often applied on dropped-out activations to not penalize the activations set to zero</li>
</ul></li>
<li>temporal activation regularization (TAR)
<ul>
<li>linked to the fact we are predicting tokens in a sentence</li>
<li>the outputs of our LSTMs should somewhat make sense when we read them in order</li>
<li>TAR encourages that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible</li>
<li><code>loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()</code>
<ul>
<li>alpha and beta are tunable hyperparameters</li>
</ul></li>
<li>applied to non-dropped-out activations (because the zeros in the dropped-out activations create big differences)</li>
</ul></li>
</ul>
</section>
<section id="training-a-weight-tied-regularized-lstm" class="level3">
<h3 class="anchored" data-anchor-id="training-a-weight-tied-regularized-lstm">Training a Weight-Tied Regularized LSTM</h3>
<ul>
<li>need to return the normal output from the LSTM, the dropped-out activations, and the activations from the LSTMs</li>
</ul>
<section id="weight-tying" class="level4">
<h4 class="anchored" data-anchor-id="weight-tying">Weight Tying</h4>
<ul>
<li>in a language model, the input embeddings represent a mapping from English words to activations and the output hidden layer represents a mapping from activations to English words
<ul>
<li>these mappings could be the same</li>
</ul></li>
<li>introduced in AWD-LSTM paper</li>
<li><code>self.h_o.weight = self.i_h.weight</code></li>
</ul>
<hr>
<div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel7(Module):</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers, p):</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p)</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o.weight <span class="op">=</span> <span class="va">self</span>.i_h.weight</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [torch.zeros(n_layers, bs, n_hidden) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>        raw,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.drop(raw)</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [h_.detach() <span class="cf">for</span> h_ <span class="kw">in</span> h]</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(out),raw,out</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): </span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.h: h.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel7(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>, <span class="fl">0.5</span>),</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(), metrics<span class="op">=</span>accuracy,</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>                cbs<span class="op">=</span>[ModelResetter, RNNRegularizer(alpha<span class="op">=</span><span class="dv">2</span>, beta<span class="op">=</span><span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>RNNRegularizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>fastai.callback.rnn.RNNRegularizer</code></pre>
<hr>
<div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>print_source(RNNRegularizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>class RNNRegularizer(Callback):
    "Add AR and TAR regularization"
    order,run_valid = RNNCallback.order+1,False
    def __init__(self, alpha=0., beta=0.): store_attr()
    def after_loss(self):
        if not self.training: return
        if self.alpha: self.learn.loss_grad += self.alpha * self.rnn.out.float().pow(2).mean()
        if self.beta:
            h = self.rnn.raw_out
            if len(h)&gt;1: self.learn.loss_grad += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()</code></pre>
<hr>
<div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> TextLearner(dls, LMModel7(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>, <span class="fl">0.4</span>),</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>                    loss_func<span class="op">=</span>CrossEntropyLossFlat(), metrics<span class="op">=</span>accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>learn.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>LMModel7(
  (i_h): Embedding(30, 64)
  (rnn): LSTM(64, 64, num_layers=2, batch_first=True)
  (drop): Dropout(p=0.4, inplace=False)
  (h_o): Linear(in_features=64, out_features=30, bias=True)
)</code></pre>
<hr>
<div class="sourceCode" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">1e-2</span>, wd<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto;">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
epoch
</th>
<th>
train_loss
</th>
<th>
valid_loss
</th>
<th>
accuracy
</th>
<th>
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
2.620218
</td>
<td>
1.797085
</td>
<td>
0.484294
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
1
</td>
<td>
1.622718
</td>
<td>
1.452620
</td>
<td>
0.652181
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0.864787
</td>
<td>
0.726230
</td>
<td>
0.773275
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
3
</td>
<td>
0.434755
</td>
<td>
0.699705
</td>
<td>
0.828613
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
4
</td>
<td>
0.225359
</td>
<td>
0.579946
</td>
<td>
0.842855
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
5
</td>
<td>
0.126518
</td>
<td>
0.571510
</td>
<td>
0.850911
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
6
</td>
<td>
0.076041
</td>
<td>
0.444107
</td>
<td>
0.874349
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
7
</td>
<td>
0.051340
</td>
<td>
0.366569
</td>
<td>
0.882487
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
8
</td>
<td>
0.037389
</td>
<td>
0.547799
</td>
<td>
0.854818
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0.027291
</td>
<td>
0.392787
</td>
<td>
0.880615
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
10
</td>
<td>
0.022100
</td>
<td>
0.354383
</td>
<td>
0.889648
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
11
</td>
<td>
0.018304
</td>
<td>
0.380172
</td>
<td>
0.885417
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
12
</td>
<td>
0.015668
</td>
<td>
0.384031
</td>
<td>
0.885010
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
13
</td>
<td>
0.013562
</td>
<td>
0.389092
</td>
<td>
0.884033
</td>
<td>
00:01
</td>
</tr>
<tr>
<td>
14
</td>
<td>
0.012376
</td>
<td>
0.383106
</td>
<td>
0.885254
</td>
<td>
00:01
</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> This performance is significantly better than the regular LSTM.</p>
</section>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/">Deep Learning for Coders with fastai &amp; PyTorch</a></li>
<li><a href="https://github.com/fastai/fastbook">The fastai book GitHub Repository</a></li>
</ul>
<p><strong>Previous:</strong> <a href="../chapter-11/">Notes on fastai Book Ch. 11</a></p>
<p><strong>Next:</strong> <a href="../chapter-13/">Notes on fastai Book Ch. 13</a></p>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>I’m Christian Mills, a deep learning consultant specializing in computer vision and practical AI implementations.</li>
<li>I help clients leverage cutting-edge AI technologies to solve real-world problems.</li>
<li>Learn more <a href="../../../about.html">about me</a> or reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a> to discuss your project.</li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>