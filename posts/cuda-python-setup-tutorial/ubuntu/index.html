<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-03-08">
<meta name="description" content="Learn how to setup up NVIDIA CUDA on Ubuntu with the Mamba/Conda package manager. Based on Jeremy Howard’s lecture, Getting Started With CUDA for Python Programmers.">

<title>Christian Mills - Setting Up CUDA for Python on Ubuntu</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Christian Mills - Setting Up CUDA for Python on Ubuntu">
<meta property="og:description" content="Learn how to setup up NVIDIA CUDA on Ubuntu with the Mamba/Conda package manager. Based on Jeremy Howard’s lecture, Getting Started With CUDA for Python Programmers.">
<meta property="og:image" content="christianjmills.com/posts/cuda-python-setup-tutorial/social-media/cover.jpg">
<meta property="og:site_name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Setting Up CUDA for Python on Ubuntu">
<meta name="twitter:description" content="Learn how to setup up NVIDIA CUDA on Ubuntu with the Mamba/Conda package manager. Based on Jeremy Howard’s lecture, Getting Started With CUDA for Python Programmers.">
<meta name="twitter:image" content="christianjmills.com/posts/cuda-python-setup-tutorial/social-media/cover.jpg">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#getting-started-with-the-code" id="toc-getting-started-with-the-code" class="nav-link" data-scroll-target="#getting-started-with-the-code">Getting Started with the Code</a></li>
  <li><a href="#installing-nvidia-drivers" id="toc-installing-nvidia-drivers" class="nav-link" data-scroll-target="#installing-nvidia-drivers">Installing NVIDIA Drivers</a>
  <ul>
  <li><a href="#check-for-existing-drivers" id="toc-check-for-existing-drivers" class="nav-link" data-scroll-target="#check-for-existing-drivers">Check for Existing Drivers</a></li>
  <li><a href="#view-available-drivers" id="toc-view-available-drivers" class="nav-link" data-scroll-target="#view-available-drivers">View available drivers</a></li>
  <li><a href="#install-the-drivers" id="toc-install-the-drivers" class="nav-link" data-scroll-target="#install-the-drivers">Install the Drivers</a></li>
  </ul></li>
  <li><a href="#setting-up-a-python-environment" id="toc-setting-up-a-python-environment" class="nav-link" data-scroll-target="#setting-up-a-python-environment">Setting Up a Python Environment</a>
  <ul>
  <li><a href="#install-mamba-package-manager" id="toc-install-mamba-package-manager" class="nav-link" data-scroll-target="#install-mamba-package-manager">Install Mamba Package Manager</a></li>
  <li><a href="#create-a-python-environment" id="toc-create-a-python-environment" class="nav-link" data-scroll-target="#create-a-python-environment">Create a Python Environment</a></li>
  <li><a href="#install-cuda-package" id="toc-install-cuda-package" class="nav-link" data-scroll-target="#install-cuda-package">Install CUDA Package</a></li>
  <li><a href="#install-pytorch" id="toc-install-pytorch" class="nav-link" data-scroll-target="#install-pytorch">Install PyTorch</a></li>
  <li><a href="#install-additional-dependencies" id="toc-install-additional-dependencies" class="nav-link" data-scroll-target="#install-additional-dependencies">Install additional dependencies</a></li>
  </ul></li>
  <li><a href="#importing-the-required-dependencies" id="toc-importing-the-required-dependencies" class="nav-link" data-scroll-target="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
  <li><a href="#setting-up-the-project" id="toc-setting-up-the-project" class="nav-link" data-scroll-target="#setting-up-the-project">Setting Up the Project</a>
  <ul>
  <li><a href="#capture-c-output" id="toc-capture-c-output" class="nav-link" data-scroll-target="#capture-c-output">Capture C++ Output</a></li>
  <li><a href="#make-cuda-operations-synchronous" id="toc-make-cuda-operations-synchronous" class="nav-link" data-scroll-target="#make-cuda-operations-synchronous">Make CUDA Operations Synchronous</a></li>
  </ul></li>
  <li><a href="#converting-rgb-images-to-grayscale-in-python" id="toc-converting-rgb-images-to-grayscale-in-python" class="nav-link" data-scroll-target="#converting-rgb-images-to-grayscale-in-python">Converting RGB Images to Grayscale in Python</a>
  <ul>
  <li><a href="#download-a-test-image" id="toc-download-a-test-image" class="nav-link" data-scroll-target="#download-a-test-image">Download a Test Image</a></li>
  <li><a href="#load-the-test-image" id="toc-load-the-test-image" class="nav-link" data-scroll-target="#load-the-test-image">Load the Test Image</a></li>
  <li><a href="#baseline-python-implementation" id="toc-baseline-python-implementation" class="nav-link" data-scroll-target="#baseline-python-implementation">Baseline Python Implementation</a>
  <ul class="collapse">
  <li><a href="#define-a-function-to-convert-an-rgb-tensor-to-grayscale" id="toc-define-a-function-to-convert-an-rgb-tensor-to-grayscale" class="nav-link" data-scroll-target="#define-a-function-to-convert-an-rgb-tensor-to-grayscale">Define a function to convert an RGB Tensor to Grayscale</a></li>
  <li><a href="#time-the-python-implementation" id="toc-time-the-python-implementation" class="nav-link" data-scroll-target="#time-the-python-implementation">Time the Python implementation</a></li>
  <li><a href="#verify-the-result" id="toc-verify-the-result" class="nav-link" data-scroll-target="#verify-the-result">Verify the Result</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#converting-rgb-images-to-grayscale-in-cuda" id="toc-converting-rgb-images-to-grayscale-in-cuda" class="nav-link" data-scroll-target="#converting-rgb-images-to-grayscale-in-cuda">Converting RGB Images to Grayscale in CUDA</a>
  <ul>
  <li><a href="#define-the-cuda-code" id="toc-define-the-cuda-code" class="nav-link" data-scroll-target="#define-the-cuda-code">Define the CUDA Code</a>
  <ul class="collapse">
  <li><a href="#define-the-utility-includes-and-definitions-for-pytorch-and-cuda" id="toc-define-the-utility-includes-and-definitions-for-pytorch-and-cuda" class="nav-link" data-scroll-target="#define-the-utility-includes-and-definitions-for-pytorch-and-cuda">Define the utility includes and definitions for PyTorch and CUDA</a></li>
  <li><a href="#define-the-cuda-kernel-for-rgb-to-grayscale-conversion" id="toc-define-the-cuda-kernel-for-rgb-to-grayscale-conversion" class="nav-link" data-scroll-target="#define-the-cuda-kernel-for-rgb-to-grayscale-conversion">Define the CUDA kernel for RGB to Grayscale conversion</a></li>
  <li><a href="#define-the-pytorch-function-to-convert-rgb-to-grayscale" id="toc-define-the-pytorch-function-to-convert-rgb-to-grayscale" class="nav-link" data-scroll-target="#define-the-pytorch-function-to-convert-rgb-to-grayscale">Define the PyTorch function to convert RGB to Grayscale</a></li>
  </ul></li>
  <li><a href="#build-the-pytorch-extension" id="toc-build-the-pytorch-extension" class="nav-link" data-scroll-target="#build-the-pytorch-extension">Build the PyTorch Extension</a></li>
  <li><a href="#getting-information-about-the-module" id="toc-getting-information-about-the-module" class="nav-link" data-scroll-target="#getting-information-about-the-module">Getting Information About the Module</a>
  <ul class="collapse">
  <li><a href="#get-the-module-path" id="toc-get-the-module-path" class="nav-link" data-scroll-target="#get-the-module-path">Get the module path</a></li>
  <li><a href="#get-the-module-content" id="toc-get-the-module-content" class="nav-link" data-scroll-target="#get-the-module-content">Get the module content</a></li>
  <li><a href="#get-the-module-attributes" id="toc-get-the-module-attributes" class="nav-link" data-scroll-target="#get-the-module-attributes">Get the module attributes</a></li>
  </ul></li>
  <li><a href="#test-the-pytorch-extension" id="toc-test-the-pytorch-extension" class="nav-link" data-scroll-target="#test-the-pytorch-extension">Test the PyTorch Extension</a>
  <ul class="collapse">
  <li><a href="#prepare-the-image-tensor" id="toc-prepare-the-image-tensor" class="nav-link" data-scroll-target="#prepare-the-image-tensor">Prepare the image tensor</a></li>
  <li><a href="#time-the-cuda-implementation" id="toc-time-the-cuda-implementation" class="nav-link" data-scroll-target="#time-the-cuda-implementation">Time the CUDA implementation</a></li>
  <li><a href="#verify-the-result-1" id="toc-verify-the-result-1" class="nav-link" data-scroll-target="#verify-the-result-1">Verify the Result</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Setting Up CUDA for Python on Ubuntu</h1>
  <div class="quarto-categories">
    <div class="quarto-category">mamba</div>
    <div class="quarto-category">conda</div>
    <div class="quarto-category">cuda</div>
    <div class="quarto-category">getting-started</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>

<div>
  <div class="description">
    Learn how to setup up NVIDIA CUDA on Ubuntu with the Mamba/Conda package manager. Based on Jeremy Howard’s lecture, <strong>Getting Started With CUDA for Python Programmers</strong>.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 8, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#getting-started-with-the-code">Getting Started with the Code</a></li>
<li><a href="#installing-nvidia-drivers">Installing NVIDIA Drivers</a></li>
<li><a href="#setting-up-a-python-environment">Setting Up a Python Environment</a></li>
<li><a href="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
<li><a href="#setting-up-the-project">Setting Up the Project</a></li>
<li><a href="#converting-rgb-images-to-grayscale-in-python">Converting RGB Images to Grayscale in Python</a></li>
<li><a href="#converting-rgb-images-to-grayscale-in-cuda">Converting RGB Images to Grayscale in CUDA</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This tutorial covers a convenient method for installing CUDA within a Python environment. CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for general computing on Graphics Processing Units (GPUs).</p>
<p>Installing CUDA directly within Python environments helps streamline working with projects that use custom CUDA kernels like <a href="https://github.com/IDEA-Research/GroundingDINO">GroundingDINO</a> and creating custom kernels ourselves. Writing custom CUDA kernels can be beneficial when optimizing computational tasks for speed and efficiency, such as fusing operations in a PyTorch project.</p>
<p>The core steps and examples in this tutorial are from the following lecture by <a href="https://jeremy.fast.ai/">Jeremy Howard</a>:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=nOxKexn3iBo"><strong>Getting Started With CUDA for Python Programmers</strong></a></li>
</ul>
<p>The tutorial covers each step, from installing NVIDIA graphics drivers in Ubuntu to verifying our CUDA installation by creating a custom kernel with PyTorch. While the provided steps for installing NVIDIA graphics drivers are specific to Ubuntu, the steps to install CUDA within Python environments should work for other Linux distros and WSL.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Installing Ubuntu">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Installing Ubuntu
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>The Ubuntu website provides <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">a step-by-step guide</a> to installing Ubuntu on your PC, and you can install it alongside an existing operating system.</p>
<ul>
<li><strong>Tutorial:</strong> <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">Install Ubuntu with a Bootable USB Stick</a></li>
</ul>
</div>
</div>
</div>
</section>
<section id="getting-started-with-the-code" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-the-code">Getting Started with the Code</h2>
<p>The tutorial code is available as a <a href="https://jupyter.org/">Jupyter Notebook</a>, which you can run locally or in a cloud-based environment like <a href="https://colab.research.google.com/">Google Colab</a>, which <a href="../../../posts/google-colab-getting-started-tutorial/#using-hardware-acceleration">provides free access</a> to a CUDA-enabled GPU. I have dedicated tutorials for those new to these platforms or who need guidance setting up:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Setup Guides">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Setup Guides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../posts/google-colab-getting-started-tutorial/"><strong>Getting Started with Google Colab</strong></a></li>
<li><a href="../../../posts/mamba-getting-started-tutorial-windows/"><strong>Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows</strong></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Tutorial Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tutorial Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<thead>
<tr class="header">
<th>Platform</th>
<th>Jupyter Notebook</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Google Colab</td>
<td><a href="https://colab.research.google.com/github/cj-mills/cuda-pytorch-tutorials/blob/main/notebooks/pytorch-verify-cuda-install.ipynb">Open In Colab</a></td>
</tr>
<tr class="even">
<td>Linux</td>
<td><a href="https://github.com/cj-mills/cuda-pytorch-tutorials/blob/main/notebooks/pytorch-verify-cuda-install.ipynb">GitHub Repository</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="installing-nvidia-drivers" class="level2">
<h2 class="anchored" data-anchor-id="installing-nvidia-drivers">Installing NVIDIA Drivers</h2>
<p>We need to ensure we have NVIDIA GPU drivers installed before installing CUDA, so let’s first check if we already have them.</p>
<section id="check-for-existing-drivers" class="level4">
<h4 class="anchored" data-anchor-id="check-for-existing-drivers">Check for Existing Drivers</h4>
<p>Open a terminal window (<code>Ctrl</code>+<code>Alt</code>+<code>T</code>) and run the following command to see if you already have NVIDIA drivers installed:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> /proc/driver/nvidia/version</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Sample output with NVIDIA drivers:</p>
<pre class="text"><code>NVRM version: NVIDIA UNIX x86_64 Kernel Module  535.154.05  Thu Dec 28 15:37:48 UTC 2023
GCC version:  gcc version 12.3.0 (Ubuntu 12.3.0-1ubuntu1~22.04) </code></pre>
<hr>
<p>You can skip to the <a href="#setting-up-a-python-environment">next section</a> if you already have NVIDIA drivers.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Uninstall Existing NVIDIA Drivers &amp; CUDA">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Uninstall Existing NVIDIA Drivers &amp; CUDA
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You can run the following terminal commands to uninstall any existing NVIDIA and CUDA packages, if you want to start fresh:</p>
<ul>
<li>Press <code>Ctrl</code>+<code>Alt</code>+<code>T</code> to open a terminal window.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove all installed NVIDIA packages</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get remove <span class="at">--purge</span> <span class="st">'nvidia-.*'</span> <span class="at">-y</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove all installed CUDA packages</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get remove <span class="at">--purge</span> <span class="st">'cuda-.*'</span> <span class="at">-y</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up any remaining dependencies</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get autoremove <span class="at">-y</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get autoclean</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the initial RAM filesystem to ensure it doesn't include any NVIDIA drivers</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> update-initramfs <span class="at">-u</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Reboot the system</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="view-available-drivers" class="level3">
<h3 class="anchored" data-anchor-id="view-available-drivers">View available drivers</h3>
<p>Ubuntu includes a convenient <a href="https://ubuntu.com/server/docs/nvidia-drivers-installation">command-line tool</a> for installing drivers. Open a terminal (<code>Ctrl</code>+<code>Alt</code>+<code>T</code>) and run the following command to see the available GPU drivers.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the available drivers for your hardware</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> ubuntu-drivers list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here is the output on Ubuntu 22.04 with an RTX 40-series GPU:</p>
<pre class="text"><code>$ sudo ubuntu-drivers list
nvidia-driver-545, (kernel modules provided by linux-modules-nvidia-545-generic-hwe-22.04)
nvidia-driver-535-open, (kernel modules provided by linux-modules-nvidia-535-open-generic-hwe-22.04)
nvidia-driver-545-open, (kernel modules provided by linux-modules-nvidia-545-open-generic-hwe-22.04)
nvidia-driver-525, (kernel modules provided by linux-modules-nvidia-525-generic-hwe-22.04)
nvidia-driver-535-server-open, (kernel modules provided by linux-modules-nvidia-535-server-open-generic-hwe-22.04)
nvidia-driver-535-server, (kernel modules provided by linux-modules-nvidia-535-server-generic-hwe-22.04)
nvidia-driver-525-open, (kernel modules provided by linux-modules-nvidia-525-open-generic-hwe-22.04)
nvidia-driver-535, (kernel modules provided by linux-modules-nvidia-535-generic-hwe-22.04)
nvidia-driver-525-server, (kernel modules provided by linux-modules-nvidia-525-server-generic-hwe-22.04)</code></pre>
<hr>
</section>
<section id="install-the-drivers" class="level3">
<h3 class="anchored" data-anchor-id="install-the-drivers">Install the Drivers</h3>
<p>Next, we run the <code>install</code> command where we can stick with the default driver version or manually specify one.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Default</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Manual</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install the driver that is considered the best match for your hardware</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> ubuntu-drivers install</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install driver version 535</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> ubuntu-drivers install nvidia:535</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>After that, we need to reboot the computer.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reboot the system</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once back in Ubuntu, we can create a Python environment and install CUDA.</p>
</section>
</section>
<section id="setting-up-a-python-environment" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-a-python-environment">Setting Up a Python Environment</h2>
<p>We will use the <a href="https://mamba.readthedocs.io/en/latest/">Mamba</a> package manager to create the Python environment. You can learn more about it in my <a href="../../../posts/mamba-getting-started-tutorial-windows/#introduction">getting started</a> tutorial. Feel free to use <a href="https://docs.anaconda.com/free/miniconda/">Conda</a> instead of Mamba if you already have that installed.</p>
<section id="install-mamba-package-manager" class="level3">
<h3 class="anchored" data-anchor-id="install-mamba-package-manager">Install Mamba Package Manager</h3>
<p>The following bash commands will download the latest release, install it, and relaunch the current bash shell to apply the relevant changes:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="st">"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-</span><span class="va">$(</span><span class="fu">uname</span><span class="va">)</span><span class="st">-</span><span class="va">$(</span><span class="fu">uname</span> <span class="at">-m</span><span class="va">)</span><span class="st">.sh"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">bash</span> Mambaforge-<span class="va">$(</span><span class="fu">uname</span><span class="va">)</span>-<span class="va">$(</span><span class="fu">uname</span> <span class="at">-m</span><span class="va">)</span>.sh <span class="at">-b</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ex">~/mambaforge/bin/mamba</span> init</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">bash</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="create-a-python-environment" class="level3">
<h3 class="anchored" data-anchor-id="create-a-python-environment">Create a Python Environment</h3>
<p>Next, we’ll create a Python environment and activate it.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Mamba</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Conda</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">--name</span> cuda-env python=3.11 <span class="at">-y</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate cuda-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">--name</span> cuda-env python=3.11 <span class="at">-y</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate cuda-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="install-cuda-package" class="level3">
<h3 class="anchored" data-anchor-id="install-cuda-package">Install CUDA Package</h3>
<p>We will later use PyTorch to verify our CUDA installation, so let’s ensure we install the appropriate CUDA version. You can use the link below to check the latest CUDA version supported by PyTorch. At the time of writing, that was CUDA version <code>12.1</code>.</p>
<ul>
<li><a href="https://pytorch.org/get-started/locally/">PyTorch: Get Started</a></li>
</ul>
<p>Run the following command to install CUDA in our Python environment with <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation">Conda/Mamba</a>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Mamba</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Conda</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> install cuda <span class="at">-c</span> nvidia/label/cuda-12.1.0 <span class="at">-y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install cuda <span class="at">-c</span> nvidia/label/cuda-12.1.0 <span class="at">-y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>The command installs CUDA into our <code>cuda-env</code> environment’s root directory.</p>
<pre class="text"><code>$ find $CONDA_PREFIX/include -name cuda
/home/innom-dt/mambaforge/envs/cuda-env/include/thrust/system/cuda
/home/innom-dt/mambaforge/envs/cuda-env/include/cuda</code></pre>
<hr>
</section>
<section id="install-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="install-pytorch">Install PyTorch</h3>
<p>Run the following command to install PyTorch with CUDA version <code>12.1</code>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Mamba</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Conda</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> install pytorch torchvision torchaudio pytorch-cuda=12.1 <span class="at">-c</span> pytorch <span class="at">-c</span> nvidia/label/cuda-12.1.0 <span class="at">-y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install pytorch torchvision torchaudio pytorch-cuda=12.1 <span class="at">-c</span> pytorch <span class="at">-c</span> nvidia/label/cuda-12.1.0 <span class="at">-y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="install-additional-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="install-additional-dependencies">Install additional dependencies</h3>
<p>We also need to install some additional libraries for our test code.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>jupyter</code></td>
<td>An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. (<a href="https://jupyter.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>ninja</code></td>
<td>Ninja is a small build system with a focus on speed. (<a href="https://pypi.org/project/ninja/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>pandas</code></td>
<td>This package provides fast, powerful, and flexible data analysis and manipulation tools. (<a href="https://pandas.pydata.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>pillow</code></td>
<td>The Python Imaging Library adds image processing capabilities. (<a href="https://pillow.readthedocs.io/en/stable/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>wurlitzer</code></td>
<td>Capture C-level output in context managers. (<a href="https://pypi.org/project/wurlitzer/">link</a>)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the following commands to install these additional libraries:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter ninja pandas pillow wurlitzer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With our environment set up, we can open our Jupyter Notebook and dive into the code.</p>
</section>
</section>
<section id="importing-the-required-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-required-dependencies">Importing the Required Dependencies</h2>
<p>First, we will import the necessary Python modules into our Jupyter Notebook and verify that PyTorch can find our CUDA installation.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Python Standard Library dependencies</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, gzip, pickle</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PIL for image manipulation</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch dependencies</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision <span class="im">as</span> tv</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> tvf</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> io</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.cpp_extension <span class="im">import</span> load_inline, CUDA_HOME</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the CUDA install path </span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(CUDA_HOME)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>/home/innom-dt/mambaforge/envs/cuda-env</code></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can tell from the above print statement that PyTorch correctly detected the path for our Python environment’s CUDA install.</p>
</div>
</div>
</section>
<section id="setting-up-the-project" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-project">Setting Up the Project</h2>
<p>In this section, we set up some basics for our project, such as enabling the capture of C/C++ output in the notebook and making CUDA operations synchronous for easier debugging.</p>
<section id="capture-c-output" class="level3">
<h3 class="anchored" data-anchor-id="capture-c-output">Capture C++ Output</h3>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable capture of C/C++ output in the notebook cells.</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext wurlitzer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="make-cuda-operations-synchronous" class="level3">
<h3 class="anchored" data-anchor-id="make-cuda-operations-synchronous">Make CUDA Operations Synchronous</h3>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set environment variable to make CUDA operations synchronous for easier debugging</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'CUDA_LAUNCH_BLOCKING'</span>]<span class="op">=</span><span class="st">'1'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="converting-rgb-images-to-grayscale-in-python" class="level2">
<h2 class="anchored" data-anchor-id="converting-rgb-images-to-grayscale-in-python">Converting RGB Images to Grayscale in Python</h2>
<p>To gauge the performance of CUDA over standard Python, we will convert an RGB image to grayscale.</p>
<section id="download-a-test-image" class="level3">
<h3 class="anchored" data-anchor-id="download-a-test-image">Download a Test Image</h3>
<p>We can download a copy of Vincent van Gogh’s <em>The Starry Night</em> painting from <a href="https://commons.wikimedia.org/wiki/Main_Page">Wikimedia Commons</a> for our test image.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the URL of the test image</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>test_img_url <span class="op">=</span> <span class="st">'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the file name from the URL</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>test_img_name <span class="op">=</span> Path(test_img_url).name</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Path object for the image file</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> Path(test_img_name)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the image file does not exist in the local directory</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> img_path.exists():</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the file does not exist, download it from the URL to the local directory</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    urlretrieve(test_img_url, img_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="load-the-test-image" class="level3">
<h3 class="anchored" data-anchor-id="load-the-test-image">Load the Test Image</h3>
<p>Once downloaded, we will load the image as an RGB tensor.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the test image as an RGB tensor</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>img_tensor <span class="op">=</span> io.read_image(<span class="bu">str</span>(img_path))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img_tensor.shape)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the image tensor</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>resized_img_tensor <span class="op">=</span> tvf.resize(img_tensor, <span class="dv">512</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(resized_img_tensor.shape)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the tensor as a PIL image</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>Image.fromarray(resized_img_tensor.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>torch.Size([3, 1014, 1280])
torch.Size([3, 512, 646])</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_16_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="baseline-python-implementation" class="level3">
<h3 class="anchored" data-anchor-id="baseline-python-implementation">Baseline Python Implementation</h3>
<p>Next, we will define a function that uses a standard for-loop to iterate through the RGB pixel values to create a new grayscale tensor.</p>
<section id="define-a-function-to-convert-an-rgb-tensor-to-grayscale" class="level4">
<h4 class="anchored" data-anchor-id="define-a-function-to-convert-an-rgb-tensor-to-grayscale">Define a function to convert an RGB Tensor to Grayscale</h4>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2gray_py(rgb_tensor, gray_coeffs<span class="op">=</span>[<span class="fl">0.2989</span>, <span class="fl">0.5870</span>, <span class="fl">0.1140</span>]):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the channel (c), height (h), and width (w) dimensions of the input image tensor</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    c, h, w <span class="op">=</span> rgb_tensor.shape</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the total number of pixels in the image (height * width)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> h <span class="op">*</span> w</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten the input image tensor from CxHxW format to a long array to simplify processing</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    rgb_tensor <span class="op">=</span> rgb_tensor.flatten()</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an empty tensor 'res' to hold the grayscale values, with the same datatype and device as the input</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> torch.empty(n, dtype<span class="op">=</span>rgb_tensor.dtype, device<span class="op">=</span>rgb_tensor.device)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop through each pixel to compute the grayscale value</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply the grayscale conversion formula: 0.2989 * Red + 0.5870 * Green + 0.1140 * Blue</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        res[i] <span class="op">=</span> gray_coeffs[<span class="dv">0</span>] <span class="op">*</span> rgb_tensor[i] <span class="op">+</span> gray_coeffs[<span class="dv">1</span>] <span class="op">*</span> rgb_tensor[i<span class="op">+</span>n] <span class="op">+</span> gray_coeffs[<span class="dv">2</span>] <span class="op">*</span> rgb_tensor[i<span class="op">+</span><span class="dv">2</span><span class="op">*</span>n]</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape the resulting flat grayscale values back into a 2D image (height x width)</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.view(h, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="time-the-python-implementation" class="level4">
<h4 class="anchored" data-anchor-id="time-the-python-implementation">Time the Python implementation</h4>
<p>We can use the IPython magic command <code>%%time</code> to gauge the performance of our Python implementation.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>img_gray_tensor <span class="op">=</span> rgb2gray_py(resized_img_tensor)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img_gray_tensor.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>torch.Size([512, 646])
CPU times: user 6.53 s, sys: 0 ns, total: 6.53 s
Wall time: 6.53 s</code></pre>
<p>On my system’s i7-11700K CPU, it takes nearly seven seconds to iterate through all the pixels in the <code>512x646</code> image.</p>
</section>
<section id="verify-the-result" class="level4">
<h4 class="anchored" data-anchor-id="verify-the-result">Verify the Result</h4>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the result as a PIL grayscale image</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>Image.fromarray(img_gray_tensor.numpy(), mode<span class="op">=</span><span class="st">'L'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_23_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>With our baseline established, let’s make an equivalent CUDA implementation to leverage the parallel processing capabilities of a GPU.</p>
</section>
</section>
</section>
<section id="converting-rgb-images-to-grayscale-in-cuda" class="level2">
<h2 class="anchored" data-anchor-id="converting-rgb-images-to-grayscale-in-cuda">Converting RGB Images to Grayscale in CUDA</h2>
<p>We can use PyTorch’s <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline"><code>load_inline</code></a> function to build a custom CUDA kernel and load it as a PyTorch extension. The function takes the CUDA code as a Python string and automatically compiles it.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Processing on NVIDA GPUs">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Processing on NVIDA GPUs
</div>
</div>
<div class="callout-body-container callout-body">
<section id="streaming-multiprocessors-sms" class="level5">
<h5 class="anchored" data-anchor-id="streaming-multiprocessors-sms">Streaming Multiprocessors (SMs):</h5>
<ul>
<li>SMs are the fundamental execution units in NVIDIA GPUs.</li>
<li>Each one can execute multiple threads concurrently.</li>
</ul>
</section>
<section id="thread-blocks" class="level5">
<h5 class="anchored" data-anchor-id="thread-blocks">Thread Blocks:</h5>
<ul>
<li>A thread block is a group of threads that can cooperate through shared memory and synchronization.</li>
<li>All threads in a block run on the same SM, meaning they can share resources such as shared memory and can synchronize their execution with each other.</li>
<li><code>2^31</code> max blocks for dim <code>0</code>, <code>2^16</code> max for dims <code>1</code> &amp; <code>2</code></li>
<li><code>1024</code> max threads per block (use a multiple of <code>32</code>)</li>
</ul>
</section>
<section id="shared-memory" class="level5">
<h5 class="anchored" data-anchor-id="shared-memory">Shared Memory:</h5>
<ul>
<li>Shared memory is a small space in GPU memory shared among the threads in a block.</li>
<li>It is much faster than global memory (the main GPU memory) but also limited in size.</li>
<li>Threads in the same block can use shared memory to share data efficiently.</li>
</ul>
</section>
<section id="concurrency" class="level5">
<h5 class="anchored" data-anchor-id="concurrency">Concurrency:</h5>
<ul>
<li>In CUDA, all threads in a block have the potential to run concurrently.</li>
<li>The actual concurrency depends on the number of CUDA cores per SM and the resources required by the threads.</li>
</ul>
</section>
</div>
</div>
<section id="define-the-cuda-code" class="level3">
<h3 class="anchored" data-anchor-id="define-the-cuda-code">Define the CUDA Code</h3>
<p>We will define the CUDA code in sections.</p>
<section id="define-the-utility-includes-and-definitions-for-pytorch-and-cuda" class="level4">
<h4 class="anchored" data-anchor-id="define-the-utility-includes-and-definitions-for-pytorch-and-cuda">Define the utility includes and definitions for PyTorch and CUDA</h4>
<p>Let’s start by adding the required header files and macros. The following code adds the headers for the PyTorch extension library and CUDA-specific exceptions for error handling.</p>
<p>It also defines a few macros to check if a tensor is on a CUDA device and if the tensor is contiguous in memory.</p>
<p>Lastly, it defines a function to calculate the ceiling of an integer division. We use this later for calculating how to split work on the GPU.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Include the necessary headers for PyTorch and CUDA functionality.</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/extension.h&gt;</span><span class="pp"> </span><span class="co">// PyTorch extension library for custom C++ and CUDA extensions.</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdio.h&gt;</span><span class="pp"> </span><span class="co">// Standard I/O for debugging (e.g., printf).</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;c10/cuda/CUDAException.h&gt;</span><span class="pp"> </span><span class="co">// CUDA-specific exceptions for error handling.</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to check if a tensor is allocated on a CUDA device.</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">// If not, it throws an error.</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">// x: The tensor to check.</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_CUDA</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>TORCH_CHECK<span class="op">(</span>x<span class="op">.</span>device<span class="op">().</span>is_cuda<span class="op">(),</span><span class="pp"> </span><span class="op">#</span>x<span class="pp"> </span><span class="st">" must be a CUDA tensor"</span><span class="op">)</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to check if a tensor is contiguous in memory.</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">// If not, it throws an error.</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">// Contiguous tensors are often required for efficient CUDA operations.</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">// x: The tensor to check.</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_CONTIGUOUS</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>TORCH_CHECK<span class="op">(</span>x<span class="op">.</span>is_contiguous<span class="op">(),</span><span class="pp"> </span><span class="op">#</span>x<span class="pp"> </span><span class="st">" must be contiguous"</span><span class="op">)</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to perform both CUDA and contiguity checks on a tensor.</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">// This is a convenience macro to ensure a tensor is suitable for CUDA operations.</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_INPUT</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>CHECK_CUDA<span class="op">(</span>x<span class="op">);</span><span class="pp"> </span>CHECK_CONTIGUOUS<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="co">// Function to calculate the ceiling of an integer division.</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="co">// This is often used to determine the number of blocks needed in a CUDA kernel launch</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a><span class="co">// when the total number of threads does not evenly divide by the number of threads per block.</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="co">// a: The numerator in the division.</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co">// b: The denominator in the division.</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a><span class="co">// Returns: The smallest integer greater than or equal to a/b.</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a><span class="kw">inline</span> <span class="dt">unsigned</span> <span class="dt">int</span> cdiv<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">int</span> a<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">int</span> b<span class="op">)</span> <span class="op">{</span> </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>a <span class="op">+</span> b <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> b<span class="op">;</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>cuda_utils_macros <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="vs">// Include the necessary headers for PyTorch and CUDA functionality.</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="vs">#include &lt;torch/extension.h&gt; // PyTorch extension library for custom C++ and CUDA extensions.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="vs">#include &lt;stdio.h&gt; // Standard I/O for debugging (e.g., printf).</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="vs">#include &lt;c10/cuda/CUDAException.h&gt; // CUDA-specific exceptions for error handling.</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to check if a tensor is allocated on a CUDA device.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="vs">// If not, it throws an error.</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="vs">// x: The tensor to check.</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="vs">#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to check if a tensor is contiguous in memory.</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="vs">// If not, it throws an error.</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="vs">// Contiguous tensors are often required for efficient CUDA operations.</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="vs">// x: The tensor to check.</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="vs">#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="vs">// Macro to perform both CUDA and contiguity checks on a tensor.</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="vs">// This is a convenience macro to ensure a tensor is suitable for CUDA operations.</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="vs">#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="vs">// Function to calculate the ceiling of an integer division.</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="vs">// This is often used to determine the number of blocks needed in a CUDA kernel launch</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="vs">// when the total number of threads does not evenly divide by the number of threads per block.</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="vs">// a: The numerator in the division.</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="vs">// b: The denominator in the division.</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="vs">// Returns: The smallest integer greater than or equal to a/b.</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a><span class="vs">inline unsigned int cdiv(unsigned int a, unsigned int b) { </span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="vs">    return (a + b - 1) / b;</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="define-the-cuda-kernel-for-rgb-to-grayscale-conversion" class="level4">
<h4 class="anchored" data-anchor-id="define-the-cuda-kernel-for-rgb-to-grayscale-conversion">Define the CUDA kernel for RGB to Grayscale conversion</h4>
<p>Next, we define the CUDA kernel for converting RGB tensors to grayscale. The kernel takes pointers to the memory locations for the input RGB and output grayscale tensors, along with the total number of pixels.</p>
<p>Instead of iterating through each pixel, the kernel applies the grayscale conversion formula to each pixel in parallel.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">// CUDA kernel to convert an RGB image to grayscale.</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Inputs:</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">// - x: Pointer to the input image data in RGB format.</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co">// - out: Pointer to the output image data in grayscale.</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">// - n: The total number of pixels in the image.</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>__global__ <span class="dt">void</span> rgb_to_grayscale_kernel<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> x<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the global thread index.</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> i <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the thread index is within the bounds of the image data.</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Convert the RGB values to grayscale using the luminosity method.</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">// The luminosity method is a weighted sum of the R, G, and B values.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Weights: 0.2989 for Red, 0.5870 for Green, and 0.1140 for Blue.</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="fl">0.2989</span> <span class="op">*</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> <span class="fl">0.5870</span> <span class="op">*</span> x<span class="op">[</span>i <span class="op">+</span> n<span class="op">]</span> <span class="op">+</span> <span class="fl">0.1140</span> <span class="op">*</span> x<span class="op">[</span>i <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> n<span class="op">];</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>cuda_kernel <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="vs">// CUDA kernel to convert an RGB image to grayscale.</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="vs">// Inputs:</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="vs">// - x: Pointer to the input image data in RGB format.</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="vs">// - out: Pointer to the output image data in grayscale.</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="vs">// - n: The total number of pixels in the image.</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="vs">__global__ void rgb_to_grayscale_kernel(unsigned char* x, unsigned char* out, int n) {</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate the global thread index.</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="vs">    int i = blockIdx.x * blockDim.x + threadIdx.x;</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Ensure the thread index is within the bounds of the image data.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="vs">    if (i &lt; n) {</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Convert the RGB values to grayscale using the luminosity method.</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="vs">        // The luminosity method is a weighted sum of the R, G, and B values.</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="vs">        // Weights: 0.2989 for Red, 0.5870 for Green, and 0.1140 for Blue.</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="vs">        out[i] = 0.2989 * x[i] + 0.5870 * x[i + n] + 0.1140 * x[i + 2 * n];</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>__global__</code> qualifier keyword indicates the kernel will be callable from the CPU or GPU and run on the GPU.</p>
</div>
</div>
</section>
<section id="define-the-pytorch-function-to-convert-rgb-to-grayscale" class="level4">
<h4 class="anchored" data-anchor-id="define-the-pytorch-function-to-convert-rgb-to-grayscale">Define the PyTorch function to convert RGB to Grayscale</h4>
<p>Last, we must define a PyTorch function to use the custom CUDA kernel within Python.</p>
<p>The function will take an RGB tensor as input, initialize the output grayscale tensor, launch the CUDA kernel, and return the updated grayscale tensor.</p>
<p>We need to specify how to divide the work for the list or pixel values among thread blocks on the GPU. For our function, we will set each block to have <code>256</code> threads and then use the number of pixels to determine how many blocks we need.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Function to convert a PyTorch tensor representing an RGB image to grayscale.</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Input:</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">// - input: A PyTorch tensor of the input RGB image.</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">// Output:</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">// - A PyTorch tensor of the output grayscale image.</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>torch<span class="op">::</span>Tensor rgb_to_grayscale<span class="op">(</span>torch<span class="op">::</span>Tensor input<span class="op">)</span> <span class="op">{</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Perform CUDA and contiguity checks</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    CHECK_INPUT<span class="op">(</span>input<span class="op">);</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Extract the height and width from the input tensor's dimensions.</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> h <span class="op">=</span> input<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> w <span class="op">=</span> input<span class="op">.</span>size<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    printf<span class="op">(</span><span class="st">"</span><span class="sc">\n</span><span class="st">h*w: </span><span class="sc">%d</span><span class="st">*</span><span class="sc">%d\n</span><span class="st">"</span><span class="op">,</span> h<span class="op">,</span> w<span class="op">);</span> <span class="co">// Debug print statement for dimensions.</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create an empty output tensor with the same dimensions as the input.</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> output <span class="op">=</span> torch<span class="op">::</span>empty<span class="op">({</span>h<span class="op">,</span> w<span class="op">},</span> input<span class="op">.</span>options<span class="op">());</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Define the number of threads per block.</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> threads <span class="op">=</span> <span class="dv">256</span><span class="op">;</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the number of blocks needed for the conversion, ensuring</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">// we have enough blocks to cover all pixels.</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Launch the CUDA kernel with calculated dimensions.</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    rgb_to_grayscale_kernel<span class="op">&lt;&lt;&lt;</span>cdiv<span class="op">(</span>w<span class="op">*</span>h<span class="op">,</span> threads<span class="op">),</span> threads<span class="op">&gt;&gt;&gt;(</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        input<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span> output<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span> w<span class="op">*</span>h<span class="op">);</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Check for any errors during kernel launch or execution.</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    C10_CUDA_KERNEL_LAUNCH_CHECK<span class="op">();</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Return the output tensor.</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output<span class="op">;</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>pytorch_function <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="vs">// Function to convert a PyTorch tensor representing an RGB image to grayscale.</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="vs">// Input:</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="vs">// - input: A PyTorch tensor of the input RGB image.</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="vs">// Output:</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="vs">// - A PyTorch tensor of the output grayscale image.</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="vs">torch::Tensor rgb_to_grayscale(torch::Tensor input) {</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Perform CUDA and contiguity checks</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="vs">    CHECK_INPUT(input);</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Extract the height and width from the input tensor's dimensions.</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="vs">    int h = input.size(1);</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="vs">    int w = input.size(2);</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="vs">    printf("\nh*w: </span><span class="sc">%d</span><span class="vs">*</span><span class="sc">%d</span><span class="vs">\n", h, w); // Debug print statement for dimensions.</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Create an empty output tensor with the same dimensions as the input.</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="vs">    auto output = torch::empty({h, w}, input.options());</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Define the number of threads per block.</span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="vs">    int threads = 256;</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Calculate the number of blocks needed for the conversion, ensuring</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="vs">    // we have enough blocks to cover all pixels.</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Launch the CUDA kernel with calculated dimensions.</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="vs">    rgb_to_grayscale_kernel&lt;&lt;&lt;cdiv(w*h, threads), threads&gt;&gt;&gt;(</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="vs">        input.data_ptr&lt;unsigned char&gt;(), output.data_ptr&lt;unsigned char&gt;(), w*h);</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Check for any errors during kernel launch or execution.</span></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="vs">    C10_CUDA_KERNEL_LAUNCH_CHECK();</span></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a><span class="vs">    // Return the output tensor.</span></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a><span class="vs">    return output;</span></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a><span class="vs">'''</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="build-the-pytorch-extension" class="level3">
<h3 class="anchored" data-anchor-id="build-the-pytorch-extension">Build the PyTorch Extension</h3>
<p>With the code for our CUDA file defined, we can pass it to the <code>load_inline</code> function to compile it.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the CUDA source code</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>cuda_src <span class="op">=</span> cuda_utils_macros <span class="op">+</span> cuda_kernel <span class="op">+</span> pytorch_function</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the C++ source code</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>cpp_src <span class="op">=</span> <span class="st">"torch::Tensor rgb_to_grayscale(torch::Tensor input);"</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># A flag indicating whether to use optimization flags for CUDA compilation.</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>opt<span class="op">=</span><span class="va">False</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile and load the CUDA and C++ sources as an inline PyTorch extension</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>module <span class="op">=</span> load_inline(</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    cuda_sources<span class="op">=</span>[cuda_src],                  <span class="co"># List of CUDA source code strings.</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    cpp_sources<span class="op">=</span>[cpp_src],                    <span class="co"># List of C++ source code strings.</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    functions<span class="op">=</span>[<span class="st">'rgb_to_grayscale'</span>],           <span class="co"># List of function names to be included in the extension.</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    extra_cuda_cflags<span class="op">=</span>[<span class="st">"-O2"</span>] <span class="cf">if</span> opt <span class="cf">else</span> [], <span class="co"># Enable optimization flags if `opt` is True.</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,                             <span class="co"># Enable verbose output if `verbose` is True.</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"inline_ext"</span>                         <span class="co"># Name of the generated extension module.</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>    Using /home/innom-dt/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
    Detected CUDA files, patching ldflags
    Emitting ninja build file /home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/build.ninja...
    Building extension module inline_ext...
    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)


    [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/TH -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/THC -isystem /home/innom-dt/mambaforge/envs/cuda-env/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/main.cpp -o main.o 
    [2/3] /home/innom-dt/mambaforge/envs/cuda-env/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/TH -isystem /home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/include/THC -isystem /home/innom-dt/mambaforge/envs/cuda-env/include -isystem /home/innom-dt/mambaforge/envs/cuda-env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -std=c++17 -c /home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/cuda.cu -o cuda.cuda.o 
    [3/3] c++ main.o cuda.cuda.o -shared -L/home/innom-dt/mambaforge/envs/cuda-env/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/innom-dt/mambaforge/envs/cuda-env/lib -lcudart -o inline_ext.so


    Loading extension module inline_ext...</code></pre>
</section>
<section id="getting-information-about-the-module" class="level3">
<h3 class="anchored" data-anchor-id="getting-information-about-the-module">Getting Information About the Module</h3>
<p>With our PyTorch extension loaded as a Python module, we can get the path to the module and explore the files created during the build process.</p>
<section id="get-the-module-path" class="level4">
<h4 class="anchored" data-anchor-id="get-the-module-path">Get the module path</h4>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the path to the extension module</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Module Path: </span><span class="sc">{</span>module<span class="sc">.</span><span class="va">__file__</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Module Path: /home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/inline_ext.so</code></pre>
</section>
<section id="get-the-module-content" class="level4">
<h4 class="anchored" data-anchor-id="get-the-module-content">Get the module content</h4>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the content of the module folder as a Pandas DataFrame</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(Path(module.<span class="va">__file__</span>).parent.iterdir())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/inline_ext.so
</td>
</tr>
<tr>
<th>
1
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/cuda.cu
</td>
</tr>
<tr>
<th>
2
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/.ninja_deps
</td>
</tr>
<tr>
<th>
3
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/main.cpp
</td>
</tr>
<tr>
<th>
4
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/build.ninja
</td>
</tr>
<tr>
<th>
5
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/.ninja_log
</td>
</tr>
<tr>
<th>
6
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/main.o
</td>
</tr>
<tr>
<th>
7
</th>
<td>
/home/innom-dt/.cache/torch_extensions/py311_cu121/inline_ext/cuda.cuda.o
</td>
</tr>
</tbody>
</table>
</div>
<p>If we open the <code>cuda.cu</code> file, we can see the code we fed the <code>load_inline</code> function:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="C++ Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
C++ Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/types.h&gt;</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda.h&gt;</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;cuda_runtime.h&gt;</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Include the necessary headers for PyTorch and CUDA functionality.</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;torch/extension.h&gt;</span><span class="pp"> </span><span class="co">// PyTorch extension library for custom C++ and CUDA extensions.</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdio.h&gt;</span><span class="pp"> </span><span class="co">// Standard I/O for debugging (e.g., printf).</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;c10/cuda/CUDAException.h&gt;</span><span class="pp"> </span><span class="co">// CUDA-specific exceptions for error handling.</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to check if a tensor is allocated on a CUDA device.</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">// If not, it throws an error.</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="co">// x: The tensor to check.</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_CUDA</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>TORCH_CHECK<span class="op">(</span>x<span class="op">.</span>device<span class="op">().</span>is_cuda<span class="op">(),</span><span class="pp"> </span><span class="op">#</span>x<span class="pp"> </span><span class="st">" must be a CUDA tensor"</span><span class="op">)</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to check if a tensor is contiguous in memory.</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="co">// If not, it throws an error.</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co">// Contiguous tensors are often required for efficient CUDA operations.</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co">// x: The tensor to check.</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_CONTIGUOUS</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>TORCH_CHECK<span class="op">(</span>x<span class="op">.</span>is_contiguous<span class="op">(),</span><span class="pp"> </span><span class="op">#</span>x<span class="pp"> </span><span class="st">" must be contiguous"</span><span class="op">)</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co">// Macro to perform both CUDA and contiguity checks on a tensor.</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="co">// This is a convenience macro to ensure a tensor is suitable for CUDA operations.</span></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CHECK_INPUT</span><span class="op">(</span>x<span class="op">)</span><span class="pp"> </span>CHECK_CUDA<span class="op">(</span>x<span class="op">);</span><span class="pp"> </span>CHECK_CONTIGUOUS<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="co">// Function to calculate the ceiling of an integer division.</span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a><span class="co">// This is often used to determine the number of blocks needed in a CUDA kernel launch</span></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="co">// when the total number of threads does not evenly divide by the number of threads per block.</span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="co">// a: The numerator in the division.</span></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co">// b: The denominator in the division.</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="co">// Returns: The smallest integer greater than or equal to a/b.</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="kw">inline</span> <span class="dt">unsigned</span> <span class="dt">int</span> cdiv<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">int</span> a<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">int</span> b<span class="op">)</span> <span class="op">{</span> </span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>a <span class="op">+</span> b <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> b<span class="op">;</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="co">// CUDA kernel to convert an RGB image to grayscale.</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a><span class="co">// Inputs:</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a><span class="co">// - x: Pointer to the input image data in RGB format.</span></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="co">// - out: Pointer to the output image data in grayscale.</span></span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a><span class="co">// - n: The total number of pixels in the image.</span></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>__global__ <span class="dt">void</span> rgb_to_grayscale_kernel<span class="op">(</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> x<span class="op">,</span> <span class="dt">unsigned</span> <span class="dt">char</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the global thread index.</span></span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> i <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the thread index is within the bounds of the image data.</span></span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Convert the RGB values to grayscale using the luminosity method.</span></span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">// The luminosity method is a weighted sum of the R, G, and B values.</span></span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Weights: 0.2989 for Red, 0.5870 for Green, and 0.1140 for Blue.</span></span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="fl">0.2989</span> <span class="op">*</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> <span class="fl">0.5870</span> <span class="op">*</span> x<span class="op">[</span>i <span class="op">+</span> n<span class="op">]</span> <span class="op">+</span> <span class="fl">0.1140</span> <span class="op">*</span> x<span class="op">[</span>i <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> n<span class="op">];</span></span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a><span class="co">// Function to convert a PyTorch tensor representing an RGB image to grayscale.</span></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a><span class="co">// Input:</span></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a><span class="co">// - input: A PyTorch tensor of the input RGB image.</span></span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a><span class="co">// Output:</span></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a><span class="co">// - A PyTorch tensor of the output grayscale image.</span></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a>torch<span class="op">::</span>Tensor rgb_to_grayscale<span class="op">(</span>torch<span class="op">::</span>Tensor input<span class="op">)</span> <span class="op">{</span></span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Perform CUDA and contiguity checks</span></span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>    CHECK_INPUT<span class="op">(</span>input<span class="op">);</span></span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Extract the height and width from the input tensor's dimensions.</span></span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> h <span class="op">=</span> input<span class="op">.</span>size<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> w <span class="op">=</span> input<span class="op">.</span>size<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>    printf<span class="op">(</span><span class="st">"</span><span class="sc">\n</span><span class="st">h*w: </span><span class="sc">%d</span><span class="st">*</span><span class="sc">%d\n</span><span class="st">"</span><span class="op">,</span> h<span class="op">,</span> w<span class="op">);</span> <span class="co">// Debug print statement for dimensions.</span></span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create an empty output tensor with the same dimensions as the input.</span></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> output <span class="op">=</span> torch<span class="op">::</span>empty<span class="op">({</span>h<span class="op">,</span> w<span class="op">},</span> input<span class="op">.</span>options<span class="op">());</span></span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Define the number of threads per block.</span></span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> threads <span class="op">=</span> <span class="dv">256</span><span class="op">;</span></span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Calculate the number of blocks needed for the conversion, ensuring</span></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">// we have enough blocks to cover all pixels.</span></span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Launch the CUDA kernel with calculated dimensions.</span></span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a>    rgb_to_grayscale_kernel<span class="op">&lt;&lt;&lt;</span>cdiv<span class="op">(</span>w<span class="op">*</span>h<span class="op">,</span> threads<span class="op">),</span> threads<span class="op">&gt;&gt;&gt;(</span></span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a>        input<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span> output<span class="op">.</span>data_ptr<span class="op">&lt;</span><span class="dt">unsigned</span> <span class="dt">char</span><span class="op">&gt;(),</span> w<span class="op">*</span>h<span class="op">);</span></span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Check for any errors during kernel launch or execution.</span></span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a>    C10_CUDA_KERNEL_LAUNCH_CHECK<span class="op">();</span></span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Return the output tensor.</span></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output<span class="op">;</span></span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="get-the-module-attributes" class="level4">
<h4 class="anchored" data-anchor-id="get-the-module-attributes">Get the module attributes</h4>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the attribute names of the module as a Pandas DataFrame</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(<span class="bu">dir</span>(module))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
<strong>doc</strong>
</td>
</tr>
<tr>
<th>
1
</th>
<td>
<strong>file</strong>
</td>
</tr>
<tr>
<th>
2
</th>
<td>
<strong>loader</strong>
</td>
</tr>
<tr>
<th>
3
</th>
<td>
<strong>name</strong>
</td>
</tr>
<tr>
<th>
4
</th>
<td>
<strong>package</strong>
</td>
</tr>
<tr>
<th>
5
</th>
<td>
<strong>spec</strong>
</td>
</tr>
<tr>
<th>
6
</th>
<td>
rgb_to_grayscale
</td>
</tr>
</tbody>
</table>
</div>
<p>We can see from the list of module attributes that the <code>rgb_to_grayscale</code> PyTorch function we defined is available.</p>
</section>
</section>
<section id="test-the-pytorch-extension" class="level3">
<h3 class="anchored" data-anchor-id="test-the-pytorch-extension">Test the PyTorch Extension</h3>
<p>Now, all that’s left is to test the custom CUDA kernel to see how it compares to the baseline Python implementation.</p>
<section id="prepare-the-image-tensor" class="level4">
<h4 class="anchored" data-anchor-id="prepare-the-image-tensor">Prepare the image tensor</h4>
<p>First, we must move the RGB tensor to the GPU and ensure it’s contiguous in memory.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the tensor to a contiguous format and move it to the default CUDA device</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>img_tensor_cuda <span class="op">=</span> resized_img_tensor.contiguous().cuda()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="time-the-cuda-implementation" class="level4">
<h4 class="anchored" data-anchor-id="time-the-cuda-implementation">Time the CUDA implementation</h4>
<p>We will use the same <code>%%time</code> magic command to gauge the performance.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> module.rgb_to_grayscale(img_tensor_cuda).cpu()</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>h,w <span class="op">=</span> res.shape</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>h,w,h<span class="op">*</span>w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>CPU times: user 725 µs, sys: 26 µs, total: 751 µs
Wall time: 559 µs

h*w: 512*646

(512, 646, 330752)</code></pre>
<p>As a reminder, the baseline Python implementation took <code>6.53</code> seconds (<code>6,530,000</code> microseconds (<code>µs</code>) for the same input on the CPU.</p>
</section>
<section id="verify-the-result-1" class="level4">
<h4 class="anchored" data-anchor-id="verify-the-result-1">Verify the Result</h4>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>Image.fromarray(res.numpy(), mode<span class="op">=</span><span class="st">'L'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_48_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This tutorial covered the steps for setting up CUDA within a Python environment and using PyTorch to build a custom CUDA kernel.</p>
<p>If you found this topic interesting, I recommend checking out the lectures from the CUDA MODE reading group and joining its discord:</p>
<ul>
<li><a href="https://github.com/cuda-mode/lectures">CUDA Mode Lectures</a></li>
<li><a href="https://discord.gg/cuda-mode-1189498204333543425">CUDA Mode Discord</a></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Getting Started With CUDA</strong> lecture by Jeremy Howard is <a href="https://github.com/cuda-mode/lectures?tab=readme-ov-file#lecture-3-getting-started-with-cuda">lecture 3</a>, and the follow-up is <a href="https://github.com/cuda-mode/lectures?tab=readme-ov-file#lecture-5-going-further-with-cuda-for-python-programmers">lecture 5</a>.</p>
</div>
</div>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Next Steps">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Next Steps
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Feel free to post questions or problems related to this tutorial in the comments below. I try to make time to address them on Thursdays and Fridays.</li>
<li>If you would like to explore my <a href="../../../about.html#services">services</a> for your project, you can reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a></li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>