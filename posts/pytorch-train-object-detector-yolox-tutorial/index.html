<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2023-08-21">
<meta name="description" content="Learn how to train YOLOX models for real-time object detection in PyTorch by creating a hand gesture detection model.">

<title>Christian Mills - Training YOLOX Models for Real-Time Object Detection in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Christian Mills - Training YOLOX Models for Real-Time Object Detection in PyTorch">
<meta property="og:description" content="Learn how to train YOLOX models for real-time object detection in PyTorch by creating a hand gesture detection model.">
<meta property="og:image" content="https://christianjmills.com/posts/social-media/cover.jpg">
<meta property="og:site_name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Training YOLOX Models for Real-Time Object Detection in PyTorch">
<meta name="twitter:description" content="Learn how to train YOLOX models for real-time object detection in PyTorch by creating a hand gesture detection model.">
<meta name="twitter:image" content="https://christianjmills.com/posts/social-media/cover.jpg">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#getting-started-with-the-code" id="toc-getting-started-with-the-code" class="nav-link" data-scroll-target="#getting-started-with-the-code">Getting Started with the Code</a></li>
  <li><a href="#setting-up-your-python-environment" id="toc-setting-up-your-python-environment" class="nav-link" data-scroll-target="#setting-up-your-python-environment">Setting Up Your Python Environment</a>
  <ul>
  <li><a href="#creating-a-python-environment" id="toc-creating-a-python-environment" class="nav-link" data-scroll-target="#creating-a-python-environment">Creating a Python Environment</a></li>
  <li><a href="#installing-pytorch" id="toc-installing-pytorch" class="nav-link" data-scroll-target="#installing-pytorch">Installing PyTorch</a></li>
  <li><a href="#installing-additional-libraries" id="toc-installing-additional-libraries" class="nav-link" data-scroll-target="#installing-additional-libraries">Installing Additional Libraries</a></li>
  <li><a href="#installing-utility-packages" id="toc-installing-utility-packages" class="nav-link" data-scroll-target="#installing-utility-packages">Installing Utility Packages</a></li>
  </ul></li>
  <li><a href="#importing-the-required-dependencies" id="toc-importing-the-required-dependencies" class="nav-link" data-scroll-target="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
  <li><a href="#setting-up-the-project" id="toc-setting-up-the-project" class="nav-link" data-scroll-target="#setting-up-the-project">Setting Up the Project</a>
  <ul>
  <li><a href="#setting-a-random-number-seed" id="toc-setting-a-random-number-seed" class="nav-link" data-scroll-target="#setting-a-random-number-seed">Setting a Random Number Seed</a></li>
  <li><a href="#setting-the-device-and-data-type" id="toc-setting-the-device-and-data-type" class="nav-link" data-scroll-target="#setting-the-device-and-data-type">Setting the Device and Data Type</a></li>
  <li><a href="#setting-the-directory-paths" id="toc-setting-the-directory-paths" class="nav-link" data-scroll-target="#setting-the-directory-paths">Setting the Directory Paths</a></li>
  </ul></li>
  <li><a href="#loading-and-exploring-the-dataset" id="toc-loading-and-exploring-the-dataset" class="nav-link" data-scroll-target="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a>
  <ul>
  <li><a href="#setting-the-dataset-path" id="toc-setting-the-dataset-path" class="nav-link" data-scroll-target="#setting-the-dataset-path">Setting the Dataset Path</a></li>
  <li><a href="#downloading-the-dataset" id="toc-downloading-the-dataset" class="nav-link" data-scroll-target="#downloading-the-dataset">Downloading the Dataset</a></li>
  <li><a href="#getting-the-image-and-annotation-folders" id="toc-getting-the-image-and-annotation-folders" class="nav-link" data-scroll-target="#getting-the-image-and-annotation-folders">Getting the Image and Annotation Folders</a></li>
  <li><a href="#get-image-file-paths" id="toc-get-image-file-paths" class="nav-link" data-scroll-target="#get-image-file-paths">Get Image File Paths</a></li>
  <li><a href="#get-bounding-box-annotations" id="toc-get-bounding-box-annotations" class="nav-link" data-scroll-target="#get-bounding-box-annotations">Get Bounding Box Annotations</a></li>
  <li><a href="#inspecting-the-class-distribution" id="toc-inspecting-the-class-distribution" class="nav-link" data-scroll-target="#inspecting-the-class-distribution">Inspecting the Class Distribution</a>
  <ul class="collapse">
  <li><a href="#get-image-classes" id="toc-get-image-classes" class="nav-link" data-scroll-target="#get-image-classes">Get image classes</a></li>
  <li><a href="#visualize-the-class-distribution" id="toc-visualize-the-class-distribution" class="nav-link" data-scroll-target="#visualize-the-class-distribution">Visualize the class distribution</a></li>
  </ul></li>
  <li><a href="#visualizing-bounding-box-annotations" id="toc-visualizing-bounding-box-annotations" class="nav-link" data-scroll-target="#visualizing-bounding-box-annotations">Visualizing Bounding Box Annotations</a>
  <ul class="collapse">
  <li><a href="#generate-a-color-map" id="toc-generate-a-color-map" class="nav-link" data-scroll-target="#generate-a-color-map">Generate a color map</a></li>
  <li><a href="#download-a-font-file" id="toc-download-a-font-file" class="nav-link" data-scroll-target="#download-a-font-file">Download a font file</a></li>
  <li><a href="#define-the-bounding-box-annotation-function" id="toc-define-the-bounding-box-annotation-function" class="nav-link" data-scroll-target="#define-the-bounding-box-annotation-function">Define the bounding box annotation function</a></li>
  </ul></li>
  <li><a href="#selecting-a-sample-image" id="toc-selecting-a-sample-image" class="nav-link" data-scroll-target="#selecting-a-sample-image">Selecting a Sample Image</a>
  <ul class="collapse">
  <li><a href="#load-the-sample-image" id="toc-load-the-sample-image" class="nav-link" data-scroll-target="#load-the-sample-image">Load the sample image</a></li>
  <li><a href="#inspect-the-corresponding-annotation-data" id="toc-inspect-the-corresponding-annotation-data" class="nav-link" data-scroll-target="#inspect-the-corresponding-annotation-data">Inspect the corresponding annotation data</a></li>
  <li><a href="#annotate-sample-image" id="toc-annotate-sample-image" class="nav-link" data-scroll-target="#annotate-sample-image">Annotate sample image</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#selecting-a-model" id="toc-selecting-a-model" class="nav-link" data-scroll-target="#selecting-a-model">Selecting a Model</a>
  <ul>
  <li><a href="#exploring-available-models" id="toc-exploring-available-models" class="nav-link" data-scroll-target="#exploring-available-models">Exploring Available Models</a></li>
  <li><a href="#loading-the-yolox-tiny-model" id="toc-loading-the-yolox-tiny-model" class="nav-link" data-scroll-target="#loading-the-yolox-tiny-model">Loading the YOLOX-Tiny Model</a></li>
  <li><a href="#get-normalization-statistics" id="toc-get-normalization-statistics" class="nav-link" data-scroll-target="#get-normalization-statistics">Get Normalization Statistics</a></li>
  <li><a href="#summarizing-the-model" id="toc-summarizing-the-model" class="nav-link" data-scroll-target="#summarizing-the-model">Summarizing the Model</a></li>
  </ul></li>
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data">Preparing the Data</a>
  <ul>
  <li><a href="#training-validation-split" id="toc-training-validation-split" class="nav-link" data-scroll-target="#training-validation-split">Training-Validation Split</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data Augmentation</a>
  <ul class="collapse">
  <li><a href="#set-training-image-size" id="toc-set-training-image-size" class="nav-link" data-scroll-target="#set-training-image-size">Set training image size</a></li>
  <li><a href="#initialize-the-transforms" id="toc-initialize-the-transforms" class="nav-link" data-scroll-target="#initialize-the-transforms">Initialize the transforms</a></li>
  <li><a href="#test-the-transforms" id="toc-test-the-transforms" class="nav-link" data-scroll-target="#test-the-transforms">Test the transforms</a></li>
  </ul></li>
  <li><a href="#training-dataset-class" id="toc-training-dataset-class" class="nav-link" data-scroll-target="#training-dataset-class">Training Dataset Class</a></li>
  <li><a href="#image-transforms" id="toc-image-transforms" class="nav-link" data-scroll-target="#image-transforms">Image Transforms</a></li>
  <li><a href="#initialize-datasets" id="toc-initialize-datasets" class="nav-link" data-scroll-target="#initialize-datasets">Initialize Datasets</a></li>
  <li><a href="#inspect-samples" id="toc-inspect-samples" class="nav-link" data-scroll-target="#inspect-samples">Inspect Samples</a>
  <ul class="collapse">
  <li><a href="#inspect-training-set-sample" id="toc-inspect-training-set-sample" class="nav-link" data-scroll-target="#inspect-training-set-sample">Inspect training set sample</a></li>
  <li><a href="#inspect-validation-set-sample" id="toc-inspect-validation-set-sample" class="nav-link" data-scroll-target="#inspect-validation-set-sample">Inspect validation set sample</a></li>
  </ul></li>
  <li><a href="#initialize-dataloaders" id="toc-initialize-dataloaders" class="nav-link" data-scroll-target="#initialize-dataloaders">Initialize DataLoaders</a></li>
  </ul></li>
  <li><a href="#fine-tuning-the-model" id="toc-fine-tuning-the-model" class="nav-link" data-scroll-target="#fine-tuning-the-model">Fine-tuning the Model</a>
  <ul>
  <li><a href="#define-the-training-loop" id="toc-define-the-training-loop" class="nav-link" data-scroll-target="#define-the-training-loop">Define the Training Loop</a></li>
  <li><a href="#set-the-model-checkpoint-path" id="toc-set-the-model-checkpoint-path" class="nav-link" data-scroll-target="#set-the-model-checkpoint-path">Set the Model Checkpoint Path</a></li>
  <li><a href="#save-the-normalization-stats" id="toc-save-the-normalization-stats" class="nav-link" data-scroll-target="#save-the-normalization-stats">Save the Normalization Stats</a></li>
  <li><a href="#save-the-color-map" id="toc-save-the-color-map" class="nav-link" data-scroll-target="#save-the-color-map">Save the Color Map</a></li>
  <li><a href="#configure-the-training-parameters" id="toc-configure-the-training-parameters" class="nav-link" data-scroll-target="#configure-the-training-parameters">Configure the Training Parameters</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">Train the Model</a></li>
  </ul></li>
  <li><a href="#making-predictions-with-the-model" id="toc-making-predictions-with-the-model" class="nav-link" data-scroll-target="#making-predictions-with-the-model">Making Predictions with the Model</a>
  <ul>
  <li><a href="#preparing-the-model-for-inference" id="toc-preparing-the-model-for-inference" class="nav-link" data-scroll-target="#preparing-the-model-for-inference">Preparing the Model for Inference</a>
  <ul class="collapse">
  <li><a href="#wrap-the-model-with-preprocessing-and-post-processing-steps" id="toc-wrap-the-model-with-preprocessing-and-post-processing-steps" class="nav-link" data-scroll-target="#wrap-the-model-with-preprocessing-and-post-processing-steps">Wrap the model with preprocessing and post-processing steps</a></li>
  </ul></li>
  <li><a href="#preparing-input-data" id="toc-preparing-input-data" class="nav-link" data-scroll-target="#preparing-input-data">Preparing Input Data</a>
  <ul class="collapse">
  <li><a href="#pass-the-input-data-to-the-model" id="toc-pass-the-input-data-to-the-model" class="nav-link" data-scroll-target="#pass-the-input-data-to-the-model">Pass the input data to the model</a></li>
  </ul></li>
  <li><a href="#filtering-model-output" id="toc-filtering-model-output" class="nav-link" data-scroll-target="#filtering-model-output">Filtering Model Output</a>
  <ul class="collapse">
  <li><a href="#annotate-image-using-bounding-box-proposals" id="toc-annotate-image-using-bounding-box-proposals" class="nav-link" data-scroll-target="#annotate-image-using-bounding-box-proposals">Annotate image using bounding box proposals</a></li>
  </ul></li>
  <li><a href="#testing-the-model-on-new-data" id="toc-testing-the-model-on-new-data" class="nav-link" data-scroll-target="#testing-the-model-on-new-data">Testing the Model on New Data</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#recommended-tutorials" id="toc-recommended-tutorials" class="nav-link" data-scroll-target="#recommended-tutorials">Recommended Tutorials</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Training YOLOX Models for Real-Time Object Detection in PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">object-detection</div>
    <div class="quarto-category">yolox</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>

<div>
  <div class="description">
    Learn how to train YOLOX models for real-time object detection in PyTorch by creating a hand gesture detection model.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 21, 2023</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">March 30, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="../../series/tutorials/pytorch-train-object-detector-yolox-series.html"><strong>Training YOLOX Models for Real-Time Object Detection in PyTorch</strong></a></p>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#getting-started-with-the-code">Getting Started with the Code</a></li>
<li><a href="#setting-up-your-python-environment">Setting Up Your Python Environment</a></li>
<li><a href="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
<li><a href="#setting-up-the-project">Setting Up the Project</a></li>
<li><a href="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a></li>
<li><a href="#selecting-a-model">Selecting a Model</a></li>
<li><a href="#preparing-the-data">Preparing the Data</a></li>
<li><a href="#fine-tuning-the-model">Fine-tuning the Model</a></li>
<li><a href="#making-predictions-with-the-model">Making Predictions with the Model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to this hands-on guide to training real-time object detection models in <a href="https://pytorch.org/">PyTorch</a>. Object detectors can identify and locate multiple objects within images and videos, allowing you to quantify them and track their location. The <a href="https://arxiv.org/abs/2107.08430">YOLOX</a> model we’ll use is fast and accurate, making it well-suited for real-time applications.</p>
<p>In this tutorial, we create a hand gesture detector that identifies and locates various hand gestures within images. Real-time gesture detection has many applications ranging from human-computer interaction and sign-language interpretation to augmented reality and interactive gaming experiences.</p>
<p>After completing the tutorial, you will have a real-time gesture detector, plus a blueprint for training YOLOX models on other object detection tasks. You can then deploy models trained with this tutorial’s code in real-time applications. For an example, check out the demo video below from a <a href="https://github.com/cj-mills/barracuda-inference-yolox-demo">project</a> made with the <a href="https://unity.com/">Unity game engine</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="./video/barracuda-inference-yolox-demo.mp4" class="img-fluid quarto-figure quarto-figure-center" controls=""><a href="./video/barracuda-inference-yolox-demo.mp4">Video</a></video></p>
</figure>
</div>
<p>This tutorial is suitable for anyone with rudimentary PyTorch experience. If you are new to PyTorch and want to start with a beginner-focused project, check out my tutorial on fine-tuning image classifiers.</p>
<ul>
<li><a href="../pytorch-train-image-classifier-timm-hf-tutorial/">Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners</a></li>
</ul>
</section>
<section id="getting-started-with-the-code" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-the-code">Getting Started with the Code</h2>
<p>The tutorial code is available as a <a href="https://jupyter.org/">Jupyter Notebook</a>, which you can run locally or in a cloud-based environment like <a href="https://colab.research.google.com/">Google Colab</a>. I have dedicated tutorials for those new to these platforms or who need guidance setting up:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Setup Guides">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Setup Guides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><a href="../../posts/google-colab-getting-started-tutorial/"><strong>Getting Started with Google Colab</strong></a></p></li>
<li><p><a href="../../posts/mamba-getting-started-tutorial-windows/"><strong>Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows</strong></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Tutorial Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tutorial Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Platform</th>
<th>Jupyter Notebook</th>
<th>Utility File</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Google Colab</td>
<td><a href="https://colab.research.google.com/github/cj-mills/pytorch-yolox-object-detection-tutorial-code/blob/main/notebooks/pytorch-yolox-object-detector-training-colab.ipynb">Open In Colab</a></td>
<td></td>
</tr>
<tr class="even">
<td>Linux</td>
<td><a href="https://github.com/cj-mills/pytorch-yolox-object-detection-tutorial-code/blob/main/notebooks/pytorch-yolox-object-detector-training.ipynb">GitHub Repository</a></td>
<td></td>
</tr>
<tr class="odd">
<td>Windows</td>
<td><a href="https://github.com/cj-mills/pytorch-yolox-object-detection-tutorial-code/blob/main/notebooks/pytorch-yolox-object-detector-training-windows.ipynb">GitHub Repository</a></td>
<td><a href="https://github.com/cj-mills/pytorch-yolox-object-detection-tutorial-code/blob/main/notebooks/windows_utils.py">windows_utils.py</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="COCO Annotation Format">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
COCO Annotation Format
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’ve added a notebook for training models on the <a href="https://cocodataset.org/#home">COCO dataset</a> to this tutorial’s GitHub repository. You can modify it to train models on other datasets that follow the COCO annotation format.</p>
<ul>
<li><a href="https://github.com/cj-mills/pytorch-yolox-object-detection-tutorial-code/blob/main/notebooks/pytorch-yolox-object-detector-training-coco.ipynb">pytorch-yolox-object-detector-training-coco.ipynb</a></li>
</ul>
</div>
</div>
</section>
<section id="setting-up-your-python-environment" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-your-python-environment">Setting Up Your Python Environment</h2>
<p>Before diving into the code, we’ll cover the steps to create a local Python environment and install the necessary dependencies. The dedicated Colab Notebook includes the code to install the required dependencies in Google Colab.</p>
<section id="creating-a-python-environment" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-python-environment">Creating a Python Environment</h3>
<p>First, we’ll create a Python environment using <a href="https://docs.conda.io/en/latest/">Conda</a>/<a href="https://mamba.readthedocs.io/en/latest/">Mamba</a>. Open a terminal with Conda/Mamba installed and run the following commands:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Conda</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Mamba</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.10 environment</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">--name</span> pytorch-env python=3.10 <span class="at">-y</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.10 environment</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">--name</span> pytorch-env python=3.10 <span class="at">-y</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="installing-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="installing-pytorch">Installing PyTorch</h3>
<p>Next, we’ll install PyTorch. Run the appropriate command for your hardware and operating system.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Linux/Windows (CUDA)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Mac</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">Linux (CPU)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-4" role="tab" aria-controls="tabset-2-4" aria-selected="false">Windows (CPU)</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MPS (Metal Performance Shaders) acceleration is available on MacOS 12.3+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch for CPU only</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-4-tab">
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch for CPU only</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="installing-additional-libraries" class="level3">
<h3 class="anchored" data-anchor-id="installing-additional-libraries">Installing Additional Libraries</h3>
<p>We also need to install some additional libraries for our project.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>jupyter</code></td>
<td>An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. (<a href="https://jupyter.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>matplotlib</code></td>
<td>This package provides a comprehensive collection of visualization tools to create high-quality plots, charts, and graphs for data exploration and presentation. (<a href="https://matplotlib.org/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>pandas</code></td>
<td>This package provides fast, powerful, and flexible data analysis and manipulation tools. (<a href="https://pandas.pydata.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>pillow</code></td>
<td>The Python Imaging Library adds image processing capabilities. (<a href="https://pillow.readthedocs.io/en/stable/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>torchtnt</code></td>
<td>A library for PyTorch training tools and utilities. (<a href="https://pytorch.org/tnt/stable/">link</a>)</td>
</tr>
<tr class="even">
<td><code>tqdm</code></td>
<td>A Python library that provides fast, extensible progress bars for loops and other iterable objects in Python. (<a href="https://tqdm.github.io/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>tabulate</code></td>
<td>Pretty-print tabular data in Python. (<a href="https://pypi.org/project/tabulate/">link</a>)</td>
</tr>
<tr class="even">
<td><code>pyarrow</code></td>
<td>This library provides a Python API for functionality provided by the <a href="https://arrow.apache.org/">Arrow</a> C++ libraries, along with tools for Arrow integration and interoperability with pandas. (<a href="https://pypi.org/project/pyarrow/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>fastparquet</code></td>
<td>A python implementation of the parquet format, used implicitly by Pandas. (<a href="https://pypi.org/project/fastparquet/">link</a>)</td>
</tr>
<tr class="even">
<td><code>distinctipy</code></td>
<td>A lightweight python package providing functions to generate colours that are visually distinct from one another. (<a href="https://distinctipy.readthedocs.io/en/latest/">link</a>)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the following commands to install these additional libraries:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install additional dependencies</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install distinctipy jupyter matplotlib pandas pillow torchtnt==0.2.0 tqdm</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Install extra dependencies for pandas</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tabulate pyarrow fastparquet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="installing-utility-packages" class="level3">
<h3 class="anchored" data-anchor-id="installing-utility-packages">Installing Utility Packages</h3>
<p>Walking through the code for the YOLOX model and the code for computing loss values during training would make this tutorial unreasonably long. Therefore, I included that code in a dedicated <a href="https://pypi.org/project/cjm-yolox-pytorch/">pip package.</a> A link to the documentation is available in the table below.</p>
<p>We’ll also install some additional packages I made to help us handle images, interact with PyTorch, and work with Pandas DataFrames. These utility packages provide shortcuts for routine tasks and keep our code clean and readable.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cjm_pandas_utils</code></td>
<td>Some utility functions for working with Pandas. (<a href="https://cj-mills.github.io/cjm-pandas-utils/">link</a>)</td>
</tr>
<tr class="even">
<td><code>cjm_pil_utils</code></td>
<td>Some PIL utility functions I frequently use. (<a href="https://cj-mills.github.io/cjm-pil-utils/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>cjm_psl_utils</code></td>
<td>Some utility functions using the Python Standard Library. (<a href="https://cj-mills.github.io/cjm-psl-utils/">link</a>)</td>
</tr>
<tr class="even">
<td><code>cjm_pytorch_utils</code></td>
<td>Some utility functions for working with PyTorch. (<a href="https://cj-mills.github.io/cjm-pytorch-utils/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>cjm_torchvision_tfms</code></td>
<td>Some custom Torchvision tranforms. (<a href="https://cj-mills.github.io/cjm-torchvision-tfms/">link</a>)</td>
</tr>
<tr class="even">
<td><code>cjm_yolox_pytorch</code></td>
<td>A PyTorch implementation of the <a href="https://arxiv.org/abs/2107.08430">YOLOX</a> object detection model based on <a href="https://github.com/open-mmlab">OpenMMLab</a>’s implementation in the <a href="https://github.com/open-mmlab/mmdetection">mmdetection</a> library. (<a href="https://cj-mills.github.io/cjm-yolox-pytorch/">link</a>)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the following commands to install the utility packages:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install YOLOX utility package</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>pip install cjm_yolox_pytorch</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Install additional utility packages</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>pip install cjm_pandas_utils cjm_pil_utils cjm_psl_utils cjm_pytorch_utils cjm_torchvision_tfms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="importing-the-required-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-required-dependencies">Importing the Required Dependencies</h2>
<p>With our environment set up, let’s dive into the code. First, we will import the necessary Python packages into our Jupyter Notebook.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Python Standard Library dependencies</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> multiprocessing</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Import utility functions</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_psl_utils.core <span class="im">import</span> download_file, file_extract</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pil_utils.core <span class="im">import</span> resize_img, get_img_files, stack_imgs</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pytorch_utils.core <span class="im">import</span> tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pandas_utils.core <span class="im">import</span> markdown_to_pandas, convert_to_numeric, convert_to_string</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_torchvision_tfms.core <span class="im">import</span> ResizeMax, PadSquare, CustomRandomIoUCrop, CustomRandomAugment</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Import YOLOX package</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_yolox_pytorch.model <span class="im">import</span> build_model, MODEL_CFGS, NORM_STATS</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_yolox_pytorch.utils <span class="im">import</span> generate_output_grids</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_yolox_pytorch.loss <span class="im">import</span> YOLOXLoss</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_yolox_pytorch.inference <span class="im">import</span> YOLOXInferenceWrapper</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the distinctipy module</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> distinctipy <span class="im">import</span> distinctipy</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib for creating plots</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PIL for image manipulation</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch dependencies</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.amp <span class="im">import</span> autocast</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtnt.utils <span class="im">import</span> get_module_summary</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Import torchvision dependencies</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>torchvision.disable_beta_transforms_warning()</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.tv_tensors <span class="im">import</span> BoundingBoxes</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.v2  <span class="im">as</span> transforms</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> functional <span class="im">as</span> TF</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="setting-up-the-project" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-project">Setting Up the Project</h2>
<p>In this section, we set up some basics for our project, such as initializing random number generators, setting the PyTorch device to run the model, and preparing the folders for our project and datasets.</p>
<section id="setting-a-random-number-seed" class="level3">
<h3 class="anchored" data-anchor-id="setting-a-random-number-seed">Setting a Random Number Seed</h3>
<p>First, we set the seed for generating random numbers using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#set_seed"><code>set_seed</code></a> function from the <code>cjm_pytorch_utils</code> package.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1234</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>set_seed(seed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="setting-the-device-and-data-type" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-device-and-data-type">Setting the Device and Data Type</h3>
<p>Next, we determine the device to use for training and set the data type of our tensors using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#get_torch_device"><code>get_torch_device</code></a> function from the <code>cjm_pytorch_utils</code> package.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_torch_device()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> torch.float32</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>device, dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>('cuda', torch.float32)</code></pre>
</section>
<section id="setting-the-directory-paths" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-directory-paths">Setting the Directory Paths</h3>
<p>We can then set up a directory for our project to store our results and other related files. The following code creates the folder in the current directory (<code>./</code>). Update the path if that is not suitable for you.</p>
<p>We also need a place to store our datasets and a location to download the zip file containing the dataset. Readers following the tutorial on their local machine should select locations with read and write access to store archive files and datasets. For a cloud service like Google Colab, you can set it to the current directory.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The name for the project</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>project_name <span class="op">=</span> <span class="ss">f"pytorch-yolox-object-detector"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The path for the project folder</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>project_dir <span class="op">=</span> Path(<span class="ss">f"./</span><span class="sc">{</span>project_name<span class="sc">}</span><span class="ss">/"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the project directory if it does not already exist</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>project_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store datasets</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>dataset_dir <span class="op">=</span> Path(<span class="st">"/mnt/980_1TB_2/Datasets/"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the dataset directory if it does not exist</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>dataset_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store archive files</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>archive_dir <span class="op">=</span> dataset_dir<span class="op">/</span><span class="st">'../Archive'</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the archive directory if it does not exist</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>archive_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project Directory:"</span>: project_dir, </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Directory:"</span>: dataset_dir, </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Directory:"</span>: archive_dir</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_47dc1">
<thead>
</thead>
<tbody>
<tr>
<th id="T_47dc1_level0_row0" class="row_heading level0 row0">
Project Directory:
</th>
<td id="T_47dc1_row0_col0" class="data row0 col0">
pytorch-yolox-object-detector
</td>
</tr>
<tr>
<th id="T_47dc1_level0_row1" class="row_heading level0 row1">
Dataset Directory:
</th>
<td id="T_47dc1_row1_col0" class="data row1 col0">
/mnt/980_1TB_2/Datasets
</td>
</tr>
<tr>
<th id="T_47dc1_level0_row2" class="row_heading level0 row2">
Archive Directory:
</th>
<td id="T_47dc1_row2_col0" class="data row2 col0">
/mnt/980_1TB_2/Datasets/../Archive
</td>
</tr>
</tbody>
</table>
</div>
<p>Double-check the project and dataset directories exist in the specified paths and that you can add files to them before continuing. At this point, our project is set up and ready to go. In the next section, we will download and explore the dataset.</p>
</section>
</section>
<section id="loading-and-exploring-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-exploring-the-dataset">Loading and Exploring the Dataset</h2>
<p>Now that we set up the project, we can start working with our dataset. The dataset is a downscaled subset of <a href="https://github.com/hukenovs/hagrid">HaGRID</a> (HAnd Gesture Recognition Image Dataset), which contains 18 distinct hand gestures and an additional <code>no_gesture</code> class for idle hands.</p>
<p>I made four subsets of different sizes available on Hugging Face Hub. The most compact subset of the dataset is approximately 1 GB, with 31,833 samples. You will need about 2 GB to store the archive file and extracted dataset.</p>
<p>We will download and access the dataset directly rather than through Hugging Face’s <a href="https://huggingface.co/docs/datasets/index">Datasets</a> library.</p>
<section id="setting-the-dataset-path" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-dataset-path">Setting the Dataset Path</h3>
<p>We first need to construct the name for the chosen Hugging Face Hub dataset and define where to download and extract the dataset.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the name of the dataset</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">'hagrid-sample-30k-384p'</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset_name = 'hagrid-sample-120k-384p'</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset_name = 'hagrid-sample-250k-384p'</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset_name = 'hagrid-sample-500k-384p'</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset name by combining the username and dataset name</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>hf_dataset <span class="op">=</span> <span class="ss">f'cj-mills/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the zip file that contains the dataset</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>archive_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>archive_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">.zip'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the directory where the dataset will be extracted</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>dataset_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HuggingFace Dataset:"</span>: hf_dataset, </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Path:"</span>: archive_path, </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Path:"</span>: dataset_path</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_fffb6">
<thead>
</thead>
<tbody>
<tr>
<th id="T_fffb6_level0_row0" class="row_heading level0 row0">
HuggingFace Dataset:
</th>
<td id="T_fffb6_row0_col0" class="data row0 col0">
cj-mills/hagrid-sample-30k-384p
</td>
</tr>
<tr>
<th id="T_fffb6_level0_row1" class="row_heading level0 row1">
Archive Path:
</th>
<td id="T_fffb6_row1_col0" class="data row1 col0">
/mnt/980_1TB_2/Datasets/../Archive/hagrid-sample-30k-384p.zip
</td>
</tr>
<tr>
<th id="T_fffb6_level0_row2" class="row_heading level0 row2">
Dataset Path:
</th>
<td id="T_fffb6_row2_col0" class="data row2 col0">
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="downloading-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="downloading-the-dataset">Downloading the Dataset</h3>
<p>We can now download the dataset archive file and extract the dataset using the <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#download_file"><code>download_file</code></a> and <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#file_extract"><code>file_extract</code></a> functions from the <code>cjm_psl_utils</code> package. We can delete the archive afterward to save space.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset URL</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dataset_url <span class="op">=</span> <span class="ss">f"https://huggingface.co/datasets/</span><span class="sc">{</span>hf_dataset<span class="sc">}</span><span class="ss">/resolve/main/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">.zip"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"HuggingFace Dataset URL: </span><span class="sc">{</span>dataset_url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set whether to delete the archive file after extracting the dataset</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>delete_archive <span class="op">=</span> <span class="va">True</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset if not present</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> dataset_path.is_dir():</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Dataset folder already exists"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading dataset..."</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    download_file(dataset_url, archive_dir)    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracting dataset..."</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    file_extract(fname<span class="op">=</span>archive_path, dest<span class="op">=</span>dataset_dir)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Delete the archive if specified</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> delete_archive: archive_path.unlink()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="getting-the-image-and-annotation-folders" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-image-and-annotation-folders">Getting the Image and Annotation Folders</h3>
<p>The dataset has two folders containing the sample images and annotation files. The image folder organizes samples for each gesture class into separate subfolders. Each image subfolder has a corresponding JSON annotation file.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of the items in the 'dataset_path' directory</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>dir_content <span class="op">=</span> [item <span class="cf">for</span> item <span class="kw">in</span> dataset_path.iterdir() <span class="cf">if</span> item.is_dir()]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the path of the 'ann_train_val' directory</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>annotation_dir <span class="op">=</span> dataset_path<span class="op">/</span><span class="st">'ann_train_val'</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the 'ann_train_val' directory from the list of items</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>dir_content.remove(annotation_dir)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the path of the remaining directory, which is assumed to be the image directory</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>img_dir <span class="op">=</span> dir_content[<span class="dv">0</span>]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the paths of the annotation and image directories</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Annotation Directory: </span><span class="sc">{</span>annotation_dir<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image Directory: </span><span class="sc">{</span>img_dir<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of files in the 'annotation_dir' directory</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>annotation_file_paths <span class="op">=</span> <span class="bu">list</span>(annotation_dir.glob(<span class="st">'*.json'</span>))</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of folders in the 'img_dir' directory</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>img_folder_paths <span class="op">=</span> [folder <span class="cf">for</span> folder <span class="kw">in</span> img_dir.iterdir() <span class="cf">if</span> folder.is_dir()]</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the names of the folders using a Pandas DataFrame</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Image Folder"</span>: [folder.name <span class="cf">for</span> folder <span class="kw">in</span> img_folder_paths], </span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>              <span class="st">"Annotation File"</span>:[<span class="bu">file</span>.name <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> annotation_file_paths]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Annotation Directory: /mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/ann_train_val
Image Directory: /mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Image Folder
</th>
<th>
Annotation File
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
train_val_call
</td>
<td>
call.json
</td>
</tr>
<tr>
<th>
1
</th>
<td>
train_val_dislike
</td>
<td>
dislike.json
</td>
</tr>
<tr>
<th>
2
</th>
<td>
train_val_fist
</td>
<td>
fist.json
</td>
</tr>
<tr>
<th>
3
</th>
<td>
train_val_four
</td>
<td>
four.json
</td>
</tr>
<tr>
<th>
4
</th>
<td>
train_val_like
</td>
<td>
like.json
</td>
</tr>
<tr>
<th>
5
</th>
<td>
train_val_mute
</td>
<td>
mute.json
</td>
</tr>
<tr>
<th>
6
</th>
<td>
train_val_ok
</td>
<td>
ok.json
</td>
</tr>
<tr>
<th>
7
</th>
<td>
train_val_one
</td>
<td>
one.json
</td>
</tr>
<tr>
<th>
8
</th>
<td>
train_val_palm
</td>
<td>
palm.json
</td>
</tr>
<tr>
<th>
9
</th>
<td>
train_val_peace
</td>
<td>
peace.json
</td>
</tr>
<tr>
<th>
10
</th>
<td>
train_val_peace_inverted
</td>
<td>
peace_inverted.json
</td>
</tr>
<tr>
<th>
11
</th>
<td>
train_val_rock
</td>
<td>
rock.json
</td>
</tr>
<tr>
<th>
12
</th>
<td>
train_val_stop
</td>
<td>
stop.json
</td>
</tr>
<tr>
<th>
13
</th>
<td>
train_val_stop_inverted
</td>
<td>
stop_inverted.json
</td>
</tr>
<tr>
<th>
14
</th>
<td>
train_val_three
</td>
<td>
three.json
</td>
</tr>
<tr>
<th>
15
</th>
<td>
train_val_three2
</td>
<td>
three2.json
</td>
</tr>
<tr>
<th>
16
</th>
<td>
train_val_two_up
</td>
<td>
two_up.json
</td>
</tr>
<tr>
<th>
17
</th>
<td>
train_val_two_up_inverted
</td>
<td>
two_up_inverted.json
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-image-file-paths" class="level3">
<h3 class="anchored" data-anchor-id="get-image-file-paths">Get Image File Paths</h3>
<p>Each image file has a unique name that we can use to locate the corresponding annotation data. Let’s make a dictionary that maps image names to file paths. The dictionary will allow us to retrieve the file path for a given image more efficiently.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all image files in the 'img_dir' directory</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img_dict <span class="op">=</span> {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span>.stem : <span class="bu">file</span> <span class="co"># Create a dictionary that maps file names to file paths</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> folder <span class="kw">in</span> img_folder_paths <span class="co"># Iterate through each image folder</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> get_img_files(folder) <span class="co"># Get a list of image files in each image folder</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of image files</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of Images: </span><span class="sc">{</span><span class="bu">len</span>(img_dict)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first five entries from the dictionary using a Pandas DataFrame</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>pd.DataFrame.from_dict(img_dict, orient<span class="op">=</span><span class="st">'index'</span>).head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Number of Images: 31833</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
00005c9c-3548-4a8f-9d0b-2dd4aff37fc9
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00005c9c-3548-4a8f-9d0b-2dd4aff37fc9.jpg
</td>
</tr>
<tr>
<th>
0020a3db-82d8-47aa-8642-2715d4744db5
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_call/0020a3db-82d8-47aa-8642-2715d4744db5.jpg
</td>
</tr>
<tr>
<th>
004ac93f-0f7c-49a4-aadc-737e0ad4273c
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_call/004ac93f-0f7c-49a4-aadc-737e0ad4273c.jpg
</td>
</tr>
<tr>
<th>
006cac69-d3f0-47f9-aac9-38702d038ef1
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_call/006cac69-d3f0-47f9-aac9-38702d038ef1.jpg
</td>
</tr>
<tr>
<th>
00973fac-440e-4a56-b60c-2a06d5fb155d
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-sample-30k-384p/hagrid_30k/train_val_call/00973fac-440e-4a56-b60c-2a06d5fb155d.jpg
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-bounding-box-annotations" class="level3">
<h3 class="anchored" data-anchor-id="get-bounding-box-annotations">Get Bounding Box Annotations</h3>
<p>Next, we read the contents of the JSON annotation files into a Pandas DataFrame so we can easily query the annotations. Reading each JSON file can be slow, so I added a <a href="https://parquet.apache.org/">parquet</a> file with a premade annotation DataFrame.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the file path for the annotations DataFrame</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>annotation_df_pq_path <span class="op">=</span> dataset_path<span class="op">/</span><span class="st">'annotations_df.parquet'</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> annotation_df_pq_path.is_file():</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the annotations DataFrame if present</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    annotation_df <span class="op">=</span> pd.read_parquet(annotation_df_pq_path)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a generator that yields Pandas DataFrames containing the data from each JSON file</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    cls_dataframes <span class="op">=</span> (pd.read_json(f).transpose() <span class="cf">for</span> f <span class="kw">in</span> tqdm(annotation_file_paths))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate the DataFrames into a single DataFrame</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    annotation_df <span class="op">=</span> pd.concat(cls_dataframes, ignore_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep only the rows that correspond to the filenames in the 'img_dict' dictionary</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    annotation_df <span class="op">=</span> annotation_df.loc[<span class="bu">list</span>(img_dict.keys())]</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the annotations DataFrame to disk</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    annotation_df.to_parquet(annotation_df_pq_path)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first 5 rows of the DataFrame</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>annotation_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
bboxes
</th>
<th>
labels
</th>
<th>
leading_hand
</th>
<th>
leading_conf
</th>
<th>
user_id
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
00005c9c-3548-4a8f-9d0b-2dd4aff37fc9
</th>
<td>
[[0.23925175, 0.28595301, 0.25055143, 0.20777627]]
</td>
<td>
<a href="#call">call</a>
</td>
<td>
right
</td>
<td>
1.0
</td>
<td>
5a389ffe1bed6660a59f4586c7d8fe2770785e5bf79b09334aa951f6f119c024
</td>
</tr>
<tr>
<th>
0020a3db-82d8-47aa-8642-2715d4744db5
</th>
<td>
[[0.5801012999999999, 0.53265105, 0.14562138, 0.12286348]]
</td>
<td>
<a href="#call">call</a>
</td>
<td>
left
</td>
<td>
1.0
</td>
<td>
0d6da2c87ef8eabeda2dcfee2dc5b5035e878137a91b149c754a59804f3dce32
</td>
</tr>
<tr>
<th>
004ac93f-0f7c-49a4-aadc-737e0ad4273c
</th>
<td>
[[0.46294793, 0.26419774, 0.13834939000000002, 0.10784189]]
</td>
<td>
<a href="#call">call</a>
</td>
<td>
right
</td>
<td>
1.0
</td>
<td>
d50f05d9d6ca9771938cec766c3d621ff863612f9665b0e4d991c086ec04acc9
</td>
</tr>
<tr>
<th>
006cac69-d3f0-47f9-aac9-38702d038ef1
</th>
<td>
[[0.38799208, 0.44643898, 0.27068787, 0.18277858]]
</td>
<td>
<a href="#call">call</a>
</td>
<td>
right
</td>
<td>
1.0
</td>
<td>
998f6ad69140b3a59cb9823ba680cce62bf2ba678058c2fc497dbbb8b22b29fe
</td>
</tr>
<tr>
<th>
00973fac-440e-4a56-b60c-2a06d5fb155d
</th>
<td>
[[0.40980118, 0.38144198, 0.08338464, 0.06229785], [0.6122035100000001, 0.6780825500000001, 0.04700606, 0.07640522]]
</td>
<td>
[call, no_gesture]
</td>
<td>
right
</td>
<td>
1.0
</td>
<td>
4bb3ee1748be58e05bd1193939735e57bb3c0ca59a7ee38901744d6b9e94632e
</td>
</tr>
</tbody>
</table>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that one of the samples contains a <code>no_gesture</code> label to identify an idle hand in the image.</p>
</div>
</div>
</section>
<section id="inspecting-the-class-distribution" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-class-distribution">Inspecting the Class Distribution</h3>
<p>Now that we have the annotation data, we can get the unique class names and inspect the distribution of samples among the gesture classes.</p>
<section id="get-image-classes" class="level4">
<h4 class="anchored" data-anchor-id="get-image-classes">Get image classes</h4>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the number of samples for each object class</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> annotation_df[<span class="st">'labels'</span>].explode().value_counts()</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of unique labels in the 'annotation_df' DataFrame</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> class_counts.index.tolist()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the labels and the corresponding number of samples using a Pandas DataFrame</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(class_counts)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
count
</th>
</tr>
<tr>
<th>
labels
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
no_gesture
</th>
<td>
7052
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
1855
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
1811
</td>
</tr>
<tr>
<th>
four
</th>
<td>
1805
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
1803
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
1783
</td>
</tr>
<tr>
<th>
one
</th>
<td>
1778
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
1770
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
1769
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
1765
</td>
</tr>
<tr>
<th>
call
</th>
<td>
1763
</td>
</tr>
<tr>
<th>
three
</th>
<td>
1751
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
1750
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
1748
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
1742
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
1737
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
1736
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
1734
</td>
</tr>
<tr>
<th>
like
</th>
<td>
1732
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="visualize-the-class-distribution" class="level4">
<h4 class="anchored" data-anchor-id="visualize-the-class-distribution">Visualize the class distribution</h4>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>class_counts.plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Class distribution'</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Classes'</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(class_counts.index)), class_counts.index, rotation<span class="op">=</span><span class="dv">75</span>)  <span class="co"># Set the x-axis tick labels</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_27_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Each class, excluding <code>no_gesture</code>, has roughly the same number of samples. There are approximately four times as many <code>no_gesture</code> samples because of the immense variety of non-matching hand positions.</p>
</section>
</section>
<section id="visualizing-bounding-box-annotations" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-bounding-box-annotations">Visualizing Bounding Box Annotations</h3>
<p>Lastly, we will visualize the bounding box for one of the sample images to demonstrate how to interpret the annotations.</p>
<section id="generate-a-color-map" class="level4">
<h4 class="anchored" data-anchor-id="generate-a-color-map">Generate a color map</h4>
<p>While not required, assigning a unique color to bounding boxes for each object class enhances visual distinction, allowing for easier identification of different objects in the scene. We can use the <a href="https://distinctipy.readthedocs.io/en/latest/"><code>distinctipy</code></a> package to generate a visually distinct colormap.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a list of colors with a length equal to the number of labels</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> distinctipy.get_colors(<span class="bu">len</span>(class_names))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a copy of the color map in integer format</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>int_colors <span class="op">=</span> [<span class="bu">tuple</span>(<span class="bu">int</span>(c<span class="op">*</span><span class="dv">255</span>) <span class="cf">for</span> c <span class="kw">in</span> color) <span class="cf">for</span> color <span class="kw">in</span> colors]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a color swatch to visualize the color map</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>distinctipy.color_swatch(colors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_30_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="download-a-font-file" class="level4">
<h4 class="anchored" data-anchor-id="download-a-font-file">Download a font file</h4>
<p>The <a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html"><code>draw_bounding_boxes</code></a> function included with torchvision uses a pretty small font size. We can increase the font size if we use a custom font. Font files are available on sites like <a href="https://fonts.google.com/">Google Fonts</a>, or we can use one included with the operating system.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the name of the font file</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>font_file <span class="op">=</span> <span class="st">'KFOlCnqEu92Fr1MmEU9vAw.ttf'</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the font file</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>download_file(<span class="ss">f"https://fonts.gstatic.com/s/roboto/v30/</span><span class="sc">{</span>font_file<span class="sc">}</span><span class="ss">"</span>, <span class="st">"./"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="define-the-bounding-box-annotation-function" class="level4">
<h4 class="anchored" data-anchor-id="define-the-bounding-box-annotation-function">Define the bounding box annotation function</h4>
<p>Let’s make a partial function using <code>draw_bounding_boxes</code> since we’ll use the same box thickness and font each time we visualize bounding boxes.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>draw_bboxes <span class="op">=</span> partial(draw_bounding_boxes, fill<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">2</span>, font<span class="op">=</span>font_file, font_size<span class="op">=</span><span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="selecting-a-sample-image" class="level3">
<h3 class="anchored" data-anchor-id="selecting-a-sample-image">Selecting a Sample Image</h3>
<p>We can use the unique ID for an image in the image dictionary to get the image’s file path and the associated annotations from the annotation DataFrame.</p>
<section id="load-the-sample-image" class="level4">
<h4 class="anchored" data-anchor-id="load-the-sample-image">Load the sample image</h4>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the file ID of the first image file</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>file_id <span class="op">=</span> <span class="bu">list</span>(img_dict.keys())[<span class="dv">0</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the associated image file as a RGB image</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> Image.<span class="bu">open</span>(img_dict[file_id]).convert(<span class="st">'RGB'</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the dimensions of the image</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image Dims: </span><span class="sc">{</span>sample_img<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the image</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>sample_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Image Dims: (384, 512)</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_37_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="inspect-the-corresponding-annotation-data" class="level4">
<h4 class="anchored" data-anchor-id="inspect-the-corresponding-annotation-data">Inspect the corresponding annotation data</h4>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the row from the 'annotation_df' DataFrame corresponding to the 'file_id'</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>annotation_df.loc[file_id].to_frame()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
00005c9c-3548-4a8f-9d0b-2dd4aff37fc9
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
bboxes
</th>
<td>
[[0.23925175, 0.28595301, 0.25055143, 0.20777627]]
</td>
</tr>
<tr>
<th>
labels
</th>
<td>
<a href="#call">call</a>
</td>
</tr>
<tr>
<th>
leading_hand
</th>
<td>
right
</td>
</tr>
<tr>
<th>
leading_conf
</th>
<td>
1.0
</td>
</tr>
<tr>
<th>
user_id
</th>
<td>
5a389ffe1bed6660a59f4586c7d8fe2770785e5bf79b09334aa951f6f119c024
</td>
</tr>
</tbody>
</table>
</div>
<p>The bounding box annotations are in the format <code>[top-left X, top-left Y, width, height]</code>. The HaGRID dataset also normalizes bounding box annotations for this dataset to the range <code>[0,1]</code> based on the image dimensions. Therefore, we need to scale the <code>top-left X</code> and <code>width</code> values by the image width and the <code>top-left Y</code> and <code>height</code> values by the image height.</p>
</section>
<section id="annotate-sample-image" class="level4">
<h4 class="anchored" data-anchor-id="annotate-sample-image">Annotate sample image</h4>
<p>The <code>draw_bounding_boxes</code> function expects bounding box annotations in <code>[top-left X, top-left Y, bottom-right X, bottom-right Y]</code> format, so we’ll use the <a href="https://pytorch.org/vision/stable/generated/torchvision.ops.box_convert.html#torchvision.ops.box_convert"><code>box_convert</code></a> function included with torchvision to convert the bounding box annotations from <code>[x,y,w,h]</code> to <code>[x,y,x,y]</code> format.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the labels and bounding box annotations for the sample image</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> annotation_df.loc[file_id][<span class="st">'labels'</span>]</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>bboxes <span class="op">=</span> annotation_df.loc[file_id][<span class="st">'bboxes'</span>]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the bounding boxes in the image size scale</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>width, height <span class="op">=</span> sample_img.size</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>bboxes <span class="op">=</span> [[x<span class="op">*</span>width, y<span class="op">*</span>height, w<span class="op">*</span>width, h<span class="op">*</span>height] <span class="cf">for</span> x, y, w, h <span class="kw">in</span> bboxes]</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the sample image with labels and bounding boxes</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(sample_img), </span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(bboxes), <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>labels, </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels]]</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_41_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>We have loaded the dataset, inspected its class distribution, and visualized the bounding box annotations for a sample image. In the next section, we will select and load our model.</p>
</section>
</section>
</section>
<section id="selecting-a-model" class="level2">
<h2 class="anchored" data-anchor-id="selecting-a-model">Selecting a Model</h2>
<p>I provide five predefined model configurations of different sizes in the <a href="https://cj-mills.github.io/cjm-yolox-pytorch/"><code>cjm_yolox_pytorch</code></a> package. Each predefined config comes with a model checkpoint trained on the <a href="https://cocodataset.org/">COCO</a> (Common Objects in Context) dataset.</p>
<section id="exploring-available-models" class="level3">
<h3 class="anchored" data-anchor-id="exploring-available-models">Exploring Available Models</h3>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(MODEL_CFGS).transpose()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
deepen_factor
</th>
<th>
widen_factor
</th>
<th>
neck_in_channels
</th>
<th>
neck_out_channels
</th>
<th>
neck_num_csp_blocks
</th>
<th>
head_in_channels
</th>
<th>
head_feat_channels
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
yolox_tiny
</th>
<td>
0.33
</td>
<td>
0.375
</td>
<td>
[96, 192, 384]
</td>
<td>
96
</td>
<td>
1
</td>
<td>
96
</td>
<td>
96
</td>
</tr>
<tr>
<th>
yolox_s
</th>
<td>
0.33
</td>
<td>
0.5
</td>
<td>
[128, 256, 512]
</td>
<td>
128
</td>
<td>
1
</td>
<td>
128
</td>
<td>
128
</td>
</tr>
<tr>
<th>
yolox_m
</th>
<td>
0.67
</td>
<td>
0.75
</td>
<td>
[192, 384, 768]
</td>
<td>
192
</td>
<td>
2
</td>
<td>
192
</td>
<td>
192
</td>
</tr>
<tr>
<th>
yolox_l
</th>
<td>
1.0
</td>
<td>
1.0
</td>
<td>
[256, 512, 1024]
</td>
<td>
256
</td>
<td>
3
</td>
<td>
256
</td>
<td>
256
</td>
</tr>
<tr>
<th>
yolox_x
</th>
<td>
1.33
</td>
<td>
1.25
</td>
<td>
[320, 640, 1280]
</td>
<td>
320
</td>
<td>
4
</td>
<td>
320
</td>
<td>
320
</td>
</tr>
</tbody>
</table>
</div>
<p>We’ll go with the <code>yolox_tiny</code> configuration as it is the most efficient and sufficiently accurate on this dataset.</p>
</section>
<section id="loading-the-yolox-tiny-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-the-yolox-tiny-model">Loading the YOLOX-Tiny Model</h3>
<p>We can initialize a <code>yolox_tiny</code> model with the pretrained weights and the appropriate number of output classes using the <a href="https://cj-mills.github.io/cjm-yolox-pytorch/model.html#build_model"><code>build_model</code></a> function included with the <code>cjm_yolox_pytorch</code> package.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the YOLOX model configuration</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model_type <span class="op">=</span> <span class="st">'yolox_tiny'</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model_type = 'yolox_s'</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model_type = 'yolox_m'</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model_type = 'yolox_l'</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model_type = 'yolox_x'</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set whether to initialize the model with pretrained weights</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>pretrained <span class="op">=</span> <span class="va">True</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a YOLOX model with the number of output classes equal to the number of class names</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_model(model_type, <span class="bu">len</span>(class_names), pretrained<span class="op">=</span>pretrained).to(device<span class="op">=</span>device, dtype<span class="op">=</span>dtype)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add attributes to store the device and model name for later reference</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>model.device <span class="op">=</span> device</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>model.name <span class="op">=</span> model_type</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Get stride values for processing output</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>strides <span class="op">=</span> model.bbox_head.strides</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="get-normalization-statistics" class="level3">
<h3 class="anchored" data-anchor-id="get-normalization-statistics">Get Normalization Statistics</h3>
<p>Next, we need the normalization stats used during the pretraining process.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve normalization statistics (mean and std) specific to the pretrained checkpoints</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>norm_stats <span class="op">=</span> [<span class="op">*</span>NORM_STATS[model_type].values()] <span class="cf">if</span> pretrained <span class="cf">else</span> ([<span class="fl">0.5</span>]<span class="op">*</span><span class="dv">3</span>, [<span class="fl">1.0</span>]<span class="op">*</span><span class="dv">3</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the mean and standard deviation</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(norm_stats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.5
</td>
<td>
0.5
</td>
<td>
0.5
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1.0
</td>
<td>
1.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="summarizing-the-model" class="level3">
<h3 class="anchored" data-anchor-id="summarizing-the-model">Summarizing the Model</h3>
<p>Before moving on, let’s generate a summary of our model to get an overview of its performance characteristics. We can use this to gauge the difference in computational requirements between the model configs.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input to the model</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>backbone_inp <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">256</span>, <span class="dv">256</span>).to(device)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(): neck_inp <span class="op">=</span> model.backbone(backbone_inp)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(): head_inp <span class="op">=</span> model.neck(neck_inp)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a summary of the model as a Pandas DataFrame</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>backbone_summary <span class="op">=</span> markdown_to_pandas(<span class="ss">f"</span><span class="sc">{</span>get_module_summary(model.backbone, [backbone_inp])<span class="sc">}</span><span class="ss">"</span>).iloc[<span class="dv">0</span>]</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>neck_summary <span class="op">=</span> markdown_to_pandas(<span class="ss">f"</span><span class="sc">{</span>get_module_summary(model.neck, [neck_inp])<span class="sc">}</span><span class="ss">"</span>).iloc[<span class="dv">0</span>]</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>head_summary <span class="op">=</span> markdown_to_pandas(<span class="ss">f"</span><span class="sc">{</span>get_module_summary(model.bbox_head, [head_inp])<span class="sc">}</span><span class="ss">"</span>).iloc[<span class="dv">0</span>]</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> pd.concat([backbone_summary, neck_summary, head_summary], axis<span class="op">=</span><span class="dv">1</span>).transpose()</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>parameters_df <span class="op">=</span> summary_df[<span class="st">'# Parameters'</span>].<span class="bu">apply</span>(convert_to_numeric)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>trainable_parameters <span class="op">=</span> summary_df[<span class="st">'# Trainable Parameters'</span>].<span class="bu">apply</span>(convert_to_numeric)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>size_df <span class="op">=</span> summary_df[<span class="st">'Size (bytes)'</span>].<span class="bu">apply</span>(convert_to_numeric)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>forward_flops_df <span class="op">=</span> summary_df[<span class="st">'Forward FLOPs'</span>].<span class="bu">apply</span>(convert_to_numeric)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co"># compute sums and add a new row</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>total_row <span class="op">=</span> {<span class="st">'Type'</span>: <span class="ss">f'</span><span class="sc">{</span>model_type<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>             <span class="st">'# Parameters'</span>: convert_to_string(parameters_df.<span class="bu">sum</span>()),</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>             <span class="st">'# Trainable Parameters'</span>: convert_to_string(trainable_parameters.<span class="bu">sum</span>()),</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>             <span class="st">'Size (bytes)'</span>: convert_to_string(size_df.<span class="bu">sum</span>()),</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>             <span class="st">'Forward FLOPs'</span>: convert_to_string(forward_flops_df.<span class="bu">sum</span>()), </span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>             <span class="st">'In size'</span>: backbone_summary[<span class="st">'In size'</span>], </span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>             <span class="st">'Out size'</span>: head_summary[<span class="st">'Out size'</span>]}</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> pd.concat([pd.DataFrame([total_row]), summary_df], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove extra columns</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>summary_df.drop([<span class="st">'In size'</span>, <span class="st">'Out size'</span>, <span class="st">'Contains Uninitialized Parameters?'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Type
</th>
<th>
# Parameters
</th>
<th>
# Trainable Parameters
</th>
<th>
Size (bytes)
</th>
<th>
Forward FLOPs
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
yolox_tiny
</td>
<td>
5.1 M
</td>
<td>
5.1 M
</td>
<td>
20.2 M
</td>
<td>
1.206 G
</td>
</tr>
<tr>
<th>
1
</th>
<td>
CSPDarknet
</td>
<td>
2.4 M
</td>
<td>
2.4 M
</td>
<td>
9.5 M
</td>
<td>
500 M
</td>
</tr>
<tr>
<th>
2
</th>
<td>
YOLOXPAFPN
</td>
<td>
1.7 M
</td>
<td>
1.7 M
</td>
<td>
6.7 M
</td>
<td>
257 M
</td>
</tr>
<tr>
<th>
3
</th>
<td>
YOLOXHead
</td>
<td>
1.0 M
</td>
<td>
1.0 M
</td>
<td>
4.0 M
</td>
<td>
449 M
</td>
</tr>
</tbody>
</table>
</div>
<p>The above table shows the summary for the entire <code>yolox_tiny</code> model and the model’s backbone, neck, and head individually. The model has approximately <code>5.1</code> million trainable parameters. It takes up <code>20.2</code> Megabytes of space and performs around <code>1.2</code> billion floating point operations for a single <code>256x256</code> RGB image.</p>
<p>For reference, the <code>yolox_x</code> model has about <code>99</code> million trainable parameters, takes up over <code>395</code> MB, and performs around <code>22.5</code> billion floating point operations for the same input.</p>
<p>That completes the model selection and setup. In the next section, we will prepare our dataset for training.</p>
</section>
</section>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">Preparing the Data</h2>
<p>The data preparation involves several steps, such as applying data augmentation techniques, setting up the train-validation split for the dataset, resizing and padding the images, defining the training dataset class, and initializing DataLoaders to feed data to the model.</p>
<section id="training-validation-split" class="level3">
<h3 class="anchored" data-anchor-id="training-validation-split">Training-Validation Split</h3>
<p>Let’s begin by defining the training-validation split. We’ll randomly select 90% of the available samples for the training set and use the remaining 10% for the validation set.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the list of image IDs</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>img_keys <span class="op">=</span> <span class="bu">list</span>(img_dict.keys())</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the image IDs</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>random.shuffle(img_keys)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the percentage of the images that should be used for training</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>train_pct <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>val_pct <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the index at which to split the subset of image paths into training and validation sets</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>train_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_keys)<span class="op">*</span>train_pct)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>val_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_keys)<span class="op">*</span>(train_pct<span class="op">+</span>val_pct))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the subset of image paths into training and validation sets</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>train_keys <span class="op">=</span> img_keys[:train_split]</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>val_keys <span class="op">=</span> img_keys[train_split:]</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of images in the training and validation sets</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Samples:"</span>: <span class="bu">len</span>(train_keys),</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Validation Samples:"</span>: <span class="bu">len</span>(val_keys)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_5ef14">
<thead>
</thead>
<tbody>
<tr>
<th id="T_5ef14_level0_row0" class="row_heading level0 row0">
Training Samples:
</th>
<td id="T_5ef14_row0_col0" class="data row0 col0">
28649
</td>
</tr>
<tr>
<th id="T_5ef14_level0_row1" class="row_heading level0 row1">
Validation Samples:
</th>
<td id="T_5ef14_row1_col0" class="data row1 col0">
3184
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>Next, we can define what data augmentations to apply to images during training. I created a few custom image transforms to help streamline the code.</p>
<p>The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#customrandomaugment">first</a> mimics the <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.TrivialAugmentWide.html#torchvision.transforms.v2.TrivialAugmentWide">TrivialAugmentWide</a> transform available through torchvision, but supports bounding box annotations. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#customrandomioucrop">second</a> extends torchvision’s <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomIoUCrop.html#torchvision.transforms.v2.RandomIoUCrop"><code>RandomIoUCrop</code></a> transform to give the user more control over how much it crops into bounding box areas. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax">third</a> resizes images based on their largest dimension rather than their smallest. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare">fourth</a> applies square padding and allows the padding to be applied equally on both sides or randomly split between the two sides.</p>
<p>All four are available through the <a href="https://cj-mills.github.io/cjm-torchvision-tfms/"><code>cjm-torchvision-tfms</code></a> package.</p>
<section id="set-training-image-size" class="level4">
<h4 class="anchored" data-anchor-id="set-training-image-size">Set training image size</h4>
<p>First, we’ll set the size to use for training. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax"><code>ResizeMax</code></a> transform will resize images so that the longest dimension equals this value while preserving the aspect ratio. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare"><code>PadSquare</code></a> transform will then pad the other side to make all the input squares. The input width and height must be multiples of the max <a href="https://christianjmills.com/posts/pytorch-train-object-detector-yolox-tutorial/#loading-the-yolox-tiny-model">stride value</a>.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Show a list of potential input resolutions</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([<span class="bu">max</span>(strides)<span class="op">*</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>,<span class="dv">21</span>)])</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set training image size to a multiple of the max stride value</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>train_sz <span class="op">=</span> <span class="dv">384</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>[224, 256, 288, 320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640]</code></pre>
</section>
<section id="initialize-the-transforms" class="level4">
<h4 class="anchored" data-anchor-id="initialize-the-transforms">Initialize the transforms</h4>
<p>Now we can initialize the transform objects. We will set the fill color for the transforms to black.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the fill color as black</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>fill <span class="op">=</span> (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a CustomRandomAugment object</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>random_aug_tfm <span class="op">=</span> CustomRandomAugment()</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RandomIoUCrop object</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>iou_crop <span class="op">=</span> CustomRandomIoUCrop(min_scale<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>                               max_scale<span class="op">=</span><span class="fl">1.0</span>, </span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>                               min_aspect_ratio<span class="op">=</span><span class="fl">0.5</span>, </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>                               max_aspect_ratio<span class="op">=</span><span class="fl">2.0</span>, </span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>                               sampler_options<span class="op">=</span>[<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>, <span class="fl">1.0</span>],</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>                               trials<span class="op">=</span><span class="dv">400</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>                              )</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>iou_crop.fill <span class="op">=</span> fill</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `ResizeMax` object</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>resize_max <span class="op">=</span> ResizeMax(max_sz<span class="op">=</span>train_sz)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `PadSquare` object</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>pad_square <span class="op">=</span> PadSquare(shift<span class="op">=</span><span class="va">True</span>, fill<span class="op">=</span>fill)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="test-the-transforms" class="level4">
<h4 class="anchored" data-anchor-id="test-the-transforms">Test the transforms</h4>
<p>We’ll pass input through the <code>CustomRandomIoUCrop</code> transform first and then through <code>CustomRandomAugment</code>, <code>ResizeMax</code>, and <code>PadSquare</code>. We can pass the result through a final resize operation to ensure both sides match the <code>train_sz</code> value.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare bounding box targets</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> {<span class="st">'boxes'</span>: BoundingBoxes(torchvision.ops.box_convert(torch.Tensor(bboxes), <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                                <span class="bu">format</span><span class="op">=</span><span class="st">'xyxy'</span>, </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                                canvas_size<span class="op">=</span>sample_img.size[::<span class="op">-</span><span class="dv">1</span>]), </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>           <span class="st">'labels'</span>: labels}</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop the image</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>cropped_img, targets <span class="op">=</span> iou_crop(sample_img, targets)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Augment the image</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>augmented_img, targets <span class="op">=</span> random_aug_tfm(cropped_img, targets)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the image</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>resized_img, targets <span class="op">=</span> resize_max(augmented_img, targets)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad the image</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>padded_img, targets <span class="op">=</span> pad_square(resized_img, targets)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the padded image is the target size</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>resize <span class="op">=</span> transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>resized_padded_img, targets <span class="op">=</span> resize(padded_img, targets)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the augmented image with updated labels and bounding boxes</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(resized_padded_img), </span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>targets[<span class="st">'boxes'</span>], </span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>targets[<span class="st">'labels'</span>], </span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels]]</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the annotated image</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>display(tensor_to_pil(annotated_tensor))</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Source Image:"</span>: sample_img.size,</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cropped Image:"</span>: cropped_img.size,</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Image:"</span>: resized_img.size,</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Padded Image:"</span>: padded_img.size,</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Padded Image:"</span>: resized_padded_img.size,</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_60_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_e96df">
<thead>
</thead>
<tbody>
<tr>
<th id="T_e96df_level0_row0" class="row_heading level0 row0">
Source Image:
</th>
<td id="T_e96df_row0_col0" class="data row0 col0">
(384, 512)
</td>
</tr>
<tr>
<th id="T_e96df_level0_row1" class="row_heading level0 row1">
Cropped Image:
</th>
<td id="T_e96df_row1_col0" class="data row1 col0">
(294, 228)
</td>
</tr>
<tr>
<th id="T_e96df_level0_row2" class="row_heading level0 row2">
Resized Image:
</th>
<td id="T_e96df_row2_col0" class="data row2 col0">
(382, 297)
</td>
</tr>
<tr>
<th id="T_e96df_level0_row3" class="row_heading level0 row3">
Padded Image:
</th>
<td id="T_e96df_row3_col0" class="data row3 col0">
(382, 382)
</td>
</tr>
<tr>
<th id="T_e96df_level0_row4" class="row_heading level0 row4">
Resized Padded Image:
</th>
<td id="T_e96df_row4_col0" class="data row4 col0">
(384, 384)
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="training-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="training-dataset-class">Training Dataset Class</h3>
<p>Now we can define a custom dataset class to load images, extract the bounding box annotation, and apply the image transforms during training.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HagridDataset(Dataset):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class represents a PyTorch Dataset for a collection of images and their annotations.</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co">    The class is designed to load images along with their corresponding bounding box annotations and labels.</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_keys, annotation_df, img_dict, class_to_idx, transforms<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Constructor for the HagridDataset class.</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">        img_keys (list): List of unique identifiers for images.</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="co">        annotation_df (DataFrame): DataFrame containing the image annotations.</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co">        img_dict (dict): Dictionary mapping image identifiers to image file paths.</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co">        class_to_idx (dict): Dictionary mapping class labels to indices.</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co">        transforms (callable, optional): Optional transform to be applied on a sample.</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Dataset, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_keys <span class="op">=</span> img_keys  <span class="co"># List of image keys</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._annotation_df <span class="op">=</span> annotation_df  <span class="co"># DataFrame containing annotations</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_dict <span class="op">=</span> img_dict  <span class="co"># Dictionary mapping image keys to image paths</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_to_idx <span class="op">=</span> class_to_idx  <span class="co"># Dictionary mapping class names to class indices</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._transforms <span class="op">=</span> transforms  <span class="co"># Image transforms to be applied</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the length of the dataset.</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a><span class="co">        int: The number of items in the dataset.</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._img_keys)</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a><span class="co">        Fetch an item from the dataset at the specified index.</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a><span class="co">        index (int): Index of the item to fetch from the dataset.</span></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: A tuple containing the image and its associated target (annotations).</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Retrieve the key for the image at the specified index</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>        img_key <span class="op">=</span> <span class="va">self</span>._img_keys[index]</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the annotations for this image</span></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>        annotation <span class="op">=</span> <span class="va">self</span>._annotation_df.loc[img_key]</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the image and its target (bounding boxes and labels)</span></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>        image, target <span class="op">=</span> <span class="va">self</span>._load_image_and_target(annotation)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply the transformations, if any</span></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._transforms:</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>            image, target <span class="op">=</span> <span class="va">self</span>._transforms(image, target)</span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, target</span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _load_image_and_target(<span class="va">self</span>, annotation):</span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Load an image and its target (bounding boxes and labels).</span></span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a><span class="co">        annotation (pandas.Series): The annotations for an image.</span></span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: A tuple containing the image and a dictionary with 'boxes' and 'labels' keys.</span></span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Retrieve the file path of the image</span></span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>        filepath <span class="op">=</span> <span class="va">self</span>._img_dict[annotation.name]</span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read the image file</span></span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(filepath).convert(<span class="st">'RGB'</span>)</span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the bounding boxes in the image size scale</span></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a>        bbox_list <span class="op">=</span> np.array([bbox<span class="op">*</span>(image.size<span class="op">*</span><span class="dv">2</span>) <span class="cf">for</span> bbox <span class="kw">in</span> annotation.bboxes])</span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert bounding box coordinates from [xmin, ymin, width, height] to [xmin, ymin, xmax, ymax]</span></span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>        bbox_tensor <span class="op">=</span> torchvision.ops.box_convert(torch.Tensor(bbox_list), <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>)</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a BoundingBoxes object with the bounding boxes</span></span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> BoundingBoxes(bbox_tensor, <span class="bu">format</span><span class="op">=</span><span class="st">'xyxy'</span>, canvas_size<span class="op">=</span>image.size[::<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert the class labels to indices</span></span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.Tensor([<span class="va">self</span>._class_to_idx[label] <span class="cf">for</span> label <span class="kw">in</span> annotation.labels])</span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, {<span class="st">'boxes'</span>: boxes, <span class="st">'labels'</span>: labels}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-transforms" class="level3">
<h3 class="anchored" data-anchor-id="image-transforms">Image Transforms</h3>
<p>We’ll add additional data augmentations with the IoU crop and random augment transforms to help the model generalize.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Transform</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>RandomZoomOut</code></td>
<td>Randomly pad images, videos, bounding boxes and masks creating a zoom out effect. (<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomZoomOut.html#torchvision.transforms.v2.RandomZoomOut">link</a>)</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms for data augmentation</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>data_aug_tfms <span class="op">=</span> transforms.Compose(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    transforms<span class="op">=</span>[</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        transforms.RandomChoice([</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>            transforms.RandomZoomOut(fill<span class="op">=</span>fill, side_range<span class="op">=</span>(<span class="fl">1.125</span>, <span class="fl">1.5</span>)), </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>            iou_crop</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        ], p<span class="op">=</span>[<span class="fl">0.35</span>, <span class="fl">0.65</span>]),</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        random_aug_tfm,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to resize and pad input images</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>resize_pad_tfm <span class="op">=</span> transforms.Compose([</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    resize_max, </span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    pad_square,</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to sanitize bounding boxes and normalize input data</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>final_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    transforms.ToImage(),</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    transforms.SanitizeBoundingBoxes(),</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(<span class="op">*</span>norm_stats),</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the transformations for training and validation datasets</span></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>train_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    data_aug_tfms,</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    resize_pad_tfm, </span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    final_tfms,</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>valid_tfms <span class="op">=</span> transforms.Compose([resize_pad_tfm, final_tfms])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Always use the <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.SanitizeBoundingBoxes.html#torchvision.transforms.v2.SanitizeBoundingBoxes"><code>SanitizeBoundingBoxes</code></a> transform to clean up annotations after using data augmentations that alter bounding boxes (e.g., cropping, warping, etc.).</p>
</div>
</div>
</section>
<section id="initialize-datasets" class="level3">
<h3 class="anchored" data-anchor-id="initialize-datasets">Initialize Datasets</h3>
<p>Now we can create our training and validation dataset objects using the dataset splits and transforms.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mapping from class names to class indices</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>class_to_idx <span class="op">=</span> {c: i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(class_names)}</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the datasets using the defined transformations</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> HagridDataset(train_keys, annotation_df, img_dict, class_to_idx, train_tfms)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> HagridDataset(val_keys, annotation_df, img_dict, class_to_idx, valid_tfms)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of samples in the training and validation datasets</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Training dataset size:'</span>: <span class="bu">len</span>(train_dataset),</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Validation dataset size:'</span>: <span class="bu">len</span>(valid_dataset)}</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_3e0d6">
<thead>
</thead>
<tbody>
<tr>
<th id="T_3e0d6_level0_row0" class="row_heading level0 row0">
Training dataset size:
</th>
<td id="T_3e0d6_row0_col0" class="data row0 col0">
28649
</td>
</tr>
<tr>
<th id="T_3e0d6_level0_row1" class="row_heading level0 row1">
Validation dataset size:
</th>
<td id="T_3e0d6_row1_col0" class="data row1 col0">
3184
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inspect-samples" class="level3">
<h3 class="anchored" data-anchor-id="inspect-samples">Inspect Samples</h3>
<p>Let’s verify the dataset objects work properly by inspecting the first samples from the training and validation sets.</p>
<section id="inspect-training-set-sample" class="level4">
<h4 class="anchored" data-anchor-id="inspect-training-set-sample">Inspect training set sample</h4>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>dataset_sample <span class="op">=</span> train_dataset[<span class="dv">0</span>]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>(denorm_img_tensor(dataset_sample[<span class="dv">0</span>].cpu(), <span class="op">*</span>norm_stats)<span class="op">*</span><span class="dv">255</span>).to(dtype<span class="op">=</span>torch.uint8), </span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>dataset_sample[<span class="dv">1</span>][<span class="st">'boxes'</span>], </span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[class_names[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]], </span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]]</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_69_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="inspect-validation-set-sample" class="level4">
<h4 class="anchored" data-anchor-id="inspect-validation-set-sample">Inspect validation set sample</h4>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>dataset_sample <span class="op">=</span> valid_dataset[<span class="dv">0</span>]</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>(denorm_img_tensor(dataset_sample[<span class="dv">0</span>].cpu(), <span class="op">*</span>norm_stats)<span class="op">*</span><span class="dv">255</span>).to(dtype<span class="op">=</span>torch.uint8), </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>dataset_sample[<span class="dv">1</span>][<span class="st">'boxes'</span>], </span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[class_names[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]], </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]]</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_71_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="initialize-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="initialize-dataloaders">Initialize DataLoaders</h3>
<p>The last step before training is to instantiate the DataLoaders for the training and validation sets. Try decreasing the <code>bs</code> and <code>prefetch_factor</code> values if you encounter memory limitations.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the training batch size</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of worker processes for loading data.</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>num_workers <span class="op">=</span> multiprocessing.cpu_count()</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters for DataLoader</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>data_loader_params <span class="op">=</span> {</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>: bs,  <span class="co"># Batch size for data loading</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num_workers'</span>: num_workers,  <span class="co"># Number of subprocesses to use for data loading</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'collate_fn'</span>: <span class="kw">lambda</span> batch: <span class="bu">tuple</span>(<span class="bu">zip</span>(<span class="op">*</span>batch)),</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'drop_last'</span>: <span class="va">True</span>,</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory'</span>: <span class="st">'cuda'</span> <span class="kw">in</span> device,</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory_device'</span>: device <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">''</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for training data. Data is shuffled for every epoch.</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, <span class="op">**</span>data_loader_params, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for validation data. Shuffling is not necessary for validation data.</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>valid_dataloader <span class="op">=</span> DataLoader(valid_dataset, <span class="op">**</span>data_loader_params)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of batches in the training and validation DataLoaders</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Number of batches in train DataLoader:'</span>: <span class="bu">len</span>(train_dataloader),</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Number of batches in validation DataLoader:'</span>: <span class="bu">len</span>(valid_dataloader)}</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_8ed31">
<thead>
</thead>
<tbody>
<tr>
<th id="T_8ed31_level0_row0" class="row_heading level0 row0">
Number of batches in train DataLoader:
</th>
<td id="T_8ed31_row0_col0" class="data row0 col0">
895
</td>
</tr>
<tr>
<th id="T_8ed31_level0_row1" class="row_heading level0 row1">
Number of batches in validation DataLoader:
</th>
<td id="T_8ed31_row1_col0" class="data row1 col0">
99
</td>
</tr>
</tbody>
</table>
</div>
<p>That completes the data preparation. Now we can finally train our hand gesture detector.</p>
</section>
</section>
<section id="fine-tuning-the-model" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-the-model">Fine-tuning the Model</h2>
<p>In this section, we will implement the training code and fine-tune our model.</p>
<section id="define-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="define-the-training-loop">Define the Training Loop</h3>
<p>The following function performs a single pass through the training or validation set.</p>
<p>The model takes in a batch of images and returns a tuple containing three variables. The first variable stores the image classification predictions for each proposed bounding box, which we use to determine the object type for a given bounding box. The second holds the coordinates and dimensions for all potential bounding boxes. The third variable contains probability scores for each proposed detection, indicating the likelihood it is an object.</p>
<p>The model always makes the same number of predictions for a given image size. Given a <code>384x384</code> image, the model will make <code>(384/8)*(384/8) + (384/16)*(384/16) + (384/32)*(384/32) = 3024</code> predictions. However, many of those predictions get filtered out when performing inference.</p>
<p>During training, we must determine which of the model’s predictions to pair with the ground truth annotations from our dataset before calculating the loss. YOLOX uses an approach called <a href="https://cj-mills.github.io/cjm-yolox-pytorch/simota.html#simotaassigner">SimOTA</a> for this step. The <a href="https://cj-mills.github.io/cjm-yolox-pytorch/loss.html#yoloxloss">YOLOXLoss</a> class performs this assignment automatically when <a href="https://cj-mills.github.io/cjm-yolox-pytorch/loss.html#yoloxloss.__call__">called</a>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(model, dataloader, optimizer, lr_scheduler, loss_func, device, scaler, epoch_id, is_training):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to run a single training or evaluation epoch.</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co">        model: A PyTorch model to train or evaluate.</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co">        dataloader: A PyTorch DataLoader providing the data.</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: The optimizer to use for training the model.</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_func: The loss function used for training.</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co">        device: The device (CPU or GPU) to run the model on.</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co">        scaler: Gradient scaler for mixed-precision training.</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co">        is_training: Boolean flag indicating whether the model is in training or evaluation mode.</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co">        The average loss for the epoch.</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the model to training mode if is_training is True, otherwise set it to evaluation mode</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    model.train() <span class="cf">if</span> is_training <span class="cf">else</span> model.<span class="bu">eval</span>()</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Initialize the total loss for this epoch</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    progress_bar <span class="op">=</span> tqdm(total<span class="op">=</span><span class="bu">len</span>(dataloader), desc<span class="op">=</span><span class="st">"Train"</span> <span class="cf">if</span> is_training <span class="cf">else</span> <span class="st">"Eval"</span>)  <span class="co"># Initialize a progress bar</span></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over the data</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_id, (inputs, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move inputs and targets to the specified device</span></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> torch.stack(inputs).to(device)</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract the ground truth bounding boxes and labels</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>        gt_bboxes, gt_labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>[(d[<span class="st">'boxes'</span>].to(device), d[<span class="st">'labels'</span>].to(device)) <span class="cf">for</span> d <span class="kw">in</span> targets])</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass with Automatic Mixed Precision (AMP) context manager</span></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> autocast(torch.device(device).<span class="bu">type</span>):</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>            cls_scores, bbox_preds, objectnesses <span class="op">=</span> model(inputs)</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the loss</span></span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>            losses <span class="op">=</span> loss_func(cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels)</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="bu">sum</span>([loss <span class="cf">for</span> loss <span class="kw">in</span> losses.values()])  <span class="co"># Sum up the losses</span></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If in training mode, backpropagate the error and update the weights</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scaler:</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>                scaler.scale(loss).backward()</span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>                scaler.step(optimizer)</span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>                old_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>                scaler.update()</span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>                new_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> new_scaler <span class="op">&gt;=</span> old_scaler:</span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>                    lr_scheduler.step()</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb45-50"><a href="#cb45-50" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb45-51"><a href="#cb45-51" aria-hidden="true" tabindex="-1"></a>                lr_scheduler.step()</span>
<span id="cb45-52"><a href="#cb45-52" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb45-53"><a href="#cb45-53" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb45-54"><a href="#cb45-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-55"><a href="#cb45-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the total loss</span></span>
<span id="cb45-56"><a href="#cb45-56" aria-hidden="true" tabindex="-1"></a>        loss_item <span class="op">=</span> loss.item()</span>
<span id="cb45-57"><a href="#cb45-57" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss_item</span>
<span id="cb45-58"><a href="#cb45-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-59"><a href="#cb45-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the progress bar</span></span>
<span id="cb45-60"><a href="#cb45-60" aria-hidden="true" tabindex="-1"></a>        progress_bar_dict <span class="op">=</span> <span class="bu">dict</span>(loss<span class="op">=</span>loss_item, avg_loss<span class="op">=</span>epoch_loss<span class="op">/</span>(batch_id<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb45-61"><a href="#cb45-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb45-62"><a href="#cb45-62" aria-hidden="true" tabindex="-1"></a>            progress_bar_dict.update(lr<span class="op">=</span>lr_scheduler.get_last_lr()[<span class="dv">0</span>])</span>
<span id="cb45-63"><a href="#cb45-63" aria-hidden="true" tabindex="-1"></a>        progress_bar.set_postfix(progress_bar_dict)</span>
<span id="cb45-64"><a href="#cb45-64" aria-hidden="true" tabindex="-1"></a>        progress_bar.update()</span>
<span id="cb45-65"><a href="#cb45-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-66"><a href="#cb45-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If loss is NaN or infinity, stop training</span></span>
<span id="cb45-67"><a href="#cb45-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb45-68"><a href="#cb45-68" aria-hidden="true" tabindex="-1"></a>            stop_training_message <span class="op">=</span> <span class="ss">f"Loss is NaN or infinite at epoch </span><span class="sc">{</span>epoch_id<span class="sc">}</span><span class="ss">, batch </span><span class="sc">{</span>batch_id<span class="sc">}</span><span class="ss">. Stopping training."</span></span>
<span id="cb45-69"><a href="#cb45-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> math.isnan(loss_item) <span class="kw">and</span> math.isfinite(loss_item), stop_training_message</span>
<span id="cb45-70"><a href="#cb45-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-71"><a href="#cb45-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cleanup and close the progress bar </span></span>
<span id="cb45-72"><a href="#cb45-72" aria-hidden="true" tabindex="-1"></a>    progress_bar.close()</span>
<span id="cb45-73"><a href="#cb45-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-74"><a href="#cb45-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the average loss for this epoch</span></span>
<span id="cb45-75"><a href="#cb45-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> (batch_id <span class="op">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we define the <code>train_loop</code> function, which executes the main training loop. It iterates over each epoch, runs through the training and validation sets, and saves the best model based on the validation loss.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>               train_dataloader, </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>               valid_dataloader, </span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>               optimizer, </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>               loss_func, </span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>               lr_scheduler, </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>               device, </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>               epochs, </span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>               checkpoint_path, </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>               use_scaler<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Main training loop.</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="co">        model: A PyTorch model to train.</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co">        train_dataloader: A PyTorch DataLoader providing the training data.</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">        valid_dataloader: A PyTorch DataLoader providing the validation data.</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: The optimizer to use for training the model.</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_func: The loss function used for training.</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a><span class="co">        lr_scheduler: The learning rate scheduler.</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="co">        device: The device (CPU or GPU) to run the model on.</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="co">        epochs: The number of epochs to train for.</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a><span class="co">        checkpoint_path: The path where to save the best model checkpoint.</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a><span class="co">        use_scaler: Whether to scale graidents when using a CUDA device</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> torch.cuda.amp.GradScaler() <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span> <span class="kw">and</span> use_scaler <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>    best_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)  <span class="co"># Initialize the best validation loss</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over the epochs</span></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs), desc<span class="op">=</span><span class="st">"Epochs"</span>):</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run a training epoch and get the training loss</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> run_epoch(model, train_dataloader, optimizer, lr_scheduler, loss_func, device, scaler, epoch, is_training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run an evaluation epoch and get the validation loss</span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>            valid_loss <span class="op">=</span> run_epoch(model, valid_dataloader, <span class="va">None</span>, <span class="va">None</span>, loss_func, device, scaler, epoch, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the validation loss is lower than the best validation loss seen so far, save the model checkpoint</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> valid_loss <span class="op">&lt;</span> best_loss:</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>            best_loss <span class="op">=</span> valid_loss</span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), checkpoint_path)</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Save metadata about the training process</span></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a>            training_metadata <span class="op">=</span> {</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>                <span class="st">'epoch'</span>: epoch,</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>                <span class="st">'train_loss'</span>: train_loss,</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>                <span class="st">'valid_loss'</span>: valid_loss, </span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>                <span class="st">'learning_rate'</span>: lr_scheduler.get_last_lr()[<span class="dv">0</span>],</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>                <span class="st">'model_architecture'</span>: model.name</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> <span class="bu">open</span>(Path(checkpoint_path.parent<span class="op">/</span><span class="st">'training_metadata.json'</span>), <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>                json.dump(training_metadata, f)</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the device is a GPU, empty the cache</span></span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">!=</span> <span class="st">'cpu'</span>:</span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>        <span class="bu">getattr</span>(torch, device.<span class="bu">type</span>).empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="set-the-model-checkpoint-path" class="level3">
<h3 class="anchored" data-anchor-id="set-the-model-checkpoint-path">Set the Model Checkpoint Path</h3>
<p>Before we proceed with training, let’s generate a timestamp for the training session and create a directory to save the checkpoints during training.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>timestamp <span class="op">=</span> datetime.datetime.now().strftime(<span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">_%H-%M-%S"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a directory to store the checkpoints if it does not already exist</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>checkpoint_dir <span class="op">=</span> Path(project_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the checkpoint directory if it does not already exist</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>checkpoint_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The model checkpoint path</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>checkpoint_path <span class="op">=</span> checkpoint_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>model<span class="sc">.</span>name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(checkpoint_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pytorch-yolox-object-detector/2024-02-17_00-31-07/yolox_tiny.pth</code></pre>
<p>Let’s save a copy of the normalization stats and the colormap for the current dataset in the training folder for future use.</p>
</section>
<section id="save-the-normalization-stats" class="level3">
<h3 class="anchored" data-anchor-id="save-the-normalization-stats">Save the Normalization Stats</h3>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert tuples to dictionaries for easier JSON representation</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>norm_stats_dict <span class="op">=</span> {<span class="st">"mean"</span>: norm_stats[<span class="dv">0</span>], <span class="st">"std_dev"</span>: norm_stats[<span class="dv">1</span>]}</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Export to JSON</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/norm_stats.json"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    json.dump(norm_stats_dict, f)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the name of the file that the normalization stats were written to</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/norm_stats.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pytorch-yolox-object-detector/2024-02-17_00-31-07/norm_stats.json</code></pre>
</section>
<section id="save-the-color-map" class="level3">
<h3 class="anchored" data-anchor-id="save-the-color-map">Save the Color Map</h3>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a color map and write it to a JSON file</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>color_map <span class="op">=</span> {<span class="st">'items'</span>: [{<span class="st">'label'</span>: label, <span class="st">'color'</span>: color} <span class="cf">for</span> label, color <span class="kw">in</span> <span class="bu">zip</span>(class_names, colors)]}</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_path<span class="sc">.</span>name<span class="sc">}</span><span class="ss">-colormap.json"</span>, <span class="st">"w"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    json.dump(color_map, <span class="bu">file</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the name of the file that the color map was written to</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_path<span class="sc">.</span>name<span class="sc">}</span><span class="ss">-colormap.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pytorch-yolox-object-detector/2024-02-17_00-31-07/hagrid-sample-30k-384p-colormap.json</code></pre>
</section>
<section id="configure-the-training-parameters" class="level3">
<h3 class="anchored" data-anchor-id="configure-the-training-parameters">Configure the Training Parameters</h3>
<p>Now, we can configure the parameters for training. We must define the learning rate, the number of training epochs and instantiate the optimizer, learning rate scheduler, and a <code>YOLOXLoss</code> object.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate for the model</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">7e-4</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of training epochs</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW optimizer; includes weight decay for regularization</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate scheduler; adjusts the learning rate during training</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer, </span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>                                                   max_lr<span class="op">=</span>lr, </span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>                                                   total_steps<span class="op">=</span>epochs<span class="op">*</span><span class="bu">len</span>(train_dataloader))</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the loss function for evaluating model predictions</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>yolox_loss <span class="op">=</span> YOLOXLoss(num_classes<span class="op">=</span>model.bbox_head.cls_out_channels, </span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>                       bbox_loss_weight<span class="op">=</span><span class="fl">10.0</span>, </span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>                       use_l1<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">Train the Model</h3>
<p>Finally, we can train the model using the <code>train_loop</code> function. Training time will depend on the available hardware.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Training usually takes around 1 hour and 52 minutes on the free GPU tier of Google Colab.</p>
</div>
</div>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>train_loop(model<span class="op">=</span>model, </span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>           train_dataloader<span class="op">=</span>train_dataloader,</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>           valid_dataloader<span class="op">=</span>valid_dataloader,</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>           optimizer<span class="op">=</span>optimizer, </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>           loss_func<span class="op">=</span>yolox_loss, </span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>           lr_scheduler<span class="op">=</span>lr_scheduler, </span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>           device<span class="op">=</span>torch.device(device), </span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>           epochs<span class="op">=</span>epochs, </span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>           checkpoint_path<span class="op">=</span>checkpoint_path,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>           use_scaler<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Epochs: 100% |██████████| 10/10 [23:44&lt;00:00, 143.24s/it]
Train: 100% |██████████| 895/895 [02:10&lt;00:00, 9.06it/s, loss=6.65, avg_loss=11.7, lr=0.000194]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 12.44it/s, loss=6.48, avg_loss=6.21]
Train: 100% |██████████| 895/895 [02:11&lt;00:00, 10.22it/s, loss=5.27, avg_loss=6.07, lr=0.00053]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 13.22it/s, loss=5.26, avg_loss=4.68]
Train: 100% |██████████| 895/895 [02:07&lt;00:00, 10.44it/s, loss=5.53, avg_loss=5.25, lr=0.0007]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 12.95it/s, loss=4.5, avg_loss=4.1]
Train: 100% |██████████| 895/895 [02:09&lt;00:00, 9.47it/s, loss=4.67, avg_loss=4.85, lr=0.000666]
Eval: 100% |██████████| 99/99 [00:12&lt;00:00, 11.98it/s, loss=4.48, avg_loss=3.89]
Train: 100% |██████████| 895/895 [02:12&lt;00:00, 9.84it/s, loss=4.11, avg_loss=4.5, lr=0.000569]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 12.70it/s, loss=3.99, avg_loss=3.54]
Train: 100% |██████████| 895/895 [02:09&lt;00:00, 10.21it/s, loss=3.91, avg_loss=4.18, lr=0.000429]
Eval: 100% |██████████| 99/99 [00:12&lt;00:00, 12.94it/s, loss=3.96, avg_loss=3.33]
Train: 100% |██████████| 895/895 [02:11&lt;00:00, 9.38it/s, loss=3.64, avg_loss=3.82, lr=0.000273]
Eval: 100% |██████████| 99/99 [00:12&lt;00:00, 11.29it/s, loss=3.77, avg_loss=3.1]
Train: 100% |██████████| 895/895 [02:10&lt;00:00, 10.33it/s, loss=3.91, avg_loss=3.52, lr=0.000133]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 12.95it/s, loss=3.46, avg_loss=2.85]
Train: 100% |██████████| 895/895 [02:09&lt;00:00, 10.51it/s, loss=2.96, avg_loss=3.25, lr=3.52e-5]
Eval: 100% |██████████| 99/99 [00:11&lt;00:00, 12.68it/s, loss=3.25, avg_loss=2.74]
Train: 100% |██████████| 895/895 [02:14&lt;00:00, 9.69it/s, loss=3.05, avg_loss=3.12, lr=4.96e-9]
Eval: 100% |██████████| 99/99 [00:12&lt;00:00, 12.01it/s, loss=3.29, avg_loss=2.71]</code></pre>
<p>At last, we have our hand gesture detector. To wrap up the tutorial, we can test our model by performing inference on individual images.</p>
</section>
</section>
<section id="making-predictions-with-the-model" class="level2">
<h2 class="anchored" data-anchor-id="making-predictions-with-the-model">Making Predictions with the Model</h2>
<p>In this final part of the tutorial, we will cover how to perform inference on individual images with our YOLOX model and filter the predictions.</p>
<section id="preparing-the-model-for-inference" class="level3">
<h3 class="anchored" data-anchor-id="preparing-the-model-for-inference">Preparing the Model for Inference</h3>
<p>Whenever we make predictions with the model, we must normalize the input data, scale the predicted bounding boxes, and calculate the associated confidence scores. Since these steps are always required, I included a <a href="https://cj-mills.github.io/cjm-yolox-pytorch/inference.html#yoloxinferencewrapper">wrapper class</a> with the <code>cjm_yolox_pytorch</code> package.</p>
<section id="wrap-the-model-with-preprocessing-and-post-processing-steps" class="level4">
<h4 class="anchored" data-anchor-id="wrap-the-model-with-preprocessing-and-post-processing-steps">Wrap the model with preprocessing and post-processing steps</h4>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the normalization stats to tensors</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>mean_tensor <span class="op">=</span> torch.tensor(norm_stats[<span class="dv">0</span>]).view(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>std_tensor <span class="op">=</span> torch.tensor(norm_stats[<span class="dv">1</span>]).view(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model to evaluation mode</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()<span class="op">;</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap the model with preprocessing and post-processing steps</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>wrapped_model <span class="op">=</span> YOLOXInferenceWrapper(model, mean_tensor, std_tensor).to(device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With our model prepped with the preprocessing and post-processing steps, we only need to prepare some input data.</p>
</section>
</section>
<section id="preparing-input-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-input-data">Preparing Input Data</h3>
<p>Let’s start with a random image from the validation set. That way, we have some ground truth bounding boxes to compare against. Unlike during training, we won’t stick to square input dimensions for inference. However, we still need to ensure both input dimensions are multiples of the max <a href="https://christianjmills.com/posts/pytorch-train-object-detector-yolox-tutorial/#loading-the-yolox-tiny-model">stride value</a>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a random item from the validation set</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>file_id <span class="op">=</span> random.choice(val_keys)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># file_id = '1bd84a50-12ce-43d2-a092-70aca798c8db'</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># file_id = 'f493ef0a-465e-46ba-ab28-8ffd97d74c4a'</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co"># file_id = '00973fac-440e-4a56-b60c-2a06d5fb155d'</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="co"># file_id = 'd83a4f5a-7712-4267-91d5-b7e18fff04f3'</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the image file path associated with the file ID</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>test_file <span class="op">=</span> img_dict[file_id]</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the test file</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(test_file).convert(<span class="st">'RGB'</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the test image</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>resized_img <span class="op">=</span> resize_img(test_img, target_sz<span class="op">=</span>train_sz, divisor<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the input dimensions are multiples of the max stride</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>input_dims <span class="op">=</span> [dim <span class="op">-</span> dim <span class="op">%</span> <span class="bu">max</span>(strides) <span class="cf">for</span> dim <span class="kw">in</span> resized_img.size]</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the offsets from the resized image dimensions to the input dimensions</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>offsets <span class="op">=</span> (np.array(resized_img.size) <span class="op">-</span> input_dims)<span class="op">/</span><span class="dv">2</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the scale between the source image and the resized image</span></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>min_img_scale <span class="op">=</span> <span class="bu">min</span>(test_img.size) <span class="op">/</span> <span class="bu">min</span>(resized_img.size)</span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop the resized image to the input dimensions</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> resized_img.crop(box<span class="op">=</span>[<span class="op">*</span>offsets, <span class="op">*</span>resized_img.size<span class="op">-</span>offsets])</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the target labels and bounding boxes</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>target_labels <span class="op">=</span> annotation_df.loc[file_id][<span class="st">'labels'</span>]</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>target_bboxes <span class="op">=</span> annotation_df.loc[file_id][<span class="st">'bboxes'</span>]</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale bounding boxes according to image dimensions</span></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>target_bboxes <span class="op">=</span> [bbox<span class="op">*</span>(resized_img.size<span class="op">*</span><span class="dv">2</span>) <span class="cf">for</span> bbox <span class="kw">in</span> target_bboxes]</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Offset bounding boxes according to input dimensions</span></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>target_bboxes <span class="op">=</span> np.array([bbox<span class="op">/</span>min_img_scale<span class="op">-</span>[<span class="op">*</span>offsets, <span class="dv">0</span>, <span class="dv">0</span>] <span class="cf">for</span> bbox <span class="kw">in</span> target_bboxes])</span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor from the test image and annotate it</span></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(input_img), </span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(target_bboxes), <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>target_labels, </span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> target_labels]]</span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the annotated test image</span></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a>annotated_test_img <span class="op">=</span> tensor_to_pil(annotated_tensor)</span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a>display(annotated_test_img)</span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas DataFrame for easy formatting</span></span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Image Size:"</span>: resized_img.size,</span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Dims:"</span>: input_dims,</span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Offsets:"</span>: offsets,</span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Min Image Scale:"</span>: min_img_scale,</span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Image Size:"</span>: input_img.size, </span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Target BBoxes:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>bbox<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> label, bbox <span class="kw">in</span> <span class="bu">zip</span>(target_labels, np.<span class="bu">round</span>(target_bboxes, decimals<span class="op">=</span><span class="dv">3</span>))]</span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_93_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_ce5be">
<thead>
</thead>
<tbody>
<tr>
<th id="T_ce5be_level0_row0" class="row_heading level0 row0">
Resized Image Size:
</th>
<td id="T_ce5be_row0_col0" class="data row0 col0">
(384, 511)
</td>
</tr>
<tr>
<th id="T_ce5be_level0_row1" class="row_heading level0 row1">
Input Dims:
</th>
<td id="T_ce5be_row1_col0" class="data row1 col0">
[384, 480]
</td>
</tr>
<tr>
<th id="T_ce5be_level0_row2" class="row_heading level0 row2">
Offsets:
</th>
<td id="T_ce5be_row2_col0" class="data row2 col0">
[ 0. 15.5]
</td>
</tr>
<tr>
<th id="T_ce5be_level0_row3" class="row_heading level0 row3">
Min Image Scale:
</th>
<td id="T_ce5be_row3_col0" class="data row3 col0">
1.000000
</td>
</tr>
<tr>
<th id="T_ce5be_level0_row4" class="row_heading level0 row4">
Input Image Size:
</th>
<td id="T_ce5be_row4_col0" class="data row4 col0">
(384, 480)
</td>
</tr>
<tr>
<th id="T_ce5be_level0_row5" class="row_heading level0 row5">
Target BBoxes:
</th>
<td id="T_ce5be_row5_col0" class="data row5 col0">
[‘palm:[208.828 180.933 117.177 151.437]’]
</td>
</tr>
</tbody>
</table>
</div>
<section id="pass-the-input-data-to-the-model" class="level4">
<h4 class="anchored" data-anchor-id="pass-the-input-data-to-the-model">Pass the input data to the model</h4>
<p>Now we can convert the test image to a tensor and pass it to the wrapped model.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the model and input data are on the same device</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>wrapped_model.to(device)<span class="op">;</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> transforms.Compose([transforms.ToImage(), </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>                                   transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>)])(input_img)[<span class="va">None</span>].to(device)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a prediction with the model</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    model_output <span class="op">=</span> wrapped_model(input_tensor)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>model_output.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>torch.Size([1, 3780, 6])</code></pre>
<p>With an input resolution of <code>384x480</code>, there are <code>3780</code> bounding box proposals. Each contains the top-left X and Y coordinates and dimensions for a bounding box, plus the class index and the associated confidence score. Most of these proposals are useless, so we’ll filter them out.</p>
</section>
</section>
<section id="filtering-model-output" class="level3">
<h3 class="anchored" data-anchor-id="filtering-model-output">Filtering Model Output</h3>
<p>We first use a threshold value to remove proposals the model is not confident about. Then, we can use the <a href="https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html#torchvision.ops.nms"><code>nms</code></a> function included with torchvision to remove overlapping bounding boxes using non-maximum suppression.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the bounding box confidence threshold</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>bbox_conf_thresh <span class="op">=</span> <span class="fl">0.35</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Move model output to the CPU</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>model_output <span class="op">=</span> model_output.to(<span class="st">'cpu'</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the proposals based on the confidence threshold</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>max_probs <span class="op">=</span> model_output[:, : ,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> max_probs <span class="op">&gt;</span> bbox_conf_thresh</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> model_output[mask]</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the proposals by probability in descending order</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> proposals[proposals[..., <span class="op">-</span><span class="dv">1</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the non-max suppression threshold</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>iou_thresh <span class="op">=</span> <span class="fl">0.45</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter bouning box proposals using NMS</span></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>proposal_indices <span class="op">=</span> torchvision.ops.nms(</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(proposals[:, :<span class="op">-</span><span class="dv">2</span>], <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>    scores<span class="op">=</span>proposals[:, <span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>    iou_threshold<span class="op">=</span>iou_thresh</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> proposals[proposal_indices]</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the proposals to a Pandas DataFrame</span></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>proposals_df <span class="op">=</span> pd.DataFrame([</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'x0'</span>: x0, <span class="st">'y0'</span>: y0, <span class="st">'width'</span>: w, <span class="st">'height'</span>: h, <span class="st">'label'</span>: label, <span class="st">'prob'</span>: prob} </span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x0, y0, w, h, label, prob <span class="kw">in</span> proposals.numpy()</span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(proposals_df) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add the label names to the DataFrame</span></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>    proposals_df[<span class="st">'label'</span>] <span class="op">=</span> proposals_df[<span class="st">'label'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: class_names[<span class="bu">int</span>(x)])</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the proposals Dataframe</span></span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>proposals_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
x0
</th>
<th>
y0
</th>
<th>
width
</th>
<th>
height
</th>
<th>
label
</th>
<th>
prob
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
207.83989
</td>
<td>
181.730957
</td>
<td>
116.061073
</td>
<td>
146.640503
</td>
<td>
palm
</td>
<td>
0.959833
</td>
</tr>
</tbody>
</table>
</div>
<p>By the end, we have a single <code>one</code> gesture proposal. All that’s left is to see how it compares to the ground-truth bounding box for this sample.</p>
<section id="annotate-image-using-bounding-box-proposals" class="level4">
<h4 class="anchored" data-anchor-id="annotate-image-using-bounding-box-proposals">Annotate image using bounding box proposals</h4>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract x0, y0, width, height columns</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>pred_bboxes <span class="op">=</span> proposals[:,:<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract label and prob columns as lists</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> [class_names[<span class="bu">int</span>(idx)] <span class="cf">for</span> idx <span class="kw">in</span> proposals[:,<span class="dv">4</span>]]</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>pred_probs <span class="op">=</span> proposals[:,<span class="dv">5</span>]</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(input_img), </span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(pred_bboxes, <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span> <span class="cf">for</span> label, prob <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_probs)], </span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[class_names.index(i)] <span class="cf">for</span> i <span class="kw">in</span> pred_labels]</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the annotated test image with the predicted bounding boxes</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>display(stack_imgs([annotated_test_img, tensor_to_pil(annotated_tensor)]))</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas DataFrame for easy formatting</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Target BBoxes:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>bbox<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> label, bbox <span class="kw">in</span> <span class="bu">zip</span>(target_labels, np.<span class="bu">round</span>(target_bboxes, decimals<span class="op">=</span><span class="dv">3</span>))],</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Predicted BBoxes:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>bbox<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> label, bbox <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_bboxes.<span class="bu">round</span>(decimals<span class="op">=</span><span class="dv">3</span>).numpy())],</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Confidence Scores:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span> <span class="cf">for</span> label, prob <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_probs)]</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_99_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_0b4a3">
<thead>
</thead>
<tbody>
<tr>
<th id="T_0b4a3_level0_row0" class="row_heading level0 row0">
Target BBoxes:
</th>
<td id="T_0b4a3_row0_col0" class="data row0 col0">
[‘palm:[208.828 180.933 117.177 151.437]’]
</td>
</tr>
<tr>
<th id="T_0b4a3_level0_row1" class="row_heading level0 row1">
Predicted BBoxes:
</th>
<td id="T_0b4a3_row1_col0" class="data row1 col0">
[‘palm:[207.84 181.731 116.061 146.64 ]’]
</td>
</tr>
<tr>
<th id="T_0b4a3_level0_row2" class="row_heading level0 row2">
Confidence Scores:
</th>
<td id="T_0b4a3_row2_col0" class="data row2 col0">
[‘palm: 95.98%’]
</td>
</tr>
</tbody>
</table>
</div>
<p>The predicted bounding box is not a perfect match to the ground-truth values, but it’s pretty close. Now let’s test the model on a brand-new image.</p>
</section>
</section>
<section id="testing-the-model-on-new-data" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-model-on-new-data">Testing the Model on New Data</h3>
<p>If we deploy the model in a real-world setting, we might want to scale predicted bounding boxes back up to a high-resolution source image. Below, we’ll show how to do this using an input image with a different aspect ratio than the source image. The test images are from the free stock photo site, <a href="https://www.pexels.com/photo/man-doing-rock-and-roll-sign-2769554/">Pexels</a>.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>test_img_name <span class="op">=</span> <span class="st">"pexels-2769554-man-doing-rock-and-roll-sign.jpg"</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-elina-volkova-16191659.jpg'</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-joshua-roberts-12922530.jpg'</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-luke-barky-2899727.jpg'</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-ketut-subiyanto-4584599.jpg'</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-nataliya-vaitkevich-5411990.jpg'</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-darina-belonogova-7886753.jpg'</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-katrin-bolovtsova-6706013.jpg'</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-leo-vinicius-3714450.jpg'</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># test_img_name = 'pexels-diva-plavalaguna-6937816.jpg'</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>test_img_url <span class="op">=</span> <span class="ss">f"https://huggingface.co/datasets/cj-mills/pexel-hand-gesture-test-images/resolve/main/</span><span class="sc">{</span>test_img_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>download_file(test_img_url, <span class="st">'./'</span>, <span class="va">False</span>)</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(test_img_name)</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>original_size <span class="op">=</span> test_img.size</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop test image so it's height is not a multiple of the max stride</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> test_img.crop(box<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, test_img.width, test_img.height<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>display(test_img)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Test Image Size:"</span>: original_size, </span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cropped Image Size:"</span>: test_img.size</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_101_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_ffd5c">
<thead>
</thead>
<tbody>
<tr>
<th id="T_ffd5c_level0_row0" class="row_heading level0 row0">
Test Image Size:
</th>
<td id="T_ffd5c_row0_col0" class="data row0 col0">
(640, 960)
</td>
</tr>
<tr>
<th id="T_ffd5c_level0_row1" class="row_heading level0 row1">
Cropped Image Size:
</th>
<td id="T_ffd5c_row1_col0" class="data row1 col0">
(640, 958)
</td>
</tr>
</tbody>
</table>
</div>
<p>Since the input and source images have different aspect ratios, we’ll offset any predicted bounding box coordinates.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize image without cropping to multiple of the max stride</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>resized_img <span class="op">=</span> resize_img(test_img, target_sz<span class="op">=</span>train_sz, divisor<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the input dimensions that multiples of the max stride</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>input_dims <span class="op">=</span> [dim <span class="op">-</span> dim <span class="op">%</span> <span class="bu">max</span>(strides) <span class="cf">for</span> dim <span class="kw">in</span> resized_img.size]</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the offsets from the resized image dimensions to the input dimensions</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>offsets <span class="op">=</span> (np.array(resized_img.size) <span class="op">-</span> input_dims)<span class="op">/</span><span class="dv">2</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the scale between the source image and the resized image</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>min_img_scale <span class="op">=</span> <span class="bu">min</span>(test_img.size) <span class="op">/</span> <span class="bu">min</span>(resized_img.size)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop the resized image to the input dimensions</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> resized_img.crop(box<span class="op">=</span>[<span class="op">*</span>offsets, <span class="op">*</span>resized_img.size<span class="op">-</span>offsets])</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>display(input_img)</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Image Size:"</span>: resized_img.size,</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Dims:"</span>: input_dims,</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Offsets:"</span>: offsets,</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Min Image Scale:"</span>: min_img_scale,</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Image Size:"</span>: input_img.size</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_102_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_1c2aa">
<thead>
</thead>
<tbody>
<tr>
<th id="T_1c2aa_level0_row0" class="row_heading level0 row0">
Resized Image Size:
</th>
<td id="T_1c2aa_row0_col0" class="data row0 col0">
(384, 574)
</td>
</tr>
<tr>
<th id="T_1c2aa_level0_row1" class="row_heading level0 row1">
Input Dims:
</th>
<td id="T_1c2aa_row1_col0" class="data row1 col0">
[384, 544]
</td>
</tr>
<tr>
<th id="T_1c2aa_level0_row2" class="row_heading level0 row2">
Offsets:
</th>
<td id="T_1c2aa_row2_col0" class="data row2 col0">
[ 0. 15.]
</td>
</tr>
<tr>
<th id="T_1c2aa_level0_row3" class="row_heading level0 row3">
Min Image Scale:
</th>
<td id="T_1c2aa_row3_col0" class="data row3 col0">
1.666667
</td>
</tr>
<tr>
<th id="T_1c2aa_level0_row4" class="row_heading level0 row4">
Input Image Size:
</th>
<td id="T_1c2aa_row4_col0" class="data row4 col0">
(384, 544)
</td>
</tr>
</tbody>
</table>
</div>
<p>With our input image prepared, we can pass it through the model and perform the same filtering steps. But this time, we’ll offset the <code>(x,y)</code> coordinates for the predicted bounding boxes and scale the dimensions to the source resolution.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> transforms.Compose([transforms.ToImage(), </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>                                   transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>)])(input_img)[<span class="va">None</span>].to(device)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>wrapped_model.to(device)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    model_output <span class="op">=</span> wrapped_model(input_tensor).to(<span class="st">'cpu'</span>)</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the proposals based on the confidence threshold</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>max_probs <span class="op">=</span> model_output[:, : ,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> max_probs <span class="op">&gt;</span> bbox_conf_thresh</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> model_output[mask]</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the proposals by probability in descending order</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> proposals[proposals[..., <span class="op">-</span><span class="dv">1</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter bouning box proposals using NMS</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>proposal_indices <span class="op">=</span> torchvision.ops.nms(</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(proposals[:, :<span class="op">-</span><span class="dv">2</span>], <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    scores<span class="op">=</span>proposals[:, <span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    iou_threshold<span class="op">=</span>iou_thresh</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> proposals[proposal_indices]</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Offset and scale the predicted bounding boxes</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>pred_bboxes <span class="op">=</span> (proposals[:,:<span class="dv">4</span>]<span class="op">+</span>torch.Tensor([<span class="op">*</span>offsets, <span class="dv">0</span>, <span class="dv">0</span>]))<span class="op">*</span>min_img_scale</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract label and prob columns as lists</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> [class_names[<span class="bu">int</span>(idx)] <span class="cf">for</span> idx <span class="kw">in</span> proposals[:,<span class="dv">4</span>]]</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>pred_probs <span class="op">=</span> proposals[:,<span class="dv">5</span>]</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(test_img), </span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(pred_bboxes), <span class="st">'xywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span> <span class="cf">for</span> label, prob <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_probs)], </span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[class_names.index(i)] <span class="cf">for</span> i <span class="kw">in</span> pred_labels]</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>display(tensor_to_pil(annotated_tensor))</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas Series for easy formatting</span></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Predicted BBoxes:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>bbox<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> label, bbox <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_bboxes.<span class="bu">round</span>(decimals<span class="op">=</span><span class="dv">3</span>).numpy())],</span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Confidence Scores:"</span>: [<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span> <span class="cf">for</span> label, prob <span class="kw">in</span> <span class="bu">zip</span>(pred_labels, pred_probs)]</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Rock &amp; No Gesture</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Mute</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-3" role="tab" aria-controls="tabset-3-3" aria-selected="false">Peace Inverted</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-4" role="tab" aria-controls="tabset-3-4" aria-selected="false">Call</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-5-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-5" role="tab" aria-controls="tabset-3-5" aria-selected="false">Palm</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-6-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-6" role="tab" aria-controls="tabset-3-6" aria-selected="false">Like</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-7-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-7" role="tab" aria-controls="tabset-3-7" aria-selected="false">Ok</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-8-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-8" role="tab" aria-controls="tabset-3-8" aria-selected="false">Peace</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-9-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-9" role="tab" aria-controls="tabset-3-9" aria-selected="false">Rock</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_103_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘rock:[342.451 240.994 111.934 109.824]’, ‘no_gesture:[191.77 516.789 105.333 80.797]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘rock: 91.52%’, ‘no_gesture: 87.33%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<img src="./images/mute.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘mute:[191.9 446.118 294.302 502.099]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘mute: 76.21%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-3-tab">
<img src="./images/peace_inverted.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘peace_inverted:[370.955 387.639 133.596 236.746]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘peace_inverted: 94.24%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-4-tab">
<img src="./images/call.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘call:[294.758 209.666 90.34 70.097]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘call: 90.57%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-5" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-5-tab">
<img src="./images/palm.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘palm:[ 99.852 122.838 247.511 208.458]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘palm: 81.47%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-6" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-6-tab">
<img src="./images/like.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘like:[188.877 318.4 102.481 151.627]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘like: 93.64%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-7" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-7-tab">
<img src="./images/ok.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘ok:[125.575 251.352 355.902 518.605]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘ok: 44.65%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-8" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-8-tab">
<img src="./images/peace.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘peace:[110.392 218.664 137.574 168.629]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘peace: 78.71%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-9" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-9-tab">
<img src="./images/rock.png" class="img-fluid quarto-figure quarto-figure-center">
<div style="overflow-x:auto; max-height:500px">
<table id="T_56f25">
<thead>
</thead>
<tbody>
<tr>
<th id="T_56f25_level0_row0" class="row_heading level0 row0">
Predicted BBoxes:
</th>
<td id="T_56f25_row0_col0" class="data row0 col0">
[‘rock:[ 71.192 97.024 169.156 170.206]’]
</td>
</tr>
<tr>
<th id="T_56f25_level0_row1" class="row_heading level0 row1">
Confidence Scores:
</th>
<td id="T_56f25_row1_col0" class="data row1 col0">
[‘rock: 66.71%’]
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Google Colab Users
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Don’t forget to download the model checkpoint and class labels from the Colab Environment’s file browser. (<a href="https://christianjmills.com/posts/google-colab-getting-started-tutorial/#working-with-data">tutorial link</a>)</li>
<li>Once you finish training and download the files, turn off hardware acceleration for the Colab Notebook to save GPU time. (<a href="https://christianjmills.com/posts/google-colab-getting-started-tutorial/#using-hardware-acceleration">tutorial link</a>)</li>
</ol>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Congratulations on completing this tutorial for training real-time object detection models in PyTorch! By now, you have successfully built a hand gesture detector that can identify and locate various gestures within images. The skills and knowledge you’ve acquired here serve as a solid foundation for future object detection projects.</p>
</section>
<section id="recommended-tutorials" class="level2">
<h2 class="anchored" data-anchor-id="recommended-tutorials">Recommended Tutorials</h2>
<ul>
<li><a href="./onnx-export/"><strong>Exporting YOLOX Models from PyTorch to ONNX</strong></a><strong>:</strong> Learn how to export YOLOX models from PyTorch to ONNX and perform inference using ONNX Runtime.</li>
<li><a href="./tfjs-export/"><strong>Exporting YOLOX Models from PyTorch to TensorFlow.js</strong></a><strong>:</strong> Learn how to export YOLOX models from PyTorch to TensorFlow.js to leverage efficient object detection in web applications.</li>
</ul>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Next Steps">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Next Steps
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Feel free to post questions or problems related to this tutorial in the comments below. I try to make time to address them on Thursdays and Fridays.</li>
<li>If you would like to explore my <a href="../../about.html#services">services</a> for your project, you can reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a></li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024, Christian J. Mills
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>