<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2024-01-29">
<meta name="description" content="Learn how to train Keypoint R-CNN models on custom datasets with PyTorch.">

<title>Christian Mills - Training Keypoint R-CNN Models with PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Christian Mills - Training Keypoint R-CNN Models with PyTorch">
<meta property="og:description" content="Learn how to train Keypoint R-CNN models on custom datasets with PyTorch.">
<meta property="og:image" content="https://christianjmills.com/posts/social-media/cover.jpg">
<meta property="og:site_name" content="Christian Mills">
<meta name="twitter:title" content="Christian Mills - Training Keypoint R-CNN Models with PyTorch">
<meta name="twitter:description" content="Learn how to train Keypoint R-CNN models on custom datasets with PyTorch.">
<meta name="twitter:image" content="https://christianjmills.com/posts/social-media/cover.jpg">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#getting-started-with-the-code" id="toc-getting-started-with-the-code" class="nav-link" data-scroll-target="#getting-started-with-the-code">Getting Started with the Code</a></li>
  <li><a href="#setting-up-your-python-environment" id="toc-setting-up-your-python-environment" class="nav-link" data-scroll-target="#setting-up-your-python-environment">Setting Up Your Python Environment</a>
  <ul>
  <li><a href="#creating-a-python-environment" id="toc-creating-a-python-environment" class="nav-link" data-scroll-target="#creating-a-python-environment">Creating a Python Environment</a></li>
  <li><a href="#installing-pytorch" id="toc-installing-pytorch" class="nav-link" data-scroll-target="#installing-pytorch">Installing PyTorch</a></li>
  <li><a href="#installing-additional-libraries" id="toc-installing-additional-libraries" class="nav-link" data-scroll-target="#installing-additional-libraries">Installing Additional Libraries</a></li>
  <li><a href="#installing-utility-packages" id="toc-installing-utility-packages" class="nav-link" data-scroll-target="#installing-utility-packages">Installing Utility Packages</a></li>
  </ul></li>
  <li><a href="#importing-the-required-dependencies" id="toc-importing-the-required-dependencies" class="nav-link" data-scroll-target="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
  <li><a href="#setting-up-the-project" id="toc-setting-up-the-project" class="nav-link" data-scroll-target="#setting-up-the-project">Setting Up the Project</a>
  <ul>
  <li><a href="#setting-a-random-number-seed" id="toc-setting-a-random-number-seed" class="nav-link" data-scroll-target="#setting-a-random-number-seed">Setting a Random Number Seed</a></li>
  <li><a href="#setting-the-device-and-data-type" id="toc-setting-the-device-and-data-type" class="nav-link" data-scroll-target="#setting-the-device-and-data-type">Setting the Device and Data Type</a></li>
  <li><a href="#setting-the-directory-paths" id="toc-setting-the-directory-paths" class="nav-link" data-scroll-target="#setting-the-directory-paths">Setting the Directory Paths</a></li>
  </ul></li>
  <li><a href="#loading-and-exploring-the-dataset" id="toc-loading-and-exploring-the-dataset" class="nav-link" data-scroll-target="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a>
  <ul>
  <li><a href="#setting-the-dataset-path" id="toc-setting-the-dataset-path" class="nav-link" data-scroll-target="#setting-the-dataset-path">Setting the Dataset Path</a></li>
  <li><a href="#downloading-the-dataset" id="toc-downloading-the-dataset" class="nav-link" data-scroll-target="#downloading-the-dataset">Downloading the Dataset</a></li>
  <li><a href="#get-image-file-paths" id="toc-get-image-file-paths" class="nav-link" data-scroll-target="#get-image-file-paths">Get Image File Paths</a></li>
  <li><a href="#get-image-annotations" id="toc-get-image-annotations" class="nav-link" data-scroll-target="#get-image-annotations">Get Image Annotations</a></li>
  <li><a href="#inspecting-the-class-distribution" id="toc-inspecting-the-class-distribution" class="nav-link" data-scroll-target="#inspecting-the-class-distribution">Inspecting the Class Distribution</a>
  <ul class="collapse">
  <li><a href="#get-image-classes" id="toc-get-image-classes" class="nav-link" data-scroll-target="#get-image-classes">Get image classes</a></li>
  <li><a href="#visualize-the-class-distribution" id="toc-visualize-the-class-distribution" class="nav-link" data-scroll-target="#visualize-the-class-distribution">Visualize the class distribution</a></li>
  </ul></li>
  <li><a href="#visualizing-image-annotations" id="toc-visualizing-image-annotations" class="nav-link" data-scroll-target="#visualizing-image-annotations">Visualizing Image Annotations</a>
  <ul class="collapse">
  <li><a href="#generate-a-color-map" id="toc-generate-a-color-map" class="nav-link" data-scroll-target="#generate-a-color-map">Generate a color map</a></li>
  <li><a href="#download-a-font-file" id="toc-download-a-font-file" class="nav-link" data-scroll-target="#download-a-font-file">Download a font file</a></li>
  <li><a href="#define-the-bounding-box-annotation-function" id="toc-define-the-bounding-box-annotation-function" class="nav-link" data-scroll-target="#define-the-bounding-box-annotation-function">Define the bounding box annotation function</a></li>
  </ul></li>
  <li><a href="#annotate-sample-image" id="toc-annotate-sample-image" class="nav-link" data-scroll-target="#annotate-sample-image">Annotate sample image</a></li>
  </ul></li>
  <li><a href="#loading-the-keypoint-r-cnn-model" id="toc-loading-the-keypoint-r-cnn-model" class="nav-link" data-scroll-target="#loading-the-keypoint-r-cnn-model">Loading the Keypoint R-CNN Model</a>
  <ul>
  <li><a href="#summarizing-the-model" id="toc-summarizing-the-model" class="nav-link" data-scroll-target="#summarizing-the-model">Summarizing the Model</a></li>
  </ul></li>
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data">Preparing the Data</a>
  <ul>
  <li><a href="#training-validation-split" id="toc-training-validation-split" class="nav-link" data-scroll-target="#training-validation-split">Training-Validation Split</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data Augmentation</a>
  <ul class="collapse">
  <li><a href="#set-training-image-size" id="toc-set-training-image-size" class="nav-link" data-scroll-target="#set-training-image-size">Set training image size</a></li>
  <li><a href="#initialize-custom-transforms" id="toc-initialize-custom-transforms" class="nav-link" data-scroll-target="#initialize-custom-transforms">Initialize custom transforms</a></li>
  <li><a href="#test-the-transforms" id="toc-test-the-transforms" class="nav-link" data-scroll-target="#test-the-transforms">Test the transforms</a></li>
  </ul></li>
  <li><a href="#training-dataset-class" id="toc-training-dataset-class" class="nav-link" data-scroll-target="#training-dataset-class">Training Dataset Class</a></li>
  <li><a href="#image-transforms" id="toc-image-transforms" class="nav-link" data-scroll-target="#image-transforms">Image Transforms</a></li>
  <li><a href="#initialize-datasets" id="toc-initialize-datasets" class="nav-link" data-scroll-target="#initialize-datasets">Initialize Datasets</a></li>
  <li><a href="#inspect-samples" id="toc-inspect-samples" class="nav-link" data-scroll-target="#inspect-samples">Inspect Samples</a>
  <ul class="collapse">
  <li><a href="#inspect-training-set-sample" id="toc-inspect-training-set-sample" class="nav-link" data-scroll-target="#inspect-training-set-sample">Inspect training set sample</a></li>
  <li><a href="#inspect-validation-set-sample" id="toc-inspect-validation-set-sample" class="nav-link" data-scroll-target="#inspect-validation-set-sample">Inspect validation set sample</a></li>
  </ul></li>
  <li><a href="#initialize-dataloaders" id="toc-initialize-dataloaders" class="nav-link" data-scroll-target="#initialize-dataloaders">Initialize DataLoaders</a></li>
  </ul></li>
  <li><a href="#fine-tuning-the-model" id="toc-fine-tuning-the-model" class="nav-link" data-scroll-target="#fine-tuning-the-model">Fine-tuning the Model</a>
  <ul>
  <li><a href="#define-utility-functions" id="toc-define-utility-functions" class="nav-link" data-scroll-target="#define-utility-functions">Define Utility Functions</a>
  <ul class="collapse">
  <li><a href="#define-a-function-to-create-a-bounding-box-that-encapsulates-the-key-points" id="toc-define-a-function-to-create-a-bounding-box-that-encapsulates-the-key-points" class="nav-link" data-scroll-target="#define-a-function-to-create-a-bounding-box-that-encapsulates-the-key-points">Define a function to create a bounding box that encapsulates the key points</a></li>
  <li><a href="#define-a-conditional-autocast-context-manager" id="toc-define-a-conditional-autocast-context-manager" class="nav-link" data-scroll-target="#define-a-conditional-autocast-context-manager">Define a conditional <code>autocast</code> context manager</a></li>
  </ul></li>
  <li><a href="#define-the-training-loop" id="toc-define-the-training-loop" class="nav-link" data-scroll-target="#define-the-training-loop">Define the Training Loop</a></li>
  <li><a href="#set-the-model-checkpoint-path" id="toc-set-the-model-checkpoint-path" class="nav-link" data-scroll-target="#set-the-model-checkpoint-path">Set the Model Checkpoint Path</a></li>
  <li><a href="#save-the-color-map" id="toc-save-the-color-map" class="nav-link" data-scroll-target="#save-the-color-map">Save the Color Map</a></li>
  <li><a href="#configure-the-training-parameters" id="toc-configure-the-training-parameters" class="nav-link" data-scroll-target="#configure-the-training-parameters">Configure the Training Parameters</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">Train the Model</a></li>
  </ul></li>
  <li><a href="#making-predictions-with-the-model" id="toc-making-predictions-with-the-model" class="nav-link" data-scroll-target="#making-predictions-with-the-model">Making Predictions with the Model</a>
  <ul>
  <li><a href="#prepare-input-data" id="toc-prepare-input-data" class="nav-link" data-scroll-target="#prepare-input-data">Prepare Input Data</a></li>
  <li><a href="#get-target-annotation-data" id="toc-get-target-annotation-data" class="nav-link" data-scroll-target="#get-target-annotation-data">Get Target Annotation Data</a></li>
  <li><a href="#pass-input-data-to-the-model" id="toc-pass-input-data-to-the-model" class="nav-link" data-scroll-target="#pass-input-data-to-the-model">Pass Input Data to the Model</a></li>
  <li><a href="#filter-the-model-output" id="toc-filter-the-model-output" class="nav-link" data-scroll-target="#filter-the-model-output">Filter the Model Output</a></li>
  <li><a href="#compare-model-predictions-with-the-source-annotations" id="toc-compare-model-predictions-with-the-source-annotations" class="nav-link" data-scroll-target="#compare-model-predictions-with-the-source-annotations">Compare Model Predictions with the Source Annotations</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#recommended-tutorials" id="toc-recommended-tutorials" class="nav-link" data-scroll-target="#recommended-tutorials">Recommended Tutorials</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Training Keypoint R-CNN Models with PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">keypoint-rcnn</div>
    <div class="quarto-category">keypoint-estimation</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>

<div>
  <div class="description">
    Learn how to train Keypoint R-CNN models on custom datasets with PyTorch.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 29, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">September 11, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../series/tutorials/pytorch-train-keypoint-rcnn-series.html"><strong>Training Keypoint R-CNN Models with PyTorch</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#getting-started-with-the-code">Getting Started with the Code</a></li>
<li><a href="#setting-up-your-python-environment">Setting Up Your Python Environment</a></li>
<li><a href="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
<li><a href="#setting-up-the-project">Setting Up the Project</a></li>
<li><a href="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a></li>
<li><a href="#loading-the-keypoint-r-cnn-model">Loading the Keypoint R-CNN Model</a></li>
<li><a href="#preparing-the-data">Preparing the Data</a></li>
<li><a href="#fine-tuning-the-model">Fine-tuning the Model</a></li>
<li><a href="#making-predictions-with-the-model">Making Predictions with the Model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to this hands-on guide to training Keypoint R-CNN models in PyTorch. Keypoint estimation models predict the locations of points on a given object or person, allowing us to recognize and interpret poses, gestures, or significant parts of objects.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/pytorch-keypoint-r-cnn-tutorial-hero-image.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>The tutorial walks through setting up a Python environment, loading the raw keypoint annotations, annotating and augmenting images, creating a custom <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Dataset</a> class to feed samples to a model, finetuning a Keypoint R-CNN model, and performing inference.</p>
<p>This guide is suitable for beginners and experienced practitioners, providing the code, explanations, and resources needed to understand and implement each step. Upon completion, you will have a solid foundation for training custom key point estimation models for other projects.</p>
</section>
<section id="getting-started-with-the-code" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-the-code">Getting Started with the Code</h2>
<p>The tutorial code is available as a <a href="https://jupyter.org/">Jupyter Notebook</a>, which you can run locally or in a cloud-based environment like <a href="https://colab.research.google.com/">Google Colab</a>. I have dedicated tutorials for those new to these platforms or who need guidance setting up:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Setup Guides">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Setup Guides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../posts/google-colab-getting-started-tutorial/"><strong>Getting Started with Google Colab</strong></a></li>
<li><a href="../../posts/mamba-getting-started-tutorial-windows/"><strong>Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows</strong></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Tutorial Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tutorial Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Platform</th>
<th>Jupyter Notebook</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Google Colab</td>
<td><a href="https://colab.research.google.com/github/cj-mills/pytorch-keypoint-rcnn-tutorial-code/blob/main/notebooks/pytorch-keypoint-r-cnn-training-labelme.ipynb">Open In Colab</a></td>
</tr>
<tr class="even">
<td>Linux</td>
<td><a href="https://github.com/cj-mills/pytorch-keypoint-rcnn-tutorial-code/blob/main/notebooks/pytorch-keypoint-r-cnn-training-labelme.ipynb">GitHub Repository</a></td>
</tr>
<tr class="odd">
<td>Windows</td>
<td><a href="https://github.com/cj-mills/pytorch-keypoint-rcnn-tutorial-code/blob/main/notebooks/pytorch-keypoint-r-cnn-training-labelme-windows.ipynb">GitHub Repository</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="macOS &amp; Windows Users">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
macOS &amp; Windows Users
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code in this tutorial targets Linux platforms, but most of it should also work on macOS and Windows.</p>
<p>However, Python multiprocessing works differently on those platforms, requiring some changes to leverage multi-processing for the <code>DataLoader</code> objects.</p>
<p>I’ve made a dedicated version of the tutorial code to run on Windows. The included changes should also work on macOS, but I don’t have a Mac to verify.</p>
</div>
</div>
</section>
<section id="setting-up-your-python-environment" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-your-python-environment">Setting Up Your Python Environment</h2>
<p>Before diving into the code, we’ll cover the steps to create a local Python environment and install the necessary dependencies.</p>
<section id="creating-a-python-environment" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-python-environment">Creating a Python Environment</h3>
<p>First, we’ll create a Python environment using <a href="https://docs.conda.io/en/latest/">Conda</a>/<a href="https://mamba.readthedocs.io/en/latest/">Mamba</a>. Open a terminal with Conda/Mamba installed and run the following commands:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Conda</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Mamba</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.11 environment</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">--name</span> pytorch-env python=3.11 <span class="at">-y</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.11 environment</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">--name</span> pytorch-env python=3.11 <span class="at">-y</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="installing-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="installing-pytorch">Installing PyTorch</h3>
<p>Next, we’ll install <a href="https://pytorch.org/">PyTorch</a>. Run the appropriate command for your hardware and operating system.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Linux/Windows (CUDA)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Mac</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">Linux (CPU)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-4" role="tab" aria-controls="tabset-2-4" aria-selected="false">Windows (CPU)</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MPS (Metal Performance Shaders) acceleration is available on MacOS 12.3+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch for CPU only</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-4-tab">
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch for CPU only</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="installing-additional-libraries" class="level3">
<h3 class="anchored" data-anchor-id="installing-additional-libraries">Installing Additional Libraries</h3>
<p>We also need to install some additional libraries for our project.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>jupyter</code></td>
<td>An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. (<a href="https://jupyter.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>matplotlib</code></td>
<td>This package provides a comprehensive collection of visualization tools to create high-quality plots, charts, and graphs for data exploration and presentation. (<a href="https://matplotlib.org/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>pandas</code></td>
<td>This package provides fast, powerful, and flexible data analysis and manipulation tools. (<a href="https://pandas.pydata.org/">link</a>)</td>
</tr>
<tr class="even">
<td><code>pillow</code></td>
<td>The Python Imaging Library adds image processing capabilities. (<a href="https://pillow.readthedocs.io/en/stable/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>torchtnt</code></td>
<td>A Python library that provides fast, extensible progress bars for loops and other iterable objects in Python. (<a href="https://tqdm.github.io/">link</a>)</td>
</tr>
<tr class="even">
<td><code>tabulate</code></td>
<td>Pretty-print tabular data in Python. (<a href="https://pypi.org/project/tabulate/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>tqdm</code></td>
<td>A Python library that provides fast, extensible progress bars for loops and other iterable objects in Python. (<a href="https://tqdm.github.io/">link</a>)</td>
</tr>
<tr class="even">
<td><code>distinctipy</code></td>
<td>A lightweight python package providing functions to generate colours that are visually distinct from one another. (<a href="https://distinctipy.readthedocs.io/en/latest/">link</a>)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the following command to install these additional libraries:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install additional dependencies</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install distinctipy jupyter matplotlib pandas pillow torchtnt==0.2.0 tabulate tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="installing-utility-packages" class="level3">
<h3 class="anchored" data-anchor-id="installing-utility-packages">Installing Utility Packages</h3>
<p>We will also install some utility packages I made, which provide shortcuts for routine tasks.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cjm_pandas_utils</code></td>
<td>Some utility functions for working with Pandas. (<a href="https://cj-mills.github.io/cjm-pandas-utils/">link</a>)</td>
</tr>
<tr class="even">
<td><code>cjm_pil_utils</code></td>
<td>Some PIL utility functions I frequently use. (<a href="https://cj-mills.github.io/cjm-pil-utils/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>cjm_psl_utils</code></td>
<td>Some utility functions using the Python Standard Library. (<a href="https://cj-mills.github.io/cjm-psl-utils/">link</a>)</td>
</tr>
<tr class="even">
<td><code>cjm_pytorch_utils</code></td>
<td>Some utility functions for working with PyTorch. (<a href="https://cj-mills.github.io/cjm-pytorch-utils/">link</a>)</td>
</tr>
<tr class="odd">
<td><code>cjm_torchvision_tfms</code></td>
<td>Some custom Torchvision tranforms. (<a href="https://cj-mills.github.io/cjm-torchvision-tfms/">link</a>)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the following command to install the utility packages:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install additional utility packages</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>pip install cjm_pandas_utils cjm_pil_utils cjm_psl_utils cjm_pytorch_utils cjm_torchvision_tfms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With our environment set up, we can open our Jupyter Notebook and dive into the code.</p>
</section>
</section>
<section id="importing-the-required-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-required-dependencies">Importing the Required Dependencies</h2>
<p>First, we will import the necessary Python modules into our Jupyter Notebook.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Python Standard Library dependencies</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> contextmanager</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> multiprocessing</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import utility functions</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pandas_utils.core <span class="im">import</span> markdown_to_pandas</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pil_utils.core <span class="im">import</span> resize_img, get_img_files, stack_imgs</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_psl_utils.core <span class="im">import</span> download_file, file_extract</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pytorch_utils.core <span class="im">import</span> set_seed, pil_to_tensor, tensor_to_pil, get_torch_device, denorm_img_tensor, move_data_to_device</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_torchvision_tfms.core <span class="im">import</span> ResizeMax, PadSquare, CustomRandomIoUCrop, RandomPixelCopy</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the distinctipy module</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> distinctipy <span class="im">import</span> distinctipy</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib for creating plots</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the pandas package</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PIL for image manipulation</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch dependencies</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.amp <span class="im">import</span> autocast</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtnt.utils <span class="im">import</span> get_module_summary</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Import torchvision dependencies</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>torchvision.disable_beta_transforms_warning()</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.tv_tensors <span class="im">import</span> BoundingBoxes</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.v2  <span class="im">as</span> transforms</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Keypoint R-CNN</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.keypoint_rcnn <span class="im">import</span> KeypointRCNNPredictor</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.rpn <span class="im">import</span> AnchorGenerator</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection <span class="im">import</span> keypointrcnn_resnet50_fpn</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Torchvision provides dedicated <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.Tensor</code></a> subclasses for different annotation types called <a href="https://pytorch.org/vision/stable/tv_tensors.html"><code>TVTensors</code></a>. Torchvision’s V2 transforms use these subclasses to update the annotations based on the applied image augmentations. While there is currently no dedicated TVTensor class for keypoint annotations, we can use the one for <a href="https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html">bounding boxes</a> instead. Torchvision does include a <a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_keypoints.html"><code>draw_keypoints</code></a> function, but we might as well stick with the <a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html"><code>draw_bounding_boxes</code></a> function to annotate images.</p>
</section>
<section id="setting-up-the-project" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-project">Setting Up the Project</h2>
<p>In this section, we set up some basics for our project, such as initializing random number generators, setting the PyTorch device to run the model, and preparing the folders for our project and datasets.</p>
<section id="setting-a-random-number-seed" class="level3">
<h3 class="anchored" data-anchor-id="setting-a-random-number-seed">Setting a Random Number Seed</h3>
<p>First, we set the seed for generating random numbers using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#set_seed">set_seed</a> function from the <code>cjm_pytorch_utils</code> package.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">123</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>set_seed(seed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="setting-the-device-and-data-type" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-device-and-data-type">Setting the Device and Data Type</h3>
<p>Next, we determine the device to use for training using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#get_torch_device">get_torch_device</a> function from the <code>cjm_pytorch_utils</code> package and set the data type of our tensors.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_torch_device()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> torch.float32</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>device, dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>('cuda', torch.float32)</code></pre>
</section>
<section id="setting-the-directory-paths" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-directory-paths">Setting the Directory Paths</h3>
<p>We can then set up a directory for our project to store our results and other related files. We also need a place to store our dataset. The following code creates the folders in the current directory (<code>./</code>). Update the path if that is not suitable for you.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The name for the project</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>project_name <span class="op">=</span> <span class="ss">f"pytorch-keypoint-r-cnn"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The path for the project folder</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>project_dir <span class="op">=</span> Path(<span class="ss">f"./</span><span class="sc">{</span>project_name<span class="sc">}</span><span class="ss">/"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the project directory if it does not already exist</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>project_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store datasets</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>dataset_dir <span class="op">=</span> Path(<span class="st">"./Datasets/"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the dataset directory if it does not exist</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>dataset_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store archive files</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>archive_dir <span class="op">=</span> dataset_dir<span class="op">/</span><span class="st">'../Archive'</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the archive directory if it does not exist</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>archive_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Series with the paths and converting it to a DataFrame for display</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project Directory:"</span>: project_dir,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Directory:"</span>: dataset_dir, </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Directory:"</span>: archive_dir</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_c7875">
<thead>
</thead>
<tbody>
<tr>
<th id="T_c7875_level0_row0" class="row_heading level0 row0">
Project Directory:
</th>
<td id="T_c7875_row0_col0" class="data row0 col0">
pytorch-keypoint-r-cnn
</td>
</tr>
<tr>
<th id="T_c7875_level0_row1" class="row_heading level0 row1">
Dataset Directory:
</th>
<td id="T_c7875_row1_col0" class="data row1 col0">
Datasets
</td>
</tr>
<tr>
<th id="T_c7875_level0_row2" class="row_heading level0 row2">
Archive Directory:
</th>
<td id="T_c7875_row2_col0" class="data row2 col0">
Datasets/../Archive
</td>
</tr>
</tbody>
</table>
</div>
<p>Double-check the project and dataset directories exist in the specified paths and that you can add files to them before continuing. At this point, our project is set up and ready to go. In the next section, we will download and explore the dataset.</p>
</section>
</section>
<section id="loading-and-exploring-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-exploring-the-dataset">Loading and Exploring the Dataset</h2>
<p>I annotated a small dataset with key points for this tutorial using images from the free stock photo site <a href="https://www.pexels.com/">Pexels</a>. The dataset is available on <a href="https://huggingface.co/">HuggingFace Hub</a> at the link below:</p>
<ul>
<li><strong>Dataset Repository:</strong> <a href="https://huggingface.co/datasets/cj-mills/labelme-keypoint-eyes-noses-dataset/tree/main">labelme-keypoint-eyes-noses-dataset</a></li>
</ul>
<p>The dataset contains 2D coordinates for eyes and noses on human faces.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Keypoint Annotation Format">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Keypoint Annotation Format
</div>
</div>
<div class="callout-body-container callout-body">
<p>The keypoints for this dataset use the <a href="https://github.com/labelmeai/labelme">LabelMe</a> annotation format. You can learn more about this format and how to work with such annotations in the tutorial linked below:</p>
<ul>
<li><a href="../../posts/torchvision-labelme-annotation-tutorials/keypoints/">Working with LabelMe Keypoint Annotations in Torchvision</a></li>
</ul>
</div>
</div>
<section id="setting-the-dataset-path" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-dataset-path">Setting the Dataset Path</h3>
<p>First, we construct the name for the Hugging Face Hub dataset and set where to download and extract the dataset.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the name of the dataset</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">'labelme-keypoint-eyes-noses-dataset'</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset name by combining the username and dataset name</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>hf_dataset <span class="op">=</span> <span class="ss">f'cj-mills/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the zip file that contains the dataset</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>archive_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>archive_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">.zip'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the directory where the dataset will be extracted</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>dataset_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Series with the dataset name and paths and converting it to a DataFrame for display</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HuggingFace Dataset:"</span>: hf_dataset, </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Path:"</span>: archive_path, </span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Path:"</span>: dataset_path</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_be76c">
<thead>
</thead>
<tbody>
<tr>
<th id="T_be76c_level0_row0" class="row_heading level0 row0">
HuggingFace Dataset:
</th>
<td id="T_be76c_row0_col0" class="data row0 col0">
cj-mills/labelme-keypoint-eyes-noses-dataset
</td>
</tr>
<tr>
<th id="T_be76c_level0_row1" class="row_heading level0 row1">
Archive Path:
</th>
<td id="T_be76c_row1_col0" class="data row1 col0">
Datasets/../Archive/labelme-keypoint-eyes-noses-dataset.zip
</td>
</tr>
<tr>
<th id="T_be76c_level0_row2" class="row_heading level0 row2">
Dataset Path:
</th>
<td id="T_be76c_row2_col0" class="data row2 col0">
Datasets/labelme-keypoint-eyes-noses-dataset
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="downloading-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="downloading-the-dataset">Downloading the Dataset</h3>
<p>We can now download the archive file and extract the dataset using the <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#download_file"><code>download_file</code></a> and <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#file_extract"><code>file_extract</code></a> functions from the <code>cjm_psl_utils</code> package. We can delete the archive afterward to save space.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset URL</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dataset_url <span class="op">=</span> <span class="ss">f"https://huggingface.co/datasets/</span><span class="sc">{</span>hf_dataset<span class="sc">}</span><span class="ss">/resolve/main/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">.zip"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"HuggingFace Dataset URL: </span><span class="sc">{</span>dataset_url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set whether to delete the archive file after extracting the dataset</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>delete_archive <span class="op">=</span> <span class="va">True</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset if not present</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> dataset_path.is_dir():</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Dataset folder already exists"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading dataset..."</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    download_file(dataset_url, archive_dir)    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracting dataset..."</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    file_extract(fname<span class="op">=</span>archive_path, dest<span class="op">=</span>dataset_dir)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Delete the archive if specified</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> delete_archive: archive_path.unlink()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="get-image-file-paths" class="level3">
<h3 class="anchored" data-anchor-id="get-image-file-paths">Get Image File Paths</h3>
<p>Next, we will make a dictionary that maps each image’s unique name to its file path, allowing us to retrieve the file path for a given image more efficiently.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of image files in the dataset</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>img_file_paths <span class="op">=</span> get_img_files(dataset_path)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dictionary that maps file names to file paths</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>img_dict <span class="op">=</span> {<span class="bu">file</span>.stem : <span class="bu">file</span> <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> (img_file_paths)}</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of image files</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of Images: </span><span class="sc">{</span><span class="bu">len</span>(img_dict)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first five entries from the dictionary using a Pandas DataFrame</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>pd.DataFrame.from_dict(img_dict, orient<span class="op">=</span><span class="st">'index'</span>).head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Number of Images: 200</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
denim-jacket-fashion-fashion-model-1848570
</th>
<td>
Datasets/labelme-keypoint-eyes-noses-dataset/denim-jacket-fashion-fashion-model-1848570.jpg
</td>
</tr>
<tr>
<th>
dried-dry-face-2965690
</th>
<td>
Datasets/labelme-keypoint-eyes-noses-dataset/dried-dry-face-2965690.jpg
</td>
</tr>
<tr>
<th>
elderly-face-old-person-2856346
</th>
<td>
Datasets/labelme-keypoint-eyes-noses-dataset/elderly-face-old-person-2856346.jpg
</td>
</tr>
<tr>
<th>
elderly-hair-man-1319289
</th>
<td>
Datasets/labelme-keypoint-eyes-noses-dataset/elderly-hair-man-1319289.jpg
</td>
</tr>
<tr>
<th>
face-facial-expression-fashion-2592000
</th>
<td>
Datasets/labelme-keypoint-eyes-noses-dataset/face-facial-expression-fashion-2592000.jpg
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-image-annotations" class="level3">
<h3 class="anchored" data-anchor-id="get-image-annotations">Get Image Annotations</h3>
<p>We will then read the content of the JSON annotation file associated with each image into a single Pandas DataFrame so we can easily query the annotations.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of JSON files in the dataset</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>annotation_file_paths <span class="op">=</span> <span class="bu">list</span>(dataset_path.glob(<span class="st">'*.json'</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a generator that yields Pandas DataFrames containing the data from each JSON file</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>cls_dataframes <span class="op">=</span> (pd.read_json(f, orient<span class="op">=</span><span class="st">'index'</span>).transpose() <span class="cf">for</span> f <span class="kw">in</span> tqdm(annotation_file_paths))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the DataFrames into a single DataFrame</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>annotation_df <span class="op">=</span> pd.concat(cls_dataframes, ignore_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign the image file name as the index for each row</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>annotation_df[<span class="st">'index'</span>] <span class="op">=</span> annotation_df.<span class="bu">apply</span>(<span class="kw">lambda</span> row: row[<span class="st">'imagePath'</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>annotation_df <span class="op">=</span> annotation_df.set_index(<span class="st">'index'</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep only the rows that correspond to the filenames in the 'img_dict' dictionary</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>annotation_df <span class="op">=</span> annotation_df.loc[<span class="bu">list</span>(img_dict.keys())]</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first 5 rows of the DataFrame</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>annotation_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
version
</th>
<th>
flags
</th>
<th>
shapes
</th>
<th>
imagePath
</th>
<th>
imageData
</th>
<th>
imageHeight
</th>
<th>
imageWidth
</th>
</tr>
<tr>
<th>
index
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
denim-jacket-fashion-fashion-model-1848570
</th>
<td>
5.3.1
</td>
<td>
{}
</td>
<td>
[{‘label’: ‘left-eye’, ‘points’: [[329.17073170731703, 252.59756097560972]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘nose’, ‘points’: [[323.68292682926835, 291.0121951219512]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘right-eye’, ‘points’: [[260.2682926829268, 234.91463414634143]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}]
</td>
<td>
denim-jacket-fashion-fashion-model-1848570.jpg
</td>
<td>
None
</td>
<td>
768
</td>
<td>
512
</td>
</tr>
<tr>
<th>
dried-dry-face-2965690
</th>
<td>
5.3.1
</td>
<td>
{}
</td>
<td>
[{‘label’: ‘right-eye’, ‘points’: [[201.7317073170732, 351.9878048780488]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘left-eye’, ‘points’: [[333.43902439024396, 342.23170731707313]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘nose’, ‘points’: [[271.2439024390244, 436.1341463414634]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}]
</td>
<td>
dried-dry-face-2965690.jpg
</td>
<td>
None
</td>
<td>
768
</td>
<td>
512
</td>
</tr>
<tr>
<th>
elderly-face-old-person-2856346
</th>
<td>
5.3.1
</td>
<td>
{}
</td>
<td>
[{‘label’: ‘left-eye’, ‘points’: [[302.3414634146342, 286.1341463414634]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘nose’, ‘points’: [[243.80487804878055, 339.79268292682923]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘right-eye’, ‘points’: [[196.2439024390244, 286.7439024390244]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}]
</td>
<td>
elderly-face-old-person-2856346.jpg
</td>
<td>
None
</td>
<td>
768
</td>
<td>
512
</td>
</tr>
<tr>
<th>
elderly-hair-man-1319289
</th>
<td>
5.3.1
</td>
<td>
{}
</td>
<td>
[{‘label’: ‘right-eye’, ‘points’: [[490.910569105691, 175.71544715447155]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘left-eye’, ‘points’: [[548.6341463414634, 167.58536585365852]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘nose’, ‘points’: [[526.6829268292682, 201.73170731707316]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}]
</td>
<td>
elderly-hair-man-1319289.jpg
</td>
<td>
None
</td>
<td>
512
</td>
<td>
768
</td>
</tr>
<tr>
<th>
face-facial-expression-fashion-2592000
</th>
<td>
5.3.1
</td>
<td>
{}
</td>
<td>
[{‘label’: ‘left-eye’, ‘points’: [[301.45454545454544, 106.85561497326205]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘right-eye’, ‘points’: [[250.65240641711233, 115.94652406417114]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}, {‘label’: ‘nose’, ‘points’: [[272.0427807486631, 121.29411764705884]], ‘group_id’: None, ‘description’: ’‘, ’shape_type’: ‘point’, ‘flags’: {}}]
</td>
<td>
face-facial-expression-fashion-2592000.jpg
</td>
<td>
None
</td>
<td>
672
</td>
<td>
512
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inspecting-the-class-distribution" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-class-distribution">Inspecting the Class Distribution</h3>
<p>Now that we have the annotation data, we can extract the unique class names and inspect the class distribution. A balanced dataset (where each class has approximately the same number of instances) is ideal for training a machine-learning model.</p>
<section id="get-image-classes" class="level4">
<h4 class="anchored" data-anchor-id="get-image-classes">Get image classes</h4>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explode the 'shapes' column in the annotation_df dataframe</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the pandas Series function to the 'shapes' column of the dataframe</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>shapes_df <span class="op">=</span> annotation_df[<span class="st">'shapes'</span>].explode().to_frame().shapes.<span class="bu">apply</span>(pd.Series)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of unique labels in the 'annotation_df' DataFrame</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> shapes_df[<span class="st">'label'</span>].unique().tolist()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display labels using a Pandas DataFrame</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(class_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
left-eye
</td>
</tr>
<tr>
<th>
1
</th>
<td>
nose
</td>
</tr>
<tr>
<th>
2
</th>
<td>
right-eye
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="visualize-the-class-distribution" class="level4">
<h4 class="anchored" data-anchor-id="visualize-the-class-distribution">Visualize the class distribution</h4>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the number of samples for each object class</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> shapes_df[<span class="st">'label'</span>].value_counts()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>class_counts.plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Class distribution'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Classes'</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(class_counts.index)), class_counts.index, rotation<span class="op">=</span><span class="dv">75</span>)  <span class="co"># Set the x-axis tick labels</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_26_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="visualizing-image-annotations" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-image-annotations">Visualizing Image Annotations</h3>
<p>In this section, we will annotate a single image with its bounding boxes using torchvision’s <a href="https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html"><code>BoundingBoxes</code></a> class and <a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html"><code>draw_bounding_boxes</code></a> function.</p>
<section id="generate-a-color-map" class="level4">
<h4 class="anchored" data-anchor-id="generate-a-color-map">Generate a color map</h4>
<p>First, we will generate a color map for the object classes.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a list of colors with a length equal to the number of labels</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> distinctipy.get_colors(<span class="bu">len</span>(class_names))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a copy of the color map in integer format</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>int_colors <span class="op">=</span> [<span class="bu">tuple</span>(<span class="bu">int</span>(c<span class="op">*</span><span class="dv">255</span>) <span class="cf">for</span> c <span class="kw">in</span> color) <span class="cf">for</span> color <span class="kw">in</span> colors]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a color swatch to visualize the color map</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>distinctipy.color_swatch(colors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_30_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="download-a-font-file" class="level4">
<h4 class="anchored" data-anchor-id="download-a-font-file">Download a font file</h4>
<p>The <a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html"><code>draw_bounding_boxes</code></a> function included with torchvision uses a pretty small font size. We can increase the font size if we use a custom font. Font files are available on sites like <a href="https://fonts.google.com/">Google Fonts</a>, or we can use one included with the operating system.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the name of the font file</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>font_file <span class="op">=</span> <span class="st">'KFOlCnqEu92Fr1MmEU9vAw.ttf'</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the font file</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>download_file(<span class="ss">f"https://fonts.gstatic.com/s/roboto/v30/</span><span class="sc">{</span>font_file<span class="sc">}</span><span class="ss">"</span>, <span class="st">"./"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="define-the-bounding-box-annotation-function" class="level4">
<h4 class="anchored" data-anchor-id="define-the-bounding-box-annotation-function">Define the bounding box annotation function</h4>
<p>We can make a partial function using <code>draw_bounding_boxes</code> since we’ll use the same box thickness and font each time we visualize bounding boxes.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>draw_bboxes <span class="op">=</span> partial(draw_bounding_boxes, fill<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">4</span>, font<span class="op">=</span>font_file, font_size<span class="op">=</span><span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="annotate-sample-image" class="level3">
<h3 class="anchored" data-anchor-id="annotate-sample-image">Annotate sample image</h3>
<p>Finally, we will open a sample image and annotate it with it’s associated bounding boxes.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the file ID of the first image file</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>file_id <span class="op">=</span> <span class="bu">list</span>(img_dict.keys())[<span class="dv">0</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the associated image file as a RGB image</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> Image.<span class="bu">open</span>(img_dict[file_id]).convert(<span class="st">'RGB'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the labels and bounding box annotations for the sample image</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [shape[<span class="st">'label'</span>] <span class="cf">for</span> shape <span class="kw">in</span> annotation_df.loc[file_id][<span class="st">'shapes'</span>]]</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>keypoints <span class="op">=</span> torch.tensor(np.array([shape[<span class="st">'points'</span>] <span class="cf">for</span> shape <span class="kw">in</span> annotation_df.loc[file_id][<span class="st">'shapes'</span>]])).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>BBOX_DIM <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>keypoints_bboxes <span class="op">=</span> torch.cat((keypoints, torch.ones(<span class="bu">len</span>(keypoints), <span class="dv">2</span>)<span class="op">*</span>BBOX_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the sample image with labels and bounding boxes</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(sample_img), </span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(keypoints_bboxes), <span class="st">'cxcywh'</span>, <span class="st">'xyxy'</span>),</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>labels, </span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels]]</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_36_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="loading-the-keypoint-r-cnn-model" class="level2">
<h2 class="anchored" data-anchor-id="loading-the-keypoint-r-cnn-model">Loading the Keypoint R-CNN Model</h2>
<p>TorchVision provides <a href="https://pytorch.org/vision/stable/models.html#table-of-all-available-keypoint-detection-weights">checkpoints</a> for the Keypoint R-CNN model trained on the <a href="https://cocodataset.org/">COCO</a> (Common Objects in Context) dataset. We can initialize a model with these pretrained weights using the <a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html?highlight=keypointrcnn_resnet50_fpn"><code>keypointrcnn_resnet50_fpn</code></a> function. We must then replace the keypoint predictor for the pretrained model with a new one for our dataset.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keypointrcnn_resnet50_fpn(weights<span class="op">=</span><span class="st">'DEFAULT'</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the classifier head with the number of keypoints</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>in_features <span class="op">=</span> model.roi_heads.keypoint_predictor.kps_score_lowres.in_channels</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>model.roi_heads.keypoint_predictor <span class="op">=</span> KeypointRCNNPredictor(in_channels<span class="op">=</span>in_features, num_keypoints<span class="op">=</span><span class="bu">len</span>(class_names))</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model's device and data type</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>model.to(device<span class="op">=</span>device, dtype<span class="op">=</span>dtype)<span class="op">;</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add attributes to store the device and model name for later reference</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>model.device <span class="op">=</span> device</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>model.name <span class="op">=</span> <span class="st">'keypointrcnn_resnet50_fpn'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model internally normalizes input using the mean and standard deviation values used during the pretraining process, so we do not need to keep track of them separately.</p>
<section id="summarizing-the-model" class="level3">
<h3 class="anchored" data-anchor-id="summarizing-the-model">Summarizing the Model</h3>
<p>Before moving on, let’s generate a summary of our model to get an overview of its performance characteristics. We can use this to gauge the computational requirements for deploying the model.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input to the model</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>test_inp <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">256</span>, <span class="dv">256</span>).to(device)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a summary of the model as a Pandas DataFrame</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> markdown_to_pandas(<span class="ss">f"</span><span class="sc">{</span>get_module_summary(model.<span class="bu">eval</span>(), [test_inp])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the summary to only the model</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> summary_df[summary_df.index <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the column "Contains Uninitialized Parameters?"</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>summary_df.drop([<span class="st">'In size'</span>, <span class="st">'Out size'</span>, <span class="st">'Contains Uninitialized Parameters?'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Type
</th>
<th>
# Parameters
</th>
<th>
# Trainable Parameters
</th>
<th>
Size (bytes)
</th>
<th>
Forward FLOPs
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
KeypointRCNN
</td>
<td>
59.0 M
</td>
<td>
58.8 M
</td>
<td>
236 M
</td>
<td>
144 G
</td>
</tr>
</tbody>
</table>
</div>
<p>The above table shows the model has approximately <code>58.8</code> million trainable parameters. It takes up <code>263</code> Megabytes and performs around <code>144</code> billion floating point operations for a single <code>256x256</code> RGB image. This model internally resizes input images and executes the same number of floating point operations for different input resolutions.</p>
<p>That completes the model setup. In the next section, we will prepare our dataset for training.</p>
</section>
</section>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">Preparing the Data</h2>
<p>The data preparation involves several steps, such as applying data augmentation techniques, setting up the train-validation split for the dataset, resizing and padding the images, defining the training dataset class, and initializing DataLoaders to feed data to the model.</p>
<section id="training-validation-split" class="level3">
<h3 class="anchored" data-anchor-id="training-validation-split">Training-Validation Split</h3>
<p>Let’s begin by defining the training-validation split. We’ll randomly select 90% of the available samples for the training set and use the remaining 10% for the validation set.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the list of image IDs</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>img_keys <span class="op">=</span> <span class="bu">list</span>(img_dict.keys())</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the image IDs</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>random.shuffle(img_keys)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the percentage of the images that should be used for training</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>train_pct <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>val_pct <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the index at which to split the subset of image paths into training and validation sets</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>train_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_keys)<span class="op">*</span>train_pct)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>val_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_keys)<span class="op">*</span>(train_pct<span class="op">+</span>val_pct))</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the subset of image paths into training and validation sets</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>train_keys <span class="op">=</span> img_keys[:train_split]</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>val_keys <span class="op">=</span> img_keys[train_split:]</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of images in the training and validation sets</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Samples:"</span>: <span class="bu">len</span>(train_keys),</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Validation Samples:"</span>: <span class="bu">len</span>(val_keys)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_39a47">
<thead>
</thead>
<tbody>
<tr>
<th id="T_39a47_level0_row0" class="row_heading level0 row0">
Training Samples:
</th>
<td id="T_39a47_row0_col0" class="data row0 col0">
180
</td>
</tr>
<tr>
<th id="T_39a47_level0_row1" class="row_heading level0 row1">
Validation Samples:
</th>
<td id="T_39a47_row1_col0" class="data row1 col0">
20
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>Here, we will define some data augmentations to apply to images during training. I created a few custom image transforms to help streamline the code.</p>
<p>The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#customrandomioucrop">first</a> extends the <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomIoUCrop.html#torchvision.transforms.v2.RandomIoUCrop"><code>RandomIoUCrop</code></a> transform included with torchvision to give the user more control over how much it crops into bounding box areas. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax">second</a> resizes images based on their largest dimension rather than their smallest. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare">third</a> applies square padding and allows the padding to be applied equally on both sides or randomly split between the two sides.</p>
<p>All three are available through the <a href="https://cj-mills.github.io/cjm-torchvision-tfms/"><code>cjm-torchvision-tfms</code></a> package.</p>
<section id="set-training-image-size" class="level4">
<h4 class="anchored" data-anchor-id="set-training-image-size">Set training image size</h4>
<p>First, we will specify the image size to use during training.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set training image size</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>train_sz <span class="op">=</span> <span class="dv">512</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initialize-custom-transforms" class="level4">
<h4 class="anchored" data-anchor-id="initialize-custom-transforms">Initialize custom transforms</h4>
<p>Next, we can initialize the transform objects.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RandomIoUCrop object</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>iou_crop <span class="op">=</span> CustomRandomIoUCrop(min_scale<span class="op">=</span><span class="fl">0.3</span>, </span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                               max_scale<span class="op">=</span><span class="fl">1.0</span>, </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>                               min_aspect_ratio<span class="op">=</span><span class="fl">0.5</span>, </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>                               max_aspect_ratio<span class="op">=</span><span class="fl">2.0</span>, </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>                               sampler_options<span class="op">=</span>[<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>, <span class="fl">1.0</span>],</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>                               trials<span class="op">=</span><span class="dv">400</span>, </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>                               jitter_factor<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `ResizeMax` object</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>resize_max <span class="op">=</span> ResizeMax(max_sz<span class="op">=</span>train_sz)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `PadSquare` object</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>pad_square <span class="op">=</span> PadSquare(shift<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="test-the-transforms" class="level4">
<h4 class="anchored" data-anchor-id="test-the-transforms">Test the transforms</h4>
<p>Torchvision’s V2 image transforms take an image and a <code>targets</code> dictionary. The <code>targets</code> dictionary contains the annotations and labels for the image.</p>
<p>We will pass input through the <code>CustomRandomIoUCrop</code> transform first and then through <code>ResizeMax</code> and <code>PadSquare</code>. We can pass the result through a final resize operation to ensure both sides match the <code>train_sz</code> value.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Always use the <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.SanitizeBoundingBoxes.html#torchvision.transforms.v2.SanitizeBoundingBoxes"><code>SanitizeBoundingBoxes</code></a> transform to clean up annotations after using data augmentations that alter bounding boxes (e.g., cropping, warping, etc.).</p>
</div>
</div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the labels for the sample</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [shape[<span class="st">'label'</span>] <span class="cf">for</span> shape <span class="kw">in</span> annotation_df.loc[file_id][<span class="st">'shapes'</span>]]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare bounding box targets</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> {<span class="st">'boxes'</span>: BoundingBoxes(torchvision.ops.box_convert(keypoints_bboxes, <span class="st">'cxcywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                                <span class="bu">format</span><span class="op">=</span><span class="st">'xyxy'</span>, </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>                                canvas_size<span class="op">=</span>sample_img.size[::<span class="op">-</span><span class="dv">1</span>]), </span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>           <span class="st">'labels'</span>: torch.Tensor([class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels])}</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop the image</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>cropped_img, targets <span class="op">=</span> iou_crop(sample_img, targets)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the image</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>resized_img, targets <span class="op">=</span> resize_max(cropped_img, targets)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad the image</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>padded_img, targets <span class="op">=</span> pad_square(resized_img, targets)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the padded image is the target size</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>resize <span class="op">=</span> transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>resized_padded_img, targets <span class="op">=</span> resize(padded_img, targets)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>sanitized_img, targets <span class="op">=</span> transforms.SanitizeBoundingBoxes()(resized_padded_img, targets)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Get colors for dataset sample</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>sample_colors <span class="op">=</span> [int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels]]</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the augmented image with updated labels and bounding boxes</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(sanitized_img), </span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>targets[<span class="st">'boxes'</span>], </span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[class_names[<span class="bu">int</span>(label.item())] <span class="cf">for</span> label <span class="kw">in</span> targets[<span class="st">'labels'</span>]], </span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>sample_colors,</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the annotated image</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>display(tensor_to_pil(annotated_tensor))</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Source Image:"</span>: sample_img.size,</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cropped Image:"</span>: cropped_img.size,</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Image:"</span>: resized_img.size,</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Padded Image:"</span>: padded_img.size,</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Padded Image:"</span>: resized_padded_img.size,</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_52_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_5be68">
<thead>
</thead>
<tbody>
<tr>
<th id="T_5be68_level0_row0" class="row_heading level0 row0">
Source Image:
</th>
<td id="T_5be68_row0_col0" class="data row0 col0">
(512, 768)
</td>
</tr>
<tr>
<th id="T_5be68_level0_row1" class="row_heading level0 row1">
Cropped Image:
</th>
<td id="T_5be68_row1_col0" class="data row1 col0">
(512, 768)
</td>
</tr>
<tr>
<th id="T_5be68_level0_row2" class="row_heading level0 row2">
Resized Image:
</th>
<td id="T_5be68_row2_col0" class="data row2 col0">
(341, 511)
</td>
</tr>
<tr>
<th id="T_5be68_level0_row3" class="row_heading level0 row3">
Padded Image:
</th>
<td id="T_5be68_row3_col0" class="data row3 col0">
(511, 511)
</td>
</tr>
<tr>
<th id="T_5be68_level0_row4" class="row_heading level0 row4">
Resized Padded Image:
</th>
<td id="T_5be68_row4_col0" class="data row4 col0">
(512, 512)
</td>
</tr>
</tbody>
</table>
</div>
<p>Now that we know how to apply data augmentations, we can put all the steps we’ve covered into a custom Dataset class.</p>
</section>
</section>
<section id="training-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="training-dataset-class">Training Dataset Class</h3>
<p>The following custom Dataset class is responsible for loading a single image, preparing the associated annotations, applying any image transforms, and returning the final <code>image</code> tensor and its <code>target</code> dictionary during training.</p>
<p>We will be applying the <code>SanitizeBoundingBoxes</code> transform here as well. This transform can remove key points if a previous transform moves them outside the image dimensions. The Keypoint R-CNN model still expects values for key points even when not visible, so we will fill the target annotations with dummy values as needed.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LabelMeKeypointDataset(Dataset):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A PyTorch Dataset class for handling LabelMe image keypoints.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This class extends PyTorch's Dataset and is designed to work with image data and</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">    associated keypoints annotations. It supports loading images and corresponding</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">    keypoints annotations, and applying transformations.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">        img_keys (list): List of image keys.</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        annotation_df (DataFrame): DataFrame containing annotations for each image.</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">        img_dict (dict): Dictionary mapping image keys to their file paths.</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co">        class_to_idx (dict): Dictionary mapping class names to class indices.</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co">        transforms (callable, optional): Transformations to be applied to the images and targets.</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_keys, annotation_df, img_dict, class_to_idx, transforms<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes the LabelMeKeypointDataset with image keys, annotations, and other relevant information.</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co">            img_keys (list): List of image keys.</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="co">            annotation_df (DataFrame): DataFrame containing annotations for each image.</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="co">            img_dict (dict): Dictionary mapping image keys to their file paths.</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="co">            class_to_idx (dict): Dictionary mapping class names to class indices.</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="co">            transforms (callable, optional): Transformations to be applied to the images and targets.</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Dataset, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_keys <span class="op">=</span> img_keys</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._annotation_df <span class="op">=</span> annotation_df</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_dict <span class="op">=</span> img_dict</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_to_idx <span class="op">=</span> class_to_idx</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._transforms <span class="op">=</span> transforms</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sanitize_bboxes <span class="op">=</span> torchvision.transforms.v2.SanitizeBoundingBoxes()</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.BBOX_DIM <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.DUMMY_VALUE <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the number of items in the dataset.</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a><span class="co">            int: Number of items in the dataset.</span></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._img_keys)</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Retrieves an item from the dataset at the specified index.</span></span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a><span class="co">            index (int): Index of the item to retrieve.</span></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a><span class="co">            tuple: A tuple containing the image and its corresponding target (annotations).</span></span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>        img_key <span class="op">=</span> <span class="va">self</span>._img_keys[index]</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a>        annotation <span class="op">=</span> <span class="va">self</span>._annotation_df.loc[img_key]</span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a>        image, target <span class="op">=</span> <span class="va">self</span>._load_image_and_target(annotation)</span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Applying transformations if specified</span></span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._transforms:</span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>            image, target <span class="op">=</span> <span class="va">self</span>._transforms(image, target)</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill any missing keypoints with dummy values</span></span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> <span class="va">self</span>._fill_and_order_target(target)</span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, target</span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> order_points_by_labels(<span class="va">self</span>, data, label_order):</span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Extracts and orders points from a list of dictionaries based on a given order of labels.</span></span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a><span class="co">        :param data: List of dictionaries containing labels and points.</span></span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a><span class="co">        :param label_order: List of labels in the desired order.</span></span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a><span class="co">        :return: List of points in the specified label order.</span></span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-79"><a href="#cb31-79" aria-hidden="true" tabindex="-1"></a>        ordered_points <span class="op">=</span> []</span>
<span id="cb31-80"><a href="#cb31-80" aria-hidden="true" tabindex="-1"></a>        label_to_points <span class="op">=</span> {item[<span class="st">'label'</span>]: item[<span class="st">'points'</span>] <span class="cf">for</span> item <span class="kw">in</span> data}</span>
<span id="cb31-81"><a href="#cb31-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-82"><a href="#cb31-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> label <span class="kw">in</span> label_order:</span>
<span id="cb31-83"><a href="#cb31-83" aria-hidden="true" tabindex="-1"></a>            points <span class="op">=</span> label_to_points.get(label)</span>
<span id="cb31-84"><a href="#cb31-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> points <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-85"><a href="#cb31-85" aria-hidden="true" tabindex="-1"></a>                ordered_points.extend(points)</span>
<span id="cb31-86"><a href="#cb31-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-87"><a href="#cb31-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ordered_points</span>
<span id="cb31-88"><a href="#cb31-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-89"><a href="#cb31-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _load_image_and_target(<span class="va">self</span>, annotation):</span>
<span id="cb31-90"><a href="#cb31-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-91"><a href="#cb31-91" aria-hidden="true" tabindex="-1"></a><span class="co">        Loads an image and its corresponding target (annotations) based on the provided annotation.</span></span>
<span id="cb31-92"><a href="#cb31-92" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb31-93"><a href="#cb31-93" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-94"><a href="#cb31-94" aria-hidden="true" tabindex="-1"></a><span class="co">            annotation (DataFrame row): Annotation data for a specific image.</span></span>
<span id="cb31-95"><a href="#cb31-95" aria-hidden="true" tabindex="-1"></a><span class="co">            Returns:</span></span>
<span id="cb31-96"><a href="#cb31-96" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: A tuple containing the loaded image and its corresponding target data.</span></span>
<span id="cb31-97"><a href="#cb31-97" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-98"><a href="#cb31-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the image from the file path specified in the annotations</span></span>
<span id="cb31-99"><a href="#cb31-99" aria-hidden="true" tabindex="-1"></a>        filepath <span class="op">=</span> <span class="va">self</span>._img_dict[annotation.name]</span>
<span id="cb31-100"><a href="#cb31-100" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(filepath).convert(<span class="st">'RGB'</span>)</span>
<span id="cb31-101"><a href="#cb31-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-102"><a href="#cb31-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extracting keypoints from the annotation and converting them to a tensor</span></span>
<span id="cb31-103"><a href="#cb31-103" aria-hidden="true" tabindex="-1"></a>        keypoints <span class="op">=</span> <span class="va">self</span>.order_points_by_labels(annotation[<span class="st">'shapes'</span>], <span class="va">self</span>._class_to_idx.keys())</span>
<span id="cb31-104"><a href="#cb31-104" aria-hidden="true" tabindex="-1"></a>        keypoints <span class="op">=</span> torch.tensor(np.array(keypoints, dtype<span class="op">=</span>np.float32)).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb31-105"><a href="#cb31-105" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-106"><a href="#cb31-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adding an offset to create bounding boxes around keypoints</span></span>
<span id="cb31-107"><a href="#cb31-107" aria-hidden="true" tabindex="-1"></a>        keypoints_bboxes <span class="op">=</span> torch.cat((keypoints, torch.ones(<span class="bu">len</span>(keypoints), <span class="dv">2</span>) <span class="op">*</span> <span class="va">self</span>.BBOX_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-108"><a href="#cb31-108" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb31-109"><a href="#cb31-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert bounding box format and create a BoundingBoxes object</span></span>
<span id="cb31-110"><a href="#cb31-110" aria-hidden="true" tabindex="-1"></a>        bbox_tensor <span class="op">=</span> torchvision.ops.box_convert(keypoints_bboxes, <span class="st">'cxcywh'</span>, <span class="st">'xyxy'</span>)</span>
<span id="cb31-111"><a href="#cb31-111" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> BoundingBoxes(bbox_tensor, <span class="bu">format</span><span class="op">=</span><span class="st">'xyxy'</span>, canvas_size<span class="op">=</span>image.size[::<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb31-112"><a href="#cb31-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-113"><a href="#cb31-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create tensor for labels based on the class indices</span></span>
<span id="cb31-114"><a href="#cb31-114" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.Tensor([<span class="va">self</span>._class_to_idx[label] <span class="cf">for</span> label <span class="kw">in</span> <span class="va">self</span>._class_to_idx.keys()])</span>
<span id="cb31-115"><a href="#cb31-115" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-116"><a href="#cb31-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, {<span class="st">'boxes'</span>: boxes, <span class="st">'labels'</span>: labels}</span>
<span id="cb31-117"><a href="#cb31-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-118"><a href="#cb31-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _fill_and_order_target(<span class="va">self</span>, target):</span>
<span id="cb31-119"><a href="#cb31-119" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-120"><a href="#cb31-120" aria-hidden="true" tabindex="-1"></a><span class="co">        Fills and orders the target bounding boxes and labels based on the class index.</span></span>
<span id="cb31-121"><a href="#cb31-121" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb31-122"><a href="#cb31-122" aria-hidden="true" tabindex="-1"></a><span class="co">        This method ensures that each target has a bounding box and label for each class,</span></span>
<span id="cb31-123"><a href="#cb31-123" aria-hidden="true" tabindex="-1"></a><span class="co">        even if some classes are not present in the original target. Missing classes</span></span>
<span id="cb31-124"><a href="#cb31-124" aria-hidden="true" tabindex="-1"></a><span class="co">        are filled with dummy values.</span></span>
<span id="cb31-125"><a href="#cb31-125" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb31-126"><a href="#cb31-126" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-127"><a href="#cb31-127" aria-hidden="true" tabindex="-1"></a><span class="co">            target (dict): A dictionary containing 'boxes' and 'labels' keys, where</span></span>
<span id="cb31-128"><a href="#cb31-128" aria-hidden="true" tabindex="-1"></a><span class="co">                           'boxes' is a tensor of bounding boxes and 'labels' is a tensor</span></span>
<span id="cb31-129"><a href="#cb31-129" aria-hidden="true" tabindex="-1"></a><span class="co">                           of labels corresponding to these boxes.</span></span>
<span id="cb31-130"><a href="#cb31-130" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb31-131"><a href="#cb31-131" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-132"><a href="#cb31-132" aria-hidden="true" tabindex="-1"></a><span class="co">            dict: The updated target dictionary with boxes and labels ordered and filled</span></span>
<span id="cb31-133"><a href="#cb31-133" aria-hidden="true" tabindex="-1"></a><span class="co">                  according to the class index.</span></span>
<span id="cb31-134"><a href="#cb31-134" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-135"><a href="#cb31-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-136"><a href="#cb31-136" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize new boxes with dummy values for each class</span></span>
<span id="cb31-137"><a href="#cb31-137" aria-hidden="true" tabindex="-1"></a>        new_boxes <span class="op">=</span> torch.full((<span class="bu">len</span>(<span class="va">self</span>._class_to_idx), <span class="dv">4</span>), <span class="va">self</span>.DUMMY_VALUE)</span>
<span id="cb31-138"><a href="#cb31-138" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepare labels tensor based on the class indices</span></span>
<span id="cb31-139"><a href="#cb31-139" aria-hidden="true" tabindex="-1"></a>        new_labels <span class="op">=</span> torch.tensor(<span class="bu">list</span>(<span class="va">self</span>._class_to_idx.values()), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb31-140"><a href="#cb31-140" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-141"><a href="#cb31-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Iterate over each class label</span></span>
<span id="cb31-142"><a href="#cb31-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(new_labels):</span>
<span id="cb31-143"><a href="#cb31-143" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check if the current label exists in the target's labels</span></span>
<span id="cb31-144"><a href="#cb31-144" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> label <span class="kw">in</span> target[<span class="st">'labels'</span>]:</span>
<span id="cb31-145"><a href="#cb31-145" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Find the index of the current label in the target's labels</span></span>
<span id="cb31-146"><a href="#cb31-146" aria-hidden="true" tabindex="-1"></a>                idx <span class="op">=</span> (target[<span class="st">'labels'</span>] <span class="op">==</span> label).nonzero(as_tuple<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb31-147"><a href="#cb31-147" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Assign the corresponding box to the new boxes tensor</span></span>
<span id="cb31-148"><a href="#cb31-148" aria-hidden="true" tabindex="-1"></a>                new_boxes[i] <span class="op">=</span> target[<span class="st">'boxes'</span>][idx]</span>
<span id="cb31-149"><a href="#cb31-149" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-150"><a href="#cb31-150" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the target dictionary with the new boxes and labels</span></span>
<span id="cb31-151"><a href="#cb31-151" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">'boxes'</span>] <span class="op">=</span> new_boxes</span>
<span id="cb31-152"><a href="#cb31-152" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">'labels'</span>] <span class="op">=</span> new_labels</span>
<span id="cb31-153"><a href="#cb31-153" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-154"><a href="#cb31-154" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> target</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-transforms" class="level3">
<h3 class="anchored" data-anchor-id="image-transforms">Image Transforms</h3>
<p>Here, we will specify and organize all the image transforms to apply during training.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms for data augmentation</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>data_aug_tfms <span class="op">=</span> transforms.Compose(</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    transforms<span class="op">=</span>[</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>        transforms.ColorJitter(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                brightness <span class="op">=</span> (<span class="fl">0.8</span>, <span class="fl">1.125</span>),</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>                contrast <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">1.5</span>),</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>                saturation <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">1.5</span>),</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                hue <span class="op">=</span> (<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">0.05</span>),</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        transforms.RandomGrayscale(),</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        transforms.RandomEqualize(),</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        RandomPixelCopy(max_pct<span class="op">=</span><span class="fl">0.025</span>),</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        transforms.RandomPerspective(distortion_scale<span class="op">=</span><span class="fl">0.15</span>, p<span class="op">=</span><span class="fl">0.5</span>, fill<span class="op">=</span>(<span class="dv">123</span>, <span class="dv">117</span>, <span class="dv">104</span>)),</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        transforms.RandomRotation(degrees<span class="op">=</span><span class="dv">90</span>, fill<span class="op">=</span>(<span class="dv">123</span>, <span class="dv">117</span>, <span class="dv">104</span>)),</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        iou_crop,</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to resize and pad input images</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>resize_pad_tfm <span class="op">=</span> transforms.Compose([</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    resize_max, </span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    pad_square,</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to sanitize bounding boxes and normalize input data</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>final_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>    transforms.ToImage(), </span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    transforms.SanitizeBoundingBoxes(),</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the transformations for training and validation datasets</span></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>train_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    data_aug_tfms, </span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    resize_pad_tfm, </span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    final_tfms</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>valid_tfms <span class="op">=</span> transforms.Compose([resize_pad_tfm, final_tfms])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initialize-datasets" class="level3">
<h3 class="anchored" data-anchor-id="initialize-datasets">Initialize Datasets</h3>
<p>Now, we can create the dataset objects for the training and validation sets using the image dictionary, the annotation DataFrame, and the image transforms.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mapping from class names to class indices</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>class_to_idx <span class="op">=</span> {c: i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(class_names)}</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the dataset using the defined transformations</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> LabelMeKeypointDataset(train_keys, annotation_df, img_dict, class_to_idx, train_tfms)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> LabelMeKeypointDataset(val_keys, annotation_df, img_dict, class_to_idx, valid_tfms)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of samples in the training and validation datasets</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Training dataset size:'</span>: <span class="bu">len</span>(train_dataset),</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Validation dataset size:'</span>: <span class="bu">len</span>(valid_dataset)}</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_408f0">
<thead>
</thead>
<tbody>
<tr>
<th id="T_408f0_level0_row0" class="row_heading level0 row0">
Training dataset size:
</th>
<td id="T_408f0_row0_col0" class="data row0 col0">
180
</td>
</tr>
<tr>
<th id="T_408f0_level0_row1" class="row_heading level0 row1">
Validation dataset size:
</th>
<td id="T_408f0_row1_col0" class="data row1 col0">
20
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inspect-samples" class="level3">
<h3 class="anchored" data-anchor-id="inspect-samples">Inspect Samples</h3>
<p>Let’s verify the dataset objects work correctly by inspecting the first samples from the training and validation sets.</p>
<section id="inspect-training-set-sample" class="level4">
<h4 class="anchored" data-anchor-id="inspect-training-set-sample">Inspect training set sample</h4>
<p>Since our custom dataset fills missing annotations with dummy values, we will pass the target dictionary through the <code>SanitizeBoundingBoxes</code> function again.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a sample image and its target annotations</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>dataset_sample <span class="op">=</span> train_dataset[<span class="dv">0</span>]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sanitize bounding boxes to remove dummy values</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> dataset_sample[<span class="dv">1</span>]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>targets[<span class="st">'boxes'</span>] <span class="op">=</span> BoundingBoxes(targets[<span class="st">'boxes'</span>], <span class="bu">format</span><span class="op">=</span><span class="st">'xyxy'</span>, canvas_size<span class="op">=</span>dataset_sample[<span class="dv">0</span>].shape[<span class="dv">1</span>:])</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>sanitized_image, sanitized_targets <span class="op">=</span> transforms.SanitizeBoundingBoxes()(dataset_sample[<span class="dv">0</span>], targets)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the sample image with the sanitized annotations</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>(sanitized_image<span class="op">*</span><span class="dv">255</span>).to(dtype<span class="op">=</span>torch.uint8), </span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>sanitized_targets[<span class="st">'boxes'</span>], </span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[class_names[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> sanitized_targets[<span class="st">'labels'</span>]], </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> sanitized_targets[<span class="st">'labels'</span>]]</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_62_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="inspect-validation-set-sample" class="level4">
<h4 class="anchored" data-anchor-id="inspect-validation-set-sample">Inspect validation set sample</h4>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dataset_sample <span class="op">=</span> valid_dataset[<span class="dv">0</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>(dataset_sample[<span class="dv">0</span>]<span class="op">*</span><span class="dv">255</span>).to(dtype<span class="op">=</span>torch.uint8), </span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>dataset_sample[<span class="dv">1</span>][<span class="st">'boxes'</span>], </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[class_names[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]], </span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[<span class="bu">int</span>(i.item())] <span class="cf">for</span> i <span class="kw">in</span> dataset_sample[<span class="dv">1</span>][<span class="st">'labels'</span>]]</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>tensor_to_pil(annotated_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_64_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="initialize-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="initialize-dataloaders">Initialize DataLoaders</h3>
<p>The last step before training is to instantiate the DataLoaders for the training and validation sets. Try decreasing the batch size if you encounter memory limitations.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the training batch size</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of worker processes for loading data. This should be the number of CPUs available.</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>num_workers <span class="op">=</span> multiprocessing.cpu_count()</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters for DataLoader</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>data_loader_params <span class="op">=</span> {</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>: bs,  <span class="co"># Batch size for data loading</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num_workers'</span>: num_workers,  <span class="co"># Number of subprocesses to use for data loading</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'persistent_workers'</span>: <span class="va">True</span>,  <span class="co"># If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory'</span>: <span class="st">'cuda'</span> <span class="kw">in</span> device,  <span class="co"># If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory_device'</span>: device <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">''</span>,  <span class="co"># Specifies the device where the data should be loaded. Commonly set to use the GPU.</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'collate_fn'</span>: <span class="kw">lambda</span> batch: <span class="bu">tuple</span>(<span class="bu">zip</span>(<span class="op">*</span>batch)),</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for training data. Data is shuffled for every epoch.</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, <span class="op">**</span>data_loader_params, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for validation data. Shuffling is not necessary for validation data.</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>valid_dataloader <span class="op">=</span> DataLoader(valid_dataset, <span class="op">**</span>data_loader_params)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of batches in the training and validation DataLoaders</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of batches in train DataLoader: </span><span class="sc">{</span><span class="bu">len</span>(train_dataloader)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of batches in validation DataLoader: </span><span class="sc">{</span><span class="bu">len</span>(valid_dataloader)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Number of batches in train DataLoader: 45
Number of batches in validation DataLoader: 5</code></pre>
</section>
</section>
<section id="fine-tuning-the-model" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-the-model">Fine-tuning the Model</h2>
<p>In this section, we will implement the training code and fine-tune our model.</p>
<section id="define-utility-functions" class="level3">
<h3 class="anchored" data-anchor-id="define-utility-functions">Define Utility Functions</h3>
<p>First, we need to define a couple of utility functions.</p>
<section id="define-a-function-to-create-a-bounding-box-that-encapsulates-the-key-points" class="level4">
<h4 class="anchored" data-anchor-id="define-a-function-to-create-a-bounding-box-that-encapsulates-the-key-points">Define a function to create a bounding box that encapsulates the key points</h4>
<p>The Keypoint R-CNN model expects a bounding box encapsulating the points associated with a given person/object. We could include these bounding box annotations in our dataset (e.g., have bounding boxes around each face). However, dynamically making one large enough to contain the key points will suffice.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> keypoints_to_bbox(keypoints, offset<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert a tensor of keypoint coordinates to a bounding box.</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">    keypoints (Tensor): A tensor of shape (N, 2), where N is the number of keypoints.</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor: A tensor representing the bounding box [xmin, ymin, xmax, ymax].</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    x_coordinates, y_coordinates <span class="op">=</span> keypoints[:, <span class="dv">0</span>], keypoints[:, <span class="dv">1</span>]</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    xmin <span class="op">=</span> torch.<span class="bu">min</span>(x_coordinates)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    ymin <span class="op">=</span> torch.<span class="bu">min</span>(y_coordinates)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    xmax <span class="op">=</span> torch.<span class="bu">max</span>(x_coordinates)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    ymax <span class="op">=</span> torch.<span class="bu">max</span>(y_coordinates)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    bbox <span class="op">=</span> torch.tensor([xmin<span class="op">-</span>offset, ymin<span class="op">-</span>offset, xmax<span class="op">+</span>offset, ymax<span class="op">+</span>offset])</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bbox</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="define-a-conditional-autocast-context-manager" class="level4">
<h4 class="anchored" data-anchor-id="define-a-conditional-autocast-context-manager">Define a conditional <code>autocast</code> context manager</h4>
<p>The autocast context manager that handles mixed-precision training on CPUs does not fully support the Keypoint R-CNN model. Therefore, we will only use mixed-precision training when not using the CPU.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="at">@contextmanager</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conditional_autocast(device):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co">    A context manager for conditional automatic mixed precision (AMP).</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This context manager applies automatic mixed precision for operations if the</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co">    specified device is not a CPU. It's a no-op (does nothing) if the device is a CPU.</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Mixed precision can speed up computations and reduce memory usage on compatible</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">    hardware, primarily GPUs.</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="co">    device (str): The device type, e.g., 'cuda' or 'cpu', which determines whether</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co">                  autocasting is applied.</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Yields:</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co">    None - This function does not return any value but enables the wrapped code</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co">           block to execute under the specified precision context.</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if the specified device is not a CPU</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'cpu'</span> <span class="kw">not</span> <span class="kw">in</span> device:</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the device is not a CPU, enable autocast for the specified device type.</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Autocast will automatically choose the precision (e.g., float16) for certain</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># operations to improve performance.</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> autocast(device_type<span class="op">=</span>device):</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the device is a CPU, autocast is not applied.</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This yields control back to the with-block with no changes.</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="define-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="define-the-training-loop">Define the Training Loop</h3>
<p>The following function performs a single pass through the training or validation set.</p>
<p>As mentioned earlier, the Keypoint R-CNN model expects values for key points even when not visible. We indicate which key points are visible, with a <code>1</code> for visible and a <code>0</code> for not.</p>
<p>The model has different behavior when in <code>training</code> mode versus <code>evaluation</code> mode. In training mode, it calculates the loss internally for the key point estimation task and returns a dictionary with the individual loss values. We can sum up these separate values to get the total loss.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to run a single training/validation epoch</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(model, dataloader, optimizer, lr_scheduler, device, scaler, epoch_id, is_training):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to run a single training or evaluation epoch.</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co">        model: A PyTorch model to train or evaluate.</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co">        dataloader: A PyTorch DataLoader providing the data.</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: The optimizer to use for training the model.</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_func: The loss function used for training.</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co">        device: The device (CPU or GPU) to run the model on.</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co">        scaler: Gradient scaler for mixed-precision training.</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co">        is_training: Boolean flag indicating whether the model is in training or evaluation mode.</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The average loss for the epoch.</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set model to training mode</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the average loss for the current epoch </span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize progress bar with total number of batches in the dataloader</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    progress_bar <span class="op">=</span> tqdm(total<span class="op">=</span><span class="bu">len</span>(dataloader), desc<span class="op">=</span><span class="st">"Train"</span> <span class="cf">if</span> is_training <span class="cf">else</span> <span class="st">"Eval"</span>)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over data batches</span></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_id, (inputs, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move inputs and targets to the specified device</span></span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> torch.stack(inputs).to(device)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract the ground truth bounding boxes and labels</span></span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>        gt_bboxes, gt_labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>[(d[<span class="st">'boxes'</span>].to(device), d[<span class="st">'labels'</span>].to(device)) <span class="cf">for</span> d <span class="kw">in</span> targets])</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert ground truth bounding boxes from 'xyxy' to 'cxcywh' format and only keep center coordinates</span></span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>        gt_keypoints <span class="op">=</span> torchvision.ops.box_convert(torch.stack(gt_bboxes), <span class="st">'xyxy'</span>, <span class="st">'cxcywh'</span>)[:,:,:<span class="dv">2</span>]</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize a visibility tensor with ones, indicating all keypoints are visible</span></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>        visibility <span class="op">=</span> torch.ones(<span class="bu">len</span>(inputs),gt_keypoints.shape[<span class="dv">1</span>],<span class="dv">1</span>).to(device)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a visibility mask based on whether the bounding boxes are valid (greater than or equal to 0)</span></span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>        visibility_mask <span class="op">=</span> (torch.stack(gt_bboxes) <span class="op">&gt;=</span> <span class="fl">0.</span>)[..., <span class="dv">0</span>].view(visibility.shape).to(device)</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate the keypoints with the visibility mask, adding a visibility channel to keypoints</span></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>        gt_keypoints_with_visibility <span class="op">=</span> torch.concat((</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>            gt_keypoints, </span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>            visibility<span class="op">*</span>visibility_mask</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>        ), dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert keypoints to bounding boxes for each input and move them to the specified device</span></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>        gt_object_bboxes <span class="op">=</span> torch.vstack([keypoints_to_bbox(keypoints) <span class="cf">for</span> keypoints <span class="kw">in</span> gt_keypoints]).to(device)</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize ground truth labels as tensor of ones and move them to the specified device</span></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>        gt_labels <span class="op">=</span> torch.ones(<span class="bu">len</span>(inputs), dtype<span class="op">=</span>torch.int64).to(device)</span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepare the targets for the Keypoint R-CNN model</span></span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This includes bounding boxes, labels, and keypoints with visibility for each input image</span></span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a>        keypoint_rcnn_targets <span class="op">=</span> [</span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'boxes'</span> : boxes[<span class="va">None</span>], <span class="st">'labels'</span>: labels[<span class="va">None</span>], <span class="st">'keypoints'</span>: keypoints[<span class="va">None</span>]}</span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> boxes, labels, keypoints <span class="kw">in</span> <span class="bu">zip</span>(gt_object_bboxes, gt_labels, gt_keypoints_with_visibility)</span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass with Automatic Mixed Precision (AMP) context manager</span></span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> conditional_autocast(torch.device(device).<span class="bu">type</span>):</span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> is_training:</span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>                losses <span class="op">=</span> model(inputs.to(device), move_data_to_device(keypoint_rcnn_targets, device))</span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>                    losses <span class="op">=</span> model(inputs.to(device), move_data_to_device(keypoint_rcnn_targets, device))</span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the loss</span></span>
<span id="cb40-70"><a href="#cb40-70" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="bu">sum</span>([loss <span class="cf">for</span> loss <span class="kw">in</span> losses.values()])  <span class="co"># Sum up the losses</span></span>
<span id="cb40-71"><a href="#cb40-71" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb40-72"><a href="#cb40-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If in training mode</span></span>
<span id="cb40-73"><a href="#cb40-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb40-74"><a href="#cb40-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scaler:</span>
<span id="cb40-75"><a href="#cb40-75" aria-hidden="true" tabindex="-1"></a>                scaler.scale(loss).backward()</span>
<span id="cb40-76"><a href="#cb40-76" aria-hidden="true" tabindex="-1"></a>                scaler.step(optimizer)</span>
<span id="cb40-77"><a href="#cb40-77" aria-hidden="true" tabindex="-1"></a>                old_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb40-78"><a href="#cb40-78" aria-hidden="true" tabindex="-1"></a>                scaler.update()</span>
<span id="cb40-79"><a href="#cb40-79" aria-hidden="true" tabindex="-1"></a>                new_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb40-80"><a href="#cb40-80" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> new_scaler <span class="op">&gt;=</span> old_scaler:</span>
<span id="cb40-81"><a href="#cb40-81" aria-hidden="true" tabindex="-1"></a>                    lr_scheduler.step()</span>
<span id="cb40-82"><a href="#cb40-82" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb40-83"><a href="#cb40-83" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb40-84"><a href="#cb40-84" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb40-85"><a href="#cb40-85" aria-hidden="true" tabindex="-1"></a>                lr_scheduler.step()</span>
<span id="cb40-86"><a href="#cb40-86" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb40-87"><a href="#cb40-87" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb40-88"><a href="#cb40-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-89"><a href="#cb40-89" aria-hidden="true" tabindex="-1"></a>        loss_item <span class="op">=</span> loss.item()</span>
<span id="cb40-90"><a href="#cb40-90" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss_item</span>
<span id="cb40-91"><a href="#cb40-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update progress bar</span></span>
<span id="cb40-92"><a href="#cb40-92" aria-hidden="true" tabindex="-1"></a>        progress_bar.set_postfix(loss<span class="op">=</span>loss_item, </span>
<span id="cb40-93"><a href="#cb40-93" aria-hidden="true" tabindex="-1"></a>                                 avg_loss<span class="op">=</span>epoch_loss<span class="op">/</span>(batch_id<span class="op">+</span><span class="dv">1</span>), </span>
<span id="cb40-94"><a href="#cb40-94" aria-hidden="true" tabindex="-1"></a>                                 lr<span class="op">=</span>lr_scheduler.get_last_lr()[<span class="dv">0</span>] <span class="cf">if</span> is_training <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb40-95"><a href="#cb40-95" aria-hidden="true" tabindex="-1"></a>        progress_bar.update()</span>
<span id="cb40-96"><a href="#cb40-96" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-97"><a href="#cb40-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If loss is NaN or infinity, stop training</span></span>
<span id="cb40-98"><a href="#cb40-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb40-99"><a href="#cb40-99" aria-hidden="true" tabindex="-1"></a>            stop_training_message <span class="op">=</span> <span class="ss">f"Loss is NaN or infinite at epoch </span><span class="sc">{</span>epoch_id<span class="sc">}</span><span class="ss">, batch </span><span class="sc">{</span>batch_id<span class="sc">}</span><span class="ss">. Stopping training."</span></span>
<span id="cb40-100"><a href="#cb40-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> math.isnan(loss_item) <span class="kw">and</span> math.isfinite(loss_item), stop_training_message</span>
<span id="cb40-101"><a href="#cb40-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-102"><a href="#cb40-102" aria-hidden="true" tabindex="-1"></a>    progress_bar.close()</span>
<span id="cb40-103"><a href="#cb40-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> (batch_id <span class="op">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we define the <code>train_loop</code> function, which executes the main training loop. It iterates over each epoch, runs through the training and validation sets, and saves the best model based on the validation loss.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>               train_dataloader, </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>               valid_dataloader, </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>               optimizer,  </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>               lr_scheduler, </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>               device, </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>               epochs, </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>               checkpoint_path, </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>               use_scaler<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Main training loop.</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co">        model: A PyTorch model to train.</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co">        train_dataloader: A PyTorch DataLoader providing the training data.</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="co">        valid_dataloader: A PyTorch DataLoader providing the validation data.</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: The optimizer to use for training the model.</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co">        lr_scheduler: The learning rate scheduler.</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co">        device: The device (CPU or GPU) to run the model on.</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="co">        epochs: The number of epochs to train for.</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="co">        checkpoint_path: The path where to save the best model checkpoint.</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="co">        use_scaler: Whether to scale graidents when using a CUDA device</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a><span class="co">        None</span></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU</span></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> torch.cuda.amp.GradScaler() <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span> <span class="kw">and</span> use_scaler <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>    best_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)  <span class="co"># Initialize the best validation loss</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over the epochs</span></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs), desc<span class="op">=</span><span class="st">"Epochs"</span>):</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run a training epoch and get the training loss</span></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> run_epoch(model, train_dataloader, optimizer, lr_scheduler, device, scaler, epoch, is_training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run an evaluation epoch and get the validation loss</span></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>            valid_loss <span class="op">=</span> run_epoch(model, valid_dataloader, <span class="va">None</span>, <span class="va">None</span>, device, scaler, epoch, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the validation loss is lower than the best validation loss seen so far, save the model checkpoint</span></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> valid_loss <span class="op">&lt;</span> best_loss:</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>            best_loss <span class="op">=</span> valid_loss</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), checkpoint_path)</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Save metadata about the training process</span></span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>            training_metadata <span class="op">=</span> {</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>                <span class="st">'epoch'</span>: epoch,</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>                <span class="st">'train_loss'</span>: train_loss,</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>                <span class="st">'valid_loss'</span>: valid_loss, </span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>                <span class="st">'learning_rate'</span>: lr_scheduler.get_last_lr()[<span class="dv">0</span>],</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>                <span class="st">'model_architecture'</span>: model.name</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> <span class="bu">open</span>(Path(checkpoint_path.parent<span class="op">/</span><span class="st">'training_metadata.json'</span>), <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>                json.dump(training_metadata, f)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the device is a GPU, empty the cache</span></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">!=</span> <span class="st">'cpu'</span>:</span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">getattr</span>(torch, device.<span class="bu">type</span>).empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="set-the-model-checkpoint-path" class="level3">
<h3 class="anchored" data-anchor-id="set-the-model-checkpoint-path">Set the Model Checkpoint Path</h3>
<p>Before we proceed with training, let’s generate a timestamp for the training session and create a directory to save the checkpoints during training.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>timestamp <span class="op">=</span> datetime.datetime.now().strftime(<span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">_%H-%M-%S"</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a directory to store the checkpoints if it does not already exist</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>checkpoint_dir <span class="op">=</span> Path(project_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the checkpoint directory if it does not already exist</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>checkpoint_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The model checkpoint path</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>checkpoint_path <span class="op">=</span> checkpoint_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>model<span class="sc">.</span>name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(checkpoint_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>pytorch-keypoint-r-cnn/2024-01-28_17-07-09/keypointrcnn_resnet50_fpn.pth</code></pre>
<p>Let’s also save a copy of the colormap for the current dataset in the training folder for future use.</p>
</section>
<section id="save-the-color-map" class="level3">
<h3 class="anchored" data-anchor-id="save-the-color-map">Save the Color Map</h3>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a color map and write it to a JSON file</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>color_map <span class="op">=</span> {<span class="st">'items'</span>: [{<span class="st">'label'</span>: label, <span class="st">'color'</span>: color} <span class="cf">for</span> label, color <span class="kw">in</span> <span class="bu">zip</span>(class_names, colors)]}</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_path<span class="sc">.</span>name<span class="sc">}</span><span class="ss">-colormap.json"</span>, <span class="st">"w"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    json.dump(color_map, <span class="bu">file</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the name of the file that the color map was written to</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_path<span class="sc">.</span>name<span class="sc">}</span><span class="ss">-colormap.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>pytorch-keypoint-r-cnn/2024-01-28_17-07-09/labelme-keypoint-eyes-noses-dataset-colormap.json</code></pre>
</section>
<section id="configure-the-training-parameters" class="level3">
<h3 class="anchored" data-anchor-id="configure-the-training-parameters">Configure the Training Parameters</h3>
<p>Now, we can configure the parameters for training. We must specify the learning rate and number of training epochs. We will also instantiate the optimizer and learning rate scheduler.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate for the model</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">5e-4</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of training epochs</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">70</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW optimizer; includes weight decay for regularization</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate scheduler; adjusts the learning rate during training</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer, </span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>                                                   max_lr<span class="op">=</span>lr, </span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>                                                   total_steps<span class="op">=</span>epochs<span class="op">*</span><span class="bu">len</span>(train_dataloader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">Train the Model</h3>
<p>Finally, we can train the model using the <code>train_loop</code> function. Training time will depend on the available hardware.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Training usually takes around 30 minutes on the free GPU tier of Google Colab.</p>
</div>
</div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>train_loop(model<span class="op">=</span>model, </span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>           train_dataloader<span class="op">=</span>train_dataloader,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>           valid_dataloader<span class="op">=</span>valid_dataloader,</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>           optimizer<span class="op">=</span>optimizer, </span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>           lr_scheduler<span class="op">=</span>lr_scheduler, </span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>           device<span class="op">=</span>torch.device(device), </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>           epochs<span class="op">=</span>epochs, </span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>           checkpoint_path<span class="op">=</span>checkpoint_path,</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>           use_scaler<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled" title="Training Progress">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Training Progress
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code>Epochs: 100% |██████████| 70/70 [07:29&lt;00:00, 6.55s/it]
Train: 100% |██████████| 45/45 [00:07&lt;00:00, 8.58it/s, avg_loss=6.95, loss=6.07, lr=2.27e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 8.31it/s, avg_loss=5.17, loss=5.31, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.50it/s, avg_loss=5.42, loss=4.87, lr=3.07e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.40it/s, avg_loss=4.3, loss=4.14, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 7.09it/s, avg_loss=4.85, loss=4.88, lr=4.38e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.72it/s, avg_loss=4.54, loss=4.73, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.12it/s, avg_loss=4.55, loss=4.27, lr=6.18e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 13.73it/s, avg_loss=4.16, loss=3.78, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.77it/s, avg_loss=4.37, loss=4.64, lr=8.42e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.63it/s, avg_loss=3.79, loss=3.36, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.46it/s, avg_loss=4.53, loss=6.24, lr=0.000111]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.26it/s, avg_loss=3.81, loss=3.25, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.30it/s, avg_loss=4.39, loss=4.33, lr=0.00014]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.50it/s, avg_loss=3.93, loss=3.63, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.64it/s, avg_loss=4.2, loss=4.98, lr=0.000173]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.93it/s, avg_loss=3.85, loss=3.1, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.43it/s, avg_loss=4.37, loss=4.64, lr=0.000207]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.57it/s, avg_loss=4.49, loss=4.54, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.54it/s, avg_loss=4.26, loss=3.53, lr=0.000242]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.36it/s, avg_loss=4.11, loss=4.03, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.48it/s, avg_loss=4.38, loss=4.53, lr=0.000278]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.60it/s, avg_loss=4.34, loss=3.82, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.51it/s, avg_loss=4.58, loss=4.45, lr=0.000314]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.03it/s, avg_loss=4.42, loss=4.41, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.58it/s, avg_loss=4.47, loss=3.38, lr=0.000348]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.05it/s, avg_loss=4.24, loss=3.27, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.53it/s, avg_loss=4.44, loss=5.01, lr=0.00038]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.75it/s, avg_loss=4.22, loss=4.14, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.58it/s, avg_loss=4.54, loss=4.36, lr=0.00041]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.24it/s, avg_loss=4.02, loss=3.7, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.83it/s, avg_loss=4.55, loss=3.89, lr=0.000436]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.35it/s, avg_loss=4.04, loss=3.33, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.58it/s, avg_loss=4.57, loss=4.49, lr=0.000459]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.79it/s, avg_loss=4.68, loss=4.85, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.84it/s, avg_loss=4.57, loss=4.47, lr=0.000477]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.73it/s, avg_loss=3.98, loss=3.36, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.83it/s, avg_loss=4.4, loss=4.59, lr=0.00049]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.27it/s, avg_loss=4.11, loss=3.59, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.76it/s, avg_loss=4.59, loss=4.98, lr=0.000497]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.49it/s, avg_loss=3.98, loss=3.41, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.85it/s, avg_loss=4.35, loss=4.5, lr=0.0005]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.15it/s, avg_loss=4, loss=3.34, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.05it/s, avg_loss=4.6, loss=5.02, lr=0.000499]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 13.54it/s, avg_loss=4.14, loss=3.99, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.92it/s, avg_loss=4.5, loss=3.75, lr=0.000498]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.00it/s, avg_loss=4.38, loss=4.55, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.39it/s, avg_loss=4.25, loss=3.95, lr=0.000495]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.46it/s, avg_loss=3.72, loss=3.16, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.55it/s, avg_loss=4.26, loss=5.19, lr=0.000492]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.02it/s, avg_loss=4.54, loss=4.14, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.46it/s, avg_loss=4.15, loss=3.68, lr=0.000487]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.73it/s, avg_loss=3.94, loss=3.61, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.77it/s, avg_loss=4.3, loss=3.22, lr=0.000482]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.81it/s, avg_loss=3.71, loss=3.57, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.42it/s, avg_loss=4.08, loss=3.55, lr=0.000475]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.04it/s, avg_loss=3.88, loss=3.6, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.42it/s, avg_loss=4.18, loss=3.19, lr=0.000468]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.78it/s, avg_loss=3.84, loss=3.7, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.52it/s, avg_loss=4.09, loss=3.7, lr=0.000459]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.19it/s, avg_loss=3.91, loss=3.65, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.16it/s, avg_loss=3.93, loss=4.28, lr=0.00045]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.02it/s, avg_loss=3.8, loss=3.52, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.15it/s, avg_loss=4.04, loss=3.38, lr=0.00044]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.09it/s, avg_loss=3.88, loss=4.04, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.36it/s, avg_loss=4.1, loss=3.53, lr=0.000429]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.44it/s, avg_loss=3.7, loss=2.95, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.62it/s, avg_loss=4.05, loss=4.06, lr=0.000418]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.18it/s, avg_loss=3.78, loss=3.28, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.20it/s, avg_loss=3.95, loss=3.53, lr=0.000406]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.48it/s, avg_loss=3.44, loss=3.38, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.27it/s, avg_loss=3.86, loss=2.82, lr=0.000393]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.31it/s, avg_loss=3.63, loss=3, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.35it/s, avg_loss=3.97, loss=3.48, lr=0.000379]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.21it/s, avg_loss=3.62, loss=3.22, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.46it/s, avg_loss=3.72, loss=3.94, lr=0.000365]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.22it/s, avg_loss=3.45, loss=2.83, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.30it/s, avg_loss=3.75, loss=3.34, lr=0.000351]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.41it/s, avg_loss=3.52, loss=3.38, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.63it/s, avg_loss=3.7, loss=4.19, lr=0.000336]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.00it/s, avg_loss=3.56, loss=2.9, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.47it/s, avg_loss=3.65, loss=4.22, lr=0.000321]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.94it/s, avg_loss=3.67, loss=3.11, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.50it/s, avg_loss=3.58, loss=4.13, lr=0.000305]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.86it/s, avg_loss=3.55, loss=2.98, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.55it/s, avg_loss=3.54, loss=3.29, lr=0.00029]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.17it/s, avg_loss=3.42, loss=2.62, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.25it/s, avg_loss=3.51, loss=3.97, lr=0.000274]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.54it/s, avg_loss=3.33, loss=2.68, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.39it/s, avg_loss=3.5, loss=2.83, lr=0.000258]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.36it/s, avg_loss=3.27, loss=2.94, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.22it/s, avg_loss=3.45, loss=4.09, lr=0.000242]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.64it/s, avg_loss=3.63, loss=3.29, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.37it/s, avg_loss=3.44, loss=2.97, lr=0.000226]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 13.97it/s, avg_loss=3.44, loss=2.87, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.26it/s, avg_loss=3.35, loss=2.87, lr=0.00021]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.52it/s, avg_loss=3.35, loss=2.94, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.29it/s, avg_loss=3.32, loss=3.1, lr=0.000194]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.92it/s, avg_loss=3.58, loss=3.28, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.67it/s, avg_loss=3.21, loss=3.25, lr=0.000179]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.04it/s, avg_loss=3.36, loss=2.86, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.12it/s, avg_loss=3.29, loss=2.95, lr=0.000163]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.03it/s, avg_loss=3.36, loss=2.87, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.07it/s, avg_loss=3.21, loss=3.99, lr=0.000148]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.45it/s, avg_loss=3.32, loss=2.96, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.23it/s, avg_loss=3.21, loss=2.92, lr=0.000134]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.38it/s, avg_loss=3.15, loss=2.81, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.05it/s, avg_loss=3.13, loss=2.58, lr=0.00012]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.48it/s, avg_loss=3.39, loss=2.86, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.27it/s, avg_loss=3.07, loss=2.13, lr=0.000107]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.16it/s, avg_loss=3.15, loss=2.68, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 7.66it/s, avg_loss=3.12, loss=3.1, lr=9.39e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.06it/s, avg_loss=3.27, loss=2.85, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 7.56it/s, avg_loss=3.02, loss=3.05, lr=8.17e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.53it/s, avg_loss=3.24, loss=2.74, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.42it/s, avg_loss=2.99, loss=2.36, lr=7.02e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.08it/s, avg_loss=3.1, loss=2.56, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.24it/s, avg_loss=2.93, loss=2.53, lr=5.94e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.33it/s, avg_loss=3.21, loss=2.85, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.11it/s, avg_loss=2.98, loss=2.77, lr=4.94e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.40it/s, avg_loss=3.31, loss=2.95, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.16it/s, avg_loss=3.04, loss=3.37, lr=4.03e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.74it/s, avg_loss=3.15, loss=2.93, lr=]
Train: 100% |██████████| 45/45 [00:05&lt;00:00, 8.33it/s, avg_loss=3, loss=3.06, lr=3.2e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.16it/s, avg_loss=3.1, loss=2.8, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.36it/s, avg_loss=2.92, loss=2.94, lr=2.46e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.99it/s, avg_loss=3.23, loss=2.85, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.45it/s, avg_loss=2.86, loss=2.2, lr=1.81e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 15.14it/s, avg_loss=3.06, loss=2.78, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.53it/s, avg_loss=2.94, loss=2.69, lr=1.26e-5]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.66it/s, avg_loss=3.07, loss=2.53, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.29it/s, avg_loss=2.86, loss=2.94, lr=8.09e-6]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.19it/s, avg_loss=3.04, loss=2.48, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.56it/s, avg_loss=2.79, loss=2.45, lr=4.54e-6]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 13.92it/s, avg_loss=3.15, loss=2.65, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 7.95it/s, avg_loss=2.87, loss=2.57, lr=2.01e-6]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.27it/s, avg_loss=3.02, loss=2.29, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.31it/s, avg_loss=2.93, loss=2.63, lr=4.93e-7]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.89it/s, avg_loss=2.96, loss=2.65, lr=]
Train: 100% |██████████| 45/45 [00:06&lt;00:00, 8.32it/s, avg_loss=2.87, loss=2.75, lr=2.25e-9]
Eval: 100% |██████████| 5/5 [00:00&lt;00:00, 14.32it/s, avg_loss=3.07, loss=2.65, lr=]</code></pre>
</div>
</div>
</div>
<p>At last, we have our fine-tuned Keypoint R-CNN model. To wrap up the tutorial, we can test our model by performing inference on individual images.</p>
</section>
</section>
<section id="making-predictions-with-the-model" class="level2">
<h2 class="anchored" data-anchor-id="making-predictions-with-the-model">Making Predictions with the Model</h2>
<p>In this final part of the tutorial, we will cover how to perform inference on individual images with our Mask R-CNN model and filter the predictions.</p>
<section id="prepare-input-data" class="level3">
<h3 class="anchored" data-anchor-id="prepare-input-data">Prepare Input Data</h3>
<p>Let’s use an image from the validation set. That way, we have some ground truth annotation data to compare against. Unlike during training, we won’t stick to square input dimensions for inference.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a random item from the validation set</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>file_id <span class="op">=</span> val_keys[<span class="dv">0</span>]</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the image file path associated with the file ID</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>test_file <span class="op">=</span> img_dict[file_id]</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the test file</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(test_file).convert(<span class="st">'RGB'</span>)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> resize_img(test_img, target_sz<span class="op">=</span>train_sz, divisor<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the scale between the source image and the resized image</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>min_img_scale <span class="op">=</span> <span class="bu">min</span>(test_img.size) <span class="op">/</span> <span class="bu">min</span>(input_img.size)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>display(test_img)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas DataFrame for easy formatting</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Source Image Size:"</span>: test_img.size,</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Dims:"</span>: input_img.size,</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Min Image Scale:"</span>: min_img_scale,</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Input Image Size:"</span>: input_img.size</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_87_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_0ac3f">
<thead>
</thead>
<tbody>
<tr>
<th id="T_0ac3f_level0_row0" class="row_heading level0 row0">
Source Image Size:
</th>
<td id="T_0ac3f_row0_col0" class="data row0 col0">
(512, 768)
</td>
</tr>
<tr>
<th id="T_0ac3f_level0_row1" class="row_heading level0 row1">
Input Dims:
</th>
<td id="T_0ac3f_row1_col0" class="data row1 col0">
(512, 768)
</td>
</tr>
<tr>
<th id="T_0ac3f_level0_row2" class="row_heading level0 row2">
Min Image Scale:
</th>
<td id="T_0ac3f_row2_col0" class="data row2 col0">
1.000000
</td>
</tr>
<tr>
<th id="T_0ac3f_level0_row3" class="row_heading level0 row3">
Input Image Size:
</th>
<td id="T_0ac3f_row3_col0" class="data row3 col0">
(512, 768)
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-target-annotation-data" class="level3">
<h3 class="anchored" data-anchor-id="get-target-annotation-data">Get Target Annotation Data</h3>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the source annotations for the test image</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>gt_labels <span class="op">=</span> [shape[<span class="st">'label'</span>] <span class="cf">for</span> shape <span class="kw">in</span> annotation_df.loc[file_id][<span class="st">'shapes'</span>]]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>gt_keypoints <span class="op">=</span> torch.tensor(np.array([shape[<span class="st">'points'</span>] <span class="cf">for</span> shape <span class="kw">in</span> annotation_df.loc[file_id][<span class="st">'shapes'</span>]])).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>gt_keypoints_bboxes <span class="op">=</span> torch.cat((gt_keypoints, torch.ones(<span class="bu">len</span>(gt_keypoints), <span class="dv">2</span>)<span class="op">*</span>BBOX_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pass-input-data-to-the-model" class="level3">
<h3 class="anchored" data-anchor-id="pass-input-data-to-the-model">Pass Input Data to the Model</h3>
<p>Now, we can convert the test image to a tensor and pass it to the model. Ensure the model is set to evaluation mode to get predictions instead of loss values.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model to evaluation mode</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()<span class="op">;</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the model and input data are on the same device</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>model.to(device)<span class="op">;</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> transforms.Compose([transforms.ToImage(), </span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>                                   transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>)])(input_img)[<span class="va">None</span>].to(device)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a prediction with the model</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    model_output <span class="op">=</span> model(input_tensor)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="filter-the-model-output" class="level3">
<h3 class="anchored" data-anchor-id="filter-the-model-output">Filter the Model Output</h3>
<p>The model performs most post-processing steps internally, so we only need to filter the output based on the desired confidence threshold. The model returns predictions as a list of dictionaries. Each dictionary stores bounding boxes, label indices, confidence scores, and key points for a single sample in the input batch.</p>
<p>Since we resized the test image, we must scale the key points to the source resolution.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the confidence threshold</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>conf_threshold <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the output based on the confidence threshold</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>scores_mask <span class="op">=</span> model_output[<span class="st">'scores'</span>] <span class="op">&gt;</span> conf_threshold</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract and scale the predicted keypoints</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>predicted_keypoints <span class="op">=</span> (model_output[<span class="st">'keypoints'</span>][scores_mask])[:,:,:<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>)<span class="op">*</span>min_img_scale</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="compare-model-predictions-with-the-source-annotations" class="level3">
<h3 class="anchored" data-anchor-id="compare-model-predictions-with-the-source-annotations">Compare Model Predictions with the Source Annotations</h3>
<p>Finally, we can compare the model predictions with the ground-truth annotations.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the test image with the ground-truth annotations</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>gt_annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(test_img), </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(gt_keypoints_bboxes), <span class="st">'cxcywh'</span>, <span class="st">'xyxy'</span>),</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels=gt_labels, </span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> gt_labels]]</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the labels and bounding box annotations for the test image</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> class_names<span class="op">*</span><span class="bu">sum</span>(scores_mask).item()</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>keypoints_bboxes <span class="op">=</span> torch.cat((predicted_keypoints.cpu(), torch.ones(<span class="bu">len</span>(predicted_keypoints), <span class="dv">2</span>)), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the test image with the model predictions</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>annotated_tensor <span class="op">=</span> draw_bboxes(</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>transforms.PILToTensor()(test_img), </span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    boxes<span class="op">=</span>torchvision.ops.box_convert(torch.Tensor(keypoints_bboxes), <span class="st">'cxcywh'</span>, <span class="st">'xyxy'</span>), </span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels=labels, </span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[int_colors[i] <span class="cf">for</span> i <span class="kw">in</span> [class_names.index(label) <span class="cf">for</span> label <span class="kw">in</span> labels]]</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>stack_imgs([tensor_to_pil(gt_annotated_tensor), tensor_to_pil(annotated_tensor)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_95_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>The model appears to have learned to detect eyes and noses as desired.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Congratulations on completing this tutorial for training Keypoint R-CNN models in PyTorch! The skills and knowledge you acquired here provide a solid foundation for future projects.</p>
<p>As a next step, perhaps try annotating a keypoint dataset with <a href="https://github.com/labelmeai/labelme">LabelMe</a> for your own Keypoint R-CNN model or experiment with the data augmentations to see how they impact model accuracy.</p>
</section>
<section id="recommended-tutorials" class="level2">
<h2 class="anchored" data-anchor-id="recommended-tutorials">Recommended Tutorials</h2>
<ul>
<li><a href="../../posts/pytorch-train-keypoint-rcnn-tutorial/onnx-export/"><strong>Exporting Keypoint R-CNN Models from PyTorch to ONNX</strong></a><strong>:</strong> Learn how to export Keypoint R-CNN models from PyTorch to ONNX and perform inference using ONNX Runtime.</li>
<li><a href="../../posts/pytorch-train-mask-rcnn-tutorial/"><strong>Training Mask R-CNN Models with PyTorch</strong></a><strong>:</strong> Learn how to train Mask R-CNN models on custom datasets with PyTorch.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Questions:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Questions:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Feel free to post questions or problems related to this tutorial in the comments below. I try to make time to address them on Thursdays and Fridays.</li>
</ul>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>I’m Christian Mills, a deep learning consultant specializing in computer vision and practical AI implementations.</li>
<li>I help clients leverage cutting-edge AI technologies to solve real-world problems.</li>
<li>Learn more <a href="../../about.html">about me</a> or reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a> to discuss your project.</li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>