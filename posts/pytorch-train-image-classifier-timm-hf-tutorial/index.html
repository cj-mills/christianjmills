<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christian Mills">
<meta name="dcterms.date" content="2023-05-24">
<meta name="description" content="Learn how to fine-tune image classification models with PyTorch and the timm library by creating a hand gesture recognizer in this easy-to-follow guide for beginners.">

<title>Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners – Christian Mills</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners – Christian Mills">
<meta property="og:description" content="Learn how to fine-tune image classification models with PyTorch and the timm library by creating a hand gesture recognizer in this easy-to-follow guide for beginners.">
<meta property="og:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta property="og:site_name" content="Christian Mills">
<meta property="og:image:height" content="284">
<meta property="og:image:width" content="526">
<meta name="twitter:title" content="Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners – Christian Mills">
<meta name="twitter:description" content="Learn how to fine-tune image classification models with PyTorch and the timm library by creating a hand gesture recognizer in this easy-to-follow guide for beginners.">
<meta name="twitter:image" content="https://christianjmills.com/images/default-preview-image-black.png">
<meta name="twitter:creator" content="@cdotjdotmills">
<meta name="twitter:site" content="@cdotjdotmills">
<meta name="twitter:image-height" content="284">
<meta name="twitter:image-width" content="526">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Christian Mills</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/tutorials/index.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../series/notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:christian@christianjmills.com"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cj-mills"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cdotjdotmills"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/christianjmills"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#getting-started-with-the-code" id="toc-getting-started-with-the-code" class="nav-link" data-scroll-target="#getting-started-with-the-code">Getting Started with the Code</a></li>
  <li><a href="#setting-up-your-python-environment" id="toc-setting-up-your-python-environment" class="nav-link" data-scroll-target="#setting-up-your-python-environment">Setting Up Your Python Environment</a>
  <ul>
  <li><a href="#creating-a-python-environment" id="toc-creating-a-python-environment" class="nav-link" data-scroll-target="#creating-a-python-environment">Creating a Python Environment</a></li>
  <li><a href="#installing-pytorch" id="toc-installing-pytorch" class="nav-link" data-scroll-target="#installing-pytorch">Installing PyTorch</a></li>
  <li><a href="#installing-additional-libraries" id="toc-installing-additional-libraries" class="nav-link" data-scroll-target="#installing-additional-libraries">Installing Additional Libraries</a></li>
  <li><a href="#installing-utility-packages" id="toc-installing-utility-packages" class="nav-link" data-scroll-target="#installing-utility-packages">Installing Utility Packages</a></li>
  <li><a href="#launching-jupyter-notebook" id="toc-launching-jupyter-notebook" class="nav-link" data-scroll-target="#launching-jupyter-notebook">Launching Jupyter Notebook</a></li>
  </ul></li>
  <li><a href="#importing-the-required-dependencies" id="toc-importing-the-required-dependencies" class="nav-link" data-scroll-target="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
  <li><a href="#importing-the-required-dependencies-1" id="toc-importing-the-required-dependencies-1" class="nav-link" data-scroll-target="#importing-the-required-dependencies-1">Importing the Required Dependencies</a></li>
  <li><a href="#setting-up-the-project" id="toc-setting-up-the-project" class="nav-link" data-scroll-target="#setting-up-the-project">Setting Up the Project</a>
  <ul>
  <li><a href="#setting-a-random-number-seed" id="toc-setting-a-random-number-seed" class="nav-link" data-scroll-target="#setting-a-random-number-seed">Setting a Random Number Seed</a></li>
  <li><a href="#setting-the-device-and-data-type" id="toc-setting-the-device-and-data-type" class="nav-link" data-scroll-target="#setting-the-device-and-data-type">Setting the Device and Data Type</a></li>
  <li><a href="#setting-the-directory-paths" id="toc-setting-the-directory-paths" class="nav-link" data-scroll-target="#setting-the-directory-paths">Setting the Directory Paths</a></li>
  </ul></li>
  <li><a href="#loading-and-exploring-the-dataset" id="toc-loading-and-exploring-the-dataset" class="nav-link" data-scroll-target="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a>
  <ul>
  <li><a href="#setting-the-dataset-path" id="toc-setting-the-dataset-path" class="nav-link" data-scroll-target="#setting-the-dataset-path">Setting the Dataset Path</a></li>
  <li><a href="#downloading-the-dataset" id="toc-downloading-the-dataset" class="nav-link" data-scroll-target="#downloading-the-dataset">Downloading the Dataset</a></li>
  <li><a href="#get-image-folders" id="toc-get-image-folders" class="nav-link" data-scroll-target="#get-image-folders">Get Image Folders</a></li>
  <li><a href="#get-image-file-paths" id="toc-get-image-file-paths" class="nav-link" data-scroll-target="#get-image-file-paths">Get Image File Paths</a></li>
  <li><a href="#inspecting-the-class-distribution" id="toc-inspecting-the-class-distribution" class="nav-link" data-scroll-target="#inspecting-the-class-distribution">Inspecting the Class Distribution</a>
  <ul class="collapse">
  <li><a href="#get-image-classes" id="toc-get-image-classes" class="nav-link" data-scroll-target="#get-image-classes">Get image classes</a></li>
  <li><a href="#visualize-the-class-distribution" id="toc-visualize-the-class-distribution" class="nav-link" data-scroll-target="#visualize-the-class-distribution">Visualize the class distribution</a></li>
  </ul></li>
  <li><a href="#visualizing-sample-images" id="toc-visualizing-sample-images" class="nav-link" data-scroll-target="#visualizing-sample-images">Visualizing Sample Images</a></li>
  </ul></li>
  <li><a href="#selecting-a-model" id="toc-selecting-a-model" class="nav-link" data-scroll-target="#selecting-a-model">Selecting a Model</a>
  <ul>
  <li><a href="#exploring-available-models" id="toc-exploring-available-models" class="nav-link" data-scroll-target="#exploring-available-models">Exploring Available Models</a></li>
  <li><a href="#choosing-the-resnet18-d-model" id="toc-choosing-the-resnet18-d-model" class="nav-link" data-scroll-target="#choosing-the-resnet18-d-model">Choosing the ResNet18-D Model</a></li>
  <li><a href="#inspecting-the-model-configuration" id="toc-inspecting-the-model-configuration" class="nav-link" data-scroll-target="#inspecting-the-model-configuration">Inspecting the Model Configuration</a></li>
  <li><a href="#retrieving-normalization-statistics" id="toc-retrieving-normalization-statistics" class="nav-link" data-scroll-target="#retrieving-normalization-statistics">Retrieving Normalization Statistics</a></li>
  <li><a href="#loading-the-model" id="toc-loading-the-model" class="nav-link" data-scroll-target="#loading-the-model">Loading the Model</a></li>
  <li><a href="#summarizing-the-model" id="toc-summarizing-the-model" class="nav-link" data-scroll-target="#summarizing-the-model">Summarizing the Model</a></li>
  </ul></li>
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data">Preparing the Data</a>
  <ul>
  <li><a href="#training-validation-split" id="toc-training-validation-split" class="nav-link" data-scroll-target="#training-validation-split">Training-Validation Split</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data Augmentation</a>
  <ul class="collapse">
  <li><a href="#set-training-image-size" id="toc-set-training-image-size" class="nav-link" data-scroll-target="#set-training-image-size">Set training image size</a></li>
  <li><a href="#initialize-image-transforms" id="toc-initialize-image-transforms" class="nav-link" data-scroll-target="#initialize-image-transforms">Initialize image transforms</a></li>
  <li><a href="#test-the-transforms" id="toc-test-the-transforms" class="nav-link" data-scroll-target="#test-the-transforms">Test the transforms</a></li>
  </ul></li>
  <li><a href="#training-dataset-class" id="toc-training-dataset-class" class="nav-link" data-scroll-target="#training-dataset-class">Training Dataset Class</a></li>
  <li><a href="#image-transforms" id="toc-image-transforms" class="nav-link" data-scroll-target="#image-transforms">Image Transforms</a></li>
  <li><a href="#initialize-datasets" id="toc-initialize-datasets" class="nav-link" data-scroll-target="#initialize-datasets">Initialize Datasets</a></li>
  <li><a href="#inspect-samples" id="toc-inspect-samples" class="nav-link" data-scroll-target="#inspect-samples">Inspect Samples</a></li>
  <li><a href="#training-batch-size" id="toc-training-batch-size" class="nav-link" data-scroll-target="#training-batch-size">Training Batch Size</a></li>
  <li><a href="#initialize-dataloaders" id="toc-initialize-dataloaders" class="nav-link" data-scroll-target="#initialize-dataloaders">Initialize DataLoaders</a></li>
  </ul></li>
  <li><a href="#fine-tuning-the-model" id="toc-fine-tuning-the-model" class="nav-link" data-scroll-target="#fine-tuning-the-model">Fine-tuning the Model</a>
  <ul>
  <li><a href="#define-the-training-loop" id="toc-define-the-training-loop" class="nav-link" data-scroll-target="#define-the-training-loop">Define the Training Loop</a></li>
  <li><a href="#set-the-model-checkpoint-path" id="toc-set-the-model-checkpoint-path" class="nav-link" data-scroll-target="#set-the-model-checkpoint-path">Set the Model Checkpoint Path</a></li>
  <li><a href="#saving-the-class-labels" id="toc-saving-the-class-labels" class="nav-link" data-scroll-target="#saving-the-class-labels">Saving the Class Labels</a></li>
  <li><a href="#configure-the-training-parameters" id="toc-configure-the-training-parameters" class="nav-link" data-scroll-target="#configure-the-training-parameters">Configure the Training Parameters</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">Train the Model</a></li>
  </ul></li>
  <li><a href="#making-predictions-with-the-model" id="toc-making-predictions-with-the-model" class="nav-link" data-scroll-target="#making-predictions-with-the-model">Making Predictions with the Model</a>
  <ul>
  <li><a href="#testing-the-model-on-new-data" id="toc-testing-the-model-on-new-data" class="nav-link" data-scroll-target="#testing-the-model-on-new-data">Testing the Model on New Data</a></li>
  </ul></li>
  <li><a href="#exploring-the-in-browser-demo" id="toc-exploring-the-in-browser-demo" class="nav-link" data-scroll-target="#exploring-the-in-browser-demo">Exploring the In-Browser Demo</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#recommended-tutorials" id="toc-recommended-tutorials" class="nav-link" data-scroll-target="#recommended-tutorials">Recommended Tutorials</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">image-classification</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>

<div>
  <div class="description">
    Learn how to fine-tune image classification models with PyTorch and the timm library by creating a hand gesture recognizer in this easy-to-follow guide for beginners.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christian Mills </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 24, 2023</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">September 21, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../series/tutorials/pytorch-train-image-classifier-series.html"><strong>Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners</strong></a></li>
</ul>
</div>
</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#getting-started-with-the-code">Getting Started with the Code</a></li>
<li><a href="#setting-up-your-python-environment">Setting Up Your Python Environment</a></li>
<li><a href="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
<li><a href="#setting-up-the-project">Setting Up the Project</a></li>
<li><a href="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a></li>
<li><a href="#selecting-a-model">Selecting a Model</a></li>
<li><a href="#preparing-the-data">Preparing the Data</a></li>
<li><a href="#fine-tuning-the-model">Fine-tuning the Model</a></li>
<li><a href="#making-predictions-with-the-model">Making Predictions with the Model</a></li>
<li><a href="#exploring-the-in-browser-demo">Exploring the In-Browser Demo</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to this hands-on guide to fine-tuning image classifiers with <a href="https://pytorch.org/">PyTorch</a> and the <a href="https://github.com/huggingface/pytorch-image-models">timm library</a>. Fine-tuning refers to taking a pre-trained model and adjusting its parameters using a new dataset to enhance its performance on a specific task. We can leverage pre-trained models to achieve high performance even when working with limited data and computational resources. The timm library further aids our goal with its wide range of pre-trained models, catering to diverse needs and use cases.</p>
<p>In this tutorial, we develop a hand gesture recognizer. Hand gesture recognition has many real-world applications, ranging from human-computer interaction and sign-language translation to creating immersive gaming experiences. By the end of this tutorial, you will have a practical hand gesture recognizer and a solid foundation to apply to other image classification tasks. You’ll also be able to interact with a model trained with this tutorial’s code through an in-browser demo that runs locally on your computer. Check out the video below for a quick preview.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="./video/pytorch-timm-gesture-recognition-tutorial-demo.mp4" class="img-fluid quarto-figure quarto-figure-center" controls=""><a href="./video/pytorch-timm-gesture-recognition-tutorial-demo.mp4">Video</a></video></p>
</figure>
</div>
<p>This guide is structured so that you don’t need a deep understanding of deep learning to complete it. If you follow the instructions, you can make it through. Yet, if you are eager to delve deeper into machine learning and deep learning, I recommend fast.ai’s <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a> course. The course employs a hands-on approach that starts you off training models from the get-go and gradually digs deeper into the foundational concepts.</p>
<p>Let’s dive in and start training our hand gesture classifier!</p>
</section>
<section id="getting-started-with-the-code" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-the-code">Getting Started with the Code</h2>
<p>The tutorial code is available as a <a href="https://jupyter.org/">Jupyter Notebook</a>, which you can run locally or in a cloud-based environment like <a href="https://colab.research.google.com/">Google Colab</a>. If you’re new to these platforms or need some guidance setting up, I’ve created dedicated tutorials to help you:</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Setup Guides">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Setup Guides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../posts/google-colab-getting-started-tutorial/"><strong>Getting Started with Google Colab</strong></a><strong>:</strong> This tutorial introduces you to Google Colab, a free, cloud-based Jupyter Notebook service. You’ll learn to write, run, and share Python code directly in your browser.</li>
<li><a href="../../posts/mamba-getting-started-tutorial-windows/"><strong>Setting Up a Local Python Environment with Mamba for Machine Learning Projects on Windows</strong></a><strong>:</strong> This tutorial guides you through installing the Mamba package manager on Windows, setting up a local Python environment, and installing PyTorch and Jupyter for machine learning projects.</li>
</ul>
</div>
</div>
</div>
<p>No matter your choice of environment, you’ll be well-prepared to follow along with the rest of this tutorial. You can download the notebook from the tutorial’s GitHub repository or open the notebook directly in Google Colab using the links below.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Tutorial Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tutorial Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Platform</th>
<th>Jupyter Notebook</th>
<th>Utility File</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Google Colab</td>
<td><a href="https://colab.research.google.com/github/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/pytorch-timm-image-classifier-training-colab.ipynb">Open In Colab</a></td>
<td></td>
</tr>
<tr class="even">
<td>Linux</td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/pytorch-timm-image-classifier-training.ipynb">GitHub Repository</a></td>
<td></td>
</tr>
<tr class="odd">
<td>Linux (Intel Arc)</td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/intel-arc-pytorch-timm-image-classifier-training.ipynb">GitHub Repository</a></td>
<td></td>
</tr>
<tr class="even">
<td>Windows</td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/pytorch-timm-image-classifier-training-windows.ipynb">GitHub Repository</a></td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/windows_utils_hf.py">windows_utils_hf.py</a></td>
</tr>
<tr class="odd">
<td>Windows (Intel Arc)</td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/intel-arc-pytorch-timm-image-classifier-training-windows.ipynb">GitHub Repository</a></td>
<td><a href="https://github.com/cj-mills/pytorch-timm-gesture-recognition-tutorial-code/blob/main/notebooks/windows_utils_hf.py">windows_utils_hf.py</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="macOS &amp; Windows Users">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
macOS &amp; Windows Users
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code in this tutorial targets Linux platforms, but most of it should also work on macOS and Windows.</p>
<p>However, Python multiprocessing works differently on those platforms, requiring some changes to leverage multi-processing for the <code>DataLoader</code> objects.</p>
<p>I’ve made a dedicated version of the tutorial code to run on Windows. The included changes should also work on macOS, but I don’t have a Mac to verify.</p>
</div>
</div>
</section>
<section id="setting-up-your-python-environment" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-your-python-environment">Setting Up Your Python Environment</h2>
<p>Before diving into the code, we’ll create a Python environment and install the necessary libraries. Creating a dedicated environment will ensure our project has all its dependencies in one place and does not interfere with other Python projects you may have.</p>
<p>Please note that this section is for readers setting up a local Python environment on their machines. If you’re following this tutorial on a cloud-based platform like Google Colab, the platform already provides an isolated environment with many Python libraries pre-installed. In that case, you may skip this section and directly proceed to the <a href="#importing-the-required-dependencies">code sections</a>. However, you may still need to install certain libraries specific to this tutorial using similar <code>pip install</code> commands within your notebook. The dedicated Colab Notebook contains the instructions for running it in Google Colab.</p>
<section id="creating-a-python-environment" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-python-environment">Creating a Python Environment</h3>
<p>First, we’ll create a Python environment using <a href="https://docs.conda.io/en/latest/">Conda</a>. Conda is a package manager that can create isolated Python environments. These environments are like sandboxed spaces where you can install Python libraries without affecting the rest of your system.</p>
<p>To create a new Python environment, open a terminal with Conda/Mamba installed and run the following commands:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Conda</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Mamba</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.11 environment</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">--name</span> pytorch-env python=3.11 <span class="at">-y</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new Python 3.11 environment</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">--name</span> pytorch-env python=3.11 <span class="at">-y</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate the environment</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate pytorch-env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>The first command creates a new Python environment named <code>pytorch-env</code> using Python 3.11. The <code>-y</code> flag confirms that we want to proceed with the installation. After building the environment, the second command activates it, setting it as the active Python environment.</p>
</section>
<section id="installing-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="installing-pytorch">Installing PyTorch</h3>
<p>PyTorch is a popular open-source machine learning framework that enables users to perform tensor computations, build dynamic computational graphs, and implement custom machine learning architectures. To install PyTorch with CUDA support (which allows PyTorch to leverage NVIDIA GPUs for faster training), we’ll use the following command:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Linux (CUDA)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Mac</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">Windows (CUDA)</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MPS (Metal Performance Shaders) acceleration is available on MacOS 12.3+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Installation instructions for specific hardware and operating systems are available in the “Get Started” section of the PyTorch website.</p>
<ul>
<li><a href="https://pytorch.org/get-started/locally/">PyTorch: Get Started</a></li>
</ul>
</section>
<section id="installing-additional-libraries" class="level3">
<h3 class="anchored" data-anchor-id="installing-additional-libraries">Installing Additional Libraries</h3>
<p>We also need to install some additional libraries for our project. If you’re new to Python or haven’t used some of these packages before, don’t worry.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Descriptions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Descriptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://jupyter.org/"><code>jupyter</code></a>: An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.</li>
<li><a href="https://matplotlib.org/"><code>matplotlib</code></a>: This package provides a comprehensive collection of visualization tools to create high-quality plots, charts, and graphs for data exploration and presentation.</li>
<li><a href="https://pandas.pydata.org/"><code>pandas</code></a>: This package provides fast, powerful, and flexible data analysis and manipulation tools.</li>
<li><a href="https://pillow.readthedocs.io/en/stable/"><code>pillow</code></a>: The Python Imaging Library adds image processing capabilities.</li>
<li><a href="https://github.com/huggingface/pytorch-image-models"><code>timm</code></a>: The timm library provides state-of-the-art (SOTA) computer vision models, layers, utilities, optimizers, schedulers, data loaders, augmentations, and training/evaluation scripts.</li>
<li><a href="https://pytorch.org/torcheval/stable/"><code>torcheval</code></a>: A library with simple and straightforward tooling for model evaluations.</li>
<li><a href="https://pytorch.org/tnt/stable/"><code>torchtnt</code></a>: A library for PyTorch training tools and utilities.</li>
<li><a href="https://tqdm.github.io/"><code>tqdm</code></a>: A Python library that provides fast, extensible progress bars for loops and other iterable objects in Python.</li>
</ul>
</div>
</div>
</div>
<p>To install these additional libraries, we’ll use the following command:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install additional dependencies</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter matplotlib pandas pillow timm torcheval torchtnt==0.2.0 tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="installing-utility-packages" class="level3">
<h3 class="anchored" data-anchor-id="installing-utility-packages">Installing Utility Packages</h3>
<p>Finally, we will install some utility packages I made to help us handle images (<a href="https://cj-mills.github.io/cjm-pil-utils/"><code>cjm_pil_utils</code></a>), interact with PyTorch (<a href="https://cj-mills.github.io/cjm-pytorch-utils/"><code>cjm_pytorch_utils</code></a>), work with pandas DataFrames (<a href="https://cj-mills.github.io/cjm-pandas-utils/"><code>cjm_pandas_utils</code></a>), and apply image transforms (<a href="https://cj-mills.github.io/cjm-torchvision-tfms/"><code>cjm_torchvision_tfms</code></a>):</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install utility packages</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install cjm_pandas_utils cjm_psl_utils cjm_pil_utils cjm_pytorch_utils cjm_torchvision_tfms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, our environment is all setup and ready to go! Remember, these libraries are just tools. If you don’t fully understand them yet, don’t worry. As we go through the tutorial, we’ll learn more about these tools and see them in action.</p>
</section>
<section id="launching-jupyter-notebook" class="level3">
<h3 class="anchored" data-anchor-id="launching-jupyter-notebook">Launching Jupyter Notebook</h3>
<p>Now that our environment is ready, it’s time to launch Jupyter Notebook. Jupyter Notebooks provide an interactive coding environment where we’ll work for the rest of this tutorial. To launch Jupyter Notebook, navigate to the location where you have stored the tutorial notebook (if you downloaded it) from a terminal with the <code>pytorch-env</code> environment active, and type the following command:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This command will open a new tab in your default web browser, showing the Jupyter file browser. From the Jupyter file browser, you can open the tutorial notebook or create a new one to start the next section.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you close your terminal, the Jupyter Notebook server will stop. So, keep your terminal running while you’re working on the tutorial.</p>
</div>
</div>
</section>
</section>
<section id="importing-the-required-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-required-dependencies">Importing the Required Dependencies</h2>
<p>With our environment set up, it’s time to start the coding part of this tutorial. First, we will import the necessary Python packages into our Jupyter Notebook. Here’s a brief overview of how we’ll use these packages:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Package Uses">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Package Uses
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>matplotlib</strong>: We use the matplotlib package to explore the dataset samples and class distribution.</li>
<li><strong>NumPy</strong>: We’ll use it to store PIL Images as arrays of pixel values.</li>
<li><strong>pandas</strong>: We use Pandas <code>DataFrame</code> and <code>Series</code> objects to format data as tables.</li>
<li><strong>PIL (Pillow)</strong>: We’ll use it for opening and working with image files.</li>
<li><strong>Python Standard Library dependencies</strong>: These are built-in modules that come with Python. We’ll use them for various tasks like handling file paths (<a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path"><code>pathlib.Path</code></a>), manipulating JSON files (<a href="https://docs.python.org/3/library/json.html"><code>json</code></a>), random number generation (<a href="https://docs.python.org/3/library/random.html"><code>random</code></a>), multiprocessing (<a href="https://docs.python.org/3/library/multiprocessing.html"><code>multiprocessing</code></a>), mathematical operations (<a href="https://docs.python.org/3/library/math.html"><code>math</code></a>), copying Python objects (<a href="https://docs.python.org/3/library/copy.html"><code>copy</code></a>), file matching patterns (<a href="https://docs.python.org/3/library/glob.html"><code>glob</code></a>), working with dates and times (<a href="https://docs.python.org/3/library/datetime.html"><code>datetime</code></a>), and interacting with the operating system (<a href="https://docs.python.org/3/library/os.html"><code>os</code></a>).</li>
<li><strong>PyTorch dependencies</strong>: We’ll use PyTorch’s various modules for building our model, processing data, and training.</li>
<li><strong>timm library</strong>: We’ll use the timm library to download and prepare a pre-trained model for fine-tuning.</li>
<li><strong>tqdm</strong>: We use the library to track the progress of longer processes like training.</li>
<li><strong>Utility functions</strong>: These are helper functions from the packages we installed earlier. They provide shortcuts for routine tasks and keep our code clean and readable.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="importing-the-required-dependencies-1" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-required-dependencies-1">Importing the Required Dependencies</h2>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Python Standard Library dependencies</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> copy</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> multiprocessing</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import utility functions</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pandas_utils.core <span class="im">import</span> markdown_to_pandas</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pil_utils.core <span class="im">import</span> resize_img, get_img_files</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_psl_utils.core <span class="im">import</span> download_file, file_extract</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_pytorch_utils.core <span class="im">import</span> set_seed, pil_to_tensor, tensor_to_pil, get_torch_device, denorm_img_tensor</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cjm_torchvision_tfms.core <span class="im">import</span> ResizeMax, PadSquare</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib for creating plots</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy </span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Import pandas module for data manipulation</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not truncate the contents of cells and display all rows and columns</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'max_colwidth'</span>, <span class="va">None</span>, <span class="st">'display.max_rows'</span>, <span class="va">None</span>, <span class="st">'display.max_columns'</span>, <span class="va">None</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PIL for image manipulation</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Import timm library</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch dependencies</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.amp <span class="im">import</span> autocast</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>torchvision.disable_beta_transforms_warning()</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.v2  <span class="im">as</span> transforms</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> functional <span class="im">as</span> TF</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtnt.utils <span class="im">import</span> get_module_summary</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torcheval.metrics <span class="im">import</span> MulticlassAccuracy</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Having successfully imported the dependencies, we are ready to move to the next step.</p>
</section>
<section id="setting-up-the-project" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-project">Setting Up the Project</h2>
<p>In this section, we set up some basics for our project. First, we set a seed for generating random numbers using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#set_seed"><code>set_seed</code></a> function from the <code>cjm_pytorch_utils</code> package.</p>
<section id="setting-a-random-number-seed" class="level3">
<h3 class="anchored" data-anchor-id="setting-a-random-number-seed">Setting a Random Number Seed</h3>
<p>A fixed seed value is helpful when training deep-learning models for reproducibility, debugging, and comparison. Having reproducible results allows others to confirm your findings. Using a fixed seed can make it easier to find bugs as it ensures the same inputs produce the same outputs. Likewise, using fixed seed values lets you compare performance between models and training parameters. That said, it’s often a good idea to test different seed values to see how your model’s performance varies between them. Also, don’t use a fixed seed value when you deploy the final model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1234</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>set_seed(seed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="setting-the-device-and-data-type" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-device-and-data-type">Setting the Device and Data Type</h3>
<p>Next, we determine the device to run our computations on and the data type of our tensors using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#get_torch_device"><code>get_torch_device</code></a> function from the <code>cjm_pytorch_utils</code> package.</p>
<p>PyTorch can run on either a CPU or a GPU. The <code>get_torch_device</code> function will automatically check if a supported Nvidia or Mac GPU is available. Otherwise, it will use the CPU. We’ll use the device and type variables to ensure all tensors and model weights are on the correct device and have the same data type. Otherwise, we might get errors.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_torch_device()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> torch.float32</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>device, dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>('cuda', torch.float32)</code></pre>
</section>
<section id="setting-the-directory-paths" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-directory-paths">Setting the Directory Paths</h3>
<p>We’ll then set up a directory for our project to store our results and other related files. The code currently creates the folder in the current directory (<code>./</code>). Update the path if that is not suitable for you.</p>
<p>We also need a place to store our datasets. We’re going to create a directory for this purpose. If running locally, select a suitable folder location to store the dataset. For a cloud service like Google Colab, you can set it to the current directory.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The name for the project</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>project_name <span class="op">=</span> <span class="ss">f"pytorch-timm-image-classifier"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The path for the project folder</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>project_dir <span class="op">=</span> Path(<span class="ss">f"./</span><span class="sc">{</span>project_name<span class="sc">}</span><span class="ss">/"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the project directory if it does not already exist</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>project_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store datasets</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>dataset_dir <span class="op">=</span> Path(<span class="st">"/mnt/980_1TB_2/Datasets/"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the dataset directory if it does not exist</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>dataset_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define path to store archive files</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>archive_dir <span class="op">=</span> dataset_dir<span class="op">/</span><span class="st">'../Archive'</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the archive directory if it does not exist</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>archive_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Series with the paths and converting it to a DataFrame for display</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project Directory:"</span>: project_dir,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Directory:"</span>: dataset_dir, </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Directory:"</span>: archive_dir</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_5d4eb">
<thead>
</thead>
<tbody>
<tr>
<th id="T_5d4eb_level0_row0" class="row_heading level0 row0">
Project Directory:
</th>
<td id="T_5d4eb_row0_col0" class="data row0 col0">
pytorch-timm-image-classifier
</td>
</tr>
<tr>
<th id="T_5d4eb_level0_row1" class="row_heading level0 row1">
Dataset Directory:
</th>
<td id="T_5d4eb_row1_col0" class="data row1 col0">
/mnt/980_1TB_2/Datasets
</td>
</tr>
<tr>
<th id="T_5d4eb_level0_row2" class="row_heading level0 row2">
Archive Directory:
</th>
<td id="T_5d4eb_row2_col0" class="data row2 col0">
/mnt/980_1TB_2/Datasets/../Archive
</td>
</tr>
</tbody>
</table>
</div>
<p>Double-check the project and dataset directories exist in the specified paths and that you can add files to them before continuing.</p>
<p>At this point, our environment is set up and ready to go. We’ve set our random seed, determined our computation device, and set up directories for our project and dataset. In the next section, we will download and explore the dataset.</p>
</section>
</section>
<section id="loading-and-exploring-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="loading-and-exploring-the-dataset">Loading and Exploring the Dataset</h2>
<p>Now that we set up our project, we can start working with our dataset. The dataset we’ll use is a downscaled subset of <a href="https://github.com/hukenovs/hagrid">HaGRID</a> (HAnd Gesture Recognition Image Dataset) that I modified for image classification tasks. The dataset contains images for <code>18</code> distinct hand gestures and an additional <code>no_gesture</code> class for idle hands. The dataset is approximately <code>3.8 GB</code>, but you will need about <code>7.6 GB</code> to store the archive file and extracted dataset.</p>
<ul>
<li><strong>HuggingFace Hub Dataset Repository:</strong> <a href="https://huggingface.co/datasets/cj-mills/hagrid-classification-512p-no-gesture-150k-zip">cj-mills/hagrid-classification-512p-no-gesture-150k-zip</a></li>
</ul>
<p>The following steps demonstrate how to load the dataset from the HuggingFace Hub, inspect the dataset, and visualize some sample images.</p>
<section id="setting-the-dataset-path" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-dataset-path">Setting the Dataset Path</h3>
<p>We first need to construct the name for the chosen Hugging Face Hub dataset and define where to download and extract the dataset.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the name of the dataset</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">'hagrid-classification-512p-no-gesture-150k-zip'</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset name by combining the username and dataset name</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>hf_dataset <span class="op">=</span> <span class="ss">f'cj-mills/</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the zip file that contains the dataset</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>archive_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>archive_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">.</span>removesuffix(<span class="st">"-zip"</span>)<span class="sc">}</span><span class="ss">.zip'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the path to the directory where the dataset will be extracted</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> Path(<span class="ss">f'</span><span class="sc">{</span>dataset_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>dataset_name<span class="sc">.</span>removesuffix(<span class="st">"-zip"</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Series with the dataset name and paths and converting it to a DataFrame for display</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HuggingFace Dataset:"</span>: hf_dataset, </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Archive Path:"</span>: archive_path, </span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dataset Path:"</span>: dataset_path</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_7edfc">
<thead>
</thead>
<tbody>
<tr>
<th id="T_7edfc_level0_row0" class="row_heading level0 row0">
HuggingFace Dataset:
</th>
<td id="T_7edfc_row0_col0" class="data row0 col0">
cj-mills/hagrid-classification-512p-no-gesture-150k-zip
</td>
</tr>
<tr>
<th id="T_7edfc_level0_row1" class="row_heading level0 row1">
Archive Path:
</th>
<td id="T_7edfc_row1_col0" class="data row1 col0">
/mnt/980_1TB_2/Datasets/../Archive/hagrid-classification-512p-no-gesture-150k.zip
</td>
</tr>
<tr>
<th id="T_7edfc_level0_row2" class="row_heading level0 row2">
Dataset Path:
</th>
<td id="T_7edfc_row2_col0" class="data row2 col0">
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="downloading-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="downloading-the-dataset">Downloading the Dataset</h3>
<p>We can now download the archive file and extract the dataset using the <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#download_file"><code>download_file</code></a> and <a href="https://cj-mills.github.io/cjm-psl-utils/core.html#file_extract"><code>file_extract</code></a> functions from the <code>cjm_psl_utils</code> package. We can delete the archive afterward to save space.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the HuggingFace Hub dataset URL</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dataset_url <span class="op">=</span> <span class="ss">f"https://huggingface.co/datasets/</span><span class="sc">{</span>hf_dataset<span class="sc">}</span><span class="ss">/resolve/main/</span><span class="sc">{</span>dataset_name<span class="sc">.</span>removesuffix(<span class="st">'-zip'</span>)<span class="sc">}</span><span class="ss">.zip"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"HuggingFace Dataset URL: </span><span class="sc">{</span>dataset_url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set whether to delete the archive file after extracting the dataset</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>delete_archive <span class="op">=</span> <span class="va">True</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset if not present</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> dataset_path.is_dir():</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Dataset folder already exists"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Downloading dataset..."</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    download_file(dataset_url, archive_dir)    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracting dataset..."</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    file_extract(fname<span class="op">=</span>archive_path, dest<span class="op">=</span>dataset_dir)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Delete the archive if specified</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> delete_archive: archive_path.unlink()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="get-image-folders" class="level3">
<h3 class="anchored" data-anchor-id="get-image-folders">Get Image Folders</h3>
<p>The dataset organizes samples for each gesture class into separate sub-folders.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>img_folder_paths <span class="op">=</span> [folder <span class="cf">for</span> folder <span class="kw">in</span> dataset_path.iterdir() <span class="cf">if</span> folder.is_dir()]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the names of the folders using a Pandas DataFrame</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Image Folder"</span>: [folder.name <span class="cf">for</span> folder <span class="kw">in</span> img_folder_paths]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Image Folder
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
call
</td>
</tr>
<tr>
<th>
1
</th>
<td>
dislike
</td>
</tr>
<tr>
<th>
2
</th>
<td>
fist
</td>
</tr>
<tr>
<th>
3
</th>
<td>
four
</td>
</tr>
<tr>
<th>
4
</th>
<td>
like
</td>
</tr>
<tr>
<th>
5
</th>
<td>
mute
</td>
</tr>
<tr>
<th>
6
</th>
<td>
no_gesture
</td>
</tr>
<tr>
<th>
7
</th>
<td>
ok
</td>
</tr>
<tr>
<th>
8
</th>
<td>
one
</td>
</tr>
<tr>
<th>
9
</th>
<td>
palm
</td>
</tr>
<tr>
<th>
10
</th>
<td>
peace
</td>
</tr>
<tr>
<th>
11
</th>
<td>
peace_inverted
</td>
</tr>
<tr>
<th>
12
</th>
<td>
rock
</td>
</tr>
<tr>
<th>
13
</th>
<td>
stop
</td>
</tr>
<tr>
<th>
14
</th>
<td>
stop_inverted
</td>
</tr>
<tr>
<th>
15
</th>
<td>
three
</td>
</tr>
<tr>
<th>
16
</th>
<td>
three2
</td>
</tr>
<tr>
<th>
17
</th>
<td>
two_up
</td>
</tr>
<tr>
<th>
18
</th>
<td>
two_up_inverted
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-image-file-paths" class="level3">
<h3 class="anchored" data-anchor-id="get-image-file-paths">Get Image File Paths</h3>
<p>Now that we have the image image folder paths, we can get the file paths for all the images in the dataset.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of all image file paths from the image folders</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>class_file_paths <span class="op">=</span> [get_img_files(folder) <span class="cf">for</span> folder <span class="kw">in</span> img_folder_paths]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all image files in the 'img_dir' directory</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>img_paths <span class="op">=</span> [</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> folder <span class="kw">in</span> class_file_paths <span class="co"># Iterate through each image folder</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> folder <span class="co"># Get a list of image files in each image folder</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of image files</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of Images: </span><span class="sc">{</span><span class="bu">len</span>(img_paths)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first five entries using a Pandas DataFrame</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(img_paths).head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Number of Images: 153735</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k/call/3ffbf0a0-1837-42cd-8f13-33977a2b47aa.jpeg
</td>
</tr>
<tr>
<th>
1
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k/call/7f4d415e-f570-42c3-aa5a-7c907d2d461e.jpeg
</td>
</tr>
<tr>
<th>
2
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k/call/0003d6d1-3489-4f57-ab7a-44744dba93fd.jpeg
</td>
</tr>
<tr>
<th>
3
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k/call/00084dfa-60a2-4c8e-9bd9-25658382b8b7.jpeg
</td>
</tr>
<tr>
<th>
4
</th>
<td>
/mnt/980_1TB_2/Datasets/hagrid-classification-512p-no-gesture-150k/call/0010543c-be59-49e7-8f6d-fbea8f5fdc6b.jpeg
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inspecting-the-class-distribution" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-class-distribution">Inspecting the Class Distribution</h3>
<p>Next, we get the names of all the classes in our dataset and inspect the distribution of images among these classes. A balanced dataset (where each class has approximately the same number of instances) is ideal for training a machine-learning model.</p>
<section id="get-image-classes" class="level4">
<h4 class="anchored" data-anchor-id="get-image-classes">Get image classes</h4>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the number of samples for each image class</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>class_counts_dict <span class="op">=</span> {folder[<span class="dv">0</span>].parent.name:<span class="bu">len</span>(folder) <span class="cf">for</span> folder <span class="kw">in</span> class_file_paths}</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a list of unique labels</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> <span class="bu">list</span>(class_counts_dict.keys())</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the labels and the corresponding number of samples using a Pandas DataFrame</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> pd.DataFrame.from_dict({<span class="st">'Count'</span>:class_counts_dict})</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>class_counts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Count
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
call
</th>
<td>
6939
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
7028
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
6882
</td>
</tr>
<tr>
<th>
four
</th>
<td>
7183
</td>
</tr>
<tr>
<th>
like
</th>
<td>
6823
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
7139
</td>
</tr>
<tr>
<th>
no_gesture
</th>
<td>
27823
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
6924
</td>
</tr>
<tr>
<th>
one
</th>
<td>
7062
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
7050
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
6965
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
6876
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
6883
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
6893
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
7142
</td>
</tr>
<tr>
<th>
three
</th>
<td>
6940
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
6870
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
7346
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
6967
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="visualize-the-class-distribution" class="level4">
<h4 class="anchored" data-anchor-id="visualize-the-class-distribution">Visualize the class distribution</h4>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>class_counts.plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Class distribution'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Classes'</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(class_counts.index)), class_names)  <span class="co"># Set the x-axis tick labels</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">75</span>)  <span class="co"># Rotate x-axis labels</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.gca().legend().set_visible(<span class="va">False</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_27_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Each class, excluding the <code>no_gesture</code> class, has roughly the same number of samples. The <code>no_gesture</code> class contains approximately four times as many images because of the immense variety of non-matching hand positions.</p>
</section>
</section>
<section id="visualizing-sample-images" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-sample-images">Visualizing Sample Images</h3>
<p>Lastly, we will visualize the first sample image of each class in our dataset. Visualizing the samples helps us get a feel for the kind of images we’re working with and whether they’re suitable for the task at hand.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the first image found for each class</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sample_image_paths <span class="op">=</span> [folder[<span class="dv">0</span>] <span class="cf">for</span> folder <span class="kw">in</span> class_file_paths]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>sample_labels <span class="op">=</span> [path.parent.stem <span class="cf">for</span> path <span class="kw">in</span> sample_image_paths]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the number of rows and columns</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>grid_size <span class="op">=</span> math.floor(math.sqrt(<span class="bu">len</span>(sample_image_paths)))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>n_rows <span class="op">=</span> grid_size<span class="op">+</span>(<span class="dv">1</span> <span class="cf">if</span> grid_size<span class="op">**</span><span class="dv">2</span> <span class="op">&lt;</span> <span class="bu">len</span>(sample_image_paths) <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>n_cols <span class="op">=</span> grid_size</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure for the grid</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">12</span>))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axs.flatten()):</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If we have an image for this subplot</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(sample_image_paths) <span class="kw">and</span> sample_image_paths[i]:</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the image to the subplot</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        ax.imshow(np.array(Image.<span class="bu">open</span>(sample_image_paths[i])))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set the title to the corresponding class name</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        ax.set_title(sample_labels[i])</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove the axis</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If no image, hide the subplot</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the grid</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_29_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>We have loaded the dataset, inspected its class distribution, and visualized some sample images. In the next section, we will select and load our model.</p>
</section>
</section>
<section id="selecting-a-model" class="level2">
<h2 class="anchored" data-anchor-id="selecting-a-model">Selecting a Model</h2>
<p>Choosing a suitable model for your task is crucial for the success of your machine learning project. The model you select will depend on several factors, including the size and nature of your dataset, the problem you’re trying to solve, and the computational resources you have at your disposal.</p>
<section id="exploring-available-models" class="level3">
<h3 class="anchored" data-anchor-id="exploring-available-models">Exploring Available Models</h3>
<p>You can explore the pretrained models available in the timm library using the <code>timm.list_models()</code> method. The library has hundreds of models, so we’ll narrow our search to the <a href="https://arxiv.org/abs/1512.03385">ResNet18</a> family of models. ResNet 18 models are popular for image classification tasks due to their balance of accuracy and speed.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(timm.list_models(<span class="st">'resnet18*'</span>, pretrained<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
resnet18.a1_in1k
</td>
</tr>
<tr>
<th>
1
</th>
<td>
resnet18.a2_in1k
</td>
</tr>
<tr>
<th>
2
</th>
<td>
resnet18.a3_in1k
</td>
</tr>
<tr>
<th>
3
</th>
<td>
resnet18.fb_ssl_yfcc100m_ft_in1k
</td>
</tr>
<tr>
<th>
4
</th>
<td>
resnet18.fb_swsl_ig1b_ft_in1k
</td>
</tr>
<tr>
<th>
5
</th>
<td>
resnet18.gluon_in1k
</td>
</tr>
<tr>
<th>
6
</th>
<td>
resnet18.tv_in1k
</td>
</tr>
<tr>
<th>
7
</th>
<td>
resnet18d.ra2_in1k
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="choosing-the-resnet18-d-model" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-resnet18-d-model">Choosing the ResNet18-D Model</h3>
<p>For this tutorial, I went with the pretrained <a href="https://github.com/huggingface/pytorch-image-models/blob/4b8cfa6c0a355a9b3cb2a77298b240213fb3b921/timm/models/resnet.py#L869">ResNet 18-D</a> model. This model’s balance of accuracy and speed makes it suitable for real-time applications, such as hand gesture recognition. While this model is a good all-rounder, others may work better for specific applications. For example, some models are designed to run on mobile devices and may sacrifice some accuracy for improved performance. Whatever your requirements are, the timm library likely has a suitable model for your needs. Feel free to try different models and see how they compare.</p>
</section>
<section id="inspecting-the-model-configuration" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-model-configuration">Inspecting the Model Configuration</h3>
<p>Next, we will inspect the configuration of our chosen model. The model config gives us information about the pretraining process for the model.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the resnet module</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timm.models <span class="im">import</span> resnet <span class="im">as</span> model_family</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the base model variant to use</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> <span class="st">'resnet18d'</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>version <span class="op">=</span> <span class="st">"ra2_in1k"</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the default configuration of the chosen model</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>model_cfg <span class="op">=</span> model_family.default_cfgs[base_model].default.to_dict()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the default configuration values</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>pd.DataFrame.from_dict(model_cfg, orient<span class="op">=</span><span class="st">'index'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
url
</th>
<td>
https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet18d_ra2-48a79e06.pth
</td>
</tr>
<tr>
<th>
hf_hub_id
</th>
<td>
timm/
</td>
</tr>
<tr>
<th>
custom_load
</th>
<td>
False
</td>
</tr>
<tr>
<th>
input_size
</th>
<td>
(3, 224, 224)
</td>
</tr>
<tr>
<th>
test_input_size
</th>
<td>
(3, 288, 288)
</td>
</tr>
<tr>
<th>
fixed_input_size
</th>
<td>
False
</td>
</tr>
<tr>
<th>
interpolation
</th>
<td>
bicubic
</td>
</tr>
<tr>
<th>
crop_pct
</th>
<td>
0.875
</td>
</tr>
<tr>
<th>
test_crop_pct
</th>
<td>
0.95
</td>
</tr>
<tr>
<th>
crop_mode
</th>
<td>
center
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
(0.485, 0.456, 0.406)
</td>
</tr>
<tr>
<th>
std
</th>
<td>
(0.229, 0.224, 0.225)
</td>
</tr>
<tr>
<th>
num_classes
</th>
<td>
1000
</td>
</tr>
<tr>
<th>
pool_size
</th>
<td>
(7, 7)
</td>
</tr>
<tr>
<th>
first_conv
</th>
<td>
conv1.0
</td>
</tr>
<tr>
<th>
classifier
</th>
<td>
fc
</td>
</tr>
<tr>
<th>
origin_url
</th>
<td>
https://github.com/huggingface/pytorch-image-models
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="retrieving-normalization-statistics" class="level3">
<h3 class="anchored" data-anchor-id="retrieving-normalization-statistics">Retrieving Normalization Statistics</h3>
<p>Before we can use the ResNet18-D model, we need to normalize our dataset. Normalization is a process that changes the range of pixel intensity values to make the neural network converge faster during training. It is performed by subtracting the mean from the pixel values and dividing by the standard deviation of the dataset. The mean and standard deviation values specific to the dataset used in the pretraining process of our model are called normalization statistics. To do this, we will retrieve the normalization statistics (mean and std) specific to our pretrained model.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve normalization statistics (mean and std) specific to the pretrained model</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>mean, std <span class="op">=</span> model_cfg[<span class="st">'mean'</span>], model_cfg[<span class="st">'std'</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>norm_stats <span class="op">=</span> (mean, std)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>norm_stats</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))</code></pre>
</section>
<section id="loading-the-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-the-model">Loading the Model</h3>
<p>We can now load our model. We’ll set the number of output classes equal to the number of image classes in our dataset. We’ll also specify the device and data type for the model.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pretrained ResNet model with the number of output classes equal to the number of class names</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 'timm.create_model' function automatically downloads and initializes the pretrained weights</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="ss">f'</span><span class="sc">{</span>base_model<span class="sc">}</span><span class="ss">.</span><span class="sc">{</span>version<span class="sc">}</span><span class="ss">'</span>, pretrained<span class="op">=</span><span class="va">True</span>, num_classes<span class="op">=</span><span class="bu">len</span>(class_names))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the device and data type for the model</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device<span class="op">=</span>device, dtype<span class="op">=</span>dtype)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add attributes to store the device and model name for later reference</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>model.device <span class="op">=</span> device</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>model.name <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>base_model<span class="sc">}</span><span class="ss">.</span><span class="sc">{</span>version<span class="sc">}</span><span class="ss">'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="summarizing-the-model" class="level3">
<h3 class="anchored" data-anchor-id="summarizing-the-model">Summarizing the Model</h3>
<p>Finally, let’s generate a summary of our model. The summary gives us an overview of its structure and performance characteristics.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input to the model</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>test_inp <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">256</span>, <span class="dv">256</span>).to(device)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a summary of the model as a Pandas DataFrame</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> markdown_to_pandas(<span class="ss">f"</span><span class="sc">{</span>get_module_summary(model, [test_inp])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the summary to only contain Conv2d layers and the model</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>summary_df <span class="op">=</span> summary_df[(summary_df.index <span class="op">==</span> <span class="dv">0</span>) <span class="op">|</span> (summary_df[<span class="st">'Type'</span>] <span class="op">==</span> <span class="st">'Conv2d'</span>)]</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the column "Contains Uninitialized Parameters?"</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>summary_df.drop(<span class="st">'Contains Uninitialized Parameters?'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Type
</th>
<th>
# Parameters
</th>
<th>
# Trainable Parameters
</th>
<th>
Size (bytes)
</th>
<th>
Forward FLOPs
</th>
<th>
Backward FLOPs
</th>
<th>
In size
</th>
<th>
Out size
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
ResNet
</td>
<td>
11.2 M
</td>
<td>
11.2 M
</td>
<td>
44.9 M
</td>
<td>
2.7 G
</td>
<td>
5.3 G
</td>
<td>
[1, 3, 256, 256]
</td>
<td>
[1, 19]
</td>
</tr>
<tr>
<th>
2
</th>
<td>
Conv2d
</td>
<td>
864
</td>
<td>
864
</td>
<td>
3.5 K
</td>
<td>
14.2 M
</td>
<td>
14.2 M
</td>
<td>
[1, 3, 256, 256]
</td>
<td>
[1, 32, 128, 128]
</td>
</tr>
<tr>
<th>
5
</th>
<td>
Conv2d
</td>
<td>
9.2 K
</td>
<td>
9.2 K
</td>
<td>
36.9 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 32, 128, 128]
</td>
<td>
[1, 32, 128, 128]
</td>
</tr>
<tr>
<th>
8
</th>
<td>
Conv2d
</td>
<td>
18.4 K
</td>
<td>
18.4 K
</td>
<td>
73.7 K
</td>
<td>
301 M
</td>
<td>
603 M
</td>
<td>
[1, 32, 128, 128]
</td>
<td>
[1, 64, 128, 128]
</td>
</tr>
<tr>
<th>
14
</th>
<td>
Conv2d
</td>
<td>
36.9 K
</td>
<td>
36.9 K
</td>
<td>
147 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 64, 64, 64]
</td>
<td>
[1, 64, 64, 64]
</td>
</tr>
<tr>
<th>
19
</th>
<td>
Conv2d
</td>
<td>
36.9 K
</td>
<td>
36.9 K
</td>
<td>
147 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 64, 64, 64]
</td>
<td>
[1, 64, 64, 64]
</td>
</tr>
<tr>
<th>
23
</th>
<td>
Conv2d
</td>
<td>
36.9 K
</td>
<td>
36.9 K
</td>
<td>
147 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 64, 64, 64]
</td>
<td>
[1, 64, 64, 64]
</td>
</tr>
<tr>
<th>
28
</th>
<td>
Conv2d
</td>
<td>
36.9 K
</td>
<td>
36.9 K
</td>
<td>
147 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 64, 64, 64]
</td>
<td>
[1, 64, 64, 64]
</td>
</tr>
<tr>
<th>
33
</th>
<td>
Conv2d
</td>
<td>
73.7 K
</td>
<td>
73.7 K
</td>
<td>
294 K
</td>
<td>
75.5 M
</td>
<td>
150 M
</td>
<td>
[1, 64, 64, 64]
</td>
<td>
[1, 128, 32, 32]
</td>
</tr>
<tr>
<th>
38
</th>
<td>
Conv2d
</td>
<td>
147 K
</td>
<td>
147 K
</td>
<td>
589 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 128, 32, 32]
</td>
<td>
[1, 128, 32, 32]
</td>
</tr>
<tr>
<th>
43
</th>
<td>
Conv2d
</td>
<td>
8.2 K
</td>
<td>
8.2 K
</td>
<td>
32.8 K
</td>
<td>
8.4 M
</td>
<td>
16.8 M
</td>
<td>
[1, 64, 32, 32]
</td>
<td>
[1, 128, 32, 32]
</td>
</tr>
<tr>
<th>
46
</th>
<td>
Conv2d
</td>
<td>
147 K
</td>
<td>
147 K
</td>
<td>
589 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 128, 32, 32]
</td>
<td>
[1, 128, 32, 32]
</td>
</tr>
<tr>
<th>
51
</th>
<td>
Conv2d
</td>
<td>
147 K
</td>
<td>
147 K
</td>
<td>
589 K
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 128, 32, 32]
</td>
<td>
[1, 128, 32, 32]
</td>
</tr>
<tr>
<th>
56
</th>
<td>
Conv2d
</td>
<td>
294 K
</td>
<td>
294 K
</td>
<td>
1.2 M
</td>
<td>
75.5 M
</td>
<td>
150 M
</td>
<td>
[1, 128, 32, 32]
</td>
<td>
[1, 256, 16, 16]
</td>
</tr>
<tr>
<th>
61
</th>
<td>
Conv2d
</td>
<td>
589 K
</td>
<td>
589 K
</td>
<td>
2.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 256, 16, 16]
</td>
<td>
[1, 256, 16, 16]
</td>
</tr>
<tr>
<th>
66
</th>
<td>
Conv2d
</td>
<td>
32.8 K
</td>
<td>
32.8 K
</td>
<td>
131 K
</td>
<td>
8.4 M
</td>
<td>
16.8 M
</td>
<td>
[1, 128, 16, 16]
</td>
<td>
[1, 256, 16, 16]
</td>
</tr>
<tr>
<th>
69
</th>
<td>
Conv2d
</td>
<td>
589 K
</td>
<td>
589 K
</td>
<td>
2.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 256, 16, 16]
</td>
<td>
[1, 256, 16, 16]
</td>
</tr>
<tr>
<th>
74
</th>
<td>
Conv2d
</td>
<td>
589 K
</td>
<td>
589 K
</td>
<td>
2.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 256, 16, 16]
</td>
<td>
[1, 256, 16, 16]
</td>
</tr>
<tr>
<th>
79
</th>
<td>
Conv2d
</td>
<td>
1.2 M
</td>
<td>
1.2 M
</td>
<td>
4.7 M
</td>
<td>
75.5 M
</td>
<td>
150 M
</td>
<td>
[1, 256, 16, 16]
</td>
<td>
[1, 512, 8, 8]
</td>
</tr>
<tr>
<th>
84
</th>
<td>
Conv2d
</td>
<td>
2.4 M
</td>
<td>
2.4 M
</td>
<td>
9.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 512, 8, 8]
</td>
<td>
[1, 512, 8, 8]
</td>
</tr>
<tr>
<th>
89
</th>
<td>
Conv2d
</td>
<td>
131 K
</td>
<td>
131 K
</td>
<td>
524 K
</td>
<td>
8.4 M
</td>
<td>
16.8 M
</td>
<td>
[1, 256, 8, 8]
</td>
<td>
[1, 512, 8, 8]
</td>
</tr>
<tr>
<th>
92
</th>
<td>
Conv2d
</td>
<td>
2.4 M
</td>
<td>
2.4 M
</td>
<td>
9.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 512, 8, 8]
</td>
<td>
[1, 512, 8, 8]
</td>
</tr>
<tr>
<th>
97
</th>
<td>
Conv2d
</td>
<td>
2.4 M
</td>
<td>
2.4 M
</td>
<td>
9.4 M
</td>
<td>
150 M
</td>
<td>
301 M
</td>
<td>
[1, 512, 8, 8]
</td>
<td>
[1, 512, 8, 8]
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>We can see from the summary that the ResNet18-D model is about <code>45 MB</code> in size and needs to perform about <code>2.7 billion</code> floating-point operations to process a single <code>256x256</code> input image. For context, the larger mid-size ResNet50-D model is about <code>95 MB</code> and performs about <code>5.7 billion</code> floating-point ops for the same image. On the other end, the tiniest variant of the mobile-optimized MobileNetV3 model is <code>2.4 MB</code> and only takes about <code>30 million</code> floating-point operations.</p>
<p>That is valuable information when considering how we will deploy the fine-tuned model. For example, the in-browser demo I mentioned will first download the model to your local machine. The larger the model, the longer it will take for the demo to start. Likewise, the number of floating-point operations will influence what hardware can run the model smoothly. For real-time applications, even milliseconds can matter for inference speed. Inference involves making predictions with a trained model on new, unseen data.</p>
<p>The model architecture also influences inference speed beyond the raw number of floating-point operations. The MobileNetv3 architecture is tuned to mobile phone CPUs, while the ResNet architectures can better leverage larger GPUs.</p>
<p>That completes the model selection and setup. In the next section, we will prepare our dataset for training.</p>
</section>
</section>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">Preparing the Data</h2>
<p>Next, we will prepare our data for the model training process. The data preparation involves several steps, such as applying data augmentation techniques, setting up the train-validation split for the dataset, resizing and padding the images, defining the training dataset class, and initializing DataLoaders to feed data to the model.</p>
<section id="training-validation-split" class="level3">
<h3 class="anchored" data-anchor-id="training-validation-split">Training-Validation Split</h3>
<p>Let’s begin by splitting the dataset into training and validation sets. The model will use the training set to update its parameters, and we will use the validation set to evaluate the model’s performance on data it has not seen before. Validation sets are needed when training models because we want to verify the model can generalize well to new data before we deploy it.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the image paths</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>random.shuffle(img_paths)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the percentage of the images that should be used for training</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>train_pct <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>val_pct <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the index at which to split the subset of image paths into training and validation sets</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>train_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_paths)<span class="op">*</span>train_pct)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>val_split <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(img_paths)<span class="op">*</span>(train_pct<span class="op">+</span>val_pct))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the subset of image paths into training and validation sets</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>train_paths <span class="op">=</span> img_paths[:train_split]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>val_paths <span class="op">=</span> img_paths[train_split:]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of images in the training and validation sets</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Samples:"</span>: <span class="bu">len</span>(train_paths),</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Validation Samples:"</span>: <span class="bu">len</span>(val_paths)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_2871e">
<thead>
</thead>
<tbody>
<tr>
<th id="T_2871e_level0_row0" class="row_heading level0 row0">
Training Samples:
</th>
<td id="T_2871e_row0_col0" class="data row0 col0">
138361
</td>
</tr>
<tr>
<th id="T_2871e_level0_row1" class="row_heading level0 row1">
Validation Samples:
</th>
<td id="T_2871e_row1_col0" class="data row1 col0">
15374
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>Next, we’ll define what data augmentations to apply to images during training. Data augmentation is a technique that effectively expands the size and diversity of a dataset by creating variations of existing samples. It helps the model learn general features instead of memorizing specific examples.</p>
<p>We’ll use <a href="https://arxiv.org/abs/2103.10158">trivial augmentation</a>, which applies a single, random transform to each image. This simple method can be highly effective for data augmentation.</p>
<p>We will also use a couple of image transforms I made for resizing and padding images to make them a uniform size. Making all the input images the same size allows us to feed samples to the model in batches, allowing us to leverage our GPU more efficiently.</p>
<p>The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax">first</a> custom transform resizes images based on their largest dimension rather than their smallest. The <a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare">second</a> applies square padding and allows the padding to be applied equally on both sides or randomly split between the two sides.</p>
<section id="set-training-image-size" class="level4">
<h4 class="anchored" data-anchor-id="set-training-image-size">Set training image size</h4>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>train_sz <span class="op">=</span> <span class="dv">288</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initialize-image-transforms" class="level4">
<h4 class="anchored" data-anchor-id="initialize-image-transforms">Initialize image transforms</h4>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the fill color for padding images</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>fill <span class="op">=</span> (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `ResizeMax` object</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>resize_max <span class="op">=</span> ResizeMax(max_sz<span class="op">=</span>train_sz)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a `PadSquare` object</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>pad_square <span class="op">=</span> PadSquare(shift<span class="op">=</span><span class="va">True</span>, fill<span class="op">=</span>fill)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create a TrivialAugmentWide object</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>trivial_aug <span class="op">=</span> transforms.TrivialAugmentWide(fill<span class="op">=</span>fill)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can test the augmentations by applying them to a sample image and displaying the result. Remember, the augmentations should be different each time.</p>
</section>
<section id="test-the-transforms" class="level4">
<h4 class="anchored" data-anchor-id="test-the-transforms">Test the transforms</h4>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> Image.<span class="bu">open</span>(img_paths[<span class="dv">11</span>])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>sample_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_53_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Augment the image</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>augmented_img <span class="op">=</span> trivial_aug(sample_img)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize the image</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>resized_img <span class="op">=</span> resize_max(augmented_img)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad the image</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>padded_img <span class="op">=</span> pad_square(resized_img)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the padded image is the target size</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>resize <span class="op">=</span> transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>resized_padded_img <span class="op">=</span> resize(padded_img)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the annotated image</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>display(resized_padded_img)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Source Image:"</span>: sample_img.size,</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Image:"</span>: resized_img.size,</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Padded Image:"</span>: padded_img.size,</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Resized Padded Image:"</span>: resized_padded_img.size,</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>}).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_54_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_f017b">
<thead>
</thead>
<tbody>
<tr>
<th id="T_f017b_level0_row0" class="row_heading level0 row0">
Source Image:
</th>
<td id="T_f017b_row0_col0" class="data row0 col0">
(512, 399)
</td>
</tr>
<tr>
<th id="T_f017b_level0_row1" class="row_heading level0 row1">
Resized Image:
</th>
<td id="T_f017b_row1_col0" class="data row1 col0">
(287, 224)
</td>
</tr>
<tr>
<th id="T_f017b_level0_row2" class="row_heading level0 row2">
Padded Image:
</th>
<td id="T_f017b_row2_col0" class="data row2 col0">
(287, 287)
</td>
</tr>
<tr>
<th id="T_f017b_level0_row3" class="row_heading level0 row3">
Resized Padded Image:
</th>
<td id="T_f017b_row3_col0" class="data row3 col0">
(288, 288)
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="training-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="training-dataset-class">Training Dataset Class</h3>
<p>Next, we define a custom PyTorch Dataset class that will get used in a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a> to create batches. This class fetches a sample from the dataset at a given index and returns the transformed image and its corresponding label index.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageDataset(Dataset):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A PyTorch Dataset class for handling images.</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This class extends PyTorch's Dataset and is designed to work with image data. </span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">    It supports loading images, and applying transformations.</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">        img_paths (list): List of image file paths.</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">        class_to_idx (dict): Dictionary mapping class names to class indices.</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">        transforms (callable, optional): Transformations to be applied to the images.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_paths, class_to_idx, transforms<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Initializes the ImageDataset with image keys and other relevant information.</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co">            img_paths (list): List of image file paths.</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="co">            class_to_idx (dict): Dictionary mapping class names to class indices.</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="co">            transforms (callable, optional): Transformations to be applied to the images.</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Dataset, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_paths <span class="op">=</span> img_paths</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_to_idx <span class="op">=</span> class_to_idx</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._transforms <span class="op">=</span> transforms</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the number of items in the dataset.</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a><span class="co">            int: Number of items in the dataset.</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._img_paths)</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="co">        Retrieves an item from the dataset at the specified index.</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a><span class="co">            index (int): Index of the item to retrieve.</span></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a><span class="co">            tuple: A tuple containing the image and its corresponding label.</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>        img_path <span class="op">=</span> <span class="va">self</span>._img_paths[index]</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>        image, label <span class="op">=</span> <span class="va">self</span>._load_image(img_path)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Applying transformations if specified</span></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._transforms:</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>._transforms(image)</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _load_image(<span class="va">self</span>, img_path):</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Loads an image from the provided image path.</span></span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a><span class="co">            img_path (string): Image path.</span></span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a><span class="co">            Returns:</span></span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: A tuple containing the loaded image and its corresponding target data.</span></span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the image from the file path</span></span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(img_path).convert(<span class="st">'RGB'</span>)</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, <span class="va">self</span>._class_to_idx[img_path.parent.name]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-transforms" class="level3">
<h3 class="anchored" data-anchor-id="image-transforms">Image Transforms</h3>
<p>We’ll then define the transformations for the training and validation datasets. Note that we only apply data augmentation to the training dataset. Both datasets will have their images resized and padded and the pixel values normalized.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to resize and pad input images</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>resize_pad_tfm <span class="op">=</span> transforms.Compose([</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    resize_max, </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    pad_square,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    transforms.Resize([train_sz] <span class="op">*</span> <span class="dv">2</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms to sanitize bounding boxes and normalize input data</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>final_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    transforms.ToImage(), </span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    transforms.ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(<span class="op">*</span>norm_stats),</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the transformations for training and validation datasets</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Data augmentation is performed only on the training dataset</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>train_tfms <span class="op">=</span> transforms.Compose([</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    trivial_aug,</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    resize_pad_tfm, </span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    final_tfms</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>valid_tfms <span class="op">=</span> transforms.Compose([resize_pad_tfm, final_tfms])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initialize-datasets" class="level3">
<h3 class="anchored" data-anchor-id="initialize-datasets">Initialize Datasets</h3>
<p>We instantiate the PyTorch datasets using the dataset splits, class names, and defined transformations.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mapping from class names to class indices</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>class_to_idx <span class="op">=</span> {c: i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(class_names)}</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the dataset using the defined transformations</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> ImageDataset(train_paths, class_to_idx, train_tfms)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> ImageDataset(val_paths, class_to_idx, valid_tfms)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of samples in the training and validation datasets</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>pd.Series({</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Training dataset size:'</span>: <span class="bu">len</span>(train_dataset),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Validation dataset size:'</span>: <span class="bu">len</span>(valid_dataset)}</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>).to_frame().style.hide(axis<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table id="T_ada5f">
<thead>
</thead>
<tbody>
<tr>
<th id="T_ada5f_level0_row0" class="row_heading level0 row0">
Training dataset size:
</th>
<td id="T_ada5f_row0_col0" class="data row0 col0">
138361
</td>
</tr>
<tr>
<th id="T_ada5f_level0_row1" class="row_heading level0 row1">
Validation dataset size:
</th>
<td id="T_ada5f_row1_col0" class="data row1 col0">
15374
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="inspect-samples" class="level3">
<h3 class="anchored" data-anchor-id="inspect-samples">Inspect Samples</h3>
<p>Let’s inspect a sample from the training and validation sets to verify that the data preparation steps get applied correctly.</p>
<p><strong>Inspect training set sample</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the label for the first image in the training set</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>class_names[train_dataset[<span class="dv">0</span>][<span class="dv">1</span>]]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the first image in the training set</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>TF.to_pil_image(denorm_img_tensor(train_dataset[<span class="dv">0</span>][<span class="dv">0</span>], <span class="op">*</span>norm_stats))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Label: three</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_66_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p><strong>Inspect validation set sample</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the label for the first image in the validation set</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>class_names[valid_dataset[<span class="dv">0</span>][<span class="dv">1</span>]]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the first image in the validation set</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>TF.to_pil_image(denorm_img_tensor(valid_dataset[<span class="dv">0</span>][<span class="dv">0</span>], <span class="op">*</span>norm_stats))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Label: stop_inverted</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_68_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>We then define the batch size for training and initialize the <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">DataLoaders</a>, which are used to efficiently create batches of data for the model to process during training.</p>
</section>
<section id="training-batch-size" class="level3">
<h3 class="anchored" data-anchor-id="training-batch-size">Training Batch Size</h3>
<p>The batch size indicates how many sample images get fed to the model at once. The larger the batch size, the more GPU memory we need. The current batch size should be fine for most modern GPUs. If you still get an out-of-memory error, try lowering the batch size to <code>8</code>, then restart the Jupyter Notebook.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">32</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="initialize-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="initialize-dataloaders">Initialize DataLoaders</h3>
<p>Next, we initialize the DataLoaders for the training and validation datasets. We’ll set the number of worker processes for loading data to the number of available CPUs.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of worker processes for loading data. This should be the number of CPUs available.</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>num_workers <span class="op">=</span> multiprocessing.cpu_count()<span class="co">#//2</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters for DataLoader</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>data_loader_params <span class="op">=</span> {</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>: bs,  <span class="co"># Batch size for data loading</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num_workers'</span>: num_workers,  <span class="co"># Number of subprocesses to use for data loading</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'persistent_workers'</span>: <span class="va">True</span>,  <span class="co"># If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory'</span>: <span class="st">'cuda'</span> <span class="kw">in</span> device,  <span class="co"># If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pin_memory_device'</span>: device <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">''</span>,  <span class="co"># Specifies the device where the data should be loaded. Commonly set to use the GPU.</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for training data. Data is shuffled for every epoch.</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, <span class="op">**</span>data_loader_params, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoader for validation data. Shuffling is not necessary for validation data.</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>valid_dataloader <span class="op">=</span> DataLoader(valid_dataset, <span class="op">**</span>data_loader_params)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of batches in the training and validation DataLoaders</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of batches in train DataLoader: </span><span class="sc">{</span><span class="bu">len</span>(train_dataloader)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of batches in validation DataLoader: </span><span class="sc">{</span><span class="bu">len</span>(valid_dataloader)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Number of batches in train DataLoader: 4324
Number of batches in validation DataLoader: 481</code></pre>
<p>That completes the data preparation. Now we can finally train our hand gesture recognizer.</p>
</section>
</section>
<section id="fine-tuning-the-model" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-the-model">Fine-tuning the Model</h2>
<p>In this section, we will implement the training code and fine-tune our model. The training process revolves around the concept of an ‘epoch’. Each epoch represents one complete pass through the entire training and validation datasets. To help with this, we will define a function called <code>run_epoch</code> to handle a single training/validation epoch and one called <code>train_loop</code> to execute the main training loop.</p>
<section id="define-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="define-the-training-loop">Define the Training Loop</h3>
<p>Let’s start by defining the <code>run_epoch</code> function. This function runs a single training or validation epoch and calculates the loss and performance metric for the given dataset. The term ‘loss’ refers to a number representing how far our model’s predictions are from the actual values. The goal of training is to minimize this value. We use the <code>autocast</code> context manager to perform mixed precision training. Mixed-precision training involves performing some operations in <code>16-bit</code> precision to speed up training and reduce memory requirements. Modern GPUs tend to have specialized hardware to accelerate these lower-precision operations, and this feature allows us to utilize that.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to run a single training/validation epoch</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_epoch(model, dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch_id, is_training):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set model to training mode if 'is_training' is True, else set to evaluation mode</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    model.train() <span class="cf">if</span> is_training <span class="cf">else</span> model.<span class="bu">eval</span>()</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reset the performance metric</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    metric.reset()</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the average loss for the current epoch </span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize progress bar with total number of batches in the dataloader</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    progress_bar <span class="op">=</span> tqdm(total<span class="op">=</span><span class="bu">len</span>(dataloader), desc<span class="op">=</span><span class="st">"Train"</span> <span class="cf">if</span> is_training <span class="cf">else</span> <span class="st">"Eval"</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over data batches</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_id, (inputs, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move inputs and targets to the specified device (e.g., GPU)</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Enables gradient calculation if 'is_training' is True</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.set_grad_enabled(is_training):</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Automatic Mixed Precision (AMP) context manager for improved performance</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> autocast(torch.device(device).<span class="bu">type</span>):</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs) <span class="co"># Forward pass</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> torch.nn.functional.cross_entropy(outputs, targets) <span class="co"># Compute loss</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the performance metric</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>        metric.update(outputs.detach().cpu(), targets.detach().cpu())</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If in training mode</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scaler:</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>                scaler.scale(loss).backward()</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>                scaler.step(optimizer)</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>                old_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>                scaler.update()</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>                new_scaler <span class="op">=</span> scaler.get_scale()</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> new_scaler <span class="op">&gt;=</span> old_scaler:</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>                    lr_scheduler.step()</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>                lr_scheduler.step()</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>        loss_item <span class="op">=</span> loss.item()</span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss_item</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update progress bar</span></span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>        progress_bar.set_postfix(accuracy<span class="op">=</span>metric.compute().item(), </span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>                                 loss<span class="op">=</span>loss_item, </span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a>                                 avg_loss<span class="op">=</span>epoch_loss<span class="op">/</span>(batch_id<span class="op">+</span><span class="dv">1</span>), </span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>                                 lr<span class="op">=</span>lr_scheduler.get_last_lr()[<span class="dv">0</span>] <span class="cf">if</span> is_training <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>        progress_bar.update()</span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If loss is NaN or infinity, stop training</span></span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a>            stop_training_message <span class="op">=</span> <span class="ss">f"Loss is NaN or infinite at epoch </span><span class="sc">{</span>epoch_id<span class="sc">}</span><span class="ss">, batch </span><span class="sc">{</span>batch_id<span class="sc">}</span><span class="ss">. Stopping training."</span></span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="kw">not</span> math.isnan(loss_item) <span class="kw">and</span> math.isfinite(loss_item), stop_training_message</span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a>    progress_bar.close()</span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> (batch_id <span class="op">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function performs one pass through the given dataset. It first sets the model to training or evaluation mode depending on whether we are training or validating. Then, for each batch of data, it performs a forward pass (calculating the predictions of the model), computes the loss, and then, if in training mode, performs a backward pass to adjust the model’s parameters.</p>
<p>Next, we’ll define the <code>train_loop</code> function, which executes the main training loop. It iterates over each epoch, runs through the training and validation sets, and saves the best model based on the validation loss.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Main training loop</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, train_dataloader, valid_dataloader, optimizer, metric, lr_scheduler, device, epochs, checkpoint_path, use_scaler<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> GradScaler() <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span> <span class="kw">and</span> use_scaler <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    best_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over each epoch</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs), desc<span class="op">=</span><span class="st">"Epochs"</span>):</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run training epoch and compute training loss</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> run_epoch(model, train_dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch, is_training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run validation epoch and compute validation loss</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>            valid_loss <span class="op">=</span> run_epoch(model, valid_dataloader, <span class="va">None</span>, metric, <span class="va">None</span>, device, scaler, epoch, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If current validation loss is lower than the best one so far, save model and update best loss</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> valid_loss <span class="op">&lt;</span> best_loss:</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>            best_loss <span class="op">=</span> valid_loss</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>            metric_value <span class="op">=</span> metric.compute().item()</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), checkpoint_path)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>            training_metadata <span class="op">=</span> {</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>                <span class="st">'epoch'</span>: epoch,</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>                <span class="st">'train_loss'</span>: train_loss,</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>                <span class="st">'valid_loss'</span>: valid_loss, </span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>                <span class="st">'metric_value'</span>: metric_value,</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>                <span class="st">'learning_rate'</span>: lr_scheduler.get_last_lr()[<span class="dv">0</span>],</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>                <span class="st">'model_architecture'</span>: model.name</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Save best_loss and metric_value in a JSON file</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> <span class="bu">open</span>(Path(checkpoint_path.parent<span class="op">/</span><span class="st">'training_metadata.json'</span>), <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>                json.dump(training_metadata, f)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the device is a GPU, empty the cache</span></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device.<span class="bu">type</span> <span class="op">!=</span> <span class="st">'cpu'</span>:</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">getattr</span>(torch, device.<span class="bu">type</span>).empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function coordinates the training process. It runs the previously defined <code>run_epoch</code> function for each epoch in the training process, calculating the training and validation losses. It saves the model state as a checkpoint when the model achieves a lower validation loss. It will also save data such as the current epoch, loss values, metric value, learning rate, and model name to a JSON file.</p>
</section>
<section id="set-the-model-checkpoint-path" class="level3">
<h3 class="anchored" data-anchor-id="set-the-model-checkpoint-path">Set the Model Checkpoint Path</h3>
<p>Before we proceed with training, let’s generate a timestamp for the training session and create a directory to store the checkpoints. These checkpoints will allow us to save the model state periodically. That enables us to load the model checkpoint later to resume training, export the model to a different format or perform inference directly.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>timestamp <span class="op">=</span> datetime.datetime.now().strftime(<span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st">_%H-%M-%S"</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a directory to store the checkpoints if it does not already exist</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>checkpoint_dir <span class="op">=</span> Path(project_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the checkpoint directory if it does not already exist</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>checkpoint_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The model checkpoint path</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>checkpoint_path <span class="op">=</span> checkpoint_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>model<span class="sc">.</span>name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(checkpoint_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pytorch-timm-image-classifier/2024-02-01_20-28-44/resnet18d.ra2_in1k.pth</code></pre>
</section>
<section id="saving-the-class-labels" class="level3">
<h3 class="anchored" data-anchor-id="saving-the-class-labels">Saving the Class Labels</h3>
<p>Let’s save the dataset class labels in a dedicated JSON file so we don’t need to load the whole dataset to make predictions with the model in the future. I cover how to load the model checkpoint we saved earlier and use it for inference in a <a href="./onnx-export/">follow-up tutorial</a>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save class labels</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> {<span class="st">"classes"</span>: <span class="bu">list</span>(class_names)}</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set file path</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>class_labels_path <span class="op">=</span> checkpoint_dir<span class="op">/</span><span class="ss">f"</span><span class="sc">{</span>dataset_name<span class="sc">}</span><span class="ss">-classes.json"</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save class labels in JSON format</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(class_labels_path, <span class="st">"w"</span>) <span class="im">as</span> write_file:</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    json.dump(class_labels, write_file)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_labels_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>pytorch-timm-image-classifier/2024-02-01_20-28-44/hagrid-classification-512p-no-gesture-150k-zip-classes.json</code></pre>
</section>
<section id="configure-the-training-parameters" class="level3">
<h3 class="anchored" data-anchor-id="configure-the-training-parameters">Configure the Training Parameters</h3>
<p>Now, let’s configure the parameters for training. We’ll define the learning rate, number of training epochs, optimizer, learning rate scheduler, and performance metric. The learning rate determines how much we adjust the model in response to the estimated error each time the weights are updated. Choosing an optimal learning rate is essential for good model performance.</p>
<p>We’re using <a href="https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html">AdamW</a> as our optimizer, which includes weight decay for regularization, and the <a href="https://sgugger.github.io/the-1cycle-policy.html">OneCycleLR</a> scheduler to adjust the learning rate during training. The one-cycle learning rate policy is a training approach where the learning rate starts low, increases gradually to a maximum, then decreases again, all within a single iteration or epoch, aiming to converge faster and yield better performance.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate for the model</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of training epochs</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW optimizer; includes weight decay for regularization</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>lr, eps<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate scheduler; adjusts the learning rate during training</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer, </span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>                                                   max_lr<span class="op">=</span>lr, </span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>                                                   total_steps<span class="op">=</span>epochs<span class="op">*</span><span class="bu">len</span>(train_dataloader))</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Performance metric: Multiclass Accuracy</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>metric <span class="op">=</span> MulticlassAccuracy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ll use <a href="https://pytorch.org/torcheval/stable/generated/torcheval.metrics.MulticlassAccuracy.html#torcheval.metrics.MulticlassAccuracy">Multiclass Accuracy</a> for our performance metric as this is a multiclass classification problem where each image falls into one of many classes.</p>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">Train the Model</h3>
<p>Finally, we can train the model using the <code>train_loop</code> function. Training time will depend on the available hardware. Feel free to take a break if the progress bar indicates it will take a while.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Google Colab Users
</div>
</div>
<div class="callout-body-container callout-body">
<p>Training usually takes around 1 hour and 20 minutes on the free GPU tier of Google Colab.</p>
</div>
</div>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>train_loop(model<span class="op">=</span>model, </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>           train_dataloader<span class="op">=</span>train_dataloader, </span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>           valid_dataloader<span class="op">=</span>valid_dataloader, </span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>           optimizer<span class="op">=</span>optimizer, </span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>           metric<span class="op">=</span>metric, </span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>           lr_scheduler<span class="op">=</span>lr_scheduler, </span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>           device<span class="op">=</span>torch.device(device), </span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>           epochs<span class="op">=</span>epochs, </span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>           checkpoint_path<span class="op">=</span>checkpoint_path, </span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>           use_scaler<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Epochs: 100% 3/3 [08:29&lt;00:00, 169.16s/it]
Train: 100% |█████████| 4324/4324 [02:39&lt;00:00, 45.49it/s, accuracy=0.856, avg_loss=0.497, loss=0.371, lr=0.000994]
Eval: 100% 481/481 [00:14&lt;00:00, 59.90it/s, accuracy=0.968, avg_loss=0.101, loss=0.174, lr=]
Train: 100% |█████████| 4324/4324 [02:33&lt;00:00, 44.08it/s, accuracy=0.94, avg_loss=0.191, loss=0.103, lr=0.000463]
Eval: 100% 481/481 [00:14&lt;00:00, 63.44it/s, accuracy=0.988, avg_loss=0.0381, loss=0.0357, lr=]
Train: 100% |█████████| 4324/4324 [02:33&lt;00:00, 45.00it/s, accuracy=0.975, avg_loss=0.0789, loss=0.168, lr=4.48e-9]
Eval: 100% 481/481 [00:14&lt;00:00, 65.80it/s, accuracy=0.995, avg_loss=0.0172, loss=0.00552, lr=]</code></pre>
<p>At last, we have our hand gesture recognizer. The readout for the final validation run indicates the model achieved an approximate <code>99.5%</code> accuracy, meaning it missed less than <code>100</code> of the <code>15,374</code> samples in the validation set. To wrap up the tutorial, we’ll test our fine-tuned model by performing inference on individual images.</p>
</section>
</section>
<section id="making-predictions-with-the-model" class="level2">
<h2 class="anchored" data-anchor-id="making-predictions-with-the-model">Making Predictions with the Model</h2>
<p>In this final part of the tutorial, you will learn how to make predictions with the fine-tuned model on individual images, allowing us to see the model in action and assess its performance. Understanding how to apply trained models is crucial for implementing them in real-world applications.</p>
<p>Let’s start by selecting an image from the validation set.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose an item from the validation set</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>test_file <span class="op">=</span> val_paths[<span class="dv">0</span>]</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Open the test file</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(test_file).convert(<span class="st">'RGB'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ll use the same size as the input images we used during training but allow non-square input. The <a href="https://cj-mills.github.io/cjm-pil-utils/core.html#resize_img"><code>resize_img</code></a> function will scale the image so the smallest dimension is the specified inference size while maintaining the original aspect ratio.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the minimum input dimension for inference </span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> resize_img(test_img, target_sz<span class="op">=</span>train_sz, divisor<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We then convert the image to a normalized tensor using the <a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#pil_to_tensor"><code>pil_to_tensor</code></a> function and move it to the device where our model resides (CPU or GPU).</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to a normalized tensor and move it to the device</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>img_tensor <span class="op">=</span> pil_to_tensor(input_img, <span class="op">*</span>norm_stats).to(device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With our image prepared, we can now use our model to make a prediction. The following code block runs our model in a no-gradient context using <code>torch.no_grad()</code>. That informs PyTorch that we do not need to keep track of gradients in this operation, saving memory.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a prediction with the model</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model(img_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After obtaining the raw prediction, we apply the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">Softmax</a> function to convert these values into probabilities that sum up to 1.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the model predictions to add up to 1</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>pred_scores <span class="op">=</span> torch.softmax(pred, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then, we retrieve the highest confidence score and its corresponding class index. The class index is converted into the actual class name.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the highest confidence score</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>confidence_score <span class="op">=</span> pred_scores.<span class="bu">max</span>()</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the class index with the highest confidence score and convert it to the class name</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>pred_class <span class="op">=</span> class_names[torch.argmax(pred_scores)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We then format the prediction data as a <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">Pandas Dataframe</a>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>display(test_img)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted Class: </span><span class="sc">{</span>pred_class<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas DataFrame for easy formatting</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>confidence_score_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Confidence Score'</span>:{</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        name:<span class="ss">f'</span><span class="sc">{</span>score<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span> <span class="cf">for</span> name, score <span class="kw">in</span> <span class="bu">zip</span>(class_names, pred_scores.cpu().numpy()[<span class="dv">0</span>])</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>confidence_score_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_87_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>Predicted Class: stop_inverted</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Confidence Score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
call
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
four
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
like
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
no_gesture
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
one
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
100.00%
</td>
</tr>
<tr>
<th>
three
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
0.00%
</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>For this sample, the model was 100% confident in its prediction. The model will likely be less sure about images it has not seen before.</p>
<section id="testing-the-model-on-new-data" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-model-on-new-data">Testing the Model on New Data</h3>
<p>Let’s try some new images from the free stock photo site, <a href="https://www.pexels.com">Pexels</a>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>file_name <span class="op">=</span> <span class="st">'pexels-elina-volkova-16191659.jpg'</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-joshua-roberts-12922530.jpg'</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-luke-barky-2899727.jpg'</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-ketut-subiyanto-4584599.jpg'</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-nataliya-vaitkevich-5411990.jpg'</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-darina-belonogova-7886753.jpg'</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-katrin-bolovtsova-6706013.jpg'</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-leo-vinicius-3714450.jpg'</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># file_name = 'pexels-diva-plavalaguna-6937816.jpg'</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>test_img_url <span class="op">=</span> <span class="ss">f"https://huggingface.co/datasets/cj-mills/pexel-hand-gesture-test-images/resolve/main/</span><span class="sc">{</span>file_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>test_img_path <span class="op">=</span> Path(file_name)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> test_img_path.is_file():</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Image already exists."</span>)</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    urllib.request.urlretrieve(test_img_url, test_img_path)</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Image downloaded."</span>)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>test_img <span class="op">=</span> Image.<span class="bu">open</span>(test_img_path)</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>test_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="text"><code>Image already exists.</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_89_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the minimum input dimension for inference </span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>infer_sz <span class="op">=</span> train_sz</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>inp_img <span class="op">=</span> resize_img(test_img.copy(), infer_sz)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to a normalized tensor and move it to the device</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>img_tensor <span class="op">=</span> pil_to_tensor(inp_img, <span class="op">*</span>norm_stats).to(device<span class="op">=</span>device)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a prediction with the model</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model(img_tensor)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the model predictions to add up to 1</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>pred_scores <span class="op">=</span> torch.softmax(pred, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the highest confidence score</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>confidence_score <span class="op">=</span> pred_scores.<span class="bu">max</span>()</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the class index with the highest confidence score and convert it to the class name</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>pred_class <span class="op">=</span> class_names[torch.argmax(pred_scores)]</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>display(test_img)</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted Class: </span><span class="sc">{</span>pred_class<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the prediction data as a Pandas DataFrame for easy formatting</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>confidence_score_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Confidence Score'</span>:{</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>        name:<span class="ss">f'</span><span class="sc">{</span>score<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span> <span class="cf">for</span> name, score <span class="kw">in</span> <span class="bu">zip</span>(class_names, pred_scores.cpu().numpy()[<span class="dv">0</span>])</span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>confidence_score_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Mute</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Call</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-3" role="tab" aria-controls="tabset-3-3" aria-selected="false">No Gesture/Rock</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/output_90_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>Predicted Class: mute</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Confidence Score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
call
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
four
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
like
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
100.00%
</td>
</tr>
<tr>
<th>
no_gesture
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
one
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
three
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
0.00%
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
0.00%
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/call.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>Predicted Class: call</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Confidence Score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
call
</th>
<td>
83.18%
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
7.98%
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
0.09%
</td>
</tr>
<tr>
<th>
four
</th>
<td>
0.01%
</td>
</tr>
<tr>
<th>
like
</th>
<td>
2.98%
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
0.09%
</td>
</tr>
<tr>
<th>
no_gesture
</th>
<td>
0.40%
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
0.28%
</td>
</tr>
<tr>
<th>
one
</th>
<td>
1.38%
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
0.17%
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
0.07%
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
0.19%
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
2.52%
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
0.08%
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
0.07%
</td>
</tr>
<tr>
<th>
three
</th>
<td>
0.02%
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
0.07%
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
0.08%
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
0.31%
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tabset-3-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-3-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/rock.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<pre class="text"><code>Predicted Class: no_gesture</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Confidence Score
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
call
</th>
<td>
0.78%
</td>
</tr>
<tr>
<th>
dislike
</th>
<td>
1.33%
</td>
</tr>
<tr>
<th>
fist
</th>
<td>
0.07%
</td>
</tr>
<tr>
<th>
four
</th>
<td>
0.11%
</td>
</tr>
<tr>
<th>
like
</th>
<td>
0.18%
</td>
</tr>
<tr>
<th>
mute
</th>
<td>
0.49%
</td>
</tr>
<tr>
<th>
no_gesture
</th>
<td>
60.31%
</td>
</tr>
<tr>
<th>
ok
</th>
<td>
0.84%
</td>
</tr>
<tr>
<th>
one
</th>
<td>
0.34%
</td>
</tr>
<tr>
<th>
palm
</th>
<td>
0.03%
</td>
</tr>
<tr>
<th>
peace
</th>
<td>
0.34%
</td>
</tr>
<tr>
<th>
peace_inverted
</th>
<td>
2.44%
</td>
</tr>
<tr>
<th>
rock
</th>
<td>
31.91%
</td>
</tr>
<tr>
<th>
stop
</th>
<td>
0.01%
</td>
</tr>
<tr>
<th>
stop_inverted
</th>
<td>
0.02%
</td>
</tr>
<tr>
<th>
three
</th>
<td>
0.06%
</td>
</tr>
<tr>
<th>
three2
</th>
<td>
0.53%
</td>
</tr>
<tr>
<th>
two_up
</th>
<td>
0.02%
</td>
</tr>
<tr>
<th>
two_up_inverted
</th>
<td>
0.20%
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>The model confidently predicted the image class for the <code>mute</code> and <code>call</code> images but was uncertain about the last image. It gave a <code>60.31%</code> probability to the <code>no_gesture</code> class and a <code>31.91%</code> score to the <code>rock</code> class.</p>
<p>The hand gesture closely resembles the <code>rock</code> gesture samples from the dataset but is different enough to make the prediction uncertainty understandable. Training with a different random seed can cause the model to more confidently pick the <code>rock</code> class over the <code>no_gesture</code> class.</p>
<hr>
<p>We now have a functioning hand-gesture recognizer and know how to make predictions with it on individual images. Before we wrap up this tutorial, let’s check out the in-browser demo I mentioned at the beginning of the post.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Google Colab Users
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Don’t forget to download the model checkpoint and class labels from the Colab Environment’s file browser. (<a href="https://christianjmills.com/posts/google-colab-getting-started-tutorial/#working-with-data">tutorial link</a>)</li>
<li>Once you finish training and download the files, turn off hardware acceleration for the Colab Notebook to save GPU time. (<a href="https://christianjmills.com/posts/google-colab-getting-started-tutorial/#using-hardware-acceleration">tutorial link</a>)</li>
</ol>
</div>
</div>
</section>
</section>
<section id="exploring-the-in-browser-demo" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-in-browser-demo">Exploring the In-Browser Demo</h2>
<p>You’ve gotten your hands dirty with the code. Now let’s see our fine-tuned model in action! I’ve set up an online demo that allows you to interact with a hand gesture recognizer trained with this tutorial’s code in your web browser. No downloads or installations are required.</p>
<p>The demo includes sample images that you can use to test the model. Try these images first to see how the model interprets different hand gestures. Once you’re ready, you can switch on your webcam to provide live input to the model.</p>
<p>Online demos are a great way to see and share the fruits of your labor and explore ways to apply your hand gesture recognizer in real-world scenarios.</p>
<p>I invite you to share any interesting results or experiences with the demo in the comments below. Whether it’s a tricky input image the model handles or a surprising failure case, I’d love to hear about it!</p>
<p>Check out the demo below, and have fun exploring!</p>
<ul>
<li><a href="https://cj-mills.github.io/pytorch-timm-gesture-recognition-tutorial-code/">In-Browser Hand Gesture Recognition Demo</a></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Congratulations on completing this tutorial on fine-tuning image classifiers with PyTorch and the timm library! You’ve taken significant strides in your machine learning journey by creating a practical hand gesture recognizer.</p>
<p>Throughout this tutorial, we’ve covered many topics, including setting up your Python environment, importing necessary dependencies, project initialization, dataset loading and exploration, model selection, data preparation, and model fine-tuning. Finally, we made predictions with our fine-tuned model on individual images and tested the model with an interactive, in-browser demo.</p>
<p>This hands-on tutorial underscored the practical applications of fine-tuning image classification models, especially when working with limited data and computational resources. The hand gesture recognizer you’ve built has many real-world applications, and you now have a solid foundation to tackle other image classification tasks.</p>
<p>If you’re intrigued by the underlying concepts leveraged in this tutorial and wish to deepen your understanding, I recommend fast.ai’s <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a> course. By the end, you’ll thoroughly understand the model and training code and have the know-how to implement them from scratch.</p>
<p>While our tutorial concludes here, your journey in deep learning is far from over. In the upcoming tutorials, we’ll explore topics such as incorporating preprocessing and post-processing steps into the model, exporting the model to different formats for deployment, using the fine-tuned model to identify flawed training samples in our dataset, and building interactive in-browser demo projects similar to the one featured in this tutorial.</p>
<p>Once again, congratulations on your achievement, and keep learning!</p>
</section>
<section id="recommended-tutorials" class="level2">
<h2 class="anchored" data-anchor-id="recommended-tutorials">Recommended Tutorials</h2>
<ul>
<li><a href="./onnx-export/"><strong>Exporting timm Image Classifiers from Pytorch to ONNX</strong></a><strong>:</strong> Learn how to export timm image classification models from PyTorch to ONNX and perform inference using ONNX Runtime.</li>
<li><a href="../pytorch-train-object-detector-yolox-tutorial/"><strong>Training YOLOX Models for Real-Time Object Detection in Pytorch</strong></a>: Learn how to train YOLOX models for real-time object detection in PyTorch by creating a hand gesture detection model.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Questions:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Questions:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Feel free to post questions or problems related to this tutorial in the comments below. I try to make time to address them on Thursdays and Fridays.</li>
</ul>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-tip callout-titled" title="About Me:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Me:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>I’m Christian Mills, a deep learning consultant specializing in computer vision and practical AI implementations.</li>
<li>I help clients leverage cutting-edge AI technologies to solve real-world problems.</li>
<li>Learn more <a href="../../about.html">about me</a> or reach out via email at <a href="mailto:christian@christianjmills.com">christian@christianjmills.com</a> to discuss your project.</li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<!-- Cloudflare Web Analytics --><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;56b8d2f624604c4891327b3c0d9f6703&quot;}"></script><!-- End Cloudflare Web Analytics -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/christianjmills\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="cj-mills/christianjmills" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<p>Content licensed under CC BY-NC-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>© 2024 Christian J. Mills</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://opensource.org/licenses/MIT">
<p>Code samples licensed under the MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>