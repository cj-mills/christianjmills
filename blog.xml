<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Christian Mills</title>
<link>christianjmills.com/blog.html</link>
<atom:link href="christianjmills.com/blog.xml" rel="self" type="application/rss+xml"/>
<description>This is Christian Mills&#39; personal Blog.</description>
<image>
<url>christianjmills.com/images/logo.png</url>
<title>Christian Mills</title>
<link>christianjmills.com/blog.html</link>
<height>144</height>
<width>144</width>
</image>
<generator>quarto-1.4.555</generator>
<lastBuildDate>Fri, 19 Jul 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Conference Talk 9: Why Fine-Tuning is Dead</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-009/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Trends in Machine Learning</li>
<li>Performance Observations: Fine-tuning vs.&nbsp;RAG</li>
<li>The Moving Target: Fine-tuning and Frontier Models</li>
<li>The Difficulty of Fine-tuning: Prioritizing Fundamentals</li>
<li>Extrapolating Trends: Context Size, Price, and Latency</li>
<li>Conclusion</li>
<li>Q&amp;A Session</li>
</ul>
<section id="trends-in-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="trends-in-machine-learning">Trends in Machine Learning</h2>
<ul>
<li><strong>Focus on fundamentals:</strong> Throughout ML history, focusing on “boring” fundamentals like data quality and SQL queries often yielded better results than chasing the latest “cool” technology.
<ul>
<li><strong>2009:</strong> Data analysis and SQL were more impactful than training ML models.</li>
<li><strong>2012-2014:</strong> XGBoost outperformed deep learning for many tasks.</li>
<li><strong>2015:</strong> Data cleaning and error correction were more effective than inventing new loss functions.</li>
<li><strong>2023:</strong> Prompting and RAG often outperform fine-tuning.</li>
</ul></li>
<li><strong>Fine-tuning’s uncertain future:</strong> While fine-tuning became popular with the rise of pre-trained models like ResNet and BERT, LLMs might be shifting the paradigm again towards prompt-based and RAG-augmented approaches.</li>
</ul>
<section id="qa-on-trends-and-embedding-models" class="level3">
<h3 class="anchored" data-anchor-id="qa-on-trends-and-embedding-models">Q&amp;A on Trends and Embedding Models</h3>
<ul>
<li><strong>Fine-tuning’s future relevance:</strong> Fine-tuning might still become more valuable in the future, just like deep learning did after an initial period of limited practical use.</li>
<li><strong>Embedding models:</strong> While LLMs are currently the focus of improvement, embedding models might also see advancements that reduce the need for fine-tuning. However, domain-specific applications might still require fine-tuning or hybrid approaches like combining keyword search with embedding search.</li>
<li><strong>Domain-specific ranking and retrieval:</strong> RAG, combined with techniques like query expansion driven by LLMs, might be able to address the need for domain-specific ranking and retrieval without fine-tuning embedding models.</li>
<li><strong>Benchmarking prompting vs.&nbsp;fine-tuning:</strong> Limited data exists comparing prompting and RAG to fine-tuning, but existing research suggests RAG often outperforms fine-tuning.</li>
</ul>
</section>
</section>
<section id="performance-observations-fine-tuning-vs.-rag" class="level2">
<h2 class="anchored" data-anchor-id="performance-observations-fine-tuning-vs.-rag">Performance Observations: Fine-tuning vs.&nbsp;RAG</h2>
<ul>
<li><strong>RAG often outperforms fine-tuning:</strong> Several studies indicate that RAG, even without fine-tuning, often achieves comparable or better performance than fine-tuning alone, especially for larger models and knowledge-based tasks.
<ul>
<li><strong>Forum Post:</strong> <a href="https://community.openai.com/t/fine-tuning-vs-context-injection-rag/550286/1">Fine-tuning vs Context-Injection (RAG)</a></li>
</ul></li>
<li><strong>Prioritization over “versus”:</strong> While combining RAG and fine-tuning can yield incremental improvements, prioritizing RAG is crucial due to its efficiency and effectiveness.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2403.01432">Fine Tuning vs.&nbsp;Retrieval Augmented Generation for Less Popular Knowledge</a></li>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2312.05934">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</a></li>
</ul></li>
<li><strong>Fine-tuning for style, not knowledge:</strong> Fine-tuning might be less suitable for incorporating domain knowledge into a model compared to RAG, which directly provides relevant context.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2401.08406">RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture</a></li>
</ul></li>
<li><strong>The evolving definition of “knowledge”:</strong> Distinguishing between “knowledge” and “style” in LLMs is complex and changes with each model generation. What previously required fine-tuning (e.g., writing style) might be achievable through prompting in newer models.</li>
</ul>
<section id="qa-on-fine-tuning-rag-and-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="qa-on-fine-tuning-rag-and-knowledge">Q&amp;A on Fine-tuning, RAG, and Knowledge</h3>
<ul>
<li><strong>Fine-tuning effectiveness and model size:</strong> Fine-tuning might be more beneficial for smaller models compared to larger ones, which are already more capable of learning from context.</li>
<li><strong>Domain-specific knowledge and RAG:</strong> Even for smaller models, RAG remains crucial for tasks involving domain-specific knowledge.</li>
<li><strong>Evaluating fine-tuning success:</strong> The choice of evaluation metric significantly impacts the perceived effectiveness of fine-tuning. For tasks like style adherence, fine-tuning might appear more beneficial than RAG.</li>
<li><strong>The blurry line between style and content:</strong> The distinction between “style” and “content” can be ambiguous, making it difficult to definitively determine when fine-tuning is beneficial.</li>
</ul>
</section>
<section id="audience-questions-and-examples-of-fine-tuning-success" class="level3">
<h3 class="anchored" data-anchor-id="audience-questions-and-examples-of-fine-tuning-success">Audience Questions and Examples of Fine-tuning Success</h3>
<ul>
<li><strong>Complex knowledge bases and fine-tuning:</strong> When dealing with large, curated knowledge bases, it’s crucial to evaluate whether the next generation of LLMs, combined with RAG, might be sufficient without fine-tuning.</li>
<li><strong>Adding knowledge via prompting and RAG:</strong> In many cases, adding knowledge to the model can be achieved through prompting, RAG, or a combination of both, eliminating the need for fine-tuning.</li>
<li><strong>Fine-tuning for multilingual models:</strong> Fine-tuning might be beneficial for improving the performance of multilingual models on languages with limited training data, as it leverages the model’s existing understanding of language mapping.</li>
<li><strong>Fine-tuning for code generation:</strong> While fine-tuning can be used to adapt code generation models to specific styles and conventions, RAG remains highly effective for providing codebase context.</li>
<li><strong>Contextual learning vs.&nbsp;fine-tuning:</strong> LLMs are demonstrating impressive abilities to learn from context, potentially replacing the need for fine-tuning in scenarios where sufficient context can be provided.</li>
</ul>
</section>
</section>
<section id="the-moving-target-fine-tuning-and-frontier-models" class="level2">
<h2 class="anchored" data-anchor-id="the-moving-target-fine-tuning-and-frontier-models">The Moving Target: Fine-tuning and Frontier Models</h2>
<ul>
<li><strong>Rapid LLM advancements:</strong> The rapid pace of LLM development makes fine-tuning a moving target, as newer models often surpass the performance of previously fine-tuned models.</li>
<li><strong>Bloomberg GPT example:</strong> Bloomberg GPT, a large language model pre-trained on financial data, initially outperformed existing models on financial tasks. However, its performance was subsequently surpassed by newer models like GPT-4.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2303.17564">BloombergGPT: A Large Language Model for Finance</a></li>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2305.05862">Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks</a></li>
</ul></li>
<li><strong>The cost of keeping up:</strong> Continuously fine-tuning on new model releases can be prohibitively expensive, especially for large datasets. Prompt-based and RAG-based pipelines offer more flexibility and cost-effectiveness.</li>
<li><strong>Fine-tuning effectiveness and model scale:</strong> Fine-tuning might become less effective as models grow larger and more capable.</li>
</ul>
</section>
<section id="the-difficulty-of-fine-tuning-prioritizing-fundamentals" class="level2">
<h2 class="anchored" data-anchor-id="the-difficulty-of-fine-tuning-prioritizing-fundamentals">The Difficulty of Fine-tuning: Prioritizing Fundamentals</h2>
<ul>
<li><strong>The 80/20 rule of ML:</strong> Similar to traditional ML, most effort in LLM development should be dedicated to data work (80%), followed by engineering (18%), debugging (2%), and architecture research (0%).</li>
<li><strong>Fine-tuning as a last resort:</strong> Fine-tuning should only be considered after thoroughly addressing fundamentals like data quality, evaluation, prompting, and RAG.</li>
<li><strong>Hierarchy of needs:</strong> Prioritize building a solid ML system with robust evaluation, prompting, and RAG before attempting to fine-tune.
<ul>
<li><strong>Book:</strong> <a href="https://www.mlpowered.com/book/">Building Machine Learning Powered Applications: Going from Idea to Product</a></li>
<li><strong>Continuous Integration</strong>
<ul>
<li>Model Backtesting</li>
<li>Model Evaluation</li>
<li>Experimentation Framework</li>
</ul></li>
<li><strong>Application Logic</strong>
<ul>
<li>Input Validation → Filtering Logic → Model Code → Output Validation → Displaying Logic</li>
</ul></li>
<li><strong>Monitoring</strong>
<ul>
<li>Monitoring Input Distribution</li>
<li>Monitoring Latency</li>
<li>Monitoring Output Distribution</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="extrapolating-trends-context-size-price-and-latency" class="level2">
<h2 class="anchored" data-anchor-id="extrapolating-trends-context-size-price-and-latency">Extrapolating Trends: Context Size, Price, and Latency</h2>
<ul>
<li><strong>Decreasing costs and increasing capabilities:</strong> LLM costs are rapidly decreasing, while context windows and processing speeds are increasing.</li>
<li><strong>The impact of future trends:</strong> If these trends continue, providing sufficient context to highly capable LLMs might become more efficient than fine-tuning for many use cases.</li>
<li><strong>Context window limitations:</strong> Fine-tuning might still be necessary for applications requiring context exceeding the limits of available LLMs. However, techniques like prefix caching could mitigate this need.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li><strong>Finetuning is:</strong>
<ul>
<li>expensive and complex</li>
<li>has become less valuable</li>
<li>often underperforms simpler approaches</li>
</ul></li>
<li><strong>Models are continuously becoming:</strong>
<ul>
<li>cheaper</li>
<li>smarter</li>
<li>faster</li>
<li>longer context</li>
</ul></li>
<li><strong>Always start with:</strong>
<ul>
<li>prompting</li>
<li>making a train/test set</li>
<li>RAG</li>
</ul></li>
<li><strong>Treat finetuning as a niche/last resort solution</strong>
<ul>
<li>like cloud vs on prem</li>
</ul></li>
</ul>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="question-1" class="level3">
<h3 class="anchored" data-anchor-id="question-1">Question #1</h3>
<ul>
<li><strong>Context window size and computational cost:</strong> Passing large amounts of context through an LLM for every request can be computationally expensive. However, increasing LLM efficiency and advancements like prefix caching could mitigate this cost.</li>
<li><strong>Fine-tuning complexity:</strong> Fine-tuning increasingly complex and larger LLMs might become more challenging, potentially outweighing the benefits compared to context-based approaches.</li>
</ul>
</section>
<section id="question-2" class="level3">
<h3 class="anchored" data-anchor-id="question-2">Question #2</h3>
<ul>
<li><strong>Dynamic few-shot learning:</strong> Dynamically selecting and providing relevant few-shot examples from a database is a powerful technique for improving LLM performance without fine-tuning.</li>
<li><strong>Iterative prompt and example improvement:</strong> Invest time in iteratively refining prompts and curating effective few-shot examples before considering fine-tuning.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-009/</guid>
  <pubDate>Fri, 19 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 8: Creating, curating, and cleaning data for LLMs</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-008/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Reusing Existing Datasets</li>
<li>Creating Your Own Dataset</li>
<li>Common Dataset Genres</li>
<li>Synthetic Data</li>
<li>Improving Data</li>
<li>Human Annotation</li>
<li>Example Datasets</li>
<li>Case Study: LLM Summarizer</li>
<li>Resources</li>
<li>Q&amp;A</li>
</ul>
<section id="reusing-existing-datasets" class="level2">
<h2 class="anchored" data-anchor-id="reusing-existing-datasets">Reusing Existing Datasets</h2>
<ul>
<li><strong>🤗 Datasets:</strong> <a href="https://huggingface.co/datasets">https://huggingface.co/datasets</a></li>
<li><strong>What kind of existing dataset?</strong>
<ul>
<li><strong>Consider the use case:</strong> Datasets like <code>fineweb</code> are designed for pre-training, not fine-tuning.
<ul>
<li><strong>🤗 Dataset:</strong> <a href="HuggingFaceFW/fineweb">HuggingFaceFW/fineweb</a></li>
</ul></li>
<li><strong>Look beyond research datasets:</strong> Community-contributed datasets can offer unique and valuable data.</li>
</ul></li>
<li><strong>Browsing datasets</strong>
<ul>
<li><strong>Use tags:</strong> Filter datasets based on specific formats or tasks (e.g., DPO datasets).
<ul>
<li><a href="https://huggingface.co/datasets?other=dpo">🤗 DPO Datasets</a></li>
</ul></li>
</ul></li>
<li><strong>Searching for datasets</strong>
<ul>
<li><strong>Full-text search:</strong> Useful if dataset names are not descriptive enough.</li>
</ul></li>
<li><strong>Reviewing if a dataset is suitable: Vibe checks</strong>
<ul>
<li><strong>Dataset viewer:</strong> Provides a preview of the data, including metadata and example rows.</li>
<li><strong>Analyze conversation length and content:</strong> Assess if the dataset aligns with your target application.</li>
</ul></li>
</ul>
</section>
<section id="creating-your-own-dataset" class="level2">
<h2 class="anchored" data-anchor-id="creating-your-own-dataset">Creating Your Own Dataset</h2>
<ul>
<li><strong>Adapt existing NLP datasets:</strong> Restructure data from classic NLP tasks for LLM fine-tuning.</li>
<li><strong>Leverage user feedback:</strong> Analyze existing user interactions (e.g., thumbs up/down) for preference data.</li>
<li><strong>Synthetic data:</strong> A powerful method for jumpstarting dataset creation.</li>
<li><strong>Getting data?</strong>
<ul>
<li><strong>Data format:</strong> Ensure the data format closely resembles the intended use case for the LLM.</li>
<li><strong>Preprocessing:</strong> The effort invested in data preparation will benefit the deployment stage.</li>
</ul></li>
<li><strong>What kind of dataset do you need for fine-tuning</strong>
<ul>
<li><strong>Specificity over diversity:</strong> Focus on data relevant to your specific use case, even if it means the model loses some general abilities.</li>
<li><strong>Data diversity should reflect the target application:</strong> Don’t aim for broad diversity if your application is narrow.</li>
</ul></li>
</ul>
</section>
<section id="common-dataset-genres" class="level2">
<h2 class="anchored" data-anchor-id="common-dataset-genres">Common Dataset Genres</h2>
<ul>
<li><strong>SFT (Supervised Fine Tuning)</strong>
<ul>
<li><strong>Structure:</strong> Question and answer pairs.</li>
</ul></li>
<li><strong>RLHF (Reinforcement Learning by Human Feedback)</strong>
<ul>
<li><strong>Structure:</strong> Similar to SFT, but with additional preference information.</li>
</ul></li>
<li><strong>DPO (Direct Preference Optimization)</strong>
<ul>
<li><strong>Structure:</strong> Input, chosen response, and rejected response.</li>
<li><strong>Flexibility:</strong> “Chosen” and “rejected” can be generated creatively (e.g., using human-written text as “chosen” and model-generated text as “rejected”).</li>
</ul></li>
<li><strong>KTO (Kahneman-Tversky Optimization)</strong>
<ul>
<li><strong>argilla’s Collections:</strong> <a href="https://huggingface.co/collections/argilla/preference-datasets-for-kto-65f98314d7c1b04ab54d41a7">Preference Datasets for KTO</a></li>
<li><strong>Structure:</strong> Model response and a binary preference (thumbs up/down).</li>
<li><strong>Easy to collect:</strong> Users can readily provide simple preference feedback.</li>
</ul></li>
<li><strong>SPIN/ORPO</strong>
<ul>
<li><strong>HuggingFaceH4’s Collections:</strong> <a href="https://huggingface.co/collections/HuggingFaceH4/awesome-sft-datasets-65788b571bf8e371c4e4241a">Awesome SFT datasets</a></li>
<li><strong>SPIN:</strong> Iterative approach to reduce data requirements by synthetically generating responses.</li>
<li><strong>ORPO:</strong> Similar to DPO but doesn’t require a separate supervised fine-tuning step, making it more efficient.</li>
</ul></li>
</ul>
</section>
<section id="synthetic-data" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-data">Synthetic Data</h2>
<ul>
<li><strong>Definition:</strong> Data generated by LLMs, often used for fine-tuning other LLMs.</li>
<li><strong>Methods:</strong>
<ul>
<li>Generating prompts and completions from scratch.</li>
<li>Rephrasing existing prompts to improve quality and diversity.</li>
<li>AI feedback: Using LLMs to judge the quality of other LLM outputs.</li>
</ul></li>
</ul>
<section id="basic-taxonomy-of-synthetic-data-uses" class="level3">
<h3 class="anchored" data-anchor-id="basic-taxonomy-of-synthetic-data-uses">Basic Taxonomy of Synthetic Data Uses</h3>
<ul>
<li><p><strong>Blog Post:</strong> <a href="https://www.interconnects.ai/p/llm-synthetic-data">Synthetic data: Anthropic’s CAI, from fine-tuning to pretraining, OpenAI’s Superalignment, tips, types, and open examples</a></p></li>
<li><p><strong>Instructions</strong></p>
<ul>
<li><strong>Generated text for SFT / IFT</strong></li>
<li><strong>Completions</strong>
<ul>
<li><code>Prompt</code> ➡️ :robot: ➡️ <code>instruction</code></li>
</ul></li>
</ul></li>
<li><p><strong>Self-Instruct Bootstrapping</strong></p>
<ul>
<li><strong>Process</strong>
<ul>
<li><code>Prompt</code> ➡️ :robot: ➡️ <code>More prompts</code> ➡️ :robot: ➡️ <code>instruction</code> :arrow_right: Filtering :repeat:</li>
</ul></li>
</ul></li>
<li><p><strong>Using LLM to Generate Corrected Sample</strong></p>
<ul>
<li><strong>Process</strong>
<ul>
<li><code>principles</code> + <code>instruction</code> ➡️ :robot: ➡️ <code>corrected instruction</code> :arrow_right: Repeat to filter :repeat:</li>
</ul></li>
</ul></li>
<li><p><strong>Preferences</strong></p>
<ul>
<li><strong>Scoring / Choosing Response for RM / RLHF Training</strong>
<ul>
<li><code>instruction-1</code>…<code>instruction-N</code> :arrow_right: :robot: ➡️ <code>scores</code> or <code>chosen/rejected</code> :arrow_right: Filtering :repeat:</li>
</ul></li>
</ul></li>
<li><p><strong>Critiques</strong></p>
<ul>
<li><strong>Initial Instruction</strong>
<ul>
<li>(rejected response)</li>
</ul></li>
<li><strong>Using LLM Principles to Generate Pairwise Completions</strong>
<ul>
<li><code>initial instruction</code> + <code>principles</code> ➡️ :robot: ➡️ <code>corrected instruction</code> (chosen response) :arrow_right: Filtering :repeat:</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li><strong>Alpaca 7B model:</strong> Trained on synthetic data generated by prompting a language model with instructions.
<ul>
<li><strong>Blog Post:</strong> <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca: A Strong, Replicable Instruction-Following Model</a></li>
<li><strong>Challenges:</strong> Synthetic data can contain hallucinations, toxicity, and biases inherited from the generating model.
<ul>
<li><strong>Example:</strong> Alpaca 7B exhibiting hallucinations and inaccurate claims.</li>
</ul></li>
</ul></li>
<li><strong>Ultrafeedback paper:</strong> Used multiple models and GPT-4 for judging synthetic data quality based on multiple criteria.
<ul>
<li><strong>Paper:</strong> UltraFeedback: Boosting Language Models with Scaled AI Feedback</li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/OpenBMB/UltraFeedback">https://github.com/OpenBMB/UltraFeedback</a></li>
<li><strong>Challenges:</strong> Coding errors, API failures, and data handling issues can severely impact data quality, even with advanced models.
<ul>
<li><strong>Example:</strong> Errors in the Ultrafeedback dataset highlighted the importance of human review and data cleaning.</li>
</ul></li>
</ul></li>
<li><strong>Scaling challenges:</strong> Generating high-quality synthetic data at scale requires careful consideration of cost, vendor lock-in, and data quality.</li>
</ul>
</section>
<section id="tools" class="level3">
<h3 class="anchored" data-anchor-id="tools">Tools</h3>
<ul>
<li><strong>Outlines:</strong>
<ul>
<li>Enables structured text generation in various formats (JSON, Regex, etc.) by modifying token sampling.</li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/outlines-dev/outlines">https://github.com/outlines-dev/outlines</a></li>
</ul></li>
<li><strong>DSPy:</strong>
<ul>
<li>Focuses on “programming” LLM behavior through a defined signature, optimizing prompts and fine-tuning for specific tasks.</li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/stanfordnlp/dspy">https://github.com/stanfordnlp/dspy</a></li>
<li><strong>Blog Post:</strong> <a href="https://hamel.dev/blog/posts/prompt/index.html">Fuck You, Show Me The Prompt.</a></li>
</ul></li>
<li><strong>distilabel:</strong>
<ul>
<li>A pipeline framework for synthetic data generation and AI feedback, emphasizing scalability and dataset engineer workflows.</li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/argilla-io/distilabel">https://github.com/argilla-io/distilabel</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="improving-data" class="level2">
<h2 class="anchored" data-anchor-id="improving-data">Improving Data</h2>
<section id="ensuring-sufficient-quantity" class="level3">
<h3 class="anchored" data-anchor-id="ensuring-sufficient-quantity">Ensuring sufficient quantity</h3>
<ul>
<li>More ≠ better</li>
<li>Be okay with “throwing away” annotations and data</li>
<li><strong>Data Requirements</strong> (based on paper reported numbers)
<ul>
<li><strong>SFT:</strong> <code>10K</code></li>
<li><strong>ORPO:</strong> <code>7K</code></li>
<li><strong>DPO:</strong> <code>3K</code></li>
<li><strong>SPIN:</strong> <code>2K</code></li>
</ul></li>
<li>Data requirements will be higher for more diverse inputs + outputs (would you have used BERT a few years ago to do this task?)</li>
<li><strong>Blog Post:</strong> https://argilla.io/blog/mantisnlp-rlhf-part-9/</li>
</ul>
</section>
<section id="deduplication" class="level3">
<h3 class="anchored" data-anchor-id="deduplication">Deduplication</h3>
<ul>
<li><strong>Blog Post:</strong> <a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale</a></li>
<li><strong>Challenges:</strong> Deduplication pipelines are often not reproducible or well-documented.</li>
<li><strong>Approaches:</strong>
<ul>
<li>Intuitive rules and metadata filtering.</li>
<li>Topic-wise deduplication.</li>
<li>Custom metadata and feature engineering.</li>
<li>Embedding similarity and exemplar selection.</li>
</ul></li>
</ul>
</section>
<section id="rule-based" class="level3">
<h3 class="anchored" data-anchor-id="rule-based">Rule based</h3>
<ul>
<li><strong>Regex:</strong> Useful for simple rule-based cleaning (e.g., removing unwanted phrases or patterns).</li>
</ul>
</section>
<section id="quality" class="level3">
<h3 class="anchored" data-anchor-id="quality">Quality</h3>
<ul>
<li><strong>Tutorial:</strong> <a href="https://hlasse.github.io/TextDescriptives/tutorials/filter_corpus_using_quality.html">Filtering corpora using Quality</a></li>
<li><strong>Techniques:</strong>
<ul>
<li>Basic heuristics</li>
<li>Topic modeling</li>
<li>Embeddings</li>
<li>Classifiers (don’t dismiss these even if you have $$$)
<ul>
<li><strong>Model:</strong> <a href="https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0">MoritzLaurer/deberta-v3-large-zeroshot-v2.0</a></li>
<li><strong>Model:</strong> <a href="https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier">HuggingFaceFW/fineweb-edu-classifier</a></li>
</ul></li>
<li>LLM as judge/juries/rationalizers</li>
<li>Human annotation</li>
</ul></li>
</ul>
</section>
</section>
<section id="human-annotation" class="level2">
<h2 class="anchored" data-anchor-id="human-annotation">Human Annotation</h2>
<ul>
<li><strong>Finding the right balance:</strong> Choose tools and approaches that fit your needs and budget, from simple spreadsheets to fully customized solutions.</li>
<li><strong>Custom</strong>
<ul>
<li><strong>Example:</strong> Vincent D. Warmerdon’s bulk annotation tool using Bokeh and Pandas for interactive data exploration and annotation.
<ul>
<li><strong>GitHub Repository:</strong> <a href="koaning/bulk">koaning/bulk</a></li>
</ul></li>
</ul></li>
<li><strong>Notebooks</strong>
<ul>
<li><strong>Example:</strong> ipyannotations for in-notebook annotation with customizable callbacks for post-processing and active learning.
<ul>
<li><strong>Documentation:</strong> <a href="https://ipyannotations.readthedocs.io/en/latest/">ipyannotations</a></li>
</ul></li>
</ul></li>
<li><strong>Apps</strong>
<ul>
<li><strong>Example:</strong> Gradio, Streamlit, and Shiny for building custom web apps with intuitive UIs for annotation tasks.</li>
</ul></li>
<li><strong>Lilac</strong>
<ul>
<li><strong>Website:</strong> <a href="https://www.lilacml.com/">https://www.lilacml.com/</a></li>
<li><strong>Features:</strong> Dataset overview, semantic search, and integrated annotation tools.</li>
</ul></li>
<li><strong>Argilla</strong>
<ul>
<li><strong>Website:</strong> <a href="https://argilla.io/">https://argilla.io/</a></li>
<li><strong>Features:</strong> Similar to Lilac, with a focus on dataset management and annotation workflows.</li>
</ul></li>
</ul>
</section>
<section id="example-datasets" class="level2">
<h2 class="anchored" data-anchor-id="example-datasets">Example Datasets</h2>
<ul>
<li><p><strong>distilabel Orca Pairs for DPO</strong></p>
<ul>
<li><strong>Dataset:</strong> <a href="argilla/distilabel-intel-orca-dpo-pairs">argilla/distilabel-intel-orca-dpo-pairs</a></li>
<li><strong>Why it’s cool and what you can learn:</strong>
<ul>
<li>Filtering: less is more</li>
<li>Choosing the “stronger” model as always chosen doesn’t always work</li>
<li>Doing some data focused work using humans can make a big impact on model performance</li>
</ul></li>
</ul></li>
<li><p><strong>Gutenberg DPO</strong></p>
<ul>
<li><strong>Dataset:</strong> <a href="jondurbin/gutenberg-dpo-v0.1">jondurbin/gutenberg-dpo-v0.1</a></li>
<li><strong>Approach:</strong> Uses human-written books and LLM-generated summaries to create a DPO dataset for improving LLM writing ability.</li>
</ul></li>
<li><p><strong>PlatVR KTO Dataset</strong></p>
<ul>
<li><strong>Dataset:</strong> <a href="ITG/PlatVR-kto">ITG/PlatVR-kto</a></li>
<li><strong>Approach:</strong> Collects thumbs up/down ratings on model outputs to create a KTO dataset for a vision-language task.</li>
<li><strong>Why it’s cool and what you can learn:</strong>
<ul>
<li>Example of how KTO datasets can work well as a data flywheel</li>
<li>Users submit a prompt. They get a response they 👍/👎 that response</li>
<li>Cheap to collect and can be useful data even if you don’t end up using KTO</li>
</ul></li>
<li><strong>Disclaimer:</strong> The creation process was done using a crowdsourcing methodology. Therefore, the preferences in the data align with the user group that participated in the process (i.e., these are real preference data).</li>
</ul></li>
</ul>
</section>
<section id="case-study-llm-summarizer" class="level2">
<h2 class="anchored" data-anchor-id="case-study-llm-summarizer">Case Study: LLM Summarizer</h2>
<ul>
<li><strong>Goal:</strong> Build an LLM-based summarizer for data set cards on Hugging Face.</li>
<li><strong>Approach:</strong> Uses a Distilabel pipeline to generate and judge summaries, incorporating both model and human feedback.</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 293.81 399.20" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 395.2)">
<title>LLM Summarizer</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-395.2 289.81,-395.2 289.81,4 -4,4"></polygon>
<!-- Load dataset card in markdown format -->
<g id="node1" class="node">
<title>Load dataset card in markdown format</title>
<polygon fill="none" stroke="black" points="259.71,-391.2 26.1,-391.2 26.1,-355.2 259.71,-355.2 259.71,-391.2"></polygon>
<text text-anchor="middle" x="142.9" y="-369" font-family="Times,serif" font-size="14.00">Load dataset card in markdown format</text>
</g>
<!-- Parse out YAML and remove unwanted content -->
<g id="node2" class="node">
<title>Parse out YAML and remove unwanted content</title>
<polygon fill="none" stroke="black" points="285.17,-302.4 0.64,-302.4 0.64,-266.4 285.17,-266.4 285.17,-302.4"></polygon>
<text text-anchor="middle" x="142.9" y="-280.2" font-family="Times,serif" font-size="14.00">Parse out YAML and remove unwanted content</text>
</g>
<!-- Load dataset card in markdown format&#45;&gt;Parse out YAML and remove unwanted content -->
<g id="edge1" class="edge">
<title>Load dataset card in markdown format-&gt;Parse out YAML and remove unwanted content</title>
<path fill="none" stroke="black" d="M142.9,-355.05C142.9,-342.92 142.9,-326.42 142.9,-312.52"></path>
<polygon fill="black" stroke="black" points="146.4,-312.51 142.9,-302.51 139.4,-312.51 146.4,-312.51"></polygon>
<text text-anchor="middle" x="160.6" y="-324.6" font-family="Times,serif" font-size="14.00">Step 1</text>
</g>
<!-- Pass reduced text to LLM -->
<g id="node3" class="node">
<title>Pass reduced text to LLM</title>
<polygon fill="none" stroke="black" points="223.04,-213.6 62.77,-213.6 62.77,-177.6 223.04,-177.6 223.04,-213.6"></polygon>
<text text-anchor="middle" x="142.9" y="-191.4" font-family="Times,serif" font-size="14.00">Pass reduced text to LLM</text>
</g>
<!-- Parse out YAML and remove unwanted content&#45;&gt;Pass reduced text to LLM -->
<g id="edge2" class="edge">
<title>Parse out YAML and remove unwanted content-&gt;Pass reduced text to LLM</title>
<path fill="none" stroke="black" d="M142.9,-266.25C142.9,-254.12 142.9,-237.62 142.9,-223.72"></path>
<polygon fill="black" stroke="black" points="146.4,-223.71 142.9,-213.71 139.4,-223.71 146.4,-223.71"></polygon>
<text text-anchor="middle" x="160.6" y="-235.8" font-family="Times,serif" font-size="14.00">Step 2</text>
</g>
<!-- Get summary of dataset card -->
<g id="node4" class="node">
<title>Get summary of dataset card</title>
<polygon fill="none" stroke="black" points="231.34,-124.8 54.47,-124.8 54.47,-88.8 231.34,-88.8 231.34,-124.8"></polygon>
<text text-anchor="middle" x="142.9" y="-102.6" font-family="Times,serif" font-size="14.00">Get summary of dataset card</text>
</g>
<!-- Pass reduced text to LLM&#45;&gt;Get summary of dataset card -->
<g id="edge3" class="edge">
<title>Pass reduced text to LLM-&gt;Get summary of dataset card</title>
<path fill="none" stroke="black" d="M142.9,-177.45C142.9,-165.32 142.9,-148.82 142.9,-134.92"></path>
<polygon fill="black" stroke="black" points="146.4,-134.91 142.9,-124.91 139.4,-134.91 146.4,-134.91"></polygon>
<text text-anchor="middle" x="160.6" y="-147" font-family="Times,serif" font-size="14.00">Step 3</text>
</g>
<!-- Get feedback from users about summary quality -->
<g id="node5" class="node">
<title>Get feedback from users about summary quality</title>
<polygon fill="none" stroke="black" points="285.71,-36 0.1,-36 0.1,0 285.71,0 285.71,-36"></polygon>
<text text-anchor="middle" x="142.9" y="-13.8" font-family="Times,serif" font-size="14.00">Get feedback from users about summary quality</text>
</g>
<!-- Get summary of dataset card&#45;&gt;Get feedback from users about summary quality -->
<g id="edge4" class="edge">
<title>Get summary of dataset card-&gt;Get feedback from users about summary quality</title>
<path fill="none" stroke="black" d="M142.9,-88.65C142.9,-76.52 142.9,-60.02 142.9,-46.12"></path>
<polygon fill="black" stroke="black" points="146.4,-46.11 142.9,-36.11 139.4,-46.11 146.4,-46.11"></polygon>
<text text-anchor="middle" x="160.6" y="-58.2" font-family="Times,serif" font-size="14.00">Step 4</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="the-distilabel-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="the-distilabel-pipeline">The distilabel pipeline</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 693.68 742.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 738)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-738 689.68,-738 689.68,4 -4,4"></polygon>
<!-- llama_summary -->
<g id="node1" class="node">
<title>llama_summary</title>
<polygon fill="lightblue" stroke="black" points="269.47,-408 92.33,-408 92.33,-370 269.47,-370 269.47,-408"></polygon>
<text text-anchor="middle" x="180.9" y="-381.5" font-family="sans-serif:bold" font-size="25.00">llama_summary</text>
</g>
<!-- combine_columns -->
<g id="node13" class="node">
<title>combine_columns</title>
<path fill="none" stroke="black" d="M374.7,-334C374.7,-334 199.1,-334 199.1,-334 193.1,-334 187.1,-328 187.1,-322 187.1,-322 187.1,-308 187.1,-308 187.1,-302 193.1,-296 199.1,-296 199.1,-296 374.7,-296 374.7,-296 380.7,-296 386.7,-302 386.7,-308 386.7,-308 386.7,-322 386.7,-322 386.7,-328 380.7,-334 374.7,-334"></path>
<text text-anchor="middle" x="286.9" y="-307.5" font-family="sans-serif:bold" font-size="25.00">combine_columns</text>
</g>
<!-- llama_summary&#45;&gt;combine_columns -->
<g id="edge9" class="edge">
<title>llama_summary-&gt;combine_columns</title>
<path fill="none" stroke="black" d="M207.65,-369.83C220.96,-360.79 237.22,-349.75 251.54,-340.02"></path>
<polygon fill="black" stroke="black" points="253.86,-342.68 260.16,-334.16 249.92,-336.89 253.86,-342.68"></polygon>
</g>
<!-- mistral_summary -->
<g id="node2" class="node">
<title>mistral_summary</title>
<polygon fill="lightpink" stroke="black" points="478.38,-408 287.42,-408 287.42,-370 478.38,-370 478.38,-408"></polygon>
<text text-anchor="middle" x="382.9" y="-381.5" font-family="sans-serif:bold" font-size="25.00">mistral_summary</text>
</g>
<!-- mistral_summary&#45;&gt;combine_columns -->
<g id="edge10" class="edge">
<title>mistral_summary-&gt;combine_columns</title>
<path fill="none" stroke="black" d="M358.68,-369.83C346.74,-360.88 332.17,-349.96 319.29,-340.3"></path>
<polygon fill="black" stroke="black" points="321.22,-337.36 311.12,-334.16 317.02,-342.96 321.22,-337.36"></polygon>
</g>
<!-- zephyr_summary -->
<g id="node3" class="node">
<title>zephyr_summary</title>
<polygon fill="lightgreen" stroke="black" points="685.46,-408 496.34,-408 496.34,-370 685.46,-370 685.46,-408"></polygon>
<text text-anchor="middle" x="590.9" y="-381.5" font-family="sans-serif:bold" font-size="25.00">zephyr_summary</text>
</g>
<!-- zephyr_summary&#45;&gt;combine_columns -->
<g id="edge11" class="edge">
<title>zephyr_summary-&gt;combine_columns</title>
<path fill="none" stroke="black" d="M514.58,-369.92C471.49,-359.72 417.52,-346.94 372.93,-336.38"></path>
<polygon fill="black" stroke="black" points="373.53,-332.92 362.99,-334.02 371.91,-339.73 373.53,-332.92"></polygon>
</g>
<!-- llama_3_70_b -->
<g id="node4" class="node">
<title>llama_3_70_b</title>
<polygon fill="lightblue" stroke="black" points="198.08,-512 47.72,-512 47.72,-444 198.08,-444 198.08,-512"></polygon>
<text text-anchor="middle" x="122.9" y="-485.5" font-family="sans-serif:bold" font-size="25.00">llama-3-70-B</text>
<text text-anchor="middle" x="122.9" y="-455.5" font-family="sans-serif:bold" font-size="25.00">Instruct</text>
</g>
<!-- llama_3_70_b&#45;&gt;llama_summary -->
<g id="edge7" class="edge">
<title>llama_3_70_b-&gt;llama_summary</title>
<path fill="none" stroke="black" d="M145.16,-443.61C151.13,-434.65 157.51,-425.08 163.2,-416.55"></path>
<polygon fill="black" stroke="black" points="166.2,-418.35 168.84,-408.09 160.38,-414.47 166.2,-418.35"></polygon>
</g>
<!-- ultrafeedback -->
<g id="node5" class="node">
<title>ultrafeedback</title>
<polygon fill="lightblue" stroke="black" points="203.92,-260 51.88,-260 51.88,-222 203.92,-222 203.92,-260"></polygon>
<text text-anchor="middle" x="127.9" y="-233.5" font-family="sans-serif:bold" font-size="25.00">ultrafeedback</text>
</g>
<!-- llama_3_70_b&#45;&gt;ultrafeedback -->
<g id="edge8" class="edge">
<title>llama_3_70_b-&gt;ultrafeedback</title>
<path fill="none" stroke="black" d="M98.97,-443.83C92.44,-432.94 86.24,-420.44 82.9,-408 69.77,-359.18 93.69,-302.34 111.39,-269.43"></path>
<polygon fill="black" stroke="black" points="114.6,-270.86 116.41,-260.42 108.49,-267.45 114.6,-270.86"></polygon>
</g>
<!-- remove_bad_ratings -->
<g id="node14" class="node">
<title>remove_bad_ratings</title>
<path fill="none" stroke="black" d="M226,-186C226,-186 29.8,-186 29.8,-186 23.8,-186 17.8,-180 17.8,-174 17.8,-174 17.8,-160 17.8,-160 17.8,-154 23.8,-148 29.8,-148 29.8,-148 226,-148 226,-148 232,-148 238,-154 238,-160 238,-160 238,-174 238,-174 238,-180 232,-186 226,-186"></path>
<text text-anchor="middle" x="127.9" y="-159.5" font-family="sans-serif:bold" font-size="25.00">remove_bad_ratings</text>
</g>
<!-- ultrafeedback&#45;&gt;remove_bad_ratings -->
<g id="edge13" class="edge">
<title>ultrafeedback-&gt;remove_bad_ratings</title>
<path fill="none" stroke="black" d="M127.9,-221.83C127.9,-214.13 127.9,-204.97 127.9,-196.42"></path>
<polygon fill="black" stroke="black" points="131.4,-196.41 127.9,-186.41 124.4,-196.41 131.4,-196.41"></polygon>
</g>
<!-- to_argilla -->
<g id="node6" class="node">
<title>to_argilla</title>
<polygon fill="white" stroke="black" points="111.7,-112 0.1,-112 0.1,-74 111.7,-74 111.7,-112"></polygon>
<text text-anchor="middle" x="55.9" y="-85.5" font-family="sans-serif:bold" font-size="25.00">to_argilla</text>
</g>
<!-- end -->
<g id="node9" class="node">
<title>end</title>
<text text-anchor="middle" x="127.9" y="-11.5" font-family="sans-serif:bold" font-size="25.00">🤗</text>
</g>
<!-- to_argilla&#45;&gt;end -->
<g id="edge17" class="edge">
<title>to_argilla-&gt;end</title>
<path fill="none" stroke="black" d="M74.07,-73.83C82.62,-65.28 92.96,-54.94 102.28,-45.62"></path>
<polygon fill="black" stroke="black" points="104.89,-47.96 109.49,-38.41 99.94,-43.01 104.89,-47.96"></polygon>
</g>
<!-- push_to_hub -->
<g id="node7" class="node">
<title>push_to_hub</title>
<polygon fill="white" stroke="black" points="269.53,-112 130.27,-112 130.27,-74 269.53,-74 269.53,-112"></polygon>
<text text-anchor="middle" x="199.9" y="-85.5" font-family="sans-serif:bold" font-size="25.00">Push to Hub</text>
</g>
<!-- push_to_hub&#45;&gt;end -->
<g id="edge16" class="edge">
<title>push_to_hub-&gt;end</title>
<path fill="none" stroke="black" d="M181.73,-73.83C173.19,-65.28 162.84,-54.94 153.52,-45.62"></path>
<polygon fill="black" stroke="black" points="155.86,-43.01 146.31,-38.41 150.91,-47.96 155.86,-43.01"></polygon>
</g>
<!-- start -->
<g id="node8" class="node">
<title>start</title>
<text text-anchor="middle" x="382.9" y="-707.5" font-family="sans-serif:bold" font-size="25.00">🤗</text>
</g>
<!-- load_dataset -->
<g id="node10" class="node">
<title>load_dataset</title>
<path fill="none" stroke="black" d="M441.35,-660C441.35,-660 324.45,-660 324.45,-660 318.45,-660 312.45,-654 312.45,-648 312.45,-648 312.45,-634 312.45,-634 312.45,-628 318.45,-622 324.45,-622 324.45,-622 441.35,-622 441.35,-622 447.35,-622 453.35,-628 453.35,-634 453.35,-634 453.35,-648 453.35,-648 453.35,-654 447.35,-660 441.35,-660"></path>
<text text-anchor="middle" x="382.9" y="-633.5" font-family="sans-serif:bold" font-size="25.00">load_dataset</text>
</g>
<!-- start&#45;&gt;load_dataset -->
<g id="edge1" class="edge">
<title>start-&gt;load_dataset</title>
<path fill="none" stroke="black" d="M382.9,-695.83C382.9,-688.13 382.9,-678.97 382.9,-670.42"></path>
<polygon fill="black" stroke="black" points="386.4,-670.41 382.9,-660.41 379.4,-670.41 386.4,-670.41"></polygon>
</g>
<!-- card_filter -->
<g id="node11" class="node">
<title>card_filter</title>
<path fill="none" stroke="black" d="M431,-586C431,-586 334.8,-586 334.8,-586 328.8,-586 322.8,-580 322.8,-574 322.8,-574 322.8,-560 322.8,-560 322.8,-554 328.8,-548 334.8,-548 334.8,-548 431,-548 431,-548 437,-548 443,-554 443,-560 443,-560 443,-574 443,-574 443,-580 437,-586 431,-586"></path>
<text text-anchor="middle" x="382.9" y="-559.5" font-family="sans-serif:bold" font-size="25.00">card_filter</text>
</g>
<!-- load_dataset&#45;&gt;card_filter -->
<g id="edge2" class="edge">
<title>load_dataset-&gt;card_filter</title>
<path fill="none" stroke="black" d="M382.9,-621.83C382.9,-614.13 382.9,-604.97 382.9,-596.42"></path>
<polygon fill="black" stroke="black" points="386.4,-596.41 382.9,-586.41 379.4,-596.41 386.4,-596.41"></polygon>
</g>
<!-- format_input_card -->
<g id="node12" class="node">
<title>format_input_card</title>
<path fill="none" stroke="black" d="M471.95,-497C471.95,-497 293.85,-497 293.85,-497 287.85,-497 281.85,-491 281.85,-485 281.85,-485 281.85,-471 281.85,-471 281.85,-465 287.85,-459 293.85,-459 293.85,-459 471.95,-459 471.95,-459 477.95,-459 483.95,-465 483.95,-471 483.95,-471 483.95,-485 483.95,-485 483.95,-491 477.95,-497 471.95,-497"></path>
<text text-anchor="middle" x="382.9" y="-470.5" font-family="sans-serif:bold" font-size="25.00">format_input_card</text>
</g>
<!-- card_filter&#45;&gt;format_input_card -->
<g id="edge3" class="edge">
<title>card_filter-&gt;format_input_card</title>
<path fill="none" stroke="black" d="M382.9,-547.97C382.9,-536.19 382.9,-520.56 382.9,-507.16"></path>
<polygon fill="black" stroke="black" points="386.4,-507 382.9,-497 379.4,-507 386.4,-507"></polygon>
</g>
<!-- format_input_card&#45;&gt;llama_summary -->
<g id="edge4" class="edge">
<title>format_input_card-&gt;llama_summary</title>
<path fill="none" stroke="black" d="M341.05,-458.97C309.36,-445.33 265.67,-426.51 231.94,-411.98"></path>
<polygon fill="black" stroke="black" points="233.27,-408.74 222.7,-408 230.5,-415.17 233.27,-408.74"></polygon>
</g>
<!-- format_input_card&#45;&gt;mistral_summary -->
<g id="edge5" class="edge">
<title>format_input_card-&gt;mistral_summary</title>
<path fill="none" stroke="black" d="M382.9,-458.97C382.9,-447.19 382.9,-431.56 382.9,-418.16"></path>
<polygon fill="black" stroke="black" points="386.4,-418 382.9,-408 379.4,-418 386.4,-418"></polygon>
</g>
<!-- format_input_card&#45;&gt;zephyr_summary -->
<g id="edge6" class="edge">
<title>format_input_card-&gt;zephyr_summary</title>
<path fill="none" stroke="black" d="M426,-458.97C458.63,-445.33 503.61,-426.51 538.35,-411.98"></path>
<polygon fill="black" stroke="black" points="539.98,-415.09 547.86,-408 537.28,-408.63 539.98,-415.09"></polygon>
</g>
<!-- combine_columns&#45;&gt;ultrafeedback -->
<g id="edge12" class="edge">
<title>combine_columns-&gt;ultrafeedback</title>
<path fill="none" stroke="black" d="M246.78,-295.83C225.61,-286.24 199.45,-274.4 177.07,-264.27"></path>
<polygon fill="black" stroke="black" points="178.29,-260.97 167.73,-260.04 175.4,-267.35 178.29,-260.97"></polygon>
</g>
<!-- remove_bad_ratings&#45;&gt;to_argilla -->
<g id="edge14" class="edge">
<title>remove_bad_ratings-&gt;to_argilla</title>
<path fill="none" stroke="black" d="M109.73,-147.83C101.19,-139.28 90.84,-128.94 81.52,-119.62"></path>
<polygon fill="black" stroke="black" points="83.86,-117.01 74.31,-112.41 78.91,-121.96 83.86,-117.01"></polygon>
</g>
<!-- remove_bad_ratings&#45;&gt;push_to_hub -->
<g id="edge15" class="edge">
<title>remove_bad_ratings-&gt;push_to_hub</title>
<path fill="none" stroke="black" d="M146.07,-147.83C154.62,-139.28 164.96,-128.94 174.28,-119.62"></path>
<polygon fill="black" stroke="black" points="176.89,-121.96 181.49,-112.41 171.94,-117.01 176.89,-121.96"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li><strong>Key steps:</strong>
<ul>
<li>Data loading and filtering.</li>
<li>Prompt formatting.</li>
<li>Summary generation using multiple models.</li>
<li>LLM-based judging using Ultrafeedback and Lama three.</li>
<li>Human review and annotation using Argilla.</li>
</ul></li>
<li><strong>Iterative process:</strong> Experiment with different prompts, models, and judging criteria to optimize the pipeline.</li>
</ul>
</section>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><strong>GitHub repository:</strong> <a href="davanstrien/data-for-fine-tuning-llms">davanstrien/data-for-fine-tuning-llms</a>
<ul>
<li>Contains notebooks with code examples for deduplication, data checks, and synthetic data generation.</li>
</ul></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/davanstrien/awesome-synthetic-datasets">davanstrien/awesome-synthetic-datasets</a>
<ul>
<li>Organizes resources focused on helping people get started with building synthetic datasets.</li>
</ul></li>
</ul>
</section>
<section id="qa" class="level2">
<h2 class="anchored" data-anchor-id="qa">Q&amp;A</h2>
<ul>
<li><strong>Question:</strong> How to generate synthetic data when fine-tuning on proprietary data and human annotation is expensive?</li>
<li><strong>Answer:</strong> The decision to use synthetic data vs.&nbsp;proprietary data involves trade-offs related to data ownership, privacy, and control over the model.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-008/</guid>
  <pubDate>Thu, 18 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 7: Best Practices For Fine Tuning Mistral</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-007/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Mistral Overview</li>
<li>Customization</li>
<li>Demos</li>
</ul>
<section id="mistral-overview" class="level2">
<h2 class="anchored" data-anchor-id="mistral-overview">Mistral Overview</h2>
<ul>
<li><strong><a href="https://mistral.ai/">Mistral AI</a>:</strong> Paris-based team (50+ people) specializing in large language models (LLMs).</li>
<li><strong>Model Timeline:</strong>
<ul>
<li><strong>Sept 2023:</strong> Mistral 7b released
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral 7B</a></li>
</ul></li>
<li><strong>Dec 2023:</strong> Mistral 8x7b, Mistral Medium (commercial), API platform launched
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral of experts</a></li>
</ul></li>
<li><strong>Feb 2024:</strong> Mistral Small and Mistral Large (flagship) released
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/mistral-large/">Au Large</a></li>
</ul></li>
<li><strong>Feb 2024:</strong> Le Chat - free conversational AI interface launched
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/le-chat-mistral/">Le Chat</a></li>
</ul></li>
<li><strong>Apr 2024:</strong> Open-source 8x22b model released
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/mixtral-8x22b/">Cheaper, Better, Faster, Stronger</a></li>
</ul></li>
<li><strong>May 2024:</strong> Codestral - specialized model for code generation (80+ languages)
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/codestral/">Codestral: Hello, World!</a></li>
<li><strong>LangChain Tutorial:</strong> <a href="https://www.youtube.com/watch?v=zXFxmI9f06M">Self-correcting code assistants with Codestral</a></li>
</ul></li>
</ul></li>
<li><strong>Model Offerings:</strong>
<ul>
<li><strong>Open-source (Apache 2 License):</strong> Mistral 7b, 8x7b, 8x22b
<ul>
<li><strong>Homepage:</strong> <a href="https://mistral.ai/technology/#models">Open source models</a></li>
<li><strong>Docs:</strong> <a href="https://docs.mistral.ai/getting-started/open_weight_models/">Open-weight models</a></li>
</ul></li>
<li><strong>Enterprise-Grade:</strong> Mistral Small, Mistral Large (supports fine-tuning)</li>
<li><strong>Specialized:</strong> Codestral for coding, Embedding model</li>
</ul></li>
<li><strong>Fine-Tuning:</strong>
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/customization/">My Tailor is Mistral</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/mistral-finetune">mistral-finetune</a></li>
</ul></li>
</ul>
</section>
<section id="customization" class="level2">
<h2 class="anchored" data-anchor-id="customization">Customization</h2>
<ul>
<li><strong>Blog Post:</strong> <a href="https://mistral.ai/news/customization/">My Tailor is Mistral</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/mistral-finetune">mistral-finetune</a></li>
<li><strong>Documentation:</strong> <a href="https://docs.mistral.ai/getting-started/customization/">Model customization</a></li>
<li><strong>Developer Examples:</strong> <a href="https://docs.mistral.ai/getting-started/stories/">Model customization</a></li>
</ul>
<section id="benefits-of-prompting" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-prompting">Benefits of Prompting</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://docs.mistral.ai/guides/prompting_capabilities/">Prompting Capabilities</a></li>
<li><strong>Out-of-the-box functionality:</strong> No data or training required.</li>
<li><strong>Easy updates:</strong> Adaptable to new workflows and prototyping.</li>
</ul>
</section>
<section id="benefits-of-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-fine-tuning">Benefits of Fine-Tuning</h3>
<ul>
<li><strong>Guide:</strong> <a href="https://docs.mistral.ai/guides/finetuning/">Fine-tuning</a></li>
<li><strong>Performance:</strong> Often outperforms prompting and even larger models.</li>
<li><strong>Efficiency:</strong> Faster and cheaper than using large prompts.</li>
<li><strong>Task Alignment:</strong> Tailored to specific tasks and behaviors.</li>
<li><strong>Knowledge Integration:</strong> Ability to teach new facts and information.</li>
</ul>
</section>
</section>
<section id="demos" class="level2">
<h2 class="anchored" data-anchor-id="demos">Demos</h2>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/cookbook">Mistral Cookbook</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/mistral-inference">Mistral Inference</a></li>
</ul>
<section id="mistral-api" class="level3">
<h3 class="anchored" data-anchor-id="mistral-api">Mistral API</h3>
<ul>
<li><p><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/client-python">mistralai/client-python</a></p></li>
<li><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-U</span> mistral-api<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>=0.4.2</span></code></pre></div></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Model Name Structure">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model Name Structure
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Fine-tuned model names have a specific structure:</p>
<ul>
<li><pre class="text"><code>  ft:open-mistral-7b:b6e34a5e:20240531:a29e61db</code></pre></li>
</ul></li>
<li><p><code>ft</code>: Indicates a fine-tuned model.</p></li>
<li><p><code>open-mistral-7b</code>: Specifies the base model used.</p></li>
<li><p><code>b6e34a5e:20240531:a29e61db</code>: Represents the specific fine-tuned version.</p></li>
</ul>
</div>
</div>
</section>
<section id="abstract-generator-demo" class="level3">
<h3 class="anchored" data-anchor-id="abstract-generator-demo">Abstract Generator Demo</h3>
<ul>
<li><p>Generates abstracts based on inputted research paper titles.</p></li>
<li><p>It was trained on title-abstract pairs from <a href="https://arxiv.org/">arxiv.org</a>.</p></li>
<li><p>Highlights how effective fine-tuning can be for specific tasks.</p></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">client.chat(</span>
<span id="cb3-2">  model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ft:mistral-small-latest:8e2706f0:20240604:d861257a'</span>,</span>
<span id="cb3-3">  messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb3-4">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Fine-tuning is all you need"</span>}</span>
<span id="cb3-5">  ],</span>
<span id="cb3-6">  temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb3-7">).choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span></code></pre></div>
<pre class="text"><code>'Fine-tuning is a common practice in deep learning, where a pre-trained model is fine-tuned on a downstream task.\nIn this paper, we show that fine-tuning is all you need for a wide range of tasks.\nWe propose a simple yet effective fine-tuning recipe that works well for both vision and language tasks.\nOur recipe includes a learning rate warmup stage, a cosine learning rate decay schedule, and a stochastic weight averaging (SWA) stage.\nWe show that our recipe can significantly improve the performance of fine-tuning on a wide range of tasks, including image classification, object detection, instance segmentation, semantic segmentation, and language understanding.\nWe also show that our recipe can improve the performance of fine-tuning on small datasets, where the performance of fine-tuning is usually worse than training from scratch.\nOur recipe is simple and easy to implement, and we hope it will be useful for the deep learning community.'</code></pre>
</div>
</div></li>
</ul>
</section>
<section id="medical-chatbot-demo" class="level3">
<h3 class="anchored" data-anchor-id="medical-chatbot-demo">Medical Chatbot Demo</h3>
<ul>
<li>Trained on the HuggingFace dataset for AI medical chatbots.
<ul>
<li><strong>HuggingFace Dataset:</strong> <a href="https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot">ruslanmv/ai-medical-chatbot</a></li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">client.chat(</span>
<span id="cb5-2">  model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ft:open-mistral-7b:b6e34a5e:20240531:a29e61db'</span>,</span>
<span id="cb5-3">  messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb5-4">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hello doctor, My reverse elbow armpits have developed a darker (my skin color is fair) pigmentation. This pigmentation has also affected the whole of my ..."</span>}</span>
<span id="cb5-5">  ],</span>
<span id="cb5-6">  temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb5-7">).choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span></code></pre></div>
<pre class="text"><code>'Hi, It seems that you might be having some fungal infection. Apply clotrimazole cream locally. Take tablet fluconazole 150 mg once a week for three weeks. Keep local part clean and dry. Avoid oily and spicy food. Ok and take care.'</code></pre>
</div>
</div></li>
</ul>
</section>
<section id="news-article-stylist-economist-style-guide-demo" class="level3">
<h3 class="anchored" data-anchor-id="news-article-stylist-economist-style-guide-demo">News Article Stylist (Economist Style Guide) Demo</h3>
<ul>
<li><p>Showcases how to generate training data using a larger model (e.g., Mistral Large) when you don’t have an existing dataset.</p></li>
<li><p><strong>Process:</strong></p>
<ol type="1">
<li><p><strong>Define Prompt:</strong> “You are a news article stylist following the Economist style guide.”</p></li>
<li><p><strong>Generate Data:</strong> Use Mistral Large to rewrite news articles in the Economist style, providing guidelines and examples.</p></li>
<li><p><strong>Fine-tune:</strong> Train a smaller model (e.g., Mistral 7B) on the generated data.</p></li>
</ol></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Incoming Florida Senate President Bill Galvano named the Naples Republican the Senate's majority leader for the upcoming legislative session. Kathleen Passidomo was unimpressed ..."</span></span>
<span id="cb7-2">response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> client.chat(</span>
<span id="cb7-3">  model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ft:mistral-small-latest:b6e34a5e:20240604:ee1ab18b'</span>,</span>
<span id="cb7-4">  messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb7-5">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: news}</span>
<span id="cb7-6">  ],</span>
<span id="cb7-7">  temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb7-8">)</span>
<span id="cb7-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(response.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content)</span></code></pre></div>
<pre class="text"><code>Kathleen Passidomo, a Naples Republican, has been named the Senate's majority leader for the upcoming legislative session by incoming Florida Senate President Bill Galvano. Passidomo was impressed with the appointment because of her good rapport with both Senate Republicans and Democrats and her ability to build consensus. Passidomo agrees that her people skills will be key in shaping the Senate agenda for the coming session, which begins on March 5. Committee meetings will start on December 11. Although the legislative priorities will be the same as last year, including water quality and mental health. In a message to other senators, Galvano praised Passidomo's ability to not only extract favorable consideration from other senators but to maintain the respect of the members. Committee chairmen recognize leadership's ability to guide the Senate to a top spot now sets Passidomo up for even greater things in the future. Assuming she keeps her seat in the 2022 election and Republicans maintain control of the Senate in ...</code></pre>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Please carefully review the provided news article and identify all stylistic issues that detract from its clarity, precision, and sophistication. To ensure the accuracy of your analysis, quote the specific text and explain the reason for your suggested improvement."</span></span>
<span id="cb9-2">response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> client.chat(</span>
<span id="cb9-3">  model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ft:mistral-small-latest:b6e34a5e:20240604:ee1ab18b'</span>,</span>
<span id="cb9-4">  messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb9-5">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: prompt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>news}</span>
<span id="cb9-6">  ],</span>
<span id="cb9-7">  temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb9-8">)</span>
<span id="cb9-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(response.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content)</span></code></pre></div>
<pre class="text"><code>1. The article starts with a repetitive sentence, "Incoming Florida Senate President Bill Galvano named the Naples Republican the Senate's majority leader for the upcoming legislative session." This can be simplified to avoid repetition.
2. The use of informal language, such as "you can put my office in the men's room for all I care," detracts from the article's professional tone.
3. The article uses indirect speech in some instances, such as "What he was saying is that he was naming her senate majority leader," which can be replaced with direct speech for clarity and conciseness.
4. The sentence "Galvano says he picked Passidomo for the post because she has a good rapport with both Senate Republicans and Democrats and is a consensus builder" can be rephrased to avoid redundancy.
5. The use of parentheses in the sentence "Most of it (the job) is member relations, helping members get their bills heard" can be avoided by rephrasing the sentence.
6. The sentence "The Senate majority leader only gets one vote, just like other senators, so what's the upside for constituents?" can be rephrased to avoid the use of a question and make it more assertive.
7. The sentence "That table includes the Senate president, president pro tempore, in this case Sen. David Simmons, Passidomo as majority leader and perhaps one or two others" can be rephrased for clarity.
8. The sentence "Traditionally, legislators in leadership positions don't file as many bills as rank and file members" can be rephrased to avoid the use of "don't" and make it more assertive.
9. The sentence "Some recent predecessors in the post haven't filed any" can be rephrased to avoid the use of "haven't" and make it more assertive.
10. The sentence "In a message to other senators, Galvano praised Passidomo's efforts on the latter" can be rephrased to avoid the use of "the latter" and make it more precise.
11. The sentence "The role of the Senate majority leader isn't to extract favorable consideration from other senators" can be rephrased to avoid the use of "isn't" and make it more assertive.
12. The sentence "Being named to a top spot now sets Passidomo up for even greater things in the future" can be rephrased to avoid the use of "sets up" and make it more assertive.
13. The sentence "Assuming she keeps her seat in the 2022 election and Republicans maintain control of the Senate in 2022 and beyond, she could be Senate president herself in the session that follows" can be rephrased for clarity and conciseness.
14. The sentence "Passidomo said she was taken by surprise by her sudden promotion" can be rephrased to avoid the use of "was taken by surprise" and make it more assertive.
15. The sentence "Third floor, to be exact" can be avoided as it does not add any significant information to the article.</code></pre>
</div>
</div></li>
</ul>
</section>
<section id="mistral-fine-tune-api-walkthrough" class="level3">
<h3 class="anchored" data-anchor-id="mistral-fine-tune-api-walkthrough">Mistral Fine-tune API Walkthrough</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://docs.mistral.ai/guides/finetuning/">https://docs.mistral.ai/guides/finetuning/</a></li>
<li><strong>Jupyter Notebook:</strong> <a href="https://github.com/mistralai/cookbook/blob/main/mistral/fine_tune/mistral_finetune_api.ipynb">mistral/fine_tune/mistral_finetune_api.ipynb</a></li>
</ul>
<ol type="1">
<li><strong>Data Preparation:</strong>
<ul>
<li><strong>Format:</strong> Data should be in JSON format.
<ul>
<li><strong>Size Limits:</strong>
<ul>
<li>Training data: Each file &lt;= 512 MB (multiple files allowed).</li>
<li>Evaluation data: Total size &lt;= 1 MB.</li>
</ul></li>
</ul></li>
<li><strong>Reformatting:</strong> Use provided scripts to adapt data from sources like HuggingFace.
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/mistralai/mistral-finetune/blob/main/utils/reformat_data.py">mistral-finetune/utils/reformat_data.py</a></li>
</ul></li>
<li><strong>Validation:</strong> The <code>mistral-finetune</code> repository includes a data validation script.
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/mistralai/mistral-finetune/blob/main/utils/validate_data.py">mistral-finetune/utils/validate_data.py</a></li>
</ul></li>
</ul></li>
<li><strong>Uploading Data:</strong>
<ul>
<li><strong>Documentation:</strong> <a href="https://docs.mistral.ai/guides/finetuning/#upload-dataset">Upload dataset</a></li>
<li>Use the <code>files.create</code> function, specifying file name and purpose (“fine-tune”).</li>
</ul></li>
<li><strong>Creating a Fine-tuning Job:</strong>
<ul>
<li>Provide file IDs for training and evaluation data.</li>
<li>Choose the base model (Mistral 7B or Mistral Small).</li>
<li>Set hyperparameters (e.g., learning rate, number of steps).</li>
</ul></li>
<li><strong>Monitoring Progress:</strong>
<ul>
<li>Retrieve job status and metrics using the job ID.</li>
</ul></li>
<li><strong>Using the Fine-tuned Model:</strong>
<ul>
<li>Access the fine-tuned model using the provided model name (retrieved from the completed job).</li>
</ul></li>
<li><strong>Weight &amp; Biases Integration (Optional)</strong>:
<ul>
<li>Configure API key for tracking metrics and visualizations.</li>
</ul></li>
</ol>
</section>
<section id="getting-started-fine-tuning-mistral-7b-local" class="level3">
<h3 class="anchored" data-anchor-id="getting-started-fine-tuning-mistral-7b-local">Getting Started Fine-Tuning Mistral 7B (Local)</h3>
<ul>
<li><p><strong>Jupyter Notebook:</strong> <a href="tutorials/mistral_finetune_7b.ipynb">tutorials/mistral_finetune_7b.ipynb</a></p></li>
<li><p>Covers fine-tuning Mistral 7B</p></li>
</ul>
<p><strong>Steps:</strong></p>
<ol type="1">
<li><strong>Clone Repository:</strong> <code>git clone https://github.com/mistralai/mistral-finetune.git</code></li>
<li><strong>Install Dependencies:</strong> Follow <a href="https://github.com/mistralai/mistral-finetune?tab=readme-ov-file#mistral-finetune">instructions</a> in the repository.</li>
<li><strong>Download Model:</strong> Download the desired Mistral model (e.g., 7Bv3).</li>
<li><strong>Prepare Data:</strong> Similar to the API walkthrough.</li>
<li><strong>Configure Training:</strong>
<ul>
<li>Use a configuration file (<code>.yaml</code>) to specify data paths, model parameters, and hyperparameters.</li>
<li>Adjust sequence length based on available GPU memory.</li>
</ul></li>
<li><strong>Start Training:</strong> Execute the training script.</li>
<li><strong>Inference:</strong>
<ul>
<li><p>Utilize the <code>mistral-inference</code> package.</p>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/mistralai/mistral-inference">mistral-inference</a></li>
<li><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb11-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install mistral-inference</span></code></pre></div></li>
</ul></li>
<li><p>Load the tokenizer, base model, and fine-tuned LoRA weights.</p></li>
<li><p>Generate text.</p></li>
</ul></li>
</ol>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-007/</guid>
  <pubDate>Thu, 18 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Workshop 4: Instrumenting &amp; Evaluating LLMs</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/workshop-004/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Serving Overview</li>
<li>Model Deployment Patterns</li>
<li>Case Study: Honeycomb - Replicate</li>
<li>Deploying Large Language Models</li>
<li>Lessons from Building A Serverless Platform - Predibase</li>
<li>Batch vs Real Time and Modal</li>
<li>Q&amp;A Session</li>
</ul>
<section id="serving-overview" class="level2">
<h2 class="anchored" data-anchor-id="serving-overview">Serving Overview</h2>
<section id="recap-on-loras" class="level3">
<h3 class="anchored" data-anchor-id="recap-on-loras">Recap on LoRAs</h3>
<ul>
<li><strong>LoRA (Low-Rank Adaptation)</strong>: A parameter-efficient fine-tuning technique that introduces small adapter matrices into the model’s layers, significantly reducing the number of trainable parameters compared to full fine-tuning.</li>
<li><strong>Benefits of LoRA</strong>: Reduced memory requirements during training and deployment, enabling fine-tuning on consumer-grade hardware and efficient serving of multiple adapters.</li>
<li><strong>Deployment Options</strong>:
<ul>
<li><strong>Keep LoRA separate</strong>: Store LoRA weights in a separate file and load them during inference.</li>
<li><strong>Merge LoRA with base model</strong>: Combine the learned LoRA weights with the original model weights into a single file.</li>
<li><strong>Hot-swapping adapters</strong>: Dynamically load and unload adapters on demand, sharing the base model among multiple adapters.</li>
</ul></li>
</ul>
</section>
<section id="performance-vs-costs" class="level3">
<h3 class="anchored" data-anchor-id="performance-vs-costs">Performance vs Costs</h3>
<ul>
<li><strong>Key Trade-off</strong>: Balancing performance (latency and throughput) with cost (GPU usage and idle time).</li>
<li><strong>Factors Influencing Performance and Cost</strong>:
<ul>
<li><strong>GPU speed:</strong> More powerful GPUs offer lower latency but are more expensive.</li>
<li><strong>Model size:</strong> Larger models generally perform better but require more resources and time.</li>
<li><strong>Engineering optimizations:</strong> Platform-level optimizations can improve efficiency.</li>
<li><strong>Cold start vs.&nbsp;idle time:</strong> Loading models onto GPUs takes time (cold start), but keeping them loaded incurs idle time cost.</li>
</ul></li>
<li><strong>Hot-swapping adapters</strong>: A strategy to mitigate the cold start vs.&nbsp;idle time trade-off by serving multiple LoRAs on the same GPU, ensuring consistent traffic and reducing idle time.</li>
</ul>
</section>
<section id="many-applications-arent-real-time" class="level3">
<h3 class="anchored" data-anchor-id="many-applications-arent-real-time">Many Applications Aren’t Real-Time</h3>
<ul>
<li><strong>Real-time vs.&nbsp;batch/offline processing</strong>: Many LLM applications do not require real-time responses, allowing for batch processing and reducing cost by scaling down GPUs when not in use.</li>
<li><strong>Examples of batch/offline use cases</strong>:
<ul>
<li>Generating alt text for images</li>
<li>Extracting information from documents</li>
<li>Editing text</li>
<li>Analytics tools</li>
</ul></li>
</ul>
</section>
<section id="real-time-vs-batchoffline" class="level3">
<h3 class="anchored" data-anchor-id="real-time-vs-batchoffline">Real-Time vs Batch/Offline</h3>
<ul>
<li><strong>Real-time use cases</strong>: Applications like chatbots and code assistants require low latency responses.</li>
<li><strong>Batch/offline use cases</strong>: Tasks like data analysis, text summarization, and content generation can be processed in batches.</li>
</ul>
</section>
<section id="merging-lora-to-base" class="level3">
<h3 class="anchored" data-anchor-id="merging-lora-to-base">Merging LoRA to Base</h3>
<ul>
<li><p><strong>Workflow example</strong>:</p>
<ol type="1">
<li><p>Train a LoRA model and save the adapter weights.</p></li>
<li><p>Merge the LoRA weights with the base model weights into a single file (potentially sharded for large models).</p>
<ul>
<li><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">root@724562262aec:/workspace/demo#</span> ls outputs/qlora-out/</span>
<span id="cb1-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">README.md</span>         checkpoint-1         checkpoint-4         tokenizer.json</span>
<span id="cb1-3">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">adapter_config.json</span>    checkpoint-2         config.json          tokenizer_config.json</span>
<span id="cb1-4">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">adapter_model.bin</span>     checkpoint-3         special_tokens_map.json</span></code></pre></div>
<ul>
<li><code>adapter_model.bin</code> size: 168 MB</li>
</ul></li>
<li><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb2-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">root@724562262aec:/workspace/demo#</span> python3 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> axolotl.cli.merge_lora ./qlora.yml <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--dora_model_dir</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./outputs/qlora-out"</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">root@724562262aec:/workspace/demo#</span> ls outputs/qlora-out/merged</span>
<span id="cb3-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">config.json</span>             pytorch_model-00003-of-00004.bin   tokenizer.json</span>
<span id="cb3-3">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">generation_config.json</span>        pytorch_model-00004-of-00004.bin   tokenizer_config.json</span>
<span id="cb3-4">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pytorch_model-00001-of-00004.bin</span>   pytorch_model.bin.index.json</span>
<span id="cb3-5">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pytorch_model-00002-of-00004.bin</span>   special_tokens_map.json        </span></code></pre></div>
<ul>
<li>merged <code>.bin</code> files: 16 GB</li>
</ul></li>
</ul></li>
<li><p>Push the merged model files to a platform like HuggingFace Hub.</p></li>
</ol></li>
</ul>
</section>
<section id="push-model-files-to-hf-hub" class="level3">
<h3 class="anchored" data-anchor-id="push-model-files-to-hf-hub">Push Model Files to HF Hub</h3>
<ul>
<li><strong>HuggingFace inference endpoints</strong>: A platform for serving models with options for automatic scaling and GPU selection.</li>
<li><strong>Workflow example</strong>:
<ol type="1">
<li><p>Create a HuggingFace repository.</p>
<ul>
<li><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb4-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-U</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"huggingface_hub[cli]"</span></span>
<span id="cb4-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">huggingface-cli</span> repo create conference-demo</span></code></pre></div></li>
</ul></li>
<li><p>Copy the merged model files to the repository.</p>
<ul>
<li><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cp</span> ./outputs/qlora-out/merged/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span> conference-demo</span></code></pre></div></li>
</ul></li>
<li><p>Use Git LFS to track large files.</p>
<ul>
<li><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb6-1">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> lfs track <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*.bin"</span></span></code></pre></div></li>
</ul></li>
<li><p>Push the repository to HuggingFace Hub.</p>
<ul>
<li><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb7-1">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> add <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div></li>
</ul></li>
<li><p>Deploy the model using HuggingFace inference endpoints, choosing appropriate scaling and GPU options.</p>
<ul>
<li><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb8-1">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> commit <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-am</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Push merged files"</span></span>
<span id="cb8-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> push origin main</span></code></pre></div></li>
<li><strong>HuggingFace Hub:</strong> <a href="https://huggingface.co/dansbecker/conference-demo/tree/main">dansbecker/conference-demo</a></li>
</ul></li>
</ol></li>
</ul>
</section>
</section>
<section id="model-deployment-patterns" class="level2">
<h2 class="anchored" data-anchor-id="model-deployment-patterns">Model Deployment Patterns</h2>
<ul>
<li><strong>Blog Post:</strong> <a href="https://outerbounds.com/blog/the-many-ways-to-deploy-a-model/">The Many Ways to Deploy a Model</a></li>
</ul>
<section id="the-many-faces-of-deployments" class="level3">
<h3 class="anchored" data-anchor-id="the-many-faces-of-deployments">The Many Faces of Deployments</h3>
<ul>
<li><table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Factors</th>
<th>Simple, lots of tools</th>
<th>Some tools, customization may be needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Speed (time to response)</strong></td>
<td>Slow: Results needed in minutes e.g.&nbsp;portfolio optimization</td>
<td>Fast: Results needed in milliseconds e.g.&nbsp;high-frequency trading</td>
</tr>
<tr class="even">
<td><strong>Scale (requests/second)</strong></td>
<td>Low: 10 request/sec or less e.g.&nbsp;an internal dashboard</td>
<td>High: 10k requests / sec or more e.g.&nbsp;a popular e-commerce site</td>
</tr>
<tr class="odd">
<td><strong>Pace of improvement</strong></td>
<td>Low: Updates infrequently e.g.&nbsp;a stable, marginal model</td>
<td>High: Constant iteration needed e.g.&nbsp;an innovative, important model</td>
</tr>
<tr class="even">
<td><strong>Real-time inputs needed?</strong></td>
<td>No real-time inputs e.g.&nbsp;analyze past data</td>
<td>Yes, real-time inputs e.g.&nbsp;targeted travel ads</td>
</tr>
<tr class="odd">
<td><strong>Reliability requirement</strong></td>
<td>Low: Ok to fail occasionally e.g.&nbsp;a proof of concept</td>
<td>High: Must not fail e.g.&nbsp;a fraud detection model</td>
</tr>
<tr class="even">
<td><strong>Model complexity</strong></td>
<td>Simple models e.g.&nbsp;linear regression</td>
<td>Complex models e.g.&nbsp;LLMs</td>
</tr>
</tbody>
</table></li>
</ul>
</section>
<section id="simple-model-serving" class="level3">
<h3 class="anchored" data-anchor-id="simple-model-serving">Simple Model Serving</h3>
<ul>
<li><strong>Direct interface with model library</strong>: Using frameworks like <a href="https://fastapi.tiangolo.com/">FastAPI</a> to serve models with minimal overhead.</li>
<li><strong>Suitable for</strong>: Proof of concepts, small-scale applications with low performance demands.</li>
</ul>
</section>
<section id="advanced-model-serving" class="level3">
<h3 class="anchored" data-anchor-id="advanced-model-serving">Advanced Model Serving</h3>
<ul>
<li><strong>Complex architectures</strong>: Auto-scaling clusters, load balancers, specialized components for pre- and post-processing.</li>
<li><strong>Example</strong>: <a href="https://kubernetes.io/">Kubernetes</a> with <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html">Triton Inference Server</a> and <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>.</li>
</ul>
</section>
<section id="kinds-of-model-serving" class="level3">
<h3 class="anchored" data-anchor-id="kinds-of-model-serving">Kinds of Model Serving</h3>
<ul>
<li><strong>Decision Tree</strong>:</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 793.93 732.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 728)">
<title>Decision Tree</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-728 789.93,-728 789.93,4 -4,4"></polygon>
<!-- A -->
<g id="node1" class="node">
<title>A</title>
<path fill="ghostwhite" stroke="ghostwhite" d="M295.35,-724C295.35,-724 12.22,-724 12.22,-724 6.22,-724 0.22,-718 0.22,-712 0.22,-712 0.22,-668 0.22,-668 0.22,-662 6.22,-656 12.22,-656 12.22,-656 295.35,-656 295.35,-656 301.35,-656 307.35,-662 307.35,-668 307.35,-668 307.35,-712 307.35,-712 307.35,-718 301.35,-724 295.35,-724"></path>
<text text-anchor="middle" x="153.78" y="-697.5" font-family="sans-serif:bold" font-size="25.00">Is there a finite enough set of</text>
<text text-anchor="middle" x="153.78" y="-667.5" font-family="sans-serif:bold" font-size="25.00">inputs known in advance?</text>
</g>
<!-- B -->
<g id="node2" class="node">
<title>B</title>
<path fill="#9bcd9b" stroke="#9bcd9b" d="M178.69,-590C178.69,-590 66.87,-590 66.87,-590 60.87,-590 54.87,-584 54.87,-578 54.87,-578 54.87,-534 54.87,-534 54.87,-528 60.87,-522 66.87,-522 66.87,-522 178.69,-522 178.69,-522 184.69,-522 190.69,-528 190.69,-534 190.69,-534 190.69,-578 190.69,-578 190.69,-584 184.69,-590 178.69,-590"></path>
<text text-anchor="middle" x="122.78" y="-563.5" font-family="sans-serif:bold" font-size="25.00">Precompute</text>
<text text-anchor="middle" x="122.78" y="-533.5" font-family="sans-serif:bold" font-size="25.00">responses</text>
</g>
<!-- A&#45;&gt;B -->
<g id="edge1" class="edge">
<title>A-&gt;B</title>
<path fill="none" stroke="black" d="M145.96,-655.69C141.26,-635.68 135.3,-610.31 130.6,-590.3"></path>
<text text-anchor="middle" x="161.22" y="-615.5" font-family="sans-serif:bold" font-size="25.00">Yes</text>
</g>
<!-- C -->
<g id="node3" class="node">
<title>C</title>
<path fill="ghostwhite" stroke="ghostwhite" d="M491.41,-590C491.41,-590 220.16,-590 220.16,-590 214.16,-590 208.16,-584 208.16,-578 208.16,-578 208.16,-534 208.16,-534 208.16,-528 214.16,-522 220.16,-522 220.16,-522 491.41,-522 491.41,-522 497.41,-522 503.41,-528 503.41,-534 503.41,-534 503.41,-578 503.41,-578 503.41,-584 497.41,-590 491.41,-590"></path>
<text text-anchor="middle" x="355.78" y="-563.5" font-family="sans-serif:bold" font-size="25.00">Is it ok to return responses</text>
<text text-anchor="middle" x="355.78" y="-533.5" font-family="sans-serif:bold" font-size="25.00">asynchronously in minutes?</text>
</g>
<!-- A&#45;&gt;C -->
<g id="edge2" class="edge">
<title>A-&gt;C</title>
<path fill="none" stroke="black" d="M204.49,-655.86C235.23,-635.78 274.31,-610.24 305.05,-590.15"></path>
<text text-anchor="middle" x="289.06" y="-615.5" font-family="sans-serif:bold" font-size="25.00">No</text>
</g>
<!-- D -->
<g id="node4" class="node">
<title>D</title>
<path fill="#9bcd9b" stroke="#9bcd9b" d="M385.74,-441C385.74,-441 171.82,-441 171.82,-441 165.82,-441 159.82,-435 159.82,-429 159.82,-429 159.82,-385 159.82,-385 159.82,-379 165.82,-373 171.82,-373 171.82,-373 385.74,-373 385.74,-373 391.74,-373 397.74,-379 397.74,-385 397.74,-385 397.74,-429 397.74,-429 397.74,-435 391.74,-441 385.74,-441"></path>
<text text-anchor="middle" x="278.78" y="-414.5" font-family="sans-serif:bold" font-size="25.00">Trigger a workflow to</text>
<text text-anchor="middle" x="278.78" y="-384.5" font-family="sans-serif:bold" font-size="25.00">compute responses</text>
</g>
<!-- C&#45;&gt;D -->
<g id="edge3" class="edge">
<title>C-&gt;D</title>
<path fill="none" stroke="black" d="M338.31,-521.64C325.71,-497.59 308.77,-465.25 296.19,-441.22"></path>
<text text-anchor="middle" x="348.22" y="-481.5" font-family="sans-serif:bold" font-size="25.00">Yes</text>
</g>
<!-- E -->
<g id="node5" class="node">
<title>E</title>
<path fill="gold" stroke="gold" d="M628.03,-456C628.03,-456 427.53,-456 427.53,-456 421.53,-456 415.53,-450 415.53,-444 415.53,-444 415.53,-370 415.53,-370 415.53,-364 421.53,-358 427.53,-358 427.53,-358 628.03,-358 628.03,-358 634.03,-358 640.03,-364 640.03,-370 640.03,-370 640.03,-444 640.03,-444 640.03,-450 634.03,-456 628.03,-456"></path>
<text text-anchor="middle" x="527.78" y="-429.5" font-family="sans-serif:bold" font-size="25.00">Are you comfortable</text>
<text text-anchor="middle" x="527.78" y="-399.5" font-family="sans-serif:bold" font-size="25.00">operating services</text>
<text text-anchor="middle" x="527.78" y="-369.5" font-family="sans-serif:bold" font-size="25.00">by yourself?</text>
</g>
<!-- C&#45;&gt;E -->
<g id="edge4" class="edge">
<title>C-&gt;E</title>
<path fill="none" stroke="black" d="M394.81,-521.64C417.41,-502.33 446.25,-477.68 471.29,-456.28"></path>
<text text-anchor="middle" x="463.06" y="-481.5" font-family="sans-serif:bold" font-size="25.00">No</text>
</g>
<!-- F -->
<g id="node6" class="node">
<title>F</title>
<path fill="gold" stroke="gold" d="M429.55,-262C429.55,-262 174.01,-262 174.01,-262 168.01,-262 162.01,-256 162.01,-250 162.01,-250 162.01,-206 162.01,-206 162.01,-200 168.01,-194 174.01,-194 174.01,-194 429.55,-194 429.55,-194 435.55,-194 441.55,-200 441.55,-206 441.55,-206 441.55,-250 441.55,-250 441.55,-256 435.55,-262 429.55,-262"></path>
<text text-anchor="middle" x="301.78" y="-235.5" font-family="sans-serif:bold" font-size="25.00">Do you require large scale</text>
<text text-anchor="middle" x="301.78" y="-205.5" font-family="sans-serif:bold" font-size="25.00">or low latency?</text>
</g>
<!-- E&#45;&gt;F -->
<g id="edge5" class="edge">
<title>E-&gt;F</title>
<path fill="none" stroke="black" d="M466.33,-357.87C427.5,-327.46 378.36,-288.97 343.92,-262"></path>
<text text-anchor="middle" x="461.22" y="-317.5" font-family="sans-serif:bold" font-size="25.00">Yes</text>
</g>
<!-- I -->
<g id="node9" class="node">
<title>I</title>
<path fill="#9bcd9b" stroke="#9bcd9b" d="M774.09,-292C774.09,-292 471.48,-292 471.48,-292 465.48,-292 459.48,-286 459.48,-280 459.48,-280 459.48,-176 459.48,-176 459.48,-170 465.48,-164 471.48,-164 471.48,-164 774.09,-164 774.09,-164 780.09,-164 786.09,-170 786.09,-176 786.09,-176 786.09,-280 786.09,-280 786.09,-286 780.09,-292 774.09,-292"></path>
<text text-anchor="middle" x="622.78" y="-265.5" font-family="sans-serif:bold" font-size="25.00">Use a managed model</text>
<text text-anchor="middle" x="622.78" y="-235.5" font-family="sans-serif:bold" font-size="25.00">hosting service:</text>
<text text-anchor="middle" x="622.78" y="-175.5" font-family="sans-serif:bold" font-size="25.00">Amazon SageMaker, Anyscale</text>
</g>
<!-- E&#45;&gt;I -->
<g id="edge6" class="edge">
<title>E-&gt;I</title>
<path fill="none" stroke="black" d="M553.74,-357.63C564.56,-337.47 577.28,-313.78 588.83,-292.26"></path>
<text text-anchor="middle" x="594.06" y="-317.5" font-family="sans-serif:bold" font-size="25.00">No</text>
</g>
<!-- G -->
<g id="node7" class="node">
<title>G</title>
<path fill="#9bcd9b" stroke="#9bcd9b" d="M285.92,-98C285.92,-98 25.64,-98 25.64,-98 19.64,-98 13.64,-92 13.64,-86 13.64,-86 13.64,-12 13.64,-12 13.64,-6 19.64,0 25.64,0 25.64,0 285.92,0 285.92,0 291.92,0 297.92,-6 297.92,-12 297.92,-12 297.92,-86 297.92,-86 297.92,-92 291.92,-98 285.92,-98"></path>
<text text-anchor="middle" x="155.78" y="-71.5" font-family="sans-serif:bold" font-size="25.00">Deploy an advanced stack:</text>
<text text-anchor="middle" x="155.78" y="-11.5" font-family="sans-serif:bold" font-size="25.00">NVIDIA</text>
</g>
<!-- F&#45;&gt;G -->
<g id="edge7" class="edge">
<title>F-&gt;G</title>
<path fill="none" stroke="black" d="M274.32,-193.71C252.1,-166.77 220.51,-128.47 195.53,-98.18"></path>
<text text-anchor="middle" x="253.22" y="-123.5" font-family="sans-serif:bold" font-size="25.00">Yes</text>
</g>
<!-- H -->
<g id="node8" class="node">
<title>H</title>
<path fill="#9bcd9b" stroke="#9bcd9b" d="M567.62,-83C567.62,-83 327.94,-83 327.94,-83 321.94,-83 315.94,-77 315.94,-71 315.94,-71 315.94,-27 315.94,-27 315.94,-21 321.94,-15 327.94,-15 327.94,-15 567.62,-15 567.62,-15 573.62,-15 579.62,-21 579.62,-27 579.62,-27 579.62,-71 579.62,-71 579.62,-77 573.62,-83 567.62,-83"></path>
<text text-anchor="middle" x="447.78" y="-56.5" font-family="sans-serif:bold" font-size="25.00">Deploy a simple service:</text>
<text text-anchor="middle" x="447.78" y="-26.5" font-family="sans-serif:bold" font-size="25.00">OpenLLM, FastAPI</text>
</g>
<!-- F&#45;&gt;H -->
<g id="edge8" class="edge">
<title>F-&gt;H</title>
<path fill="none" stroke="black" d="M329.24,-193.71C355.35,-162.06 394.38,-114.74 420.44,-83.14"></path>
<text text-anchor="middle" x="408.06" y="-123.5" font-family="sans-serif:bold" font-size="25.00">No</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="gpu-poor-benchmark-wrong-but-useful" class="level3">
<h3 class="anchored" data-anchor-id="gpu-poor-benchmark-wrong-but-useful">GPU Poor Benchmark (Wrong, but useful)</h3>
<ul>
<li><strong>Benchmarking inference servers</strong>: Experiment with different servers to find the best fit for your use case.</li>
<li><strong>Observations</strong>:
<ul>
<li><a href="https://github.com/vllm-project/vllm">vLLM</a>: Easy to use, good performance trade-offs.</li>
<li>NVIDIA stack (<a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html">Triton + TensorRT</a>): High performance but complex to use.</li>
<li><strong>Quantization</strong>: Can significantly impact performance, but evaluate quality trade-offs.</li>
</ul></li>
</ul>
</section>
</section>
<section id="case-study-honeycomb---replicate" class="level2">
<h2 class="anchored" data-anchor-id="case-study-honeycomb---replicate">Case Study: Honeycomb - Replicate</h2>
<ul>
<li><p><strong><a href="https://replicate.com/">Replicate</a>:</strong> Run AI with an API</p></li>
<li><p><a href="https://replicate.com/hamelsmu/honeycomb-4-awq">hamelsmu/honeycomb-4-awq</a>: Honeycomb NLQ Generator hosted with vLLM + AWQ Quantized</p></li>
<li><p><strong>HoneyComb Model:</strong> <a href="https://huggingface.co/parlance-labs/hc-mistral-alpaca-merged-awq">parlance-labs/hc-mistral-alpaca-merged-awq</a></p></li>
</ul>
<section id="why-replicate" class="level3">
<h3 class="anchored" data-anchor-id="why-replicate">Why Replicate?</h3>
<ul>
<li><strong>Real-time Use Case:</strong> The Honeycomb example demands real-time responses within the Honeycomb interface, making a platform like Replicate ideal.</li>
<li><strong>User-Friendly Playground:</strong> Replicate provides a playground environment with structured input, beneficial for non-technical users to interact with the model.</li>
<li><strong>Permalink Functionality:</strong> Replicate generates permalinks for predictions, which simplifies debugging and sharing specific scenarios with collaborators.</li>
<li><strong>Built-in Documentation and API:</strong> Replicate automatically generates documentation and API endpoints for easy integration and sharing.</li>
<li><strong>Example Saving:</strong> The platform allows users to save specific examples for future reference and testing.</li>
</ul>
</section>
<section id="show-me-the-code" class="level3">
<h3 class="anchored" data-anchor-id="show-me-the-code">Show Me the Code</h3>
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/parlance-labs/ftcourse/tree/master/replicate-examples/mistral-vllm-awq">ftcourse/replicate-examples/mistral-vllm-awq</a></li>
<li><strong><a href="https://cog.run/">Cog</a>:</strong> Containers for machine learning</li>
</ul>
<p><strong>Files:</strong></p>
<ul>
<li><code>cog.yaml</code>: Defines the Docker environment and specifies the entry point (<code>predict.py</code>).</li>
<li><code>predict.py</code>: Contains the model loading, setup, and prediction logic.</li>
</ul>
<p><strong>Steps:</strong> 1. <strong>Environment Setup:</strong> - Install Cog (a Docker wrapper that simplifies CUDA management). - Download the model weights from Hugging Face Hub (optional, for local testing). 2. <strong>Code Structure:</strong> - <code>cog.yaml</code>: - Specifies the base Docker image and dependencies. - Defines the <code>predict.py</code> file as the entry point. - ```yaml # Configuration for Cog ⚙️ # Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md</p>
<pre><code>      build:
       # set to true if your model requires a GPU
       gpu: true
       cuda: "12.1"
      
       # python version in the form '3.8' or '3.8.12'
       python_version: "3.11"
      
       # a list of packages in the format &lt;package-name&gt;==&lt;version&gt;
       python_packages:
        - "hf_transfer==0.1.4"
        - "aiohttp[speedups]"
        - "torch==2.1.2"
      
       # commands run after the environment is setup
       run:
        - pip install "pydantic&lt;2.0.0"
        - CUDA_HOME=/usr/local/cuda pip install --ignore-installed vllm==0.3.0
        - pip install https://r2.drysys.workers.dev/tmp/cog-0.10.0a6-py3-none-any.whl
        - bash -c 'ln -s /usr/local/lib/python3.11/site-packages/torch/lib/lib{nv,cu}* /usr/lib'
        - pip install scipy==1.11.4 sentencepiece==0.1.99 protobuf==4.23.4
        - ln -sf $(which echo) $(which pip)
      
      predict: "predict.py:Predictor"
      ```
- `predict.py`:
    - **Prompt Template:** Sets the structure for interacting with the LLM.
    - **Setup:**
        - Defines a `Predictor` class.
        - Loads the quantized model from Hugging Face Hub during initialization.
    - **Predict Function:**
        - Takes the natural language query and schema as input.
        - Processes the input through the LLM using vLLM.
        - Returns the generated Honeycomb query.
    
    - ```python
        import os
        os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
        import torch
        from cog import BasePredictor
        from vllm import LLM, SamplingParams
        
        
        MODEL_ID = 'parlance-labs/hc-mistral-alpaca-merged-awq'
        MAX_TOKENS=2500
        
        PROMPT_TEMPLATE = """Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query.
        
        ### Instruction:
        
        NLQ: "{nlq}"
        
        Columns: {cols}
        
        ### Response:
        """
        
        class Predictor(BasePredictor):
            
          def setup(self):
            n_gpus = torch.cuda.device_count()
            self.sampling_params = SamplingParams(stop_token_ids=[2], temperature=0, ignore_eos=True, max_tokens=2500)
        
            self.llm = LLM(model='parlance-labs/hc-mistral-alpaca-merged-awq', 
                    tensor_parallel_size=n_gpus, quantization="AWQ")
        
          def predict(self, nlq: str, cols: str) -&gt; str:    
            _p = PROMPT_TEMPLATE.format(nlq=nlq, cols=cols)
            out = self.llm.generate(_p, sampling_params=self.sampling_params, use_tqdm=False)
            return out[0].outputs[0].text.strip().strip('"')
        ```</code></pre>
<ol start="3" type="1">
<li><strong>Local Testing:</strong>
<ul>
<li><strong>Run Cog Server:</strong> <code>cog run</code> starts a local web server for interacting with the model.
<ul>
<li><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb10-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">cog</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> CUDA_VISIBLE_DEVICES=0 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-p</span> 5000 python <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> cog.server.http</span></code></pre></div></li>
</ul></li>
<li><strong>Direct Prediction:</strong> <code>cog predict -i input1=value1 input2=value2</code> allows for direct prediction using command-line arguments.
<ul>
<li><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb11-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">cog</span> predict <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> CUDA_VISIBLE_DEVICES=0 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-i</span> nlq=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EMISSING slowest traces"</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-i</span> cols=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"['sli.latency', 'duration_ms', 'net.transport', 'http.method', 'error', 'http.target', 'http.route', 'rpc.method', 'ip', 'http.request_content_length', 'rpc.service', 'apdex', 'name', 'message.type', 'http.host', 'service.name', 'rpc.system', 'http.scheme', 'sli.platform-time', 'type', 'http.flavor', 'span.kind', 'dc.platform-time', 'library.version', 'status_code', 'net.host.port', 'net.host.ip', 'app.request_id', 'bucket_duration_ms', 'library.name', 'sli_product', 'message.uncompressed_size', 'rpc.grpc.status_code', 'net.peer.port', 'log10_duration_ms', 'http.status_code', 'status_message', 'http.user_agent', 'net.host.name', 'span.num_links', 'message.id', 'parent_name', 'app.cart_total', 'num_products', 'product_availability', 'revenue_at_risk', 'trace.trace_id', 'trace.span_id', 'ingest_timestamp', 'http.server_name', 'trace.parent_id']"</span></span></code></pre></div></li>
</ul></li>
</ul></li>
<li><strong>Deployment to Replicate:</strong>
<ul>
<li><strong>Create a Model on Replicate:</strong>
<ul>
<li>Choose a descriptive name.</li>
<li>Select appropriate hardware based on memory and GPU requirements.</li>
<li>Choose “Custom Cog Model” as the model type.</li>
</ul></li>
<li><strong>Login to Cog:</strong> <code>cog login</code></li>
<li><strong>Push to Replicate:</strong> <code>cog push r8.im/hamelsmu/honeycomb-4-awq</code></li>
</ul></li>
</ol>
</section>
</section>
<section id="deploying-large-language-models" class="level2">
<h2 class="anchored" data-anchor-id="deploying-large-language-models">Deploying Large Language Models</h2>
<section id="deploying-llms" class="level3">
<h3 class="anchored" data-anchor-id="deploying-llms">Deploying LLMs</h3>
<ul>
<li>Deploying LLMs is challenging, even in 2024, due to the multidimensional and zero-sum nature of performance optimization and the constant evolution of technology.</li>
</ul>
<section id="challenges-in-deploying-llms" class="level4">
<h4 class="anchored" data-anchor-id="challenges-in-deploying-llms">Challenges in Deploying LLMs</h4>
<ul>
<li><strong>Multidimensional and Zero-Sum Performance:</strong> LLM performance involves trade-offs between various factors like speed, cost, and accuracy. Prioritizing one dimension often negatively impacts others.
<ul>
<li><strong>Example:</strong> Increasing batch size improves throughput (total tokens per second) but reduces single-stream performance (tokens per second for a single request), impacting user experience.</li>
</ul></li>
<li><strong>Rapid Technology Evolution:</strong> The field is constantly evolving with new serving frameworks and optimization techniques emerging frequently. Keeping up with these changes while maintaining a performant and cost-effective deployment is demanding.</li>
</ul>
</section>
<section id="llm-performance-bottlenecks" class="level4">
<h4 class="anchored" data-anchor-id="llm-performance-bottlenecks">LLM Performance Bottlenecks</h4>
<p>Two primary factors contribute to slow LLM inference:</p>
<ul>
<li><strong>Memory Bandwidth:</strong> Transformers require frequent data transfers between slow device memory and faster memory caches on GPUs.</li>
<li><strong>Software Overhead:</strong> Launching and scheduling each operation in a model’s forward pass involves communication between CPU and GPU, creating overhead.</li>
</ul>
</section>
<section id="techniques-for-optimizing-llm-performance" class="level4">
<h4 class="anchored" data-anchor-id="techniques-for-optimizing-llm-performance">Techniques for Optimizing LLM Performance</h4>
<ul>
<li><strong>Memory Bandwidth Optimization:</strong>
<ul>
<li><strong>CUDA Kernel Optimization:</strong> Techniques like kernel fusion aim to minimize data transfer by combining multiple kernels into one.</li>
<li><strong>Flash Attention:</strong> Improves efficiency by minimizing data movement during attention calculations.</li>
<li><strong>Paged Attention:</strong> Optimizes data storage and transfer for increased efficiency.</li>
<li><strong>Quantization:</strong> Reduces model size by using lower-precision data types, allowing for faster data transfer.</li>
<li><strong>Speculative Decoding:</strong> Generates multiple tokens in parallel, discarding incorrect ones, to potentially reduce latency.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2405.19325">Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/feifeibear/LLMSpeculativeSampling">feifeibear/LLMSpeculativeSampling</a></li>
</ul></li>
</ul></li>
<li><strong>Software Overhead Reduction:</strong>
<ul>
<li><strong>CUDA Kernel Optimization:</strong> Fewer, more efficient kernels lead to fewer kernel launches.</li>
<li><strong>CUDA Graphs:</strong> Traces and combines all kernel launches in a forward pass into a single unit, reducing CPU-GPU communication.</li>
</ul></li>
<li><strong>Runtime Optimizations:</strong>
<ul>
<li><strong>Continuous Batching:</strong> Enables efficient processing of requests with varying lengths by continuously adding and removing them from batches during inference.</li>
<li><strong>KV Caching:</strong> Stores key-value embeddings during inference, avoiding redundant calculations for repeated inputs.</li>
<li><strong>Hardware Upgrades:</strong> Using more powerful GPUs directly improves performance.</li>
<li><strong>Input/Output Length Optimization:</strong> Shorter inputs and outputs reduce the number of tokens processed, potentially improving latency.</li>
</ul></li>
</ul>
</section>
<section id="continuous-batching" class="level4">
<h4 class="anchored" data-anchor-id="continuous-batching">Continuous Batching</h4>
<ul>
<li><p>Continuous batching is a significant advancement in LLM serving that addresses limitations of traditional micro-batching.</p></li>
<li><p><strong>Blog Post:</strong> <a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a></p></li>
<li><p><strong>How it Works:</strong> Processes requests as a stream of individual token generation steps, allowing for dynamic addition and removal of requests within a batch.</p></li>
<li><p><strong>Benefits:</strong></p>
<ul>
<li>Eliminates the need to wait for a complete batch before processing, reducing latency.</li>
<li>Enables efficient handling of requests with varying lengths.</li>
</ul></li>
<li><p><strong>Consequences:</strong></p>
<ul>
<li>Results in dynamic batch sizes, making performance less predictable.</li>
<li>Requires careful consideration of performance SLAs and user experience.</li>
</ul></li>
</ul>
</section>
<section id="inference-servers" class="level4">
<h4 class="anchored" data-anchor-id="inference-servers">Inference Servers</h4>
<p>Various inference servers are available, each with its own strengths and weaknesses:</p>
<ul>
<li><strong>Examples:</strong> <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://huggingface.co/docs/text-generation-inference/en/index">TGI</a>, <a href="https://fastgen.com/">FastGen</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, <a href="https://github.com/sgl-project/sglang">SGLang</a>, <a href="https://ollama.com/">Ollama</a>, <a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a>, <a href="https://github.com/turboderp/exllama">Exllama</a>, <a href="https://github.com/mlc-ai/mlc-llm">MLC</a>, <a href="https://github.com/predibase/lorax">LoRAX</a></li>
<li><strong>Common Features:</strong> Continuous batching, specialized kernels, support for different optimization techniques.</li>
</ul>
</section>
<section id="performance-tuning" class="level4">
<h4 class="anchored" data-anchor-id="performance-tuning">Performance Tuning</h4>
<p>Understanding and tuning for different performance metrics is crucial:</p>
<ul>
<li><strong>Total Tokens Per Second (Throughput):</strong> Measures the overall token generation rate across all requests.</li>
<li><strong>Single Stream Tokens Per Second (Latency):</strong> Measures the token generation rate for a single request, reflecting user experience.</li>
<li><strong>Requests Per Second:</strong> Measures how many requests can be completed per second.</li>
</ul>
<p><strong>Key Considerations:</strong></p>
<ul>
<li>Increasing batch size generally improves throughput but reduces single-stream performance.</li>
<li>Finding the right balance between these metrics depends on the specific use case and desired user experience.</li>
<li>Clearly define performance SLOs and consider both throughput and latency when evaluating performance.</li>
</ul>
</section>
</section>
<section id="simplifying-llm-deployment" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-llm-deployment">Simplifying LLM Deployment</h3>
<section id="prioritize-modularity" class="level4">
<h4 class="anchored" data-anchor-id="prioritize-modularity">Prioritize Modularity</h4>
<p>Building a modular LLM serving stack is essential for navigating the challenges of the rapidly evolving technology landscape.</p>
<ul>
<li><strong>Benefits of Modularity:</strong>
<ul>
<li><strong>Flexibility:</strong> Easily switch between different serving frameworks as needed to leverage new features or optimizations.</li>
<li><strong>Experimentation:</strong> Enables efficient testing and comparison of different frameworks and configurations.</li>
</ul></li>
<li><strong>Challenges:</strong>
<ul>
<li><strong>Compatibility Issues:</strong> Features and optimizations from different frameworks may not always work together seamlessly.</li>
<li><strong>Lack of Documentation:</strong> New features and their interactions may not be well-documented, requiring experimentation and debugging.</li>
</ul></li>
</ul>
</section>
</section>
<section id="simplify-llm-deployment-with-replicate" class="level3">
<h3 class="anchored" data-anchor-id="simplify-llm-deployment-with-replicate">Simplify LLM Deployment with Replicate</h3>
<p>Replicate is a serverless infrastructure that aims to simplify LLM deployment and experimentation.</p>
<section id="replicate-features-and-workflow" class="level4">
<h4 class="anchored" data-anchor-id="replicate-features-and-workflow">Replicate Features and Workflow</h4>
<ul>
<li><strong><a href="https://cog.run/">COG</a>:</strong> Open-source tool for packaging models and serving code, providing control over the serving framework.
<ul>
<li><a href="https://github.com/replicate/cog-vllm">Cog-vLLM</a>: Run vLLM on Replicate</li>
</ul></li>
<li><strong>Hugging Face Integration:</strong> Streamlined workflow for pulling and deploying models from Hugging Face.</li>
<li><strong>Performance Optimizations:</strong> Caching mechanisms and other optimizations to improve model download and cold boot times.</li>
<li><strong>Open Source Approach:</strong> Replicate’s model serving infrastructure is open source, allowing for customization and contributions.</li>
</ul>
<p><strong>Workflow Example:</strong></p>
<ol type="1">
<li><strong>Create a Training:</strong> Specify the model, Hugging Face ID, and other configurations through Replicate’s web interface.</li>
<li><strong>Transfer Weights:</strong> Replicate downloads weights from Hugging Face and pushes them to its optimized storage.</li>
<li><strong>Deploy and Access Model:</strong> Once the training is complete, the model is deployed and accessible through Replicate’s API or client libraries.</li>
<li><strong>Customize with COG:</strong> Utilize COG to customize the serving environment, experiment with different frameworks, and add features.</li>
</ol>
<p><strong>Key Advantages:</strong></p>
<ul>
<li><strong>Simplified Deployment:</strong> Replicate abstracts away infrastructure complexities, making it easy to deploy and serve models.</li>
<li><strong>Framework Flexibility:</strong> Supports multiple serving frameworks like vLLM and TRT-LLM, allowing for experimentation and optimization.</li>
<li><strong>Open Source and Customizable:</strong> Provides transparency and control over the serving environment.</li>
</ul>
</section>
</section>
</section>
<section id="lessons-from-building-a-serverless-platform---predibase" class="level2">
<h2 class="anchored" data-anchor-id="lessons-from-building-a-serverless-platform---predibase">Lessons from Building A Serverless Platform - Predibase</h2>
<section id="predibase-overview" class="level3">
<h3 class="anchored" data-anchor-id="predibase-overview">Predibase Overview</h3>
<ul>
<li><a href="https://predibase.com/">Predibase</a> is a managed platform for fine-tuning and serving LLMs.</li>
<li>It offers an end-to-end solution for prompting, fine-tuning, and deploying LLMs serverlessly or in dedicated environments.</li>
</ul>
</section>
<section id="the-case-for-fine-tuned-llms" class="level3">
<h3 class="anchored" data-anchor-id="the-case-for-fine-tuned-llms">The Case for Fine-Tuned LLMs</h3>
<ul>
<li><strong>General Intelligence vs.&nbsp;Task Specificity:</strong> General-purpose LLMs like ChatGPT are powerful but inefficient for specific tasks. Fine-tuning allows for models tailored to specific business needs, reducing cost and latency.</li>
<li><strong>Cost of Serving Multiple Models:</strong> Serving numerous fine-tuned models on dedicated deployments becomes expensive.</li>
<li><strong><a href="https://github.com/predibase/lorax">LoRAX</a> - A Solution for Efficient Serving:</strong>
<ul>
<li>Lorax is an open-source framework built on HuggingFace’s TGI, designed for efficient fine-tuned LLM inference.</li>
<li>It enables serving multiple fine-tuned models concurrently on a single deployment by sharing base model parameters and using heterogeneous batching of LoRA adapters.</li>
<li>This approach results in significant cost savings compared to dedicated deployments or fine-tuning via OpenAI’s API.</li>
</ul></li>
</ul>
</section>
<section id="deploying-your-fine-tuned-model-practical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="deploying-your-fine-tuned-model-practical-considerations">Deploying Your Fine-Tuned Model: Practical Considerations</h3>
<section id="merging-adapters-pros-and-cons" class="level4">
<h4 class="anchored" data-anchor-id="merging-adapters-pros-and-cons">Merging Adapters: Pros and Cons</h4>
<ul>
<li><strong>Merging:</strong>
<ul>
<li><strong>Pros:</strong> Better baseline performance by eliminating the overhead of processing LoRA layers at runtime.</li>
<li><strong>Cons:</strong> Limits flexibility in serving multiple fine-tunes, incompatibility with certain adapters (e.g., DORA, speculative decoding), potential quantization challenges, increased disk space.</li>
</ul></li>
<li><strong>Not Merging:</strong>
<ul>
<li><strong>Pros:</strong> Allows serving multiple fine-tuned models and the base model on a single deployment, facilitates A/B testing and rapid iteration, compatibility with various adapter types.</li>
<li><strong>Cons:</strong> Potential performance overhead due to processing LoRA layers at runtime.</li>
</ul></li>
<li><strong>Decision:</strong> Whether to merge depends on individual needs and constraints.</li>
</ul>
</section>
<section id="quantization-for-training-and-inference" class="level4">
<h4 class="anchored" data-anchor-id="quantization-for-training-and-inference">Quantization for Training and Inference</h4>
<ul>
<li><strong>Challenge:</strong> Models trained with QLoRA (quantized) often show performance degradation when served using FP16 (full precision). Serving with QLoRA is slow.</li>
<li><strong>Solution:</strong> Dequantize the QLoRA weights to FP16 for inference. This maintains numerical equivalence with the quantized weights while enabling faster inference.</li>
</ul>
</section>
</section>
<section id="performance-tuning-1" class="level3">
<h3 class="anchored" data-anchor-id="performance-tuning-1">Performance Tuning</h3>
<section id="gathering-requirements" class="level4">
<h4 class="anchored" data-anchor-id="gathering-requirements">Gathering Requirements</h4>
<ul>
<li><strong>Factors:</strong> Queries per second, input/output token distribution, number of adapters.</li>
<li><strong>Impact:</strong> These factors influence ideal batch size, target throughput, and latency.</li>
<li><strong>SLOs:</strong> Define service level objectives for peak throughput, latency, and cost.</li>
</ul>
</section>
<section id="deployment-requirements" class="level4">
<h4 class="anchored" data-anchor-id="deployment-requirements">Deployment Requirements</h4>
<ul>
<li><strong>VRAM Estimation:</strong> Allocate at least 1.5x the model weights for serving, considering activations, adapters, and KV cache.</li>
</ul>
</section>
<section id="key-questions-for-choosing-deployment-options" class="level4">
<h4 class="anchored" data-anchor-id="key-questions-for-choosing-deployment-options">Key Questions for Choosing Deployment Options</h4>
<ul>
<li>VRAM needs</li>
<li>Requests per second</li>
<li>Request distribution</li>
<li>Maximum acceptable latency</li>
<li>Willingness to sacrifice quality for cost</li>
<li>Number of tasks</li>
</ul>
</section>
<section id="serverless-vs.-dedicated-deployment" class="level4">
<h4 class="anchored" data-anchor-id="serverless-vs.-dedicated-deployment">Serverless vs.&nbsp;Dedicated Deployment</h4>
<ul>
<li><strong>Serverless:</strong> Suitable for low to medium, uniformly distributed requests with latency tolerance on the order of seconds.</li>
<li><strong>Dedicated:</strong> More appropriate for high, spiky request volumes, batch processing, or when strict latency and throughput SLOs are critical.</li>
</ul>
</section>
</section>
<section id="fine-tuning-for-throughput" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-for-throughput">Fine-Tuning for Throughput</h3>
<ul>
<li><strong>Shifting the Paradigm:</strong> Move beyond focusing solely on quality and leverage fine-tuning for performance improvements.</li>
</ul>
<section id="addressing-performance-differences" class="level4">
<h4 class="anchored" data-anchor-id="addressing-performance-differences">Addressing Performance Differences:</h4>
<ul>
<li>Fine-tuned models with adapters often show slower throughput compared to base models.</li>
</ul>
</section>
<section id="speculative-decoding---the-medusa-approach" class="level4">
<h4 class="anchored" data-anchor-id="speculative-decoding---the-medusa-approach">Speculative Decoding - The Medusa Approach:</h4>
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2401.10774">MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li>
<li>Fine-tune additional projections to predict future tokens, improving throughput by reducing the number of forward passes.</li>
<li>Implement verification steps to ensure only correct tokens are accepted.</li>
</ul>
</section>
<section id="combining-quality-and-performance-with-lookahead-lora" class="level4">
<h4 class="anchored" data-anchor-id="combining-quality-and-performance-with-lookahead-lora">Combining Quality and Performance with Lookahead LoRA:</h4>
<ul>
<li>Fine-tune adapters to predict multiple tokens ahead (lookahead) while maintaining task-specific accuracy.</li>
<li>This approach has shown significant throughput improvements (2-3x) compared to base models and standard LoRA adapters.</li>
</ul>
</section>
</section>
<section id="demonstration" class="level3">
<h3 class="anchored" data-anchor-id="demonstration">Demonstration</h3>
<ul>
<li>A live demo showcased the throughput differences between a base Medusa model, a fine-tuned Medusa model, and a model using lookahead LoRA for a code generation task.</li>
<li>The lookahead LoRA model achieved significantly higher throughput, highlighting the potential of this technique.</li>
</ul>
</section>
</section>
<section id="batch-vs-real-time-and-modal" class="level2">
<h2 class="anchored" data-anchor-id="batch-vs-real-time-and-modal">Batch vs Real Time and Modal</h2>
<section id="throughput-vs.-latency" class="level3">
<h3 class="anchored" data-anchor-id="throughput-vs.-latency">Throughput vs.&nbsp;Latency</h3>
<ul>
<li><strong>Defining “Slow”</strong>: A system can be slow due to low throughput (handling few requests per unit time) or high latency (taking long to process a single request).</li>
<li><strong>Throughput</strong>: Measured in requests completed per unit time.
<ul>
<li>Relevant for batch tasks like recommendation systems, evaluations, and CI/CD.</li>
<li>Constraints often stem from upstream/downstream systems.</li>
</ul></li>
<li><strong>Latency</strong>: Measured in time taken to complete a single request.
<ul>
<li>Crucial for real-time applications like chatbots, copilots, and guardrails.</li>
<li>Human perception is the primary constraint (target ~200ms total system latency).</li>
</ul></li>
<li><strong>Cost</strong>: The hidden factor influencing throughput and latency. More resources generally improve both but at a cost.</li>
</ul>
</section>
<section id="latency-lags-throughput" class="level3">
<h3 class="anchored" data-anchor-id="latency-lags-throughput">Latency Lags Throughput</h3>
<ul>
<li><strong>Paper:</strong> <a href="https://dl.acm.org/doi/pdf/10.1145/1022594.1022596">Latency Lags Bandwidth</a></li>
<li><strong>Latency Improvements Are Hard</strong>: Historically, improving bandwidth has been easier than reducing latency due to fundamental engineering and physical limitations (e.g., speed of light).
<ul>
<li><strong>GPUs Exemplify This</strong>: GPUs are optimized for throughput with large areas dedicated to processing, while CPUs prioritize latency with a focus on caching and control flow.</li>
</ul></li>
</ul>
<section id="gpus-are-inherently-throughput-oriented." class="level4">
<h4 class="anchored" data-anchor-id="gpus-are-inherently-throughput-oriented.">GPUs are inherently throughput-oriented.</h4>
<ul>
<li><strong>Textbook:</strong> <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processors: A Hands-on Approach</a></li>
<li>GPUs have much larger areas dedicated to processing units (ALUs) compared to CPUs, which prioritize caching and control flow for lower latency.</li>
<li>This design difference allows GPUs to achieve significantly higher memory throughput, making them suitable for high-throughput tasks like LLM inference.</li>
</ul>
</section>
<section id="llm-inference-challenges" class="level4">
<h4 class="anchored" data-anchor-id="llm-inference-challenges">LLM Inference Challenges</h4>
<ul>
<li><strong>Throughput</strong>: Easily scalable by increasing batch size (with some latency tradeoffs).</li>
<li><strong>Latency</strong>: Much harder to optimize.
<ul>
<li><strong>Techniques for Improvement</strong>: Quantization, model distillation, truncation, faster hardware, and highly optimized software (e.g., CUDA kernels).</li>
<li><strong>Extreme Latency Optimization</strong>: Running models entirely on cache memory (SRAM), as done by <a href="https://wow.groq.com/lpu-inference-engine/">Groq’s LPU</a>, significantly reduces latency but may impact throughput per dollar.</li>
</ul></li>
</ul>
</section>
</section>
<section id="costs-are-high-but-falling." class="level3">
<h3 class="anchored" data-anchor-id="costs-are-high-but-falling.">Costs are high but falling.</h3>
<ul>
<li><strong>Good News</strong>: LLM inference costs are decreasing faster than Moore’s law due to hardware, algorithmic, and R&amp;D advancements.
<ul>
<li><strong>Cognitive Capability for Fixed Price</strong>: $20/megatoken now buys GPT-4 level output, significantly higher capability than what was possible a year ago.</li>
<li><strong>Falling Costs for Fixed Capability</strong>: Achieving chatGPT-level performance is now possible at a fraction of the cost compared to two years ago.</li>
</ul></li>
<li><strong>Implication</strong>: Holding onto a fixed budget and waiting for capabilities to improve is a viable strategy.</li>
</ul>
</section>
<section id="deploying-llms-on-modal" class="level3">
<h3 class="anchored" data-anchor-id="deploying-llms-on-modal">Deploying LLMs on Modal</h3>
<ul>
<li><strong>Modal’s Value Proposition</strong>:
<ul>
<li><strong>High Throughput</strong>: Easy scaling to hundreds of A100s for large-scale fine-tuning and batch inference.</li>
<li><strong>Manageable Latency</strong>: Balancing latency and cost is achievable for models up to 13B parameters, suitable for certain latency-sensitive applications.</li>
<li><strong>Competitive Cost</strong>: Offers competitive GPU pricing with potential for high utilization savings, especially for spiky workloads.</li>
</ul></li>
<li><strong>Beyond GPUs</strong>: Modal provides a complete serverless runtime with storage, compute, and web service capabilities, enabling tasks beyond just LLM inference.</li>
</ul>
</section>
<section id="modal-is-for-more-than-gpus" class="level3">
<h3 class="anchored" data-anchor-id="modal-is-for-more-than-gpus">Modal is for more than GPUs</h3>
<ul>
<li>Modal is not just a serverless GPU platform; it’s a complete runtime environment.</li>
<li><strong>Key Features</strong>:
<ul>
<li><strong>Storage</strong>: Distributed file system, queues, dictionaries, and the ability to mount local and web data.</li>
<li><strong>Compute</strong>: Functions, GPU acceleration, and other serverless compute options.</li>
<li><strong>Web Services</strong>: Web endpoints and server capabilities for deploying applications.</li>
</ul></li>
</ul>
</section>
<section id="demos" class="level3">
<h3 class="anchored" data-anchor-id="demos">Demos</h3>
<ul>
<li><strong>Code:</strong> <a href="https://modal.com/docs/examples">https://modal.com/docs/examples</a></li>
<li><strong>Abliteration LLM</strong>: Demonstrated an LLM modified to remove certain responses (e.g., refusing harmful requests). This demo encountered technical difficulties.
<ul>
<li><strong>Blog Post:</strong> <a href="https://huggingface.co/blog/mlabonne/abliteration">Uncensor any LLM with abliteration</a></li>
</ul></li>
<li><strong>Batch Inference with TRT LLM</strong>: Showcased running batch inference on Llama-3 8B using TRT LLM, achieving high throughput.</li>
<li><strong>Hot Reloading Development Server</strong>: Demonstrated the ability to make code changes locally and have them automatically redeployed on Modal.</li>
<li><strong>OpenAI Compatible Endpoint</strong>: Showcased running an OpenAI compatible endpoint on Modal using vLLM, allowing integration with tools like Instructor.</li>
</ul>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="awq-in-honeycomb-example" class="level3">
<h3 class="anchored" data-anchor-id="awq-in-honeycomb-example">AWQ in Honeycomb Example</h3>
<ul>
<li><strong>Question:</strong> Clarification sought on the mention of AWQ in the Honeycomb example.</li>
<li><strong>Answer:</strong> AWQ (quantization technique) is highlighted as a tool for model quantization, compatible and easily integrable with vLLM. The speaker shares their preference for using default or documented settings for quantization without delving into extensive customization.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></li>
</ul></li>
</ul>
</section>
<section id="pricing-fine-tuning-projects-for-enterprises" class="level3">
<h3 class="anchored" data-anchor-id="pricing-fine-tuning-projects-for-enterprises">Pricing Fine-Tuning Projects for Enterprises</h3>
<ul>
<li><strong>Question:</strong> Advice sought on determining pricing for enterprise fine-tuning projects.</li>
<li><strong>Answer:</strong> Advises against hourly rates and recommends a value-based approach, which involves:
<ul>
<li>Understanding the client’s problem and its importance.</li>
<li>Identifying key metrics the client aims to improve.</li>
<li>Collaborating to determine the project’s value to the client.
<ul>
<li>If the client does not know, should probably not take the project</li>
</ul></li>
<li>Proposing a reasonable fraction of that value as the price.</li>
</ul></li>
</ul>
</section>
<section id="gpu-optimization-in-modal-with-vllms-async-engine" class="level3">
<h3 class="anchored" data-anchor-id="gpu-optimization-in-modal-with-vllms-async-engine">GPU Optimization in Modal with vLLM’s Async Engine</h3>
<ul>
<li><strong>Question:</strong> Inquiry about optimizing GPU usage in Modal when using vLLM’s asynchronous engine and limiting concurrent requests instead of batch size.</li>
<li><strong>Answer:</strong> Charles Frye emphasizes the importance of measuring actual GPU behavior:
<ul>
<li><strong>CUDA Kernel Utilization:</strong> Monitor using tools like NVIDIA SMI to understand GPU activity.</li>
<li><strong>FLOPs Utilization:</strong> Measure and compare the achieved floating-point operations per second against the system’s theoretical maximum.</li>
<li><strong>Wattage Consumption:</strong> Observe GPU power draw as a proxy for actual workload and potential bottlenecks.</li>
</ul></li>
</ul>
</section>
<section id="hiding-api-endpoints-in-model-serving-web-apps" class="level3">
<h3 class="anchored" data-anchor-id="hiding-api-endpoints-in-model-serving-web-apps">Hiding API Endpoints in Model-Serving Web Apps</h3>
<ul>
<li><strong>Question:</strong> Strategies sought for concealing API endpoints in a web application to prevent exposure through browser inspection tools.</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Proxy Server:</strong> Routing requests through a proxy to mask internal endpoints and implement protections.</li>
<li><strong>Accepting Limitations:</strong> Recognizing that completely hiding data flow from the client-side is challenging.</li>
</ul></li>
</ul>
</section>
<section id="impact-of-input-prompt-size-on-speed" class="level3">
<h3 class="anchored" data-anchor-id="impact-of-input-prompt-size-on-speed">Impact of Input Prompt Size on Speed</h3>
<ul>
<li><strong>Question:</strong> Clarification sought on how reducing input prompt size affects processing speed, given that the entire prompt is read at once.</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Prefill Impact:</strong> Smaller prompts reduce the initial encoding time (prefill), which can be significant for very large inputs.</li>
<li><strong>Attention Calculation:</strong> Shorter sequences lead to faster attention calculations due to the quadratic complexity of attention mechanisms.</li>
<li><strong>Practical Considerations:</strong> The impact might be negligible for moderately sized prompts but becomes increasingly relevant for very large inputs like books or lengthy PDFs.</li>
</ul></li>
</ul>
</section>
<section id="resources-for-learning-continuous-batching" class="level3">
<h3 class="anchored" data-anchor-id="resources-for-learning-continuous-batching">Resources for Learning Continuous Batching</h3>
<ul>
<li><strong>Question:</strong> Recommendation requested for resources to learn about continuous batching.</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Orca Paper:</strong> Referring to the original research paper on continuous batching.</li>
<li><strong>AnyScale Blog Post:</strong> <a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a></li>
<li><strong>Practical Experimentation:</strong> Emphasizing the value of hands-on experience, benchmarking, and analyzing performance variations.</li>
</ul></li>
</ul>
</section>
<section id="request-caching-layer-in-hugging-face-and-modal" class="level3">
<h3 class="anchored" data-anchor-id="request-caching-layer-in-hugging-face-and-modal">Request Caching Layer in Hugging Face and Modal</h3>
<ul>
<li><strong>Question:</strong> Inquiry about the availability of a request caching layer in Hugging Face and Modal.</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>KV Caching:</strong> The speakers clarify that some frameworks, like TRT-LLM and vLLM, offer KV caching, which can improve performance for requests sharing similar prefixes or chat history.</li>
<li><strong>Higher-Level Caching:</strong> Expanding on the concept, they discuss the possibility of centralized KV cache databases and even caching complete requests and responses for deterministic scenarios.</li>
<li><strong>Replicate’s Approach:</strong> Joe Hoover states that Replicate doesn’t currently provide explicit features for request caching but acknowledges it as a potential future consideration.</li>
</ul></li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/workshop-004/</guid>
  <pubDate>Wed, 17 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 6: Train Almost Any LLM Model Using 🤗 autotrain</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-006/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Introduction to AutoTrain</li>
<li>Getting Started with AutoTrain</li>
<li>Fine-tuning LLMs with AutoTrain</li>
<li>Training Your Model</li>
<li>Config Files and Advanced Options</li>
<li>Additional Features and Considerations</li>
<li>Q&amp;A Session</li>
</ul>
<section id="introduction-to-autotrain" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-autotrain">Introduction to AutoTrain</h2>
<ul>
<li><strong>Homepage:</strong> <a href="https://huggingface.co/autotrain">https://huggingface.co/autotrain</a></li>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/autotrain/index">https://huggingface.co/docs/autotrain/index</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/huggingface/autotrain-advanced">autotrain-advanced</a></li>
<li>Simplifies model training and fine-tuning for users with varying levels of expertise, from beginners to experienced data scientists.</li>
<li><strong>Supported Tasks:</strong>
<ul>
<li><strong>NLP:</strong> Token classification, text classification, LLM tasks (e.g., SFT, R4, DPO, reward tuning), sentence transformer fine-tuning, etc.</li>
<li><strong>Computer Vision:</strong> Image classification, Object Detection</li>
<li><strong>Tabular Data:</strong> Classification, Regression</li>
</ul></li>
<li>Leverages the Hugging Face ecosystem, including transformers, datasets, diffusers, and Accelerate, ensuring compatibility with the latest models and tools.</li>
</ul>
</section>
<section id="getting-started-with-autotrain" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-autotrain">Getting Started with AutoTrain</h2>
<ul>
<li><strong>Create a new project:</strong>
<ul>
<li><strong>Link:</strong> <a href="https://huggingface.co/login?next=%2Fspaces%2Fautotrain-projects%2Fautotrain-advanced%3Fduplicate%3Dtrue"><code>Create new project</code></a></li>
<li>Optionally specify an organization and attach hardware (local or Hugging Face spaces).</li>
<li>Choose the desired task (e.g., LLM SFT).</li>
</ul></li>
<li><strong>User-Friendly Interface:</strong>
<ul>
<li>Select a task.</li>
<li>Upload your data or use a dataset from the Hugging Face Hub.</li>
<li>Configure parameters or use default settings.</li>
<li>Monitor training progress and logs.</li>
</ul></li>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/autotrain/quickstart_spaces#creating-a-new-autotrain-space">Creating a New AutoTrain Space</a></li>
</ul>
</section>
<section id="fine-tuning-llms-with-autotrain" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-llms-with-autotrain">Fine-tuning LLMs with AutoTrain</h2>
<section id="supervised-fine-tuning-sft-and-generic-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-fine-tuning-sft-and-generic-fine-tuning">Supervised Fine-tuning (SFT) and Generic Fine-tuning</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/trl/main/en/sft_trainer">Supervised Fine-tuning Trainer</a></li>
<li>Both trainers are similar, but SFT uses the TRL library’s SFT trainer.
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/trl/main/en/index">TRL - Transformer Reinforcement Learning</a></li>
</ul></li>
<li>Requires a “text” column in your dataset (can be mapped from a different column name).</li>
<li>Supports chat template formatting (chatML, Sapphire, tokenizer’s template).</li>
<li><strong>Example datasets:</strong>
<ul>
<li><a href="Salesforce/wikitext">Salesforce/wikitext</a>: plain text format</li>
<li>Chat format with “content” and “role” fields (requires chat template).</li>
</ul></li>
</ul>
</section>
<section id="reward-modeling" class="level3">
<h3 class="anchored" data-anchor-id="reward-modeling">Reward Modeling</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/trl/main/en/reward_trainer">Reward Modeling</a></li>
<li>Trains a custom reward model for sequence classification.</li>
<li>Dataset requires “chosen” and “rejected” text columns.</li>
</ul>
</section>
<section id="dpo-and-orpo" class="level3">
<h3 class="anchored" data-anchor-id="dpo-and-orpo">DPO and ORPO</h3>
<ul>
<li><strong>DPO - Direct Preference Optimization</strong>
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/trl/main/en/dpo_trainer">DPO Trainer</a></li>
</ul></li>
<li><strong>ORPO - Odds Ratio Preference Optimization</strong>
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/trl/main/en/orpo_trainer">ORPO Trainer</a></li>
</ul></li>
<li>ORPO is recommended over DPO as it requires less memory and compute.</li>
<li>Dataset requires “prompt,” “chosen,” and “rejected” columns (all conversations).</li>
<li>Supports chat templates.</li>
</ul>
</section>
</section>
<section id="training-your-model" class="level2">
<h2 class="anchored" data-anchor-id="training-your-model">Training Your Model</h2>
<section id="data-format" class="level3">
<h3 class="anchored" data-anchor-id="data-format">Data Format</h3>
<ul>
<li>Use CSV or JSON Lines (JSONL) format
<ul>
<li>JSONL preferred for readability and ease of use.</li>
</ul></li>
<li><strong>Format examples:</strong>
<ul>
<li>Alpaca dataset: Single “text” field with formatted text (no chat template needed).</li>
<li>Chat format: Requires chat template or offline conversion to plain text.</li>
</ul></li>
</ul>
</section>
<section id="training-locally" class="level3">
<h3 class="anchored" data-anchor-id="training-locally">Training Locally</h3>
<ul>
<li><p><strong>Documentation:</strong> <a href="https://huggingface.co/docs/autotrain/quickstart">Quickstart</a></p></li>
<li><p>Set Hugging Face token:</p>
<ul>
<li><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1">  <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">export</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">HF_TOKEN</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>your_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span></code></pre></div></li>
</ul></li>
<li><p>Run the AutoTrain app: <code>autotrain app</code></p>
<ul>
<li><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb2-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">autotrain</span> app <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--port</span> 8080 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--host</span> 127.0.0.1</span></code></pre></div></li>
</ul></li>
<li><p>Alternatively, use config files or CLI commands.</p>
<ul>
<li><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">autotrain</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--config</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>path_to_config_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span></code></pre></div></li>
</ul></li>
</ul>
<section id="local-installation" class="level4">
<h4 class="anchored" data-anchor-id="local-installation">Local Installation</h4>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">PIP</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Conda</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install autotrain-advanced</span></code></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> autotrain python=3.10</span>
<span id="cb5-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> activate autotrain</span>
<span id="cb5-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install autotrain-advanced</span>
<span id="cb5-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> install pytorch torchvision torchaudio pytorch-cuda=12.1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> pytorch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> nvidia</span>
<span id="cb5-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nvidia/label/cuda-12.1.0"</span> cuda-nvcc</span></code></pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training-on-other-platforms" class="level3">
<h3 class="anchored" data-anchor-id="training-on-other-platforms">Training on Other Platforms</h3>
<ul>
<li><strong><a href="https://jarvislabs.ai/">Jarvis Labs</a>:</strong> Provides AutoTrain templates for easy setup and training.</li>
<li><strong><a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">DGX Cloud</a>:</strong> Rent high-performance GPUs for training large models.</li>
<li><strong><a href="https://colab.research.google.com/">Google Colab</a>:</strong> Run AutoTrain directly in Colab using provided notebooks and UI.</li>
</ul>
</section>
</section>
<section id="config-files-and-advanced-options" class="level2">
<h2 class="anchored" data-anchor-id="config-files-and-advanced-options">Config Files and Advanced Options</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://huggingface.co/docs/autotrain/config">AutoTrain Configs</a></li>
<li>Config files offer more flexibility and control over training parameters.</li>
<li>Define task, base model, data paths, column mapping, hyperparameters, logging, and more.</li>
<li>Access example config files in the AutoTrain GitHub repository.
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/huggingface/autotrain-advanced/tree/main/configs">autotrain-advanced/configs</a></li>
</ul></li>
</ul>
</section>
<section id="additional-features-and-considerations" class="level2">
<h2 class="anchored" data-anchor-id="additional-features-and-considerations">Additional Features and Considerations</h2>
<ul>
<li>AutoTrain automatically handles multi-GPU training using DeepSpeed or distributed data parallel.</li>
<li>QLORA is supported on DeepSpeed for efficient training.</li>
<li>Sentence Transformer fine-tuning is available for tasks like improving RAG models.</li>
</ul>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<ul>
<li><strong>Logging:</strong> Supports Weights &amp; Biases (W&amp;B) logging when using config files.</li>
<li><strong>Mixed Precision:</strong> Supports BF16 and FP16, but not FP8.</li>
<li><strong>Parameter Compatibility:</strong> AutoTrain ensures parameter compatibility based on the chosen base model.</li>
<li><strong>Hyperparameter Optimization:</strong> Not currently supported for LLMs due to long training times.</li>
<li><strong>CPU Training:</strong> Possible, but may come with performance limitations.</li>
<li><strong>Custom Chat Templates:</strong> Can be added by modifying the <code>tokenizer_config.json</code> file of a cloned model.</li>
<li><strong>Synthetic Data Generation:</strong> Not currently supported, but users can generate their own.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-006/</guid>
  <pubDate>Fri, 12 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 6: Johno Whitaker</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-006/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key takeaways</h2>
<ul>
<li><strong>Striking a balance between GPU utilization and cache:</strong> While maximizing GPU utilization seems ideal, leaving some headroom for caching can actually improve performance.</li>
<li><strong>The economics of cloud GPU pricing:</strong> Faster, more expensive GPUs often end up being cost-effective due to reduced training time.</li>
<li><strong>Prioritizing alternative customization methods before fine-tuning:</strong> Consider techniques like prompt engineering and context injection before resorting to fine-tuning.</li>
<li><strong>The importance of quick iteration cycles in research:</strong> Start with smaller models and datasets to test ideas rapidly before scaling up.</li>
<li><strong>The value of understanding the tools and digging deeper:</strong> While off-the-shelf libraries are convenient, having a deeper understanding of the underlying code can be crucial for research and debugging.</li>
<li><strong>The potential of alternative hardware and algorithms:</strong> While GPUs and deep learning dominate the current landscape, there is hope for innovation with new programming paradigms and architectures.</li>
<li><strong>Focusing on practical skills and real-world applications:</strong> Building projects and gaining hands-on experience are invaluable, even if specific technologies become obsolete.</li>
<li><strong>The evolving role of LLMs in research:</strong> LLMs are becoming increasingly useful for tasks like paper retrieval and summarization, with the potential for even greater impact in the future.</li>
<li><strong>The importance of exploring diverse research directions:</strong> While chasing the latest trends can be tempting, exploring less crowded research areas can lead to novel and impactful contributions.</li>
</ul>
</section>
<section id="gpu-utilization-and-batch-size" class="level2">
<h2 class="anchored" data-anchor-id="gpu-utilization-and-batch-size">GPU Utilization and Batch Size</h2>
<ul>
<li>Increasing batch size generally improves performance, especially when memory bandwidth is a bottleneck.</li>
<li>However, pushing GPU memory utilization too close to 100% can hinder performance by limiting space for caching.</li>
<li>It’s crucial to leave some headroom for pre-caching layers and avoiding out-of-memory errors during evaluation or unexpected events.</li>
<li>Find the largest comfortable batch size with reasonable memory utilization.</li>
</ul>
</section>
<section id="balancing-compute-and-io-costs" class="level2">
<h2 class="anchored" data-anchor-id="balancing-compute-and-io-costs">Balancing Compute and IO Costs</h2>
<ul>
<li>While using more GPUs increases cost per hour, it also significantly reduces training time.</li>
<li>Cloud GPU pricing often reflects this trade-off, making faster GPUs cost-effective overall.</li>
<li>Consider the time cost of experimentation when choosing between more or fewer GPUs.</li>
<li>For multi-node setups, prioritize data parallelism across nodes and leverage memory-saving techniques within each node to minimize communication overhead.</li>
</ul>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<ul>
<li>Fine-tuning is not always necessary, as many models perform well with default settings or prompt engineering.</li>
<li>Focus on finding a model and basic configuration that learns effectively before extensively optimizing hyperparameters.</li>
<li>Consider the specific requirements of the application; stylistic formatting may benefit from fine-tuning, while direct knowledge integration might be better served by context injection.</li>
</ul>
</section>
<section id="fine-tuning-vs.-alternative-customization-methods" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-vs.-alternative-customization-methods">Fine-tuning vs.&nbsp;Alternative Customization Methods</h2>
<ul>
<li>Fine-tuning can be useful for customizing model behavior, but it’s not the only or always the best approach.</li>
<li>Explore alternative methods like prompt engineering, context injection, and retrieval-augmented generation.</li>
<li>Consider the trade-offs between fine-tuning efficiency, context length limitations, and the ability to incorporate external knowledge.</li>
</ul>
</section>
<section id="tpus-and-other-accelerators" class="level2">
<h2 class="anchored" data-anchor-id="tpus-and-other-accelerators">TPUs and Other Accelerators</h2>
<ul>
<li>TPUs offer high-speed memory access and interconnects, making them suitable for large-scale training.</li>
<li>The principles of memory management and optimization still apply, but the specific considerations might differ.</li>
<li>As dedicated hardware evolves, the lines between GPUs and other accelerators are blurring, with a focus on faster interconnects and larger memory pools.</li>
</ul>
</section>
<section id="optimizing-memory-usage-for-llama-models" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-memory-usage-for-llama-models">Optimizing Memory Usage for LLaMa Models</h2>
<ul>
<li>Quantization (e.g., QLoRA) can significantly reduce memory footprint without major performance degradation.</li>
<li>Consider using 4-bit quantization for base weights to free up memory.</li>
<li>Adjust batch size and gradient accumulation steps to accommodate longer sequences.</li>
<li>Be mindful of the memory overhead associated with larger vocabularies and embedding layers.</li>
</ul>
</section>
<section id="understanding-sequence-length-and-memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="understanding-sequence-length-and-memory-usage">Understanding Sequence Length and Memory Usage</h2>
<ul>
<li>Sequence length during training often represents a maximum value, and actual memory usage depends on the length of individual samples in a batch.</li>
<li>Padding to the maximum sequence length can waste compute resources.</li>
<li>Consider techniques like packing short sequences together or prioritizing longer sequences to optimize memory utilization.</li>
<li>Be aware of how truncating sequences during training might impact the model’s ability to learn effectively.</li>
</ul>
</section>
<section id="tips-for-quick-iteration-cycles" class="level2">
<h2 class="anchored" data-anchor-id="tips-for-quick-iteration-cycles">Tips for Quick Iteration Cycles</h2>
<ul>
<li>Start with smaller models and datasets to test code and ideas quickly.</li>
<li>Gradually increase model size and data complexity as needed.</li>
<li>Prioritize being able to evaluate results quickly, ideally within seconds or minutes.</li>
<li>Develop a workflow that allows for rapid testing and debugging without long waiting times.</li>
</ul>
</section>
<section id="choosing-the-right-tools-for-the-job" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-tools-for-the-job">Choosing the Right Tools for the Job</h2>
<ul>
<li>Off-the-shelf libraries like Axolotl and HuggingFace Trainer are convenient for standard training tasks.</li>
<li>For research and deeper understanding, consider using simpler training loops and libraries that provide more transparency and control.</li>
<li>Strive for a balance between ease of use and the ability to inspect and debug the underlying code.</li>
</ul>
</section>
<section id="cpu-offloading-and-gpu-memory-management" class="level2">
<h2 class="anchored" data-anchor-id="cpu-offloading-and-gpu-memory-management">CPU Offloading and GPU Memory Management</h2>
<ul>
<li>While CPU offloading exists, it’s often slow due to the speed difference between CPU and GPU RAM.</li>
<li>Explicitly manage CPU offloading rather than relying on automatic mechanisms.</li>
<li>Consider CPU offloading when dealing with very long sequences that exceed GPU memory capacity, even with quantization.</li>
</ul>
</section>
<section id="exploring-cpu-offloading-for-long-sequences" class="level2">
<h2 class="anchored" data-anchor-id="exploring-cpu-offloading-for-long-sequences">Exploring CPU Offloading for Long Sequences</h2>
<ul>
<li>CPU offloading might become more attractive as longer context lengths become increasingly important.</li>
<li>By storing weights on the CPU and transferring them to the GPU as needed, larger batches and longer sequences can be processed.</li>
<li>This approach trades off increased transfer time for the ability to handle more data within the GPU memory constraints.</li>
</ul>
</section>
<section id="curating-information-and-staying-up-to-date" class="level2">
<h2 class="anchored" data-anchor-id="curating-information-and-staying-up-to-date">Curating Information and Staying Up-to-Date</h2>
<ul>
<li>Develop a system for filtering and prioritizing information from various sources, such as colleagues, Twitter, and research papers.</li>
<li>Focus on areas of personal interest and relevance to current projects.</li>
<li>Actively seek out information that can be applied to real-world problems and experiments.</li>
</ul>
</section>
<section id="the-importance-and-joy-of-teaching" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-and-joy-of-teaching">The Importance and Joy of Teaching</h2>
<ul>
<li>Teaching is a rewarding way to learn, solidify understanding, and contribute to the community.</li>
<li>Sharing knowledge through blog posts, tutorials, and presentations can benefit both the teacher and the audience.</li>
<li>Engaging with questions and feedback from learners can spark new ideas and research directions.</li>
</ul>
</section>
<section id="the-hardware-lottery-and-future-of-ai" class="level2">
<h2 class="anchored" data-anchor-id="the-hardware-lottery-and-future-of-ai">The Hardware Lottery and Future of AI</h2>
<ul>
<li>The dominance of GPUs and deep learning might be partly due to historical coincidence rather than inherent superiority.</li>
<li>The significant investment in GPU-optimized hardware and software creates inertia.</li>
<li>Breaking free from this paradigm requires making alternative approaches more accessible and efficient.</li>
<li>Innovations in GPU programming, new hardware architectures, and a willingness to explore unconventional ideas offer hope for a more diverse AI landscape.</li>
</ul>
</section>
<section id="balancing-research-and-practical-skills" class="level2">
<h2 class="anchored" data-anchor-id="balancing-research-and-practical-skills">Balancing Research and Practical Skills</h2>
<ul>
<li>Pursuing purely novel research and building practical applications are distinct but valuable pursuits.</li>
<li>For practical impact, focus on solving current problems, learning industry-standard tools, and gaining experience with real-world data.</li>
<li>For research breakthroughs, explore less crowded areas, challenge assumptions, and develop a deep understanding of the fundamentals.</li>
</ul>
</section>
<section id="choosing-projects-for-skill-development" class="level2">
<h2 class="anchored" data-anchor-id="choosing-projects-for-skill-development">Choosing Projects for Skill Development</h2>
<ul>
<li>Engaging in projects, even if they become less relevant with time, provides valuable learning experiences and demonstrates technical skills.</li>
<li>Focus on projects that allow you to learn transferable skills like data processing, model evaluation, and problem-solving.</li>
<li>Stay adaptable and continuously update your skillset as the field evolves.</li>
</ul>
</section>
<section id="llms-impacting-research" class="level2">
<h2 class="anchored" data-anchor-id="llms-impacting-research">LLMs Impacting Research</h2>
<ul>
<li>LLMs are already proving useful for tasks like paper retrieval, summarization, and knowledge extraction.</li>
<li>Tools like undermind.ai showcase the potential of LLMs for improving research workflows.
<ul>
<li><strong><a href="https://undermind.ai/home/">undermind.ai</a>:</strong> During each search, undermind.ai examines results in stages, and uses language models to make key decisions, such as recognizing crucial information and adapting the search strategy.</li>
</ul></li>
<li>Future research directions include exploring more sophisticated applications of LLMs for tasks like knowledge synthesis, hypothesis generation, and experimental design.
<ul>
<li><strong><a href="https://sakana.ai/">sakana.ai</a>:</strong> On a quest to create a new kind of foundation model based on nature-inspired intelligence.
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/SakanaAI/evolutionary-model-merge">Evolutionary Optimization of Model Merging Recipes</a></li>
<li><strong>Blog Post:</strong> <a href="https://sakana.ai/llm-squared/">Can LLMs invent better ways to train LLMs?</a>:</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="rapid-fire-qa-highlights" class="level2">
<h2 class="anchored" data-anchor-id="rapid-fire-qa-highlights">Rapid-Fire Q&amp;A Highlights</h2>
<section id="research-interests-and-coding-style" class="level3">
<h3 class="anchored" data-anchor-id="research-interests-and-coding-style">Research Interests and Coding Style</h3>
<ul>
<li><strong>Current Focus:</strong> While still interested in GANs and diffusion models, Johno’s primary focus has shifted to LLMs.</li>
<li><strong>Coding Style:</strong>
<ul>
<li>Uses tools like Copilot for boilerplate code.</li>
<li>Emphasizes clear, tutorial-like code for better Copilot integration and beginner comprehension.</li>
<li>Values explicitness over extreme code compression, especially when teaching.</li>
<li>Adopts a mix of notebooks (VS Code, Cursor, Jupyter Classic) and scripts depending on the project.
<ul>
<li><strong><a href="https://www.cursor.com/">Cursor</a>:</strong> The AI code editor</li>
</ul></li>
<li>Leverages keyboard shortcuts and learns from colleagues like Jeremy Howard for efficiency.</li>
</ul></li>
</ul>
</section>
<section id="quantization-and-long-context" class="level3">
<h3 class="anchored" data-anchor-id="quantization-and-long-context">Quantization and Long Context</h3>
<ul>
<li><strong>Quantization Overhead:</strong> While quantization methods like QLoRA reduce memory footprint, they introduce computational overhead due to decompression, sometimes impacting performance.</li>
<li><strong>Long Context Challenges:</strong> Initially, QLoRA faced memory efficiency issues with long context lengths compared to LoRA, potentially due to gradient checkpointing implementations.
<ul>
<li>This has been addressed to some extent.</li>
</ul></li>
<li><strong>Unexpected Behavior:</strong> Practical implementations often reveal unexpected behavior compared to theoretical calculations.
<ul>
<li>Bugs, precision errors, and data duplication can arise.</li>
</ul></li>
</ul>
</section>
<section id="bit-llms-and-hybrid-approaches" class="level3">
<h3 class="anchored" data-anchor-id="bit-llms-and-hybrid-approaches">1.58-bit LLMs and Hybrid Approaches</h3>
<ul>
<li><strong>1-bit and 1.58-bit LLMs:</strong> Microsoft’s research explored extreme weight compression using 1-bit (-1, 0, 1) representation, achieving workable but diminished performance.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2402.17764">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li>
</ul></li>
<li><strong>Hybrid Approach Potential:</strong> Whitaker believes combining extreme quantization with techniques like LoRA adapters could offer a sweet spot between memory efficiency, speed, and accuracy.
<ul>
<li>Base weights in 1-bit or 2-bit.</li>
<li>LoRA adapters for fine-tuning and accuracy recovery.</li>
<li>Hardware-efficient kernels for low-bit operations.</li>
</ul></li>
<li><strong>Mobius Labs:</strong> Highlighted for their work on efficient kernels and proof-of-concept implementations of hybrid approaches.
<ul>
<li><strong>Homepage:</strong> <a href="https://www.mobiuslabs.com/">Mobius Labs</a>: Multimodal AI for the world’s scale.</li>
<li><strong>Blog Post:</strong> <a href="https://mobiusml.github.io/hqq_blog/">Half-Quadratic Quantization of Large Machine Learning Models</a></li>
</ul></li>
</ul>
</section>
<section id="alternative-architectures" class="level3">
<h3 class="anchored" data-anchor-id="alternative-architectures">Alternative Architectures</h3>
<ul>
<li><strong>Exploring Alternatives:</strong> While transformers dominate, alternative architectures like KAN, state-space models (SSMs), and recurrent models offer benefits in specific areas.
<ul>
<li><strong>State-Space Models:</strong>
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/state-spaces/mamba">mamba</a></li>
</ul></li>
<li><strong>KAN: Kolmogorov Arnold Networks:</strong>
<ul>
<li><strong>Paper: KAN:</strong> <a href="https://arxiv.org/abs/2404.19756">Kolmogorov-Arnold Networks</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/KindXiaoming/pykan">pykan</a></li>
</ul></li>
</ul></li>
<li><strong>SSMs and Long Sequences:</strong> SSMs show promise for tasks involving long sequences, such as text-to-speech and DNA analysis.</li>
<li><strong>Practical Implications:</strong> The emergence of viable alternatives provides practitioners with more tools. The core focus remains on performance and benchmark results.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-006/</guid>
  <pubDate>Thu, 11 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 5: Napkin Math For Fine Tuning with Johno Whitaker</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-005/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Introduction</li>
<li>Good News &amp; Bad News</li>
<li>Training Neural Networks</li>
<li>Why are we copying data around?</li>
<li>Goal: Keep the GPU Fed</li>
<li>Napkin Math: Understanding Memory Usage</li>
<li>Napkin Math Code Demo</li>
<li>Optimizing LLM Training for Different Hardware</li>
<li>Q&amp;A Session</li>
<li>Recap</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Resources">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Presentation Resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Slides:</strong> <a href="https://docs.google.com/presentation/d/1Ye_6zeatCWkq-fx8A--yK34uwU8oC2YQtMSTV1DgkSI/">Napkin Math For Finetuning</a></li>
</ul>
</div>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li><strong>Goal:</strong> To provide insights into the factors influencing fine-tuning performance, enabling better decision-making in training models.</li>
<li><strong>Target audience:</strong> Individuals new to training models, particularly by fine-tuning existing large language models.</li>
<li><strong>Key questions addressed:</strong>
<ul>
<li>Factors affecting model performance.</li>
<li>Strategies for improving performance.</li>
<li>Reasons for memory limitations and slow training times.</li>
<li>Understanding and adjusting various parameters in configuration files.</li>
</ul></li>
<li><strong>Approach:</strong> Utilizing a “napkin math” approach to provide a general understanding of the concepts without delving into intricate mathematical details.</li>
<li><strong>Disclaimer:</strong> Emphasizes that the information presented is simplified for clarity and may not be entirely accurate due to the constantly evolving nature of AI implementations.</li>
</ul>
</section>
<section id="good-news-bad-news" class="level2">
<h2 class="anchored" data-anchor-id="good-news-bad-news">Good News &amp; Bad News</h2>
<ul>
<li><strong>Good news:</strong> The mathematical operations underlying model training are well-understood, enabling analysis and experimentation.</li>
<li><strong>Bad news:</strong>
<ul>
<li>Discrepancies can exist between research papers, code implementations, and framework-specific implementations.</li>
<li>The complexity of the underlying processes can be daunting.</li>
<li>Keeping track of all the details and nuances is challenging.</li>
<li>Multi-GPU setups introduce additional complexity</li>
</ul></li>
<li><strong>Key takeaway:</strong> While a simplified approach can be helpful, acknowledging the inherent complexities is essential for accurate analysis and problem-solving.</li>
</ul>
</section>
<section id="training-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="training-neural-networks">Training Neural Networks</h2>
<section id="training-loop" class="level4">
<h4 class="anchored" data-anchor-id="training-loop">Training Loop</h4>
<ul>
<li><strong>The core cycle:</strong> Load data, feed it through the model, generate an answer, evaluate its quality, and update the model accordingly.</li>
<li><strong>Language model context:</strong> The correct answer typically refers to the next word in a sequence, and predictions are represented as probabilities for potential next words.</li>
<li><strong>Fine-tuning data:</strong> Datasets used for fine-tuning contain instruction-response pairs, allowing the model to learn from desired output patterns.</li>
</ul>
</section>
<section id="on-computers" class="level4">
<h4 class="anchored" data-anchor-id="on-computers">On Computers</h4>
<ul>
<li><strong>Hardware components:</strong> Understanding the roles of CPU, GPU, RAM, and their interconnectivity is crucial for performance analysis.</li>
<li><strong>Memory hierarchy:</strong> Different memory types (e.g., CPU cache, RAM, hard drive) have varying access speeds, influencing data transfer times.</li>
</ul>
</section>
<section id="training-neural-networks-1" class="level4">
<h4 class="anchored" data-anchor-id="training-neural-networks-1">Training Neural Networks</h4>
<ul>
<li><strong>Factors affecting performance:</strong>
<ul>
<li><strong>Data loading:</strong> Reading data from storage.</li>
<li><strong>Model computation:</strong> Performing mathematical operations within the model’s layers.</li>
<li><strong>Parameter storage:</strong> Memory required to store model parameters (weights and biases).</li>
<li><strong>Gradient calculation and storage:</strong> Computing and storing gradients for model updates.</li>
<li><strong>Optimizer operations:</strong> Additional storage and computations for parameter optimization.</li>
</ul></li>
<li><strong>Key takeaway:</strong> Each step in the training loop incurs computational costs and memory demands, understanding these factors is crucial for optimization.</li>
</ul>
</section>
<section id="what-takes-up-time" class="level4">
<h4 class="anchored" data-anchor-id="what-takes-up-time">What takes up time?</h4>
<ul>
<li><strong>Computation:</strong> The number of mathematical operations performed.</li>
<li><strong>Memory management:</strong> Data transfer and storage within the memory hierarchy.</li>
</ul>
</section>
</section>
<section id="why-are-we-copying-data-around" class="level2">
<h2 class="anchored" data-anchor-id="why-are-we-copying-data-around">Why are we copying data around?</h2>
<ul>
<li><strong>Memory hierarchy and data transfer:</strong>
<ul>
<li><strong>Different memory types:</strong> CPU cache, RAM, GPU RAM, hard drives, etc., have varying speeds.</li>
<li><strong>Data movement:</strong> Copying data between these memory locations consumes time, impacting performance.</li>
</ul></li>
<li><strong>Optimizing data flow:</strong> Minimizing unnecessary data transfers and utilizing faster memory options are crucial for optimization.</li>
<li><strong>Multi-GPU and distributed training:</strong>
<ul>
<li><strong>Inter-GPU communication:</strong> Sharing data between GPUs can introduce latency.</li>
<li><strong>Network communication:</strong> In multi-node setups, communication over the network becomes a bottleneck.</li>
</ul></li>
</ul>
</section>
<section id="goal-keep-the-gpu-fed" class="level2">
<h2 class="anchored" data-anchor-id="goal-keep-the-gpu-fed">Goal: Keep the GPU Fed</h2>
<ul>
<li><strong>Goal:</strong> Continuously provide the GPU with data and instructions to avoid downtime.</li>
<li><strong>Ideal:</strong> Fit the entire model and data within the GPU RAM for fastest processing.</li>
<li><strong>Bottleneck:</strong> Large models and limited GPU memory can cause frequent data loading from slower memory, slowing down the training process.</li>
</ul>
<section id="tricks-to-improve-memory-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="tricks-to-improve-memory-efficiency">Tricks to improve memory efficiency</h3>
<ul>
<li><strong>Techniques for keeping data closer to the GPU:</strong>
<ul>
<li><strong>Flash Attention and Fused Kernels:</strong> These reduce memory footprints and data transfers by changing how computations are performed.</li>
<li><strong>Gradient Checkpointing (Activation Checkpointing):</strong> Trades a small increase in compute time for significant memory savings by selectively recomputing activations during backpropagation.</li>
<li><strong>CPU Offloading:</strong> Leverages larger CPU RAM by temporarily storing parts of the model or data that are not actively being used on the GPU.</li>
<li><strong>LoRA (Low-Rank Adaptation):</strong>
<ul>
<li>Freeze most model parameters and train only a small set of adapter parameters.</li>
<li>Reduces memory requirements for gradients and optimizer states.</li>
</ul></li>
<li><strong>Quantization:</strong>
<ul>
<li>Represent model weights using fewer bits (e.g., 8-bit instead of 32-bit).</li>
<li>Reduces memory footprint, allowing for larger models or larger batch sizes.</li>
<li>Needs a little computation to dequantize</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="napkin-math-understanding-memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="napkin-math-understanding-memory-usage">Napkin Math: Understanding Memory Usage</h2>
<section id="full-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="full-fine-tuning">Full Fine-Tuning</h3>
<ul>
<li><strong>Problem:</strong> Full fine-tuning requires storing model parameters, gradients, and optimizer states, leading to high memory consumption.</li>
<li><strong>Explanation:</strong>
<ul>
<li>Each parameter in a model requires multiple bits (e.g., 32 bits) to represent its numeric value.</li>
<li>Gradients for each parameter are stored during training, consuming the same amount of memory as the parameters.</li>
<li>Optimizers like Adam store additional states (e.g., momentum), further increasing memory usage.</li>
</ul></li>
<li><strong>Example:</strong> A model with 100 million parameters using 32 bits per parameter consumes 400MB.
<ul>
<li>Considering gradients and optimizer states, the total memory usage can be 1.2GB or higher.</li>
</ul></li>
</ul>
</section>
<section id="lora-low-rank-adaptation" class="level3">
<h3 class="anchored" data-anchor-id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</h3>
<ul>
<li><strong>Solution:</strong> LoRA reduces memory usage by only training a small subset of parameters while keeping the rest frozen.
<ul>
<li><strong>Example:</strong> For every <code>1000 x 1000</code> matrix, add a <code>1000 x 32</code> matrix</li>
</ul></li>
<li><strong>Explanation:</strong>
<ul>
<li>Only trainable parameters require gradients and optimizer state storage.</li>
<li>LoRA adds a small number of trainable parameters (e.g., 1% of the model size) to each large matrix in the model.</li>
</ul></li>
<li><strong>Benefit:</strong> LoRA significantly reduces memory requirements, enabling training on larger models without fitting the entire model and its associated data into GPU memory.</li>
</ul>
</section>
<section id="quantization" class="level3">
<h3 class="anchored" data-anchor-id="quantization">Quantization</h3>
<ul>
<li><strong>Solution:</strong> Quantization reduces memory usage by representing model parameters with fewer bits.</li>
<li><strong>Explanation:</strong>
<ul>
<li>Instead of using 32 bits, quantization uses 8 bits or even 4 bits per parameter.</li>
</ul></li>
<li><strong>Benefit:</strong> Quantization shrinks the memory footprint of the model, allowing for larger model training or fitting on memory-constrained devices.</li>
<li><strong>Combination with LoRA (QLoRA):</strong> Quantize the frozen base model weights while keeping the LoRA parameters in higher precision for accurate training.</li>
</ul>
</section>
<section id="cpu-offloading-illustration" class="level3">
<h3 class="anchored" data-anchor-id="cpu-offloading-illustration">CPU Offloading Illustration</h3>
<ul>
<li><strong>Solution:</strong> Utilize CPU RAM to store model components and offload computations when GPU memory is insufficient.</li>
<li><strong>Explanation:</strong> Load and process one layer of the model on the GPU at a time, storing the remaining layers and intermediate data in CPU memory.</li>
<li><strong>Trade-off:</strong> While enabling larger model training, CPU offloading introduces latency due to data transfer between CPU and GPU.</li>
</ul>
</section>
<section id="llm-context-length-considerations" class="level3">
<h3 class="anchored" data-anchor-id="llm-context-length-considerations">LLM Context Length Considerations</h3>
<ul>
<li><strong>Problem:</strong> Large context lengths lead to increased memory consumption for storing activations.</li>
<li><strong>Explanation:</strong>
<ul>
<li>Each layer’s output activations need to be stored for gradient calculations, and these activations accumulate with longer input sequences.</li>
</ul></li>
<li><strong>Impact:</strong> Training LLMs with long context lengths requires careful memory management and might necessitate techniques like gradient checkpointing to reduce activation memory footprint.</li>
</ul>
</section>
</section>
<section id="napkin-math-code-demo" class="level2">
<h2 class="anchored" data-anchor-id="napkin-math-code-demo">Napkin Math Code Demo</h2>
<ul>
<li>Demonstrates using PyTorch’s memory tracking tools to analyze memory usage during training.</li>
</ul>
<section id="measuring-memory" class="level3">
<h3 class="anchored" data-anchor-id="measuring-memory">Measuring Memory</h3>
<ul>
<li><strong>Challenge:</strong> Limited GPU RAM restricts the size of models and batch sizes during training.</li>
<li><strong>Solution:</strong> Use PyTorch’s memory tracking capabilities to understand and optimize memory consumption.
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html"><code>torch.cuda.max_memory_allocated</code></a>: Provides a more accurate measure of actively used memory.</li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_reserved.html"><code>torch.cuda.max_memory_reserved</code></a>: Reflects total memory held by PyTorch, including pre-allocated space.</li>
</ul></li>
<li><strong>Key Takeaway:</strong> Actively monitor memory usage to make informed decisions about model size, batch size, and other hyperparameters.</li>
</ul>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch, gc, inspect, transformers</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, BitsAndBytesConfig</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> accelerate.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> set_seed</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> peft <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> prepare_model_for_kbit_training</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> peft <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LoraConfig, get_peft_model</span>
<span id="cb1-6"></span>
<span id="cb1-7">transformers.logging.set_verbosity_warning()</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span></span></code></pre></div>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> cleanup():</span>
<span id="cb3-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Free up memory and reset stats"""</span></span>
<span id="cb3-3">    gc.collect()</span>
<span id="cb3-4">    torch.cuda.empty_cache()</span>
<span id="cb3-5">    torch.cuda.reset_peak_memory_stats(device)</span>
<span id="cb3-6">cleanup()</span></code></pre></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> print_memory_stats():</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Print two different measures of GPU memory usage"""</span></span>
<span id="cb4-3">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Max memory allocated: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>cuda<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>max_memory_allocated(device)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">GB"</span>)</span>
<span id="cb4-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory</span></span>
<span id="cb4-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Max memory reserved: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>cuda<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>max_memory_reserved(device)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">GB"</span>) </span>
<span id="cb4-6">print_memory_stats()</span></code></pre></div>
<pre class="text"><code>Max memory allocated: 0.00GB
Max memory reserved: 0.00GB</code></pre>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># QLoRA Forward + Backward pass</span></span>
<span id="cb6-2"></span>
<span id="cb6-3">cleanup() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clean slate each time</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Quantization config for QLoRA versions</span></span>
<span id="cb6-6">bnb_config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BitsAndBytesConfig(</span>
<span id="cb6-7">    load_in_4bit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb6-8">    bnb_4bit_compute_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16</span>
<span id="cb6-9">)</span>
<span id="cb6-10"></span>
<span id="cb6-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the model</span></span>
<span id="cb6-12">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>,</span>
<span id="cb6-14">    low_cpu_mem_usage<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb6-15">    quantization_config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bnb_config,</span>
<span id="cb6-16">    use_cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-17">)</span>
<span id="cb6-18"></span>
<span id="cb6-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This function has caused me lots of pain!</span></span>
<span id="cb6-20">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prepare_model_for_kbit_training(model)</span>
<span id="cb6-21"></span>
<span id="cb6-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add LoRA</span></span>
<span id="cb6-23">config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LoraConfig(</span>
<span id="cb6-24">    r<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb6-25">    lora_alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>,</span>
<span id="cb6-26">    lora_dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,</span>
<span id="cb6-27">    bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>,</span>
<span id="cb6-28">    task_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CAUSAL_LM"</span></span>
<span id="cb6-29">)</span>
<span id="cb6-30">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_peft_model(model, config)</span>
<span id="cb6-31"></span>
<span id="cb6-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Move to GPU</span></span>
<span id="cb6-33">model.to(device)</span>
<span id="cb6-34"></span>
<span id="cb6-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prepare the data</span></span>
<span id="cb6-36">bs, ctx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1200</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># batch size and context length</span></span>
<span id="cb6-37">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>, (bs, ctx), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb6-38"></span>
<span id="cb6-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Forward pass</span></span>
<span id="cb6-40">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(data, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data)</span>
<span id="cb6-41"></span>
<span id="cb6-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Backward pass</span></span>
<span id="cb6-43">output.loss.backward()</span>
<span id="cb6-44"></span>
<span id="cb6-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Print max memory stats</span></span>
<span id="cb6-46">print_memory_stats()</span>
<span id="cb6-47"></span>
<span id="cb6-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Cleanup</span></span>
<span id="cb6-49"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">del</span> model, output, data</span>
<span id="cb6-50">cleanup()</span></code></pre></div>
<pre class="text"><code>Max memory allocated: 6.42GB
Max memory reserved: 7.09GB</code></pre>
</section>
<section id="memory-history" class="level3">
<h3 class="anchored" data-anchor-id="memory-history">Memory History</h3>
<ul>
<li><strong>Challenge:</strong> Identifying specific memory bottlenecks within the training process.</li>
<li><strong>Solution:</strong> PyTorch’s <code>record_memory_history</code> function helps visualize memory usage over time.</li>
<li><strong>Tool:</strong> Upload the generated pickle file to Pytorch’s Memory Visualizer for a detailed graphical representation.
<ul>
<li><strong>Link:</strong> <a href="https://pytorch.org/memory_viz">https://pytorch.org/memory_viz</a></li>
</ul></li>
<li><strong>Documentation:</strong> <a href="https://pytorch.org/docs/stable/torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li><strong>Tutorial:</strong> <a href="https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html">How to save memory by fusing the optimizer step into the backward pass</a></li>
</ul>
<section id="benefits-of-memory-history-analysis" class="level4">
<h4 class="anchored" data-anchor-id="benefits-of-memory-history-analysis">Benefits of Memory History Analysis:</h4>
<ul>
<li><strong>Identify memory spikes:</strong> pinpoint operations causing significant memory increases.</li>
<li><strong>Understand memory allocation patterns:</strong> Visualize how memory usage evolves throughout the training process (forward pass, backward pass, gradient updates).</li>
<li><strong>Evaluate optimization strategies:</strong> Observe the impact of techniques like gradient checkpointing and quantization on memory consumption.</li>
</ul>
</section>
<section id="real-world-examples" class="level4">
<h4 class="anchored" data-anchor-id="real-world-examples">Real-World Examples</h4>
<ul>
<li><strong>Bug Detection:</strong> Unusually high memory spikes revealed incorrect implementations or unintended use of trainable parameters.</li>
<li><strong>Memory Leak Identification:</strong> Gradual memory escalation over multiple training steps pointed to issues with gradient or loss clearing.</li>
<li><strong>Kernel Optimization:</strong> Analysis highlighted inefficiencies in HuggingFace’s Transformers library, prompting a switch back to a more memory-efficient attention kernel.</li>
</ul>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">cleanup()</span>
<span id="cb8-2"></span>
<span id="cb8-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the model</span></span>
<span id="cb8-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb8-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>,</span>
<span id="cb8-6">    low_cpu_mem_usage<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-7">    quantization_config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bnb_config,</span>
<span id="cb8-8">    use_cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb8-9">)</span>
<span id="cb8-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prepare_model_for_kbit_training(model)</span>
<span id="cb8-11">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_peft_model(model, config)</span>
<span id="cb8-12">model.to(device)</span>
<span id="cb8-13"></span>
<span id="cb8-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prep the data</span></span>
<span id="cb8-15">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb8-16"></span>
<span id="cb8-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Start recording</span></span>
<span id="cb8-18">torch.cuda.memory._record_memory_history()</span>
<span id="cb8-19"></span>
<span id="cb8-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># FOrward and backwards pass</span></span>
<span id="cb8-21">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(data, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data)</span>
<span id="cb8-22">output.loss.backward()</span>
<span id="cb8-23"></span>
<span id="cb8-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save the snapshot and stop recording</span></span>
<span id="cb8-25">torch.cuda.memory._dump_snapshot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"memory_snapshot.pickle"</span>)</span>
<span id="cb8-26">torch.cuda.memory._record_memory_history(enabled<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) </span>
<span id="cb8-27"></span>
<span id="cb8-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clean up</span></span>
<span id="cb8-29"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">del</span> model, output, data</span>
<span id="cb8-30">cleanup()</span></code></pre></div>
<p>Results viewed in <a href="https://pytorch.org/memory_viz">https://pytorch.org/memory_viz</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-005/images/image.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
</section>
<section id="optimizing-llm-training-for-different-hardware" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-llm-training-for-different-hardware">Optimizing LLM Training for Different Hardware</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Benchmarking QLoRA+FSDP">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Benchmarking QLoRA+FSDP
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Case Study:</strong> <a href="https://github.com/AnswerDotAI/fsdp_qlora/blob/main/benchmarks_03_2024.md">A Dual 3090 ‘Basement Rig’</a></li>
</ul>
</div>
</div>
<section id="gpu-limitations-data-transfer-bottleneck" class="level3">
<h3 class="anchored" data-anchor-id="gpu-limitations-data-transfer-bottleneck">GPU Limitations &amp; Data Transfer Bottleneck</h3>
<ul>
<li>Training LLMs often involves frequent data transfers between CPU memory and GPU memory.</li>
<li>In systems with limited GPU memory or slow interconnects, these transfers become a significant bottleneck.</li>
<li>Each layer of the model might require loading weights onto the GPU, performing computations, and then repeating the process for the next layer.</li>
</ul>
</section>
<section id="optimizing-for-memory-bound-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-for-memory-bound-scenarios">Optimizing for Memory-Bound Scenarios</h3>
<ul>
<li><strong>Goal:</strong> Minimize data transfer cycles by maximizing computation done per load.
<ul>
<li><strong>Example:</strong> Using a larger batch size allows processing more samples per load, reducing the total number of loads required.</li>
</ul></li>
<li><strong>Techniques:</strong>
<ul>
<li><strong>Quantization:</strong> Reduces the precision of model weights, making them faster to transfer and process.</li>
<li><strong>QLoRa:</strong> A memory-efficient training method that further reduces memory footprint compared to standard LoRa.</li>
</ul></li>
<li><strong>Example:</strong> The “3090 basement rig” showed significant speed improvements when using these techniques due to a larger achievable batch size.</li>
</ul>
</section>
<section id="impact-of-hardware-on-optimization-strategies" class="level3">
<h3 class="anchored" data-anchor-id="impact-of-hardware-on-optimization-strategies">Impact of Hardware on Optimization Strategies</h3>
<ul>
<li><p><strong>Memory Bandwidth Constraints:</strong> On slower machines or those with limited interconnect bandwidth, optimizing for memory bandwidth is crucial. Techniques like quantization and larger batch sizes yield significant gains.</p></li>
<li><p><strong>High-End Systems:</strong></p>
<ul>
<li>Systems like the H100 with fast NVLink interconnects and ample GPU memory minimize the data transfer bottleneck.</li>
<li>Model weights can reside in fast GPU memory, even across multiple GPUs, allowing for rapid loading.</li>
<li>Computation becomes the limiting factor, not memory bandwidth.</li>
</ul></li>
<li><p><strong>Example:</strong> In tests on an H100, increasing batch size or using quantization didn’t significantly impact runtime. The bottleneck was compute speed, not data transfer.</p></li>
</ul>
</section>
<section id="adapting-to-hardware-limitations" class="level3">
<h3 class="anchored" data-anchor-id="adapting-to-hardware-limitations">Adapting to Hardware Limitations</h3>
<ul>
<li><strong>Understand Your Hardware:</strong> Identify whether your system is memory-bound or compute-bound.</li>
<li><strong>Optimize Accordingly:</strong>
<ul>
<li><strong>Memory-Bound:</strong> Prioritize techniques like quantization, QLoRa, and increasing batch size to minimize data transfer.</li>
<li><strong>Compute-Bound:</strong> Focus on maximizing computational efficiency, as data transfer is less of a concern.</li>
</ul></li>
</ul>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="cpu-offloading" class="level3">
<h3 class="anchored" data-anchor-id="cpu-offloading">CPU Offloading</h3>
<ul>
<li><p><strong>Question:</strong> Is CPU offloading useful for training or just inference?</p></li>
<li><p><strong>Answer:</strong> CPU offloading can be beneficial for training large models that exceed single GPU memory, even with multiple GPUs. It allows for larger batch sizes by storing model weights on the CPU and copying them to the GPU as needed. However, it’s less effective with smaller models or abundant GPU memory.</p></li>
<li><p><strong>Recommendation:</strong> Start with default settings and gradually introduce optimizations like quantization, batch size increases, data parallelism, and sharding, evaluating performance gains at each step.</p></li>
</ul>
</section>
<section id="quantization-sweet-spot" class="level3">
<h3 class="anchored" data-anchor-id="quantization-sweet-spot">Quantization Sweet Spot</h3>
<ul>
<li><p><strong>Question:</strong> How to find the optimal balance between compression and accuracy with quantization?</p></li>
<li><p><strong>Answer:</strong> Quantization below 4 bits can negatively impact accuracy. Combining 4-bit quantization with LoRA (adapters) often maintains performance comparable to no quantization.</p></li>
<li><p><strong>Recommendation:</strong> Use 4-bit quantization with LoRA as a starting point. Keep the LoRA in higher precision.</p></li>
</ul>
</section>
<section id="gradient-accumulation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-accumulation">Gradient Accumulation</h3>
<ul>
<li><p><strong>Question:</strong> Clarification on gradient accumulation and micro batch size.</p></li>
<li><p><strong>Answer:</strong> Gradient accumulation simulates larger batch sizes by accumulating gradients over multiple smaller “micro batches” before updating model weights. This is useful when target batch sizes exceed GPU memory.</p></li>
<li><p><strong>Example:</strong> To achieve an effective batch size of 32 with a GPU limited to 8 samples, run 4 micro batches of 8 samples each before updating.</p></li>
<li><p><strong>Recommendation:</strong> Use gradient accumulation to reach a reasonable effective batch size (16, 32, 64), particularly when training with small batch sizes (1 or 2).</p></li>
</ul>
</section>
<section id="multiple-loras" class="level3">
<h3 class="anchored" data-anchor-id="multiple-loras">Multiple LoRAs</h3>
<ul>
<li><p><strong>Question:</strong> Do multiple LoRAs update the same or different model parameters?</p></li>
<li><p><strong>Answer:</strong> Typically, multiple LoRAs update the same weights. Each LoRA targets specific layers defined in its configuration.</p></li>
</ul>
</section>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<ul>
<li><strong>Balancing Act:</strong> Fine-tuning involves minimizing data movement and maximizing computation efficiency.</li>
<li><strong>Memory Management:</strong>
<ul>
<li><strong>LoRA:</strong> Reduces memory by training a smaller set of parameters.</li>
<li><strong>Quantization:</strong> Stores weights using fewer bits.</li>
<li><strong>Experimentation:</strong> Key to finding the right trade-offs between memory, speed, and accuracy.</li>
</ul></li>
<li><strong>Memory Allocation:</strong>
<ul>
<li><strong>Model Size:</strong> Larger models require more memory for weights and gradients.</li>
<li><strong>Batch Size and Context Length:</strong> Increasing either requires more memory for activations and intermediate values.</li>
</ul></li>
<li><strong>Memory Optimization Strategies:</strong>
<ul>
<li>Reduce context length if possible.</li>
<li>Quantize weights.</li>
<li>Offload to CPU.</li>
</ul></li>
<li><strong>Practical Estimation:</strong> Instead of complex formulas, estimate memory requirements based on measurements from a single layer and extrapolate for the entire model.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-005/</guid>
  <pubDate>Sun, 07 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 5: LangChain/LangSmith</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-005/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<section id="langsmiths-position-in-the-observability-market" class="level2">
<h2 class="anchored" data-anchor-id="langsmiths-position-in-the-observability-market">LangSmith’s Position in the Observability Market</h2>
<ul>
<li><strong>Question:</strong> How does LangSmith differentiate itself from other observability tools in the market?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>LLM Application Focus:</strong> LangSmith is specifically designed for LLM applications, offering specialized features like message and document visualization for debugging.
<ul>
<li><strong>Guide:</strong> <a href="https://docs.smith.langchain.com/how_to_guides/tracing/log_retriever_trace">Log retriever traces</a></li>
</ul></li>
<li><strong>Chains of LLM Calls:</strong> It emphasizes visualizing and analyzing entire chains of LLM calls and retrieval steps, which is crucial for complex applications.
<ul>
<li><strong>Tutorial:</strong> <a href="https://docs.smith.langchain.com/tutorials/Developers/observability">Add observability to your LLM application</a></li>
</ul></li>
<li><strong>Human-in-the-Loop Features:</strong> LangSmith prioritizes human interaction with features like:
<ul>
<li>Data visualization</li>
<li>Annotation queues for collaboration with subject matter experts</li>
<li>Side-by-side comparisons for evaluating improvements</li>
<li>Alignment of evaluators with human preferences</li>
<li><strong>How-to guides:</strong> <a href="https://docs.smith.langchain.com/how_to_guides/human_feedback">Human feedback</a></li>
</ul></li>
<li><strong>Pairwise Evaluation:</strong> LangSmith enables pairwise evaluation of models, leading to more stable results.
<ul>
<li><strong>Guide:</strong> <a href="https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise">Run pairwise evaluations</a></li>
</ul></li>
<li><strong>Strong Support and Openness:</strong> LangSmith is praised for its excellent support, responsive team, and open APIs that allow integration with other tools.</li>
</ul></li>
</ul>
</section>
<section id="langsmiths-support-for-human-annotation-and-action-items" class="level2">
<h2 class="anchored" data-anchor-id="langsmiths-support-for-human-annotation-and-action-items">LangSmith’s Support for Human Annotation and Action Items</h2>
<ul>
<li><strong>Question:</strong> What support does LangSmith offer for human annotation, annotation queues, and taking action on user feedback?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Annotation Queues:</strong>
<ul>
<li>Data can be sent to annotation queues programmatically (e.g., based on user thumbs down) or manually.</li>
<li>Annotators can provide feedback, edit outputs, and add corrected examples to datasets.</li>
<li><strong>Guide:</strong> <a href="https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues">Use annotation queues</a></li>
</ul></li>
<li><strong>Datasets for Improvement:</strong> Corrected examples in datasets can be used for testing and future model improvement.
<ul>
<li><strong>Concept:</strong> <a href="https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples">Evaluation</a></li>
</ul></li>
<li><strong>Few-Shot Learning:</strong> LangSmith aims to be a platform for gathering few-shot example datasets, which can be used for personalization by pulling down the most similar examples during runtime.</li>
</ul></li>
</ul>
</section>
<section id="understanding-the-langchain-lang-namespace" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-langchain-lang-namespace">Understanding the LangChain “Lang” Namespace</h2>
<ul>
<li><strong>Question:</strong> What’s the difference between Langchain, Langsmith, Langgraph, Langflow, and Langserve?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong><a href="https://www.langchain.com/">LangChain</a>:</strong> The foundational open-source package for building LLM apps, offering a runtime, abstractions, integrations, and off-the-shelf chains.</li>
<li><strong><a href="https://www.langflow.org/">LangFlow</a> (Not Langchain Company):</strong> A low-code/no-code UI built on top of LangChain.</li>
<li><strong><a href="https://python.langchain.com/v0.2/docs/langserve/">LangServe</a>:</strong> Simplifies deploying LangChain applications by exposing them as <a href="https://fastapi.tiangolo.com/">FastAPI</a> endpoints.</li>
<li><strong><a href="https://langchain-ai.github.io/langgraph/">LangGraph</a>:</strong> An extension of LangChain specifically designed for building and managing highly controllable agent-based workflows.</li>
<li><strong><a href="https://www.langchain.com/langsmith">LangSmith</a>:</strong> A standalone observability and testing tool for LLM apps, usable with or without LangChain.</li>
</ul></li>
</ul>
</section>
<section id="when-to-use-langchain-vs.-langgraph" class="level2">
<h2 class="anchored" data-anchor-id="when-to-use-langchain-vs.-langgraph">When to Use LangChain vs.&nbsp;LangGraph</h2>
<ul>
<li><strong>Question:</strong> When would you choose LangChain, and when is LangGraph the better option?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>LangChain:</strong> Ideal for beginners and for rapidly prototyping simple LLM applications with single LLM calls.</li>
<li><strong>LangGraph:</strong> Suited for advanced teams building complex, agentic workflows that require:
<ul>
<li>Cyclical agent execution</li>
<li>Fine-grained control</li>
<li>Built-in persistence</li>
<li>Streaming and background modes</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="popularity-of-typescript-vs.-python-in-llm-tools" class="level2">
<h2 class="anchored" data-anchor-id="popularity-of-typescript-vs.-python-in-llm-tools">Popularity of TypeScript vs.&nbsp;Python in LLM Tools</h2>
<ul>
<li><strong>Question:</strong> How does the usage of TypeScript APIs compare to Python APIs in LangChain and related tools?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Python Dominates:</strong> Python remains more popular overall, possibly due to:
<ul>
<li>A larger community focused on LLM application prototyping.</li>
<li>Stronger ecosystem for data engineering tasks related to retrieval.</li>
</ul></li>
<li><strong>TypeScript for Generative UI:</strong> TypeScript is gaining traction, especially for applications involving generative UI, which is more challenging to implement in Python.</li>
</ul></li>
</ul>
</section>
<section id="generative-ui-explained" class="level2">
<h2 class="anchored" data-anchor-id="generative-ui-explained">Generative UI Explained</h2>
<ul>
<li><strong>Question:</strong> What is generative UI, and how does it work?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Beyond Simple Chat:</strong> Generative UI enables LLMs to return more than text; they send UI components to create richer interfaces.</li>
<li><strong>Example:</strong> Instead of a list of weather data, an LLM might return a dynamic graph component with zoom and interaction capabilities.</li>
<li><strong>Vercel AI SDK Integration:</strong> LangChain now integrates with Vercel’s AI SDK for easier development of generative UI experiences.
<ul>
<li><strong><a href="https://sdk.vercel.ai/docs/introduction">Vercel AI SDK</a>:</strong> TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="defining-agentic-in-the-context-of-llms" class="level2">
<h2 class="anchored" data-anchor-id="defining-agentic-in-the-context-of-llms">Defining “Agentic” in the Context of LLMs</h2>
<ul>
<li><strong>Question:</strong> What does “agentic” mean in the context of LLMs, and is it a significant distinction?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>LLM in Control:</strong> An agentic system is one where the LLM controls the application’s control flow and decision-making process.</li>
<li><strong>More Than Function Calling:</strong> While related to function calling, agentic systems go further by enabling LLMs to loop, adapt, and make dynamic decisions about the next steps.</li>
<li><strong>Implications for Development:</strong> This distinction introduces new challenges and considerations in UX design, observability, and testing.</li>
</ul></li>
</ul>
</section>
<section id="langchainlangsmith-features-for-agentic-workflows" class="level2">
<h2 class="anchored" data-anchor-id="langchainlangsmith-features-for-agentic-workflows">LangChain/LangSmith Features for Agentic Workflows</h2>
<ul>
<li><strong>Question:</strong> What features in LangChain/LangSmith specifically aid in developing and managing agentic workflows?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>LangGraph’s Strengths:</strong>
<ul>
<li><strong>Controllability:</strong> LangGraph’s low-level design provides a high degree of control, which is essential for managing complex agents.</li>
<li><strong>Persistence and Human-in-the-Loop:</strong> Built-in persistence and easy access to execution history enable checkpointing, resuming from specific states, and human intervention when needed.</li>
</ul></li>
<li><strong>LangSmith’s Role:</strong> While not agent-specific, LangSmith’s observability features are particularly valuable for debugging and understanding complex, agentic applications.</li>
</ul></li>
</ul>
</section>
<section id="multiple-llm-collaboration-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="multiple-llm-collaboration-in-practice">Multiple LLM Collaboration in Practice</h2>
<ul>
<li><strong>Question:</strong> Is the idea of using multiple LLMs with different strengths in a single application realistic?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Planning and Execution:</strong> A common pattern involves a powerful LLM (e.g., GPT-4) for high-level planning and decision-making, while specialized or more cost-effective models (e.g., specialized code generation models) handle specific tasks.</li>
</ul></li>
</ul>
</section>
<section id="building-evaluation-sets-with-langsmith" class="level2">
<h2 class="anchored" data-anchor-id="building-evaluation-sets-with-langsmith">Building Evaluation Sets with LangSmith</h2>
<ul>
<li><strong>Question:</strong> What’s the most effective way to use LangSmith for creating evaluation sets?</li>
<li><strong>Answer:</strong>
<ol type="1">
<li><strong>Manual Seeding:</strong> Begin with a small set (5-10) of manually crafted examples.</li>
<li><strong>Production Feedback Loop:</strong> Integrate with production logs to capture real-user interactions and identify edge cases.</li>
<li><strong>Iterative Refinement:</strong>
<ul>
<li>Manually add challenging or interesting cases to the dataset.</li>
<li>Encourage user feedback and incorporate relevant examples.</li>
<li>Consider synthetic data generation to expand the dataset, but prioritize human review and labeling.</li>
</ul></li>
</ol></li>
<li><strong>Concepts:</strong> <a href="https://docs.smith.langchain.com/concepts/evaluation">Evaluation</a></li>
<li><strong>How-to guides:</strong> <a href="https://docs.smith.langchain.com/how_to_guides/evaluation">Evaluation</a></li>
<li><strong>Tutorial:</strong> <a href="https://docs.smith.langchain.com/tutorials/Developers/evaluation">Evaluate your LLM application</a></li>
</ul>
</section>
<section id="recommended-stack-for-full-stack-ml-engineers" class="level2">
<h2 class="anchored" data-anchor-id="recommended-stack-for-full-stack-ml-engineers">Recommended Stack for Full-Stack ML Engineers</h2>
<ul>
<li><strong>Question:</strong> What’s a good technology stack for Python-centric ML engineers who want to build and ship full-stack applications?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Python-First Options:</strong>
<ul>
<li><strong><a href="https://streamlit.io/">Streamlit</a>/<a href="https://www.gradio.app/">Gradio</a>:</strong> Excellent for rapid prototyping and simpler applications.</li>
<li><strong><a href="https://python.langchain.com/v0.2/docs/templates/">LangChain Templates</a>:</strong> Explore and adapt existing LangChain repositories with Python backends.</li>
<li><strong><a href="https://github.com/langchain-ai/langserve">LangServe</a>:</strong> Easily deploy LangChain apps.</li>
</ul></li>
<li><strong>Long-Term Goal:</strong> Aim to become proficient in 2-3 languages (Python, JavaScript/TypeScript, SQL) for greater flexibility and control over the entire application stack.</li>
<li><strong>Tips:</strong>
<ul>
<li>Leverage LLMs (like ChatGPT) to assist with JavaScript/TypeScript code generation and understanding.</li>
<li>Don’t shy away from forking and modifying existing repositories to learn and adapt.</li>
</ul></li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-005/</guid>
  <pubDate>Sat, 06 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 4: Modal with Charles Frye</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-004/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<section id="understanding-modal" class="level2">
<h2 class="anchored" data-anchor-id="understanding-modal">Understanding Modal</h2>
<ul>
<li><strong>ELI5:</strong> Modal allows users to run their Python code in the cloud effortlessly.</li>
<li><strong>In-depth:</strong> Modal is a remote procedure call (RPC) framework designed for Python, particularly excelling in data-intensive workloads like those involving large models or datasets.</li>
<li><strong>Advantages over Traditional Cloud Functions (AWS Lambda, Google Cloud Functions):</strong>
<ul>
<li><strong>Focus:</strong> Handles compute-heavy and data-intensive tasks more effectively.</li>
<li><strong>Simplicity:</strong> Removes the need for server management.</li>
<li><strong>Scalability:</strong> Scales resources dynamically based on demand.</li>
</ul></li>
<li><strong>Key Features:</strong>
<ul>
<li>Seamless local development with automatic code deployment to the cloud.</li>
<li>Simplified parallel processing and scaling.</li>
<li>Integration with popular frameworks like <a href="https://fastapi.tiangolo.com/">FastAPI</a>.</li>
</ul></li>
<li><strong>Founders’ Vision:</strong> Address common infrastructure challenges faced by data scientists and ML practitioners, emphasizing rapid development cycles and feedback loops.</li>
</ul>
</section>
<section id="startup-times-and-optimization" class="level2">
<h2 class="anchored" data-anchor-id="startup-times-and-optimization">Startup Times and Optimization</h2>
<ul>
<li><strong>Concern:</strong> Modal’s startup time compared to traditional server-based solutions.</li>
<li><strong>Modal’s Performance:</strong>
<ul>
<li>Container startup (Docker run): 1-2 seconds (50th percentile startup time).</li>
<li>Slowdown occurs when environment setup requires loading large elements into memory (e.g., language model weights).</li>
</ul></li>
<li><strong>Solutions for Faster Startup:</strong>
<ul>
<li><strong>Keep Warm:</strong> Leave applications running for minimal latency, especially crucial for GPU-bound tasks. (Trade-off: potentially higher cost for idle resources).
<ul>
<li><a href="https://modal.com/docs/guide/cold-start#keep-containers-warm-for-longer-with-container_idle_timeout">Keep containers warm for longer with <code>container_idle_timeout</code></a></li>
</ul></li>
<li><strong>CUDA Checkpointing:</strong> New feature under integration, expected to accelerate subsequent invocations.
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/NVIDIA/cuda-checkpoint">cuda-checkpoint</a></li>
</ul></li>
<li><strong>CPU Tasks:</strong> Easily sliced and diced, making them cost-effective in keep-warm mode due to minimal resource consumption during idle periods.</li>
<li><strong>Optimization Potential:</strong> The LLM fine-tuning repo hasn’t been fully optimized for boot times; improvements are possible.
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/modal-labs/llm-finetuning">llm-finetuning</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="local-development-and-modal-integration" class="level2">
<h2 class="anchored" data-anchor-id="local-development-and-modal-integration">Local Development and Modal Integration</h2>
<ul>
<li><strong>Challenge:</strong> Integrating a complex FastAPI application with local databases and debugging tools, then deploying it seamlessly to Modal.</li>
<li><strong>Thin Client Approach:</strong>
<ul>
<li>Modal examples typically employ a thin client architecture for simplicity in dependency management.</li>
<li>Local development within the thin client can be limited due to the absence of specific dependencies.</li>
</ul></li>
<li><strong>Solutions:</strong>
<ul>
<li><strong>Modal’s Remote Development Tools:</strong> Shell access, VS Code integration, and JupyterLab instances within Modal’s environment.
<ul>
<li><a href="https://modal.com/docs/reference/cli/launch#modal-launch"><code>modal launch</code></a>: Open a serverless app instance on Modal.
<ul>
<li><a href="https://modal.com/docs/reference/cli/launch#modal-launch-jupyter"><code>modal launch jupyter</code></a>: Start Jupyter Lab on Modal.</li>
<li><a href="https://modal.com/docs/reference/cli/launch#modal-launch-jupyter"><code>modal launch vscode</code></a>: Start Visual Studio Code on Modal.</li>
</ul></li>
<li><a href="https://modal.com/docs/reference/cli/shell#modal-shell"><code>modal shell</code></a>: Run an interactive shell inside a Modal image.</li>
</ul></li>
<li><strong>Thick Client Architecture:</strong>
<ul>
<li>Build a local environment mirroring the Modal environment.</li>
<li>Utilize tools like Dockerfiles, requirements.txt, poetry, or conda for consistent dependency management.</li>
</ul></li>
<li><strong>Resource:</strong> Explore the <code>awesome-modal</code> repository on GitHub for production-ready examples, some utilizing a thicker client approach.
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/modal-labs/awesome-modal">awesome-modal</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="iterative-development-workflow" class="level2">
<h2 class="anchored" data-anchor-id="iterative-development-workflow">Iterative Development Workflow</h2>
<ul>
<li><strong>Challenge:</strong> Fine-tuning models locally on a small scale with debugging and then scaling up on Modal with a full dataset and larger models.</li>
<li><strong>Recommendations:</strong>
<ul>
<li><strong><a href="https://modal.com/docs/reference/modal.Image"><code>modal.Image</code></a> Class:</strong>
<ul>
<li>Base class for container images to run functions in.</li>
<li>Utilize for environment definition, ensuring consistency between local and remote setups.</li>
<li><strong>Guide:</strong> <a href="https://modal.com/docs/guide/custom-container">Custom containers</a></li>
</ul></li>
<li><strong>Dependency Management:</strong> Leverage tools like <code>pip freeze</code> and poetry for tighter control over environments.</li>
<li><strong>Hardware Considerations:</strong> Be mindful of potential discrepancies between local and Modal GPUs.</li>
</ul></li>
</ul>
</section>
<section id="modal-for-cpu-intensive-workloads" class="level2">
<h2 class="anchored" data-anchor-id="modal-for-cpu-intensive-workloads">Modal for CPU-Intensive Workloads</h2>
<ul>
<li><strong>Question:</strong> Is Modal suitable for parallel CPU-bound tasks rather than just GPU acceleration?</li>
<li><strong>Answer:</strong> Yes, Modal is highly recommended for CPU-intensive and parallelizable tasks.</li>
<li><strong>Reasons:</strong>
<ul>
<li><strong>Cost-Effectiveness:</strong> CPUs are cheaper on Modal due to efficient time-slicing and readily available resources.</li>
<li><strong>Simplified Parallelization:</strong> Modal’s architecture and tools streamline the execution of parallel CPU workloads.</li>
</ul></li>
</ul>
</section>
<section id="cost-comparison-and-value-proposition" class="level2">
<h2 class="anchored" data-anchor-id="cost-comparison-and-value-proposition">Cost Comparison and Value Proposition</h2>
<ul>
<li><strong>Concern:</strong> Fine-tuning on Modal appears more expensive than platforms like Jarvis Labs.</li>
<li><strong>Modal’s Pricing:</strong>
<ul>
<li>Transparent, based on underlying cloud provider costs.</li>
<li>No hidden fees or inflated pricing strategies.</li>
</ul></li>
<li><strong>When Modal Wins:</strong>
<ul>
<li><strong>High Operational Overhead:</strong> Modal excels when the effort of managing servers (spinning up, down, utilization tracking) outweighs the raw compute cost.</li>
<li><strong>Unpredictable Workloads:</strong> Serverless nature shines when demand fluctuates, and predicting utilization is challenging.</li>
<li><strong>Scalability Needs:</strong> Modal simplifies scaling to thousands of GPUs, surpassing the limitations of individual users or smaller organizations.</li>
<li><strong>GPU Accessibility:</strong> Modal offers readily available GPUs, circumventing the challenges of procurement and allocation.</li>
<li><strong>Developer Experience:</strong> Streamlined workflow and reduced operational burden can justify a potential price premium for some users.</li>
</ul></li>
</ul>
</section>
<section id="understanding-modals-cost-structure" class="level2">
<h2 class="anchored" data-anchor-id="understanding-modals-cost-structure">Understanding Modal’s Cost Structure</h2>
<ul>
<li><strong>Question:</strong> How can a keep-warm FastAPI app on Modal cost only 30 cents per month when CPU core pricing suggests a much higher cost?</li>
<li><strong>Explanation:</strong>
<ul>
<li><strong>Time-Slicing:</strong> CPUs are shared efficiently, and Modal only charges for actual usage, not idle time.</li>
<li><strong>Low Utilization:</strong> Web apps typically have low average CPU utilization, further reducing costs.</li>
<li><strong>RAM-Based Pricing:</strong> During idle periods, charges are primarily determined by RAM usage, which is often minimal for lightweight apps.</li>
</ul></li>
</ul>
</section>
<section id="streaming-output-from-llms" class="level2">
<h2 class="anchored" data-anchor-id="streaming-output-from-llms">Streaming Output from LLMs</h2>
<ul>
<li><strong>Question:</strong> Availability of examples showcasing streamed output from LLMs in FastAPI apps.</li>
<li><strong>Answer:</strong>
<ul>
<li>Examples for streaming and FastAPI integration are available in the documentation:
<ul>
<li><a href="https://modal.com/docs/examples/vllm_mixtral">Fast inference with vLLM (Mixtral 8x7B)</a></li>
<li><a href="https://modal.com/docs/examples/llm-voice-chat">QuiLLMan: Voice Chat with LLMs</a></li>
</ul></li>
</ul></li>
<li><strong>Modal’s Async Support:</strong> Modal simplifies asynchronous programming, making streaming implementations easier.
<ul>
<li><strong>Guide:</strong> <a href="https://modal.com/docs/guide/async">Asynchronous API usage</a></li>
</ul></li>
</ul>
</section>
<section id="code-portability-and-modal-dependency" class="level2">
<h2 class="anchored" data-anchor-id="code-portability-and-modal-dependency">Code Portability and Modal Dependency</h2>
<ul>
<li><strong>Concern:</strong> Modal’s decorators might hinder code portability to other environments.</li>
<li><strong>Response:</strong>
<ul>
<li>While Modal promotes a specific architecture for performance and cost optimization, code can be written to minimize tight coupling.</li>
<li>Decorators can be removed or bypassed if needed to port code to different environments.</li>
<li>Achieving portability often involves trade-offs in performance and cost-effectiveness.</li>
</ul></li>
</ul>
</section>
<section id="data-privacy" class="level2">
<h2 class="anchored" data-anchor-id="data-privacy">Data Privacy</h2>
<ul>
<li><strong>Question:</strong> Modal’s policy on data privacy and potential use of user data for model training.</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Commitment to Security:</strong> Modal is <a href="https://modal.com/blog/soc2">SOC 2 compliant</a> and working towards SOC 2 Type 2 certification, demonstrating a high standard of data security.</li>
<li><strong>User Data Protection:</strong> Modal treats user application data as confidential. Permission is sought before reviewing data, even for support purposes.</li>
<li><strong>No User Data Training:</strong> Modal, as an infrastructure company, doesn’t use customer data for training internal models.</li>
</ul></li>
</ul>
</section>
<section id="running-databases-on-modal" class="level2">
<h2 class="anchored" data-anchor-id="running-databases-on-modal">Running Databases on Modal</h2>
<ul>
<li><strong>Question:</strong> Feasibility of running a key-value store (e.g., <a href="https://github.com/google/leveldb">LevelDB</a>) on Modal for a development web endpoint.</li>
<li><strong>Recommendations:</strong>
<ul>
<li><strong>Modal’s Built-in Solutions:</strong>
<ul>
<li><a href="https://modal.com/docs/reference/cli/dict"><code>modal.Dict</code></a>: Offers a persistent, distributed key-value store accessible to all Modal functions.</li>
<li><a href="https://modal.com/docs/reference/cli/queue"><code>modal.Queue</code></a>: Provides a distributed queue system similar to <a href="https://redis.io/">Redis</a>.</li>
</ul></li>
<li><strong>Alternative Approach for Analytic Databases:</strong>
<ul>
<li>Host databases externally (not ideal on Modal).</li>
<li>Mount cloud storage (e.g., S3) containing data in formats like Parquet or Arrow to Modal functions.</li>
<li>Utilize libraries like <a href="https://duckdb.org/">DuckDB</a> for efficient querying within the Modal environment.</li>
<li><strong>Example:</strong> <a href="https://modal.com/docs/examples/s3_bucket_mount#analyze-nyc-yellow-taxi-data-with-duckdb-on-parquet-files-from-s3">Analyze NYC yellow taxi data with DuckDB on Parquet files from S3</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="balancing-cost-and-uptime-for-gpu-inference" class="level2">
<h2 class="anchored" data-anchor-id="balancing-cost-and-uptime-for-gpu-inference">Balancing Cost and Uptime for GPU Inference</h2>
<ul>
<li><strong>Question:</strong> Finding the sweet spot between cost and uptime for GPU inference when needing varying levels of availability.</li>
<li><strong>Rule of Thumb:</strong>
<ul>
<li>Modal tends to be more cost-effective when utilization is 60% or lower.</li>
<li>Consider factors like acceptable latency and workload characteristics (batch jobs vs.&nbsp;real-time requests).</li>
</ul></li>
</ul>
</section>
<section id="local-vs.-cloud-workload-distribution" class="level2">
<h2 class="anchored" data-anchor-id="local-vs.-cloud-workload-distribution">Local vs.&nbsp;Cloud Workload Distribution</h2>
<ul>
<li><strong>Question:</strong> Deciding when to utilize a local GPU (e.g., RTX 4090) versus offloading to Modal, considering cost and time efficiency.</li>
<li><strong>Workload Breakdown:</strong>
<ul>
<li><strong>Inference:</strong> Local GPUs are well-suited due to typically small batch sizes, making VRAM less of a constraint.</li>
<li><strong>Evaluations:</strong> Larger eval sets might benefit from cloud GPUs for faster throughput, especially when running multiple evaluations concurrently.</li>
<li><strong>Fine-tuning:</strong> Often memory-intensive due to gradients and optimizer states. Cloud GPUs provide ample VRAM and simplify the use of techniques like sharding or larger batch sizes.</li>
</ul></li>
<li><strong>Don’t undervalue your time:</strong> Spending a little more on faster cloud compute can save a significant amount of time versus trying to run everything locally on a single GPU.</li>
</ul>
</section>
<section id="quick-qa" class="level2">
<h2 class="anchored" data-anchor-id="quick-qa">Quick Q&amp;A</h2>
<ul>
<li><strong>Autoscaling:</strong> Modal supports autoscaling with configurable parameters.
<ul>
<li><strong>Explanation:</strong> <a href="https://modal.com/docs/guide/concurrent-inputs#how-does-autoscaling-work-on-modal">How does autoscaling work on Modal?</a></li>
<li><strong>Auto-scaling LLM inference endpoints:</strong> <a href="https://modal.com/docs/examples/text_generation_inference">Hosting any LLaMA 3 model with Text Generation Inference (TGI)</a></li>
</ul></li>
<li><strong>Docker Image Access:</strong> Downloading built Docker images is not currently supported. Users can build and provide their own images.</li>
<li><strong>Inference Serving:</strong>
<ul>
<li><a href="https://github.com/vllm-project/vllm">vLLM</a> for its ease of use and rapid development</li>
<li><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> is a potentially faster but more involved alternative.</li>
</ul></li>
<li><strong>Demo Preparation:</strong> “Hello World” and “TRT LLM” examples are good starting points.
<ul>
<li><strong>Example:</strong> <a href="https://modal.com/docs/examples/hello_world">Hello, world!</a></li>
<li><strong>Example:</strong> <a href="https://modal.com/docs/examples/trtllm_llama">Serverless TensorRT-LLM (LLaMA 3 8B)</a></li>
</ul></li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-004/</guid>
  <pubDate>Sat, 06 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 4: Inspect - An OSS Framework for LLM Evals</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Inspect AI: A Framework for Evaluating LLMs</li>
<li>Hello World Example</li>
<li>Core Concepts</li>
<li>Honeycomb Dataset Example</li>
<li>Composition</li>
<li>Tool Use</li>
<li>Agents and Tools</li>
<li>Logging</li>
<li>Models</li>
<li>Workflow</li>
<li>Q&amp;A Session</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Resources">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Presentation Resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/jjallaire/inspect-llm-workshop">inspect-llm-workshop</a></li>
<li><strong>Slides:</strong> <a href="https://raw.githubusercontent.com/jjallaire/inspect-llm-workshop/main/slides/intro-to-inspect.pdf">Intro to Inspect</a></li>
</ul>
</div>
</div>
<section id="inspect-ai-a-framework-for-evaluating-llms" class="level2">
<h2 class="anchored" data-anchor-id="inspect-ai-a-framework-for-evaluating-llms">Inspect AI: A Framework for Evaluating LLMs</h2>
<ul>
<li>Inspect AI is a Python package for creating LLM evaluations developed through a collaboration between J.J. Allaire and the <a href="https://www.aisi.gov.uk/">UK AI Safety Institute</a>.</li>
<li>Designed to address the limitations of existing evaluation tools for developing more complex evals.</li>
<li>Focuses on providing a great experience for developing evals that can be reproducibly run at scale.</li>
<li><strong>Github Repository:</strong> <a href="https://github.com/UKGovernmentBEIS/inspect_ai">UKGovernmentBEIS/inspect_ai</a></li>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai">https://ukgovernmentbeis.github.io/inspect_ai</a></li>
<li><strong>Installation:</strong>
<ul>
<li><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install inspect-ai</span></code></pre></div></li>
</ul></li>
<li><strong>VS Code Extension:</strong>
<ul>
<li><strong>Marketplace:</strong> <a href="https://marketplace.visualstudio.com/items?itemName=ukaisi.inspect-ai">Inspect AI</a></li>
<li><strong>Source Code:</strong> <a href="https://github.com/UKGovernmentBEIS/inspect_ai/tree/main/tools/vscode">inspect-vscode</a></li>
</ul></li>
</ul>
</section>
<section id="hello-world-example" class="level2">
<h2 class="anchored" data-anchor-id="hello-world-example">Hello World Example</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/#sec-hello-inspect">Hello, Inspect</a></li>
</ul>
<div class="sourceCode" id="annotated-cell-1" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Task, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>, task</span>
<span id="annotated-cell-1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.dataset <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> example_dataset</span>
<span id="annotated-cell-1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.scorer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> model_graded_fact</span>
<span id="annotated-cell-1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.solver <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> (               </span>
<span id="annotated-cell-1-5">  chain_of_thought, generate, self_critique   </span>
<span id="annotated-cell-1-6">)                                             </span>
<span id="annotated-cell-1-7"></span>
<span id="annotated-cell-1-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="annotated-cell-1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> theory_of_mind():</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-10" class="code-annotation-target">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="annotated-cell-1-11">        dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>example_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theory_of_mind"</span>), </span>
<span id="annotated-cell-1-12">        plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-13" class="code-annotation-target">          chain_of_thought(),</span>
<span id="annotated-cell-1-14">          generate(),</span>
<span id="annotated-cell-1-15">          self_critique()</span>
<span id="annotated-cell-1-16">        ],</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-1-17" class="code-annotation-target">        scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact()</span>
<span id="annotated-cell-1-18">    )</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="10" data-code-annotation="1">The <code>Task</code> object brings together the dataset, solvers, and scorer, and is then evaluated using a model.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="13,14,15" data-code-annotation="2">In this example we are chaining together three standard solver components. It’s also possible to create a more complex custom solver that manages state and interactions internally.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="17" data-code-annotation="3">Since the output is likely to have pretty involved language, we use a model for scoring.</span>
</dd>
</dl>
</section>
<section id="core-concepts" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts">Core Concepts</h2>
<ul>
<li><strong>Dataset:</strong> List of inputs for the LLM
<ul>
<li>Usually includes targets (e.g., correct answers)</li>
</ul></li>
<li><strong>Solvers:</strong> Pipelines that execute the evaluation
<ul>
<li>Includes functions that transform the dataset inputs, call the model, and act on the model output</li>
<li>Can include things like prompt engineering and multi-turn dialogue</li>
<li>Can be composed together as layers, or can be a single layer with higher internal complexity</li>
</ul></li>
<li><strong>Scores:</strong> Evaluates the output of solvers
<ul>
<li>Ranges from simple text comparisons to model-graded assessments using custom rubrics.</li>
</ul></li>
</ul>
</section>
<section id="honeycomb-dataset-example" class="level2">
<h2 class="anchored" data-anchor-id="honeycomb-dataset-example">Honeycomb Dataset Example</h2>
<ul>
<li><strong>Jupyter Notebook:</strong> <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/main/honeycomb/queries.ipynb">honeycomb/queries.ipynb</a></li>
<li><strong>Dataset:</strong> <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/main/honeycomb/queries.csv">queries.csv</a>
<ul>
<li>~2,300 example queries (along with per-query column schemas generated offline via RAG)</li>
</ul></li>
<li><strong>Scoring Methods:</strong>
<ul>
<li><strong>validate:</strong> score using the validity checker: <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/main/honeycomb/utils.py">utils.py</a></li>
<li><strong>critique:</strong> score using the critique prompt: <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/main/honeycomb/critique.txt">critique.txt</a></li>
</ul></li>
<li><strong>Process:</strong>
<ol type="1">
<li>Load the dataset.</li>
<li>Define a pipeline that includes prompt engineering, model calling, and evaluation.</li>
<li>Apply a scoring function.</li>
</ol></li>
</ul>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/datasets.html">Datasets</a></li>
<li>Inspect uses a standard schema for Datasets
<ul>
<li>We map the raw data into that schema when reading it</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>“columns” are saved as metadata so we can use them for prompt engineering</li>
</ul>
</div>
</div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.dataset <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> csv_dataset, FieldSpec</span>
<span id="cb2-2"></span>
<span id="cb2-3">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> csv_dataset(</span>
<span id="cb2-4">    csv_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"queries.csv"</span>,</span>
<span id="cb2-5">    sample_fields<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>FieldSpec(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user_input"</span>, metadata<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"columns"</span>]),</span>
<span id="cb2-6">    shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-7">)</span></code></pre></div>
</section>
<section id="solver" class="level3">
<h3 class="anchored" data-anchor-id="solver">Solver</h3>
<ul>
<li><p><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/solvers.html">Solver</a></p></li>
<li><p>Functions that manipulate the <a href="https://ukgovernmentbeis.github.io/inspect_ai/solvers.html#task-states"><code>TaskState</code></a> (message history and model output) to perform actions like:</p>
<ul>
<li>Calling the model to generate text.</li>
<li>Engineering prompts.</li>
<li>Applying critique mechanisms.</li>
</ul></li>
<li><p><a href="https://ukgovernmentbeis.github.io/inspect_ai/solvers.html#built-in-solvers"><strong>Built-In Solvers</strong></a><strong>:</strong></p>
<ul>
<li><code>generate()</code>: Calls the model, appends the assistant message, and updates the model output</li>
<li><code>chain_of_thought()</code>: Basic chain-of-thought implementation.</li>
<li><code>prompt_template()</code>: Modifies the existing prompt by passing it through a template</li>
<li><code>multiple_choice()</code>: Handles multiple-choice questions, including shuffling options, calling the model, and unshuffling to determine the chosen answer.</li>
<li><code>self_critique()</code>
<ul>
<li>Performs self-critique by:
<ol type="1">
<li>Running a critique model on the initial output.</li>
<li>Appending the critique to the message history.</li>
<li>Calling the model again to generate a revised answer.</li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<section id="solver-prompt_with_schema" class="level4">
<h4 class="anchored" data-anchor-id="solver-prompt_with_schema">Solver: <code>prompt_with_schema()</code></h4>
<ul>
<li>Simple prompt template that substitutes the user query and the RAG-generated column schema.</li>
</ul>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.solver <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> solver</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.util <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> resource</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@solver</span></span>
<span id="cb3-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> prompt_with_schema():</span>
<span id="cb3-6"></span>
<span id="cb3-7">    prompt_template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resource(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prompt.txt"</span>)</span>
<span id="cb3-8"></span>
<span id="cb3-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> solve(state, generate):</span>
<span id="cb3-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># build the prompt</span></span>
<span id="cb3-11">        state.user_prompt.text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prompt_template.replace(</span>
<span id="cb3-12">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">prompt</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, state.user_prompt.text</span>
<span id="cb3-13">        ).replace(</span>
<span id="cb3-14">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">columns</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, state.metadata[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"columns"</span>]</span>
<span id="cb3-15">        )</span>
<span id="cb3-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> state</span>
<span id="cb3-17"></span>
<span id="cb3-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> solve</span></code></pre></div>
</section>
</section>
<section id="scorer" class="level3">
<h3 class="anchored" data-anchor-id="scorer">Scorer</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/scorers.html">Scorer</a>
<ul>
<li>Evaluates whether solvers were successful in finding the right <code>output</code> for the <code>target</code> defined in the dataset, and in what measure</li>
<li>Pluggable (i.e.&nbsp;provided from other packages)</li>
</ul></li>
<li><strong><a href="https://ukgovernmentbeis.github.io/inspect_ai/scorers.html#built-in-scorers">Built-In Scorers</a>:</strong>
<ul>
<li>Pattern matching</li>
<li>Template matching</li>
<li>Model grading</li>
<li>Human scoring</li>
</ul></li>
<li><strong>Example: Math Benchmark with Expression Equivalence</strong>
<ul>
<li><strong>Source Code:</strong> <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/main/benchmarks/mathematics.py">benchmarks/mathematics.py</a></li>
<li>Uses a custom scorer <code>expression_equivalance</code> to evaluate mathematical expressions for logical equivalence, accounting for variations in formatting or simplification.</li>
<li><strong>Scorer: <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/86d00ca6d79b4754266ba36c10be6d5a108a1695/benchmarks/mathematics.py#L56C5-L56C27"><code>expression_equivalance()</code></a></strong>
<ul>
<li>Extracts the model’s answer using regular expressions.</li>
<li>Employs a few-shot prompting technique to train a model for assessing the equivalence of mathematical expressions.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@scorer</span>(metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[accuracy(), bootstrap_std()])</span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> expression_equivalance():</span>
<span id="cb4-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score(state: TaskState, target: Target):</span>
<span id="cb4-4">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># extract answer</span></span>
<span id="cb4-5">        match <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.search(AnswerPattern.LINE, state.output.completion)</span>
<span id="cb4-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> match:</span>
<span id="cb4-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ask the model to judge equivalance</span></span>
<span id="cb4-8">            answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> match.group(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-9">            prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> EQUIVALANCE_TEMPLATE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> (</span>
<span id="cb4-10">                {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expression1"</span>: target.text, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expression2"</span>: answer}</span>
<span id="cb4-11">            )</span>
<span id="cb4-12">            result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> get_model().generate(prompt)</span>
<span id="cb4-13"></span>
<span id="cb4-14">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return the score</span></span>
<span id="cb4-15">            correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result.completion.lower() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"yes"</span></span>
<span id="cb4-16">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Score(</span>
<span id="cb4-17">                value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>CORRECT <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> correct <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> INCORRECT,</span>
<span id="cb4-18">                answer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>answer,</span>
<span id="cb4-19">                explanation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>state.output.completion,</span>
<span id="cb4-20">            )</span>
<span id="cb4-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb4-22">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Score(</span>
<span id="cb4-23">                value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>INCORRECT,</span>
<span id="cb4-24">                explanation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Answer not found in model output: "</span></span>
<span id="cb4-25">                <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>output<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>completion<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb4-26">            )</span>
<span id="cb4-27"></span>
<span id="cb4-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> score</span></code></pre></div>
</div>
</div>
</div></li>
</ul></li>
</ul></li>
</ul>
<section id="scorer-validate_scorer" class="level4">
<h4 class="anchored" data-anchor-id="scorer-validate_scorer">Scorer: <code>validate_scorer()</code></h4>
<ul>
<li>Extracts and cleans JSON output from the model.</li>
<li>Calls the <code>is_valid()</code> function with the column schema to determine if a valid query was generated.</li>
</ul>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.scorer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy, scorer, Score, CORRECT, INCORRECT</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> is_valid, json_completion</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@scorer</span>(metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[accuracy()])</span>
<span id="cb5-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> validate_scorer():</span>
<span id="cb5-6"></span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score(state, target):</span>
<span id="cb5-8">       </span>
<span id="cb5-9">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># check for valid query</span></span>
<span id="cb5-10">        query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_completion(state.output.completion)</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> is_valid(query, state.metadata[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"columns"</span>]):</span>
<span id="cb5-12">            value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>CORRECT</span>
<span id="cb5-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>: </span>
<span id="cb5-14">            value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>INCORRECT</span>
<span id="cb5-15">       </span>
<span id="cb5-16">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return score w/ query that was extracted</span></span>
<span id="cb5-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Score(value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>value, answer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>query)</span>
<span id="cb5-18"></span>
<span id="cb5-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> score</span></code></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The <a href="https://github.com/jjallaire/inspect-llm-workshop/blob/86d00ca6d79b4754266ba36c10be6d5a108a1695/honeycomb/utils.py#L7"><code>json_completion()</code></a> function takes care of some details around extracting JSON from a model completion (e.g.&nbsp;removing sorrounding backtick code block emitted by some models)</li>
</ul>
</div>
</div>
</section>
</section>
<section id="validate-task" class="level3">
<h3 class="anchored" data-anchor-id="validate-task">Validate Task</h3>
<ul>
<li><a href="https://github.com/UKGovernmentBEIS/inspect_ai/blob/53fe15a44d83400ad2847cb2f917ac282cba126d/src/inspect_ai/_eval/task/task.py#L17"><code>Task</code></a>:
<ul>
<li>The basis for defining and running evaluations</li>
<li>Parameterized with a dataset, a scorer, and metrics.</li>
<li>May optionally provide a default plan for execution.</li>
</ul></li>
</ul>
<section id="honeycomb-eval-validate" class="level4">
<h4 class="anchored" data-anchor-id="honeycomb-eval-validate">Honeycomb Eval: <code>validate()</code></h4>
<ul>
<li>Combines the dataset, solver, and scorer defined above into a <code>Task</code>.</li>
<li>Uses a predefined system message and prompt template.</li>
</ul>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>, task, Task</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.solver <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> system_message, generate</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb6-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> validate():</span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb6-7">        dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dataset,</span>
<span id="cb6-8">        plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb6-9">            system_message(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Honeycomb AI suggests queries based on user input."</span>),</span>
<span id="cb6-10">            prompt_with_schema(),</span>
<span id="cb6-11">            generate()</span>
<span id="cb6-12">        ],</span>
<span id="cb6-13">        scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>validate_scorer()</span>
<span id="cb6-14">    )</span></code></pre></div>
<ul>
<li>Run the <code>Task</code> using Inspect’s <code>eval()</code> function (limiting to 100 samples):</li>
</ul>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'__main__'</span>:</span>
<span id="cb7-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>(validate, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4-turbo"</span>, limit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/images/validate-task-1.png" class="img-fluid figure-img"></p>
<figcaption>validate-task-1</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The <code>__name__ == '__main__'</code> conditional indicates that we only want to run this cell in interactive contexts.
<ul>
<li>Allows us to also use the notebook as a module callable from <code>inspect eval</code>:</li>
<li><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval queries.ipynb@validate</span></code></pre></div></li>
</ul></li>
</ul>
</div>
</div>
</section>
</section>
<section id="eval-view" class="level3">
<h3 class="anchored" data-anchor-id="eval-view">Eval View</h3>
<ul>
<li><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb9-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">inspect</span> view</span></code></pre></div>
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code>$ inspect view
Inspect view running at http://localhost:7575/</code></pre>
</div>
</div></li>
</ul></li>
<li><p>Provides an overview of evaluation results.</p></li>
<li><p>Allows drilling down into individual samples to examine message history, inputs, and model outputs for debugging.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/images/inspect-view-validate-task.png" class="img-fluid figure-img"></p>
<figcaption>inspect-view-validate-task</figcaption>
</figure>
</div>
</section>
<section id="critique-task" class="level3">
<h3 class="anchored" data-anchor-id="critique-task">Critique Task</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/models.html">Models</a></li>
</ul>
<section id="scorer-critique_scorer" class="level4">
<h4 class="anchored" data-anchor-id="scorer-critique_scorer">Scorer: <code>critique_scorer()</code></h4>
<ul>
<li>Allows using different models (e.g., GPT-4 Turbo) to critique the generated queries.</li>
<li>Builds a critique prompt using a predefined template and the model’s output to have the critique model indicate whether the generated query is “good” or “bad”.</li>
<li>Returns a score based on the critique model’s assessment.</li>
</ul>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> get_model</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@scorer</span>(metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[accuracy()])</span>
<span id="cb11-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> critique_scorer(model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"anthropic/claude-3-5-sonnet-20240620"</span>):</span>
<span id="cb11-6"></span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score(state, target):</span>
<span id="cb11-8">       </span>
<span id="cb11-9">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># build the critic prompt</span></span>
<span id="cb11-10">        query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state.output.completion.strip()</span>
<span id="cb11-11">        critic_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resource(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"critique.txt"</span>).replace(</span>
<span id="cb11-12">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">prompt</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, state.user_prompt.text</span>
<span id="cb11-13">        ).replace(</span>
<span id="cb11-14">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">columns</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, state.metadata[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"columns"</span>]</span>
<span id="cb11-15">        ).replace(</span>
<span id="cb11-16">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">query</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, query</span>
<span id="cb11-17">        )</span>
<span id="cb11-18">       </span>
<span id="cb11-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># run the critique</span></span>
<span id="cb11-20">        result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> get_model(model).generate(critic_prompt)</span>
<span id="cb11-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">try</span>:</span>
<span id="cb11-22">            parsed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.loads(json_completion(result.completion))</span>
<span id="cb11-23">            value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CORRECT <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> parsed[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcome"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"good"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> INCORRECT</span>
<span id="cb11-24">            explanation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> parsed[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"critique"</span>]</span>
<span id="cb11-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">KeyError</span>):</span>
<span id="cb11-26">            value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> INCORRECT</span>
<span id="cb11-27">            explanation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"JSON parsing error:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>completion<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb11-28">        </span>
<span id="cb11-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return value and explanation (critique text)</span></span>
<span id="cb11-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Score(value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>value, explanation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>explanation)</span>
<span id="cb11-31"></span>
<span id="cb11-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> score</span></code></pre></div>
</section>
<section id="honeycomb-eval-critique" class="level4">
<h4 class="anchored" data-anchor-id="honeycomb-eval-critique">Honeycomb Eval: <code>critique()</code></h4>
<ul>
<li>Utilizes the same dataset and plan as <code>validate()</code> but employs a critique model for scoring.</li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb12-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> critique():</span>
<span id="cb12-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb12-4">        dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dataset,</span>
<span id="cb12-5">        plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb12-6">            system_message(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Honeycomb AI suggests queries based on user input."</span>),</span>
<span id="cb12-7">            prompt_with_schema(),</span>
<span id="cb12-8">            generate()</span>
<span id="cb12-9">        ],</span>
<span id="cb12-10">        scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>critique_scorer()</span>
<span id="cb12-11">    )</span></code></pre></div>
<ul>
<li>Run the task using <code>eval()</code> (limiting to 25 samples):</li>
</ul>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'__main__'</span>:</span>
<span id="cb13-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>(critique, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4-turbo"</span>, limit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/images/critique-task-1.png" class="img-fluid figure-img"></p>
<figcaption>critique-task-1</figcaption>
</figure>
</div>
</section>
</section>
<section id="critique-eval-view" class="level3">
<h3 class="anchored" data-anchor-id="critique-eval-view">Critique Eval View</h3>
<ul>
<li><div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb14-1">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">inspect</span> view</span></code></pre></div></li>
<li>Displays critique-based evaluation results.</li>
<li>Provides insights into the critique model’s explanations for incorrect answers, aiding in prompt or model improvement.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/images/inspect-view-critique-task.png" class="img-fluid figure-img"></p>
<figcaption>inspect-view-critique-task</figcaption>
</figure>
</div>
</section>
</section>
<section id="composition" class="level2">
<h2 class="anchored" data-anchor-id="composition">Composition</h2>
<ul>
<li>Inspect AI encourages composing evaluations by combining solvers and scorers from different sources.</li>
<li>Custom solvers and scorers can be made available in a Python package to re-use across many evals.</li>
</ul>
<section id="example-jailbreaking-with-sheppard" class="level3">
<h3 class="anchored" data-anchor-id="example-jailbreaking-with-sheppard">Example: Jailbreaking with <code>sheppard</code></h3>
<ul>
<li><p><code>sheppard</code>: An internal package that integrates jailbreak solvers to elicit responses from models they might otherwise refuse to provide.</p></li>
<li><table class="caption-top table">
<thead>
<tr class="header">
<th>Solver</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>encode()</code></td>
<td><a href="https://arxiv.org/abs/2406.08754v1">Message obfuscation jailbreak</a></td>
</tr>
<tr class="even">
<td><code>pap_jailbreak()</code></td>
<td><a href="https://arxiv.org/abs/2401.06373">Persuasion Adversarial Prompt (PAP)</a></td>
</tr>
<tr class="odd">
<td><code>payload_splitting()</code></td>
<td><a href="https://arxiv.org/abs/2302.05733">Payload splitting jailbreak</a></td>
</tr>
<tr class="even">
<td><code>cr_jailbreak()</code></td>
<td>Content reinforcement</td>
</tr>
</tbody>
</table></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example: Using sheppard to provide jailbreaks for a security eval:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Using sheppard to provide jailbreaks for a security eval:
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Task, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>, task</span>
<span id="cb16-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.scorer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> model_graded_fact</span>
<span id="cb16-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect_ai.solver <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> generate, system_message</span>
<span id="cb16-4"></span>
<span id="cb16-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sheppard <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pap_jailbreak</span>
<span id="cb16-6"></span>
<span id="cb16-7"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb16-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> security_guide():</span>
<span id="cb16-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb16-10">        dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>example_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security_guide"</span>),</span>
<span id="cb16-11">        plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb16-12">          system_message(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system.txt"</span>), </span>
<span id="cb16-13">          pap_jailbreak(),</span>
<span id="cb16-14">          generate()</span>
<span id="cb16-15">        ],</span>
<span id="cb16-16">        scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact(model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4"</span>),</span>
<span id="cb16-17">    )</span></code></pre></div>
</div>
</div></li>
</ul>
</section>
</section>
<section id="tool-use" class="level2">
<h2 class="anchored" data-anchor-id="tool-use">Tool Use</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/tools.html">Tools</a>
<ul>
<li>Python functions that can be made accessible to the model during evaluation.</li>
<li>Allow for more complex interactions, like web search or database lookups.</li>
<li><div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> TaskState:</span>
<span id="cb17-2">      messages: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[ChatMessage]</span>
<span id="cb17-3">      tools: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[ToolDef]</span>
<span id="cb17-4">      tool_choice: ToolChoice</span>
<span id="cb17-5">      output: ModelOutput</span>
<span id="cb17-6">      ...</span></code></pre></div></li>
</ul></li>
</ul>
<section id="example-biology-qa-with-web-search" class="level3">
<h3 class="anchored" data-anchor-id="example-biology-qa-with-web-search">Example: Biology QA with Web Search</h3>
<ul>
<li>Provides a web search tool to the model, enabling it to answer obscure biology questions.</li>
<li><div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">  <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb18-2">      dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>example_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"biology_qa"</span>),</span>
<span id="cb18-3">      plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb18-4">          use_tools(web_search()), </span>
<span id="cb18-5">          generate()</span>
<span id="cb18-6">      ],</span>
<span id="cb18-7">      scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_qa(template<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>GRADER_TEMPLATE),</span>
<span id="cb18-8">  )</span></code></pre></div></li>
</ul>
</section>
</section>
<section id="agents-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="agents-and-tools">Agents and Tools</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/agents.html">Agents</a>
<ul>
<li>Solvers that use tools to perform tasks.</li>
<li>Can use bespoke agent logic inside a solver (swapping various tools in and out)</li>
<li>Can integrate existing agent libraries like Langchain as solvers.</li>
</ul></li>
</ul>
<section id="agent-capture-the-flag" class="level3">
<h3 class="anchored" data-anchor-id="agent-capture-the-flag">Agent: Capture the Flag</h3>
<ul>
<li>Example of a custom agent designed for cybersecurity evaluations, where the model interacts with tools to solve capture-the-flag challenges within a Docker environment.</li>
<li><div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">  Plan(</span>
<span id="cb19-2">      steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb19-3">          init_challenge(),</span>
<span id="cb19-4">          use_tools([</span>
<span id="cb19-5">              command_exec(), create_file(),</span>
<span id="cb19-6">              decompile(), disassemble(),</span>
<span id="cb19-7">              check_flag(),</span>
<span id="cb19-8">          ]),</span>
<span id="cb19-9">          system_message(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prompts/system.txt"</span>),</span>
<span id="cb19-10">          initial_user_message(),</span>
<span id="cb19-11">          generate(),</span>
<span id="cb19-12">          check_for_flag_or_continue()</span>
<span id="cb19-13">      ],</span>
<span id="cb19-14">      cleanup<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exit_challenge()</span>
<span id="cb19-15">  )</span></code></pre></div></li>
</ul>
</section>
<section id="agent-langchain" class="level3">
<h3 class="anchored" data-anchor-id="agent-langchain">Agent: LangChain</h3>
<ul>
<li><strong>Project Folder:</strong> <a href="https://github.com/jjallaire/inspect-llm-workshop/tree/main/langchain">inspect-llm-workshop/langchain</a>
<ul>
<li>Uses <a href="https://tavily.com/">tavily</a>, a search engine optimized for LLMs and RAG</li>
</ul></li>
<li>Demonstrates integrating a Langchain agent into Inspect AI using a higher-order function.</li>
<li>Allows leveraging existing agent frameworks within Inspect AI’s evaluation pipeline.</li>
<li><div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@solver</span></span>
<span id="cb20-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> wikipedia_search() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Solver:</span>
<span id="cb20-3"></span>
<span id="cb20-4">    tavily_api <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TavilySearchAPIWrapper() </span>
<span id="cb20-5">    tools <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ([TavilySearchResults(api_wrapper<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tavily_api)] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> </span>
<span id="cb20-6">        load_tools([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"wikipedia"</span>]))</span>
<span id="cb20-7"></span>
<span id="cb20-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> agent(llm: BaseChatModel, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, Any]):</span>
<span id="cb20-9">        tools_agent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> create_openai_tools_agent(llm, tools, prompt)</span>
<span id="cb20-10">        agent_executor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AgentExecutor.from_agent_and_tools(</span>
<span id="cb20-11">            agent<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tools_agent,</span>
<span id="cb20-12">            tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tools</span>
<span id="cb20-13">        )</span>
<span id="cb20-14">        result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> agent_executor.ainvoke(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>)</span>
<span id="cb20-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"output"</span>]</span>
<span id="cb20-16"></span>
<span id="cb20-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> langchain_solver(agent)</span></code></pre></div></li>
</ul>
</section>
</section>
<section id="logging" class="level2">
<h2 class="anchored" data-anchor-id="logging">Logging</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/eval-logs.html">Eval Logs</a></li>
<li>Logging is crucial for debugging, analysis, and reproducibility.</li>
<li>Capture all context required to debug, analyse, and reproduce evaluations</li>
<li><strong>Inspect AI Logging:</strong>
<ul>
<li>Provides a rich Python API and JSON representation of the evaluation process.</li>
<li>Offers a log viewer for interactive exploration.</li>
<li>Enables programmatic access to logs for analysis and comparison.</li>
</ul></li>
</ul>
<section id="evallog" class="level3">
<h3 class="anchored" data-anchor-id="evallog">EvalLog</h3>
<ul>
<li><p><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/eval-logs.html#evallog">EvalLog</a></p></li>
<li><p>Returned from <code>eval()</code></p></li>
<li><p>Provides programmatic interface to the contents of log files</p></li>
<li><table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 24%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>status</code></td>
<td><code>str</code></td>
<td>Status of evaluation</td>
</tr>
<tr class="even">
<td><code>eval</code></td>
<td><code>EvalSpec</code></td>
<td>Top level eval details including task, model, creation time, etc.</td>
</tr>
<tr class="odd">
<td><code>plan</code></td>
<td><code>EvalPlan</code></td>
<td>List of solvers and model generation config used for the eval.</td>
</tr>
<tr class="even">
<td><code>samples</code></td>
<td><code>list[EvalSample]</code></td>
<td>Each sample evaluated, including its input, output, target, and score.</td>
</tr>
<tr class="odd">
<td><code>results</code></td>
<td><code>EvalResults</code></td>
<td>Aggregated scorer results</td>
</tr>
<tr class="even">
<td><code>stats</code></td>
<td><code>EvalStats</code></td>
<td>Model token usage stats</td>
</tr>
<tr class="odd">
<td><code>logging</code></td>
<td><code>list[LoggingMessage]</code></td>
<td>Logging messages (e.g.&nbsp;from <code>log.info()</code>, <code>log.debug()</code>, etc.</td>
</tr>
<tr class="even">
<td><code>error</code></td>
<td><code>EvalError</code></td>
<td>Error information</td>
</tr>
</tbody>
</table></li>
</ul>
</section>
</section>
<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">Models</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/models.html">Models</a></li>
<li><table class="caption-top table">
<thead>
<tr class="header">
<th>Provider</th>
<th>Model Name</th>
<th>Docs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>OpenAI</td>
<td><code>openai/gpt-3.5-turbo</code></td>
<td><a href="https://platform.openai.com/docs/models/overview">OpenAI Models</a></td>
</tr>
<tr class="even">
<td>Anthropic</td>
<td><code>anthropic/claude-3-sonnet-20240229</code></td>
<td><a href="https://docs.anthropic.com/claude/docs/models-overview">Anthropic Models</a></td>
</tr>
<tr class="odd">
<td>Google</td>
<td><code>google/gemini-1.0-pro</code></td>
<td><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models">Google Models</a></td>
</tr>
<tr class="even">
<td>Mistral</td>
<td><code>mistral/mistral-large-latest</code></td>
<td><a href="https://docs.mistral.ai/platform/endpoints/">Mistral Models</a></td>
</tr>
<tr class="odd">
<td>Hugging Face</td>
<td><code>hf/openai-community/gpt2</code></td>
<td><a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending">Hugging Face Models</a></td>
</tr>
<tr class="even">
<td>Ollama</td>
<td><code>ollama/llama3</code></td>
<td><a href="https://ollama.com/library">Ollama Models</a></td>
</tr>
<tr class="odd">
<td>TogetherAI</td>
<td><code>together/lmsys/vicuna-13b-v1.5</code></td>
<td><a href="https://docs.together.ai/docs/inference-models#chat-models">TogetherAI Models</a></td>
</tr>
<tr class="even">
<td>AWS Bedrock</td>
<td><code>bedrock/meta.llama2-70b-chat-v1</code></td>
<td><a href="https://aws.amazon.com/bedrock/">AWS Bedrock Models</a></td>
</tr>
<tr class="odd">
<td>Azure AI</td>
<td><code>azureai/azure-deployment-name</code></td>
<td><a href="https://ai.azure.com/explore/models">Azure AI Models</a></td>
</tr>
<tr class="even">
<td>Cloudflare</td>
<td><code>cf/meta/llama-2-7b-chat-fp16</code></td>
<td><a href="https://developers.cloudflare.com/workers-ai/models/#text-generation">Cloudflare Models</a></td>
</tr>
</tbody>
</table></li>
<li>Allows custom model providers.</li>
<li>Inspect AI remains agnostic to specific model implementations, allowing flexibility and future compatibility.</li>
</ul>
</section>
<section id="workflow" class="level2">
<h2 class="anchored" data-anchor-id="workflow">Workflow</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/workflow.html">Workflow</a></li>
</ul>
<section id="interactive-development" class="level3">
<h3 class="anchored" data-anchor-id="interactive-development">Interactive Development:</h3>
<ul>
<li>Designed for iterative development within notebooks.</li>
<li>Provides tools for exploration, such as grid search over parameters.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Ad-hoc exploration of an eval in a Notebook/REPL">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ad-hoc exploration of an eval in a Notebook/REPL
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb21-2">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"devops.txt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"researcher.txt"</span>],</span>
<span id="cb21-3">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grader"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hacker.txt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expert.txt"</span>],</span>
<span id="cb21-4">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grader_model"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google/gemini-1.0-pro"</span>]</span>
<span id="cb21-5">}</span>
<span id="cb21-6">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(product(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(params[name] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> name <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params)))</span>
<span id="cb21-7"></span>
<span id="cb21-8">tasks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [Task(</span>
<span id="cb21-9">    dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>json_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security_guide.jsonl"</span>),</span>
<span id="cb21-10">    plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[system_message(system), generate()],</span>
<span id="cb21-11">    scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact(template<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>grader, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>grader_model)</span>
<span id="cb21-12">) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> system, grader, grader_model <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params]</span>
<span id="cb21-13"></span>
<span id="cb21-14">logs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>(tasks, model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mistral/mistral-large-latest"</span>)</span>
<span id="cb21-15">plot_results(logs)</span></code></pre></div>
</div>
</div></li>
</ul>
</section>
<section id="task-parameters" class="level3">
<h3 class="anchored" data-anchor-id="task-parameters">Task Parameters:</h3>
<ul>
<li>Allows defining tasks with configurable parameters.</li>
<li>Enables running evaluations with varying settings from external scripts or notebooks.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Formalise variation with a parameterised @task function:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Formalise variation with a parameterised <span class="citation" data-cites="task">@task</span> function:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb22-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> security_guide(system<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"devops.txt"</span>, grader<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expert.txt"</span>):</span>
<span id="cb22-3">   <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb22-4">      dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security_guide.jsonl"</span>),</span>
<span id="cb22-5">      plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[system_message(system), generate()],</span>
<span id="cb22-6">      scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact(template<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>grader, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4"</span>)</span>
<span id="cb22-7">   )</span>
<span id="cb22-8"></span>
<span id="cb22-9">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb22-10">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"devops.txt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"researcher.txt"</span>],</span>
<span id="cb22-11">   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grader"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hacker.txt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expert.txt"</span>]</span>
<span id="cb22-12">}</span>
<span id="cb22-13">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(product(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(params[name] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> name <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params)))</span>
<span id="cb22-14"></span>
<span id="cb22-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>([security_guide(system,grader) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> system, grader <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params],</span>
<span id="cb22-16">     model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mistral/mistral-large-latest"</span>)</span></code></pre></div>
</div>
</div></li>
<li><code>@task</code> functions are registered and addressable by external driver programs (step one in development =&gt; production)
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb23-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> security_guide(system<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"devops.txt"</span>, grader<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expert.txt"</span>):</span>
<span id="cb23-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb23-4">        dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security_guide.jsonl"</span>),</span>
<span id="cb23-5">        plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[system_message(system), generate()],</span>
<span id="cb23-6">        scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact(</span>
<span id="cb23-7">            template<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>grader, </span>
<span id="cb23-8">            model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4"</span></span>
<span id="cb23-9">        )</span>
<span id="cb23-10">    )</span></code></pre></div>
<div class="sourceCode" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.py <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-T</span> system=devops.txt </span>
<span id="cb24-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.py <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-T</span> grader=hacker.txt </span></code></pre></div>
<div class="sourceCode" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.ipynb <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-T</span> system=devops.txt </span>
<span id="cb25-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.ipynb <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-T</span> grader=hacker.txt </span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> security_guide(system, grader<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expert.txt"</span>):</span>
<span id="cb26-2">   <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Task(</span>
<span id="cb26-3">      dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security_guide.jsonl"</span>),</span>
<span id="cb26-4">      plan<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[system_message(system), generate()],</span>
<span id="cb26-5">      scorer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_graded_fact(template<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>grader, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4"</span>)</span>
<span id="cb26-6">   )</span>
<span id="cb26-7"></span>
<span id="cb26-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb26-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> devops()</span>
<span id="cb26-10">   <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> security_guide(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"devops.txt"</span>)</span>
<span id="cb26-11"></span>
<span id="cb26-12"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@task</span></span>
<span id="cb26-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> researcher()</span>
<span id="cb26-14">   <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> security_guide(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"researcher.txt"</span>)</span></code></pre></div>
<div class="sourceCode" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.py@devops</span>
<span id="cb27-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> inspect eval security_guide.py@researcher</span></code></pre></div>
</div>
</div></li>
</ul></li>
</ul>
</section>
<section id="eval-suites" class="level3">
<h3 class="anchored" data-anchor-id="eval-suites">Eval Suites:</h3>
<ul>
<li><strong>Documentation:</strong> <a href="https://ukgovernmentbeis.github.io/inspect_ai/eval-suites.html">Eval Suites</a></li>
<li>Supports organizing and running multiple evaluations as suites.</li>
</ul>
</section>
<section id="resiliency" class="level3">
<h3 class="anchored" data-anchor-id="resiliency">Resiliency:</h3>
<ul>
<li>Encourages running evaluations in production environments (e.g., CI) with features like log storage and retry mechanisms.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Simplified Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simplified Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># setup log context</span></span>
<span id="cb28-2">os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"INSPECT_LOG_DIR"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./security-suite_04-07-2024"</span></span>
<span id="cb28-3"></span>
<span id="cb28-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># run the eval suite</span></span>
<span id="cb28-5">tasks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> list_tasks(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security"</span>)</span>
<span id="cb28-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>(tasks, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mistral/mistral-large-latest"</span>)</span>
<span id="cb28-7"></span>
<span id="cb28-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...later, in another process that also has INSPECT_LOG_DIR</span></span>
<span id="cb28-9">error_logs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> list_eval_logs(status <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"error"</span>)</span>
<span id="cb28-10">eval_retry(error_logs)</span></code></pre></div>
</div>
</div></li>
</ul>
</section>
<section id="provenance" class="level3">
<h3 class="anchored" data-anchor-id="provenance">Provenance:</h3>
<ul>
<li>Ensures reproducibility by storing Git repository information within the log file.</li>
<li>Allows recreating the evaluation environment and parameters from the log.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># read the log and extract the origin and commit</span></span>
<span id="cb29-2">log <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> read_eval_log(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"security-log.json"</span>)</span>
<span id="cb29-3">origin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log.spec.revision.origin</span>
<span id="cb29-4">commit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log.spec.revision.commit</span>
<span id="cb29-5"></span>
<span id="cb29-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># clone the repo, checkout the commit, install deps, and run</span></span>
<span id="cb29-7">run([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"git"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"clone"</span>, revision.origin, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval-dir"</span>])</span>
<span id="cb29-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> chdir(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval-dir"</span>):</span>
<span id="cb29-9">   run([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"git"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkout"</span>, revision.commit])</span>
<span id="cb29-10">   run([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pip"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"install"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-r"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"requirements.txt"</span>])</span>
<span id="cb29-11">   <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>(log) </span></code></pre></div>
</div>
</div></li>
</ul>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<ul>
<li><strong>Integration with Posit Products:</strong> Inspect AI is not a Posit project and currently has no plans for integration.</li>
<li><strong>Evaluating Past LLM Interactions:</strong> Inspect AI can evaluate past interactions by using message history as input.</li>
<li><strong>Expanding Evaluation Metrics:</strong> The Inspect AI team plans to expand the list of built-in metrics based on community needs.</li>
<li><strong>Future Development and Direction:</strong> Long-term development is prioritized, with a focus on community collaboration and supporting a wide range of evaluation scenarios.</li>
<li><strong>Log Sources Beyond Files:</strong> Currently, logs are primarily file-based, but future development may include database logging capabilities.</li>
<li><strong>Shareable Security Tests:</strong> The Inspect AI team anticipates the creation and sharing of security test suites within the community.</li>
<li><strong>Integration with Weights &amp; Biases:</strong> Integration with Weights &amp; Biases is planned to streamline metric tracking and visualization.</li>
<li><strong>Design Philosophy:</strong> Inspired by principles of cleanliness, simplicity, and composability.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-004/</guid>
  <pubDate>Sat, 06 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 3: Prompt Engineering Workshop</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-003/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>What is a Large Language Model?</li>
<li>Prompt Crafting</li>
<li>LLMs are Dumb Mechanical Humans</li>
<li>Building LLM Applications</li>
<li>Creating the Prompt</li>
<li>The Introduction of Chat</li>
<li>The Introduction of Tools</li>
<li>Building LLM Applications - Continued</li>
<li>Creating the Prompt: Copilot Chat</li>
<li>Tips for Defining Tools</li>
<li>Q&amp;A Session</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Slides">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Presentation Slides
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://docs.google.com/presentation/d/1PXzENGNN5NFbEDJ59wbSp8fro6dPt4xHGNN6X0KU82A/">Prompt Engineering - John Berryman</a></li>
</ul>
</div>
</div>
<section id="what-is-a-language-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-language-model">What is a Language Model?</h2>
<ul>
<li><strong>Language Model (LM):</strong> An AI system trained on vast text data to understand and generate human-like text. Its primary function is predicting the next word in a sequence.</li>
<li><strong>Large Language Model (LLM):</strong> A significantly larger and more complex LM, showcasing enhanced capabilities in understanding and generating human language.</li>
</ul>
<section id="what-is-a-large-language-model" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-large-language-model">What is a Large Language Model?</h3>
<section id="evolution-of-llms" class="level4">
<h4 class="anchored" data-anchor-id="evolution-of-llms">Evolution of LLMs:</h4>
<ul>
<li><strong>Recurrent Neural Networks (RNNs):</strong> Initial models with limitations in handling long sequences due to the bottleneck between encoder and decoder.</li>
<li><strong>Attention Mechanism:</strong> Introduced to focus on relevant parts of the input sequence, addressing the limitations of RNNs.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ul></li>
<li><strong>Transformer Architecture:</strong> Replaced RNNs by focusing entirely on attention, leading to significant improvements in performance and efficiency.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ul></li>
<li><strong>BERT and GPT:</strong>
<ul>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Utilizes the encoder part of the transformer, excelling in tasks like understanding the context of words in a sentence.</li>
<li><strong>GPT (Generative Pre-trained Transformer):</strong> Utilizes the decoder part of the transformer, specializing in generating coherent and contextually relevant text.
<ul>
<li><strong>Paper:</strong> <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="capabilities-and-concerns" class="level4">
<h4 class="anchored" data-anchor-id="capabilities-and-concerns">Capabilities and Concerns:</h4>
<ul>
<li>GPT-2 exhibited impressive unsupervised capabilities across various tasks, including translation, summarization, and question answering.
<ul>
<li><strong>Paper:</strong> <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
</ul></li>
<li>The power of LLMs raises concerns about potential misuse, as they can be manipulated to generate misleading or harmful content.</li>
</ul>
</section>
</section>
</section>
<section id="prompt-crafting" class="level2">
<h2 class="anchored" data-anchor-id="prompt-crafting">Prompt Crafting</h2>
<ul>
<li><strong>Prompt:</strong> Instructions or context provided to an LLM to guide its text generation process. Effective prompt crafting is crucial for achieving desired outputs.</li>
</ul>
<section id="technique-1-few-shot-prompting" class="level3">
<h3 class="anchored" data-anchor-id="technique-1-few-shot-prompting">Technique #1: Few-Shot Prompting</h3>
<ul>
<li><strong>Concept:</strong> Providing the LLM with a few examples of the desired input-output pattern, enabling it to understand and generalize to new, similar tasks.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example: Translating English to Spanish">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Translating English to Spanish
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Examples to set the pattern:</strong></p>
<pre class="text"><code>&gt; How are you doing today?
&lt; ¿Cómo estás hoy?

&gt; My name is John.
&lt; Mi nombre es John.</code></pre></li>
<li><p><strong>The actual task:</strong></p>
<pre class="text"><code>&gt; Can I have fries with that?
&lt; ¿Puedo tener papas fritas con eso?</code></pre></li>
</ul>
</div>
</div></li>
</ul>
</section>
<section id="technique-2-chain-of-thought-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="technique-2-chain-of-thought-reasoning">Technique #2: Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Concept:</strong> Improving LLM’s reasoning abilities by prompting them to generate a step-by-step thought process leading to the solution, especially useful for tasks involving logic and reasoning.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example: Guiding the model to break down the problem into smaller, logical steps">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Guiding the model to break down the problem into smaller, logical steps
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code># Trainging Example
Q: Jim is twice as old as Steve. Jim is 12 years how old is Steve.
A: In equation form: 12=2*a where a is Steve's age. Dividing both sides by 2 we see that a=6. Steve is 6 years old.

# Test Question
Q: It takes one baker an hour to make a cake. How long does it take 3 bakers to make 3 cakes?

# Answer with Reasoning
A: The amount of time it takes to bake a cake is the same regardless of how many cakes are made and how many people work on them. Therefore the answer is still 1 hour.</code></pre>
</div>
</div></li>
</ul>
<section id="thinking-step-by-step" class="level4">
<h4 class="anchored" data-anchor-id="thinking-step-by-step">Thinking Step-by-Step</h4>
<ul>
<li><strong>Simplified Approach:</strong> A variation of chain-of-thought reasoning where instead of providing multiple examples, the prompt directly instructs the model to “think step-by-step.”
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code>Q: It takes one baker an hour to make a cake. How long does it take 3 bakers to make 3 cakes?

# Prime the model by starting it's answer with "Let's think step-by-step."
A: Let's think step-by-step. The amount of time it takes to bake a cake is the same regardless of how many cakes are made and how many people work on them. Therefore the answer is still 1 hour.</code></pre>
</div>
</div></li>
<li><strong>Advantages:</strong>
<ul>
<li>Reduces the need for crafting numerous examples.</li>
<li>Avoids potential bias from examples bleeding into the answer.</li>
<li>Improves prompt efficiency by using shorter instructions.</li>
</ul></li>
</ul>
</section>
</section>
<section id="technique-3-document-mimicry" class="level3">
<h3 class="anchored" data-anchor-id="technique-3-document-mimicry">Technique #3: Document Mimicry</h3>
<ul>
<li><strong>Concept:</strong> Leveraging the LLM’s knowledge of specific document structures and formats to guide its output towards a desired style and content.</li>
<li><strong>Example:</strong> Crafting a prompt in the format of a customer support transcript, using headings, roles (Customer, Support Assistant), and Markdown formatting to elicit a response mimicking a helpful support interaction.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;"># IT Support Assistant</span></span>
<span id="cb6-2">The following is a transcript between an award winning IT support rep and a customer.</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">## Customer:</span></span>
<span id="cb6-5">My cable is out! And I'm going to miss the Superbowl!</span>
<span id="cb6-6"></span>
<span id="cb6-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">## Support Assistant:</span></span>
<span id="cb6-8">Let's figure out how to diagnose your problem…</span></code></pre></div>
<ul>
<li>Document type: transcript</li>
<li>Tells a story to condition a particular response</li>
<li>Uses Markdown to establish structure</li>
</ul>
</div>
</div></li>
</ul>
</section>
</section>
<section id="llms-are-dumb-mechanical-humans" class="level2">
<h2 class="anchored" data-anchor-id="llms-are-dumb-mechanical-humans">LLMs are Dumb Mechanical Humans</h2>
<ul>
<li><strong>Use Familiar Language and Constructs:</strong> LLMs perform better with language and structures commonly found in their training data.</li>
<li><strong>Avoid Overloading with Context:</strong> While providing context is essential, too much information can distract the model and hinder its performance.</li>
<li><strong>Provide Necessary Information:</strong> LLMs are not psychic; they rely on the prompt for information not present in their training data.</li>
<li><strong>Ensure Prompt Clarity:</strong> If the prompt is confusing for a human, it will likely be confusing for the LLM as well.</li>
</ul>
</section>
<section id="building-llm-applications" class="level2">
<h2 class="anchored" data-anchor-id="building-llm-applications">Building LLM Applications</h2>
<ul>
<li><strong>LLMs as Transformation Layers:</strong> LLM applications act as intermediaries between the user’s problem domain and the LLM’s text-based domain.</li>
<li><strong>Process:</strong>
<ol type="1">
<li><strong>User Request:</strong> The user interacts with the application, providing a request or input.</li>
<li><strong>Transformation to LLM Space:</strong> The application converts the user’s request into a text-based prompt understandable by the LLM.</li>
<li><strong>LLM Processing:</strong> The LLM processes the prompt and generates a text output.</li>
<li><strong>Transformation to User Space:</strong> The application converts the LLM’s text output into a format actionable and understandable by the user.</li>
</ol></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-003/images/llm-application-diagram.png" class="img-fluid figure-img"></p>
<figcaption><a href="https://docs.google.com/presentation/d/1PXzENGNN5NFbEDJ59wbSp8fro6dPt4xHGNN6X0KU82A/edit#slide=id.g2c14fe843d2_1_120">Prompt Engineering by John Berryman - Slide 14</a></figcaption>
</figure>
</div>
</section>
<section id="creating-the-prompt" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-prompt">Creating the Prompt</h2>
<ul>
<li><strong>Prompt Creation for Completion Models:</strong>
<ul>
<li><strong>Context Collection:</strong> Gather relevant information from sources like the current document, open tabs, and relevant symbols.</li>
<li><strong>Context Ranking:</strong> Prioritize the collected context based on its importance and relevance to the task.</li>
<li><strong>Context Trimming:</strong> Condense or eliminate less crucial context to fit within the LLM’s input limits.</li>
<li><strong>Document Assembly:</strong> Structure the prompt in a clear and organized manner, mimicking relevant document formats if applicable.</li>
</ul></li>
</ul>
<section id="copilot-code-completion" class="level3">
<h3 class="anchored" data-anchor-id="copilot-code-completion">Copilot Code Completion</h3>
<ul>
<li><strong>Context Collection:</strong>
<ul>
<li>Current document, open tabs, symbols used, file path.</li>
</ul></li>
<li><strong>Context Ranking:</strong>
<ul>
<li>File path (most important)</li>
<li>Current document</li>
<li>Neighboring tabs</li>
<li>Symbols (least important)</li>
</ul></li>
<li><strong>Context Trimming:</strong> Prioritizes keeping the file path, current document, and relevant snippets from open tabs.</li>
<li><strong>Document Assembly:</strong> Structures the prompt with file path at the top, followed by snippets from open tabs, and finally, the current document up to the cursor position.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="annotated-cell-7" style="background: #f1f3f5;"><pre class="sourceCode go code-annotation-code code-with-copy code-annotated"><code class="sourceCode go"><a class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-7-1" class="code-annotation-target"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// pkg/skills/search.go</span></span>
<span id="annotated-cell-7-2"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-7-3" class="code-annotation-target"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// &lt;consider this snippet from ../skill.go&gt;</span></span>
<span id="annotated-cell-7-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// type Skill interface {</span></span>
<span id="annotated-cell-7-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//    Execute(data []byte) (refs, error)</span></span>
<span id="annotated-cell-7-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// }</span></span>
<span id="annotated-cell-7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// &lt;/end snippet&gt;</span></span>
<span id="annotated-cell-7-8"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-7-9" class="code-annotation-target"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">package</span> searchskill</span>
<span id="annotated-cell-7-10"></span>
<span id="annotated-cell-7-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span></span>
<span id="annotated-cell-7-12">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"context"</span></span>
<span id="annotated-cell-7-13">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"encoding/json"</span></span>
<span id="annotated-cell-7-14">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fmt"</span></span>
<span id="annotated-cell-7-15">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"strings"</span></span>
<span id="annotated-cell-7-16">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time"</span></span>
<span id="annotated-cell-7-17"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span></span>
<span id="annotated-cell-7-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">type</span> Skill <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">struct</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-7-19" class="code-annotation-target">  █</span>
<span id="annotated-cell-7-20"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="annotated-cell-7-21"></span>
<span id="annotated-cell-7-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">type</span> params <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">struct</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-7" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="1,2" data-code-annotation="1">file path</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="3,4,5,6,7" data-code-annotation="2">snippet from open tab</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="9,11,12,13,14,15,16,17,18,20,22" data-code-annotation="3">current document</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="19" data-code-annotation="4">cursor</span>
</dd>
</dl>
</div>
</div></li>
</ul>
</section>
</section>
<section id="the-introduction-of-chat" class="level2">
<h2 class="anchored" data-anchor-id="the-introduction-of-chat">The Introduction of Chat</h2>
<ul>
<li><strong>Shift Towards Conversational Interfaces:</strong> Chat interfaces have become a popular paradigm for LLM applications.
<ul>
<li><strong>Blog Post:</strong> <a href="https://openai.com/index/chatgpt/">Introducing ChatGPT</a></li>
</ul></li>
<li><strong>ChatML:</strong> A specialized syntax used to represent chat conversations, with roles like “user” and “assistant” and special tokens to delineate messages.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="API">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
API
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb7-1"><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">messages</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">=</span> </span>
<span id="cb7-2"><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span></span>
<span id="cb7-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You are an award winning support staff representative that helps customers."</span></span>
<span id="cb7-5"> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-6"></span>
<span id="cb7-7"> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-8">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"content"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"My cable is out! And I'm going to miss the Superbowl!"</span></span>
<span id="cb7-9"> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb7-10"><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Document">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Document
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code>&lt;|im_start|&gt; system
You are an award winning IT support rep. Help the user with their request.&lt;|im_stop|&gt;

&lt;|im_start|&gt; user
My cable is out! And I'm going to miss the Superbowl!&lt;|im_stop|&gt;

&lt;|im_start|&gt; assistant
Let's figure out how to diagnose your problem…</code></pre>
</div>
</div></li>
<li><strong>Benefits of Chat-Based Interfaces:</strong>
<ul>
<li><strong>Natural Interaction:</strong> Mimics human conversation, providing a more intuitive user experience.</li>
<li><strong>System Messages:</strong> Allow developers to control the assistant’s behavior and personality.</li>
<li><strong>Enhanced Safety:</strong> Chat-based models are often fine-tuned to avoid generating harmful or inappropriate content.</li>
<li><strong>Reduced Prompt Injection Risk:</strong> Special tokens in ChatML make it difficult for users to manipulate the assistant’s behavior through malicious prompts.</li>
</ul></li>
</ul>
</section>
<section id="the-introduction-of-tools" class="level2">
<h2 class="anchored" data-anchor-id="the-introduction-of-tools">The Introduction of Tools</h2>
<ul>
<li><p><strong>Extending LLM Capabilities:</strong> Tools enable LLMs to interact with external systems and data, expanding their functionality beyond text generation.</p>
<ul>
<li><strong>Blog Post:</strong> <a href="https://openai.com/index/function-calling-and-other-api-updates/">Function calling and other API updates</a></li>
</ul></li>
<li><p><strong>Function Calling:</strong> Allows developers to define functions that the LLM can call to access external APIs or perform specific actions.</p>
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example: Get Weather Function">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Get Weather Function
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb9-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"function"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"function"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-4">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"get_weather"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-5">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"description"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Get the weather"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-6">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"parameters"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-7">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"object"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-8">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"properties"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-9">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"location"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-10">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-11">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"description"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The city and state"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-12">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb9-13">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"unit"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb9-14">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-15">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"description"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"degrees Fahrenheit or Celsius"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-16">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"enum"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"celsius"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fahrenheit"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span>
<span id="cb9-17">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb9-18">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb9-19">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"required"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"location"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb9-20">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb9-21">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb9-22"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Input:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Input:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb10-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb10-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb10-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"content"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What's the weather like in Miami?"</span></span>
<span id="cb10-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Function Call:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Function Call:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb11-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"assistant"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> </span>
<span id="cb11-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"function"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-4">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"get_weather"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb11-5">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"arguments"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"location"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Miami, FL"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'</span> </span>
<span id="cb11-6">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Real API Request:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Real API Request:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">curl</span> http://weathernow.com/miami/FL<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">?</span>deg=f</span>
<span id="cb12-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Response</span></span>
<span id="cb12-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"temp"</span><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">:</span> 78}</span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Function Response:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Function Response:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb13-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb13-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tool"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb13-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"get_weather"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb13-4">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"content"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"78ºF"</span></span>
<span id="cb13-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Assistant Response:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Assistant Response:
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb14-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"role"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"assistant"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> </span>
<span id="cb14-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"content"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"It's a balmy 78ºF"</span></span>
<span id="cb14-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div></li>
</ul></li>
<li><p><strong>Benefits of Tool Usage:</strong></p>
<ul>
<li><strong>Real-World Interaction:</strong> LLMs can now access and manipulate information in the real world through APIs.</li>
<li><strong>Flexibility in Response:</strong> Models can choose to respond to user requests by either calling functions or providing text-based answers.</li>
<li><strong>Potential for Parallel Processing:</strong> LLMs are being developed to execute multiple function calls concurrently, improving efficiency.</li>
</ul></li>
</ul>
</section>
<section id="building-llm-applications---continued" class="level2">
<h2 class="anchored" data-anchor-id="building-llm-applications---continued">Building LLM Applications - Continued</h2>
<ul>
<li><strong>Enhanced Application Architecture:</strong> With the introduction of chat and tool calling, the architecture of LLM applications becomes more sophisticated.</li>
<li><strong>Bag of Tools Agent:</strong>
<ul>
<li><strong>Prompt Crafting:</strong> Incorporates previous messages, context, tool definitions, and the user’s current request.</li>
<li><strong>Bifurcated Processing:</strong> The LLM can either call a function based on the prompt or generate a text response directly.</li>
<li><strong>Iterative Interaction:</strong> The application handles function calls, integrates results back into the prompt, and facilitates ongoing conversation.</li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example: Temperature Control">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Temperature Control
</div>
</div>
<div class="callout-body-container callout-body">
<pre class="text"><code>user: make it 2 degrees warmer in here

assistant: getTemp()
function: 70ºF
assistant: setTemp(72)
function: success
assistant: Done!

user: actually… put it back

assistant: setTemp(70)
function: success
assistant: Done again, you fickle pickle!</code></pre>
</div>
</div></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-003/images/llm-application-diagram-bag-of-tools.png" class="img-fluid figure-img"></p>
<figcaption><a href="https://docs.google.com/presentation/d/1PXzENGNN5NFbEDJ59wbSp8fro6dPt4xHGNN6X0KU82A/edit#slide=id.g2c247832288_0_62">Prompt Engineering by John Berryman - Slide 20</a></figcaption>
</figure>
</div>
</section>
<section id="creating-the-prompt-copilot-chat" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-prompt-copilot-chat">Creating the Prompt: Copilot Chat</h2>
<ul>
<li><strong>Context Collection:</strong>
<ul>
<li>Open files, highlighted code snippets, clipboard contents, relevant GitHub issues, previous messages in the conversation.</li>
</ul></li>
<li><strong>Context Ranking:</strong>
<ul>
<li>System message (essential for safety and behavior control)</li>
<li>Function definitions (if applicable)</li>
<li>User’s most recent message</li>
<li>Function call history and evaluations</li>
<li>References associated with messages</li>
<li>Historic messages (least important)</li>
</ul></li>
<li><strong>Context Trimming:</strong> Prioritizes keeping essential elements and trimming less crucial information like historic messages or function definitions if space is limited.</li>
<li><strong>Fallback Mechanisms:</strong> If the prompt becomes too large, the application should have strategies to handle the situation gracefully, such as prioritizing essential elements or informing the user about limitations.</li>
</ul>
</section>
<section id="tips-for-defining-tools" class="level2">
<h2 class="anchored" data-anchor-id="tips-for-defining-tools">Tips for Defining Tools</h2>
<ul>
<li><strong>Quantity:</strong>
<ul>
<li>Don’t have “too many” tools</li>
<li>Look for evidence of collisions</li>
</ul></li>
<li><strong>Names:</strong>
<ul>
<li>Use simple and clear names</li>
<li>Consider using typeScript format</li>
</ul></li>
<li><strong>Arguments:</strong>
<ul>
<li>Keep arguments simple and few
<ul>
<li>Don’t copy/paste your API</li>
</ul></li>
<li>Nest arguments don’t retain descriptions</li>
<li>Can use enum and default, but not minimum, maximum</li>
</ul></li>
<li><strong>Descriptions:</strong>
<ul>
<li>Keep them short and consider what the model knows
<ul>
<li>Probably understands public documentation.</li>
<li>Doesn’t know about internal company acronyms.</li>
</ul></li>
</ul></li>
<li><strong>Output:</strong> Don’t include extra “just-in-case” content</li>
<li><strong>Errors:</strong> when reasonable, send errors to model (validation errors)</li>
</ul>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="copilot-and-code-analysis" class="level3">
<h3 class="anchored" data-anchor-id="copilot-and-code-analysis">Copilot and Code Analysis</h3>
<ul>
<li><strong>Question:</strong> Can Copilot analyze codebases beyond open tabs to provide more context-aware suggestions?</li>
<li><strong>Answer:</strong> While not currently available, Copilot’s code analysis capabilities are under active development and expected to improve.</li>
<li><strong>Related Ideas:</strong> <a href="https://sourcegraph.com/">Sourcegraph</a> was mentioned as a company with interesting code analysis tools.</li>
</ul>
</section>
<section id="few-shot-prompting" class="level3">
<h3 class="anchored" data-anchor-id="few-shot-prompting">Few-Shot Prompting</h3>
<ul>
<li><strong>Question:</strong> How many examples are ideal for few-shot prompting, and where should they be placed?</li>
<li><strong>Answer:</strong> There’s no single answer, as it depends on the task and model. Experimentation is key.</li>
<li><strong>Best Practices:</strong>
<ul>
<li><strong>Log Probabilities:</strong> Analyze the log probabilities of predicted tokens to gauge if the model is grasping the pattern from the examples. High and leveling off probabilities suggest sufficient examples.</li>
<li><strong>Placement:</strong> For completion models, examples go directly in the prompt. For chat assistants, consider the message flow and potentially use fake user messages to position examples effectively.</li>
</ul></li>
</ul>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<ul>
<li><strong>Question:</strong> What hyperparameters should be adjusted when iterating on prompts, and how do they impact results?</li>
<li><strong>Answer:</strong> Temperature and the number of completions are key parameters to experiment with.</li>
<li><strong>Parameter Explanations:</strong>
<ul>
<li><strong>Temperature:</strong> Controls the randomness of the model’s output.
<ul>
<li>0 = deterministic, less creative</li>
<li>0.7 = a good balance for creativity (used in Copilot Chat)</li>
<li>1 = follows the natural probability distribution</li>
<li>Higher values increase randomness, potentially leading to gibberish.</li>
</ul></li>
<li><strong>Number of Completions (n):</strong> Requesting multiple completions (e.g., n=100) can be useful for evaluation or generating a wider range of outputs. Set a reasonably high temperature to avoid repetitive results.</li>
</ul></li>
</ul>
</section>
<section id="structuring-llm-outputs" class="level3">
<h3 class="anchored" data-anchor-id="structuring-llm-outputs">Structuring LLM Outputs</h3>
<ul>
<li><strong>Question:</strong> How can you guide an LLM to summarize information into a structured format like JSON?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Function Calling:</strong> Define functions within the prompt that specify the desired output structure (e.g., a function to extract restaurant details). LLMs are trained to understand and utilize JSON-like structures within function definitions.</li>
<li><strong>Simplified APIs:</strong> Avoid overly complex nested structures in function definitions. Break down tasks into smaller, more manageable steps if needed.</li>
</ul></li>
</ul>
</section>
<section id="challenges-with-complex-function-arguments" class="level3">
<h3 class="anchored" data-anchor-id="challenges-with-complex-function-arguments">Challenges with Complex Function Arguments</h3>
<ul>
<li><strong>Observation:</strong> Passing highly nested data structures as function arguments can be difficult for both humans and LLMs to interpret.</li>
<li><strong>Recommendations:</strong>
<ul>
<li><strong>Simplicity:</strong> Strive for clear and concise function arguments.</li>
<li><strong>Evaluation:</strong> Thoroughly test and evaluate how well the LLM handles complex structures.</li>
<li><strong>Iterative Refinement:</strong> Consider simplifying APIs or data structures if the LLM struggles with complexity.</li>
</ul></li>
</ul>
</section>
<section id="understanding-openais-function-calling-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="understanding-openais-function-calling-mechanism">Understanding OpenAI’s Function Calling Mechanism</h3>
<ul>
<li><strong>Question:</strong> How does OpenAI handle function calling internally?</li>
<li><strong>Answer:</strong> OpenAI transforms function definitions into a TypeScript-like format internally, adding comments for descriptions and argument details. However, nested structures may lose some type information during this process.
<ul>
<li><strong>Blog Post:</strong> <a href="http://blog.jnbrymn.com/2024/01/30/the-marvel-of-GPT-generality.html">Tool Invocation – Demonstrating the Marvel of GPT’s Flexibility</a></li>
</ul></li>
<li><strong>Key Takeaway:</strong> While LLMs can handle some complexity, being mindful of the underlying representation can help in designing more effective function calls.</li>
</ul>
</section>
<section id="improving-code-generation" class="level3">
<h3 class="anchored" data-anchor-id="improving-code-generation">Improving Code Generation</h3>
<ul>
<li><strong>Question:</strong> How to improve the quality of code generated by LLMs, especially in tools like Copilot?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Clear Comments:</strong> Provide explicit instructions within code comments to guide the model’s completions (e.g., describe the intended logic or syntax).</li>
<li><strong>Code Style:</strong> LLMs tend to mimic the style of the provided code. Writing clean and well-structured code can lead to better completions.</li>
</ul></li>
</ul>
</section>
<section id="prompt-engineering-tools" class="level3">
<h3 class="anchored" data-anchor-id="prompt-engineering-tools">Prompt Engineering Tools</h3>
<ul>
<li><strong>Question:</strong> What are your thoughts on tools like DSPy for automated prompting?
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/stanfordnlp/dspy">dspy</a></li>
</ul></li>
<li><strong>Answer:</strong>
<ul>
<li><strong>Value of Direct Interaction:</strong> Starting with direct interaction with LLMs (without intermediary tools) is crucial for building intuition and understanding.</li>
<li><strong>Potential Benefits of Tools:</strong> Tools like DSPy can automate tasks like finding optimal few-shot examples, potentially saving time and effort.</li>
<li><strong>Trade-offs:</strong> Abstraction can sometimes obscure the underlying mechanisms and limit fine-grained control.</li>
</ul></li>
</ul>
</section>
<section id="advanced-prompting-techniques" class="level3">
<h3 class="anchored" data-anchor-id="advanced-prompting-techniques">Advanced Prompting Techniques</h3>
<ul>
<li><strong>Question:</strong> Beyond chain-of-thought prompting, what other techniques are worth exploring?</li>
<li><strong>Answer:</strong>
<ul>
<li><strong>ReAct (Reason + Act):</strong> Involves defining a set of “fake” functions within the prompt, allowing the model to reason about which actions to take to solve a problem.
<ul>
<li><strong>Blog Post:</strong> <a href="https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
</ul></li>
<li><strong>Reflexion:</strong> Focuses on evaluating and iteratively improving the model’s output. For example, running generated code through tests and feeding error messages back into the prompt for correction.
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2303.11366">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
</ul></li>
</ul></li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-003/</guid>
  <pubDate>Sun, 30 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 2: LLM Eval For Text2SQL</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Importance of LLM Evaluation</li>
<li>Building Effective Evaluations</li>
<li>LLM Eval For Text2SQL Notebook</li>
<li>Q&amp;A Session</li>
</ul>
<section id="importance-of-llm-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="importance-of-llm-evaluation">Importance of LLM Evaluation</h2>
<ul>
<li><strong>Goals of Evaluation:</strong>
<ul>
<li>Determine if the system effectively solves the problem.</li>
<li>Quickly identify improvements or regressions caused by changes.</li>
<li>Analyze good and bad examples to understand model behavior.</li>
<li>Identify and address regressions, avoiding a “whack-a-mole” scenario.</li>
<li>Systematically improve the overall system quality.</li>
</ul></li>
</ul>
</section>
<section id="building-effective-evaluations" class="level2">
<h2 class="anchored" data-anchor-id="building-effective-evaluations">Building Effective Evaluations</h2>
<ul>
<li><p><strong>Three Core Components:</strong></p>
<ul>
<li><strong>Data:</strong>
<ul>
<li>Start by hardcoding data representing various scenarios.</li>
<li>Advanced methods like data generation and log sourcing can be incorporated later.</li>
</ul></li>
<li><strong>Task Function:</strong>
<ul>
<li>A function transforming input into output.</li>
<li>Can be simple (single LLM call) or complex (multi-agent system).</li>
<li>Example: A function converting natural language questions to SQL queries.</li>
</ul></li>
<li><strong>Scoring Functions:</strong>
<ul>
<li>Evaluate the quality of the output.</li>
<li>Options include:
<ul>
<li>Handwritten heuristic functions.</li>
<li>LLM-based comparisons between output and expected values.</li>
<li>Human evaluation and feedback.</li>
</ul></li>
</ul></li>
</ul></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Simple Example:">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple Example:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> braintrust <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Eval</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> autoevals <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Factuality</span>
<span id="cb1-3"></span>
<span id="cb1-4">Eval(</span>
<span id="cb1-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Say Hi Bot"</span>, <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your project name</span></span>
<span id="cb1-6">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span>: [</span>
<span id="cb1-7">        {</span>
<span id="cb1-8">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"David"</span>,</span>
<span id="cb1-9">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expected"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hi David"</span>,</span>
<span id="cb1-10">        },</span>
<span id="cb1-11">    ], <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your eval dataset</span></span>
<span id="cb1-12">    task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hi "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your LLM call</span></span>
<span id="cb1-13">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Factuality],</span>
<span id="cb1-14">)</span></code></pre></div>
</div>
</div>
</div></li>
<li><p><strong>Iterative Improvement:</strong></p>
<ul>
<li>If an evaluation reveals issues, focus on improving one of the three components:
<ul>
<li><strong>Data:</strong> Handwrite cases, generate cases, or obtain data from users.</li>
<li><strong>Task Function:</strong> Optimize prompts or refine the workflow of LLM calls.</li>
<li><strong>Scoring Functions:</strong> Improve heuristics, leverage LLM-based scoring, or integrate human evaluation.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="llm-eval-for-text2sql-notebook" class="level2">
<h2 class="anchored" data-anchor-id="llm-eval-for-text2sql-notebook">LLM Eval For Text2SQL Notebook</h2>
<div class="callout callout-style-default callout-tip callout-titled" title="Resources:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Resources:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Notebook:</strong> <a href="https://github.com/braintrustdata/braintrust-cookbook/blob/main/examples/Text2SQL-Data/Text2SQL-Data.ipynb">LLM Eval For Text2SQL</a></li>
<li><strong>Dataset:</strong> <a href="https://huggingface.co/datasets/suzyanil/nba-data">suzyanil/nba-data</a></li>
<li><strong>Tools:</strong>
<ul>
<li><strong><a href="https://duckdb.org/">DuckDB</a>:</strong> Expressive SQL tool for running queries within the notebook.</li>
<li><strong><a href="https://huggingface.co/docs/datasets/en/index">Hugging Face Datasets</a>:</strong> Provides easy access to the NBA dataset.</li>
<li><strong><a href="https://platform.openai.com/docs/overview">OpenAI API</a>:</strong> Used for accessing GPT-4 and GPT-4o models.</li>
<li><strong><a href="https://www.braintrustdata.com/docs/welcome">BrainTrust</a>:</strong> LLM evaluation platform for managing experiments, data, and results.</li>
</ul></li>
</ul>
</div>
</div>
<section id="downloading-the-data" class="level3">
<h3 class="anchored" data-anchor-id="downloading-the-data">Downloading the data</h3>
<ul>
<li><strong>Data:</strong> NBA dataset from Hugging Face (2014-2018 seasons, one row per game).</li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> duckdb</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb2-3"></span>
<span id="cb2-4">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"suzyanil/nba-data"</span>)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>]</span>
<span id="cb2-5"></span>
<span id="cb2-6">conn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>(database<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">":memory:"</span>, read_only<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-7">conn.register(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nba"</span>, data.to_pandas())</span>
<span id="cb2-8"></span>
<span id="cb2-9">conn.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SELECT * FROM nba LIMIT 5"</span>).to_df()</span></code></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
Unnamed: 0
</th>
<th>
Team
</th>
<th>
Game
</th>
<th>
Date
</th>
<th>
Home
</th>
<th>
Opponent
</th>
<th>
WINorLOSS
</th>
<th>
TeamPoints
</th>
<th>
OpponentPoints
</th>
<th>
FieldGoals
</th>
<th>
FieldGoalsAttempted
</th>
<th>
FieldGoals.
</th>
<th>
X3PointShots
</th>
<th>
X3PointShotsAttempted
</th>
<th>
X3PointShots.
</th>
<th>
FreeThrows
</th>
<th>
FreeThrowsAttempted
</th>
<th>
FreeThrows.
</th>
<th>
OffRebounds
</th>
<th>
TotalRebounds
</th>
<th>
Assists
</th>
<th>
Steals
</th>
<th>
Blocks
</th>
<th>
Turnovers
</th>
<th>
TotalFouls
</th>
<th>
Opp.FieldGoals
</th>
<th>
Opp.FieldGoalsAttempted
</th>
<th>
Opp.FieldGoals.
</th>
<th>
Opp.3PointShots
</th>
<th>
Opp.3PointShotsAttempted
</th>
<th>
Opp.3PointShots.
</th>
<th>
Opp.FreeThrows
</th>
<th>
Opp.FreeThrowsAttempted
</th>
<th>
Opp.FreeThrows.
</th>
<th>
Opp.OffRebounds
</th>
<th>
Opp.TotalRebounds
</th>
<th>
Opp.Assists
</th>
<th>
Opp.Steals
</th>
<th>
Opp.Blocks
</th>
<th>
Opp.Turnovers
</th>
<th>
Opp.TotalFouls
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
ATL
</td>
<td>
1
</td>
<td>
10/29/14
</td>
<td>
Away
</td>
<td>
TOR
</td>
<td>
L
</td>
<td>
102
</td>
<td>
109
</td>
<td>
40
</td>
<td>
80
</td>
<td>
0.500
</td>
<td>
13
</td>
<td>
22
</td>
<td>
0.591
</td>
<td>
9
</td>
<td>
17
</td>
<td>
0.529
</td>
<td>
10
</td>
<td>
42
</td>
<td>
26
</td>
<td>
6
</td>
<td>
8
</td>
<td>
17
</td>
<td>
24
</td>
<td>
37
</td>
<td>
90
</td>
<td>
0.411
</td>
<td>
8
</td>
<td>
26
</td>
<td>
0.308
</td>
<td>
27
</td>
<td>
33
</td>
<td>
0.818
</td>
<td>
16
</td>
<td>
48
</td>
<td>
26
</td>
<td>
13
</td>
<td>
9
</td>
<td>
9
</td>
<td>
22
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
ATL
</td>
<td>
2
</td>
<td>
11/1/14
</td>
<td>
Home
</td>
<td>
IND
</td>
<td>
W
</td>
<td>
102
</td>
<td>
92
</td>
<td>
35
</td>
<td>
69
</td>
<td>
0.507
</td>
<td>
7
</td>
<td>
20
</td>
<td>
0.350
</td>
<td>
25
</td>
<td>
33
</td>
<td>
0.758
</td>
<td>
3
</td>
<td>
37
</td>
<td>
26
</td>
<td>
10
</td>
<td>
6
</td>
<td>
12
</td>
<td>
20
</td>
<td>
31
</td>
<td>
81
</td>
<td>
0.383
</td>
<td>
12
</td>
<td>
32
</td>
<td>
0.375
</td>
<td>
18
</td>
<td>
21
</td>
<td>
0.857
</td>
<td>
11
</td>
<td>
44
</td>
<td>
25
</td>
<td>
5
</td>
<td>
5
</td>
<td>
18
</td>
<td>
26
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
ATL
</td>
<td>
3
</td>
<td>
11/5/14
</td>
<td>
Away
</td>
<td>
SAS
</td>
<td>
L
</td>
<td>
92
</td>
<td>
94
</td>
<td>
38
</td>
<td>
92
</td>
<td>
0.413
</td>
<td>
8
</td>
<td>
25
</td>
<td>
0.320
</td>
<td>
8
</td>
<td>
11
</td>
<td>
0.727
</td>
<td>
10
</td>
<td>
37
</td>
<td>
26
</td>
<td>
14
</td>
<td>
5
</td>
<td>
13
</td>
<td>
25
</td>
<td>
31
</td>
<td>
69
</td>
<td>
0.449
</td>
<td>
5
</td>
<td>
17
</td>
<td>
0.294
</td>
<td>
27
</td>
<td>
38
</td>
<td>
0.711
</td>
<td>
11
</td>
<td>
50
</td>
<td>
25
</td>
<td>
7
</td>
<td>
9
</td>
<td>
19
</td>
<td>
15
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
ATL
</td>
<td>
4
</td>
<td>
11/7/14
</td>
<td>
Away
</td>
<td>
CHO
</td>
<td>
L
</td>
<td>
119
</td>
<td>
122
</td>
<td>
43
</td>
<td>
93
</td>
<td>
0.462
</td>
<td>
13
</td>
<td>
33
</td>
<td>
0.394
</td>
<td>
20
</td>
<td>
26
</td>
<td>
0.769
</td>
<td>
7
</td>
<td>
38
</td>
<td>
28
</td>
<td>
8
</td>
<td>
3
</td>
<td>
19
</td>
<td>
33
</td>
<td>
48
</td>
<td>
97
</td>
<td>
0.495
</td>
<td>
6
</td>
<td>
21
</td>
<td>
0.286
</td>
<td>
20
</td>
<td>
27
</td>
<td>
0.741
</td>
<td>
11
</td>
<td>
51
</td>
<td>
31
</td>
<td>
6
</td>
<td>
7
</td>
<td>
19
</td>
<td>
30
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
ATL
</td>
<td>
5
</td>
<td>
11/8/14
</td>
<td>
Home
</td>
<td>
NYK
</td>
<td>
W
</td>
<td>
103
</td>
<td>
96
</td>
<td>
33
</td>
<td>
81
</td>
<td>
0.407
</td>
<td>
9
</td>
<td>
22
</td>
<td>
0.409
</td>
<td>
28
</td>
<td>
36
</td>
<td>
0.778
</td>
<td>
12
</td>
<td>
41
</td>
<td>
18
</td>
<td>
10
</td>
<td>
5
</td>
<td>
8
</td>
<td>
17
</td>
<td>
40
</td>
<td>
84
</td>
<td>
0.476
</td>
<td>
8
</td>
<td>
21
</td>
<td>
0.381
</td>
<td>
8
</td>
<td>
11
</td>
<td>
0.727
</td>
<td>
13
</td>
<td>
44
</td>
<td>
26
</td>
<td>
2
</td>
<td>
6
</td>
<td>
15
</td>
<td>
29
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="prototyping-text2sql" class="level3">
<h3 class="anchored" data-anchor-id="prototyping-text2sql">Prototyping Text2SQL</h3>
<ul>
<li><strong>Simple Approach:</strong> Directly use the OpenAI API client for interacting with the LLM.</li>
<li><strong>Prompt:</strong> A basic prompt instructs the model to write a SQL query based on the provided context.</li>
<li><strong>Initial Test:</strong> The prototype successfully generates a SQL query to find the team with the most wins, demonstrating basic functionality.</li>
</ul>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> textwrap <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dedent</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> braintrust</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> openai</span>
<span id="cb3-6"></span>
<span id="cb3-7">client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> braintrust.wrap_openai(</span>
<span id="cb3-8">    openai.AsyncClient(</span>
<span id="cb3-9">        api_key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"OPENAI_API_KEY"</span>],</span>
<span id="cb3-10">        base_url<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://braintrustproxy.com/v1"</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This is optional and allows us to cache responses</span></span>
<span id="cb3-11">    )</span>
<span id="cb3-12">)</span>
<span id="cb3-13"></span>
<span id="cb3-14">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> conn.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"DESCRIBE nba"</span>).to_df().to_dict(orient<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"records"</span>)</span>
<span id="cb3-15"></span>
<span id="cb3-16">TASK_MODEL <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4o"</span></span>
<span id="cb3-17"></span>
<span id="cb3-18"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@braintrust.traced</span></span>
<span id="cb3-19"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> generate_query(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>):</span>
<span id="cb3-20">    response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> client.chat.completions.create(</span>
<span id="cb3-21">        model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>TASK_MODEL,</span>
<span id="cb3-22">        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb3-23">        messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb3-24">            {</span>
<span id="cb3-25">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>,</span>
<span id="cb3-26">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: dedent(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-27"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        You are a SQL expert, and you are given a single table named nba with the following columns:</span></span>
<span id="cb3-28"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">", "</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join(column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"column_name"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">": "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"column_type"</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> columns)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-29"></span>
<span id="cb3-30"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        Write a SQL query corresponding to the user's request. Return just the query text, with no</span></span>
<span id="cb3-31"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        formatting (backticks, markdown, etc.).</span></span>
<span id="cb3-32"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span>),</span>
<span id="cb3-33">            },</span>
<span id="cb3-34">            {</span>
<span id="cb3-35">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>,</span>
<span id="cb3-36">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>,</span>
<span id="cb3-37">            },</span>
<span id="cb3-38">        ],</span>
<span id="cb3-39">    )</span>
<span id="cb3-40">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> response.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb3-41"></span>
<span id="cb3-42">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> generate_query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Who won the most games?"</span>)</span>
<span id="cb3-43"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(query)</span></code></pre></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">SELECT</span> Team, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">COUNT</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> Wins</span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FROM</span> nba</span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">WHERE</span> WINorLOSS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span></span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">GROUP</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> Team</span>
<span id="cb4-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">ORDER</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> Wins <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">DESC</span></span>
<span id="cb4-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">LIMIT</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>;</span></code></pre></div>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> execute_query(query):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> conn.query(query).fetchdf().to_dict(orient<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"records"</span>)</span>
<span id="cb5-3"></span>
<span id="cb5-4">execute_query(query)</span></code></pre></div>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb6-1"><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'Team'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'GSW'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'Wins'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">265</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span></code></pre></div>
</section>
<section id="initial-evals" class="level3">
<h3 class="anchored" data-anchor-id="initial-evals">Initial evals</h3>
<ul>
<li><strong>Goal:</strong> Evaluate the model’s ability to generate valid SQL queries, not necessarily correct answers.</li>
<li><strong>Dataset:</strong> Five manually crafted questions related to the NBA dataset.</li>
<li><strong>Task Function:</strong>
<ul>
<li>Takes a question as input.</li>
<li>Generates a SQL query using the LLM.</li>
<li>Executes the query using DuckDB.</li>
<li>Returns the query and result, handling potential errors.</li>
</ul></li>
<li><strong>Scoring Function:</strong>
<ul>
<li>Simple binary scoring: “good” if the query executes without errors, “bad” otherwise.</li>
</ul></li>
<li><strong>Initial Results:</strong>
<ul>
<li>Three out of five queries execute successfully.</li>
<li>BrainTrust UI helps visualize results and debug errors.</li>
<li>Correct queries and their results are saved as “golden data” for future reference.</li>
</ul></li>
</ul>
<section id="creating-an-initial-dataset" class="level4">
<h4 class="anchored" data-anchor-id="creating-an-initial-dataset">Creating an initial dataset</h4>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb7-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Which team won the most games?"</span>,</span>
<span id="cb7-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Which team won the most games in 2015?"</span>,</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Who led the league in 3 point shots?"</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Which team had the biggest difference in records across two consecutive years?"</span>,</span>
<span id="cb7-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is the average number of free throws per year?"</span>,</span>
<span id="cb7-7">]</span></code></pre></div>
</section>
<section id="task-function" class="level4">
<h4 class="anchored" data-anchor-id="task-function">Task function</h4>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@braintrust.traced</span></span>
<span id="cb8-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> text2sql(question):</span>
<span id="cb8-3">    query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> generate_query(question)</span>
<span id="cb8-4">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb8-5">    error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb8-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">try</span>:</span>
<span id="cb8-7">        results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> execute_query(query)</span>
<span id="cb8-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">except</span> duckdb.Error <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb8-9">        error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(e)</span>
<span id="cb8-10"></span>
<span id="cb8-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> {</span>
<span id="cb8-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>: query,</span>
<span id="cb8-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>: results,</span>
<span id="cb8-14">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"error"</span>: error,</span>
<span id="cb8-15">    }</span></code></pre></div>
</section>
<section id="scores" class="level4">
<h4 class="anchored" data-anchor-id="scores">Scores</h4>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> no_error(output):</span>
<span id="cb9-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"error"</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span></code></pre></div>
</section>
<section id="eval" class="level4">
<h4 class="anchored" data-anchor-id="eval">Eval</h4>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> braintrust <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Eval</span>
<span id="cb10-2"></span>
<span id="cb10-3">PROJECT_NAME <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LLM Eval for Text2SQL"</span></span>
<span id="cb10-4"></span>
<span id="cb10-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> Eval(</span>
<span id="cb10-6">    PROJECT_NAME,</span>
<span id="cb10-7">    experiment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial dataset"</span>,</span>
<span id="cb10-8">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: q} <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> questions],</span>
<span id="cb10-9">    task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text2sql,</span>
<span id="cb10-10">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[no_error],</span>
<span id="cb10-11">)</span></code></pre></div>
<pre class="text"><code>Experiment Initial dataset is running at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Initial%20dataset
LLM Eval for Text2SQL [experiment_name=Initial dataset] (data): 5it [00:00, 53362.65it/s]

LLM Eval for Text2SQL [experiment_name=Initial dataset] (tasks):   0%|          | 0/5 [00:00&lt;?, ?it/s]

=========================SUMMARY=========================
60.00% 'no_error' score

See results for Initial dataset at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Initial%20dataset

EvalResultWithSummary(summary="...", results=[...])</code></pre>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
id
</th>
<th>
_xact_id
</th>
<th>
input
</th>
<th>
output
</th>
<th>
expected
</th>
<th>
tags
</th>
<th>
scores
</th>
<th>
duration
</th>
<th>
estimated_cost
</th>
<th>
metadata
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
ad2710ff-4c62-486b-abf4-f7266f37e5d2
</td>
<td>
1000193294528916736
</td>
<td>
What is the average number of free throws per year?
</td>
<td>
{‘error’: ‘Binder Error: Could not choose a best candidate function for the function call “strftime(STRING_LITERAL, VARCHAR)”. In order to select one, please add explicit type casts. Candidate functions: strftime(TIMESTAMP, VARCHAR) -&gt; VARCHAR strftime(TIMESTAMP WITH TIME ZONE, VARCHAR) -&gt; VARCHAR strftime(DATE, VARCHAR) -&gt; VARCHAR’, ‘query’: ‘SELECT strftime(’%Y’, Date) AS Year, AVG(FreeThrows) AS AverageFreeThrows FROM nba GROUP BY Year;‘, ’results’: None}
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
{‘no_error’: 0}
</td>
<td>
1.728403
</td>
<td>
0.002320
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
1
</th>
<td>
5ba5d8af-82d6-4fba-a66e-9ec5ebaeb578
</td>
<td>
1000193294528916736
</td>
<td>
Which team had the biggest difference in records across two consecutive years?
</td>
<td>
{‘error’: ‘Binder Error: No function matches the given name and argument types ’year(VARCHAR)’. You might need to add explicit type casts. Candidate functions: year(DATE) -&gt; BIGINT year(TIMESTAMP) -&gt; BIGINT year(INTERVAL) -&gt; BIGINT year(TIMESTAMP WITH TIME ZONE) -&gt; BIGINT ‘, ’query’: ‘SELECT Team, ABS(SUM(CASE WHEN WINorLOSS = ’W’ THEN 1 ELSE 0 END) - LAG(SUM(CASE WHEN WINorLOSS = ‘W’ THEN 1 ELSE 0 END)) OVER (PARTITION BY Team ORDER BY Date)) AS WinDifference FROM nba GROUP BY Team, YEAR(Date) ORDER BY WinDifference DESC LIMIT 1;‘, ’results’: None}
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
{‘no_error’: 0}
</td>
<td>
1.881156
</td>
<td>
0.003005
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
2
</th>
<td>
99056cb3-a13d-4fd3-892f-b0e3fbc1dfb5
</td>
<td>
1000193294529047808
</td>
<td>
Who led the league in 3 point shots?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, SUM(X3PointShots) AS Total3PointShots FROM nba GROUP BY Team ORDER BY Total3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘Total3PointShots’: 4248}]}
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
{‘no_error’: 1}
</td>
<td>
3.824243
</td>
<td>
0.002285
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
3
</th>
<td>
f82fc67a-8594-41bd-9869-74cd7894076b
</td>
<td>
1000193294528916736
</td>
<td>
Which team won the most games in 2015?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ AND Date LIKE ‘2015%’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: []}
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
{‘no_error’: 1}
</td>
<td>
1.992921
</td>
<td>
0.002365
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
4
</th>
<td>
b9fea522-d53d-4b63-958b-12c9dac59a5a
</td>
<td>
1000193294529047808
</td>
<td>
Which team won the most games?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
{‘no_error’: 1}
</td>
<td>
4.303131
</td>
<td>
0.002240
</td>
<td>
{}
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="saving-good-data" class="level4">
<h4 class="anchored" data-anchor-id="saving-good-data">Saving Good Data</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/images/add-to-dataset.gif" class="img-fluid figure-img"></p>
<figcaption>add to dataset</figcaption>
</figure>
</div>
</section>
<section id="updating-the-eval" class="level4">
<h4 class="anchored" data-anchor-id="updating-the-eval">Updating the eval</h4>
<ul>
<li><strong>Leveraging Golden Data:</strong>
<ul>
<li>Load golden data from BrainTrust.</li>
<li>Use golden data to compare generated queries with expected answers.</li>
</ul></li>
<li><strong>Improved Prompt:</strong>
<ul>
<li>Include a sample row from the dataset in the prompt to provide context on data format.</li>
</ul></li>
<li><strong>Enhanced Scoring Functions:</strong>
<ul>
<li><strong><code>correct_result</code> Function:</strong> Compares the values returned by the generated query and the expected answer using <a href="https://www.braintrustdata.com/docs/reference/autoevals/python#jsondiff-objects">JSONDiff</a>.</li>
<li><a href="https://www.braintrustdata.com/docs/reference/autoevals/python#sql-objects"><code>SQL</code></a> score (LLM-based): Uses an LLM to assess the semantic similarity between the generated query and the reference query.</li>
</ul></li>
<li><strong>Updated Results (During Talk):</strong>
<ul>
<li>No regression on previously correct queries.
<ul>
<li>Had 1 improvement and 1 regression when testing locally</li>
</ul></li>
<li>Improvement in generating valid SQL queries.</li>
<li>Analysis of discrepancies between generated and expected answers reveals areas for improvement.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> braintrust <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> init_dataset</span>
<span id="cb12-2"></span>
<span id="cb12-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> load_data():</span>
<span id="cb12-4">    golden_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> init_dataset(PROJECT_NAME, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Golden data"</span>)</span>
<span id="cb12-5">    golden_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> d <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_data)</span>
<span id="cb12-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(golden_data) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: q} <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> questions <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_questions]</span>
<span id="cb12-7"></span>
<span id="cb12-8">pd.DataFrame(load_data())</span></code></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
id
</th>
<th>
_xact_id
</th>
<th>
created
</th>
<th>
project_id
</th>
<th>
dataset_id
</th>
<th>
input
</th>
<th>
expected
</th>
<th>
metadata
</th>
<th>
tags
</th>
<th>
span_id
</th>
<th>
root_span_id
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
09e520a6-bb32-4110-9b80-e127771a66bd
</td>
<td>
1000193294889891926
</td>
<td>
2024-06-30T00:02:42.914Z
</td>
<td>
05d79acf-0ae3-4953-bc2a-b4ccc803dcfa
</td>
<td>
20dcbf52-2109-40c4-a941-1f2f792d09e0
</td>
<td>
Who led the league in 3 point shots?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, SUM(X3PointShots) AS Total3PointShots FROM nba GROUP BY Team ORDER BY Total3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘Total3PointShots’: 4248}]}
</td>
<td>
{}
</td>
<td>
[]
</td>
<td>
09e520a6-bb32-4110-9b80-e127771a66bd
</td>
<td>
09e520a6-bb32-4110-9b80-e127771a66bd
</td>
</tr>
<tr>
<th>
1
</th>
<td>
aa079536-bebd-402a-9a98-64c9003f7a22
</td>
<td>
1000193294890547290
</td>
<td>
2024-06-30T00:02:53.720Z
</td>
<td>
05d79acf-0ae3-4953-bc2a-b4ccc803dcfa
</td>
<td>
20dcbf52-2109-40c4-a941-1f2f792d09e0
</td>
<td>
Which team won the most games?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
{}
</td>
<td>
[]
</td>
<td>
aa079536-bebd-402a-9a98-64c9003f7a22
</td>
<td>
aa079536-bebd-402a-9a98-64c9003f7a22
</td>
</tr>
<tr>
<th>
2
</th>
<td>
eca397d3-e2bb-46c3-8d84-3772506d0a11
</td>
<td>
1000193294931441802
</td>
<td>
2024-06-30T00:13:17.809Z
</td>
<td>
05d79acf-0ae3-4953-bc2a-b4ccc803dcfa
</td>
<td>
20dcbf52-2109-40c4-a941-1f2f792d09e0
</td>
<td>
Which team won the most games in 2015?
</td>
<td>
{‘error’: ’Parser Error: syntax error at or near “<code>"', 'query': '</code>sql SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ AND Date LIKE ‘%/15’ GROUP BY Team ORDER BY Wins DESC LIMIT 1; ```‘, ’results’: None}
</td>
<td>
{}
</td>
<td>
[]
</td>
<td>
eca397d3-e2bb-46c3-8d84-3772506d0a11
</td>
<td>
eca397d3-e2bb-46c3-8d84-3772506d0a11
</td>
</tr>
<tr>
<th>
3
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
Which team had the biggest difference in records across two consecutive years?
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
4
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
What is the average number of free throws per year?
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> conn.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SELECT * FROM nba LIMIT 1"</span>).to_df().to_dict(orient<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"records"</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@braintrust.traced</span></span>
<span id="cb13-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">async</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> generate_query(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>):</span>
<span id="cb13-5">    response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> client.chat.completions.create(</span>
<span id="cb13-6">        model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>TASK_MODEL,</span>
<span id="cb13-7">        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb13-8">        messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb13-9">            {</span>
<span id="cb13-10">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>,</span>
<span id="cb13-11">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: dedent(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb13-12"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        You are a SQL expert, and you are given a single table named nba with the following columns:</span></span>
<span id="cb13-13"></span>
<span id="cb13-14"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        Column | Type | Example</span></span>
<span id="cb13-15"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        -------|------|--------</span></span>
<span id="cb13-16"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_name'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_type'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>samples[column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_name'</span>]]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> columns)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb13-17"></span>
<span id="cb13-18"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        Write a DuckDB SQL query corresponding to the user's request. Return just the query text, with no</span></span>
<span id="cb13-19"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        formatting (backticks, markdown, etc.).</span></span>
<span id="cb13-20"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span>),</span>
<span id="cb13-21">            },</span>
<span id="cb13-22">            {</span>
<span id="cb13-23">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>,</span>
<span id="cb13-24">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>,</span>
<span id="cb13-25">            },</span>
<span id="cb13-26">        ],</span>
<span id="cb13-27">    )</span>
<span id="cb13-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> response.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content</span>
<span id="cb13-29"></span>
<span id="cb13-30"></span>
<span id="cb13-31"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> generate_query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Which team won the most games in 2015?"</span>))</span></code></pre></div>
<pre class="text"><code>```sql
SELECT Team, COUNT(*) AS Wins
FROM nba
WHERE WINorLOSS = 'W' AND Date LIKE '%/15'
GROUP BY Team
ORDER BY Wins DESC
LIMIT 1;
```</code></pre>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> autoevals <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> JSONDiff, Sql</span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> extract_values(results):</span>
<span id="cb15-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> [<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(result.values()) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> result <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> results]</span>
<span id="cb15-5"></span>
<span id="cb15-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> correct_result(output, expected):</span>
<span id="cb15-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> expected <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> expected.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> output.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb15-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb15-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> JSONDiff()(output<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>extract_values(output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>]), expected<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>extract_values(expected[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>])).score</span>
<span id="cb15-10"></span>
<span id="cb15-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> correct_sql(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output, expected):</span>
<span id="cb15-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> expected <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> expected.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> output.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb15-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb15-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Sql()(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>], expected<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>expected[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>]).score</span></code></pre></div>
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> Eval(</span>
<span id="cb16-2">    PROJECT_NAME,</span>
<span id="cb16-3">    experiment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With samples"</span>,</span>
<span id="cb16-4">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>load_data,</span>
<span id="cb16-5">    task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text2sql,</span>
<span id="cb16-6">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[no_error, correct_result, correct_sql],</span>
<span id="cb16-7">)</span></code></pre></div>
<pre class="text"><code>Experiment With samples is running at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/With%20samples
LLM Eval for Text2SQL [experiment_name=With samples] (data): 5it [00:00, 12905.55it/s]

LLM Eval for Text2SQL [experiment_name=With samples] (tasks):   0%|          | 0/5 [00:00&lt;?, ?it/s]

=========================SUMMARY=========================
With samples compared to Initial dataset:
60.00% (-) 'no_error'       score   (1 improvements, 1 regressions)
100.00% 'correct_result' score
100.00% 'correct_sql'    score

See results for With samples at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/With%20samples

EvalResultWithSummary(summary="...", results=[...])</code></pre>
</section>
<section id="results-1" class="level4">
<h4 class="anchored" data-anchor-id="results-1">Results</h4>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
id
</th>
<th>
_xact_id
</th>
<th>
input
</th>
<th>
output
</th>
<th>
expected
</th>
<th>
tags
</th>
<th>
scores
</th>
<th>
duration
</th>
<th>
estimated_cost
</th>
<th>
metadata
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
9d438233-8755-474e-90c9-39eaeab62639
</td>
<td>
1000193294893627520
</td>
<td>
Which team won the most games?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
[]
</td>
<td>
{‘correct_result’: 1, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
4.974092
</td>
<td>
0.003600
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1293e7ca-7800-4406-ba60-941adaa2f636
</td>
<td>
1000193294893627520
</td>
<td>
Who led the league in 3 point shots?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, SUM(X3PointShots) AS Total3PointShots FROM nba GROUP BY Team ORDER BY Total3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘Total3PointShots’: 4248}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, SUM(X3PointShots) AS Total3PointShots FROM nba GROUP BY Team ORDER BY Total3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘Total3PointShots’: 4248}]}
</td>
<td>
[]
</td>
<td>
{‘correct_result’: 1, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
5.561691
</td>
<td>
0.003559
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
2
</th>
<td>
a9954383-301c-4303-b867-2c45d841593e
</td>
<td>
1000193294893365376
</td>
<td>
What is the average number of free throws per year?
</td>
<td>
{‘error’: ‘Binder Error: Could not choose a best candidate function for the function call “strftime(STRING_LITERAL, VARCHAR)”. In order to select one, please add explicit type casts. Candidate functions: strftime(TIMESTAMP, VARCHAR) -&gt; VARCHAR strftime(TIMESTAMP WITH TIME ZONE, VARCHAR) -&gt; VARCHAR strftime(DATE, VARCHAR) -&gt; VARCHAR’, ‘query’: ‘SELECT strftime(’%Y’, Date) AS Year, AVG(FreeThrows) AS AvgFreeThrows FROM nba GROUP BY Year;‘, ’results’: None}
</td>
<td>
None
</td>
<td>
None
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: None, ‘no_error’: 0}
</td>
<td>
1.647409
</td>
<td>
0.002990
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
3
</th>
<td>
eca397d3-e2bb-46c3-8d84-3772506d0a11
</td>
<td>
1000193294893299840
</td>
<td>
Which team won the most games in 2015?
</td>
<td>
{‘error’: ’Parser Error: syntax error at or near “<code>"', 'query': '</code>sql SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ AND Date LIKE ‘%/15’ GROUP BY Team ORDER BY Wins DESC LIMIT 1; ```‘, ’results’: None}
</td>
<td>
None
</td>
<td>
None
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: None, ‘no_error’: 0}
</td>
<td>
0.235527
</td>
<td>
0.003215
</td>
<td>
{}
</td>
</tr>
<tr>
<th>
4
</th>
<td>
eab4f712-2f79-4f0c-8e56-c037a83579ea
</td>
<td>
1000193294893430912
</td>
<td>
Which team had the biggest difference in records across two consecutive years?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, ABS(SUM(CASE WHEN Date LIKE ’10/%’ THEN (CASE WHEN WINorLOSS = ‘W’ THEN 1 ELSE -1 END) ELSE 0 END) - SUM(CASE WHEN Date LIKE ‘11/%’ THEN (CASE WHEN WINorLOSS = ‘W’ THEN 1 ELSE -1 END) ELSE 0 END)) AS RecordDifference FROM nba GROUP BY Team ORDER BY RecordDifference DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘RecordDifference’: 41}]}
</td>
<td>
None
</td>
<td>
None
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: None, ‘no_error’: 1}
</td>
<td>
2.400165
</td>
<td>
0.004035
</td>
<td>
{}
</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/images/eval-2.png" class="img-fluid figure-img"></p>
<figcaption>updated eval</figcaption>
</figure>
</div>
</section>
</section>
<section id="generating-more-data" class="level3">
<h3 class="anchored" data-anchor-id="generating-more-data">Generating more data</h3>
<ul>
<li><strong>Goal:</strong> Expand the evaluation dataset beyond manually crafted questions.</li>
<li><strong>Method:</strong> Use the LLM itself to generate SQL queries and corresponding questions.</li>
<li><strong>Process:</strong>
<ul>
<li>Use function calling to structure the LLM’s output (list of questions with SQL and natural language versions).</li>
<li>Provide the database schema as context.</li>
<li>Execute generated queries to obtain expected results.</li>
<li>Add successfully executed queries and their data to the evaluation dataset.</li>
</ul></li>
<li><strong>Benefits:</strong>
<ul>
<li>Automates data generation, increasing coverage.</li>
<li>Creates a diverse set of evaluation examples.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb18-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Question(BaseModel):</span>
<span id="cb18-5">    sql: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb18-6">    question: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb18-7"></span>
<span id="cb18-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Questions(BaseModel):</span>
<span id="cb18-9">    questions: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[Question]</span>
<span id="cb18-10"></span>
<span id="cb18-11">logger <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> braintrust.init_logger(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"question generator"</span>)</span>
<span id="cb18-12"></span>
<span id="cb18-13">response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> client.chat.completions.create(</span>
<span id="cb18-14">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4o"</span>,</span>
<span id="cb18-15">    temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb18-16">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb18-17">        {</span>
<span id="cb18-18">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>,</span>
<span id="cb18-19">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: dedent(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb18-20"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        You are a SQL expert, and you are given a single table named nba with the following columns:</span></span>
<span id="cb18-21"></span>
<span id="cb18-22"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        Column | Type | Example</span></span>
<span id="cb18-23"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        -------|------|--------</span></span>
<span id="cb18-24"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_name'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_type'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>samples[column[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'column_name'</span>]]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> column <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> columns)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb18-25"></span>
<span id="cb18-26"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        Generate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the</span></span>
<span id="cb18-27"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        question that the query answers."""</span>),</span>
<span id="cb18-28">        }</span>
<span id="cb18-29">    ],</span>
<span id="cb18-30">    tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb18-31">        {</span>
<span id="cb18-32">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"function"</span>,</span>
<span id="cb18-33">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"function"</span>: {</span>
<span id="cb18-34">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"generate_questions"</span>,</span>
<span id="cb18-35">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"description"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Generate SQL queries that would be interesting to ask about this table."</span>,</span>
<span id="cb18-36">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"parameters"</span>: Questions.model_json_schema(),</span>
<span id="cb18-37">            },</span>
<span id="cb18-38">        }</span>
<span id="cb18-39">    ],</span>
<span id="cb18-40">    tool_choice<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"function"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"function"</span>: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"generate_questions"</span>}},</span>
<span id="cb18-41">)</span>
<span id="cb18-42"></span>
<span id="cb18-43">generated_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.loads(response.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.tool_calls[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].function.arguments)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"questions"</span>]</span>
<span id="cb18-44">pd.DataFrame(generated_questions)</span></code></pre></div>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
sql
</th>
<th>
question
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC;
</td>
<td>
Which team has the most wins?
</td>
</tr>
<tr>
<th>
1
</th>
<td>
SELECT Team, AVG(TeamPoints) as AvgPoints FROM nba GROUP BY Team ORDER BY AvgPoints DESC;
</td>
<td>
Which team has the highest average points per game?
</td>
</tr>
<tr>
<th>
2
</th>
<td>
SELECT Team, AVG(FieldGoals.) as AvgFieldGoalPercentage FROM nba GROUP BY Team ORDER BY AvgFieldGoalPercentage DESC;
</td>
<td>
Which team has the highest average field goal percentage?
</td>
</tr>
<tr>
<th>
3
</th>
<td>
SELECT Team, AVG(X3PointShots.) as Avg3PointPercentage FROM nba GROUP BY Team ORDER BY Avg3PointPercentage DESC;
</td>
<td>
Which team has the highest average 3-point shooting percentage?
</td>
</tr>
<tr>
<th>
4
</th>
<td>
SELECT Team, AVG(TotalRebounds) as AvgRebounds FROM nba GROUP BY Team ORDER BY AvgRebounds DESC;
</td>
<td>
Which team has the highest average total rebounds per game?
</td>
</tr>
<tr>
<th>
5
</th>
<td>
SELECT Team, AVG(Turnovers) as AvgTurnovers FROM nba GROUP BY Team ORDER BY AvgTurnovers ASC;
</td>
<td>
Which team has the lowest average turnovers per game?
</td>
</tr>
<tr>
<th>
6
</th>
<td>
SELECT Team, AVG(Assists) as AvgAssists FROM nba GROUP BY Team ORDER BY AvgAssists DESC;
</td>
<td>
Which team has the highest average assists per game?
</td>
</tr>
<tr>
<th>
7
</th>
<td>
SELECT Team, AVG(Steals) as AvgSteals FROM nba GROUP BY Team ORDER BY AvgSteals DESC;
</td>
<td>
Which team has the highest average steals per game?
</td>
</tr>
<tr>
<th>
8
</th>
<td>
SELECT Team, AVG(Blocks) as AvgBlocks FROM nba GROUP BY Team ORDER BY AvgBlocks DESC;
</td>
<td>
Which team has the highest average blocks per game?
</td>
</tr>
<tr>
<th>
9
</th>
<td>
SELECT Team, AVG(TotalFouls) as AvgFouls FROM nba GROUP BY Team ORDER BY AvgFouls ASC;
</td>
<td>
Which team has the lowest average fouls per game?
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">generated_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb19-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> generated_questions:</span>
<span id="cb19-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">try</span>:</span>
<span id="cb19-4">        result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> execute_query(q[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sql"</span>])</span>
<span id="cb19-5">        generated_dataset.append(</span>
<span id="cb19-6">            {</span>
<span id="cb19-7">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: q[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"question"</span>],</span>
<span id="cb19-8">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"expected"</span>: {</span>
<span id="cb19-9">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results"</span>: result,</span>
<span id="cb19-10">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"error"</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb19-11">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"query"</span>: q[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sql"</span>],</span>
<span id="cb19-12">                },</span>
<span id="cb19-13">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"metadata"</span>: {</span>
<span id="cb19-14">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"category"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Generated"</span>,</span>
<span id="cb19-15">                },</span>
<span id="cb19-16">            }</span>
<span id="cb19-17">        )</span>
<span id="cb19-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">except</span> duckdb.Error <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb19-19">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Query failed: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>q[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sql'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, e)</span>
<span id="cb19-20">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Skipping..."</span>)</span>
<span id="cb19-21"></span>
<span id="cb19-22">pd.DataFrame(generated_dataset)<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#[0]</span></span></code></pre></div>
<pre class="text"><code>Query failed: SELECT Team, AVG(FieldGoals.) as AvgFieldGoalPercentage FROM nba GROUP BY Team ORDER BY AvgFieldGoalPercentage DESC; Parser Error: syntax error at or near ")"
Skipping...
Query failed: SELECT Team, AVG(X3PointShots.) as Avg3PointPercentage FROM nba GROUP BY Team ORDER BY Avg3PointPercentage DESC; Parser Error: syntax error at or near ")"
Skipping...</code></pre>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
input
</th>
<th>
expected
</th>
<th>
metadata
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
Which team has the most wins?
</td>
<td>
{‘results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}, {‘Team’: ‘SAS’, ‘Wins’: 230}, {‘Team’: ‘HOU’, ‘Wins’: 217}, {‘Team’: ‘TOR’, ‘Wins’: 215}, {‘Team’: ‘CLE’, ‘Wins’: 211}, {‘Team’: ‘LAC’, ‘Wins’: 202}, {‘Team’: ‘BOS’, ‘Wins’: 196}, {‘Team’: ‘OKC’, ‘Wins’: 195}, {‘Team’: ‘POR’, ‘Wins’: 185}, {‘Team’: ‘WAS’, ‘Wins’: 179}, {‘Team’: ‘UTA’, ‘Wins’: 177}, {‘Team’: ‘ATL’, ‘Wins’: 175}, {‘Team’: ‘IND’, ‘Wins’: 173}, {‘Team’: ‘MIA’, ‘Wins’: 170}, {‘Team’: ‘MEM’, ‘Wins’: 162}, {‘Team’: ‘CHI’, ‘Wins’: 160}, {‘Team’: ‘MIL’, ‘Wins’: 160}, {‘Team’: ‘NOP’, ‘Wins’: 157}, {‘Team’: ‘CHO’, ‘Wins’: 153}, {‘Team’: ‘DET’, ‘Wins’: 152}, {‘Team’: ‘DAL’, ‘Wins’: 149}, {‘Team’: ‘DEN’, ‘Wins’: 149}, {‘Team’: ‘MIN’, ‘Wins’: 123}, {‘Team’: ‘SAC’, ‘Wins’: 121}, {‘Team’: ‘ORL’, ‘Wins’: 114}, {‘Team’: ‘NYK’, ‘Wins’: 109}, {‘Team’: ‘PHI’, ‘Wins’: 108}, {‘Team’: ‘PHO’, ‘Wins’: 107}, {‘Team’: ‘BRK’, ‘Wins’: 107}, {‘Team’: ‘LAL’, ‘Wins’: 99}], ‘error’: None, ‘query’: ’SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Which team has the highest average points per game?
</td>
<td>
{‘results’: [{‘Team’: ‘GSW’, ‘AvgPoints’: 113.54878048780488}, {‘Team’: ‘HOU’, ‘AvgPoints’: 109.54268292682927}, {‘Team’: ‘LAC’, ‘AvgPoints’: 107.21951219512195}, {‘Team’: ‘CLE’, ‘AvgPoints’: 107.16463414634147}, {‘Team’: ‘OKC’, ‘AvgPoints’: 107.15548780487805}, {‘Team’: ‘TOR’, ‘AvgPoints’: 106.30182926829268}, {‘Team’: ‘DEN’, ‘AvgPoints’: 106.26829268292683}, {‘Team’: ‘POR’, ‘AvgPoints’: 105.375}, {‘Team’: ‘BOS’, ‘AvgPoints’: 104.77743902439025}, {‘Team’: ‘WAS’, ‘AvgPoints’: 104.60060975609755}, {‘Team’: ‘NOP’, ‘AvgPoints’: 104.53353658536585}, {‘Team’: ‘MIN’, ‘AvgPoints’: 103.8140243902439}, {‘Team’: ‘PHO’, ‘AvgPoints’: 103.72256097560975}, {‘Team’: ‘SAS’, ‘AvgPoints’: 103.6951219512195}, {‘Team’: ‘ATL’, ‘AvgPoints’: 102.97560975609755}, {‘Team’: ‘CHO’, ‘AvgPoints’: 102.66768292682927}, {‘Team’: ‘IND’, ‘AvgPoints’: 102.53658536585365}, {‘Team’: ‘SAC’, ‘AvgPoints’: 102.39024390243902}, {‘Team’: ‘BRK’, ‘AvgPoints’: 102.25914634146342}, {‘Team’: ‘LAL’, ‘AvgPoints’: 102.10975609756098}, {‘Team’: ‘CHI’, ‘AvgPoints’: 102.0579268292683}, {‘Team’: ‘DAL’, ‘AvgPoints’: 101.9359756097561}, {‘Team’: ‘MIL’, ‘AvgPoints’: 101.7469512195122}, {‘Team’: ‘DET’, ‘AvgPoints’: 101.39024390243902}, {‘Team’: ‘ORL’, ‘AvgPoints’: 100.5579268292683}, {‘Team’: ‘PHI’, ‘AvgPoints’: 100.40853658536585}, {‘Team’: ‘MIA’, ‘AvgPoints’: 100.32926829268293}, {‘Team’: ‘NYK’, ‘AvgPoints’: 99.76219512195122}, {‘Team’: ‘UTA’, ‘AvgPoints’: 99.41768292682927}, {‘Team’: ‘MEM’, ‘AvgPoints’: 99.3048780487805}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(TeamPoints) as AvgPoints FROM nba GROUP BY Team ORDER BY AvgPoints DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
2
</th>
<td>
Which team has the highest average total rebounds per game?
</td>
<td>
{‘results’: [{‘Team’: ‘OKC’, ‘AvgRebounds’: 46.954268292682926}, {‘Team’: ‘CHI’, ‘AvgRebounds’: 45.75}, {‘Team’: ‘DET’, ‘AvgRebounds’: 45.15548780487805}, {‘Team’: ‘POR’, ‘AvgRebounds’: 45.11585365853659}, {‘Team’: ‘DEN’, ‘AvgRebounds’: 45.051829268292686}, {‘Team’: ‘GSW’, ‘AvgRebounds’: 44.71341463414634}, {‘Team’: ‘PHO’, ‘AvgRebounds’: 44.26829268292683}, {‘Team’: ‘CHO’, ‘AvgRebounds’: 44.25609756097561}, {‘Team’: ‘LAL’, ‘AvgRebounds’: 44.1859756097561}, {‘Team’: ‘SAS’, ‘AvgRebounds’: 43.91158536585366}, {‘Team’: ‘BOS’, ‘AvgRebounds’: 43.795731707317074}, {‘Team’: ‘HOU’, ‘AvgRebounds’: 43.64329268292683}, {‘Team’: ‘PHI’, ‘AvgRebounds’: 43.59146341463415}, {‘Team’: ‘NOP’, ‘AvgRebounds’: 43.52439024390244}, {‘Team’: ‘NYK’, ‘AvgRebounds’: 43.49085365853659}, {‘Team’: ‘UTA’, ‘AvgRebounds’: 43.420731707317074}, {‘Team’: ‘IND’, ‘AvgRebounds’: 43.35670731707317}, {‘Team’: ‘CLE’, ‘AvgRebounds’: 43.33841463414634}, {‘Team’: ‘BRK’, ‘AvgRebounds’: 43.271341463414636}, {‘Team’: ‘WAS’, ‘AvgRebounds’: 43.1219512195122}, {‘Team’: ‘TOR’, ‘AvgRebounds’: 43.051829268292686}, {‘Team’: ‘LAC’, ‘AvgRebounds’: 42.890243902439025}, {‘Team’: ‘SAC’, ‘AvgRebounds’: 42.60060975609756}, {‘Team’: ‘MIA’, ‘AvgRebounds’: 42.582317073170735}, {‘Team’: ‘ORL’, ‘AvgRebounds’: 42.48780487804878}, {‘Team’: ‘ATL’, ‘AvgRebounds’: 42.21646341463415}, {‘Team’: ‘MEM’, ‘AvgRebounds’: 41.8719512195122}, {‘Team’: ‘MIN’, ‘AvgRebounds’: 41.71341463414634}, {‘Team’: ‘DAL’, ‘AvgRebounds’: 41.292682926829265}, {‘Team’: ‘MIL’, ‘AvgRebounds’: 40.99390243902439}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalRebounds) as AvgRebounds FROM nba GROUP BY Team ORDER BY AvgRebounds DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
3
</th>
<td>
Which team has the lowest average turnovers per game?
</td>
<td>
{‘results’: [{‘Team’: ‘CHO’, ‘AvgTurnovers’: 11.579268292682928}, {‘Team’: ‘DAL’, ‘AvgTurnovers’: 11.972560975609756}, {‘Team’: ‘TOR’, ‘AvgTurnovers’: 12.21951219512195}, {‘Team’: ‘DET’, ‘AvgTurnovers’: 12.432926829268293}, {‘Team’: ‘LAC’, ‘AvgTurnovers’: 12.676829268292684}, {‘Team’: ‘SAS’, ‘AvgTurnovers’: 12.893292682926829}, {‘Team’: ‘NOP’, ‘AvgTurnovers’: 13.097560975609756}, {‘Team’: ‘MEM’, ‘AvgTurnovers’: 13.128048780487806}, {‘Team’: ‘BOS’, ‘AvgTurnovers’: 13.173780487804878}, {‘Team’: ‘CHI’, ‘AvgTurnovers’: 13.195121951219512}, {‘Team’: ‘CLE’, ‘AvgTurnovers’: 13.25}, {‘Team’: ‘MIA’, ‘AvgTurnovers’: 13.341463414634147}, {‘Team’: ‘POR’, ‘AvgTurnovers’: 13.36890243902439}, {‘Team’: ‘IND’, ‘AvgTurnovers’: 13.426829268292684}, {‘Team’: ‘MIN’, ‘AvgTurnovers’: 13.448170731707316}, {‘Team’: ‘NYK’, ‘AvgTurnovers’: 13.451219512195122}, {‘Team’: ‘ORL’, ‘AvgTurnovers’: 13.676829268292684}, {‘Team’: ‘UTA’, ‘AvgTurnovers’: 13.850609756097562}, {‘Team’: ‘LAL’, ‘AvgTurnovers’: 13.875}, {‘Team’: ‘WAS’, ‘AvgTurnovers’: 13.939024390243903}, {‘Team’: ‘DEN’, ‘AvgTurnovers’: 14.195121951219512}, {‘Team’: ‘MIL’, ‘AvgTurnovers’: 14.277439024390244}, {‘Team’: ‘OKC’, ‘AvgTurnovers’: 14.390243902439025}, {‘Team’: ‘BRK’, ‘AvgTurnovers’: 14.46951219512195}, {‘Team’: ‘ATL’, ‘AvgTurnovers’: 14.551829268292684}, {‘Team’: ‘GSW’, ‘AvgTurnovers’: 14.59451219512195}, {‘Team’: ‘SAC’, ‘AvgTurnovers’: 14.615853658536585}, {‘Team’: ‘HOU’, ‘AvgTurnovers’: 14.713414634146341}, {‘Team’: ‘PHO’, ‘AvgTurnovers’: 15.268292682926829}, {‘Team’: ‘PHI’, ‘AvgTurnovers’: 16.085365853658537}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(Turnovers) as AvgTurnovers FROM nba GROUP BY Team ORDER BY AvgTurnovers ASC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
4
</th>
<td>
Which team has the highest average assists per game?
</td>
<td>
{‘results’: [{‘Team’: ‘GSW’, ‘AvgAssists’: 29.00609756097561}, {‘Team’: ‘ATL’, ‘AvgAssists’: 24.679878048780488}, {‘Team’: ‘WAS’, ‘AvgAssists’: 24.375}, {‘Team’: ‘BOS’, ‘AvgAssists’: 24.088414634146343}, {‘Team’: ‘SAS’, ‘AvgAssists’: 23.878048780487806}, {‘Team’: ‘DEN’, ‘AvgAssists’: 23.722560975609756}, {‘Team’: ‘MIL’, ‘AvgAssists’: 23.524390243902438}, {‘Team’: ‘NOP’, ‘AvgAssists’: 23.4390243902439}, {‘Team’: ‘PHI’, ‘AvgAssists’: 23.225609756097562}, {‘Team’: ‘LAC’, ‘AvgAssists’: 23.121951219512194}, {‘Team’: ‘MIN’, ‘AvgAssists’: 22.829268292682926}, {‘Team’: ‘HOU’, ‘AvgAssists’: 22.798780487804876}, {‘Team’: ‘CLE’, ‘AvgAssists’: 22.710365853658537}, {‘Team’: ‘CHI’, ‘AvgAssists’: 22.63719512195122}, {‘Team’: ‘ORL’, ‘AvgAssists’: 22.45731707317073}, {‘Team’: ‘SAC’, ‘AvgAssists’: 22.222560975609756}, {‘Team’: ‘BRK’, ‘AvgAssists’: 22.08231707317073}, {‘Team’: ‘DAL’, ‘AvgAssists’: 22.01829268292683}, {‘Team’: ‘IND’, ‘AvgAssists’: 21.83231707317073}, {‘Team’: ‘NYK’, ‘AvgAssists’: 21.725609756097562}, {‘Team’: ‘CHO’, ‘AvgAssists’: 21.625}, {‘Team’: ‘OKC’, ‘AvgAssists’: 21.448170731707318}, {‘Team’: ‘MEM’, ‘AvgAssists’: 21.295731707317074}, {‘Team’: ‘DET’, ‘AvgAssists’: 21.234756097560975}, {‘Team’: ‘MIA’, ‘AvgAssists’: 21.15548780487805}, {‘Team’: ‘POR’, ‘AvgAssists’: 20.972560975609756}, {‘Team’: ‘LAL’, ‘AvgAssists’: 20.908536585365855}, {‘Team’: ‘TOR’, ‘AvgAssists’: 20.576219512195124}, {‘Team’: ‘PHO’, ‘AvgAssists’: 20.448170731707318}, {‘Team’: ‘UTA’, ‘AvgAssists’: 20.35670731707317}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(Assists) as AvgAssists FROM nba GROUP BY Team ORDER BY AvgAssists DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
5
</th>
<td>
Which team has the highest average steals per game?
</td>
<td>
{‘results’: [{‘Team’: ‘HOU’, ‘AvgSteals’: 9.045731707317072}, {‘Team’: ‘GSW’, ‘AvgSteals’: 8.817073170731707}, {‘Team’: ‘MIL’, ‘AvgSteals’: 8.685975609756097}, {‘Team’: ‘PHI’, ‘AvgSteals’: 8.667682926829269}, {‘Team’: ‘ATL’, ‘AvgSteals’: 8.539634146341463}, {‘Team’: ‘MEM’, ‘AvgSteals’: 8.201219512195122}, {‘Team’: ‘MIN’, ‘AvgSteals’: 8.13109756097561}, {‘Team’: ‘BOS’, ‘AvgSteals’: 8.070121951219512}, {‘Team’: ‘WAS’, ‘AvgSteals’: 8.070121951219512}, {‘Team’: ‘IND’, ‘AvgSteals’: 8.036585365853659}, {‘Team’: ‘SAS’, ‘AvgSteals’: 7.978658536585366}, {‘Team’: ‘OKC’, ‘AvgSteals’: 7.902439024390244}, {‘Team’: ‘LAC’, ‘AvgSteals’: 7.890243902439025}, {‘Team’: ‘PHO’, ‘AvgSteals’: 7.850609756097561}, {‘Team’: ‘TOR’, ‘AvgSteals’: 7.786585365853658}, {‘Team’: ‘SAC’, ‘AvgSteals’: 7.783536585365853}, {‘Team’: ‘ORL’, ‘AvgSteals’: 7.689024390243903}, {‘Team’: ‘UTA’, ‘AvgSteals’: 7.658536585365853}, {‘Team’: ‘NOP’, ‘AvgSteals’: 7.560975609756097}, {‘Team’: ‘LAL’, ‘AvgSteals’: 7.554878048780488}, {‘Team’: ‘DEN’, ‘AvgSteals’: 7.454268292682927}, {‘Team’: ‘DAL’, ‘AvgSteals’: 7.362804878048781}, {‘Team’: ‘MIA’, ‘AvgSteals’: 7.326219512195122}, {‘Team’: ‘DET’, ‘AvgSteals’: 7.310975609756097}, {‘Team’: ‘BRK’, ‘AvgSteals’: 7.036585365853658}, {‘Team’: ‘CHI’, ‘AvgSteals’: 6.945121951219512}, {‘Team’: ‘CLE’, ‘AvgSteals’: 6.935975609756097}, {‘Team’: ‘POR’, ‘AvgSteals’: 6.810975609756097}, {‘Team’: ‘CHO’, ‘AvgSteals’: 6.780487804878049}, {‘Team’: ‘NYK’, ‘AvgSteals’: 6.6310975609756095}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(Steals) as AvgSteals FROM nba GROUP BY Team ORDER BY AvgSteals DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
6
</th>
<td>
Which team has the highest average blocks per game?
</td>
<td>
{‘results’: [{‘Team’: ‘GSW’, ‘AvgBlocks’: 6.588414634146342}, {‘Team’: ‘SAS’, ‘AvgBlocks’: 5.710365853658536}, {‘Team’: ‘PHI’, ‘AvgBlocks’: 5.554878048780488}, {‘Team’: ‘MIA’, ‘AvgBlocks’: 5.515243902439025}, {‘Team’: ‘NOP’, ‘AvgBlocks’: 5.4573170731707314}, {‘Team’: ‘OKC’, ‘AvgBlocks’: 5.3689024390243905}, {‘Team’: ‘MIL’, ‘AvgBlocks’: 5.3567073170731705}, {‘Team’: ‘UTA’, ‘AvgBlocks’: 5.317073170731708}, {‘Team’: ‘NYK’, ‘AvgBlocks’: 5.2560975609756095}, {‘Team’: ‘TOR’, ‘AvgBlocks’: 5.201219512195122}, {‘Team’: ‘CHO’, ‘AvgBlocks’: 5.027439024390244}, {‘Team’: ‘CHI’, ‘AvgBlocks’: 4.963414634146342}, {‘Team’: ‘ATL’, ‘AvgBlocks’: 4.911585365853658}, {‘Team’: ‘LAC’, ‘AvgBlocks’: 4.844512195121951}, {‘Team’: ‘POR’, ‘AvgBlocks’: 4.826219512195122}, {‘Team’: ‘HOU’, ‘AvgBlocks’: 4.820121951219512}, {‘Team’: ‘ORL’, ‘AvgBlocks’: 4.655487804878049}, {‘Team’: ‘IND’, ‘AvgBlocks’: 4.6189024390243905}, {‘Team’: ‘DEN’, ‘AvgBlocks’: 4.539634146341464}, {‘Team’: ‘PHO’, ‘AvgBlocks’: 4.469512195121951}, {‘Team’: ‘BRK’, ‘AvgBlocks’: 4.4176829268292686}, {‘Team’: ‘MEM’, ‘AvgBlocks’: 4.3810975609756095}, {‘Team’: ‘MIN’, ‘AvgBlocks’: 4.317073170731708}, {‘Team’: ‘LAL’, ‘AvgBlocks’: 4.301829268292683}, {‘Team’: ‘WAS’, ‘AvgBlocks’: 4.237804878048781}, {‘Team’: ‘SAC’, ‘AvgBlocks’: 4.134146341463414}, {‘Team’: ‘BOS’, ‘AvgBlocks’: 4.134146341463414}, {‘Team’: ‘DET’, ‘AvgBlocks’: 4.0060975609756095}, {‘Team’: ‘CLE’, ‘AvgBlocks’: 3.951219512195122}, {‘Team’: ‘DAL’, ‘AvgBlocks’: 3.9451219512195124}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(Blocks) as AvgBlocks FROM nba GROUP BY Team ORDER BY AvgBlocks DESC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
7
</th>
<td>
Which team has the lowest average fouls per game?
</td>
<td>
{‘results’: [{‘Team’: ‘CHO’, ‘AvgFouls’: 17.533536585365855}, {‘Team’: ‘SAS’, ‘AvgFouls’: 18.0}, {‘Team’: ‘CHI’, ‘AvgFouls’: 18.496951219512194}, {‘Team’: ‘DET’, ‘AvgFouls’: 18.570121951219512}, {‘Team’: ‘ATL’, ‘AvgFouls’: 18.670731707317074}, {‘Team’: ‘CLE’, ‘AvgFouls’: 18.85670731707317}, {‘Team’: ‘NOP’, ‘AvgFouls’: 19.216463414634145}, {‘Team’: ‘DAL’, ‘AvgFouls’: 19.463414634146343}, {‘Team’: ‘UTA’, ‘AvgFouls’: 19.484756097560975}, {‘Team’: ‘MIN’, ‘AvgFouls’: 19.536585365853657}, {‘Team’: ‘MIA’, ‘AvgFouls’: 19.713414634146343}, {‘Team’: ‘BRK’, ‘AvgFouls’: 19.722560975609756}, {‘Team’: ‘GSW’, ‘AvgFouls’: 19.878048780487806}, {‘Team’: ‘IND’, ‘AvgFouls’: 19.88719512195122}, {‘Team’: ‘ORL’, ‘AvgFouls’: 20.057926829268293}, {‘Team’: ‘POR’, ‘AvgFouls’: 20.15548780487805}, {‘Team’: ‘SAC’, ‘AvgFouls’: 20.359756097560975}, {‘Team’: ‘DEN’, ‘AvgFouls’: 20.442073170731707}, {‘Team’: ‘NYK’, ‘AvgFouls’: 20.53048780487805}, {‘Team’: ‘LAC’, ‘AvgFouls’: 20.60670731707317}, {‘Team’: ‘TOR’, ‘AvgFouls’: 20.774390243902438}, {‘Team’: ‘HOU’, ‘AvgFouls’: 20.804878048780488}, {‘Team’: ‘BOS’, ‘AvgFouls’: 20.847560975609756}, {‘Team’: ‘LAL’, ‘AvgFouls’: 20.850609756097562}, {‘Team’: ‘OKC’, ‘AvgFouls’: 21.003048780487806}, {‘Team’: ‘WAS’, ‘AvgFouls’: 21.054878048780488}, {‘Team’: ‘MIL’, ‘AvgFouls’: 21.100609756097562}, {‘Team’: ‘MEM’, ‘AvgFouls’: 21.615853658536587}, {‘Team’: ‘PHI’, ‘AvgFouls’: 21.841463414634145}, {‘Team’: ‘PHO’, ‘AvgFouls’: 22.679878048780488}], ‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalFouls) as AvgFouls FROM nba GROUP BY Team ORDER BY AvgFouls ASC;’}
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> load_data():</span>
<span id="cb21-2">    golden_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> init_dataset(PROJECT_NAME, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Golden data"</span>)</span>
<span id="cb21-3">    golden_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> d <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_data)</span>
<span id="cb21-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> (</span>
<span id="cb21-5">        [{<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>x, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"metadata"</span>: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"category"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Golden data"</span>}} <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_data]</span>
<span id="cb21-6">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [</span>
<span id="cb21-7">            {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: q, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"metadata"</span>: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"category"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Handwritten question"</span>}}</span>
<span id="cb21-8">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> questions</span>
<span id="cb21-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_questions</span>
<span id="cb21-10">        ]</span>
<span id="cb21-11">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [x <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> generated_dataset <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> golden_questions]</span>
<span id="cb21-12">    )</span></code></pre></div>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> Eval(</span>
<span id="cb22-2">    PROJECT_NAME,</span>
<span id="cb22-3">    experiment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Generated data"</span>,</span>
<span id="cb22-4">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>load_data,</span>
<span id="cb22-5">    task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text2sql,</span>
<span id="cb22-6">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[no_error, correct_result, correct_sql],</span>
<span id="cb22-7">)</span></code></pre></div>
<pre class="text"><code>Experiment Generated data is running at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Generated%20data
LLM Eval for Text2SQL [experiment_name=Generated data] (data): 13it [00:00, 111962.94it/s]

LLM Eval for Text2SQL [experiment_name=Generated data] (tasks):   0%|          | 0/13 [00:00&lt;?, ?it/s]

=========================SUMMARY=========================
Generated data compared to With samples:
22.50% (-) 'correct_result' score   (0 improvements, 0 regressions)
84.62% (-) 'no_error'       score   (0 improvements, 0 regressions)
72.73% (-) 'correct_sql'    score   (0 improvements, 0 regressions)

See results for Generated data at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Generated%20data

EvalResultWithSummary(summary="...", results=[...])</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/images/eval-3.png" class="img-fluid figure-img"></p>
<figcaption>eval 3</figcaption>
</figure>
</div>
</section>
<section id="trying-gpt-4" class="level3">
<h3 class="anchored" data-anchor-id="trying-gpt-4">Trying GPT-4</h3>
<ul>
<li><strong>Experiment:</strong> Replace GPT-4o with GPT-4 in the task function.</li>
<li><strong>Results (During Talk):</strong>
<ul>
<li>Regression in performance across all metrics, including on the golden dataset.
<ul>
<li>Nearly a total wash between the improvements and regressions when testing locally</li>
</ul></li>
<li>Analysis suggests potential issues with date syntax and prompt formatting.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">TASK_MODEL <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4"</span></span>
<span id="cb24-2"></span>
<span id="cb24-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">await</span> Eval(</span>
<span id="cb24-4">    PROJECT_NAME,</span>
<span id="cb24-5">    experiment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Try gpt-4"</span>,</span>
<span id="cb24-6">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>load_data,</span>
<span id="cb24-7">    task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text2sql,</span>
<span id="cb24-8">    scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[no_error, correct_result, correct_sql],</span>
<span id="cb24-9">)</span></code></pre></div>
<pre class="text"><code>Experiment Try gpt-4 is running at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Try%20gpt-4
LLM Eval for Text2SQL [experiment_name=Try gpt-4] (data): 13it [00:00, 162764.04it/s]

LLM Eval for Text2SQL [experiment_name=Try gpt-4] (tasks):   0%|          | 0/13 [00:00&lt;?, ?it/s]

=========================SUMMARY=========================
Try gpt-4 compared to Generated data:
12.73% (-09.77%) 'correct_result' score (1 improvements, 1 regressions)
81.82% (+09.09%) 'correct_sql'    score (3 improvements, 2 regressions)
84.62% (-) 'no_error'       score   (1 improvements, 1 regressions)

See results for Try gpt-4 at https://www.braintrust.dev/app/Christian%20J.%20Mills%20Consulting/p/LLM%20Eval%20for%20Text2SQL/experiments/Try%20gpt-4

EvalResultWithSummary(summary="...", results=[...])</code></pre>
<section id="results-2" class="level4">
<h4 class="anchored" data-anchor-id="results-2">Results</h4>
<div style="overflow-x:auto; max-height:500px">
<table class="dataframe table table-sm table-striped small">
<thead>
<tr>
<th>
</th>
<th>
id
</th>
<th>
_xact_id
</th>
<th>
input
</th>
<th>
output
</th>
<th>
expected
</th>
<th>
tags
</th>
<th>
scores
</th>
<th>
duration
</th>
<th>
estimated_cost
</th>
<th>
metadata
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
bc0a4372-24f7-441a-8b3f-f618dd7b378f
</td>
<td>
1000193295096986240
</td>
<td>
Who led the league in 3 point shots?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, MAX(X3PointShots) as Max_3PointShots FROM nba GROUP BY Team ORDER BY Max_3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘CLE’, ‘Max_3PointShots’: 25}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, SUM(X3PointShots) AS Total3PointShots FROM nba GROUP BY Team ORDER BY Total3PointShots DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘Total3PointShots’: 4248}]}
</td>
<td>
[]
</td>
<td>
{‘correct_result’: 0.005850690381465001, ‘correct_sql’: 0, ‘no_error’: 1}
</td>
<td>
7.480919
</td>
<td>
0.018554
</td>
<td>
{‘category’: ‘Golden data’}
</td>
</tr>
<tr>
<th>
1
</th>
<td>
54c91672-cba1-4e46-a85f-b9ed90b618a1
</td>
<td>
1000193295097051776
</td>
<td>
Which team has the highest average total rebounds per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalRebounds) as AverageRebounds FROM nba GROUP BY Team ORDER BY AverageRebounds DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘OKC’, ‘AverageRebounds’: 46.954268292682926}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalRebounds) as AvgRebounds FROM nba GROUP BY Team ORDER BY AvgRebounds DESC;’, ‘results’: [{‘Team’: ‘OKC’, ‘AvgRebounds’: 46.954268292682926}, {‘Team’: ‘CHI’, ‘AvgRebounds’: 45.75}, {‘Team’: ‘DET’, ‘AvgRebounds’: 45.15548780487805}, {‘Team’: ‘POR’, ‘AvgRebounds’: 45.11585365853659}, {‘Team’: ‘DEN’, ‘AvgRebounds’: 45.051829268292686}, {‘Team’: ‘GSW’, ‘AvgRebounds’: 44.71341463414634}, {‘Team’: ‘PHO’, ‘AvgRebounds’: 44.26829268292683}, {‘Team’: ‘CHO’, ‘AvgRebounds’: 44.25609756097561}, {‘Team’: ‘LAL’, ‘AvgRebounds’: 44.1859756097561}, {‘Team’: ‘SAS’, ‘AvgRebounds’: 43.91158536585366}, {‘Team’: ‘BOS’, ‘AvgRebounds’: 43.795731707317074}, {‘Team’: ‘HOU’, ‘AvgRebounds’: 43.64329268292683}, {‘Team’: ‘PHI’, ‘AvgRebounds’: 43.59146341463415}, {‘Team’: ‘NOP’, ‘AvgRebounds’: 43.52439024390244}, {‘Team’: ‘NYK’, ‘AvgRebounds’: 43.49085365853659}, {‘Team’: ‘UTA’, ‘AvgRebounds’: 43.420731707317074}, {‘Team’: ‘IND’, ‘AvgRebounds’: 43.35670731707317}, {‘Team’: ‘CLE’, ‘AvgRebounds’: 43.33841463414634}, {‘Team’: ‘BRK’, ‘AvgRebounds’: 43.271341463414636}, {‘Team’: ‘WAS’, ‘AvgRebounds’: 43.1219512195122}, {‘Team’: ‘TOR’, ‘AvgRebounds’: 43.051829268292686}, {‘Team’: ‘LAC’, ‘AvgRebounds’: 42.890243902439025}, {‘Team’: ‘SAC’, ‘AvgRebounds’: 42.60060975609756}, {‘Team’: ‘MIA’, ‘AvgRebounds’: 42.582317073170735}, {‘Team’: ‘ORL’, ‘AvgRebounds’: 42.48780487804878}, {‘Team’: ‘ATL’, ‘AvgRebounds’: 42.21646341463415}, {‘Team’: ‘MEM’, ‘AvgRebounds’: 41.8719512195122}, {‘Team’: ‘MIN’, ‘AvgRebounds’: 41.71341463414634}, {‘Team’: ‘DAL’, ‘AvgRebounds’: 41.292682926829265}, {‘Team’: ‘MIL’, ‘AvgRebounds’: 40.99390243902439}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 0, ‘no_error’: 1}
</td>
<td>
8.128142
</td>
<td>
0.018371
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
2
</th>
<td>
ef818ada-cbf7-4dab-a6c8-8bca01d340d1
</td>
<td>
1000193295096986240
</td>
<td>
Which team has the lowest average fouls per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalFouls) as AverageFouls FROM nba GROUP BY Team ORDER BY AverageFouls ASC LIMIT 1;’, ‘results’: [{‘Team’: ‘CHO’, ‘AverageFouls’: 17.533536585365855}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TotalFouls) as AvgFouls FROM nba GROUP BY Team ORDER BY AvgFouls ASC;’, ‘results’: [{‘Team’: ‘CHO’, ‘AvgFouls’: 17.533536585365855}, {‘Team’: ‘SAS’, ‘AvgFouls’: 18}, {‘Team’: ‘CHI’, ‘AvgFouls’: 18.496951219512194}, {‘Team’: ‘DET’, ‘AvgFouls’: 18.570121951219512}, {‘Team’: ‘ATL’, ‘AvgFouls’: 18.670731707317074}, {‘Team’: ‘CLE’, ‘AvgFouls’: 18.85670731707317}, {‘Team’: ‘NOP’, ‘AvgFouls’: 19.216463414634145}, {‘Team’: ‘DAL’, ‘AvgFouls’: 19.463414634146343}, {‘Team’: ‘UTA’, ‘AvgFouls’: 19.484756097560975}, {‘Team’: ‘MIN’, ‘AvgFouls’: 19.536585365853657}, {‘Team’: ‘MIA’, ‘AvgFouls’: 19.713414634146343}, {‘Team’: ‘BRK’, ‘AvgFouls’: 19.722560975609756}, {‘Team’: ‘GSW’, ‘AvgFouls’: 19.878048780487806}, {‘Team’: ‘IND’, ‘AvgFouls’: 19.88719512195122}, {‘Team’: ‘ORL’, ‘AvgFouls’: 20.057926829268293}, {‘Team’: ‘POR’, ‘AvgFouls’: 20.15548780487805}, {‘Team’: ‘SAC’, ‘AvgFouls’: 20.359756097560975}, {‘Team’: ‘DEN’, ‘AvgFouls’: 20.442073170731707}, {‘Team’: ‘NYK’, ‘AvgFouls’: 20.53048780487805}, {‘Team’: ‘LAC’, ‘AvgFouls’: 20.60670731707317}, {‘Team’: ‘TOR’, ‘AvgFouls’: 20.774390243902438}, {‘Team’: ‘HOU’, ‘AvgFouls’: 20.804878048780488}, {‘Team’: ‘BOS’, ‘AvgFouls’: 20.847560975609756}, {‘Team’: ‘LAL’, ‘AvgFouls’: 20.850609756097562}, {‘Team’: ‘OKC’, ‘AvgFouls’: 21.003048780487806}, {‘Team’: ‘WAS’, ‘AvgFouls’: 21.054878048780488}, {‘Team’: ‘MIL’, ‘AvgFouls’: 21.100609756097562}, {‘Team’: ‘MEM’, ‘AvgFouls’: 21.615853658536587}, {‘Team’: ‘PHI’, ‘AvgFouls’: 21.841463414634145}, {‘Team’: ‘PHO’, ‘AvgFouls’: 22.679878048780488}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.661695
</td>
<td>
0.018478
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
3
</th>
<td>
80a3d921-c5c9-4ebc-97f2-4923069cb327
</td>
<td>
1000193295096920704
</td>
<td>
Which team has the highest average blocks per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Blocks) as AverageBlocks FROM nba GROUP BY Team ORDER BY AverageBlocks DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘GSW’, ‘AverageBlocks’: 6.588414634146342}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Blocks) as AvgBlocks FROM nba GROUP BY Team ORDER BY AvgBlocks DESC;’, ‘results’: [{‘Team’: ‘GSW’, ‘AvgBlocks’: 6.588414634146342}, {‘Team’: ‘SAS’, ‘AvgBlocks’: 5.710365853658536}, {‘Team’: ‘PHI’, ‘AvgBlocks’: 5.554878048780488}, {‘Team’: ‘MIA’, ‘AvgBlocks’: 5.515243902439025}, {‘Team’: ‘NOP’, ‘AvgBlocks’: 5.4573170731707314}, {‘Team’: ‘OKC’, ‘AvgBlocks’: 5.3689024390243905}, {‘Team’: ‘MIL’, ‘AvgBlocks’: 5.35670731707317}, {‘Team’: ‘UTA’, ‘AvgBlocks’: 5.3170731707317085}, {‘Team’: ‘NYK’, ‘AvgBlocks’: 5.256097560975609}, {‘Team’: ‘TOR’, ‘AvgBlocks’: 5.201219512195122}, {‘Team’: ‘CHO’, ‘AvgBlocks’: 5.027439024390244}, {‘Team’: ‘CHI’, ‘AvgBlocks’: 4.963414634146342}, {‘Team’: ‘ATL’, ‘AvgBlocks’: 4.911585365853658}, {‘Team’: ‘LAC’, ‘AvgBlocks’: 4.844512195121951}, {‘Team’: ‘POR’, ‘AvgBlocks’: 4.826219512195122}, {‘Team’: ‘HOU’, ‘AvgBlocks’: 4.820121951219512}, {‘Team’: ‘ORL’, ‘AvgBlocks’: 4.6554878048780495}, {‘Team’: ‘IND’, ‘AvgBlocks’: 4.6189024390243905}, {‘Team’: ‘DEN’, ‘AvgBlocks’: 4.539634146341464}, {‘Team’: ‘PHO’, ‘AvgBlocks’: 4.469512195121951}, {‘Team’: ‘BRK’, ‘AvgBlocks’: 4.417682926829268}, {‘Team’: ‘MEM’, ‘AvgBlocks’: 4.381097560975609}, {‘Team’: ‘MIN’, ‘AvgBlocks’: 4.3170731707317085}, {‘Team’: ‘LAL’, ‘AvgBlocks’: 4.301829268292683}, {‘Team’: ‘WAS’, ‘AvgBlocks’: 4.237804878048781}, {‘Team’: ‘SAC’, ‘AvgBlocks’: 4.134146341463414}, {‘Team’: ‘BOS’, ‘AvgBlocks’: 4.134146341463414}, {‘Team’: ‘DET’, ‘AvgBlocks’: 4.006097560975609}, {‘Team’: ‘CLE’, ‘AvgBlocks’: 3.951219512195122}, {‘Team’: ‘DAL’, ‘AvgBlocks’: 3.945121951219512}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.120250
</td>
<td>
0.017907
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
4
</th>
<td>
43822081-8065-4914-9163-392e325c5af5
</td>
<td>
1000193295096920704
</td>
<td>
Which team has the highest average steals per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Steals) as AverageSteals FROM nba GROUP BY Team ORDER BY AverageSteals DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘HOU’, ‘AverageSteals’: 9.045731707317072}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Steals) as AvgSteals FROM nba GROUP BY Team ORDER BY AvgSteals DESC;’, ‘results’: [{‘Team’: ‘HOU’, ‘AvgSteals’: 9.045731707317072}, {‘Team’: ‘GSW’, ‘AvgSteals’: 8.817073170731707}, {‘Team’: ‘MIL’, ‘AvgSteals’: 8.685975609756097}, {‘Team’: ‘PHI’, ‘AvgSteals’: 8.667682926829269}, {‘Team’: ‘ATL’, ‘AvgSteals’: 8.539634146341463}, {‘Team’: ‘MEM’, ‘AvgSteals’: 8.201219512195122}, {‘Team’: ‘MIN’, ‘AvgSteals’: 8.13109756097561}, {‘Team’: ‘WAS’, ‘AvgSteals’: 8.070121951219512}, {‘Team’: ‘BOS’, ‘AvgSteals’: 8.070121951219512}, {‘Team’: ‘IND’, ‘AvgSteals’: 8.036585365853659}, {‘Team’: ‘SAS’, ‘AvgSteals’: 7.978658536585366}, {‘Team’: ‘OKC’, ‘AvgSteals’: 7.902439024390244}, {‘Team’: ‘LAC’, ‘AvgSteals’: 7.890243902439025}, {‘Team’: ‘PHO’, ‘AvgSteals’: 7.850609756097561}, {‘Team’: ‘TOR’, ‘AvgSteals’: 7.786585365853658}, {‘Team’: ‘SAC’, ‘AvgSteals’: 7.783536585365853}, {‘Team’: ‘ORL’, ‘AvgSteals’: 7.689024390243903}, {‘Team’: ‘UTA’, ‘AvgSteals’: 7.658536585365853}, {‘Team’: ‘NOP’, ‘AvgSteals’: 7.560975609756097}, {‘Team’: ‘LAL’, ‘AvgSteals’: 7.554878048780488}, {‘Team’: ‘DEN’, ‘AvgSteals’: 7.454268292682927}, {‘Team’: ‘DAL’, ‘AvgSteals’: 7.362804878048781}, {‘Team’: ‘MIA’, ‘AvgSteals’: 7.326219512195122}, {‘Team’: ‘DET’, ‘AvgSteals’: 7.310975609756097}, {‘Team’: ‘BRK’, ‘AvgSteals’: 7.036585365853658}, {‘Team’: ‘CHI’, ‘AvgSteals’: 6.945121951219512}, {‘Team’: ‘CLE’, ‘AvgSteals’: 6.935975609756097}, {‘Team’: ‘POR’, ‘AvgSteals’: 6.810975609756097}, {‘Team’: ‘CHO’, ‘AvgSteals’: 6.7804878048780495}, {‘Team’: ‘NYK’, ‘AvgSteals’: 6.6310975609756095}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.062654
</td>
<td>
0.018146
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
5
</th>
<td>
7d0561af-0571-4b65-baba-25f3d0a97472
</td>
<td>
1000193295096920704
</td>
<td>
Which team has the highest average assists per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Assists) as AverageAssists FROM nba GROUP BY Team ORDER BY AverageAssists DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘GSW’, ‘AverageAssists’: 29.00609756097561}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Assists) as AvgAssists FROM nba GROUP BY Team ORDER BY AvgAssists DESC;’, ‘results’: [{‘Team’: ‘GSW’, ‘AvgAssists’: 29.00609756097561}, {‘Team’: ‘ATL’, ‘AvgAssists’: 24.679878048780488}, {‘Team’: ‘WAS’, ‘AvgAssists’: 24.375}, {‘Team’: ‘BOS’, ‘AvgAssists’: 24.088414634146343}, {‘Team’: ‘SAS’, ‘AvgAssists’: 23.878048780487806}, {‘Team’: ‘DEN’, ‘AvgAssists’: 23.722560975609756}, {‘Team’: ‘MIL’, ‘AvgAssists’: 23.524390243902438}, {‘Team’: ‘NOP’, ‘AvgAssists’: 23.4390243902439}, {‘Team’: ‘PHI’, ‘AvgAssists’: 23.225609756097562}, {‘Team’: ‘LAC’, ‘AvgAssists’: 23.121951219512194}, {‘Team’: ‘MIN’, ‘AvgAssists’: 22.829268292682926}, {‘Team’: ‘HOU’, ‘AvgAssists’: 22.798780487804876}, {‘Team’: ‘CLE’, ‘AvgAssists’: 22.710365853658537}, {‘Team’: ‘CHI’, ‘AvgAssists’: 22.63719512195122}, {‘Team’: ‘ORL’, ‘AvgAssists’: 22.45731707317073}, {‘Team’: ‘SAC’, ‘AvgAssists’: 22.222560975609756}, {‘Team’: ‘BRK’, ‘AvgAssists’: 22.08231707317073}, {‘Team’: ‘DAL’, ‘AvgAssists’: 22.01829268292683}, {‘Team’: ‘IND’, ‘AvgAssists’: 21.83231707317073}, {‘Team’: ‘NYK’, ‘AvgAssists’: 21.725609756097562}, {‘Team’: ‘CHO’, ‘AvgAssists’: 21.625}, {‘Team’: ‘OKC’, ‘AvgAssists’: 21.448170731707318}, {‘Team’: ‘MEM’, ‘AvgAssists’: 21.295731707317074}, {‘Team’: ‘DET’, ‘AvgAssists’: 21.234756097560975}, {‘Team’: ‘MIA’, ‘AvgAssists’: 21.15548780487805}, {‘Team’: ‘POR’, ‘AvgAssists’: 20.972560975609756}, {‘Team’: ‘LAL’, ‘AvgAssists’: 20.908536585365855}, {‘Team’: ‘TOR’, ‘AvgAssists’: 20.576219512195124}, {‘Team’: ‘PHO’, ‘AvgAssists’: 20.448170731707318}, {‘Team’: ‘UTA’, ‘AvgAssists’: 20.35670731707317}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.208672
</td>
<td>
0.018167
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
6
</th>
<td>
ec89db77-9bc6-4204-8083-e5bc0f71c3eb
</td>
<td>
1000193295096986240
</td>
<td>
Which team has the lowest average turnovers per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Turnovers) as AverageTurnovers FROM nba GROUP BY Team ORDER BY AverageTurnovers ASC LIMIT 1;’, ‘results’: [{‘Team’: ‘CHO’, ‘AverageTurnovers’: 11.579268292682928}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(Turnovers) as AvgTurnovers FROM nba GROUP BY Team ORDER BY AvgTurnovers ASC;’, ‘results’: [{‘Team’: ‘CHO’, ‘AvgTurnovers’: 11.579268292682928}, {‘Team’: ‘DAL’, ‘AvgTurnovers’: 11.972560975609756}, {‘Team’: ‘TOR’, ‘AvgTurnovers’: 12.21951219512195}, {‘Team’: ‘DET’, ‘AvgTurnovers’: 12.432926829268293}, {‘Team’: ‘LAC’, ‘AvgTurnovers’: 12.676829268292684}, {‘Team’: ‘SAS’, ‘AvgTurnovers’: 12.893292682926829}, {‘Team’: ‘NOP’, ‘AvgTurnovers’: 13.097560975609756}, {‘Team’: ‘MEM’, ‘AvgTurnovers’: 13.128048780487806}, {‘Team’: ‘BOS’, ‘AvgTurnovers’: 13.173780487804878}, {‘Team’: ‘CHI’, ‘AvgTurnovers’: 13.195121951219512}, {‘Team’: ‘CLE’, ‘AvgTurnovers’: 13.25}, {‘Team’: ‘MIA’, ‘AvgTurnovers’: 13.341463414634147}, {‘Team’: ‘POR’, ‘AvgTurnovers’: 13.36890243902439}, {‘Team’: ‘IND’, ‘AvgTurnovers’: 13.426829268292684}, {‘Team’: ‘MIN’, ‘AvgTurnovers’: 13.448170731707316}, {‘Team’: ‘NYK’, ‘AvgTurnovers’: 13.451219512195122}, {‘Team’: ‘ORL’, ‘AvgTurnovers’: 13.676829268292684}, {‘Team’: ‘UTA’, ‘AvgTurnovers’: 13.850609756097562}, {‘Team’: ‘LAL’, ‘AvgTurnovers’: 13.875}, {‘Team’: ‘WAS’, ‘AvgTurnovers’: 13.939024390243903}, {‘Team’: ‘DEN’, ‘AvgTurnovers’: 14.195121951219512}, {‘Team’: ‘MIL’, ‘AvgTurnovers’: 14.277439024390244}, {‘Team’: ‘OKC’, ‘AvgTurnovers’: 14.390243902439025}, {‘Team’: ‘BRK’, ‘AvgTurnovers’: 14.46951219512195}, {‘Team’: ‘ATL’, ‘AvgTurnovers’: 14.551829268292684}, {‘Team’: ‘GSW’, ‘AvgTurnovers’: 14.59451219512195}, {‘Team’: ‘SAC’, ‘AvgTurnovers’: 14.615853658536585}, {‘Team’: ‘HOU’, ‘AvgTurnovers’: 14.713414634146341}, {‘Team’: ‘PHO’, ‘AvgTurnovers’: 15.268292682926829}, {‘Team’: ‘PHI’, ‘AvgTurnovers’: 16.085365853658537}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.859579
</td>
<td>
0.018256
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
7
</th>
<td>
b0297b56-c7be-48e4-972b-80d831eee171
</td>
<td>
1000193295096855168
</td>
<td>
Which team has the highest average points per game?
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TeamPoints) as AveragePoints FROM nba GROUP BY Team ORDER BY AveragePoints DESC LIMIT 1;’, ‘results’: [{‘Team’: ‘GSW’, ‘AveragePoints’: 113.54878048780488}]}
</td>
<td>
{‘error’: None, ‘query’: ‘SELECT Team, AVG(TeamPoints) as AvgPoints FROM nba GROUP BY Team ORDER BY AvgPoints DESC;’, ‘results’: [{‘Team’: ‘GSW’, ‘AvgPoints’: 113.54878048780488}, {‘Team’: ‘HOU’, ‘AvgPoints’: 109.54268292682927}, {‘Team’: ‘LAC’, ‘AvgPoints’: 107.21951219512195}, {‘Team’: ‘CLE’, ‘AvgPoints’: 107.16463414634147}, {‘Team’: ‘OKC’, ‘AvgPoints’: 107.15548780487805}, {‘Team’: ‘TOR’, ‘AvgPoints’: 106.30182926829268}, {‘Team’: ‘DEN’, ‘AvgPoints’: 106.26829268292683}, {‘Team’: ‘POR’, ‘AvgPoints’: 105.375}, {‘Team’: ‘BOS’, ‘AvgPoints’: 104.77743902439025}, {‘Team’: ‘WAS’, ‘AvgPoints’: 104.60060975609755}, {‘Team’: ‘NOP’, ‘AvgPoints’: 104.53353658536585}, {‘Team’: ‘MIN’, ‘AvgPoints’: 103.8140243902439}, {‘Team’: ‘PHO’, ‘AvgPoints’: 103.72256097560975}, {‘Team’: ‘SAS’, ‘AvgPoints’: 103.6951219512195}, {‘Team’: ‘ATL’, ‘AvgPoints’: 102.97560975609755}, {‘Team’: ‘CHO’, ‘AvgPoints’: 102.66768292682927}, {‘Team’: ‘IND’, ‘AvgPoints’: 102.53658536585365}, {‘Team’: ‘SAC’, ‘AvgPoints’: 102.39024390243902}, {‘Team’: ‘BRK’, ‘AvgPoints’: 102.25914634146342}, {‘Team’: ‘LAL’, ‘AvgPoints’: 102.10975609756098}, {‘Team’: ‘CHI’, ‘AvgPoints’: 102.0579268292683}, {‘Team’: ‘DAL’, ‘AvgPoints’: 101.9359756097561}, {‘Team’: ‘MIL’, ‘AvgPoints’: 101.7469512195122}, {‘Team’: ‘DET’, ‘AvgPoints’: 101.39024390243902}, {‘Team’: ‘ORL’, ‘AvgPoints’: 100.5579268292683}, {‘Team’: ‘PHI’, ‘AvgPoints’: 100.40853658536585}, {‘Team’: ‘MIA’, ‘AvgPoints’: 100.32926829268293}, {‘Team’: ‘NYK’, ‘AvgPoints’: 99.76219512195122}, {‘Team’: ‘UTA’, ‘AvgPoints’: 99.41768292682927}, {‘Team’: ‘MEM’, ‘AvgPoints’: 99.3048780487805}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
5.232591
</td>
<td>
0.018000
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
8
</th>
<td>
50206d55-2463-4e18-8f59-6caebf735759
</td>
<td>
1000193295097051776
</td>
<td>
Which team has the most wins?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}, {‘Team’: ‘SAS’, ‘Wins’: 230}, {‘Team’: ‘HOU’, ‘Wins’: 217}, {‘Team’: ‘TOR’, ‘Wins’: 215}, {‘Team’: ‘CLE’, ‘Wins’: 211}, {‘Team’: ‘LAC’, ‘Wins’: 202}, {‘Team’: ‘BOS’, ‘Wins’: 196}, {‘Team’: ‘OKC’, ‘Wins’: 195}, {‘Team’: ‘POR’, ‘Wins’: 185}, {‘Team’: ‘WAS’, ‘Wins’: 179}, {‘Team’: ‘UTA’, ‘Wins’: 177}, {‘Team’: ‘ATL’, ‘Wins’: 175}, {‘Team’: ‘IND’, ‘Wins’: 173}, {‘Team’: ‘MIA’, ‘Wins’: 170}, {‘Team’: ‘MEM’, ‘Wins’: 162}, {‘Team’: ‘CHI’, ‘Wins’: 160}, {‘Team’: ‘MIL’, ‘Wins’: 160}, {‘Team’: ‘NOP’, ‘Wins’: 157}, {‘Team’: ‘CHO’, ‘Wins’: 153}, {‘Team’: ‘DET’, ‘Wins’: 152}, {‘Team’: ‘DEN’, ‘Wins’: 149}, {‘Team’: ‘DAL’, ‘Wins’: 149}, {‘Team’: ‘MIN’, ‘Wins’: 123}, {‘Team’: ‘SAC’, ‘Wins’: 121}, {‘Team’: ‘ORL’, ‘Wins’: 114}, {‘Team’: ‘NYK’, ‘Wins’: 109}, {‘Team’: ‘PHI’, ‘Wins’: 108}, {‘Team’: ‘PHO’, ‘Wins’: 107}, {‘Team’: ‘BRK’, ‘Wins’: 107}, {‘Team’: ‘LAL’, ‘Wins’: 99}]}
</td>
<td>
None
</td>
<td>
{‘correct_result’: 0.033333333333333, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
7.731482
</td>
<td>
0.018153
</td>
<td>
{‘category’: ‘Generated’}
</td>
</tr>
<tr>
<th>
9
</th>
<td>
80396d4b-0cba-490c-b8cb-aad792edef6d
</td>
<td>
1000193295096986240
</td>
<td>
Which team won the most games?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: [{‘Team’: ‘GSW’, ‘Wins’: 265}]}
</td>
<td>
[]
</td>
<td>
{‘correct_result’: 1, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
7.293730
</td>
<td>
0.018269
</td>
<td>
{‘category’: ‘Golden data’}
</td>
</tr>
<tr>
<th>
10
</th>
<td>
81ab8f36-c7b9-4598-9ee9-10552bfebd86
</td>
<td>
1000193295096920704
</td>
<td>
Which team won the most games in 2015?
</td>
<td>
{‘error’: None, ‘query’: ’SELECT Team, COUNT(*) as Wins FROM nba WHERE WINorLOSS = ‘W’ AND Date LIKE ‘2015%’ GROUP BY Team ORDER BY Wins DESC LIMIT 1;‘, ’results’: []}
</td>
<td>
{‘error’: ’Parser Error: syntax error at or near “<code>"', 'query': '</code>sql SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = ‘W’ AND Date LIKE ‘%/15’ GROUP BY Team ORDER BY Wins DESC LIMIT 1; ```‘, ’results’: None}
</td>
<td>
[]
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: 1, ‘no_error’: 1}
</td>
<td>
6.607797
</td>
<td>
0.018675
</td>
<td>
{‘category’: ‘Golden data’}
</td>
</tr>
<tr>
<th>
11
</th>
<td>
b4931e7a-e5c4-450d-82bb-78a81b40fbf5
</td>
<td>
1000193295096920704
</td>
<td>
What is the average number of free throws per year?
</td>
<td>
<p>{‘error’: ‘Parser Error: syntax error at or near “To”’, ‘query’: ‘To answer this question, we first need to convert the ’Date’ column from VARCHAR to DATE type. However, DuckDB SQL does not support date conversion directly. Therefore, we need to use a workaround to extract the year from the ‘Date’ column.</p>
<p>Here is the SQL query:</p>
<div class="sourceCode" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb26-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">SELECT</span> </span>
<span id="cb26-2">    SUBSTRING(<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Date</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Year</span>, </span>
<span id="cb26-3">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">AVG</span>(FreeThrows) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> AverageFreeThrows</span>
<span id="cb26-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FROM</span> </span>
<span id="cb26-5">    nba</span>
<span id="cb26-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">GROUP</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> </span>
<span id="cb26-7">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Year</span></span></code></pre></div>
This query first extracts the year from the ‘Date’ column, then calculates the average number of free throws for each year.’, ‘results’: None}
</td>
<td>
None
</td>
<td>
None
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: None, ‘no_error’: 0}
</td>
<td>
5.591774
</td>
<td>
0.023460
</td>
<td>
{‘category’: ‘Handwritten question’}
</td>
</tr>
<tr>
<th>
12
</th>
<td>
0cd008a6-53d3-4940-8022-ecbe9e77bdc9
</td>
<td>
1000193295097182848
</td>
<td>
Which team had the biggest difference in records across two consecutive years?
</td>
<td>
<p>{‘error’: ‘Parser Error: syntax error at or near “To”’, ‘query’: ’To answer this question, we first need to calculate the win-loss record for each team for each year. However, the provided table schema does not include a year column. Assuming the Date column is in the format MM/DD/YY, we can extract the year from the Date column. Then, we can calculate the win-loss record for each team for each year. Finally, we can calculate the difference in records across two consecutive years and find the team with the biggest difference.</p>
<p>Here is the SQL query:</p>
<div class="sourceCode" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb27-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">WITH</span> yearly_records <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> (</span>
<span id="cb27-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">SELECT</span> </span>
<span id="cb27-3">        Team, </span>
<span id="cb27-4">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">SUBSTR</span>(<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Date</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Year</span>, </span>
<span id="cb27-5">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">COUNT</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FILTER</span> (<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">WHERE</span> WINorLOSS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'W'</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> Wins, </span>
<span id="cb27-6">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">COUNT</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FILTER</span> (<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">WHERE</span> WINorLOSS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'L'</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> Losses</span>
<span id="cb27-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FROM</span> </span>
<span id="cb27-8">        nba </span>
<span id="cb27-9">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">GROUP</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> </span>
<span id="cb27-10">        Team, <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Year</span></span>
<span id="cb27-11">), yearly_diff <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> (</span>
<span id="cb27-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">SELECT</span> </span>
<span id="cb27-13">        Team, </span>
<span id="cb27-14">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ABS</span>((Wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> Losses) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">LAG</span>(Wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> Losses) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">OVER</span> (<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">PARTITION</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> Team <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">ORDER</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">Year</span>)) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">AS</span> Record_Difference</span>
<span id="cb27-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FROM</span> </span>
<span id="cb27-16">        yearly_records</span>
<span id="cb27-17">)</span>
<span id="cb27-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">SELECT</span> </span>
<span id="cb27-19">    Team </span>
<span id="cb27-20"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">FROM</span> </span>
<span id="cb27-21">    yearly_diff </span>
<span id="cb27-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">ORDER</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">BY</span> </span>
<span id="cb27-23">    Record_Difference <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">DESC</span> </span>
<span id="cb27-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">LIMIT</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>;</span></code></pre></div>
This query first calculates the number of wins and losses for each team for each year. Then, it calculates the difference in records (wins - losses) across two consecutive years for each team. Finally, it returns the team with the biggest difference in records.’, ‘results’: None}
</td>
<td>
None
</td>
<td>
None
</td>
<td>
{‘correct_result’: None, ‘correct_sql’: None, ‘no_error’: 0}
</td>
<td>
10.472002
</td>
<td>
0.034380
</td>
<td>
{‘category’: ‘Handwritten question’}
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="analyze-regressions" class="level4">
<h4 class="anchored" data-anchor-id="analyze-regressions">Analyze Regressions</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/images/analyze-regressions.gif" class="img-fluid figure-img"></p>
<figcaption>diff</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<ul>
<li><strong>Specialized Functionality for Text2SQL and Other Tasks</strong>
<ul>
<li>Braintrust prioritizes quality over quantity in scoring functions.</li>
<li>Provides ~20 scoring functions optimized for tasks like retrieval augmented generation (RAG).</li>
<li>Includes an implementation of the RAGAS metrics with improvements.</li>
<li>Offers tools for evaluating tool call outputs, particularly relevant for agentic workflows.</li>
<li>Provides building blocks like list-of-strings comparators with various comparison methods.</li>
</ul></li>
<li><strong>Using Open-Source or Open-Weight Models</strong>
<ul>
<li>Braintrust is agnostic to the underlying LLM.</li>
<li>Data generation and scoring functions are independent of the LLM used.</li>
<li>The task function can interface with any model via API; OpenAI compatibility is not required.</li>
<li>Open-weight models hosted on platforms like Together often have OpenAI-compatible APIs.</li>
<li>Braintrust’s proxy allows for local model hosting and integration.</li>
</ul></li>
<li><strong>Notebook Sharing</strong>
<ul>
<li>The notebook used in the presentation will be shared in the Discord channel and the “Cookbook” section on the Braintrust website.</li>
</ul></li>
<li><strong>Task Suitability and Limitations</strong>
<ul>
<li>Braintrust might not be ideal for classical machine learning tasks involving large datasets and reliance on aggregate statistics.</li>
<li>The platform’s strength lies in facilitating analysis of individual examples, which is less relevant in those scenarios.</li>
</ul></li>
<li><strong>Braintrust for Model Development and Deployment</strong>
<ul>
<li>Braintrust supports both offline evaluations (during development) and online evaluations/observability (post-deployment).</li>
<li>Tight integration between logs and evaluations allows for capturing user feedback and incorporating it into the development process.</li>
<li>The platform provides tools for rendering various output formats, including HTML and images.</li>
</ul></li>
<li><strong>Handling Complex Databases with Limited Schema Knowledge</strong>
<ul>
<li><strong>Start with Instrumentation:</strong> Log user questions, system responses, and capture user feedback from the beginning.</li>
<li><strong>Develop a Taxonomy:</strong> Categorize questions based on their nature or the specific areas of the database they target.</li>
<li><strong>Focus on Subcategories:</strong> Identify question types the system handles well and those it struggles with.</li>
<li><strong>Iterate and Improve:</strong>
<ul>
<li>Concentrate on improving performance for specific question categories.</li>
<li>Consider schema modifications or view creation to simplify data access for the LLM.</li>
</ul></li>
<li><strong>Leverage User Feedback:</strong> Use thumbs-up/thumbs-down ratings or more detailed feedback to guide improvement efforts.</li>
</ul></li>
</ul>
</section>
<section id="recommendations" class="level2">
<h2 class="anchored" data-anchor-id="recommendations">Recommendations</h2>
<ul>
<li><strong>Adopt the three-component framework</strong> (data, task function, scoring) when building your LLM evaluations.</li>
<li><strong>Don’t overcomplicate initial evaluations.</strong> Start with hardcoded data and progressively enhance complexity.</li>
<li><strong>Actively use logs and user feedback</strong> to drive iterative improvements, especially when dealing with complex, unfamiliar databases.</li>
<li><strong>Explore the Braintrust platform</strong> and its features, including the cookbook and shared notebook, for practical implementation guidance.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-002/</guid>
  <pubDate>Sat, 29 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Workshop 3: Instrumenting &amp; Evaluating LLMs</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/workshop-003/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Introduction</li>
<li>Types of Evaluations</li>
<li>Looking At Your Data</li>
<li>Harrison Chase: Langsmith for Logging &amp; Tests</li>
<li>Bryan Bischof: Spellgrounds for Prodigious Prestidigitation</li>
<li>Eugene Yan: Evaluating LLM-Generated Summaries with Out-of-Domain Fine-tuning</li>
<li>Shreya Shankar: Scaling Up Vibe Checks for LLMs</li>
<li>Q&amp;A Session</li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li><strong>Importance of Evaluation:</strong> Evaluation is crucial for iteratively improving LLMs, whether through prompt engineering, fine-tuning, or other methods.</li>
<li><strong>Data Flywheel:</strong> A fast iteration cycle requires rapid feedback from evaluations, enabling you to experiment and improve your AI quickly.
<ul>
<li><strong>Blog Post:</strong> <a href="https://hamel.dev/blog/posts/evals/">Your AI Product Needs Evals</a></li>
</ul></li>
<li><strong>Applied AI:</strong> Evaluation and data analysis are key components of applied AI, allowing you to measure progress and make informed decisions.</li>
</ul>
</section>
<section id="types-of-evaluations" class="level2">
<h2 class="anchored" data-anchor-id="types-of-evaluations">Types of Evaluations</h2>
<ul>
<li><strong>Unit Tests:</strong>
<ul>
<li>Code-based tests that validate specific expectations about LLM responses.</li>
<li>Typically fast to run and can catch basic errors.</li>
</ul></li>
<li><strong>LLM as a Judge:</strong>
<ul>
<li>Using another LLM to evaluate the quality of the primary LLM’s response.</li>
<li>Can be efficient but requires careful alignment with human judgment.</li>
</ul></li>
<li><strong>Human Evaluation:</strong>
<ul>
<li>Direct human assessment of LLM output.</li>
<li>Considered the gold standard, but can be expensive and time-consuming.</li>
</ul></li>
</ul>
<section id="example-editing-out-stereotypes-in-academic-writing" class="level3">
<h3 class="anchored" data-anchor-id="example-editing-out-stereotypes-in-academic-writing">Example: Editing Out Stereotypes In Academic Writing</h3>
<ul>
<li><strong>Goal:</strong> Automate the process of identifying and removing subconscious biases and stereotypes from text.
<ul>
<li><strong>Original Text:</strong> “Norway’s mining economy flourished during the period due to Norwegian’s natural hardiness.”</li>
<li><strong>Desired Edit:</strong> Remove the stereotype of “Norwegian’s natural hardiness.”</li>
</ul></li>
<li><strong>Approach:</strong> Leverages the experience of a team that manually reviews and edits manuscripts for biases, highlighting the importance of considering existing workflows when designing evaluations.</li>
</ul>
</section>
<section id="unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="unit-tests">Unit Tests</h3>
<ul>
<li><p><strong>Purpose:</strong> First line of defense against basic errors in LLM output.</p></li>
<li><p><strong>Identifying Failure Modes:</strong> Even seemingly complex LLM tasks often have predictable failure modes that can be tested with code.</p></li>
<li><p><strong>Abstraction and Reusability:</strong> Unit tests should be abstracted and reusable, both during development and in production.</p></li>
<li><p><strong>Logging and Tracking:</strong> Log unit test results to a database or other system for tracking progress and identifying trends.</p></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pipeline, Pipeline</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pytest</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unit Tests</span></span>
<span id="cb1-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@pytest.fixture</span>(scope<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"module"</span>)</span>
<span id="cb1-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> llm_pipeline():</span>
<span id="cb1-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> pipeline(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text-generation"</span>, model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meta-llama/Llama-2-7b-chat-hf"</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> verify_answer_contains(p: Pipeline, query: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, expected: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>):</span>
<span id="cb1-10">    result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> p(query, do_sample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, return_full_text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"generated_text"</span>]</span>
<span id="cb1-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> expected <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> result, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"The result does not contain '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>expected<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'"</span></span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> test_google_ceo(llm_pipeline):</span>
<span id="cb1-14">    verify_answer_contains(llm_pipeline, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Who is the CEO of Google?"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sundar Pichai"</span>)</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> test_2_plus_3(llm_pipeline):</span>
<span id="cb1-17">    verify_answer_contains(llm_pipeline, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is 2+3?"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"5"</span>)</span></code></pre></div>
</div>
</div>
</div></li>
</ul>
<section id="generate-data-for-each-scenario" class="level4">
<h4 class="anchored" data-anchor-id="generate-data-for-each-scenario">Generate Data For Each Scenario</h4>
<ul>
<li><strong>Feature and Scenario Breakdown:</strong> Break down the LLM application into features and scenarios to systematically generate test data.</li>
<li><strong>Example:</strong> A real estate CRM application with features like finding listings, each with scenarios like finding one listing, multiple listings, or no listings.</li>
</ul>
</section>
<section id="use-llms-to-synthetically-generate-inputs-to-the-system" class="level4">
<h4 class="anchored" data-anchor-id="use-llms-to-synthetically-generate-inputs-to-the-system">Use LLMs to synthetically generate inputs to the system</h4>
<ul>
<li><p><strong>Synthetic Data Generation:</strong> Use LLMs to generate synthetic test data for various scenarios, especially when real user data is limited.</p></li>
<li><p><strong>Example:</strong> Generating synthetic real estate agent instructions for testing a CMA (comparative market analysis) feature.</p>
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code>Write an instruction that a real estate agent can give to his assistant to create CMA's for him. The results should be a string containing the instruction like so:

```json
[
  "Create a CMA for 2430 Victory Park"
]
```

If you need a listing you can use any of the following:

&lt;SELECT address FROM listings_filters;&gt; (From minimal database)</code></pre>
</div>
</div>
</div></li>
</ul></li>
</ul>
</section>
<section id="log-results-to-database-and-visualize" class="level4">
<h4 class="anchored" data-anchor-id="log-results-to-database-and-visualize">Log Results to Database and Visualize</h4>
<ul>
<li><p>Use existing tools to systematically track unit test results to monitor progress and identify areas for improvement.</p>
<ul>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code>----------
Website - one-listing-found Results
Success: 0 Fail 26 Total 26 Average Duration: 7
Technical Tokens Leaked: 26
----------
Website - multiple-listings-found Results
Success: 0 Fail 22 Total 22 Average Duration: 4
Unknown: 1
Exposed UUIDs: 17
Failed to format JSON Output: 4
----------
Website - no-listing-found Results
Success: 25 Fail 1 Total 26 Average Duration: 3
Unknown: 1
----------</code></pre>
</div>
</div>
</div></li>
</ul></li>
<li><p>Use bar charts or other visualizations to track error rates across different scenarios and iterations.</p></li>
</ul>
</section>
<section id="unit-test-considerations" class="level4">
<h4 class="anchored" data-anchor-id="unit-test-considerations">Unit Test Considerations</h4>
<ul>
<li><strong>Strict vs.&nbsp;Leaderboard Approach:</strong>
<ul>
<li><strong>Strict:</strong> All unit tests must pass; otherwise, the pipeline is halted.</li>
<li><strong>Leaderboard:</strong> Track the number of passing tests over iterations to measure progress.</li>
</ul></li>
<li><strong>Use Case Specificity:</strong>
<ul>
<li><strong>Public-facing products:</strong> Prioritize tests that prevent data leaks and ensure user privacy.</li>
<li><strong>Internal tools:</strong> Focus on identifying major issues, as minor errors might be less critical.</li>
</ul></li>
</ul>
</section>
</section>
<section id="llm-as-a-judge" class="level3">
<h3 class="anchored" data-anchor-id="llm-as-a-judge">LLM as a Judge</h3>
<ul>
<li><strong>Alignment with Human Standard:</strong> LLM as a judge must be aligned with a trusted human standard to ensure reliable evaluation.</li>
<li><strong>Iterative Alignment:</strong> Continuously measure and improve the agreement between LLM as a judge and human evaluation.</li>
<li><strong>Tips:</strong> Use a powerful LLM, treat the judge as a mini-evaluation system, and periodically re-align with human judgment.</li>
</ul>
<section id="llm-as-a-judge-example-de-biasing-text-project" class="level4">
<h4 class="anchored" data-anchor-id="llm-as-a-judge-example-de-biasing-text-project">LLM-As-A-Judge Example: De-biasing Text Project</h4>
<ul>
<li><strong>Challenge:</strong> Lack of transitivity in LLM as a judge’s evaluation, leading to unreliable results.</li>
<li><strong>Solution:</strong> Relying on human evaluation due to the limitations of LLM as a judge in this specific use case.</li>
</ul>
</section>
</section>
<section id="human-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="human-evaluation">Human Evaluation</h3>
<ul>
<li><strong>Importance:</strong> Human evaluation is always necessary, even when using other evaluation methods, to ensure alignment and prevent over-fitting.</li>
<li><strong>Regular Data Analysis:</strong> Continuously analyze human evaluations to identify patterns, biases, and areas for improvement.</li>
<li><strong>Balancing Cost and Accuracy:</strong> Determine the appropriate level of human evaluation based on project constraints and desired accuracy.</li>
</ul>
</section>
<section id="what-worked" class="level3">
<h3 class="anchored" data-anchor-id="what-worked">What Worked</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 42%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Writing Queries</th>
<th>Debiasing Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unit Tests</td>
<td>Good</td>
<td>Too Rigid</td>
</tr>
<tr class="even">
<td>LLM as a judge</td>
<td>Pretty Good</td>
<td>Not transitive</td>
</tr>
<tr class="odd">
<td>Human Evaluation</td>
<td>Some labor required, aided by LLM as a judge</td>
<td>Labor intensive, which was ok for this task</td>
</tr>
</tbody>
</table>
</section>
<section id="evaluation-workflow" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-workflow">Evaluation Workflow</h3>
<ul>
<li><strong>Iterative Feedback Loop:</strong> Evaluation enables a fast feedback loop for prompt engineering, fine-tuning, and other improvements.</li>
<li><strong>Hidden Complexity:</strong> Building effective evaluation systems is not trivial and requires careful consideration of various factors.</li>
</ul>
</section>
<section id="human-eval-going-wrong-in-alt-text-project" class="level3">
<h3 class="anchored" data-anchor-id="human-eval-going-wrong-in-alt-text-project">Human Eval Going Wrong in Alt Text Project</h3>
<ul>
<li><strong>Challenge:</strong> Human evaluators’ standards can drift over time, leading to misleading results, even with a fixed rubric.</li>
<li><strong>Example:</strong> Alt text generation project where human evaluators’ standards increased as they saw better models, making later models appear worse than they actually were.</li>
</ul>
<section id="ab-testing" class="level4">
<h4 class="anchored" data-anchor-id="ab-testing">A/B Testing</h4>
<ul>
<li><strong>Solution:</strong> A/B testing can control for changes in human judgment over time by randomly assigning evaluators to different models.</li>
<li><strong>Limitations:</strong> A/B testing requires sufficient data and human labelers, making it impractical for early-stage projects.</li>
</ul>
</section>
</section>
</section>
<section id="looking-at-your-data" class="level2">
<h2 class="anchored" data-anchor-id="looking-at-your-data">Looking At Your Data</h2>
<ul>
<li><strong>Crucial Importance:</strong> Looking at your data is essential for understanding LLM behavior, identifying failure modes, and improving evaluation methods.</li>
<li><strong>Common Pitfall:</strong> Many practitioners do not look at their data enough, even when they think they do.</li>
</ul>
<section id="what-is-a-trace" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-trace">What is a Trace?</h3>
<ul>
<li><strong>Definition:</strong> A trace is a sequence of events in an LLM pipeline, such as multi-turn conversations, RAG processes, or function calls.</li>
<li><strong>Importance:</strong> Traces are valuable for debugging, fine-tuning, and understanding LLM behavior.</li>
<li><strong>Representation:</strong> Traces are often represented as JSON-L files, but other formats are possible.</li>
</ul>
</section>
<section id="remove-all-friction-from-looking-at-your-data" class="level3">
<h3 class="anchored" data-anchor-id="remove-all-friction-from-looking-at-your-data">Remove All Friction from Looking at Your Data</h3>
<ul>
<li><strong>Ease of Access:</strong> Make it easy to access, filter, and navigate your data to encourage regular inspection.</li>
<li><strong>Custom Tools:</strong> Consider building custom tools using Shiny, Gradio, Streamlit, or other frameworks to streamline data exploration.
<ul>
<li><strong><a href="https://github.com/parlance-labs/langfree">langefree</a>:</strong> Tools for extraction, transformation, and curation of <code>ChatOpenAI</code> runs from LangSmith.</li>
</ul></li>
<li><strong>Key Considerations:</strong> Ensure that your tools remove enough friction and provide the necessary information for effective analysis.</li>
</ul>
</section>
<section id="rendering-logging-traces" class="level3">
<h3 class="anchored" data-anchor-id="rendering-logging-traces">Rendering &amp; Logging Traces</h3>
<ul>
<li><strong>Tools:</strong> Use tools like LangSmith, IdenticLogFire, Braintrust, Weights&amp;Biases Weave, OpenLLMetry, and Instruct to log and render traces.
<ul>
<li><strong>Commercial:</strong>
<ul>
<li><a href="https://smith.langchain.com/">Langsmith</a></li>
<li><a href="https://pydantic.dev/logfire">Pydantic LogFire</a></li>
<li><a href="https://www.braintrustdata.com/">BrainTrust</a></li>
<li><a href="https://wandb.ai/site/weave">W&amp;B Weave</a></li>
</ul></li>
<li><strong>OSS:</strong>
<ul>
<li><a href="https://ukgovernmentbeis.github.io/inspect_ai/workflow.html">Instruct</a></li>
<li><a href="https://github.com/traceloop/openllmetry">OpenLLMetry</a></li>
</ul></li>
</ul></li>
<li><strong>LangSmith:</strong> A platform for logging, testing, and visualizing LLM pipelines, with features like trace rendering, filtering, and feedback integration.</li>
</ul>
</section>
<section id="its-best-to-use-a-tool" class="level3">
<h3 class="anchored" data-anchor-id="its-best-to-use-a-tool">It’s Best to Use a Tool</h3>
<ul>
<li><strong>Off-the-Shelf Solutions:</strong> Leverage existing tools for logging traces and other evaluation tasks to focus on data analysis and model improvement.</li>
<li><strong>Tool Exploration:</strong> Explore the various tools available through workshops, office hours, and other resources to find the best fit for your needs.</li>
</ul>
</section>
</section>
<section id="harrison-chase-langsmith-for-logging-tests" class="level2">
<h2 class="anchored" data-anchor-id="harrison-chase-langsmith-for-logging-tests">Harrison Chase: Langsmith for Logging &amp; Tests</h2>
<ul>
<li><a href="https://www.langchain.com/langsmith">Langsmith Website</a></li>
<li><a href="https://docs.smith.langchain.com/">Langsmith Docs</a></li>
</ul>
<section id="langsmith-features" class="level3">
<h3 class="anchored" data-anchor-id="langsmith-features">LangSmith Features</h3>
<ul>
<li><strong>Data Visualization and Analysis:</strong> Log, visualize, and analyze interactions with your LLM applications, enabling deep dives into individual runs and identification of potential issues.</li>
<li><strong>Dataset Management and Testing:</strong> Create, manage, and test your LLM applications against diverse datasets, facilitating targeted improvements and robust evaluation.</li>
<li><strong>Experiment Tracking and Comparison:</strong> Track experiment results over time, compare different model versions, and gain insights into performance changes.</li>
<li><strong>Leveraging LLM as a Judge:</strong> Utilize LLMs for automated evaluation, streamline feedback loops, and align LLM judgments with human preferences.</li>
<li><strong>Human-in-the-Loop Feedback:</strong> Integrate human feedback seamlessly through annotation queues, enabling continuous improvement and refinement of your LLM applications.</li>
</ul>
</section>
<section id="observability-looking-at-your-data" class="level3">
<h3 class="anchored" data-anchor-id="observability-looking-at-your-data">Observability: Looking at Your Data</h3>
<ul>
<li><strong>Integration:</strong> Langsmith integrates with Langchain via environment variables and offers various entry points for non-Langchain users, including decorators, direct span logging, and project-based organization.</li>
<li><strong>Data Visualization:</strong> Provides an interface to visualize logged data, including system messages, human and AI interactions, outputs, and relevant documents, all presented in an easily digestible format.</li>
<li><strong>Transition to Playground:</strong> Allows direct navigation from a specific trace to a playground environment, facilitating rapid iteration and prompt modification.</li>
</ul>
<section id="filtering-and-dissecting-data" class="level4">
<h4 class="anchored" data-anchor-id="filtering-and-dissecting-data">Filtering and Dissecting Data</h4>
<ul>
<li><strong>Filtering Capabilities:</strong> Offers robust filtering options based on errors, latency, status, tags (e.g., LLM provider), and user feedback, enabling focused analysis of specific data subsets.</li>
<li><strong>Aggregate Statistics:</strong> Provides aggregated statistics over time, allowing for the identification of trends and patterns in application performance.</li>
<li><strong>A/B Testing and Metadata Grouping:</strong> Enables A/B testing by grouping statistics based on metadata, such as LLM provider, to compare performance across different models or configurations.</li>
</ul>
</section>
</section>
<section id="datasets-and-testing" class="level3">
<h3 class="anchored" data-anchor-id="datasets-and-testing">Datasets and Testing</h3>
<ul>
<li><strong>Dataset Creation:</strong> Supports manual example uploads, imports from existing traces (e.g., failed interactions), and the organization of data into distinct splits for targeted testing.</li>
<li><strong>Split Testing:</strong> Allows for the evaluation of LLM applications on specific data splits, enabling focused analysis and improvement of performance in identified problem areas.</li>
</ul>
<section id="tracking-experiments-over-time" class="level4">
<h4 class="anchored" data-anchor-id="tracking-experiments-over-time">Tracking Experiments Over Time</h4>
<ul>
<li><strong>Experiment Tracking:</strong> Automatically logs and displays experiment results, including metrics and performance over time, allowing for monitoring and identification of regressions.</li>
<li><strong>Experiment Comparison:</strong> Provides an interface to compare two or more experiments side-by-side, highlighting performance differences and facilitating detailed analysis of specific cases.</li>
</ul>
</section>
</section>
<section id="llm-as-a-judge-1" class="level3">
<h3 class="anchored" data-anchor-id="llm-as-a-judge-1">LLM as a Judge</h3>
<ul>
<li><strong>Automated Evaluation:</strong> Supports the use of LLMs as judges for automated evaluation, streamlining the feedback process and reducing reliance on manual review.</li>
<li><strong>Off-the-Shelf and Custom Evaluators:</strong> Offers both pre-built evaluation prompts and the flexibility to define custom evaluation functions, catering to diverse use cases.</li>
<li><strong>Aligning Human Preferences:</strong> Facilitates the alignment of LLM judgments with human preferences through few-shot learning and an upcoming correction flow feature, enabling continuous improvement of evaluation accuracy.</li>
</ul>
</section>
<section id="human-in-the-loop-feedback" class="level3">
<h3 class="anchored" data-anchor-id="human-in-the-loop-feedback">Human-in-the-Loop Feedback</h3>
<ul>
<li><strong>Annotation Queues:</strong> Provides annotation queues for efficient human feedback collection, allowing for the review, labeling, and categorization of data points to improve model performance.</li>
<li><strong>Collaborative Features:</strong> Includes features for adding notes, marking completion status, and collaborating on data annotation tasks, fostering teamwork and efficient feedback integration.</li>
</ul>
</section>
</section>
<section id="bryan-bischof-spellgrounds-for-prodigious-prestidigitation" class="level2">
<h2 class="anchored" data-anchor-id="bryan-bischof-spellgrounds-for-prodigious-prestidigitation">Bryan Bischof: Spellgrounds for Prodigious Prestidigitation</h2>
<ul>
<li><strong>Spellgrounds:</strong> An internal library for developing and running evaluations, combining systematic and use-case-specific approaches.</li>
<li><strong>Opinionated View on Evals:</strong> Evals should help determine product readiness, ensure system reliability, and aid in debugging.</li>
<li><a href="https://docs.google.com/presentation/d/1GC868XXjhxOpQEt1jUM79aW0RHjzxPp0XhpFHnYH760/edit#slide=id.p">Google Slides</a></li>
<li><a href="https://hex.tech/product/magic-ai/">hex.tech</a></li>
</ul>
<section id="preamble" class="level3">
<h3 class="anchored" data-anchor-id="preamble">Preamble</h3>
<ul>
<li><strong>Three Purposes of Evals:</strong>
<ul>
<li>Determine product readiness.</li>
<li>Ensure system reliability.</li>
<li>Aid in debugging.</li>
</ul></li>
<li><strong>Evals as Data Science:</strong> LLM evaluations are not entirely new and should leverage existing data science principles and techniques.</li>
</ul>
</section>
<section id="miscats-and-fizzled-spells-things-to-avoid" class="level3">
<h3 class="anchored" data-anchor-id="miscats-and-fizzled-spells-things-to-avoid">Miscats and Fizzled Spells: Things to avoid</h3>
<section id="thinking-llm-evaluations-are-entirely-new" class="level4">
<h4 class="anchored" data-anchor-id="thinking-llm-evaluations-are-entirely-new">Thinking LLM Evaluations Are Entirely New</h4>
<ul>
<li><strong>Leverage Existing Expertise:</strong> Data scientists have extensive experience in evaluating unpredictable outputs and mapping user problems to objective functions.</li>
<li><strong>Examples:</strong>
<ul>
<li><strong>Code generation:</strong> Execution evaluation.</li>
<li><strong>Agents:</strong> Planning as binary classification.</li>
<li><strong>Summarization:</strong> Retrieval accuracy.</li>
</ul></li>
</ul>
</section>
<section id="failing-to-include-use-case-experts" class="level4">
<h4 class="anchored" data-anchor-id="failing-to-include-use-case-experts">Failing to Include Use-Case Experts</h4>
<ul>
<li><strong>Expert Input:</strong> Users and domain experts provide valuable insights into what constitutes good output for specific use cases.</li>
<li><strong>Example:</strong> Collaborating with data scientists to define ideal chart outputs for an LLM-powered data visualization tool.</li>
</ul>
</section>
<section id="waiting-too-long-to-make-evaluations" class="level4">
<h4 class="anchored" data-anchor-id="waiting-too-long-to-make-evaluations">Waiting Too Long to Make Evaluations</h4>
<ul>
<li><strong>Early Integration:</strong> Evals should be part of the development cycle from the beginning, including RFC creation and design discussions.</li>
</ul>
</section>
<section id="not-recognizing-product-metrics-vs.-evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="not-recognizing-product-metrics-vs.-evaluation-metrics">Not Recognizing Product Metrics vs.&nbsp;Evaluation Metrics</h4>
<ul>
<li><strong>Distinct but Related:</strong>
<ul>
<li>Product metrics track overall system performance, while evaluation metrics assess specific LLM components or functionalities.</li>
<li>Product metrics provide valuable insights for designing evals, but they are not sufficient for comprehensive evaluation.</li>
</ul></li>
<li><strong>Custom Environments:</strong> Create custom datasets and environments that reflect real-world use cases, even when access to production data is limited.</li>
</ul>
</section>
<section id="buying-an-evaluation-framework-doesnt-make-it-easy" class="level4">
<h4 class="anchored" data-anchor-id="buying-an-evaluation-framework-doesnt-make-it-easy">Buying an Evaluation Framework Doesn’t Make It Easy</h4>
<ul>
<li><strong>Focus on Fundamentals:</strong> The hard part of evals is understanding user stories and input diversity, not the framework itself.</li>
<li><strong>Jupyter Notebooks:</strong> Jupyter Notebooks are powerful tools for interactive data exploration and evaluation.</li>
<li><strong>Invest Wisely:</strong> Prioritize understanding user needs and building effective evals before investing in complex frameworks.</li>
</ul>
</section>
<section id="reacing-too-early-for-llm-assisted-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="reacing-too-early-for-llm-assisted-evaluation">Reacing Too Early for LLM-Assisted Evaluation</h4>
<ul>
<li><strong>LLM Judging as a Tool:</strong> LLM judging can be valuable for scaling evaluations and identifying potential issues, but it is not a replacement for human judgment.</li>
<li><strong>Systematic Approach:</strong>
<ul>
<li>Establish a solid foundation of traditional evaluations before incorporating LLM-assisted methods.</li>
<li>Use multiple judges and periodically check for alignment with human evaluation.</li>
</ul></li>
</ul>
</section>
</section>
<section id="moderating-magic-how-to-build-your-eval-system" class="level3">
<h3 class="anchored" data-anchor-id="moderating-magic-how-to-build-your-eval-system">Moderating Magic: How to build your eval system</h3>
<section id="magic" class="level4">
<h4 class="anchored" data-anchor-id="magic">Magic</h4>
<ul>
<li><strong>Definition:</strong> An AI copilot for data science that generates code, reacts to edits, and creates visualizations.</li>
<li><a href="https://hex.tech/product/magic-ai/">Product Page</a></li>
</ul>
</section>
<section id="rag-evals" class="level4">
<h4 class="anchored" data-anchor-id="rag-evals">RAG Evals</h4>
<ul>
<li><strong>Treat RAG as Retrieval:</strong> Evaluate RAG systems like traditional retrieval systems, focusing on hit rate and relevance.</li>
<li><strong>Baselines and Calibration:</strong> Establish clear baselines and avoid treating retrieval scores as absolute confidence estimates.</li>
</ul>
</section>
<section id="planning-evals" class="level4">
<h4 class="anchored" data-anchor-id="planning-evals">Planning Evals</h4>
<ul>
<li><strong>State Machine as Classifier:</strong> Evaluate agent planning as a binary classification task, checking the correctness of each step.</li>
<li><strong>Prompt Quality:</strong> Evaluate the quality of downstream prompts generated by the planning stage, as they can be suboptimal.</li>
</ul>
</section>
<section id="agent-specific-evals" class="level4">
<h4 class="anchored" data-anchor-id="agent-specific-evals">Agent-Specific Evals</h4>
<ul>
<li><strong>Structured Output:</strong> Encourage and evaluate the use of structured output from agents to facilitate integration and consistency.</li>
<li><strong>API Interfaces:</strong> Design tightly coupled API interfaces between agent components and evaluate their consistency.</li>
</ul>
</section>
<section id="final-stage-evals" class="level4">
<h4 class="anchored" data-anchor-id="final-stage-evals">Final Stage Evals</h4>
<ul>
<li><strong>Topic:</strong> Evaluating the final output or summary generated by an agent chain.</li>
<li><strong>Recommendation:</strong> Ensure the summary accurately reflects the agent’s actions and avoid providing excessive context that can introduce noise.</li>
</ul>
</section>
<section id="experiments-are-repeated-measure-designs" class="level4">
<h4 class="anchored" data-anchor-id="experiments-are-repeated-measure-designs">Experiments are Repeated-Measure Designs</h4>
<ul>
<li><strong>Treat Updates as Experiments:</strong> Evaluate the impact of updates and bug fixes as experiments, measuring significance and comparing to historical data.</li>
<li><strong>Production Event Reruns:</strong> Rerun historical production events through updated models and use automated evals to assess improvements.</li>
</ul>
</section>
<section id="production-endpoints-minimize-drift" class="level4">
<h4 class="anchored" data-anchor-id="production-endpoints-minimize-drift">Production Endpoints Minimize Drift</h4>
<ul>
<li><strong>Direct Connection:</strong> Connect your evals framework directly to your production environment to minimize drift and ensure consistency.</li>
<li><strong>Endpoint Exposure:</strong> Expose each step of the production workflow as an endpoint to facilitate testing and debugging.</li>
</ul>
</section>
</section>
<section id="qa" class="level3">
<h3 class="anchored" data-anchor-id="qa">Q&amp;A</h3>
<ul>
<li>Leverage Jupyter Notebooks for reproducible evaluation orchestration and detailed log analysis.</li>
<li>Focus on evaluating the most critical and informative aspects of the LLM system, prioritizing evaluations that exhibit variability and potential for improvement.</li>
<li>Use bootstrap sampling to efficiently assess performance and identify areas for improvement.</li>
<li>Strive for an evaluation suite with a passing rate of 60-70% to ensure sufficient sensitivity to changes and improvements.</li>
</ul>
</section>
</section>
<section id="eugene-yan-evaluating-llm-generated-summaries-with-out-of-domain-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="eugene-yan-evaluating-llm-generated-summaries-with-out-of-domain-fine-tuning">Eugene Yan: Evaluating LLM-Generated Summaries with Out-of-Domain Fine-tuning</h2>
<section id="introduction-1" class="level3">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>
<ul>
<li><p><strong>Problem:</strong> Evaluating the factual accuracy of LLM-generated summaries and detecting hallucinations.</p></li>
<li><p><strong>Solution:</strong> Develop an evaluator model that predicts the probability of a summary being factually inconsistent with the source document.</p></li>
<li><p><strong>Approach:</strong> Frame the problem as a Natural Language Inference (NLI) task, treating “contradiction” as factual inconsistency.</p></li>
<li><p><strong>GitHub Repository:</strong> <a href="eugeneyan/visualizing-finetunes">eugeneyan/visualizing-finetunes</a></p></li>
<li><p><strong>Blog Post:</strong> <a href="https://eugeneyan.com/writing/finetuning/">Out-of-Domain Finetuning to Bootstrap Hallucination Detection</a></p></li>
</ul>
</section>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology</h3>
<ol type="1">
<li><p><strong>Data Preparation:</strong></p>
<ul>
<li>Exclude low-quality data (e.g., CNN Daily Mail from FIB).</li>
<li>Split data into train, validation, and test sets, ensuring no data leakage.</li>
<li>Balance classes within each set.</li>
</ul></li>
<li><p><strong>Model Fine-tuning:</strong></p>
<ul>
<li>Use a pre-trained NLI model (DistilBART fine-tuned on MNLI).</li>
<li>Fine-tune on FIB data alone and evaluate performance.</li>
<li>Fine-tune on USB data, then FIB data, and evaluate performance on both datasets.</li>
</ul></li>
<li><p><strong>Evaluation Metrics:</strong></p>
<ul>
<li><p><strong>Standard metrics:</strong></p>
<ul>
<li><p><strong>ROC AUC:</strong> Area Under the Receiver Operating Characteristic Curve.</p>
<ul>
<li>Measures the ability of a binary classification model to distinguish between the positive and negative classes across all possible thresholds.</li>
<li>Higher values indicate better performance, with 1 being perfect and 0.5 indicating no better performance than random guessing.</li>
</ul>
<p><strong>PR AUC:</strong> Area Under the Precision-Recall Curve.</p>
<ul>
<li>Evaluates the trade-off between precision (the accuracy of positive predictions) and recall (the ability to find all positive instances) across different thresholds.</li>
<li>Useful when dealing with imbalanced datasets.</li>
<li>A higher PR AUC indicates better performance in identifying the positive class.</li>
</ul></li>
</ul></li>
<li><p><strong>Custom metrics:</strong></p>
<ul>
<li><strong>Recall:</strong> Measures the proportion of actual positive cases that are correctly identified by the model</li>
<li><strong>Precision:</strong> Measures the proportion of positive predictions that are actually correct.</li>
</ul></li>
<li><p><strong>Visualizations:</strong> Distribution overlap of predicted probabilities for consistent and inconsistent summaries.</p></li>
</ul></li>
</ol>
</section>
<section id="overview-notebook" class="level3">
<h3 class="anchored" data-anchor-id="overview-notebook"><a href="https://github.com/eugeneyan/visualizing-finetunes/blob/main/0_overview.ipynb">Overview Notebook</a></h3>
<ul>
<li><strong>Evaluator Model:</strong> A model trained to detect factual inconsistencies in summaries, framed as a natural language inference (NLI) task.</li>
<li><strong>NLI for Factual Inconsistency:</strong> Using the “contradiction” label in NLI to identify factual inconsistencies in summaries.</li>
<li><strong>Objective:</strong> Fine-tune an evaluator model to catch hallucinations and evaluate its performance through each epoch.</li>
<li><strong>Data Blending:</strong> Demonstrating how blending data from different benchmarks can improve the evaluator model’s performance.</li>
</ul>
</section>
<section id="prepare-data-notebook" class="level3">
<h3 class="anchored" data-anchor-id="prepare-data-notebook"><a href="https://github.com/eugeneyan/visualizing-finetunes/blob/main/1_prep_data.ipynb">Prepare Data Notebook</a></h3>
<ul>
<li><strong><a href="https://huggingface.co/datasets/r-three/fib">Factual Inconsistency Benchmark (FIB)</a>:</strong> A dataset containing one-sentence summaries from news articles, with labels indicating factual consistency.</li>
<li><strong><a href="https://huggingface.co/datasets/kundank/usb">Unified Summarization Benchmark (USB)</a>:</strong> A dataset containing summaries of Wikipedia articles, with labels indicating factual consistency.</li>
<li><strong>Data Splitting and Balancing:</strong> Splitting the data into train, validation, and test sets, ensuring no data leakage and balancing positive and negative examples.</li>
</ul>
</section>
<section id="finetune-fib-notebook" class="level3">
<h3 class="anchored" data-anchor-id="finetune-fib-notebook"><a href="https://github.com/eugeneyan/visualizing-finetunes/blob/main/2_ft_fib.ipynb">Finetune FIB Notebook</a></h3>
<ul>
<li><strong>Model:</strong> Distilled BART, a pre-trained encoder-decoder model fine-tuned on MNLI.</li>
<li><strong>Fine-Tuning:</strong> Fine-tuning the model on the FIB dataset, tracking custom metrics like ROC AUC, recall, and precision.</li>
<li><strong>Results:</strong> Fine-tuning on FIB alone shows limited improvement in ROC AUC and recall, indicating the need for more data.</li>
</ul>
</section>
<section id="finetune-usb-then-fib-notebook" class="level3">
<h3 class="anchored" data-anchor-id="finetune-usb-then-fib-notebook"><a href="https://github.com/eugeneyan/visualizing-finetunes/blob/main/3_ft_usb_then_fib.ipynb">Finetune USB then FIB Notebook</a></h3>
<ul>
<li><strong>Data Blending:</strong> Fine-tuning the model on the larger USB dataset first, followed by fine-tuning on the FIB dataset.</li>
<li><strong>Results:</strong> Fine-tuning on USB significantly improves performance on both USB and FIB, demonstrating the benefits of data blending.</li>
<li><strong>Evaluator Model as a Tool:</strong> The fine-tuned evaluator model can be used to evaluate generative models, acting as a fast and scalable hallucination detector.</li>
</ul>
</section>
<section id="advantages-of-the-evaluator-model" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-the-evaluator-model">Advantages of the Evaluator Model</h3>
<ul>
<li><strong>Fast and Scalable:</strong> Evaluates summaries in milliseconds, making it suitable for real-time applications.</li>
<li><strong>Controllable:</strong> Allows setting thresholds to prioritize precision or recall based on specific needs.</li>
<li><strong>Versatile:</strong> Can be adapted to evaluate other aspects of summaries, such as relevance and information density.</li>
</ul>
</section>
<section id="evaluating-agents" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-agents">Evaluating Agents</h3>
<ul>
<li>Break down complex tasks into smaller, evaluable steps.</li>
<li>Use a combination of classification, extraction, and potentially reward model-based metrics.</li>
<li><strong>Example:</strong> Evaluating a meeting transcript summarization agent:
<ul>
<li>Step 1: Evaluate the extraction of decisions, actions, and owners (classification).</li>
<li>Step 2: Evaluate the factual consistency of extracted information against the transcript (classification using the hallucination detection model).</li>
<li>Step 3: Evaluate the quality of the final summary in terms of information density and writing style (potentially using a reward model).</li>
</ul></li>
</ul>
</section>
</section>
<section id="shreya-shankar-scaling-up-vibe-checks-for-llms" class="level2">
<h2 class="anchored" data-anchor-id="shreya-shankar-scaling-up-vibe-checks-for-llms">Shreya Shankar: Scaling Up Vibe Checks for LLMs</h2>
<ul>
<li><strong>Focus:</strong> Using LLMs to scale up human evaluation and create task-specific assertions or guardrails.</li>
<li><strong>Evaluation Assistants:</strong> Tools that aid humans in creating and refining evaluations for LLM pipelines.</li>
<li><strong>Longer Talk:</strong> <a href="https://www.youtube.com/watch?v=eGVDKegRdgM">Scaling Up “Vibe Checks” for LLMs - Shreya Shankar | Stanford MLSys #97</a></li>
</ul>
<section id="llm-pipelines" class="level3">
<h3 class="anchored" data-anchor-id="llm-pipelines">LLM Pipelines</h3>
<ul>
<li><strong>Zero-Shot Capabilities:</strong> LLM pipelines can perform complex tasks without explicit training, using prompt templates and instructions.
<ul>
<li><strong>Examples:</strong>
<ul>
<li><strong><a href="https://smith.langchain.com/hub/julia/podcaster-tweet-thread">julia/podcaster-tweet-thread</a>:</strong> Take a podcast episode transcript and turn into a tweet thread.</li>
<li><strong><a href="https://smith.langchain.com/hub/homanp/github-code-reviews">homanp/github-code-reviews</a>:</strong> This prompt reviews pull request on GitHub.</li>
<li><strong><a href="https://smith.langchain.com/hub/matu/customer_satisfaction">matu/customer_satisfaction</a>:</strong> This prompt is being use to extract services and sentiments from a customer answer to a survey.</li>
<li><strong><a href="muhsinbashir/youtube-transcript-to-article:">muhsinbashir/youtube-transcript-to-article:</a></strong> Convert any Youtube Video Transcript into an Article.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="llms-make-unpredictable-mistakes" class="level3">
<h3 class="anchored" data-anchor-id="llms-make-unpredictable-mistakes">LLMs Make Unpredictable Mistakes</h3>
<ul>
<li><strong>Instruction Following:</strong> LLMs may not always follow instructions perfectly, leading to unexpected errors and inconsistencies.</li>
<li><strong>Need for Guardrails:</strong> Evaluation and assertions are crucial for detecting and correcting LLM errors, ensuring reliable output.</li>
</ul>
</section>
<section id="vibe-checks-custom-evaluation-for-llms" class="level3">
<h3 class="anchored" data-anchor-id="vibe-checks-custom-evaluation-for-llms">Vibe Checks: Custom Evaluation for LLMs</h3>
<ul>
<li><strong>Vibe Checks:</strong> Task-specific constraints, guidelines, or assertions that define “good” output based on human judgment.</li>
<li><strong>Challenges:</strong>
<ul>
<li><strong>Subjectivity:</strong> Different users may have different expectations for the same task.</li>
<li><strong>Complexity:</strong> Metrics like “tone” are difficult to quantify and evaluate.</li>
<li><strong>Scalability:</strong> Manual vibe checks by humans are effective but don’t scale well.</li>
</ul></li>
<li><strong>Spectrum of Vibe Checks:</strong>
<ul>
<li><strong>Generic:</strong> Common ML performance metrics provided by model developers.</li>
<li><strong>Architecture-Specific:</strong> Metrics relevant to specific LLM architectures (e.g., faithfulness in RAG pipelines).</li>
<li><strong>Task-Specific:</strong> Fine-grained constraints tailored to the exact requirements of a task.</li>
</ul></li>
<li><strong>Goal:</strong> Develop scalable, codified vibe checks (validators, assertions, guardrails) that capture task-specific requirements.</li>
</ul>
</section>
<section id="evaluation-assistants-using-llms-to-build-vibe-checks" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-assistants-using-llms-to-build-vibe-checks">Evaluation Assistants: Using LLMs to Build Vibe Checks</h3>
<ul>
<li><strong>Evaluation Assistants:</strong> Tools that help humans define and implement task-specific evaluations and assertions.</li>
<li><strong>Key Idea:</strong> Leverage LLMs to scale, not replace, human judgment.</li>
<li><strong>Workflow Components:</strong>
<ul>
<li><strong>Auto-generate criteria and implementations:</strong> Use LLMs to suggest potential evaluation criteria and ways to implement them.</li>
<li><strong>Mixed Initiative Interface:</strong> Allow humans to interact with and refine LLM-generated criteria and provide feedback.</li>
</ul></li>
</ul>
</section>
<section id="auto-generated-assertions-learning-from-prompt-history" class="level3">
<h3 class="anchored" data-anchor-id="auto-generated-assertions-learning-from-prompt-history">Auto-Generated Assertions: Learning from Prompt History</h3>
<ul>
<li><strong>Challenge:</strong> Identifying relevant assertion criteria and ensuring coverage of potential failures.</li>
<li><strong><a href="https://arxiv.org/abs/2401.03038">SPADE System</a>:</strong> A two-step workflow for generating assertions.
<ol type="1">
<li><strong>Generate candidate assertions:</strong> Use LLMs to propose potential assertions.</li>
<li><strong>Filter based on human preferences:</strong> Allow humans to select and refine the most relevant assertions.</li>
</ol></li>
<li><strong>Insight:</strong> Prompt version history reveals information about developer priorities and common LLM errors.
<ul>
<li><strong>Example:</strong> Repeated edits to instructions related to sensitive information indicate a need for a corresponding assertion.</li>
</ul></li>
<li><strong>Categorizing Prompt Deltas:</strong> Analyzing how humans modify prompts helps identify common categories of edits, which can inform assertion generation.</li>
<li><strong>From Taxonomy to Assertions:</strong> LLMs can use the categorized prompt deltas and the current prompt to suggest relevant assertion criteria.</li>
<li><strong>Lessons from Deployment:</strong>
<ul>
<li>Inclusion and exclusion assertions are most common.</li>
<li>LLM-generated assertions may be redundant, incorrect, or require further refinement.</li>
</ul></li>
</ul>
</section>
<section id="evalgen-a-mixed-initiative-interface-for-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="evalgen-a-mixed-initiative-interface-for-evaluation">EvalGen: A Mixed Initiative Interface for Evaluation</h3>
<ul>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/2404.12272">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a></li>
<li><strong>Motivation:</strong> Streamline the process of creating and refining assertions, making it more efficient and user-friendly.</li>
<li><strong>Key Features:</strong>
<ul>
<li><strong>Minimize wait time:</strong> Provide rapid feedback and iteration cycles.</li>
<li><strong>Human-in-the-loop:</strong> Allow users to edit, refine, and grade LLM outputs and criteria.</li>
<li><strong>Interactive grading:</strong> Enable users to provide thumbs-up/thumbs-down feedback on LLM outputs.</li>
<li><strong>Report card:</strong> Summarize evaluation results and highlight areas for improvement.</li>
</ul></li>
<li><strong>Qualitative Study Findings:</strong>
<ul>
<li><strong>Starting point:</strong> EvalGen provides a useful starting point for assertion development, even if initial suggestions require refinement.</li>
<li><strong>Iterative process:</strong> Evaluation is an iterative process that benefits from ongoing human feedback.</li>
<li><strong>Criteria drift:</strong> User definitions of “good” and “bad” output evolve over time and with exposure to more examples.</li>
<li><strong>Code-based vs.&nbsp;LLM-based evals:</strong>
<ul>
<li>Users have different expectations and use cases for these two types of evaluations.</li>
<li>Preferred for fuzzy criteria, dirty data, and situations where humans struggle to articulate clear rules.</li>
</ul></li>
</ul></li>
<li><strong>EvalGen v2:</strong> Incorporates lessons learned from the study, including:
<ul>
<li>Dynamic criteria list for easier iteration.</li>
<li>Natural language feedback for refining criteria.</li>
<li>Support for per-criteria feedback.</li>
</ul></li>
</ul>
</section>
<section id="overall-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="overall-takeaways">Overall Takeaways</h3>
<ul>
<li><strong>Mistakes are inevitable:</strong> LLMs will make mistakes, especially at scale.</li>
<li><strong>LLMs can assist in evaluation:</strong> By leveraging prompt history and human feedback, LLMs can help create effective evaluation metrics.</li>
<li><strong>Evaluation is iterative:</strong> Continuous monitoring, feedback, and refinement are crucial for maintaining LLM accuracy and alignment with user expectations.</li>
<li><strong>Evaluation assistants are valuable:</strong> Tools like EvalGen can significantly streamline the process of developing and refining LLM evaluations.</li>
</ul>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="using-prompt-history-for-generating-assertions" class="level3">
<h3 class="anchored" data-anchor-id="using-prompt-history-for-generating-assertions">Using Prompt History for Generating Assertions</h3>
<ul>
<li><strong>Benefit:</strong> Focusing LLM’s attention when generating evaluation criteria. Instead of designing a unit test for every sentence in a long prompt, providing prompt history helps focus on key criteria.</li>
<li><strong>Focus on Iteration:</strong> Start with 2-3 criteria, refine them, and then add more, rather than starting with an overwhelming number.</li>
<li><strong>Challenges in Writing Assertions:</strong> The difficulty lies in aligning assertions with what constitutes “good” or “bad” output. This definition evolves over time and requires analyzing model output and user feedback.</li>
<li><strong>Value of Evaluation Assistants:</strong> Assist in drawing conclusions from data and defining “good” output, aiding in the continuous improvement process.</li>
</ul>
</section>
<section id="generalizability-of-assertion-criteria" class="level3">
<h3 class="anchored" data-anchor-id="generalizability-of-assertion-criteria">Generalizability of Assertion Criteria</h3>
<ul>
<li><strong>Generalization Across Models:</strong> Prompt edits and the way people interact with LLMs are similar across different models, regardless of the specific model used (Mistral, LLAMA2, ChatGPT, Claude).</li>
</ul>
</section>
<section id="unit-tests-for-specific-llm-tasks" class="level3">
<h3 class="anchored" data-anchor-id="unit-tests-for-specific-llm-tasks">Unit Tests for Specific LLM Tasks</h3>
<ul>
<li><strong>Applicability of Unit Tests:</strong> While straightforward for tasks with clear data structures (e.g., query validity), unit tests are less effective for general-purpose language models or tasks like text rewriting or summarization.</li>
</ul>
</section>
<section id="temperature-parameter-in-open-source-llms" class="level3">
<h3 class="anchored" data-anchor-id="temperature-parameter-in-open-source-llms">Temperature Parameter in Open-Source LLMs</h3>
<ul>
<li><strong>Open-Source LLMs and Temperature:</strong> Similar to OpenAI’s models, open-source LLMs have a temperature parameter. Setting it to zero ensures deterministic output, which is often desirable in production settings.</li>
</ul>
</section>
<section id="importance-of-evaluation-methods" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-evaluation-methods">Importance of Evaluation Methods</h3>
<ul>
<li><strong>Iterative Approach to Evaluation:</strong> Start building the product without extensive upfront evaluation. Implement evaluations as you learn more about the task, identify edge cases, and seek improvements.</li>
<li><strong>Don’t Let Evals Hinder Progress:</strong> Avoid evaluation paralysis. Focus on creating a minimal product and then iteratively refine it based on evaluations and user feedback.</li>
</ul>
</section>
<section id="fine-tuning-llm-as-a-judge" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-llm-as-a-judge">Fine-tuning LLM as a Judge</h3>
<ul>
<li><strong>Using Off-The-Shelf Models:</strong> It is generally recommended to use publicly available, off-the-shelf LLMs as judges instead of fine-tuning separate judge models.</li>
<li><strong>Complexity and Alignment:</strong> Fine-tuning judge models can lead to complexity and make it challenging to align them with human judgment.</li>
</ul>
</section>
<section id="starting-the-data-flywheel" class="level3">
<h3 class="anchored" data-anchor-id="starting-the-data-flywheel">Starting the Data Flywheel</h3>
<ul>
<li><strong>Start with a Prompt:</strong> Begin with a simple prompt and an off-the-shelf LLM to build a basic product and gather user data.</li>
<li><strong>Leverage Synthetic Data:</strong> Utilize the LLM’s capabilities to generate synthetic data, enabling faster iteration and unblocking progress.</li>
</ul>
</section>
<section id="do-sample-parameter-in-production" class="level3">
<h3 class="anchored" data-anchor-id="do-sample-parameter-in-production">“Do Sample” Parameter in Production</h3>
<ul>
<li><strong>Deterministic vs.&nbsp;Varied Output:</strong> Setting <code>do_sample</code> to <code>false</code> (or using zero temperature) ensures deterministic, consistent output, often preferred for production systems requiring predictable behavior.</li>
<li><strong>Use Case Dependency:</strong> For creative applications like character AI, where variety is desired, <code>do_sample</code> can be set to <code>true</code> or a non-zero temperature can be used.</li>
</ul>
</section>
<section id="preparing-data-for-ab-testing-with-llms" class="level3">
<h3 class="anchored" data-anchor-id="preparing-data-for-ab-testing-with-llms">Preparing Data for A/B Testing with LLMs</h3>
<ul>
<li><strong>Human Evaluation vs.&nbsp;LLM as Judge:</strong> While LLMs can potentially be used to choose between options, human evaluation is often more reliable.</li>
<li><strong>Context-Specific Data Preparation:</strong> Data preparation depends heavily on the specific task and why an LLM is used for A/B testing.</li>
</ul>
</section>
<section id="evaluating-retriever-performance-in-rag" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-retriever-performance-in-rag">Evaluating Retriever Performance in RAG</h3>
<ul>
<li><strong>Key Metrics:</strong>
<ul>
<li><strong>Recall@10:</strong> Measures how many relevant documents are retrieved within the top 10 results.</li>
<li><strong>Ranking (NDCG):</strong> Evaluates if the most relevant documents are ranked higher.</li>
<li><strong>Ability to Return Zero Results:</strong> Important for identifying queries with no relevant information in the index, preventing the LLM from generating incorrect responses based on irrelevant data.</li>
</ul></li>
<li><strong>Importance of Handling Irrelevant Data:</strong> Ensuring the retriever can effectively identify and handle queries with no relevant information is crucial for avoiding inaccurate or nonsensical responses from the LLM.</li>
</ul>
</section>
<section id="filtering-documents-for-factuality-and-bias-in-rag" class="level3">
<h3 class="anchored" data-anchor-id="filtering-documents-for-factuality-and-bias-in-rag">Filtering Documents for Factuality and Bias in RAG</h3>
<ul>
<li><strong>Challenges:</strong> Identifying factually incorrect or biased content within the document corpus is a complex challenge.</li>
<li><strong>Content Moderation and Exclusion:</strong> Employ content moderation techniques to identify and exclude toxic, biased, or offensive content from the retrieval index.</li>
<li><strong>Open Problem:</strong> Detecting subtle misinformation or bias remains an open research problem.</li>
</ul>
</section>
<section id="running-unit-tests-during-cicd" class="level3">
<h3 class="anchored" data-anchor-id="running-unit-tests-during-cicd">Running Unit Tests during CI/CD</h3>
<ul>
<li><strong>Local vs.&nbsp;CI/CD Execution:</strong> Running tests locally provides faster feedback during development, while integrating with CI/CD ensures consistent testing and prevents accidental deployments without proper testing.</li>
<li><strong>Use Case Dependency:</strong> The choice depends on the purpose of the tests (quality assurance vs.&nbsp;safety checks) and the sensitivity of the application.</li>
</ul>
</section>
<section id="checking-for-contamination-of-base-models" class="level3">
<h3 class="anchored" data-anchor-id="checking-for-contamination-of-base-models">Checking for Contamination of Base Models</h3>
<ul>
<li><strong>Contextual Reasoning:</strong> Analyze the likelihood of overlap between the evaluation data and the base model’s training data based on the nature and recency of the data.</li>
<li><strong>Performance Monitoring:</strong> Be wary of unexpectedly high performance, which could indicate data leakage.</li>
<li><strong>No Foolproof Solution:</strong> Data contamination is a difficult problem with no universal solution. Careful consideration and context-specific analysis are essential.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/workshop-003/</guid>
  <pubDate>Thu, 20 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 3: Gradio Q&amp;A Session with Freddy Boulton</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-003/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Gradio Resources:">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradio Resources:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Gradio Documentation:</strong> <a href="https://www.gradio.app/docs">https://www.gradio.app/docs</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/gradio-app/gradio">https://github.com/gradio-app/gradio</a></li>
<li><strong>Hugging Face Discord:</strong> <a href="https://discord.com/invite/feTf9x3ZSB">https://discord.com/invite/feTf9x3ZSB</a></li>
</ul>
</div>
</div>
<section id="gradio-demo-on-huggingface-spaces" class="level2">
<h2 class="anchored" data-anchor-id="gradio-demo-on-huggingface-spaces">1. Gradio Demo on HuggingFace Spaces</h2>
<ul>
<li>Freddy showcases a HuggingFace Space demonstrating various chatbot implementations using Gradio.
<ul>
<li><strong>Demo:</strong> <a href="gradio/chat-examples">gradio/chat-examples</a></li>
</ul></li>
<li>The demo highlights Gradio’s simplicity, requiring only ~50 lines of Python code to create a fully functional chatbot UI.</li>
<li>Key features include:
<ul>
<li>Integration with HuggingFace inference API for querying LLMs.</li>
<li>Streaming responses for a more interactive user experience.</li>
<li>Built-in functionalities like retrying, undoing, and clearing chat history.</li>
</ul></li>
<li>Gradio offers a wide range of components beyond chatbots, enabling the creation of diverse AI applications.</li>
</ul>
</section>
<section id="why-choose-gradio" class="level2">
<h2 class="anchored" data-anchor-id="why-choose-gradio">2. Why Choose Gradio?</h2>
<ul>
<li><p>Freddy addresses the competitive landscape, acknowledging tools like Streamlit, Shiny, Dash, and Flask.</p></li>
<li><p>He emphasizes Gradio’s strengths, particularly its AI-first design:</p>
<ul>
<li>High-level abstractions simplify building AI/ML applications.</li>
<li>Specialized components like the chat interface streamline development.</li>
<li>Built-in API usage allows using Gradio applications programmatically.</li>
<li>Seamless integration with Hugging Face, including access to zero GPU for free GPU usage for Hugging Face Pro subscribers.</li>
</ul></li>
</ul>
</section>
<section id="migrating-to-gradio-from-streamlit" class="level2">
<h2 class="anchored" data-anchor-id="migrating-to-gradio-from-streamlit">3. Migrating To Gradio from Streamlit</h2>
<ul>
<li>While a dedicated migration guide is not available, Freddy points out similarities between Gradio and Streamlit:
<ul>
<li>Both employ a declarative UI API, making UI design intuitive.</li>
</ul></li>
<li>Key difference:
<ul>
<li>Gradio requires explicit reactivity definition, specifying which function to run when a component changes.</li>
<li>This explicitness benefits performance and API generation but demands a slightly more imperative approach.</li>
</ul></li>
<li>Freddy refutes claims of Gradio being limited to toy use cases, citing examples like the Elements leaderboard handling significant traffic.</li>
</ul>
</section>
<section id="streaming" class="level2">
<h2 class="anchored" data-anchor-id="streaming">4. Streaming</h2>
<ul>
<li>Gradio supports streaming output beyond just text, including images, audio, and even webcam input.</li>
<li>Implementing streaming involves using a generator within the function triggered by Gradio.</li>
<li>This enables dynamic updates, such as visualizing the progression of diffusion models or real-time audio transcription.</li>
</ul>
</section>
<section id="the-gradio-repository" class="level2">
<h2 class="anchored" data-anchor-id="the-gradio-repository">5. The Gradio Repository</h2>
<ul>
<li>Freddy provides an overview of the Gradio repository, highlighting key directories:
<ul>
<li><strong>Gradio (Python):</strong> Contains source code for components, FastAPI server, and more.</li>
<li><strong>Gradio (JavaScript):</strong> Houses the JavaScript/Svelte frontend code.</li>
</ul></li>
<li>He explains the structure of components, emphasizing the <code>preprocess</code> and <code>postprocess</code> functions for handling data between the frontend and backend.</li>
<li>Contribution opportunities are abundant, with many issues labeled as “good first issue” for newcomers.</li>
</ul>
</section>
<section id="multimodality" class="level2">
<h2 class="anchored" data-anchor-id="multimodality">6. Multimodality</h2>
<ul>
<li>Gradio offers components for building multimodal applications, including chatbots that handle both text and file inputs.</li>
<li>The <code>gr.MultiModalTextbox</code> component allows users to send text and attachments.</li>
<li>Freddy demonstrates a multimodal chatbot example, showcasing how to process and respond to different input types.
<ul>
<li><strong>Demo:</strong> <a href="gradio/chatbot_multimodal">gradio/chatbot_multimodal</a></li>
</ul></li>
</ul>
</section>
<section id="gradio-based-apps-in-production" class="level2">
<h2 class="anchored" data-anchor-id="gradio-based-apps-in-production">7. Gradio-Based Apps in Production</h2>
<ul>
<li>Freddy confirms the feasibility of deploying Gradio-based applications in production, particularly for performant demos.</li>
<li>He shares a guide on maximizing Gradio performance:
<ul>
<li><strong>Guide:</strong> <a href="https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance">Setting Up a Demo for Maximum Performance</a></li>
</ul></li>
<li>Strategies for handling production workloads include:
<ul>
<li>Leveraging Gradio’s built-in queuing mechanism to manage GPU-intensive tasks.</li>
<li>Adjusting concurrency settings to optimize resource utilization.</li>
<li>Implementing batching for processing multiple requests concurrently.</li>
<li>Load balancing across multiple Gradio servers for scalability.</li>
</ul></li>
</ul>
</section>
<section id="example-usage-in-gradio" class="level2">
<h2 class="anchored" data-anchor-id="example-usage-in-gradio">8. Example Usage in Gradio</h2>
<ul>
<li><p>Freddy addresses a question about adding predefined buttons to a chatbot for initiating conversations.</p></li>
<li><p>He introduces the concept of “examples” in Gradio:</p>
<ul>
<li>Allows seeding demos with sample inputs, providing users with guidance.</li>
<li>Supports caching example outputs to showcase model behavior without consuming resources.</li>
<li>Applicable to various components, including multimodal scenarios with sample prompts and images.</li>
</ul></li>
</ul>
</section>
<section id="gradio-js-client" class="level2">
<h2 class="anchored" data-anchor-id="gradio-js-client">9. Gradio JS Client</h2>
<ul>
<li>Freddy highlights the upcoming 1.0 release of the Gradio JavaScript client, addressing past limitations.</li>
<li>The client enables integrating Hugging Face models into custom UIs, bridging the gap between existing frontends and the Hugging Face ecosystem.</li>
<li>While building custom Svelte components is possible, Freddy emphasizes the convenience of Gradio’s pre-built component library, simplifying UI development.</li>
<li>He encourages exploring the custom component gallery for inspiration and extending Gradio’s functionality.</li>
</ul>
</section>
<section id="gradio-custom-components" class="level2">
<h2 class="anchored" data-anchor-id="gradio-custom-components">10. Gradio Custom Components</h2>
<ul>
<li><p><strong>Gradio Custom Components Gallery:</strong> <a href="https://www.gradio.app/custom-components/gallery">https://www.gradio.app/custom-components/gallery</a></p></li>
<li><p>Freddy showcases various custom components from the Gradio Custom Components Gallery:</p>
<ul>
<li><strong><a href="https://freddyaboulton-gradio-pdf.hf.space/?__theme=light#h-gradio_pdf">gradio_pdf</a>:</strong> Enables building applications that interact with PDFs, such as document question answering systems.</li>
<li><strong><a href="https://simonduerr-gradio-molecule3d.hf.space/?__theme=light#h-gradio_molecule3d">gradio_molecule3d</a>:</strong> Allows visualizing and manipulating molecules within a Gradio interface.</li>
<li><strong><a href="https://radames-gradio-huggingfacehub-search.hf.space/?__theme=light#h-gradio_huggingfacehub_search">gradio_huggingfacehub_search</a>:</strong> Provides a searchable interface for accessing models and datasets from the Hub.</li>
<li><strong><a href="https://freddyaboulton-gradio-folium.hf.space/?__theme=light#h-gradio_folium">gradio_folium</a>:</strong> Enables embedding interactive maps for geospatial data visualization.</li>
</ul></li>
</ul>
</section>
<section id="gradio-for-multi-user-applications" class="level2">
<h2 class="anchored" data-anchor-id="gradio-for-multi-user-applications">11. Gradio for Multi-User Applications?</h2>
<ul>
<li><p>Freddy clarifies that Gradio supports concurrent users and discusses scaling considerations:</p>
<ul>
<li>Hardware specifications play a crucial role in determining user capacity.</li>
<li>Gradio’s queuing mechanism, concurrency settings, and batching capabilities can be tuned to optimize performance.
<ul>
<li><a href="https://www.gradio.app/guides/queuing">Queuing</a></li>
</ul></li>
<li>Hosting resource-intensive components (LLMs, models) on platforms like Hugging Face and querying them via the Gradio API can enhance scalability.</li>
</ul></li>
</ul>
</section>
<section id="gradio-community" class="level2">
<h2 class="anchored" data-anchor-id="gradio-community">12. Gradio Community</h2>
<ul>
<li><p>Freddy recommends the Hugging Face Discord as the primary hub for the Gradio community:</p>
<ul>
<li>Dedicated channels for asking questions, sharing projects, and discussing Gradio-related topics.</li>
<li>Announcements and updates from the Gradio team.</li>
</ul></li>
</ul>
</section>
<section id="multi-agent-collaboration-visualizations" class="level2">
<h2 class="anchored" data-anchor-id="multi-agent-collaboration-visualizations">13. Multi-Agent Collaboration Visualizations</h2>
<ul>
<li>Freddy shares a custom component he built for visualizing multi-agent collaboration, demonstrating its use with the Transformers agent API.
<ul>
<li><strong>Custom Component:</strong> <a href="https://www.gradio.app/custom-components/gallery?id=freddyaboulton%2Fgradio_agentchatbot">agentchatbot</a></li>
<li><strong>Langchain Agents:</strong> <a href="https://www.gradio.app/guides/gradio-and-llm-agents">Gradio &amp; LLM Agents 🤝</a></li>
</ul></li>
<li>The component showcases the agent’s chain of thought, including tool usage and intermediate outputs.</li>
<li>While multi-agent chatbots are not yet natively supported, Freddy suggests exploring custom component development for this functionality.</li>
</ul>
</section>
<section id="authentication-in-huggingface-spaces" class="level2">
<h2 class="anchored" data-anchor-id="authentication-in-huggingface-spaces">14. Authentication in HuggingFace Spaces</h2>
<ul>
<li><p>Freddy acknowledges limitations with Gradio’s built-in authentication and suggests alternative approaches:</p>
<ul>
<li><strong>Sign-in with Hugging Face button:</strong> Leverages OAuth for secure authentication without relying on cross-site cookies.</li>
<li><strong>Google OAuth integration:</strong> Allows users to authenticate using their Google accounts.</li>
</ul></li>
<li><p><strong>HuggingFace Space Demo:</strong> <a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo">ggml-org/gguf-my-repo</a></p></li>
</ul>
</section>
<section id="gradio-and-fastapi" class="level2">
<h2 class="anchored" data-anchor-id="gradio-and-fastapi">15. Gradio and FastAPI</h2>
<ul>
<li>Freddy explains that Gradio is built upon <a href="https://fastapi.tiangolo.com/">FastAPI</a>, serving a specific HTML file containing the Gradio frontend.</li>
<li>Gradio acts as a FastAPI server, handling API requests and running Python functions triggered by user interactions.</li>
<li>Integration with larger FastAPI applications is seamless using FastAPI’s sub-application functionality, allowing mounting Gradio UIs within existing applications.</li>
</ul>
</section>
<section id="authentication-and-authorization" class="level2">
<h2 class="anchored" data-anchor-id="authentication-and-authorization">16. Authentication and Authorization</h2>
<ul>
<li><p>Freddy outlines how to implement custom authentication and authorization in Gradio:</p>
<ul>
<li>Accessing the FastAPI <code>request</code> object within Gradio functions provides user information.</li>
<li>Based on user details, developers can control access to specific app functionalities or raise errors for unauthorized access.</li>
</ul></li>
</ul>
</section>
<section id="gradio-lite" class="level2">
<h2 class="anchored" data-anchor-id="gradio-lite">17. Gradio Lite</h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://www.gradio.app/guides/gradio-lite">https://www.gradio.app/guides/gradio-lite</a></li>
<li>Freddy introduces Gradio Lite, a serverless version of Gradio powered by <a href="https://pyodide.org/en/stable/">Pyodide</a>, enabling entirely client-side Python execution.</li>
<li>Benefits of Gradio Lite:
<ul>
<li>Enhanced privacy for sensitive tasks like audio transcription.</li>
<li>Seamless integration with <a href="https://huggingface.co/docs/transformers.js/en/index">transformers.js</a> for running machine learning models in the browser.</li>
</ul></li>
<li>Freddy acknowledges the evolving landscape of Python in the browser and promises to provide resources for making web requests from within Pyodide.</li>
</ul>
</section>
<section id="advanced-tables-in-gradio" class="level2">
<h2 class="anchored" data-anchor-id="advanced-tables-in-gradio">18. Advanced Tables in Gradio</h2>
<ul>
<li>While acknowledging limitations with the existing dataframe component’s filtering capabilities, Freddy highlights its flexibility in visualizing pandas dataframes.
<ul>
<li><strong>Dataframe Documentation:</strong> <a href="https://www.gradio.app/docs/gradio/dataframe">https://www.gradio.app/docs/gradio/dataframe</a></li>
</ul></li>
<li>He suggests exploring custom component development for advanced table features like <a href="https://www.ag-grid.com/">AG Grid</a>.</li>
<li>Freddy showcases the leaderboard component as an example of a custom component handling complex data processing client-side for improved performance.
<ul>
<li><a href="https://www.gradio.app/custom-components/gallery?id=freddyaboulton%2Fgradio_leaderboard">🥇 Leaderboard Component</a></li>
</ul></li>
</ul>
</section>
<section id="future-plans" class="level2">
<h2 class="anchored" data-anchor-id="future-plans">19. Future Plans</h2>
<ul>
<li>Freddy shares exciting developments on Gradio’s roadmap:
<ul>
<li><strong>Enhanced agent workflows:</strong> Improved integration with agent APIs and streamlined development of agent-based applications.</li>
<li><strong>Real-time streaming:</strong> Exploring technologies like WebRTC for high-speed, bidirectional communication between client and server, enabling <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a>-like experiences.</li>
<li><strong>More declarative UI:</strong> Introducing <code>gr.render</code> for dynamically generating UI elements based on variables, enabling more flexible and dynamic interfaces.
<ul>
<li><strong>Guide:</strong> <a href="https://www.gradio.app/guides/dynamic-apps-with-render-decorator">Dynamic Apps with the Render Decorator</a></li>
</ul></li>
</ul></li>
<li>He also emphasizes ongoing work on the Gradio client and Gradio Lite, further expanding the platform’s capabilities.</li>
</ul>
</section>
<section id="finetuning-llms-on-gradio-documentation" class="level2">
<h2 class="anchored" data-anchor-id="finetuning-llms-on-gradio-documentation">20. Finetuning LLMs on Gradio Documentation</h2>
<ul>
<li>Freddy expresses enthusiasm for the idea of an LLM fine-tuned on Gradio documentation to provide accurate and up-to-date code snippets.</li>
<li>He acknowledges the prevalence of outdated or hallucinated Gradio code from existing LLMs and encourages the community to contribute to this effort.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-003/</guid>
  <pubDate>Fri, 14 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 2: Q&amp;A Session with Zach Mueller</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-002/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li><strong>Hands-on experience is crucial for learning LLM fine-tuning:</strong> Experimenting with code and models is more valuable than solely reading about it.</li>
<li><strong>Community engagement is essential for feedback and learning:</strong> Platforms like Twitter and Discord provide valuable spaces to connect with experts and peers.</li>
<li><strong>Choosing the right data set is crucial for effective fine-tuning:</strong> Synthetic data sets and those that evolve over time offer unique advantages.</li>
<li><strong>Hardware plays a significant role in LLM training and inference:</strong> NVIDIA GPUs remain dominant.</li>
<li><strong>Model size should be determined by inference constraints and desired quality:</strong> Smaller models often provide a good balance between performance and cost.</li>
</ul>
</section>
<section id="axolotl-vs.-hf-autotrain" class="level2">
<h2 class="anchored" data-anchor-id="axolotl-vs.-hf-autotrain">1. Axolotl vs.&nbsp;HF AutoTrain</h2>
<ul>
<li><strong>Axolotl</strong> and <strong>HF AutoTrain</strong> address different aspects of LLM training.</li>
<li><strong>Axolotl</strong> focuses on high-level, rapid model training for text-based tasks.</li>
<li><strong>HF AutoTrain</strong> offers a more agnostic approach, allowing for training various models with custom data.</li>
<li><strong>Key difference:</strong> Axolotl prioritizes speed and ease of use, while HF AutoTrain provides greater flexibility.</li>
</ul>
</section>
<section id="learning-journey-for-llm-engineers" class="level2">
<h2 class="anchored" data-anchor-id="learning-journey-for-llm-engineers">2. Learning Journey for LLM Engineers</h2>
<ul>
<li><strong>Practical experience is paramount:</strong> Start by experimenting with code and building models.</li>
<li><strong>Active community engagement is crucial:</strong> Seek feedback, ask questions, and share your learnings.</li>
<li><strong>Focus on practical projects:</strong> Choose small, manageable tasks to gain hands-on experience.</li>
<li><strong>Iterate and learn from mistakes:</strong> Analyze results, identify areas for improvement, and continuously refine your approach.</li>
</ul>
</section>
<section id="finding-feedback-and-community" class="level2">
<h2 class="anchored" data-anchor-id="finding-feedback-and-community">3. Finding Feedback and Community</h2>
<ul>
<li><strong>Engage with experts on platforms like Twitter and Discord:</strong></li>
<li><strong>Be proactive and demonstrate effort:</strong> Share your work, ask specific questions, and show that you’ve attempted to solve the problem.</li>
<li><strong>Contribute to the community:</strong> Share your learnings, participate in discussions, and help others.</li>
</ul>
</section>
<section id="public-data-sets-for-llm-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="public-data-sets-for-llm-fine-tuning">4. Public Data Sets for LLM Fine-Tuning</h2>
<ul>
<li><strong>Hugging Face Data Sets:</strong> Offers a wide variety of data sets suitable for LLM fine-tuning.</li>
<li><strong>StarCoder 2 Self-Instruct Data Set:</strong> Based on code from GitHub, provides benchmarks and a transparent pipeline.</li>
<li><strong>Instruction Tuning Data Sets:</strong> Help understand the principles of fine-tuning and prepare for more complex tasks.
<ul>
<li><a href="https://www.philschmid.de/instruction-tune-llama-2">Extended Guide: Instruction-tune Llama 2</a></li>
</ul></li>
<li><strong>Synthetic Data Sets:</strong> Offer control over the data generation process and enable testing for overfitting.</li>
</ul>
</section>
<section id="accelerate-torch-compile-and-distributed-training" class="level2">
<h2 class="anchored" data-anchor-id="accelerate-torch-compile-and-distributed-training">5. Accelerate, Torch Compile, and Distributed Training</h2>
<ul>
<li><strong>FSDP (Fully Sharded Data Parallelism):</strong> Essential for training large models by distributing data and model parameters across multiple GPUs.</li>
<li><strong>DeepSpeed:</strong> Offers more configuration options than FSDP, allowing for fine-grained control over offloading and device placement.</li>
<li><strong>Torch Compile:</strong> Primarily an inference-time optimization, but PyTorch aims to integrate it into training workflows.</li>
<li><strong>Recommendation:</strong> Use FSDP for models that fit in memory across all GPUs; consider DeepSpeed for scenarios requiring offloading.</li>
</ul>
</section>
<section id="inference-precision-and-hardware" class="level2">
<h2 class="anchored" data-anchor-id="inference-precision-and-hardware">6. Inference Precision and Hardware</h2>
<ul>
<li><strong>BF16 (BFloat16):</strong> Offers a good balance between performance and accuracy for training and inference.</li>
<li><strong>FP16 (Half Precision):</strong> Can be slower than BF16, especially on hardware optimized for BF16.</li>
<li><strong>Recommendation:</strong> Train models in BF16 to ensure compatibility with a wider range of inference hardware.</li>
</ul>
</section>
<section id="downsides-of-fsdp" class="level2">
<h2 class="anchored" data-anchor-id="downsides-of-fsdp">7. Downsides of FSDP</h2>
<ul>
<li><strong>All-or-nothing approach:</strong> FSDP distributes the entire model across GPUs, which can be limiting if the model doesn’t fit in memory.</li>
<li><strong>Lack of fine-grained control:</strong> Unlike DeepSpeed, FSDP doesn’t allow for selectively offloading specific layers to the CPU.</li>
</ul>
</section>
<section id="nvlink-and-gpu-performance" class="level2">
<h2 class="anchored" data-anchor-id="nvlink-and-gpu-performance">8. NVLink and GPU Performance</h2>
<ul>
<li><strong>NVLink:</strong> Provides high-bandwidth communication between GPUs, improving performance in multi-GPU setups.</li>
<li><strong>Impact of NVLink absence:</strong> Debated, with some reporting significant performance degradation and others claiming minimal impact.</li>
<li><strong>Consumer cards and throttling:</strong> Consumer-grade GPUs might have driver-level limitations compared to their professional counterparts.</li>
<li><strong>Recommendation:</strong> Consider the RTX A4000 or RTX A4500 over RTX 4090s if budget allows:
<ul>
<li><strong>NVIDIA RTX A4500 (24GB)</strong>
<ul>
<li><a href="https://www.nvidia.com/en-us/design-visualization/rtx-a4500/">Product Page</a></li>
<li><a href="https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-4500-ada-generation/?nvid=em-VCNRTX4500ADA-PB">Purchase Page</a></li>
</ul></li>
<li><strong>NVIDIA RTX A4000 (20GB)</strong>
<ul>
<li><a href="https://www.nvidia.com/en-us/design-visualization/rtx-a4000/">Product Page</a></li>
<li><a href="https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-4000-ada-generation/index.html?nvid=em-VCNRTX4000ADA-PB">Purchase Page</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="fine-tuning-vs.-frontier-models" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-vs.-frontier-models">9. Fine-Tuning vs.&nbsp;Frontier Models</h2>
<ul>
<li><strong>Fine-tuning can achieve comparable or even surpass the performance of frontier models:</strong> Community-driven fine-tuning efforts like Teknium’s models demonstrate this potential.</li>
<li><strong>Data access remains a challenge:</strong> Closed-source models often benefit from significantly larger and proprietary data sets.</li>
</ul>
</section>
<section id="ensuring-prompting-and-tokenization-consistency" class="level2">
<h2 class="anchored" data-anchor-id="ensuring-prompting-and-tokenization-consistency">10. Ensuring Prompting and Tokenization Consistency</h2>
<ul>
<li><strong>Hugging Face Pipelines:</strong> Provide a reliable way to load and use fine-tuned models for inference.</li>
<li><strong>Chat Templating:</strong> Hugging Face’s chat templates offer a standardized approach to prompting, but they might not be directly compatible with all tools.</li>
<li><strong>Thorough Testing:</strong> Always test inference with the same tokenization and prompting procedures used during training.</li>
</ul>
</section>
<section id="running-inference-on-an-8-billion-parameter-model-with-a-24gb-gpu" class="level2">
<h2 class="anchored" data-anchor-id="running-inference-on-an-8-billion-parameter-model-with-a-24gb-gpu">11. Running Inference on an 8 Billion Parameter Model with a 24GB GPU</h2>
<ul>
<li><strong>Quantization:</strong> Techniques like AWQ (AutoAWQ) can reduce model size and memory footprint, enabling inference on less powerful hardware.</li>
<li><strong>Offloading:</strong> Offloading parts of the model to the CPU can enable inference on limited VRAM, but it comes with a performance trade-off.</li>
</ul>
</section>
<section id="training-models-in-8-bit-precision" class="level2">
<h2 class="anchored" data-anchor-id="training-models-in-8-bit-precision">12. Training Models in 8-Bit Precision</h2>
<ul>
<li><strong>Instability:</strong> Training in 8-bit precision (INT8 or FP8) can lead to instability and convergence issues.</li>
<li><strong>Experimental Support:</strong> While frameworks like PyTorch are adding support for 8-bit training, it remains experimental.</li>
<li><strong>BF16 with FP8:</strong> Some hardware platforms utilize a combination of BF16 and FP8 for training, potentially offering a middle ground.</li>
</ul>
</section>
<section id="limitations-of-accelerate" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-accelerate">13. Limitations of Accelerate</h2>
<ul>
<li><strong>Accelerate as a wrapper:</strong> Accelerate primarily acts as a wrapper around existing distributed training frameworks, so its failures often stem from underlying issues.</li>
<li><strong>Timeout issues:</strong> Occasional timeout problems have been observed, but the root cause remains unclear.</li>
</ul>
</section>
<section id="relevance-of-chinchilla-scaling-laws" class="level2">
<h2 class="anchored" data-anchor-id="relevance-of-chinchilla-scaling-laws">14. Relevance of Chinchilla Scaling Laws</h2>
<ul>
<li><strong>Still relevant for optimal resource allocation:</strong> Chinchilla scaling laws provide guidance on balancing parameters and data size for a given compute budget.</li>
<li><strong>Don’t guarantee the best model:</strong> Continuously training a model until convergence often yields the best results, regardless of scaling laws.</li>
<li><strong>Under-trained models and fine-tuning:</strong> Models trained with fewer steps than suggested by scaling laws might be more amenable to fine-tuning.</li>
</ul>
</section>
<section id="relevance-of-tensorflow-for-llm-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="relevance-of-tensorflow-for-llm-fine-tuning">15. Relevance of TensorFlow for LLM Fine-Tuning</h2>
<ul>
<li><strong>PyTorch dominance:</strong> PyTorch has become the dominant framework for LLM research and development.</li>
<li><strong>TensorFlow’s role:</strong> TensorFlow, particularly Keras, still serves as a backend in some LLM frameworks, but its popularity has diminished.</li>
</ul>
</section>
<section id="training-on-apple-silicon" class="level2">
<h2 class="anchored" data-anchor-id="training-on-apple-silicon">16. Training on Apple Silicon</h2>
<ul>
<li><strong>Inference:</strong> Apple Silicon performs well for LLM inference tasks.</li>
<li><strong>Training:</strong> Training support is improving but remains behind NVIDIA GPUs in terms of maturity and performance.</li>
<li><strong>Hardware limitations:</strong> Apple’s system-on-a-chip architecture and lack of dedicated server-grade GPUs pose challenges for large-scale training.</li>
</ul>
</section>
<section id="serving-multiple-loras-with-accelerate-inference" class="level2">
<h2 class="anchored" data-anchor-id="serving-multiple-loras-with-accelerate-inference">17. Serving Multiple LoRAs with Accelerate Inference</h2>
<ul>
<li><strong>VLLM:</strong> Supports loading and switching between multiple LoRAs during inference.</li>
<li><strong>Hot-swapping:</strong> VLLM allows for selecting different LoRAs on a per-request basis, enabling dynamic model customization.</li>
</ul>
</section>
<section id="mixture-of-loras" class="level2">
<h2 class="anchored" data-anchor-id="mixture-of-loras">18. Mixture of LoRAs</h2>
<ul>
<li><strong>Concept:</strong> Training multiple LoRAs specializing in different tasks and using a router to dynamically select the most appropriate LoRA for a given input.</li>
<li><strong>Kraken Model:</strong> An example of a model that utilizes dynamic model routing with multiple expert models.
<ul>
<li><a href="https://huggingface.co/VAGOsolutions/Kraken-LoRA">VAGOsolutions/Kraken-LoRA</a></li>
</ul></li>
</ul>
</section>
<section id="choosing-a-fine-tuning-project" class="level2">
<h2 class="anchored" data-anchor-id="choosing-a-fine-tuning-project">19. Choosing a Fine-Tuning Project</h2>
<ul>
<li><strong>Personal interest and relevance:</strong> Select projects that align with your interests and current work.</li>
<li><strong>Replicating existing work:</strong> Recreating existing projects is a valuable learning experience.</li>
<li><strong>Data availability:</strong> Choose projects with readily available or easily obtainable data sets.</li>
<li><strong>Document your process:</strong> Keep track of your experiments, results, and lessons learned.</li>
</ul>
</section>
<section id="constraints-and-sweet-spots-in-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="constraints-and-sweet-spots-in-fine-tuning">20. Constraints and Sweet Spots in Fine-Tuning</h2>
<ul>
<li><strong>Budget and hardware:</strong> Determine the available compute resources and select a model size accordingly.</li>
<li><strong>Inference time and cost:</strong> Prioritize inference efficiency, as it significantly impacts real-world deployment costs.</li>
<li><strong>Iteration speed:</strong> Smaller models allow for faster experimentation and iteration cycles.</li>
<li><strong>Quality requirements:</strong> Balance model size with the desired performance level for the specific task.
<ul>
<li>7 to 8 billion parameter models are often the sweet spot in real-world projects.</li>
</ul></li>
</ul>
</section>
<section id="fine-tuning-on-phi-3" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-on-phi-3">21. Fine-Tuning on Phi-3</h2>
<ul>
<li><strong>Limited real-world performance:</strong> Despite its size, Phi-3 has not demonstrated competitive performance in practical applications.</li>
<li><strong>Data and training methodology:</strong> Potential issues with the training data or methodology might contribute to its shortcomings.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-002/</guid>
  <pubDate>Tue, 11 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Office Hours 1: Axolotl Q&amp;A with Wing Lian</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/office-hours-001/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<section id="template-free-prompt-construction-in-axolotl" class="level2">
<h2 class="anchored" data-anchor-id="template-free-prompt-construction-in-axolotl">1. Template-Free Prompt Construction in Axolotl</h2>
<ul>
<li><strong>Purpose:</strong> Offers flexibility in defining custom chat roles and formats.</li>
<li><strong>Format:</strong> Uses simple input-output pairs with labels (true for model output, false for user input).</li>
<li><strong>Advantages:</strong>
<ul>
<li>Easier to understand for some users.</li>
<li>Translates well to platforms with existing input-output data.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Less flexible for changing chat templates later.</li>
<li>Requires more manual string handling during inference.</li>
</ul></li>
<li><strong>Recommendation:</strong> Use only if existing chat templates are insufficient.</li>
<li><strong>Documentation:</strong> <a href="https://openaccess-ai-collective.github.io/axolotl/docs/input_output.html">https://openaccess-ai-collective.github.io/axolotl/docs/input_output.html</a></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Example:">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"segments"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span></span>
<span id="cb1-3">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-4">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">true</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-5">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"text"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;s&gt;Hello</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-6">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-7">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-8">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">true</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-9">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"text"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hi there!. "</span></span>
<span id="cb1-10">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-11">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-12">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">false</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-13">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"text"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"goodbye "</span></span>
<span id="cb1-14">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-15">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-16">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">true</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-17">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"text"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"farewell&lt;/s&gt;"</span></span>
<span id="cb1-18">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb1-19">    <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span>
<span id="cb1-20"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div>
</div></li>
</ul>
</section>
<section id="how-to-decide-data-type-for-datasets-on-huggingface" class="level2">
<h2 class="anchored" data-anchor-id="how-to-decide-data-type-for-datasets-on-huggingface">2. How to Decide Data Type for Datasets on HuggingFace?</h2>
<ul>
<li><strong>Dataset Types:</strong>
<ul>
<li><strong>Alpaca:</strong> Instruction-based, with separate fields for instruction, input, and output.</li>
<li><strong>ShareGPT:</strong> Conversation-based, with variations in field names (e.g., human/user, GPT/assistant, value/content). Axolotl’s <code>type: sharegpt</code> handles most variations.</li>
<li><strong>DPO:</strong> Includes chosen and rejected responses for preference learning. Variations exist in field names and prompt construction.</li>
<li><strong>KTO:</strong> Similar to DPO but without explicit preference pairs.</li>
<li><strong>User Defined:</strong> Allows custom formatting defined in the YAML file.</li>
</ul></li>
<li><strong>Recommendation:</strong> Choose the type that best matches the dataset structure and fine-tuning objective.</li>
<li><strong>Examples:</strong>
<ul>
<li><strong><a href="https://huggingface.co/datasets/argilla/dpo-mix-7k">argilla/dpo-mix-7k</a>:</strong> Chosen, rejected fields.</li>
<li><strong><a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs">Intel/orca_dpo_pairs</a>:</strong> Question, chosen, rejected fields.</li>
<li><strong><a href="https://huggingface.co/datasets/argilla/kto-mix-15k">argilla/kto-mix-15k</a>:</strong> Prompt, completion, label, rating.</li>
</ul></li>
</ul>
</section>
<section id="dpo-and-rlhf" class="level2">
<h2 class="anchored" data-anchor-id="dpo-and-rlhf">3. DPO and RLHF</h2>
<ul>
<li><strong>DPO (Direct Preference Optimization):</strong>
<ul>
<li>Trains a model to maximize the probability of choosing preferred responses over rejected ones.</li>
<li>Simpler to implement than RLHF but limited to single-turn preference learning.</li>
</ul></li>
<li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong>
<ul>
<li>Uses reinforcement learning to optimize a model based on human feedback.</li>
<li>More complex but potentially leads to higher quality alignment and multi-turn capabilities.</li>
</ul></li>
<li><strong>Future Direction:</strong> Both DPO and RLHF are important, with a potential shift towards more robust RL-based methods.</li>
</ul>
</section>
<section id="difference-between-chat-template-and-datasets-type-parameters" class="level2">
<h2 class="anchored" data-anchor-id="difference-between-chat-template-and-datasets-type-parameters">4. Difference Between Chat Template and Datasets Type Parameters</h2>
<ul>
<li><strong>Chat Template:</strong> Defines the specific format of the chat conversation (e.g., LLAMA3, Mistral, ChatML). It gets added to the tokenizer config.</li>
<li><strong>Datasets Type:</strong> Specifies how Axolotl should parse and structure the input dataset (e.g., share_gpt, alpaca, dpo).</li>
<li><strong>Interaction:</strong>
<ul>
<li>Setting a chat template can automatically set the output format for certain dataset types.</li>
<li>The <code>chat_template</code> parameter in the YAML file overrides any default settings.</li>
</ul></li>
</ul>
</section>
<section id="no-ops-for-validation" class="level2">
<h2 class="anchored" data-anchor-id="no-ops-for-validation">5. No Ops for Validation</h2>
<ul>
<li><strong>Currently, Axolotl lacks built-in validation checks for potential issues like:</strong>
<ul>
<li>Rounding errors when saving models in different precision formats (e.g., float32 vs.&nbsp;bfloat16).</li>
<li>Tokenization discrepancies between training and inference.</li>
</ul></li>
<li><strong>Recommendation:</strong>
<ul>
<li>Carefully manage model precision during saving.</li>
<li>Implement custom checks to compare tokenization between training and inference pipelines.</li>
</ul></li>
</ul>
</section>
<section id="trust-no-one-for-tokenization" class="level2">
<h2 class="anchored" data-anchor-id="trust-no-one-for-tokenization">6. Trust No One for Tokenization</h2>
<ul>
<li><strong>Key Takeaway:</strong> Always verify the actual tokens being fed into the model, as string handling and YAML parsing can introduce subtle errors.</li>
<li><strong>Example:</strong> YAML can remove trailing spaces in non-quoted strings, potentially affecting tokenization.</li>
<li><strong>Recommendation:</strong> Implement rigorous checks to ensure consistent tokenization between training and inference.</li>
</ul>
</section>
<section id="ensuring-consistent-tokenization" class="level2">
<h2 class="anchored" data-anchor-id="ensuring-consistent-tokenization">7. Ensuring Consistent Tokenization</h2>
<ul>
<li><strong>Challenge:</strong> Tokenization differences can arise from:
<ul>
<li>Separate vs.&nbsp;concatenated tokenization of input and output strings.</li>
<li>Special token handling in different chat templates.</li>
</ul></li>
<li><strong>Recommendations:</strong>
<ul>
<li>Use the <code>chat_template</code> parameter in Axolotl to enforce consistent formatting.</li>
<li>Implement tests to compare tokenization between fine-tuning and inference setups.</li>
<li>Consider introducing minor tokenization variations during training as a form of data augmentation.</li>
</ul></li>
</ul>
</section>
<section id="tokenizer-configs-from-training-to-inference" class="level2">
<h2 class="anchored" data-anchor-id="tokenizer-configs-from-training-to-inference">8. Tokenizer Configs from Training to Inference</h2>
<ul>
<li><strong>Importance:</strong> Consistent tokenizer configurations are crucial for seamless transition from training to inference.</li>
<li><strong>Axolotl’s Approach:</strong> Setting the <code>chat_template</code> parameter in the YAML file updates the tokenizer config, which is then used by inference engines.</li>
<li><strong>Challenge:</strong> Not all inference engines may fully support or utilize the chat template information.</li>
<li><strong>Recommendation:</strong> Verify that the chosen inference engine correctly interprets and applies the tokenizer config, including the chat template.</li>
</ul>
</section>
<section id="multimodal-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="multimodal-fine-tuning">9. Multimodal Fine-tuning</h2>
<ul>
<li><strong>Current Status:</strong> Axolotl lacks native support for multimodal datasets and models.</li>
<li><strong>Challenges:</strong>
<ul>
<li>Handling image data and integrating it with text data.</li>
<li>Adapting to evolving approaches for multimodal tokenization and model architectures.</li>
</ul></li>
<li><strong>Future Direction:</strong>
<ul>
<li>Implementing dataset handling for images and other modalities.</li>
<li>Potentially supporting both Lava-like approaches and native multimodal models.</li>
</ul></li>
<li><strong>Call for Contributions:</strong> Help is needed in developing and implementing multimodal capabilities.</li>
</ul>
</section>
<section id="is-rlhf-still-a-common-fine-tuning-technique" class="level2">
<h2 class="anchored" data-anchor-id="is-rlhf-still-a-common-fine-tuning-technique">10. Is RLHF Still a Common Fine-tuning Technique?</h2>
<ul>
<li><strong>Answer:</strong> Yes, RLHF and other preference-based tuning methods (like DPO) are becoming increasingly common.</li>
<li><strong>Reasoning:</strong>
<ul>
<li>Supervised fine-tuning has limitations in achieving high-quality alignment.</li>
<li>RLHF and DPO enable learning from human preferences, leading to better model behavior.</li>
</ul></li>
<li><strong>Future Trend:</strong> Expect to see wider adoption of both RL-based and non-RL preference optimization techniques.</li>
</ul>
</section>
<section id="dpo-limitations-and-rl-advantages" class="level2">
<h2 class="anchored" data-anchor-id="dpo-limitations-and-rl-advantages">11. DPO Limitations and RL Advantages</h2>
<ul>
<li><strong>DPO Limitation:</strong> Primarily designed for single-turn preference learning.</li>
<li><strong>RL Advantages:</strong>
<ul>
<li>Supports multi-turn conversations and intermediate rewards.</li>
<li>Can lead to better alignment and more nuanced model behavior.</li>
</ul></li>
<li><strong>Trade-offs:</strong>
<ul>
<li>RLHF is more complex and data-intensive than DPO.</li>
<li>DPO is simpler to implement and doesn’t require a separate reward model.</li>
</ul></li>
</ul>
</section>
<section id="sample-files-for-paligemma-and-phi-3" class="level2">
<h2 class="anchored" data-anchor-id="sample-files-for-paligemma-and-phi-3">12. Sample Files for PaliGemma and Phi-3</h2>
<ul>
<li><strong>Phi-3:</strong>
<ul>
<li>Should work with existing Phi-2 configurations by swapping the baseline model.</li>
<li>May require setting <code>trust_remote_code: true</code> in the YAML file.</li>
</ul></li>
<li><strong>PaliGemma:</strong>
<ul>
<li>No specific examples available yet.</li>
<li>LLM fine-tuning might be possible, but full support requires multimodal dataset handling.</li>
</ul></li>
</ul>
</section>
<section id="conversational-datasets-vs.-qa-pairs" class="level2">
<h2 class="anchored" data-anchor-id="conversational-datasets-vs.-qa-pairs">13. Conversational Datasets vs.&nbsp;QA Pairs</h2>
<ul>
<li><strong>Assumption:</strong> Conversational datasets are always more effective for fine-tuning.</li>
<li><strong>Clarification:</strong> The choice depends on the specific use case and desired model behavior.</li>
<li><strong>Recommendations:</strong>
<ul>
<li><strong>QA Pairs:</strong> Suitable for single-turn interactions or when mimicking a retrieval-based system.</li>
<li><strong>Conversational Datasets:</strong> Beneficial for training models to engage in multi-turn dialogue.</li>
</ul></li>
<li><strong>Instruction Tuning:</strong> Recommended for gaining intuition about conversational datasets and fine-tuning.</li>
</ul>
</section>
<section id="training-datasets-for-completion-models" class="level2">
<h2 class="anchored" data-anchor-id="training-datasets-for-completion-models">14. Training Datasets for Completion Models</h2>
<ul>
<li><strong>Dataset Characteristics:</strong> Typically similar to pre-training datasets, often with a single “text” field.</li>
<li><strong>Examples:</strong>
<ul>
<li>Story generation datasets.</li>
<li>Any dataset focused on text completion or continuation.</li>
</ul></li>
</ul>
</section>
<section id="prompt-template-for-llama3-and-llama-index" class="level2">
<h2 class="anchored" data-anchor-id="prompt-template-for-llama3-and-llama-index">15. Prompt Template for LLAMA3 and LLAMA Index</h2>
<ul>
<li><strong>Goal:</strong> Fine-tune LLAMA3 for use with LLAMA Index, which uses an OpenAI-like message abstraction.</li>
<li><strong>Recommendation:</strong>
<ul>
<li>Choose a chat template that aligns with the message-based format (e.g., ChatML).</li>
<li>Avoid instruction-based templates as they might not be suitable for multi-turn interactions.</li>
</ul></li>
</ul>
</section>
<section id="future-directions-of-axolotl" class="level2">
<h2 class="anchored" data-anchor-id="future-directions-of-axolotl">16. Future Directions of Axolotl</h2>
<ul>
<li><strong>Areas for Contribution:</strong>
<ul>
<li>Join the <a href="">Axolotl Discord server</a> and contribute to discussions.</li>
<li>Explore the GitHub repository for <a href="https://github.com/OpenAccess-AI-Collective/axolotl/issues">open issues</a> and feature requests.</li>
<li>Developing new features and improving existing ones.</li>
</ul></li>
<li><strong>Ongoing Development:</strong>
<ul>
<li>Building a turnkey platform for simplified fine-tuning and deployment (similar to Modal).</li>
<li>Integrating DPO, PPO, and enhanced dataset pipelines.</li>
<li>Creating a user-friendly CLI and cloud integration.</li>
</ul></li>
</ul>
</section>
<section id="vram-estimation" class="level2">
<h2 class="anchored" data-anchor-id="vram-estimation">17. VRAM Estimation</h2>
<ul>
<li><strong>Need:</strong> A tool for accurate VRAM estimation based on Axolotl configurations.</li>
<li><strong>Challenges:</strong>
<ul>
<li>Complexities introduced by techniques like FSDP and DeepSpeed.</li>
<li>Variations in VRAM usage based on batch sizes and model parallelism.</li>
</ul></li>
<li><strong>Potential Approach:</strong> Leverage existing LLM math estimations and account for the impact of distributed training techniques.</li>
</ul>
</section>
<section id="vibe-checks-during-training" class="level2">
<h2 class="anchored" data-anchor-id="vibe-checks-during-training">18. Vibe Checks During Training</h2>
<ul>
<li><strong>Goal:</strong> Evaluate model performance and “vibes” during training.</li>
<li><strong>Options:</strong>
<ul>
<li><strong>Periodic Checkpointing:</strong> Pause training, run inference on the checkpoint, and resume.</li>
<li><strong>Dedicated Evaluation:</strong> Use a separate process to run inference on an eval dataset and log the results.</li>
<li><strong>Callbacks:</strong> Implement callbacks to trigger inference on demand during training.</li>
</ul></li>
<li><strong>Challenges:</strong>
<ul>
<li>VRAM limitations might make it difficult to run inference alongside training.</li>
<li>Ensuring consistent tokenization and prompt handling between training and evaluation.</li>
</ul></li>
</ul>
</section>
<section id="familiarizing-with-prompt-templates" class="level2">
<h2 class="anchored" data-anchor-id="familiarizing-with-prompt-templates">19. Familiarizing with Prompt Templates</h2>
<ul>
<li><strong>Recommendation:</strong> Use the <code>axolotl.cli.preprocess</code> command with the <code>debug</code> flag to visualize how Axolotl processes and tokenizes prompts.</li>
<li><strong>Output:</strong> Displays the tokenized prompt with color-coding to distinguish between input, output, and masked tokens.</li>
</ul>
</section>
<section id="axolotl-vs.-unsloth" class="level2">
<h2 class="anchored" data-anchor-id="axolotl-vs.-unsloth">20. Axolotl vs.&nbsp;Unsloth</h2>
<ul>
<li><strong>Unsloth:</strong>
<ul>
<li>Specialized for Lora fine-tuning.</li>
<li>Offers memory optimizations but might be limited in GPU scalability.</li>
</ul></li>
<li><strong>Axolotl:</strong>
<ul>
<li>Provides a more comprehensive framework for fine-tuning, including prompt management and dataset handling.</li>
<li>Focuses on performance optimizations like sample packing.</li>
</ul></li>
<li><strong>Recommendation:</strong> Choose the tool that best aligns with your specific needs and priorities.</li>
</ul>
</section>
<section id="quick-and-dirty-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="quick-and-dirty-fine-tuning">21. Quick and Dirty Fine-tuning</h2>
<ul>
<li><strong>Recommendation:</strong>
<ul>
<li>Start with a small “tiny llama” example for faster iteration.</li>
<li>Use Gradio inference for quick model evaluation.</li>
</ul></li>
</ul>
</section>
<section id="function-calling-fine-tunes" class="level2">
<h2 class="anchored" data-anchor-id="function-calling-fine-tunes">22. Function Calling Fine-Tunes</h2>
<ul>
<li><strong>Dataset Example:</strong> Glade datasets from the Noose team.</li>
<li><strong>Configuration:</strong> Might require specific role handling and a compatible version of the ShareGPT dataset type.</li>
</ul>
</section>
<section id="visualizing-tokenization-in-batches" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-tokenization-in-batches">23. Visualizing Tokenization in Batches</h2>
<ul>
<li><strong>Challenge:</strong> Axolotl’s sample packing happens at runtime, making it difficult to visualize tokenization in batches during pre-processing.</li>
<li><strong>Potential Approach:</strong> Modify the Transformers or LLAMA model code to print or log input IDs during the forward pass.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/office-hours-001/</guid>
  <pubDate>Tue, 11 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Conference Talk 1: Ten Commandments to Deploy Fine-Tuned Models in Production</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-001/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>1: Thou Shalt Not Fine-Tune</li>
<li>2: Thou Shalt Write a Freaking Prompt</li>
<li>3: Thou Shalt Review Thy Freaking Data</li>
<li>4: Thou Shalt Use Thy Actual Freaking Data</li>
<li>5: Thou Shalt Reserve a Test Set</li>
<li>6: Thou Shalt Choose an Appropriate Model</li>
<li>7: Thou Shalt Write Fast Evals</li>
<li>8: Also, Thou Shalt Write Slow Evals</li>
<li>9: Thou Shalt Not Fire and Forget</li>
<li>10: Thou Shalt Not Take the Commandments Too Seriously</li>
<li>Q&amp;A Highlights</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Presentation Slides">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Presentation Slides
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://docs.google.com/presentation/d/1IIRrTED0w716OsU_-PL5bONL0Pq_7E8alewvcJO1BCE/">Ten Commandments to Deploy Fine-Tuned Models in Prod</a></li>
</ul>
</div>
</div>
<section id="thou-shalt-not-fine-tune" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-not-fine-tune">Thou Shalt Not Fine-Tune</h2>
<ul>
<li><strong>Start with Prompting:</strong> Focus on crafting effective prompts and leverage techniques like few-shot learning before considering fine-tuning.</li>
<li><strong>Reasons to Fine-Tune:</strong> Only fine-tune if prompting cannot achieve desired outcomes due to:
<ul>
<li><strong>Quality:</strong> Prompting alone cannot meet the required performance standards.</li>
<li><strong>Latency:</strong> Fine-tuning allows the use of smaller, faster models for real-time applications.</li>
<li><strong>Cost:</strong> Fine-tuning enables the use of smaller, more cost-effective models at scale.</li>
</ul></li>
</ul>
</section>
<section id="thou-shalt-write-a-freaking-prompt" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-write-a-freaking-prompt">Thou Shalt Write a Freaking Prompt</h2>
<ul>
<li><strong>Establish a Baseline:</strong> A well-crafted prompt provides a performance baseline for comparison with fine-tuned models.</li>
<li><strong>Assess Task Feasibility:</strong> Trying to solve the problem with prompting reveals whether the task is achievable with the available data and model capabilities.</li>
<li><strong>Check Data Quality:</strong>
<ul>
<li>Successful prompting suggests your data has enough signal for effective model learning.</li>
<li>Failed prompting often indicates data issues like inconsistencies or insufficient information.</li>
</ul></li>
</ul>
<section id="example-logistics-company-item-valuation" class="level3">
<h3 class="anchored" data-anchor-id="example-logistics-company-item-valuation">Example: Logistics Company &amp; Item Valuation</h3>
<ul>
<li><strong>Goal:</strong> Predict item values from descriptions.</li>
<li><strong>Assumption:</strong> That descriptions contained enough information for a model to infer value.</li>
<li><strong>Outcome:</strong> Prompting revealed that the descriptions lacked sufficient detail.</li>
</ul>
</section>
<section id="heuristic-prompting-success-predicts-fine-tuning-success" class="level3">
<h3 class="anchored" data-anchor-id="heuristic-prompting-success-predicts-fine-tuning-success">Heuristic: Prompting Success Predicts Fine-tuning Success</h3>
<ul>
<li>If you can achieve reasonable performance with a well-crafted prompt, there’s a high probability (90%+) that fine-tuning will yield further improvements in latency, quality, or cost.</li>
<li>If prompting proves ineffective, successfully fine-tuning the model becomes less certain and significantly more challenging.</li>
</ul>
</section>
<section id="recommended-workflow-prototype-to-production" class="level3">
<h3 class="anchored" data-anchor-id="recommended-workflow-prototype-to-production">Recommended Workflow: Prototype to Production</h3>
<ol type="1">
<li><strong>Prototype with GPT-4:</strong> During the initial stages, focus on rapid iteration and validation of your application’s core concept. Utilize GPT-4 and prompting to experiment and refine your approach.</li>
<li><strong>Transition to Fine-tuning:</strong> Once you have a working prototype that demonstrates value and scalability, consider incorporating fine-tuning to optimize performance further.</li>
</ol>
</section>
</section>
<section id="thou-shalt-review-thy-freaking-data" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-review-thy-freaking-data">Thou Shalt Review Thy Freaking Data</h2>
<section id="importance-of-data-review" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-data-review">Importance of Data Review</h3>
<ul>
<li><strong>Evaluate Model Performance:</strong> See how well the prompt guides the model to generate desired outputs in real-world scenarios.</li>
<li><strong>Understand User Behavior:</strong> Gain insights into how users interact with the prompt and the types of inputs they provide. This is crucial for:
<ul>
<li><strong>Refining Assumptions:</strong> Avoid making inaccurate assumptions about user needs and use cases.</li>
<li><strong>Improving Prompt Design:</strong> Tailor the prompt to better align with actual usage patterns.</li>
<li><strong>Developing Targeted Tests:</strong> Create more effective tests based on real-world input data.</li>
</ul></li>
</ul>
</section>
<section id="how-to-review-data" class="level3">
<h3 class="anchored" data-anchor-id="how-to-review-data">How to Review Data</h3>
<ul>
<li><strong>Utilize Existing UI:</strong> If your system has a user interface (e.g., chat interface, classification system), leverage it to observe input-output pairs in context.</li>
<li><strong>Employ Specialized Tools:</strong> If a dedicated UI is unavailable, utilize tools like OpenPipe to visualize and analyze input and output data in a structured format.</li>
</ul>
</section>
<section id="what-to-look-for" class="level3">
<h3 class="anchored" data-anchor-id="what-to-look-for">What to Look For</h3>
<ul>
<li><strong>Input Distribution:</strong> Pay close attention to the variety, complexity, and common patterns within user inputs.</li>
<li><strong>Output Quality:</strong> Assess the relevance, accuracy, and overall quality of model outputs in response to real-world inputs.</li>
</ul>
</section>
</section>
<section id="thou-shalt-use-thy-actual-freaking-data" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-use-thy-actual-freaking-data">Thou Shalt Use Thy Actual Freaking Data</h2>
<section id="the-importance-of-using-your-actual-data" class="level3">
<h3 class="anchored" data-anchor-id="the-importance-of-using-your-actual-data">The Importance of Using Your Actual Data</h3>
<ul>
<li><strong>Don’t exclude “bad” data:</strong>
<ul>
<li>Removing data points where the base model performs poorly can lead to a model that excels in a limited domain while failing in real-world scenarios.<br>
</li>
<li>The “bad” data likely represents real-world inputs your model needs to handle.</li>
</ul></li>
<li><strong>Example:</strong> If your model struggles with a specific class of data and you exclude it, the fine-tuned model will likely repeat the mistake in production.</li>
</ul>
</section>
<section id="addressing-poor-performance" class="level3">
<h3 class="anchored" data-anchor-id="addressing-poor-performance">Addressing Poor Performance</h3>
<ul>
<li><strong>Diagnose the issue:</strong> Instead of removing “bad” data, analyze why the model struggles.
<ul>
<li>Is there a pattern in the input space where it fails?</li>
<li>Can you improve the instructions to guide the model better?</li>
</ul></li>
<li><strong>Solutions:</strong>
<ul>
<li><strong>Manually relabel data:</strong> Use a relabeling UI to correct outputs.</li>
<li><strong>Refine instructions:</strong> Experiment with different prompts and instructions to improve performance.</li>
</ul></li>
</ul>
</section>
<section id="when-imperfect-data-can-be-useful" class="level3">
<h3 class="anchored" data-anchor-id="when-imperfect-data-can-be-useful">When Imperfect Data Can Be Useful</h3>
<ul>
<li><strong>Generalization and regularization:</strong>
<ul>
<li>Large LLMs are surprisingly good at generalizing from imperfect data.</li>
<li>The training process itself acts as a form of regularization, allowing the model to learn from both correct and incorrect examples.</li>
</ul></li>
<li><strong>Training on model outputs:</strong>
<ul>
<li>Fine-tuning on the outputs of a larger model (e.g., GPT-4) can lead to a smaller model that outperforms the original due to this regularization effect.
<ul>
<li>The smaller model learns from the larger model’s successes and avoids repeating its occasional errors.</li>
</ul></li>
</ul></li>
<li><strong>Caveat:</strong> This applies mainly to larger LLMs (4B+ parameters) where errors are relatively random. If there’s a consistent pattern of errors, address it directly.</li>
</ul>
</section>
</section>
<section id="thou-shalt-reserve-a-test-set" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-reserve-a-test-set">Thou Shalt Reserve a Test Set</h2>
<ul>
<li><p><strong>Importance of a Test Set:</strong> A dedicated test set, separate from the training data, is essential to evaluate the true performance of a fine-tuned language model.</p></li>
<li><p><strong>Common Pitfalls:</strong></p>
<ul>
<li><strong>Non-representative Test Sets:</strong> Test sets with hand-picked examples, often based on perceived poor performance or customer complaints, are often not representative of the overall input data and can lead to misleading results.</li>
<li><strong>Exclusively Using Non-Random Test Sets:</strong> Relying solely on a set a specific corner cases can give a false sense of performance as the model might not generalize well to unseen data.</li>
</ul></li>
<li><p><strong>Recommendations:</strong></p>
<ul>
<li><strong>Create a Randomly Sampled Test Set:</strong> Reserve 5-10% of your data randomly as a test set. This ensures that the model’s performance is evaluated on data representative of the overall distribution.</li>
<li><strong>Maintain Separate Test Sets:</strong> Use both a randomly sampled test set for general performance evaluation and a separate set for targeted testing of specific corner cases or challenging examples.</li>
</ul></li>
</ul>
</section>
<section id="thou-shalt-choose-an-appropriate-model" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-choose-an-appropriate-model">Thou Shalt Choose an Appropriate Model</h2>
<section id="choosing-a-model" class="level3">
<h3 class="anchored" data-anchor-id="choosing-a-model">Choosing a Model</h3>
<ul>
<li><p><strong>Experimentation is Key:</strong> The cost of fine-tuning runs is relatively low, allowing for trying different models with your data.</p></li>
<li><p><strong>Dataset Size Matters:</strong></p>
<ul>
<li><strong>Small Datasets (Dozens):</strong> Larger models like Llama2 70B can often match GPT-4’s performance.</li>
<li><strong>Medium Datasets (Thousands):</strong> Llama2 7B-8B or Mistral 7B offer a good balance.</li>
<li><strong>Task Dependency:</strong> Some tasks may never reach GPT-4’s level regardless of training data.</li>
</ul></li>
<li><p><strong>Sweet Spot:</strong> 7B-8B parameter models are recommended for most production use cases due to:</p>
<ul>
<li><p><strong>Sufficient Performance:</strong> Achievable with around 1,000-5,000 training examples.</p></li>
<li><p><strong>Cost Savings:</strong> Significantly cheaper inference costs compared to GPT-4 (around 15-20 cents per million tokens vs.&nbsp;GPT-4’s much higher cost).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/conference-talk-001/images/model-chart.png" class="img-fluid figure-img"></p>
<figcaption><a href="https://docs.google.com/presentation/d/1IIRrTED0w716OsU_-PL5bONL0Pq_7E8alewvcJO1BCE/edit#slide=id.g2720912eb0c_0_9">Ten Commandments to Deploy Fine-Tuned Models in Prod - Slide 17</a></figcaption>
</figure>
</div></li>
</ul></li>
</ul>
</section>
</section>
<section id="thou-shalt-write-fast-evals" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-write-fast-evals">Thou Shalt Write Fast Evals</h2>
<section id="fast-evaluations" class="level3">
<h3 class="anchored" data-anchor-id="fast-evaluations">Fast Evaluations:</h3>
<ul>
<li>Can be integrated into the training loop or prompt engineering workflow.</li>
<li>Quick and inexpensive to run.</li>
<li>Provide immediate feedback on model performance.</li>
</ul>
</section>
<section id="llm-as-judge" class="level3">
<h3 class="anchored" data-anchor-id="llm-as-judge">LLM as Judge</h3>
<ul>
<li><strong>Default Recommendation:</strong> Use an LLM (e.g., GPT-4) to evaluate the quality of outputs.</li>
<li><strong>Method:</strong> Present the LLM judge with the input task and outputs from different models. Ask the judge to compare and rate the quality.</li>
<li><strong>Considerations:</strong>
<ul>
<li><strong>Randomization:</strong> Randomize the order of presented outputs to avoid bias towards the first option.</li>
<li><strong>Self-Preference:</strong> Be aware that LLMs tend to prefer their own outputs if evaluated on themselves.</li>
</ul></li>
<li><strong>Tools and Libraries:</strong> Utilize existing libraries (some suggested on OpenPipe) that streamline this process.</li>
</ul>
</section>
<section id="benefits-of-fast-evaluations" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-fast-evaluations">Benefits of Fast Evaluations</h3>
<ul>
<li><strong>Rapid Iteration:</strong> Quickly test changes to prompts, fine-tuning, etc., and get immediate feedback.</li>
<li><strong>Direction Confirmation:</strong> Ensure that development efforts are moving in the right direction.</li>
<li><strong>Faster Feedback Cycle:</strong> Avoid the long delays associated with slower, production-level evaluations.</li>
</ul>
</section>
</section>
<section id="also-thou-shalt-write-slow-evals" class="level2">
<h2 class="anchored" data-anchor-id="also-thou-shalt-write-slow-evals">Also, Thou Shalt Write Slow Evals</h2>
<section id="the-need-for-slow-evaluations" class="level3">
<h3 class="anchored" data-anchor-id="the-need-for-slow-evaluations">The Need for Slow Evaluations</h3>
<ul>
<li><strong>Fast vs.&nbsp;Slow Evaluations:</strong> Fast evaluations are quick checks of model performance in isolation, while slow evaluations assess the model’s impact in a real-world setting.</li>
<li><strong>Importance of Real-World Impact:</strong> Even if a model performs well in isolation, other system interactions or deployment factors (like quantization) can lead to unexpected results. Slow evaluations capture this.</li>
<li><strong>Outcome-Driven Evaluation:</strong> Design evaluations based on the desired business or product outcome. For example, if building a customer support chatbot, measure customer satisfaction with problem resolution.</li>
</ul>
</section>
<section id="examples-from-openais-chatgpt" class="level3">
<h3 class="anchored" data-anchor-id="examples-from-openais-chatgpt">Examples from OpenAI’s ChatGPT</h3>
<ul>
<li><strong>User Engagement Metrics:</strong> OpenAI tracks metrics like how often users regenerate responses or give “thumbs down” as indicators of model performance.</li>
<li><strong>Side-by-Side Comparisons:</strong> While less frequent, OpenAI sometimes presents users with two responses side-by-side, allowing them to choose the better one. This provides direct comparative feedback.</li>
</ul>
</section>
</section>
<section id="thou-shalt-not-fire-and-forget" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-not-fire-and-forget">Thou Shalt Not Fire and Forget</h2>
<section id="continuous-evaluation-is-crucial" class="level3">
<h3 class="anchored" data-anchor-id="continuous-evaluation-is-crucial">Continuous Evaluation is Crucial</h3>
<ul>
<li>After deploying a fine-tuned model, it is essential to continuously evaluate its performance using objective metrics and real-world data. This helps detect any degradation in accuracy.</li>
</ul>
</section>
<section id="data-drift" class="level3">
<h3 class="anchored" data-anchor-id="data-drift">Data Drift</h3>
<ul>
<li>The world is constantly changing, and so is the data that models encounter.</li>
<li>Data drift occurs when the input data starts to differ from the data the model was originally trained on, leading to decreased performance.</li>
</ul>
</section>
<section id="real-world-example" class="level3">
<h3 class="anchored" data-anchor-id="real-world-example">Real-World Example</h3>
<ul>
<li><strong>Problem:</strong> A customer using a fine-tuned model for extracting data from call logs experienced a decline in accuracy.
<ul>
<li>The training data only contained examples from 2023, leading the model to always use the year 2023 when extracting dates even for calls made in 2024.</li>
</ul></li>
<li><strong>Solution:</strong> The problem was solved by retraining the model with a small set of examples from 2024, demonstrating the importance of keeping the training data up-to-date.</li>
</ul>
</section>
</section>
<section id="thou-shalt-not-take-the-commandments-too-seriously" class="level2">
<h2 class="anchored" data-anchor-id="thou-shalt-not-take-the-commandments-too-seriously">Thou Shalt Not Take the Commandments Too Seriously</h2>
<ul>
<li>The above recommendations are only guidelines, not hard requirements.</li>
<li>Tailor your approach based on the specific requirements of your project and data.</li>
</ul>
</section>
<section id="qa-highlights" class="level2">
<h2 class="anchored" data-anchor-id="qa-highlights">Q&amp;A Highlights</h2>
<ul>
<li><strong>Data Quality vs.&nbsp;Effort:</strong> While higher-quality data generally leads to better results, strive for a balance between data refinement and the overall time investment.</li>
<li><strong>When to Fine-Tune:</strong> Fine-tuning is more beneficial when the task is highly specific and diverges from the capabilities of a general-purpose chatbot.</li>
<li><strong>Low-Resource Languages:</strong>
<ul>
<li>Fine-tuning for low-resource languages can be effective with sufficient data.</li>
<li>Consider pre-trained multilingual models as a starting point.</li>
</ul></li>
<li><strong>Evaluation During Training:</strong>
<ul>
<li>Fine-tune on an initial dataset.</li>
<li>If evaluation results are unsatisfactory, increase the dataset size and re-evaluate.</li>
</ul></li>
<li><strong>Deployment and Inference Optimization:</strong>
<ul>
<li><strong>Low-Rank Adaptation (LoRA):</strong>
<ul>
<li>Fine-tunes a smaller set of parameters, resulting in faster training and reduced inference costs.</li>
<li>Allows for loading multiple LoRA models simultaneously in VLM or TRT LLM, maximizing GPU utilization.</li>
</ul></li>
<li><strong>Serverless Endpoints:</strong> Services like OpenPipe, Fireworks, and OctoAI provide cost-effective deployment options by handling infrastructure and utilizing timesharing among users.</li>
</ul></li>
<li><strong>Reasoning Chains:</strong>
<ul>
<li><strong>Challenges:</strong> Complex reasoning tasks may pose difficulties for fine-tuned models, even if they can be solved by larger models like GPT-4.</li>
<li><strong>Factors Influencing Success:</strong>
<ul>
<li>Reproducibility of reasoning patterns.</li>
<li>Availability of sufficient training data covering diverse scenarios within the reasoning chain.</li>
</ul></li>
<li><strong>Recommendations:</strong>
<ul>
<li>Log the entire reasoning chain during data collection.</li>
<li>Ensure data freshness by updating traces if function definitions or other components change.</li>
<li>Consider generating synthetic data to augment the dataset and cover a wider range of scenarios, especially for complex chains.</li>
</ul></li>
</ul></li>
<li><strong>JSON Extraction and Evaluation:</strong>
<ul>
<li><strong>Tracing:</strong> Tracing all LLM calls for JSON extraction is recommended, as storage is relatively inexpensive and should not significantly impact latency in a well-designed system.</li>
<li><strong>Evaluation with Larger Models:</strong> Using a larger model like GPT-4 for evaluating extractions is acceptable. Random sampling can be used to manage costs if necessary.</li>
</ul></li>
<li><strong>Fine-Tuning for Classification with Decoder Models:</strong> While encoder models are theoretically advantageous for classification, there’s a lack of readily available, high-quality, open-source encoder models, especially for long context scenarios.</li>
<li><strong>Relabeling UI Recommendations:</strong> Explore tools like Argilla or consider building custom solutions tailored to your specific needs.</li>
<li><strong>Fine-Tuning Techniques Comparison (LoRa, QLoRa, DoRa):</strong> Start with LoRa for its efficiency and regularization benefits. Consider full fine-tuning or DoRa if LoRa’s performance is insufficient.</li>
<li><strong>Multimodal Scenarios:</strong> While production use cases for vision-language models are still emerging, advancements in open-source models are expected to drive adoption.</li>
<li><strong>Models on the Efficient Frontier:</strong> Current techniques may have reached a saturation point for model efficiency in smaller model sizes.</li>
</ul>


</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/conference-talk-001/</guid>
  <pubDate>Mon, 10 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Workshop 2: Fine-Tuning with Axolotl</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/workshop-002/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Fine-Tuning Fundamentals</li>
<li>Axolotl Framework for Fine-Tuning</li>
<li>Honeycomb Case Study: Fine-tuning LLMs for Natural Language Querying</li>
<li>Debugging Axolotl</li>
<li>Scaling Model Training with More Compute</li>
<li>Scaling Model Training with Accelerate</li>
<li>DeepSpeed and FSDP Configurations in Axolotl</li>
<li>Training on Modal</li>
<li>Q&amp;A Session</li>
</ul>
<section id="fine-tuning-fundamentals" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-fundamentals">Fine-Tuning Fundamentals</h2>
<section id="choosing-the-right-base-model" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-right-base-model">Choosing the Right Base Model</h3>
<ul>
<li>Two important factors to consider when selecting a base model are <strong>model size</strong> and <strong>model family</strong>.</li>
</ul>
<section id="model-size" class="level4">
<h4 class="anchored" data-anchor-id="model-size">Model Size</h4>
<ul>
<li><strong>7 Billion vs.&nbsp;13 Billion Parameter Models:</strong> 7 billion parameter models offer a good balance between performance and resource requirements for many use cases.</li>
<li><strong>Popularity and Practicality:</strong> 7 billion parameter base models are widely used for finetuning, evidenced by high download counts, suggesting a good starting point for most users.</li>
<li><strong>Consider Resource Constraints:</strong> Larger models (e.g., 70 billion parameters) require significant computational resources and may not be necessary for all applications.</li>
</ul>
</section>
<section id="model-family" class="level4">
<h4 class="anchored" data-anchor-id="model-family">Model Family</h4>
<ul>
<li><strong>Staying Current:</strong> Opt for recently released and well-regarded models (e.g., Llama3).</li>
<li><strong>Resources for Identifying Trends:</strong>
<ul>
<li><strong>Hugging Face Model Hub:</strong> Sort by “hotness” to find trending models.</li>
<li><strong>Local Llama Subreddit:</strong> The community actively discusses and evaluates various language models.</li>
</ul></li>
<li><strong>Prioritize Experimentation:</strong> Trying out a few popular models and iterating based on results is more beneficial than overthinking initial model selection.</li>
</ul>
</section>
</section>
<section id="lora-efficient-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="lora-efficient-fine-tuning">LoRA: Efficient Fine-tuning</h3>
<ul>
<li>LoRA (Low-Rank Adaptation) is a technique for fine-tuning large language models by optimizing a smaller set of parameters, making the process more efficient.</li>
</ul>
<section id="how-lora-works" class="level4">
<h4 class="anchored" data-anchor-id="how-lora-works">How LoRA Works</h4>
<ol type="1">
<li><strong>Simplified Model:</strong> Imagine a single layer in a language model as a matrix transforming a 4,000-dimensional input (text embedding) into a 4,000-dimensional output.<br>
</li>
<li><strong>Large Weight Matrix:</strong> This single layer involves a 4,000 x 4,000 weight matrix, amounting to 16 million weights, which is computationally expensive to fine-tune directly.</li>
<li><strong>LoRA’s Approach:</strong>
<ul>
<li>Instead of adjusting the entire weight matrix, LoRA introduces two smaller matrices (4,000 x 16 and 16 x 4,000).</li>
<li>Multiplying these matrices produces a 4,000 x 4,000 matrix, which is then added to the original weight matrix.</li>
</ul></li>
<li><strong>Reduced Parameter Count:</strong> LoRA significantly decreases the number of trainable parameters to 128,000, requiring less RAM and making fine-tuning more manageable.</li>
</ol>
</section>
<section id="benefits-and-recommendations" class="level4">
<h4 class="anchored" data-anchor-id="benefits-and-recommendations">Benefits and Recommendations</h4>
<ul>
<li><strong>Efficiency:</strong> LoRA enables fine-tuning on less powerful hardware.</li>
<li><strong>Wide Adoption:</strong> It is the dominant fine-tuning method in practice.</li>
<li><strong>Practical Recommendation:</strong> Start with LoRA for most fine-tuning tasks.</li>
</ul>
</section>
</section>
<section id="qlora-enhancing-efficiency-with-quantization" class="level3">
<h3 class="anchored" data-anchor-id="qlora-enhancing-efficiency-with-quantization">QLoRA: Enhancing Efficiency with Quantization</h3>
<ul>
<li>QLoRA builds upon LoRA by using quantization to further reduce memory requirements.</li>
</ul>
<section id="quantization" class="level4">
<h4 class="anchored" data-anchor-id="quantization">Quantization</h4>
<ul>
<li><strong>Bit Representation:</strong> Numbers in computers are stored using bits. More bits allow for finer numerical representation.</li>
<li><strong>QLoRA’s Quantization:</strong> Reduces the number of bits used to store the weights in the LoRA matrices (e.g., from 16 bits to 4 bits). This limits the possible values but significantly reduces memory usage.</li>
<li><strong>Potential Issue:</strong> Training with QLoRA involves quantizing weights, which introduces quantization errors. When these LoRAs are merged back into the original model, the resulting model differs slightly from the trained version.
<ul>
<li>This discrepancy arises because the quantized weights are not identical to the original weights, leading to slight variations in the model’s behavior.</li>
</ul></li>
</ul>
</section>
<section id="practical-implications" class="level4">
<h4 class="anchored" data-anchor-id="practical-implications">Practical Implications</h4>
<ul>
<li><strong>RAM Savings:</strong> QLoRA enables fine-tuning with even less RAM compared to standard LoRA.</li>
<li><strong>Minimal Performance Impact:</strong> While some accuracy trade-off is expected, the practical impact on results is often smaller than anticipated.</li>
<li><strong>Common Practice:</strong> Many practitioners use QLoRA as their default fine-tuning method due to its efficiency.</li>
</ul>
</section>
</section>
<section id="importance-of-data-quality" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-data-quality">Importance of Data Quality</h3>
<section id="data-improvement-over-hyperparameter-tuning" class="level4">
<h4 class="anchored" data-anchor-id="data-improvement-over-hyperparameter-tuning">Data Improvement over Hyperparameter Tuning</h4>
<ul>
<li>Many ML practitioners often prioritize hyperparameter optimization over data quality.</li>
<li>The impact of improving data quality on model performance is significantly higher than that of hyperparameter tuning.</li>
<li>Many successful LLM fine-tuners, like Teknium, creator of the Hermes models, prioritize high-quality data and data synthesis over deep mathematical understanding of the underlying transformer models.</li>
<li>Fine-Tuning After Dataset Improvement
<ul>
<li><strong>Scenario:</strong> You’ve fine-tuned a model, then improved and expanded your dataset. Should you continue fine-tuning the existing model or start from scratch?</li>
<li><strong>Recommendation:</strong> Start fine-tuning from the base model again using the enhanced dataset.</li>
</ul></li>
</ul>
</section>
<section id="axolotl-simplifying-ml-and-enabling-data-focus" class="level4">
<h4 class="anchored" data-anchor-id="axolotl-simplifying-ml-and-enabling-data-focus">Axolotl: Simplifying ML and Enabling Data Focus</h4>
<ul>
<li>Axolotl abstracts away the complexities of transformers, enabling users to focus on data and model training without needing in-depth technical knowledge.</li>
<li>Axolotl’s user-friendliness, enables a shift in focus from code debugging to data analysis.</li>
<li>This ease of use allows for more experimentation and exploration of the data, leading to a better understanding of the problem and potentially better solutions.</li>
</ul>
</section>
<section id="axolotl-built-in-best-practices-and-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="axolotl-built-in-best-practices-and-efficiency">Axolotl: Built-in Best Practices and Efficiency</h4>
<ul>
<li>Axolotl comes with sensible default values and best practices, saving users time and effort.</li>
<li>The speaker mentions “sample packing” as a specific example of a clever optimization feature within Axolotl that speeds up training.</li>
<li>Users can leverage these pre-built optimizations instead of spending time figuring them out independently.</li>
</ul>
</section>
</section>
</section>
<section id="axolotl-framework-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="axolotl-framework-for-fine-tuning">Axolotl Framework for Fine-Tuning</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<ul>
<li><strong>Axolotl</strong> simplifies the process of fine-tuning LLMs by providing a user-friendly wrapper for lower-level Hugging Face libraries.</li>
<li><strong>Key Features:</strong>
<ul>
<li><strong>Examples:</strong> Provides numerous example configuration (YAML) files as starting points for different fine-tuning scenarios.</li>
<li><strong>Flexibility:</strong> Supports various data formats, training techniques (LoRA, QLoRA, DeepSpeed, FSDP), and integrations with tools like Weights &amp; Biases.</li>
<li><strong>Ease of Use:</strong> Abstracts away many complexities, allowing users to focus on data and model experimentation.</li>
</ul></li>
</ul>
</section>
<section id="documentation" class="level3">
<h3 class="anchored" data-anchor-id="documentation">Documentation</h3>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/OpenAccess-AI-Collective/axolotl">https://github.com/OpenAccess-AI-Collective/axolotl</a>.
<ul>
<li>The README file contains most of the information needed to get started.</li>
<li>Follow the <a href="https://github.com/OpenAccess-AI-Collective/axolotl?tab=readme-ov-file#quickstart-">Quickstart guide</a> for step-by-step instructions on preprocessing, training, and testing a fine-tuned model.</li>
</ul></li>
<li><strong>Documentation Site:</strong> <a href="https://openaccess-ai-collective.github.io/axolotl/">https://openaccess-ai-collective.github.io/axolotl/</a></li>
<li><strong>Discord server:</strong> <a href="https://discord.gg/HhrNrHJPRb">https://discord.gg/HhrNrHJPRb</a></li>
</ul>
</section>
<section id="installation" class="level3">
<h3 class="anchored" data-anchor-id="installation">Installation</h3>
<section id="create-a-python-environment" class="level4">
<h4 class="anchored" data-anchor-id="create-a-python-environment">Create a Python Environment</h4>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Conda</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Mamba</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--name</span> axolotl-env python=3.11 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span></span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> activate axolotl-env</span></code></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">mamba</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--name</span> axolotl-env python=3.11 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span></span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">mamba</span> activate axolotl-env</span></code></pre></div>
</div>
</div>
</div>
</section>
<section id="install-dependencies" class="level4">
<h4 class="anchored" data-anchor-id="install-dependencies">Install Dependencies</h4>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--index-url</span> https://download.pytorch.org/whl/cu121</span>
<span id="cb3-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install packaging ninja</span></code></pre></div>
</section>
<section id="install-axolotl" class="level4">
<h4 class="anchored" data-anchor-id="install-axolotl">Install Axolotl</h4>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> clone https://github.com/OpenAccess-AI-Collective/axolotl</span>
<span id="cb4-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> axolotl</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.[flash-attn,deepspeed]'</span></span></code></pre></div>
</section>
</section>
<section id="configuration-files" class="level3">
<h3 class="anchored" data-anchor-id="configuration-files">Configuration Files</h3>
<ul>
<li><p>Define essential settings for training, including the base model, dataset, LoRA parameters, and more</p></li>
<li><p>Axolotl configurations are written in YAML and can be complex.</p>
<ul>
<li><strong><a href="https://openaccess-ai-collective.github.io/axolotl/docs/config.html">Config options</a>:</strong> A complete list of all configuration options.</li>
</ul></li>
<li><p>Start with an example config file and modify it according to your specific needs.</p>
<ul>
<li>The “examples” directory in the repo is a good place to start.</li>
<li>Copy an example and run it as-is before making modifications.</li>
<li>Focus on changing the dataset initially, then explore other parameters as needed.</li>
</ul></li>
<li><p>Leverage the Axolotl Discord channel for sharing and finding configurations.</p></li>
<li><p>Example: Fine-tuning a Mistral 7b model with QLORA.</p>
<ul>
<li>Specifies the base model, data set, storage location, and various hyperparameters.</li>
<li><div class="callout callout-style-default callout-note callout-titled" title="YAML Config">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
YAML Config
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">base_model</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> mistralai/Mistral-7B-v0.1</span></span>
<span id="cb6-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">model_type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> MistralForCausalLM</span></span>
<span id="cb6-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tokenizer_type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> LlamaTokenizer</span></span>
<span id="cb6-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">is_mistral_derived_model</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-5"></span>
<span id="cb6-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">load_in_8bit</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">load_in_4bit</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">strict</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-9"></span>
<span id="cb6-10"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_fan_in_fan_out</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data_seed</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">49</span></span>
<span id="cb6-12"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seed</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">49</span></span>
<span id="cb6-13"></span>
<span id="cb6-14"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">datasets</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-15"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">path</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> _synth_data/alpaca_synth_queries_healed.jsonl</span></span>
<span id="cb6-16"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> sharegpt</span></span>
<span id="cb6-17"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">conversation</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> alpaca</span></span>
<span id="cb6-18"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dataset_prepared_path</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> last_run_prepared</span></span>
<span id="cb6-19"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">val_set_size</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb6-20"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">output_dir</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> ./qlora-alpaca-out</span></span>
<span id="cb6-21"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">hub_model_id</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> hamel/hc-mistral-alpaca</span></span>
<span id="cb6-22"></span>
<span id="cb6-23"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">adapter</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> qlora</span></span>
<span id="cb6-24"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_model_dir</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-25"></span>
<span id="cb6-26"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sequence_len</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">896</span></span>
<span id="cb6-27"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample_packing</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-28"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pad_to_sequence_len</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-29"></span>
<span id="cb6-30"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_r</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb6-31"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_alpha</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span></span>
<span id="cb6-32"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_dropout</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span></span>
<span id="cb6-33"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_target_linear</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-34"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_fan_in_fan_out</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-35"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_target_modules</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-36"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> gate_proj</span></span>
<span id="cb6-37"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> down_proj</span></span>
<span id="cb6-38"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> up_proj</span></span>
<span id="cb6-39"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> q_proj</span></span>
<span id="cb6-40"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> v_proj</span></span>
<span id="cb6-41"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> k_proj</span></span>
<span id="cb6-42"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">-</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> o_proj</span></span>
<span id="cb6-43"></span>
<span id="cb6-44"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wandb_project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> hc-axolotl-mistral</span></span>
<span id="cb6-45"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wandb_entity</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> hamelsmu</span></span>
<span id="cb6-46"></span>
<span id="cb6-47"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gradient_accumulation_steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb6-48"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">micro_batch_size</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span></span>
<span id="cb6-49"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eval_batch_size</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span></span>
<span id="cb6-50"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">num_epochs</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb6-51"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">optimizer</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> adamw_bnb_8bit</span></span>
<span id="cb6-52"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lr_scheduler</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> cosine</span></span>
<span id="cb6-53"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">learning_rate</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0002</span></span>
<span id="cb6-54"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max_grad_norm</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span></span>
<span id="cb6-55"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">adam_beta2</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span></span>
<span id="cb6-56"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">adam_epsilon</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.00001</span></span>
<span id="cb6-57"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">save_total_limit</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb6-58"></span>
<span id="cb6-59"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">train_on_inputs</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-60"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by_length</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-61"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bf16</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-62"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fp16</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-63"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tf32</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">false</span></span>
<span id="cb6-64"></span>
<span id="cb6-65"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gradient_checkpointing</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-66"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">early_stopping_patience</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-67"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">resume_from_checkpoint</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-68"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">local_rank</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-69"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">logging_steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-70"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xformers_attention</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-71"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flash_attention</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb6-72"></span>
<span id="cb6-73"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">loss_watchdog_threshold</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.0</span></span>
<span id="cb6-74"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">loss_watchdog_patience</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb6-75"></span>
<span id="cb6-76"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">warmup_steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb6-77"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">evals_per_epoch</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb6-78"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eval_table_size</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-79"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eval_table_max_new_tokens</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb6-80"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">saves_per_epoch</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span></span>
<span id="cb6-81"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">debug</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-82"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">weight_decay</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb6-83"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fsdp</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-84"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fsdp_config</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-85"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">special_tokens</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb6-86"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bos_token</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;s&gt;"</span></span>
<span id="cb6-87"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eos_token</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;/s&gt;"</span></span>
<span id="cb6-88"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">unk_token</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;unk&gt;"</span></span>
<span id="cb6-89"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">save_safetensors</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span></code></pre></div>
</div>
</div>
</div></li>
</ul></li>
</ul>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<ul>
<li><p>Axolotl supports various data formats, including the common “alpaca” format.</p>
<ul>
<li><a href="https://huggingface.co/datasets/mhenrichsen/alpaca_2k_test">Example Dataset:</a> <a href="https://huggingface.co/datasets/mhenrichsen/alpaca_2k_test">https://huggingface.co/datasets/mhenrichsen/alpaca_2k_test</a></li>
</ul></li>
<li><p>Data format determines the structure of your training data.</p></li>
<li><div class="callout callout-style-default callout-note callout-titled" title="Alpaca Format">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alpaca Format
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<pre class="text"><code>&lt;start&gt;
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}
&lt;end&gt;</code></pre>
</div>
</div>
</div>
<ul>
<li>Each sample contains an instruction, optional input, and desired response.</li>
<li>The model is trained to predict the text following the “Response:” section.</li>
</ul></li>
</ul>
</section>
<section id="preprocessing-data" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing-data">Preprocessing Data</h3>
<ul>
<li>Axolotl preprocesses data into a string format for training.</li>
<li>The preprocessing step defines the template for formatting the input string.</li>
<li>It’s crucial to understand the chosen data template (e.g., alpaca, chatML) and how it impacts model training.</li>
</ul>
</section>
<section id="training" class="level3">
<h3 class="anchored" data-anchor-id="training">Training</h3>
<ul>
<li><p>Axolotl uses a string and a mask for training.</p>
<ul>
<li>The mask prevents certain parts of the input from influencing the loss function.
<ul>
<li>This ensures the model focuses on generating the desired output rather than replicating the entire input string.</li>
</ul></li>
<li>Use the “train on inputs” flag to modify this behavior.</li>
</ul></li>
<li><p>The debug flag in the preprocessing step helps visualize the tokenization and masking process.</p></li>
<li><p>Monitor progress and metrics using tools like Weights &amp; Biases.</p></li>
</ul>
</section>
<section id="running-axolotl" class="level3">
<h3 class="anchored" data-anchor-id="running-axolotl">Running Axolotl</h3>
<ul>
<li><p>The <a href="https://github.com/OpenAccess-AI-Collective/axolotl?tab=readme-ov-file#quickstart-">Quickstart section</a> of the readme provides the necessary commands.</p></li>
<li><p>Three main steps:</p>
<ol type="1">
<li><p>Preprocess the data.</p>
<ul>
<li><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb8-1">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># preprocess datasets - optional but recommended</span></span>
<span id="cb8-2">  <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">CUDA_VISIBLE_DEVICES</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> axolotl.cli.preprocess examples/openllama-3b/lora.yml</span></code></pre></div></li>
</ul></li>
<li><p>Train the model.</p>
<ul>
<li><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb9-1">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># finetune lora</span></span>
<span id="cb9-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">accelerate</span> launch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> axolotl.cli.train examples/openllama-3b/lora.yml</span></code></pre></div></li>
</ul></li>
<li><p>Test the model using the CLI tool or a lightweight Gradio app.</p>
<ul>
<li><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb10-1">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># inference</span></span>
<span id="cb10-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">accelerate</span> launch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> axolotl.cli.inference examples/openllama-3b/lora.yml <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb10-3">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--lora_model_dir</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./outputs/lora-out"</span></span></code></pre></div></li>
<li><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># gradio</span></span>
<span id="cb11-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">accelerate</span> launch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> axolotl.cli.inference examples/openllama-3b/lora.yml <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb11-3">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--lora_model_dir</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./outputs/lora-out"</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--gradio</span></span></code></pre></div></li>
<li><p>The Gradio app allows for easy testing and interaction with the trained model in a web browser.</p>
<ul>
<li>The Gradio app is for testing and not for production use.</li>
</ul></li>
</ul></li>
</ol></li>
</ul>
</section>
</section>
<section id="honeycomb-case-study-fine-tuning-llms-for-natural-language-querying" class="level2">
<h2 class="anchored" data-anchor-id="honeycomb-case-study-fine-tuning-llms-for-natural-language-querying">Honeycomb Case Study: Fine-tuning LLMs for Natural Language Querying</h2>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/parlance-labs/ftcourse">https://github.com/parlance-labs/ftcourse</a></li>
<li><strong>Finetuned Model:</strong> <a href="https://huggingface.co/parlance-labs/hc-mistral-alpaca">https://huggingface.co/parlance-labs/hc-mistral-alpaca</a></li>
</ul>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li><strong>Honeycomb:</strong> An observability platform with a domain-specific query language (HQL).</li>
<li><strong>Goal:</strong> Reduce the burden of learning HQL by allowing users to input natural language queries.</li>
<li><strong>Approach:</strong> Fine-tune an LLM to translate natural language queries into HQL.</li>
</ul>
</section>
<section id="initial-setup" class="level3">
<h3 class="anchored" data-anchor-id="initial-setup">Initial Setup</h3>
<ul>
<li><strong>Prompt Design:</strong>
<ul>
<li>System prompt introduces Honeycomb AI and its purpose.</li>
<li>User schema (retrieved via RAG) provides context about the data.</li>
<li>Fixed elements: Query specification (terse HQL guide), tips, and few-shot examples.</li>
<li>Completion model completes user queries into HQL using the provided context.</li>
</ul></li>
<li><strong>Limitations:</strong> Prompt engineering alone is insufficient for complex scenarios. Fine-tuning allows for tailored improvements.</li>
<li><strong>Initial Evaluation (Level 1: Unit Tests):</strong>
<ul>
<li>Implemented unit tests with assertions to check query validity and correctness.</li>
<li>Validations included: JSON validity, invalid columns, invalid filters.</li>
<li>Iteratively refined assertions as new failure modes were discovered.</li>
</ul></li>
</ul>
</section>
<section id="data-acquisition-and-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-acquisition-and-preparation">Data Acquisition and Preparation</h3>
<ul>
<li><strong>Challenge:</strong> Limited real-world data due to privacy concerns and new product launch.</li>
<li><strong>Solution:</strong> Generate synthetic data using LLMs:
<ul>
<li>Prompt engineering: Instructed LLM to augment existing data by rewording queries, substituting columns, and modifying queries while maintaining valid structure.</li>
<li>Utilized level 1 assertions to filter out nonsensical generated queries.</li>
</ul></li>
<li><strong>Data Formatting:</strong>
<ul>
<li>Prepared data in shared GPT-alpaca format for compatibility with Axolotl.</li>
<li>Structured as conversations with system, human (input), and GPT (output) roles to align with Axolotl’s input handling and training objectives.</li>
</ul></li>
<li><strong>Config Setup:</strong>
<ul>
<li>Utilized Axolotl’s “sharegpt” and “alpaca” config for data format.
<ul>
<li><strong><a href="https://openaccess-ai-collective.github.io/axolotl/docs/dataset-formats/conversation.html#sharegpt">sharegpt</a>:</strong> conversations where <code>from</code> is <code>human</code>/<code>gpt</code>.</li>
</ul></li>
<li>Set “train on inputs” to false to focus the model on generating correct HQL queries.</li>
<li>Updated Weights &amp; Biases and Hugging Face credentials for logging and model uploading.</li>
</ul></li>
</ul>
</section>
<section id="pre-training-data-validation" class="level3">
<h3 class="anchored" data-anchor-id="pre-training-data-validation">Pre-Training Data Validation</h3>
<ul>
<li><strong>Importance:</strong> Verifying data preparation and identifying potential issues before training.</li>
<li><strong>Process:</strong>
<ul>
<li>Executed <code>axolotl preprocess</code> to flatten and assemble data into Axolotl’s expected format.</li>
<li>Manually inspected preprocessed data within the <code>last_run/prepared</code> directory to ensure correct formatting and alignment with expectations.</li>
<li>Paid close attention to special tokens, spacing, and overall structure.</li>
</ul></li>
<li><strong>Tokenization:</strong>
<ul>
<li>Investigated potential tokenization inconsistencies and their impact on model performance.</li>
<li>Determined that minor inconsistencies, like extra spaces, didn’t significantly affect the final results as long as consistency was maintained during inference.</li>
</ul></li>
</ul>
</section>
<section id="training-1" class="level3">
<h3 class="anchored" data-anchor-id="training-1">Training</h3>
<ul>
<li><strong>Base Model:</strong> Mistral 7b</li>
<li><strong>Training Setup:</strong>
<ul>
<li>Referred to examples and community resources for initial Mistral configuration.</li>
<li>Experimented with different learning rates, learning rate schedulers, and batch sizes to optimize performance.</li>
</ul></li>
<li><strong>Tools:</strong>
<ul>
<li>Used Accelerate for efficient training management and distribution (covered in more detail later).</li>
<li>Integrated with Weights &amp; Biases for logging training metrics and visualizing progress.</li>
<li>Utilized Hugging Face to store and share the trained model.</li>
</ul></li>
<li><strong>Results:</strong> Model trained successfully and uploaded to Hugging Face.</li>
</ul>
</section>
<section id="sanity-checking-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="sanity-checking-and-evaluation">Sanity Checking and Evaluation</h3>
<ul>
<li><p><strong>Blog Post:</strong> <a href="https://hamel.dev/blog/posts/evals/">Your AI Product Needs Evals</a></p></li>
<li><p><strong>Sanity Check:</strong></p>
<ul>
<li>Pulled the trained model from Hugging Face.</li>
<li>Designed a template to feed natural language queries and schemas to the model.</li>
<li>Tested the model with sample inputs to verify basic functionality and output structure.</li>
</ul></li>
<li><p><strong>Level 1 Evaluation:</strong> Monitored level 1 evals (assertions) to track query correctness and identify failing assertions.</p></li>
<li><p><strong>Level 2 Evaluation:</strong></p>
<ul>
<li><strong>Challenge:</strong> Level 1 evals passed, but model output quality wasn’t satisfactory. Synthetic data and model outputs required further refinement.</li>
<li><strong>Solution:</strong> Built an LLM-based query judge:
<ul>
<li>Provided a prompt with instructions to act as a query evaluator.</li>
<li>Included few-shot examples of queries with human-written critiques.</li>
<li>Used a spreadsheet to gather critiques from Philip (domain expert) on model-generated queries.</li>
<li>Aligned the LLM judge with Philip’s feedback by iteratively refining the prompt and examples.</li>
</ul></li>
<li><strong>Outcome:</strong> The LLM judge provided valuable insights into model shortcomings and areas for data improvement.</li>
</ul></li>
</ul>
</section>
<section id="data-curation-and-filtering" class="level3">
<h3 class="anchored" data-anchor-id="data-curation-and-filtering">Data Curation and Filtering</h3>
<ul>
<li><strong>Goal:</strong> Enhance the quality and diversity of the training data.</li>
<li><strong>Approaches:</strong>
<ul>
<li><strong>Fixing Bad Data:</strong> Used the LLM judge’s critiques to automatically improve flawed queries using an LLM.</li>
<li><strong>Filtering:</strong>
<ul>
<li>Utilized level 1 and level 2 evals to filter out incorrect and low-quality queries.</li>
<li>Implemented custom filters to remove low-complexity and overly complex queries.</li>
<li>Applied deduplication techniques to remove redundant data points.
<ul>
<li>Basic deduplication: Removed entries with identical natural language queries, schemas, or outputs.</li>
<li>Semantic deduplication: Explored tools like Lilac for fuzzy concept search and clustering to identify semantically similar entries and maximize data diversity.</li>
</ul></li>
</ul></li>
<li><strong>Tools:</strong>
<ul>
<li><strong><a href="https://github.com/lilacai/lilac">lilac</a>:</strong> A tool for exploration, curation and quality control of datasets for training, fine-tuning and monitoring LLMs</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="iteration-and-refinement" class="level3">
<h3 class="anchored" data-anchor-id="iteration-and-refinement">Iteration and Refinement</h3>
<ul>
<li>The process of fine-tuning and evaluation was iterative.</li>
<li>Continuously revisited and refined different stages, including:
<ul>
<li>Updating level 1 and level 2 evals based on new insights and failure modes.</li>
<li>Retraining the model with the curated and improved dataset.</li>
<li>Re-evaluating the model using both automated and human-based methods.</li>
</ul></li>
</ul>
</section>
<section id="takeaways" class="level3">
<h3 class="anchored" data-anchor-id="takeaways">Takeaways</h3>
<p>This case study highlights the importance of a holistic approach to fine-tuning LLMs. It emphasizes the significance of: * Iterative development and evaluation. * Combining automated and human-in-the-loop techniques. * Building tools and processes that incorporate domain expertise. * Going beyond basic metrics and deeply analyzing data and model behavior to identify areas for improvement.</p>
</section>
</section>
<section id="debugging-axolotl" class="level2">
<h2 class="anchored" data-anchor-id="debugging-axolotl">Debugging Axolotl</h2>
<ul>
<li><strong>Use the latest version of Axolotl:</strong> Ensure you’re using the most up-to-date version to avoid known issues.</li>
<li><strong>Eliminate concurrency:</strong>
<ul>
<li>Use only one GPU.</li>
<li>Use a single dataset process.</li>
<li>This helps pinpoint the source of problems.</li>
</ul></li>
<li><strong>Minimize iteration time:</strong>
<ul>
<li>Use a small dataset for faster processing.</li>
<li>Start with a small model for quicker training and testing.</li>
</ul></li>
<li><strong>Clear caches:</strong>
<ul>
<li>This is especially critical for debugging dataset formation issues.</li>
<li>Cached data can lead to unexpected behavior and misinterpretations.</li>
</ul></li>
<li><strong>Debugging Tools:</strong>
<ul>
<li>Axolotl’s documentation provides detailed guidance on debugging techniques.
<ul>
<li><strong>Debugging How-To Guide:</strong> <a href="https://openaccess-ai-collective.github.io/axolotl/docs/debugging.html">https://openaccess-ai-collective.github.io/axolotl/docs/debugging.html</a></li>
</ul></li>
<li>Consider using VS Code to connect to Docker containers for easier debugging.</li>
</ul></li>
</ul>
</section>
<section id="scaling-model-training-with-more-compute" class="level2">
<h2 class="anchored" data-anchor-id="scaling-model-training-with-more-compute">Scaling Model Training with More Compute</h2>
<ul>
<li><strong>Source Slides:</strong> <a href="https://huggingface.co/spaces/muellerzr/llm-conf">https://huggingface.co/spaces/muellerzr/llm-conf</a></li>
</ul>
<section id="model-gpu-usage" class="level3">
<h3 class="anchored" data-anchor-id="model-gpu-usage">Model GPU Usage</h3>
<ul>
<li><p><strong>🤗 Model Memory Calculator:</strong> <a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</a></p></li>
<li><p><strong>Calculating memory requirements:</strong> To estimate GPU memory needed for training, consider:</p>
<ul>
<li>Each parameter uses 4 bytes.</li>
<li>Backward pass needs ~2x the model size.</li>
<li>Optimizer (e.g., Adam) needs ~4x the model size.</li>
</ul></li>
<li><p><strong>Example:</strong> BERT base (108 million parameters)</p>
<ul>
<li><p>Full precision Training: ~1.6 GB</p></li>
<li><p>Mixed precision Training: ~1-2 GB (gradients in half precision)</p></li>
<li><table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>dtype</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Gradients</th>
<th style="text-align: center;">Backward pass</th>
<th style="text-align: center;">Optimizer step</th>
<th style="text-align: center;">Highest</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float32</td>
<td style="text-align: left;">413.18 MB</td>
<td style="text-align: center;">413.18 MB</td>
<td style="text-align: center;">826.36 MB</td>
<td style="text-align: center;">1.61 GB</td>
<td style="text-align: center;">1.61 GB</td>
</tr>
<tr class="even">
<td>float16</td>
<td style="text-align: left;">413.18 MB</td>
<td style="text-align: center;">619.77 MB</td>
<td style="text-align: center;">826.36 MB</td>
<td style="text-align: center;">826.36 MB</td>
<td style="text-align: center;">826.36 MB</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li><p><strong>Scaling challenges:</strong> LLMs like LLaMA 3 8B require significant VRAM</p>
<ul>
<li><p>Full precision Training: ~112 GB</p></li>
<li><table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>dtype</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Gradients</th>
<th style="text-align: center;">Backward pass</th>
<th style="text-align: center;">Optimizer step</th>
<th style="text-align: center;">Highest</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float32</td>
<td style="text-align: left;">28.21 GB</td>
<td style="text-align: center;">28.21 GB</td>
<td style="text-align: center;">56.43 GB</td>
<td style="text-align: center;">112.84 GB</td>
<td style="text-align: center;">112.84 GB</td>
</tr>
<tr class="even">
<td>float16</td>
<td style="text-align: left;">28.21 GB</td>
<td style="text-align: center;">42.32 GB</td>
<td style="text-align: center;">56.43 GB</td>
<td style="text-align: center;">56.43 GB</td>
<td style="text-align: center;">56.43 GB</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul>
</section>
<section id="distributed-training" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training">Distributed Training</h3>
<ul>
<li><strong>Single GPU:</strong> Simplest approach, limited by single GPU memory.</li>
<li><strong>Distributed Data Parallelism (DDP):</strong>
<ul>
<li>Full model on each GPU.</li>
<li>Data split across GPUs for faster processing.</li>
<li>Limited by the memory capacity of a single GPU.</li>
</ul></li>
<li><strong>Fully Sharded Data Parallelism (FSDP) &amp; DeepSpeed:</strong>
<ul>
<li>Model, optimizer states, and gradients sharded across GPUs.</li>
<li>Enables training models larger than a single GPU’s memory.</li>
</ul></li>
</ul>
</section>
<section id="fully-sharded-data-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="fully-sharded-data-parallelism">Fully Sharded Data Parallelism</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" aria-controls="tabset-2-1" aria-selected="true">Standard data parallel training</a></li><li class="nav-item"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" aria-controls="tabset-2-2" aria-selected="false">Fully sharded data parallel training</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" aria-labelledby="tabset-2-1-tab">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/workshop-002/images/sdp.png" class="img-fluid"></p>
</div>
<div id="tabset-2-2" class="tab-pane" aria-labelledby="tabset-2-2-tab">
<p><img src="christianjmills.com/posts/mastering-llms-course-notes/workshop-002/images/fsdp.png" class="img-fluid"></p>
</div>
</div>
</div>
<ul>
<li><strong>Sharding strategies:</strong>
<ul>
<li><strong>Full Shard:</strong> Everything (optimizer state, gradients, parameters) is sharded.</li>
<li><strong>Shard Grad Op:</strong> Optimizer state and gradients sharded, model joined during backward pass.</li>
<li><strong>No Shard:</strong> Equivalent to DDP.</li>
<li><strong>Hybrid Shard:</strong> Similar to Full Shard, but keeps a full model copy on each multi-node for faster communication.</li>
</ul></li>
<li><strong>Model splitting techniques:</strong>
<ul>
<li><strong>Transformer-based:</strong> Split at Transformer layer boundaries.</li>
<li><strong>Size-based:</strong> Split after a specified number of parameters.</li>
</ul></li>
<li><strong>Offloading parameters:</strong>
<ul>
<li>Offloads gradients and parameters to RAM when VRAM is insufficient.</li>
<li>Significantly slower due to data transfer between CPU and GPU.
<ul>
<li>72 hrs on 2x4090s vs 1-2 hrs on 1xH100 for a full finetune of LLaMA 3 8B</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="additional-considerations" class="level3">
<h3 class="anchored" data-anchor-id="additional-considerations">Additional Considerations</h3>
<ul>
<li><strong>CPU RAM-efficient loading:</strong>
<ul>
<li>Use <code>device='meta'</code> to create a model skeleton without loading weights into memory.</li>
<li>Load weights only on one GPU and distribute to others when needed, saving CPU RAM.</li>
</ul></li>
<li><strong>Sync Module States:</strong> Ensures consistent model states across GPUs, crucial for FSDP.</li>
</ul>
</section>
</section>
<section id="scaling-model-training-with-accelerate" class="level2">
<h2 class="anchored" data-anchor-id="scaling-model-training-with-accelerate">Scaling Model Training with Accelerate</h2>
<ul>
<li><strong>Source Slides:</strong> <a href="https://huggingface.co/spaces/muellerzr/llm-conf">https://huggingface.co/spaces/muellerzr/llm-conf</a></li>
</ul>
<section id="accelerate-the-foundation" class="level3">
<h3 class="anchored" data-anchor-id="accelerate-the-foundation">Accelerate: The Foundation</h3>
<ul>
<li>Many popular libraries, such as HuggingFace Transformers, Axolotl, FastAI, and more, are built upon Accelerate.</li>
<li>Accelerate simplifies the complexities of distributed training, making it user-friendly.</li>
</ul>
</section>
<section id="core-components-of-accelerate" class="level3">
<h3 class="anchored" data-anchor-id="core-components-of-accelerate">Core Components of Accelerate</h3>
<p>Accelerate consists of three primary frameworks:</p>
<ul>
<li><strong>Command-Line Interface (CLI):</strong> Facilitates easy interaction and configuration, as showcased in Accelerate launch demonstrations.</li>
<li><strong>Training Library:</strong> This underlying engine powers distributed training, streamlining the process significantly.</li>
<li><strong>Big Model Inference:</strong> Designed for handling inference in large models.</li>
</ul>
</section>
<section id="key-commands-in-accelerate" class="level3">
<h3 class="anchored" data-anchor-id="key-commands-in-accelerate">Key Commands in Accelerate</h3>
<ul>
<li><strong><code>accelerate config</code>:</strong>
<ul>
<li>Configures the training environment.</li>
<li>Integrates seamlessly with configurations used in tools like Accelerate launch, allowing for consistent settings across different stages.</li>
</ul></li>
<li><strong><code>accelerate estimate-memory</code>:</strong>
<ul>
<li>Calculates and estimates memory requirements, particularly VRAM usage, which is crucial for efficient training.</li>
</ul></li>
<li><strong><code>accelerate launch</code>:</strong>
<ul>
<li>Executes the training script.</li>
</ul></li>
</ul>
</section>
<section id="why-accelerate-matters-simplifying-distributed-training" class="level3">
<h3 class="anchored" data-anchor-id="why-accelerate-matters-simplifying-distributed-training">Why Accelerate Matters: Simplifying Distributed Training</h3>
<ul>
<li>Launching and managing distributed training can be complicated, often involving different commands and setups for PyTorch, DeepSpeed, and other tools.</li>
<li>Accelerate simplifies this process. Running a basic Python script often lacks distributed training capabilities, especially distributed data parallelism.<br>
</li>
<li><strong>Example:</strong> Without Accelerate, you would need to use specific commands like <code>torchrun</code> with multiple arguments for running a script on two GPUs.</li>
<li><code>accelerate launch</code> streamlines this process by handling the complexities and allowing users to specify the desired configuration without needing to remember numerous commands.</li>
</ul>
</section>
<section id="configuration-and-execution-with-accelerate" class="level3">
<h3 class="anchored" data-anchor-id="configuration-and-execution-with-accelerate">Configuration and Execution with Accelerate</h3>
<ul>
<li>Accelerate employs config files (similar to Axolotl) to define training parameters.</li>
<li><strong>Example:</strong> A config file can specify using a local multi-GPU setup with BF16 mixed precision on eight GPUs.<br>
</li>
<li>When using FSDP, the config file can explicitly define all FSDP parameters. <code>accelerate launch</code> will automatically utilize FSDP for training based on these settings.</li>
<li>For users of Axolotl or Transformers, these config files within Accelerate offer a straightforward way to manage training configurations.</li>
</ul>
</section>
<section id="accelerate-internals-a-low-level-view" class="level3">
<h3 class="anchored" data-anchor-id="accelerate-internals-a-low-level-view">Accelerate Internals: A Low-Level View</h3>
<ul>
<li>While not essential for users primarily interacting with higher-level libraries like Axolotl or Transformers, understanding the inner workings of Accelerate can be beneficial.</li>
<li><strong>Device and Compute Agnostic:</strong> Accelerate is designed to function seamlessly across different operating systems (Mac, Windows) and hardware (CPUs, GPUs, TPUs).</li>
<li><strong>Minimal Intrusion and Complexity:</strong> The library aims to be minimally intrusive, requiring few code changes.
<ul>
<li>Developers create an “accelerator” object that prepares the environment.</li>
<li>Simply replacing the standard backward function with <code>accelerator.backwards</code> is often sufficient for integrating Accelerate.</li>
</ul></li>
</ul>
</section>
<section id="accelerate-in-action-data-sharding-and-global-steps" class="level3">
<h3 class="anchored" data-anchor-id="accelerate-in-action-data-sharding-and-global-steps">Accelerate in Action: Data Sharding and Global Steps</h3>
<ul>
<li><strong>Data Sharding:</strong> Similar to FSDP, Accelerate handles data sharding, distributing data efficiently across multiple GPUs.</li>
<li><strong>Global Step:</strong> It maintains a global training step to ensure consistency across distributed training.</li>
<li><strong>Example:</strong> If training on eight GPUs with Accelerate, instead of a single GPU with a batch size of 16, each GPU would use a batch size of 2 (2 x 8 = 16).</li>
<li>This global step management eliminates the need for manual adjustments to the learning rate or scheduler when scaling training across multiple GPUs, simplifying the process and ensuring comparable results.</li>
</ul>
</section>
<section id="protecting-training-from-dumb-decisions" class="level3">
<h3 class="anchored" data-anchor-id="protecting-training-from-dumb-decisions">Protecting Training from “Dumb Decisions”</h3>
<ul>
<li><p><strong>Avoiding BF16/FP16 conversion of model weights</strong>: Converting weights leads to irreversible precision loss and negatively impacts fine-tuning.</p></li>
<li><p><strong>Using <code>autocast</code> for Gradient Conversion</strong>: <code>autocast</code> preserves weight precision by only converting gradients, leading to more stable training and better fine-tuning capabilities.</p></li>
<li><p><strong>Exploring 8-bit Training with Transformers Engine and MSAMP</strong>:</p>
<ul>
<li><p>These technologies enable training with native 8-bit precision (not quantized versions), potentially offering significant speedups.</p></li>
<li><p>Converting the entire model to BF16 before using these technologies can lead to instability during training and suboptimal performance.</p></li>
<li><p><strong><a href="https://github.com/NVIDIA/TransformerEngine">TransformerEngine</a>:</strong> A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs</p>
<ul>
<li>Utilizes <code>autocast</code> for computations, performing them in 8-bit instead of 16-bit.</li>
</ul></li>
<li><p><strong><a href="https://github.com/Azure/MS-AMP">MS-AMP</a>:</strong> Microsoft Automatic Mixed Precision Library</p>
<ul>
<li>Allows for further experimentation with 8-bit precision, even enabling optimizer states to be stored in 8-bit.</li>
</ul></li>
<li><table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 4%">
<col style="width: 6%">
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Optimization Level</th>
<th>Computation (GEMM)</th>
<th>Comm</th>
<th>Weight</th>
<th>Master Weight</th>
<th>Weight Gradient</th>
<th>Optimizer States</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP16 AMP</td>
<td>FP16</td>
<td>FP32</td>
<td>FP32</td>
<td>N/A</td>
<td>FP32</td>
<td>FP32+FP32</td>
</tr>
<tr class="even">
<td>Nvidia TE</td>
<td>FP8</td>
<td>FP32</td>
<td>FP32</td>
<td>N/A</td>
<td>FP32</td>
<td>FP32+FP32</td>
</tr>
<tr class="odd">
<td>MS-AMP O1</td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>N/A</td>
<td>FP8</td>
<td>FP32+FP32</td>
</tr>
<tr class="even">
<td>MS-AMP O2</td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>N/A</td>
<td>FP8</td>
<td>FP8+FP16</td>
</tr>
<tr class="odd">
<td>MS-AMP O3</td>
<td>FP8</td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>FP8</td>
<td>FP8+FP16</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li><p><strong>Experimentation is Key</strong>: It’s crucial to experiment with different precision levels (FP16, BF16, 8-bit) to find the optimal balance between memory savings and training stability.</p></li>
</ul>
</section>
<section id="deepspeed-and-fsdp" class="level3">
<h3 class="anchored" data-anchor-id="deepspeed-and-fsdp">DeepSpeed and FSDP</h3>
<ul>
<li>Both tools offer comparable functionality and are largely interchangeable.
<ul>
<li>Choosing between DeepSpeed and FSDP often comes down to personal preference - aligning with Microsoft’s ecosystem or staying within the native PyTorch framework.</li>
</ul></li>
</ul>
</section>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li><a href="https://hf.co/docs/accelerate">🤗 Accelerate documentation</a></li>
<li><a href="https://huggingface.co/docs/accelerate/basic_tutorials/launch">Launching distributed code</a></li>
<li><a href="https://huggingface.co/docs/accelerate/basic_tutorials/notebook">Distributed code and Jupyter Notebooks</a></li>
<li><a href="https://huggingface.co/docs/accelerate/basic_tutorials/migration">Migrating to 🤗 Accelerate easily</a></li>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling">Big Model Inference tutorial</a></li>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/deepspeed">DeepSpeed and 🤗 Accelerate</a></li>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/fsdp">Fully Sharded Data Parallelism and 🤗 Accelerate</a></li>
<li><a href="https://huggingface.co/docs/accelerate/concept_guides/fsdp_and_deepspeed">FSDP vs DeepSpeed In-Depth</a></li>
</ul>
</section>
</section>
<section id="deepspeed-and-fsdp-configurations-in-axolotl" class="level2">
<h2 class="anchored" data-anchor-id="deepspeed-and-fsdp-configurations-in-axolotl">DeepSpeed and FSDP Configurations in Axolotl</h2>
<section id="deepspeed-and-fsdp-equivalencies" class="level3">
<h3 class="anchored" data-anchor-id="deepspeed-and-fsdp-equivalencies">DeepSpeed and FSDP Equivalencies</h3>
<ul>
<li><strong>DeepSpeed 03 is now equivalent to FSDP.</strong> This means both offer similar functionalities for distributed training.</li>
<li><strong>Other DeepSpeed options (01, 02) are not directly equivalent to FSDP.</strong></li>
<li><strong>FSDP offers greater customization</strong> by allowing users to specify which components are uploaded.</li>
</ul>
</section>
<section id="axolotl-configuration" class="level3">
<h3 class="anchored" data-anchor-id="axolotl-configuration">Axolotl Configuration</h3>
<ul>
<li><strong>Axolotl uses config files for multi-GPU training with DeepSpeed and FSDP.</strong> These configs streamline the setup process.</li>
<li><strong>Axolotl’s FSDP configs are designed to prevent mismatches with Accelerate configurations.</strong> This approach simplifies configuration and avoids common errors.</li>
<li><strong>Recommendation:</strong> Remove the Accelerate config file when using Axolotl’s FSDP configurations to prevent conflicts. Axolotl will handle the necessary Accelerate settings internally.</li>
</ul>
</section>
<section id="using-configuration-files" class="level3">
<h3 class="anchored" data-anchor-id="using-configuration-files">Using Configuration Files</h3>
<ul>
<li><strong>Axolotl provides pre-built config files for DeepSpeed (01, 02, 03, BF16) and FSDP.</strong> These offer a good starting point for most users.</li>
<li><strong>Start with a pre-built config and adjust as needed.</strong> Consult Zach’s presentations and documentation for advanced customization.</li>
<li><strong>Specify the desired config file within Axolotl’s main config file.</strong></li>
</ul>
</section>
<section id="clarifications-and-tips" class="level3">
<h3 class="anchored" data-anchor-id="clarifications-and-tips">Clarifications and Tips</h3>
<ul>
<li><strong>DeepSpeed 03 (equivalent to FSDP) requires explicit BF16 specification in the config file.</strong> This differs from DeepSpeed 01/02 where ‘auto’ can be used.</li>
<li><strong>Set BF16 directly in the DeepSpeed 03 configuration file.</strong> Failure to do so may cause issues during trainer initialization.</li>
<li><strong>DeepSpeed 01 and 02 can leverage the ‘auto’ setting for BF16 and FP16.</strong> DeepSpeed handles the data type selection after the trainer loads.</li>
</ul>
</section>
</section>
<section id="training-on-modal" class="level2">
<h2 class="anchored" data-anchor-id="training-on-modal">Training on Modal</h2>
<section id="modal-cloud-native-python-development" class="level3">
<h3 class="anchored" data-anchor-id="modal-cloud-native-python-development">Modal: Cloud-Native Python Development</h3>
<ul>
<li><strong>Modal:</strong> Cloud platform simplifying Python code execution, offering a seamless local development experience in a remote environment.
<ul>
<li><strong>Website:</strong> <a href="https://modal.com/">https://modal.com/</a></li>
<li><strong>Documentation:</strong> <a href="https://modal.com/docs/examples">https://modal.com/docs/examples</a></li>
</ul></li>
<li><strong>Key Features:</strong>
<ul>
<li><strong>Local-like Remote Development:</strong> Mimics local development while leveraging remote resources.</li>
<li><strong>Massively Parallel:</strong> Easily parallelizes tasks like hyperparameter tuning, making it efficient for Axolotl.</li>
</ul></li>
</ul>
<section id="understanding-modal" class="level4">
<h4 class="anchored" data-anchor-id="understanding-modal">Understanding Modal</h4>
<ul>
<li><strong>Explore the Documentation:</strong> Detailed Modal documentation is available; start with the “Getting Started” and “Web Endpoint” guides.
<ul>
<li><strong><a href="https://modal.com/docs/guide/webhooks">Web Endpoint Tutorial</a>:</strong> Highlights Modal’s real-time code update capabilities. Modify code and see the changes reflected in production instantly.</li>
</ul></li>
<li><strong>Modal in Action:</strong> Demonstrated through building a transcript summarizer and integrating with tools like Weights &amp; Biases and webhooks.</li>
</ul>
</section>
</section>
<section id="axolotl-and-modal-integration-llm-finetuning-repo" class="level3">
<h3 class="anchored" data-anchor-id="axolotl-and-modal-integration-llm-finetuning-repo">Axolotl and Modal Integration: <code>llm-finetuning</code> Repo</h3>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/modal-labs/llm-finetuning">https://github.com/modal-labs/llm-finetuning</a></li>
<li><strong>Purpose:</strong> Provides a workflow for fine-tuning Axolotl using Modal, abstracting some complexities.</li>
<li><strong>Key Points:</strong>
<ul>
<li><strong>Automatic LoRA Merging:</strong> By default, merges LoRA weights into the base model upon training completion (can be disabled).</li>
<li><strong>Data Flag:</strong> Requires a data flag for specifying the dataset; relying on the config file alone is insufficient.</li>
<li><strong>DeepSpeed Configuration:</strong> Sourced from the Axolotl repository.</li>
</ul></li>
</ul>
<section id="using-the-llm-finetuning-repository" class="level4">
<h4 class="anchored" data-anchor-id="using-the-llm-finetuning-repository">Using the <code>llm-finetuning</code> Repository</h4>
<ul>
<li>Follow the <a href="https://github.com/modal-labs/llm-finetuning?tab=readme-ov-file#quickstart">QuickStart guide</a> in the README.</li>
</ul>
</section>
<section id="code-structure" class="level4">
<h4 class="anchored" data-anchor-id="code-structure">Code Structure</h4>
<ul>
<li><strong><code>src</code> Folder:</strong> Contains Modal-specific code.
<ul>
<li><strong><code>training.py</code>:</strong> Includes the <code>train</code> function that wraps the Axolotl CLI command.</li>
<li><strong><code>common.py</code>:</strong> Handles environment setup, Docker container configuration, dependency installation, and secret management.</li>
</ul></li>
<li><strong>Configuration Files:</strong>
<ul>
<li><strong>Demo Configuration:</strong> Designed for a small, overfitting training run for demonstration purposes. Remember that the data flag overrides the dataset specified in the configuration file.</li>
<li><strong>DeepSpeed Configuration:</strong> Mounted from the Axolotl repository.</li>
</ul></li>
</ul>
</section>
<section id="debugging-data-within-modal" class="level4">
<h4 class="anchored" data-anchor-id="debugging-data-within-modal">Debugging Data Within Modal</h4>
<ul>
<li><strong>Inspecting Data:</strong> Crucial step before full-scale training.</li>
<li><strong>Procedure:</strong>
<ul>
<li>Run <code>modal run</code> with the <code>--preproc-only</code> flag.</li>
<li>Retrieve the run tag from the logs.</li>
<li>Access the last run prepared folder using the run tag.</li>
<li>Analyze the data similarly to the Honeycomb example, ensuring the correct format.</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="qa-session" class="level2">
<h2 class="anchored" data-anchor-id="qa-session">Q&amp;A Session</h2>
<section id="model-size-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="model-size-for-fine-tuning">Model Size for Fine-Tuning</h3>
<ul>
<li><strong>Smaller isn’t always better:</strong> While smaller models (like 5.3B parameters) are available, a 7B model often provides a good balance between performance and computational cost. Smaller models may suffer from inferior reasoning abilities.</li>
<li><strong>Context matters:</strong> The ideal model size depends on the specific use case and available resources.</li>
</ul>
</section>
<section id="adapter-configuration" class="level3">
<h3 class="anchored" data-anchor-id="adapter-configuration">Adapter Configuration</h3>
<ul>
<li><strong>Rank and Alpha:</strong>
<ul>
<li><strong>Rank:</strong> Determines the size of the adapter layers. A good starting point is 32 or 16.</li>
<li><strong>Alpha:</strong> Typically set to 2x the rank.</li>
</ul></li>
<li><strong>Experimentation is key:</strong> Finding the optimal adapter configuration requires experimentation with different values and evaluating the impact on downstream task accuracy.</li>
<li><strong>Resources:</strong>
<ul>
<li>A blog post by Sebastian Roushka discusses grid search for adapter configuration.</li>
<li>Jono Whittaker’s talk “Napkin Math for Fine-Tuning” provides insights into parameter selection.</li>
</ul></li>
</ul>
</section>
<section id="custom-evaluation-during-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="custom-evaluation-during-fine-tuning">Custom Evaluation During Fine-Tuning</h3>
<ul>
<li><strong>Current limitations:</strong> While desired, there is no streamlined way to run custom evaluations periodically during fine-tuning in Axolotl.</li>
<li><strong>Workarounds:</strong>
<ul>
<li>Use the <code>eval_table_size</code> and <code>eval_max_tokens</code> settings in Axolotl to generate and log predictions from your test dataset at specific intervals.</li>
<li>Retrieve logged predictions from Weights &amp; Biases and run custom evaluations externally.</li>
</ul></li>
</ul>
</section>
<section id="axolotl-vs.-lower-level-libraries" class="level3">
<h3 class="anchored" data-anchor-id="axolotl-vs.-lower-level-libraries">Axolotl vs.&nbsp;Lower-Level Libraries</h3>
<ul>
<li><strong>Axolotl Advantages:</strong>
<ul>
<li>Simplifies the fine-tuning process by abstracting away complexities and gluing together different libraries (e.g., integrating QLoRA with FSDP).</li>
<li>Rapidly incorporates new techniques and improvements from the fast-evolving LLM landscape.</li>
</ul></li>
<li><strong>Lower-Level Library Advantages:</strong>
<ul>
<li>Offer greater flexibility and control for advanced use cases.</li>
<li>Enable functionalities not yet implemented in Axolotl, such as custom callbacks during training (e.g., manipulating data between batches).</li>
</ul></li>
</ul>
</section>
<section id="quantization-implications-4-bit-vs.-higher-precision" class="level3">
<h3 class="anchored" data-anchor-id="quantization-implications-4-bit-vs.-higher-precision">Quantization Implications (4-bit vs.&nbsp;Higher Precision)</h3>
<ul>
<li><strong>Trade-offs:</strong>
<ul>
<li><strong>Benefits:</strong> Smaller model size, reduced RAM requirements, potentially faster inference.</li>
<li><strong>Drawbacks:</strong> Possible performance degradation, especially noticeable with 4-bit quantization.</li>
</ul></li>
<li><strong>Recommendations:</strong>
<ul>
<li>Always evaluate the performance impact of quantization on your specific task.</li>
<li>8-bit or 10-bit quantization generally offers a good balance between size reduction and performance.</li>
</ul></li>
<li><strong>QLORA considerations:</strong>
<ul>
<li>QLoRA is beneficial when GPU RAM is limited.</li>
<li>Avoid using QLoRA if you have sufficient GPU RAM, as quantization/dequantization overhead can negate potential speed gains.</li>
</ul></li>
</ul>
</section>
<section id="deterministic-output-generation" class="level3">
<h3 class="anchored" data-anchor-id="deterministic-output-generation">Deterministic Output Generation</h3>
<ul>
<li><strong>Inference vs.&nbsp;Training:</strong>
<ul>
<li>Deterministic output generation involves selecting the most likely token at inference time, rather than randomly sampling from the probability distribution.</li>
</ul></li>
<li><strong>Techniques:</strong>
<ul>
<li><strong>Greedy decoding:</strong> Selecting the most likely token at each step.</li>
<li><strong>Beam search:</strong> Exploring multiple probable decoding paths.</li>
<li><strong>Guided generation:</strong> Constraining token generation using grammars or rules to ensure specific output structures (e.g., JSON).</li>
</ul></li>
<li><strong>Fine-tuning’s role:</strong>
<ul>
<li>Fine-tuning can improve the model’s ability to learn desired syntax and structure, leading to more reliable deterministic outputs.</li>
<li>However, heavily relying on guided generation might indicate a need for better fine-tuning.</li>
</ul></li>
</ul>
</section>
<section id="additional-notes" class="level3">
<h3 class="anchored" data-anchor-id="additional-notes">Additional Notes</h3>
<ul>
<li>Mac M-series GPUs: PyTorch is supported, but MLX is recommended for a better fine-tuning experience.</li>
<li>Agentic LLM Applications: Most, if not all, real-world LLM applications involve function calls, making them “agentic.” Focus on thorough testing, including unit and integration tests, for function call workflows.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/workshop-002/</guid>
  <pubDate>Sun, 09 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>CUDA MODE Lecture 2 : Ch.1-3 PMPP Book</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/cuda-mode-notes/lecture-002/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/cuda-mode-notes.html"><strong>CUDA Mode Lecture Notes</strong></a>: My notes from the <strong>CUDA MODE</strong> reading group lectures run by <strong>Andreas Kopf</strong> and <strong>Mark Saroufim</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Lecture Information</li>
<li>Ch.1: Introduction</li>
<li>Ch.2: Heterogeneous Data Parallel Computing</li>
<li>Ch.3: Multidimensional Grids and Data</li>
</ul>
<section id="lecture-information" class="level2">
<h2 class="anchored" data-anchor-id="lecture-information">Lecture Information</h2>
<p><strong>Speaker:</strong> Andreas Kopf</p>
<p><strong>Topic:</strong> PMPP Book Ch. 1-3</p>
<p><strong>Resources:</strong></p>
<ul>
<li><strong>Lecture Slides:</strong> <a href="https://docs.google.com/presentation/d/1deqvEHdqEC4LHUpStO6z3TT77Dt84fNAvTIAxBJgDck/edit#slide=id.g2b1444253e5_1_75">CUDA Mode: Lecture 2</a></li>
<li><strong>Textbook:</strong> <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/">Programming Massively Parallel Processors</a></li>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/cuda-mode/lectures/tree/main/lecture_002">CUDA MODE Lecture 2</a></li>
<li><strong>Discord Channel:</strong> <a href="https://discord.gg/cudamode">CUDA MODE</a></li>
<li><strong>YouTube Channel:</strong> <a href="https://www.youtube.com/@CUDAMODE">CUDA MODE</a></li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=59R0B3U5I8SLKY8K&amp;t=60">1:00</a></li>
</ul>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<ul>
<li><p>Optimize GPU performance as much as possible</p></li>
<li><p>Applications:</p>
<ul>
<li>simulate and model worlds
<ul>
<li>games</li>
<li>weather</li>
<li>proteins</li>
<li>robotics</li>
</ul></li>
</ul></li>
<li><p>Bigger models are smarter</p>
<ul>
<li>speed and size improvements can have a significant impact on useability</li>
</ul></li>
<li><p>GPUs are the backbon of modern deep learning</p></li>
</ul>
</section>
<section id="history" class="level3">
<h3 class="anchored" data-anchor-id="history">History</h3>
<ul>
<li>Classic software uses sequential programs
<ul>
<li>executed one step at a time</li>
<li>relied on higher CPU clock rates for improved performance</li>
</ul></li>
<li>Higher clock rate trend for CPUs slowed in 2003 due to energy consumption and heat dissipation challenges
<ul>
<li>Increasing frequency would make the chip to hot to cool feasibly</li>
</ul></li>
<li>Multi-core CPU came up
<ul>
<li>Developers had to learn multi-threading
<ul>
<li>New challenges such as deadlocks and race conditions</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="rise-of-cuda" class="level3">
<h3 class="anchored" data-anchor-id="rise-of-cuda">Rise of CUDA</h3>
<ul>
<li>Compute Unified Device Architecture</li>
<li>CUDA is all about parallel programs
<ul>
<li>divide work among threads</li>
</ul></li>
<li>GPUs have much higher peak FLOPS than multi-core CPUs
<ul>
<li>Benefits highly parallelized programs</li>
<li>Not suitable for largely sequential programs</li>
</ul></li>
<li>CPU+GPU
<ul>
<li>Run sequential parts on CPU and numerically intensive parts on GPU</li>
</ul></li>
<li>GPGPU
<ul>
<li>Before CUDA tricks were used to compute with graphics APIs like OpenGL and Direct3D</li>
</ul></li>
<li>GPU programming is now attractive to developers due to massive availability</li>
</ul>
</section>
<section id="amdahls-law" class="level3">
<h3 class="anchored" data-anchor-id="amdahls-law">Amdahl’s Law</h3>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%0Aspeedup%20=%20(%20Slow%20%5C%20System%20%5C%20Time%20)/(Fast%20%5C%20System%20%5C%20Time)%0A"></p></li>
<li><p>achievable speedup is limited by the parallelizable portion of <img src="https://latex.codecogs.com/png.latex?p"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%0Aspeedup%20%3C%20%5Cfrac%7B1%7D%7B1-p%7D%0A"></p>
<ul>
<li>If <img src="https://latex.codecogs.com/png.latex?p"> is <img src="https://latex.codecogs.com/png.latex?90%5C%25">, <img src="https://latex.codecogs.com/png.latex?speedup%20%3C%2010X"></li>
</ul></li>
<li><p><img src="https://latex.codecogs.com/png.latex?p%20%3E%2099%5C%25"> for many real applications</p>
<ul>
<li>especially for large datasets</li>
<li>speedups <img src="https://latex.codecogs.com/png.latex?%3E%20100X"> are attainable</li>
</ul></li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li>“If you do not care about performance, parallel programming is very easy”</li>
<li>In practice, designing parallel algorithms is harder than sequential algorithms
<ul>
<li>Parallelizing recurrent computations requires nonintuitive thinking
<ul>
<li>prefix sum
<ul>
<li><a href="https://en.wikipedia.org/wiki/Prefix_sum">Wikipedia Page</a></li>
</ul></li>
</ul></li>
</ul></li>
<li>Speed is often limited by memory latency/throughput (memory bound)
<ul>
<li>Often need to read something to the GPU, perform some computation, and the write back the result
<ul>
<li>LLM inference generates token by token</li>
</ul></li>
</ul></li>
<li>Input data characteristics can significantly influence performance of parallel programs
<ul>
<li>LLMs short or large sequences</li>
<li>Might need different kernels optimized for different input shapes</li>
</ul></li>
<li>Not all applications are “embarrassingly parallel”
<ul>
<li>Synchronization imposes overhead
<ul>
<li>Need to wait for GPU operations to complete</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="main-goals-of-the-book" class="level3">
<h3 class="anchored" data-anchor-id="main-goals-of-the-book">Main Goals of the Book</h3>
<ol type="1">
<li>Parallel programming &amp; computational thinking
<ul>
<li>Aims to build a foundation for parallel programming in general</li>
<li>Uses GPUs as a learning vehicle
<ul>
<li>Techniques apply to other accelerators</li>
<li>Concepts are introduced through hands-on CUDA examples</li>
</ul></li>
</ul></li>
<li>Correct &amp; reliable parallel programing
<ul>
<li>Debugging both functions and performance</li>
<li>Understanding where things are fast and slow and how to improve the slow parts</li>
</ul></li>
<li>Scalability
<ul>
<li>Regularize and localize memory access</li>
<li>How to organize memory</li>
</ul></li>
</ol>
</section>
</section>
<section id="heterogeneous-data-parallel-computing" class="level2">
<h2 class="anchored" data-anchor-id="heterogeneous-data-parallel-computing">Heterogeneous Data Parallel Computing</h2>
<ul>
<li><p>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=ZeFGj3WVYDF_TI96&amp;t=511">8:31</a></p></li>
<li><p>heterogeneous: CPU + GPU</p></li>
<li><p>data parallelism: break work down into computations that can be executed independently</p></li>
</ul>
<section id="cuda-c" class="level3">
<h3 class="anchored" data-anchor-id="cuda-c">CUDA C</h3>
<ul>
<li>extends ANSI C with minimal new syntax</li>
<li>Terminology
<ul>
<li>CPU=host</li>
<li>GPU=device</li>
<li>Kernels: device code functions</li>
</ul></li>
<li>CUDA C source can be a mixture of host &amp; device code</li>
<li>grid of threads
<ul>
<li>Many threads are launched to execute a kernel</li>
</ul></li>
<li>CPU &amp; GPU code runs concurrently (overlapped)
<ul>
<li>Kernels launch and run on GPU asynchronously</li>
<li>Need to wait for the kernels to finish before copying data back to CPU</li>
</ul></li>
<li>Don’t be afraid to launch many threads on GPU
<ul>
<li>One thread per output tensor is fine</li>
</ul></li>
</ul>
</section>
<section id="cuda-essentials-memory-allocation" class="level3">
<h3 class="anchored" data-anchor-id="cuda-essentials-memory-allocation">CUDA Essentials: Memory Allocation</h3>
<ul>
<li><p>NVIDIA devices come with their own DRAM (device) global memory</p></li>
<li><p><code>cudaMalloc</code> &amp; <code>cudaFree</code>:</p>
<ul>
<li><p><code>cudaMalloc</code>: Allocate device global memory</p></li>
<li><p><code>cudaFree</code>: Free device global memory</p></li>
<li><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb1-1">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>A_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-2">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size_t</span> size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">sizeof</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// size in bytes</span></span>
<span id="cb1-3">  cudaMalloc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">((</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**)&amp;</span>A_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// pointer to pointer</span></span>
<span id="cb1-4">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span></span>
<span id="cb1-5">  cudaFree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>A_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span></code></pre></div></li>
<li><p>Code convention</p>
<ul>
<li><code>_d</code> for device pointer</li>
<li><code>_h</code> for host</li>
</ul></li>
<li><p><code>cudaMemcpy</code></p>
<ul>
<li><p>Copy data from CPU memory to GPU memory and vice versa</p></li>
<li><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb2-1">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// copy input vectors to device (host -&gt; device)</span></span>
<span id="cb2-2">  cudaMemcpy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>A_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> A_h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> cudaMemcpyHostToDevice<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-3">  cudaMemcpy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>B_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> B_h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> cudaMemcpyHostToDevice<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-4">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span></span>
<span id="cb2-5">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// transfer result back to CPU memory (device -&gt; host)</span></span>
<span id="cb2-6">  cudaMemcpy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>C_h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> C_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> cudaMemcpyDeviceToHost<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span></code></pre></div></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="cuda-error-handling" class="level3">
<h3 class="anchored" data-anchor-id="cuda-error-handling">CUDA Error Handling</h3>
<ul>
<li>CUDA functions return <code>cudaError_t</code>
<ul>
<li><code>cudaSuccess</code> for successful operation</li>
</ul></li>
<li>Always check returned error status</li>
</ul>
</section>
<section id="kernel-functions-fn" class="level3">
<h3 class="anchored" data-anchor-id="kernel-functions-fn">Kernel functions <code>fn&lt;&lt;&gt;&gt;</code></h3>
<ul>
<li>Launching kernel
<ul>
<li>grid of threads is launched</li>
</ul></li>
<li>All threads execute the same code
<ul>
<li>SPMD: Single Program Multiple Data</li>
</ul></li>
<li>Threads are hierarchically organized into grid blocks &amp; thread blocks
<ul>
<li>Up to 1024 threads in a thread block</li>
</ul></li>
</ul>
</section>
<section id="kernel-coordinates" class="level3">
<h3 class="anchored" data-anchor-id="kernel-coordinates">Kernel Coordinates</h3>
<ul>
<li><p>Built-in variables available inside the kernel</p>
<ul>
<li><code>blockIdx</code>: the area code for a telephone
<ul>
<li>Note: Blocks are a logical organization of threads, not physical</li>
</ul></li>
<li><code>threadIdx</code>: the local phone number</li>
<li>These are ‘coordinates’ that allow threads to identify which portion of the data to process</li>
<li>Can use <code>blockIdx</code> and <code>threadIdx</code> to uniquely identify threads</li>
<li><code>blockDim</code>: tells us the number of threads in a block</li>
</ul></li>
<li><p>For vector addition, we can calculate the array index of the thread</p>
<ul>
<li><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb3-1">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div></li>
</ul></li>
<li><p>All threads in a grid execute the same kernel code</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/book-figure-2-9.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 2.9</figcaption>
</figure>
</div>
</section>
<section id="cuda-c-keywords-for-function-declaration" class="level3">
<h3 class="anchored" data-anchor-id="cuda-c-keywords-for-function-declaration">CUDA C keywords for function declaration</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Qualifier Keyword</th>
<th>Callable From</th>
<th>Executed On</th>
<th>Executed By</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>__host__</code> (default)</td>
<td>Host</td>
<td>Host</td>
<td>Caller host thread</td>
</tr>
<tr class="even">
<td><code>__global__</code></td>
<td>Host (or Device)</td>
<td>Device</td>
<td>New grid of device threads</td>
</tr>
<tr class="odd">
<td><code>__device__</code></td>
<td>Device</td>
<td>Device</td>
<td>Caller device thread</td>
</tr>
</tbody>
</table>
<ul>
<li><p><code>__global__</code> &amp; <code>__host__</code></p>
<ul>
<li>Tell the compiler whether the function should live on the device or host</li>
<li>Declare a kernel function with <code>__global__</code>
<ul>
<li>Calling a <code>__global__</code> function launches new grid of CUDA threads</li>
</ul></li>
</ul></li>
<li><p>Functions declared with <code>__device__</code> can be called from within CUDA thread</p>
<ul>
<li>Does not launch a new thread</li>
<li>Only accessible from within kernels</li>
</ul></li>
<li><p>If both <code>__host__</code> and <code>__device__</code> are used in a function declaration</p>
<ul>
<li>CPU and GPU versions will be compiled</li>
</ul></li>
</ul>
</section>
<section id="calling-kernels" class="level3">
<h3 class="anchored" data-anchor-id="calling-kernels">Calling Kernels</h3>
<ul>
<li><p>Kernel configuration is specified between <code>&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;</code></p></li>
<li><p>Number of blocks, number of threads in each block</p></li>
<li><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Define the number of threads per block.</span></span>
<span id="cb4-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Each block will have 256 threads.</span></span>
<span id="cb4-3">dim3 numThreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the number of blocks needed to cover the entire vector.</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Use ceiling division to ensure that the number of blocks is sufficient</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// to handle all elements of the vector 'n'.</span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// The formula (n + numThreads.x - 1) / numThreads.x ensures this.</span></span>
<span id="cb4-9">dim3 numBlocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">((</span>n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> numThreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> numThreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Launch the vector addition kernel with the calculated number of blocks and threads.</span></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// This will execute the vecAddKernel function on the GPU with 'numBlocks' blocks,</span></span>
<span id="cb4-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// each containing 'numThreads.x' threads.</span></span>
<span id="cb4-14">vecAddKernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;&lt;&lt;</span>numBlocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numThreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;&gt;(</span>A_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> B_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> C_d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span></code></pre></div></li>
</ul>
</section>
<section id="compiler" class="level3">
<h3 class="anchored" data-anchor-id="compiler">Compiler</h3>
<ul>
<li>nvcc
<ul>
<li>NVIDIA C Compiler</li>
<li>Use to compiler kernels into PTX (CUDA assembly)</li>
</ul></li>
<li>PTX
<ul>
<li>Parallel Thread Execution</li>
<li>Low-level VM &amp; instruction set</li>
</ul></li>
<li>Grahics driver translates PTX into executable binary code (SASS)
<ul>
<li>SASS is the low-level assembly language that compiles to binary microcode, which executes natively on NVIDIA GPU hardware.</li>
</ul></li>
</ul>
</section>
<section id="code-example-vector-addition" class="level3">
<h3 class="anchored" data-anchor-id="code-example-vector-addition">Code Example: Vector addition</h3>
<ul>
<li><p>main concept: replace loop with a grid of threads</p></li>
<li><p>easily parallelizable</p>
<ul>
<li>all additions can be computed independently</li>
</ul></li>
<li><p>Naive GPU vector addition</p>
<ol type="1">
<li>Allocate device memory for vectors</li>
<li>Transfer inputs from host to device</li>
<li>Launch kernel and perform addition operations</li>
<li>Copy outputs from device to host</li>
<li>Free device memory</li>
</ol>
<ul>
<li>The ratio of data transfer vs compute is not very good
<ul>
<li>Normally keep data on the GPU as long as possible to asynchronously schedule many kernel launches</li>
</ul></li>
</ul></li>
<li><p>Figure from slide 13:</p>
<ul>
<li>One thread per vector element</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/slide-13-figure.png" class="img-fluid figure-img"></p>
<figcaption>slide-13-figure</figcaption>
</figure>
</div></li>
<li><p>Data sizes might not be perfectly divisible by block sizes</p>
<ul>
<li>always check bounds</li>
</ul></li>
<li><p>Prevent threads of boundary block to read/write outside allocated memory</p></li>
<li><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/**</span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@brief</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> CUDA kernel to compute the element-wise sum of two vectors.</span></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> *</span></span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * This kernel function performs the pair-wise addition of elements from</span></span>
<span id="cb5-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * vectors A and B, and stores the result in vector C.</span></span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> *</span></span>
<span id="cb5-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">A</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the first input vector (array) in device memory.</span></span>
<span id="cb5-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">B</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the second input vector (array) in device memory.</span></span>
<span id="cb5-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">C</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the output vector (array) in device memory.</span></span>
<span id="cb5-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">n</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> The number of elements in the vectors.</span></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> */</span></span>
<span id="cb5-12">__global__</span>
<span id="cb5-13"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> vecAddKernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb5-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the unique index for the thread</span></span>
<span id="cb5-15">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb5-16"></span>
<span id="cb5-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Check if the index is within the bounds of the arrays</span></span>
<span id="cb5-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb5-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Perform the element-wise addition</span></span>
<span id="cb5-20">        C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb5-21">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb5-22"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div></li>
</ul>
</section>
<section id="code-example-kernel-to-convert-an-rgb-image-to-grayscale" class="level3">
<h3 class="anchored" data-anchor-id="code-example-kernel-to-convert-an-rgb-image-to-grayscale">Code Example: Kernel to convert an RGB image to grayscale</h3>
<ul>
<li><p>Each RGB pixel can be converted individually</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%0ALuminance%20=%20r%5Ccdot%7B0.21%7D%20+%20g%5Ccdot%7B0.72%7D%20+%20b%5Ccdot%7B0.07%7D%0A"></p></li>
<li><p>Simple weighted sum</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/book-figure-2-2.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 2.2</figcaption>
</figure>
</div>
</section>
</section>
<section id="multidimensional-grids-and-data" class="level2">
<h2 class="anchored" data-anchor-id="multidimensional-grids-and-data">Multidimensional Grids and Data</h2>
<ul>
<li>Timestamp: <a href="https://youtu.be/NQ-0D5Ti2dc?si=k2a0vvryolFT8AsZ&amp;t=1495">24:55</a></li>
</ul>
<section id="cuda-grid" class="level3">
<h3 class="anchored" data-anchor-id="cuda-grid">CUDA Grid</h3>
<ul>
<li><p>2-level hierarchy</p>
<ul>
<li>Blocks and threads</li>
</ul></li>
<li><p>Idea: Map threads to multi-dimensional data (e.g., an image)</p></li>
<li><p>All threads in a grid execute the same kernel</p></li>
<li><p>Threads in the same block can access the same shared memory</p></li>
<li><p>Max block size: 1024</p></li>
<li><p>Built-in 3D coordinates of a thread</p>
<ul>
<li><code>blockIdx</code> and <code>threadIdx</code> identify which portion of the data to process</li>
</ul></li>
<li><p>shape of grid &amp; blocks</p>
<ul>
<li><code>gridDim</code>: number of blocks in the grid</li>
<li><code>blockDim</code>: number of threads in a block</li>
</ul></li>
<li><p>A multidimensional example of CUDA grid organization:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/book-figure-3-1.png" class="img-fluid figure-img" style="width:67.0%"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.1</figcaption>
</figure>
</div>
<ul>
<li><p>Grid can be different for each kernel launch</p>
<ul>
<li>Normally dependent on data shapes</li>
</ul></li>
<li><p>Typical grids contain thousands to millions of threads</p></li>
<li><p>Simple Strategy</p>
<ul>
<li>One thread per output element
<ul>
<li>One thread per pixel</li>
<li>One thread per tensor element</li>
</ul></li>
</ul></li>
<li><p>Threads can be scheduled in any order</p>
<ul>
<li>A larger thread index does not necessarily indicate the thread is running after a thread with a lower index</li>
</ul></li>
<li><p>Can use fewer than 3 dims (set others to 1)</p>
<ul>
<li><p>1D for sequences, 2D for images, etc.</p></li>
<li><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb6-1">  dim3 grid<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb6-2">  dim3 block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb6-3">  kernelFunction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;&lt;&lt;</span>grid<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;&gt;(..);</span></span>
<span id="cb6-4">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Number of threads: 128*32 = 4096</span></span></code></pre></div></li>
</ul></li>
</ul>
</section>
<section id="built-in-variables" class="level3">
<h3 class="anchored" data-anchor-id="built-in-variables">Built-in Variables</h3>
<ul>
<li><p>Built-in variables inside kernels:</p>
<ul>
<li><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb7-1">  blockIdx <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// dim3 block coordinate</span></span>
<span id="cb7-2">  threadIdx <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// dim3 thread coordinate</span></span>
<span id="cb7-3">  blockDim <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// number of threads in a block</span></span>
<span id="cb7-4">  gridDim <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// number of blocks in a grid</span></span></code></pre></div></li>
<li><code>blockDim</code> and <code>gridDim</code> have the same values in all threads</li>
</ul></li>
</ul>
</section>
<section id="nd-arrays-in-memory" class="level3">
<h3 class="anchored" data-anchor-id="nd-arrays-in-memory">nd-Arrays in Memory</h3>
<ul>
<li>memory of multi-dim arrays under the hood is a flat 1-dimensional array</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/slide-28-actual-layout.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28: Actual layout in memory</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/slide-28-logical-layout.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28: Logical view of data</figcaption>
</figure>
</div>
<ul>
<li><p>2d array can be linearized in different ways</p>
<ul>
<li><pre class="text"><code>  A B C D E F G H I</code></pre></li>
<li><p>row-major</p>
<ul>
<li><pre class="text"><code>  A B C
  D E F
  G H I</code></pre></li>
<li>Most common</li>
</ul></li>
<li><p>column-major</p>
<ul>
<li><pre class="text"><code>  A D G
  B E H
  C F I</code></pre></li>
<li>Used in fortran</li>
</ul></li>
</ul></li>
<li><p>PyTorch tensors and numpy arrays use strides to specify how elements are laid out in memory</p>
<ul>
<li>For a <img src="https://latex.codecogs.com/png.latex?4%20%5Ctimes%204"> matrix, the stride would be <img src="https://latex.codecogs.com/png.latex?4"> to get to the next row.
<ul>
<li>After four elements, you end up in the next row.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="code-example-image-blur" class="level3">
<h3 class="anchored" data-anchor-id="code-example-image-blur">Code Example: Image Blur</h3>
<ul>
<li><p>mean filter example <code>blurKernel</code>:</p>
<ul>
<li><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// CUDA kernel to perform a simple box blur on an input image</span></span>
<span id="cb11-2">__global__</span>
<span id="cb11-3"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> blurKernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>in<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> w<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the column and row index of the pixel this thread is processing</span></span>
<span id="cb11-5">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-6">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-7"></span>
<span id="cb11-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure the thread is within the image bounds</span></span>
<span id="cb11-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-10">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Variable to accumulate the sum of pixel values</span></span>
<span id="cb11-11">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Variable to count the number of valid pixels in the blur region</span></span>
<span id="cb11-12"></span>
<span id="cb11-13">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Loop over the surrounding pixels within the blur region</span></span>
<span id="cb11-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> blurRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>BLUR_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> BLUR_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++</span>blurRow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> blurCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>BLUR_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> BLUR_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++</span>blurCol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-16">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> blurRow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Current row index in the blur region</span></span>
<span id="cb11-17">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> blurCol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Current column index in the blur region</span></span>
<span id="cb11-18"></span>
<span id="cb11-19">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Check if the current pixel is within the image bounds</span></span>
<span id="cb11-20">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> w<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb11-21">                    pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> in<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> curCol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Accumulate the pixel value</span></span>
<span id="cb11-22">                    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++</span>pixels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Increment the count of valid pixels</span></span>
<span id="cb11-23">                <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-24">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-25">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-26"></span>
<span id="cb11-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the average pixel value and store it in the output image</span></span>
<span id="cb11-28">        out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)(</span>pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pixels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb11-29">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-30"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div></li>
</ul></li>
<li><p>each thread writes one output element, read multiple values</p></li>
<li><p>single plane in book, can be easily extended to multi-channel</p></li>
<li><p>shows row-major pixel memory access (in &amp; out pointers)</p></li>
<li><p>track of how many pixel values are summed</p></li>
<li><p>Handling boundary conditions for pixels near the edges of the image:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/book-figure-3-9.png" class="img-fluid figure-img"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.9</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> PIL <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image</span>
<span id="cb12-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb12-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.cpp_extension <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_inline</span></code></pre></div>
<div class="callout callout-style-default callout-note callout-titled" title="CUDA Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
CUDA Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<pre class="text"><code></code></pre>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb14-1"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;torch/types.h&gt;</span></span>
<span id="cb14-2"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;cuda.h&gt;</span></span>
<span id="cb14-3"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;cuda_runtime.h&gt;</span></span>
<span id="cb14-4"></span>
<span id="cb14-5"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;c10/cuda/CUDAException.h&gt;</span></span>
<span id="cb14-6"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;c10/cuda/CUDAStream.h&gt;</span></span>
<span id="cb14-7"></span>
<span id="cb14-8"></span>
<span id="cb14-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// CUDA kernel for applying a mean filter to an image</span></span>
<span id="cb14-10">__global__</span>
<span id="cb14-11"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> mean_filter_kernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> output<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> input<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the column, row, and channel this thread is responsible for</span></span>
<span id="cb14-13">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-14">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-15">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> channel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>z<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-16"></span>
<span id="cb14-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Base offset for the current channel</span></span>
<span id="cb14-18">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> baseOffset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> channel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> height <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-19"></span>
<span id="cb14-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure the thread is within image bounds</span></span>
<span id="cb14-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-22">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Accumulator for the pixel values</span></span>
<span id="cb14-23">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Counter for the number of pixels summed</span></span>
<span id="cb14-24"></span>
<span id="cb14-25">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Iterate over the kernel window</span></span>
<span id="cb14-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> blurRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> blurCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> blurCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-28">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> blurRow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-29">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> blurCol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-30"></span>
<span id="cb14-31">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Check if the current position is within image bounds</span></span>
<span id="cb14-32">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> height <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> curCol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-33">                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Accumulate pixel value and count the number of pixels</span></span>
<span id="cb14-34">                    pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> input<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>baseOffset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> curRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> curCol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb14-35">                    pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-36">                <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-37">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-38">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-39"></span>
<span id="cb14-40">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Write the averaged value to the output image</span></span>
<span id="cb14-41">        output<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>baseOffset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)(</span>pixVal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pixels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-42">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-43"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-44"></span>
<span id="cb14-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Helper function for ceiling unsigned integer division</span></span>
<span id="cb14-46"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inline</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> cdiv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-47">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-48"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-49"></span>
<span id="cb14-50"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Main function to apply the mean filter to an image using CUDA</span></span>
<span id="cb14-51">torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>Tensor mean_filter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>Tensor image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> radius<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb14-52">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure the input image is on the GPU, is of byte type, and radius is positive</span></span>
<span id="cb14-53">    <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">().</span>type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">()</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>kCUDA<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-54">    <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">()</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>kByte<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-55">    <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>radius <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-56"></span>
<span id="cb14-57">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Get image dimensions and number of channels</span></span>
<span id="cb14-58">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">const</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">auto</span> channels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-59">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">const</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">auto</span> height <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-60">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">const</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">auto</span> width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-61"></span>
<span id="cb14-62">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Create an empty tensor to store the result</span></span>
<span id="cb14-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">auto</span> result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>empty_like<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-64"></span>
<span id="cb14-65">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Define the number of threads per block and number of blocks</span></span>
<span id="cb14-66">    dim3 threads_per_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-67">    dim3 number_of_blocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span></span>
<span id="cb14-68">        cdiv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> threads_per_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">),</span></span>
<span id="cb14-69">        cdiv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> threads_per_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span></span>
<span id="cb14-70">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-71"></span>
<span id="cb14-72">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Launch the CUDA kernel</span></span>
<span id="cb14-73">    mean_filter_kernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;&lt;&lt;</span>number_of_blocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> threads_per_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> torch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>cuda<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span>getCurrentCUDAStream<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">()&gt;&gt;&gt;(</span></span>
<span id="cb14-74">        result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>data_ptr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;(),</span></span>
<span id="cb14-75">        image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>data_ptr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;(),</span></span>
<span id="cb14-76">        width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb14-77">        height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb14-78">        radius</span>
<span id="cb14-79">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb14-80"></span>
<span id="cb14-81">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Check for any CUDA errors (calls cudaGetLastError())</span></span>
<span id="cb14-82">    C10_CUDA_KERNEL_LAUNCH_CHECK<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb14-83"></span>
<span id="cb14-84">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Return the filtered image</span></span>
<span id="cb14-85">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb14-86"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
</div>
</div>
</div>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the CUDA kernel and C++ wrapper</span></span>
<span id="cb15-2">cuda_source <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'''</span></span>
<span id="cb15-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">#include &lt;c10/cuda/CUDAException.h&gt;</span></span>
<span id="cb15-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">#include &lt;c10/cuda/CUDAStream.h&gt;</span></span>
<span id="cb15-5"></span>
<span id="cb15-6"></span>
<span id="cb15-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">// CUDA kernel for applying a mean filter to an image</span></span>
<span id="cb15-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">__global__</span></span>
<span id="cb15-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">void mean_filter_kernel(unsigned char* output, unsigned char* input, int width, int height, int radius) {</span></span>
<span id="cb15-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Calculate the column, row, and channel this thread is responsible for</span></span>
<span id="cb15-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int col = blockIdx.x * blockDim.x + threadIdx.x;</span></span>
<span id="cb15-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span>
<span id="cb15-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int channel = threadIdx.z;</span></span>
<span id="cb15-14"></span>
<span id="cb15-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Base offset for the current channel</span></span>
<span id="cb15-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int baseOffset = channel * height * width;</span></span>
<span id="cb15-17"></span>
<span id="cb15-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Ensure the thread is within image bounds</span></span>
<span id="cb15-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    if (col &lt; width &amp;&amp; row &lt; height) {</span></span>
<span id="cb15-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        int pixVal = 0; // Accumulator for the pixel values</span></span>
<span id="cb15-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        int pixels = 0; // Counter for the number of pixels summed</span></span>
<span id="cb15-22"></span>
<span id="cb15-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        // Iterate over the kernel window</span></span>
<span id="cb15-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        for (int blurRow = -radius; blurRow &lt;= radius; blurRow += 1) {</span></span>
<span id="cb15-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            for (int blurCol = -radius; blurCol &lt;= radius; blurCol += 1) {</span></span>
<span id="cb15-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                int curRow = row + blurRow;</span></span>
<span id="cb15-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                int curCol = col + blurCol;</span></span>
<span id="cb15-28"></span>
<span id="cb15-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                // Check if the current position is within image bounds</span></span>
<span id="cb15-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                if (curRow &gt;= 0 &amp;&amp; curRow &lt; height &amp;&amp; curCol &gt;= 0 &amp;&amp; curCol &lt; width) {</span></span>
<span id="cb15-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    // Accumulate pixel value and count the number of pixels</span></span>
<span id="cb15-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    pixVal += input[baseOffset + curRow * width + curCol];</span></span>
<span id="cb15-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                    pixels += 1;</span></span>
<span id="cb15-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb15-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb15-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb15-37"></span>
<span id="cb15-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        // Write the averaged value to the output image</span></span>
<span id="cb15-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        output[baseOffset + row * width + col] = (unsigned char)(pixVal / pixels);</span></span>
<span id="cb15-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb15-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb15-42"></span>
<span id="cb15-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">// Helper function for ceiling unsigned integer division</span></span>
<span id="cb15-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">inline unsigned int cdiv(unsigned int a, unsigned int b) {</span></span>
<span id="cb15-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    return (a + b - 1) / b;</span></span>
<span id="cb15-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb15-47"></span>
<span id="cb15-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">// Main function to apply the mean filter to an image using CUDA</span></span>
<span id="cb15-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">torch::Tensor mean_filter(torch::Tensor image, int radius) {</span></span>
<span id="cb15-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Ensure the input image is on the GPU, is of byte type, and radius is positive</span></span>
<span id="cb15-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    assert(image.device().type() == torch::kCUDA);</span></span>
<span id="cb15-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    assert(image.dtype() == torch::kByte);</span></span>
<span id="cb15-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    assert(radius &gt; 0);</span></span>
<span id="cb15-54"></span>
<span id="cb15-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Get image dimensions and number of channels</span></span>
<span id="cb15-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    const auto channels = image.size(0);</span></span>
<span id="cb15-57"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    const auto height = image.size(1);</span></span>
<span id="cb15-58"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    const auto width = image.size(2);</span></span>
<span id="cb15-59"></span>
<span id="cb15-60"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Create an empty tensor to store the result</span></span>
<span id="cb15-61"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    auto result = torch::empty_like(image);</span></span>
<span id="cb15-62"></span>
<span id="cb15-63"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Define the number of threads per block and number of blocks</span></span>
<span id="cb15-64"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    dim3 threads_per_block(16, 16, channels);</span></span>
<span id="cb15-65"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    dim3 number_of_blocks(</span></span>
<span id="cb15-66"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        cdiv(width, threads_per_block.x),</span></span>
<span id="cb15-67"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        cdiv(height, threads_per_block.y)</span></span>
<span id="cb15-68"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    );</span></span>
<span id="cb15-69"></span>
<span id="cb15-70"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Launch the CUDA kernel</span></span>
<span id="cb15-71"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    mean_filter_kernel&lt;&lt;&lt;number_of_blocks, threads_per_block, 0, torch::cuda::getCurrentCUDAStream()&gt;&gt;&gt;(</span></span>
<span id="cb15-72"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        result.data_ptr&lt;unsigned char&gt;(),</span></span>
<span id="cb15-73"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        image.data_ptr&lt;unsigned char&gt;(),</span></span>
<span id="cb15-74"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        width,</span></span>
<span id="cb15-75"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        height,</span></span>
<span id="cb15-76"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        radius</span></span>
<span id="cb15-77"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    );</span></span>
<span id="cb15-78"></span>
<span id="cb15-79"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Check for any CUDA errors (calls cudaGetLastError())</span></span>
<span id="cb15-80"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    C10_CUDA_KERNEL_LAUNCH_CHECK();</span></span>
<span id="cb15-81"></span>
<span id="cb15-82"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Return the filtered image</span></span>
<span id="cb15-83"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    return result;</span></span>
<span id="cb15-84"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb15-85"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'''</span></span>
<span id="cb15-86"></span>
<span id="cb15-87">cpp_source <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"torch::Tensor mean_filter(torch::Tensor image, int radius);"</span></span></code></pre></div>
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">build_dir <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'./load_inline_cuda'</span>)</span>
<span id="cb16-2">build_dir.mkdir(exist_ok<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the defined C++/CUDA extension as a PyTorch extension.</span></span>
<span id="cb17-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This enables using the `mean_filter` function as if it were a native PyTorch function.</span></span>
<span id="cb17-3">mean_filter_extension <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_inline(</span>
<span id="cb17-4">    name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean_filter_extension'</span>,   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unique name for the extension</span></span>
<span id="cb17-5">    cpp_sources<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cpp_source,           <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># C++ source code containing the CPU implementation</span></span>
<span id="cb17-6">    cuda_sources<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cuda_source,         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CUDA source code for GPU implementation</span></span>
<span id="cb17-7">    functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean_filter'</span>],      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># List of functions to expose to Python</span></span>
<span id="cb17-8">    with_cuda<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,                   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Enable CUDA support</span></span>
<span id="cb17-9">    extra_cuda_cflags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-O2"</span>],        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compiler flags for optimizing the CUDA code</span></span>
<span id="cb17-10">    build_directory<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(build_dir),   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Directory to store the compiled extension</span></span>
<span id="cb17-11">)</span></code></pre></div>
<div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the path to the image file</span></span>
<span id="cb18-2">img_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'./Grace_Hopper.jpg'</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Open the image using PIL (Python Imaging Library)</span></span>
<span id="cb18-5">test_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(img_path)</span>
<span id="cb18-6">test_img</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/output_6_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert the image to a NumPy array, then to a PyTorch tensor</span></span>
<span id="cb19-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Rearrange the tensor dimensions from (H, W, C) to (C, H, W) and move it to GPU</span></span>
<span id="cb19-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(np.array(test_img)).permute(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).contiguous().cuda()</span>
<span id="cb19-4"></span>
<span id="cb19-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply the mean filter to the tensor using a kernel size of 8</span></span>
<span id="cb19-6">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_filter_extension.mean_filter(x, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span></code></pre></div>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert the filtered tensor back to a NumPy array, rearrange dimensions back to (H, W, C)</span></span>
<span id="cb20-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># and create an image from the array using PIL</span></span>
<span id="cb20-3">output_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.fromarray(y.cpu().permute(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).numpy())</span>
<span id="cb20-4">output_img</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/output_8_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication">Matrix Multiplication</h3>
<ul>
<li><p>Staple of science, engineering, and deep learning</p></li>
<li><p>Computer inner-products of rows and columns</p></li>
<li><p>Strategy: 1 thread per output matrix element</p></li>
<li><p>Example: Multiplying square matrices (rows == cols)</p>
<ul>
<li><div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/**</span></span>
<span id="cb21-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@brief</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Matrix multiplication kernel function.</span></span>
<span id="cb21-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> *</span></span>
<span id="cb21-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * This kernel performs the multiplication of two matrices M and N, storing the result in matrix P.</span></span>
<span id="cb21-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> *</span></span>
<span id="cb21-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">M</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the first input matrix.</span></span>
<span id="cb21-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">N</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the second input matrix.</span></span>
<span id="cb21-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">P</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> Pointer to the output matrix.</span></span>
<span id="cb21-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> * </span><span class="an" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@param</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> </span><span class="cv" style="color: #5E5E5E;
background-color: null;
font-style: italic;">Width</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> The width of the input and output matrices (assuming square matrices).</span></span>
<span id="cb21-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> */</span></span>
<span id="cb21-11">__global__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> MatrixMulKernel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> M<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> P<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> Width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb21-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the row index of the P matrix element and M matrix element</span></span>
<span id="cb21-13">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb21-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Calculate the column index of the P matrix element and N matrix element</span></span>
<span id="cb21-15">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb21-16"></span>
<span id="cb21-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure that row and column indices are within bounds</span></span>
<span id="cb21-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">((</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> Width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> Width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">))</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb21-19">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> Pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Initialize the output value for element P[row][col]</span></span>
<span id="cb21-20"></span>
<span id="cb21-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Perform the dot product of the row of M and column of N</span></span>
<span id="cb21-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> Width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++</span>k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb21-23">            Pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> M<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb21-24">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb21-25"></span>
<span id="cb21-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Store the result in the P matrix</span></span>
<span id="cb21-27">        P<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Width <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pvalue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb21-28">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb21-29"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div></li>
</ul></li>
<li><p>Matrix multiplication using multiple blocks by tiling P:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="christianjmills.com/posts/cuda-mode-notes/lecture-002/images/book-figure-3-10.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Programming Massively Parallel Processors - Figure 3.10</figcaption>
</figure>
</div>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>cuda</category>
  <category>pytorch</category>
  <guid>christianjmills.com/posts/cuda-mode-notes/lecture-002/</guid>
  <pubDate>Thu, 06 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Workshop 1: When and Why to Fine-Tune an LLM</title>
  <dc:creator>Christian Mills</dc:creator>
  <link>christianjmills.com/posts/mastering-llms-course-notes/workshop-001/</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This post is part of the following series:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="../../../series/notes/mastering-llms-course-notes.html"><strong>Mastering LLMs Course Notes</strong></a>: My notes from the course <strong>Mastering LLMs: A Conference For Developers &amp; Data Scientists</strong> by <strong>Hamel Husain</strong> and <strong>Dan Becker</strong>.</li>
</ul>
</div>
</div>
<ul>
<li>Key Takeaways</li>
<li>Course Overview</li>
<li>When to Fine-Tune</li>
<li>Understanding Fine-Tuning</li>
<li>Case Study: Logistics Company Regression Problem</li>
<li>Case Study: Honeycomb Natural Language Query Assistant</li>
<li>Q&amp;A Session #1</li>
<li>Chatbots</li>
<li>Preference Optimization</li>
<li>Evaluating Use Cases for Fine-Tuning</li>
<li>Q&amp;A Session #2</li>
</ul>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li><strong>Start simple:</strong> Focus on prompt engineering and using pre-trained models like those from OpenAI before jumping into the complexity of fine-tuning.</li>
<li><strong>Fine-tune strategically:</strong> Consider fine-tuning when you need bespoke behavior, have unique data, or require data privacy.</li>
<li><strong>Templating is crucial:</strong> Pay close attention to consistency in templating between training and inference to avoid unexpected model behavior.</li>
<li><strong>Evaluate rigorously:</strong> Use domain-specific evaluations and metrics to measure model performance and guide fine-tuning decisions.</li>
<li><strong>Preference optimization shows promise:</strong> Techniques like Direct Preference Optimization (DPO) can train models to outperform even human experts by learning from comparative feedback.</li>
</ul>
</section>
<section id="course-overview" class="level2">
<h2 class="anchored" data-anchor-id="course-overview">Course Overview</h2>
<ul>
<li><strong>Focus:</strong> Actionable insights and practical guidance from real-world experience in deploying LLMs for various business needs.</li>
<li><strong>Philosophy:</strong>
<ul>
<li>Prioritize practical value over project ideas that only sound cool.</li>
<li>Start with simple, straightforward solutions and progressively refine them.</li>
<li>Ship prototypes quickly for rapid iteration and feedback.</li>
</ul></li>
<li><strong>Workflow:</strong>
<ul>
<li>Start with prompt engineering before considering fine-tuning.
<ul>
<li>Prompt engineering provides much faster iteration and experimentation.</li>
<li>The results from prompt engineering will help inform whether fine-tuning is necessary.</li>
</ul></li>
<li>Iterate quickly with simple prototypes.
<ul>
<li>Build and show people concrete things, so they can provide feedback.</li>
<li>Simple prototypes almost always work well enough to start making progress.</li>
</ul></li>
<li>Incorporate evaluations (Evals) to measure and improve model performance.
<ul>
<li>Blog Post: <a href="https://hamel.dev/blog/posts/evals/">Your AI Product Needs Evals</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="when-to-fine-tune" class="level2">
<h2 class="anchored" data-anchor-id="when-to-fine-tune">When to Fine-Tune</h2>
<ul>
<li><strong>Don’t fine-tune for generic behavior:</strong>
<ul>
<li>Use existing powerful models like OpenAI’s GPT or Anthropic’s models via API for tasks where they excel.</li>
<li>Increasingly larger context windows allows us to fit more examples to fit into a prompt.</li>
<li>You should have some minimal evaluation system that you hit a wall on with prompting alone, before considering fine-tuning.</li>
</ul></li>
<li><strong>Do fine-tune for bespoke behavior:</strong>
<ul>
<li>When you need specific outputs or behavior not achievable through prompt engineering alone.</li>
<li>When you have a narrow, well-defined problem domain and sufficient data for training.
<ul>
<li>Fine-tuning requires examples of desired inputs and outputs for supervised learning.</li>
</ul></li>
<li>When data privacy and model ownership are critical.</li>
<li>When you need improved quality and lower latency compared to large pre-trained models.</li>
<li>Requires proper operational setup and significant value use cases.</li>
</ul></li>
<li><strong>Iteration Speed &amp; Complexity:</strong> Fine-tuning involves slower iteration cycles and operational complexities compared to using pre-trained models.</li>
</ul>
</section>
<section id="understanding-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="understanding-fine-tuning">Understanding Fine-Tuning</h2>
<ul>
<li><strong>Pre-training:</strong> Training LLMs on massive text datasets to learn language fundamentals and next-token prediction.</li>
<li><strong>Building on Pre-trained Models:</strong>
<ul>
<li>Fine-tuning adapts pre-trained models with vast general language knowledge to excel in specific domains.</li>
<li>Fine-tuning harnesses the next-token prediction mechanism used in pre-training to generate desired outputs.</li>
</ul></li>
<li><strong>Importance of Input-Output Examples</strong>
<ul>
<li>Fine-tuning requires clear examples of desired inputs and outputs.</li>
<li>Documentation alone isn’t sufficient; practical examples are necessary.</li>
<li>Mixed quality of training data (e.g., varied quality of human-written summaries) can lead to mediocre model performance.</li>
</ul></li>
<li><strong>Templating for Inference Control:</strong>
<ul>
<li>Guides the model to produce specific outputs by short-circuiting pre-trained behavior.</li>
<li>Inputs and outputs are placed within a consistent template to guide the model during inference.</li>
<li>Crucial for aligning training and inference.</li>
<li>Defines the structure of input and output text to guide the model.<br>
</li>
<li>Inconsistencies in templating are a major source of errors.
<ul>
<li>Templates must be identical between training and inference.</li>
<li>There are many kinds of templates and it is easy to misinterpret them.</li>
<li>Many tools try to abstract away and automate building templates and something often goes wrong.</li>
<li>Blog Post: <a href="https://hamel.dev/notes/llm/finetuning/05_tokenizer_gotchas.html">Tokenization Gotchas</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="case-study-logistics-company-regression-problem" class="level2">
<h2 class="anchored" data-anchor-id="case-study-logistics-company-regression-problem">Case Study: Logistics Company Regression Problem</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<ul>
<li><strong>Task:</strong> Logistics company (e.g., UPS, DHL, USPS) needed to predict item value based on an 80-character description.</li>
<li><strong>Takeaways:</strong> Highlights the importance of understanding and preparing the training data, the limitations of fine-tuning for specific regression tasks, and the practical issues encountered with this approach.</li>
</ul>
</section>
<section id="traditional-nlp-and-ml-approaches" class="level3">
<h3 class="anchored" data-anchor-id="traditional-nlp-and-ml-approaches">Traditional NLP and ML Approaches</h3>
<ul>
<li><strong>Classical Techniques:</strong> Initial consideration to use traditional NLP and ML methods.</li>
<li><strong>Bag of Words Representation:</strong> Highlighted issue where models fail to recognize unseen words or infrequent words due to limited data.</li>
</ul>
</section>
<section id="fine-tuning-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-large-language-models">Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Initial Approach:</strong> Attempted to use a large language model (LLM) with and without fine-tuning for regression.</li>
<li><strong>Outcome:</strong> The model learned patterns in the data that were not ideal for the task.</li>
</ul>
</section>
<section id="key-observations-from-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="key-observations-from-fine-tuning">Key Observations from Fine-Tuning</h3>
<ul>
<li><strong>Round Numbers:</strong> The model tended to predict round numbers frequently because past entries often used round numbers.</li>
<li><strong>Mismatch in Values:</strong> Conventional ML models can predict approximate values (e.g., $97 vs.&nbsp;$100), which is often more useful than exact but less frequent round number predictions by the LLM.</li>
<li><strong>Training Data Limitations:</strong> Training data often contained inaccuracies, such as undervalued entries to avoid insurance costs.</li>
</ul>
</section>
<section id="data-representation-and-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-representation-and-preprocessing">Data Representation and Preprocessing</h3>
<ul>
<li><strong>Description Complexity:</strong> Corporate descriptions were often abbreviated or used acronyms, making them hard to interpret both for humans and models.</li>
<li><strong>Pre-trained Model Limitations:</strong> Pre-trained models struggled with unknown abbreviations or context-specific terms not encountered during pre-training.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li><strong>Unsuccessful Case Study:</strong> The fine-tuning approach was largely unsuccessful due to predictable data issues.</li>
</ul>
</section>
<section id="insights-and-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="insights-and-recommendations">Insights and Recommendations</h3>
<ul>
<li><strong>Data Quality:</strong> Emphasized the importance of high-quality, representative training data for desired future behavior.</li>
<li><strong>Raw Data Examination:</strong> Stressed the need to carefully inspect raw data, a common yet frequently overlooked step in data science.</li>
<li><strong>Practicality of ML Solutions:</strong> For this case, traditional ML and NLP techniques did not provide satisfactory results, leading to the retention of the manual workflow.</li>
</ul>
</section>
</section>
<section id="case-study-honeycomb-natural-language-query-assistant" class="level2">
<h2 class="anchored" data-anchor-id="case-study-honeycomb-natural-language-query-assistant">Case Study: Honeycomb Natural Language Query Assistant</h2>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<ul>
<li><p><strong>Task:</strong> Building a system for Honeycomb, an observability platform that logs telemetry data about software applications, that translates natural language queries into the platform’s domain-specific query language.</p></li>
<li><p><strong>Takeaways:</strong> Highlights the importance of fine-tuning in addressing domain-specific challenges, improving model performance, and meeting business requirements such as data privacy and operational efficiency.</p></li>
</ul>
</section>
<section id="honeycomb-platform-overview" class="level3">
<h3 class="anchored" data-anchor-id="honeycomb-platform-overview">Honeycomb Platform Overview</h3>
<ul>
<li>Honeycomb is an observability platform.</li>
<li>Logs telemetry data like page load times, database response times, and application bottlenecks.</li>
<li>Users query this data using a domain-specific query language.</li>
</ul>
</section>
<section id="initial-solution-natural-language-query-assistant" class="level3">
<h3 class="anchored" data-anchor-id="initial-solution-natural-language-query-assistant">Initial Solution: Natural Language Query Assistant</h3>
<ul>
<li><strong>Problem:</strong> Users must learn a specific query language to use Honeycomb effectively.</li>
<li><strong>Solution:</strong> Create a natural language query assistant that translates user queries into Honeycomb’s query language using large language models (LLMs).</li>
<li><strong>Initial Approach:</strong>
<ul>
<li>User provides a query and schema (list of column names from the user’s data).</li>
<li>Prompt assembled with user input and schema sent to GPT-3/GPT-3.5.</li>
<li>Generated a Honeycomb query based on the prompt.</li>
</ul></li>
</ul>
</section>
<section id="prompt-structure" class="level3">
<h3 class="anchored" data-anchor-id="prompt-structure">Prompt Structure</h3>
<ol type="1">
<li><strong>System Message:</strong>
<ul>
<li>“Honeycomb AI suggests queries based on user input.”</li>
</ul></li>
<li><strong>Columns Section:</strong>
<ul>
<li>Schema from the user’s data inserted here.</li>
</ul></li>
<li><strong>Query Spec:</strong>
<ul>
<li>Simplified programming manual for Honeycomb’s query language.</li>
<li>Contains operations and comments on their usage.</li>
</ul></li>
<li><strong>Tips Section:</strong>
<ul>
<li>Guidelines to handle different failure modes and edge cases.</li>
<li>Example: Handling time ranges correctly.</li>
</ul></li>
<li><strong>Few-Shot Examples:</strong>
<ul>
<li>Examples of natural language queries and corresponding Honeycomb query outputs.</li>
</ul></li>
</ol>
</section>
<section id="challenges-with-initial-solution" class="level3">
<h3 class="anchored" data-anchor-id="challenges-with-initial-solution">Challenges with Initial Solution</h3>
<ul>
<li><strong>Expressing Query Language Nuances:</strong>
<ul>
<li>Hard to capture all idioms and best practices of the query language.</li>
<li>GPT-3.5 lacks extensive exposure to Honeycomb’s specific query language.</li>
</ul></li>
<li><strong>Tips Section Complexity:</strong>
<ul>
<li>Tips devolved into numerous if-then statements.</li>
<li>Difficult for the language model to follow multiple conditionals.</li>
</ul></li>
<li><strong>Few-Shot Examples Limitations:</strong>
<ul>
<li>Hard to cover all edge cases.</li>
<li>Dynamic few-shot examples could help but were not implemented.</li>
</ul></li>
</ul>
</section>
<section id="business-challenges" class="level3">
<h3 class="anchored" data-anchor-id="business-challenges">Business Challenges</h3>
<ul>
<li><strong>Data Privacy:</strong>
<ul>
<li>Need permission to send customer data to OpenAI.</li>
<li>Preference to keep data within a trusted boundary.</li>
</ul></li>
<li><strong>Quality vs.&nbsp;Latency Tradeoff:</strong>
<ul>
<li>GPT-4 offered higher quality but was too slow and expensive.</li>
<li>Goal: Train a smaller, faster model with comparable quality.</li>
</ul></li>
<li><strong>Narrow Domain Problem:</strong>
<ul>
<li>Honeycomb queries are a focused, narrow domain ideal for fine-tuning.</li>
</ul></li>
<li><strong>Impracticality of Extensive Prompt Engineering:</strong>
<ul>
<li>Hard to manually encode all nuances of the query language.</li>
<li>Fine-tuning with many examples is more practical.</li>
</ul></li>
</ul>
</section>
<section id="fine-tuning-solution" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-solution">Fine-Tuning Solution</h3>
<ul>
<li><strong>Advantages:</strong>
<ul>
<li>Faster, more compliant with data privacy needs.</li>
<li>Higher quality responses compared to GPT-3.5.</li>
</ul></li>
<li><strong>Implementation:</strong>
<ul>
<li>Fine-tuned a model using synthetic data provided by Honeycomb.</li>
<li>The process and challenges encountered during fine-tuning will be simulated in the course.</li>
</ul></li>
</ul>
</section>
<section id="recommendations" class="level3">
<h3 class="anchored" data-anchor-id="recommendations">Recommendations</h3>
<ol type="1">
<li><strong>Implement Fine-Tuning:</strong>
<ul>
<li>Use synthetic data to replicate and improve the model.</li>
<li>Focus on capturing edge cases and nuances in the training data.</li>
</ul></li>
<li><strong>Optimize for Performance:</strong>
<ul>
<li>Balance model size and latency to ensure quick responses without sacrificing quality.</li>
</ul></li>
<li><strong>Ensure Data Privacy:</strong>
<ul>
<li>Keep data within a trusted boundary to comply with customer privacy requirements.</li>
</ul></li>
<li><strong>Regularly Update Few-Shot Examples:</strong>
<ul>
<li>Dynamically generate examples to cover new edge cases and improve model accuracy.</li>
</ul></li>
<li><strong>Monitor and Iterate:</strong>
<ul>
<li>Continuously monitor model performance and iteratively improve based on user feedback and new data.</li>
</ul></li>
</ol>
</section>
</section>
<section id="qa-session-1" class="level2">
<h2 class="anchored" data-anchor-id="qa-session-1">Q&amp;A Session #1</h2>
<p>This Q&amp;A session covers various aspects of fine-tuning machine learning models, particularly focusing on fine-tuning versus retrieval-augmented generation (RAG), function calling, and synthetic data generation. It also touches upon the use of base models versus instruction-tuned models and the appropriate amount of data for fine-tuning.</p>
<section id="fine-tuning-vs.-rag" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-vs.-rag">Fine-Tuning vs.&nbsp;RAG</h3>
<ul>
<li><strong>Definitions</strong>:
<ul>
<li><strong>Fine-Tuning</strong>: Adjusting a pre-trained model with additional data to improve performance in specific tasks.</li>
<li><strong>RAG (Retrieval-Augmented Generation)</strong>: Combines information retrieval with generation to produce responses based on external documents.</li>
</ul></li>
<li><strong>Key Point</strong>: Fine-tuning and RAG are not mutually exclusive; they can complement each other.</li>
<li><strong>Process</strong>: Validate the need for fine-tuning by ensuring good prompts and effective RAG.</li>
</ul>
</section>
<section id="fine-tuning-for-function-calls" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-for-function-calls">Fine-Tuning for Function Calls</h3>
<ul>
<li><strong>Capability</strong>: Models can be fine-tuned to improve at making function calls.</li>
<li><strong>Examples</strong>: Open models like LLaMA 3 and LLaMA 2 have been fine-tuned for function calling.</li>
<li><strong>Challenges</strong>: Identify and use good training data with successful function call examples while filtering out failures.</li>
</ul>
</section>
<section id="data-requirements-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="data-requirements-for-fine-tuning">Data Requirements for Fine-Tuning</h3>
<ul>
<li><strong>Amount of Data</strong>: Success with as few as 100 samples, though this varies by problem scope.</li>
<li><strong>Broad Scope Problems</strong>: Require more data to cover the problem space adequately.</li>
<li><strong>Narrow Scope Problems</strong>: Can often be fine-tuned with relatively little data.</li>
</ul>
</section>
<section id="synthetic-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-data-generation">Synthetic Data Generation</h3>
<ul>
<li><strong>Importance</strong>: Helps overcome data scarcity in specific domains.</li>
<li><strong>Methods</strong>: Use powerful models to generate synthetic data, perturb existing data, and create test cases.</li>
<li><strong>Practical Example</strong>: Honeycomb example shows generating synthetic data to test and train models.</li>
</ul>
</section>
<section id="base-models-vs.-instruction-tuned-models" class="level3">
<h3 class="anchored" data-anchor-id="base-models-vs.-instruction-tuned-models">Base Models vs.&nbsp;Instruction-Tuned Models</h3>
<ul>
<li><strong>Base Models</strong>: Not fine-tuned for specific instructions, allowing more control over fine-tuning processes.</li>
<li><strong>Instruction-Tuned Models</strong>: Pre-fine-tuned to respond to instructions, useful in broader chat-based applications.</li>
<li><strong>Preference</strong>: Often uses base models to avoid template conflicts and ensure specific fine-tuning needs.</li>
</ul>
</section>
<section id="model-size-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="model-size-for-fine-tuning">Model Size for Fine-Tuning</h3>
<ul>
<li><strong>Preferred Size</strong>: Starts with smaller models (e.g., 7 billion parameters) and scales up based on complexity and performance needs.</li>
<li><strong>Trade-Offs</strong>: Larger models require more resources and justification due to higher costs and hosting difficulties.</li>
</ul>
</section>
<section id="multimodal-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-fine-tuning">Multimodal Fine-Tuning</h3>
<ul>
<li><strong>Example Project</strong>: Fine-tuning models to write alt text for images to assist visually impaired users.</li>
<li><strong>Tools</strong>: The LLaVA model is recommended for fine-tuning multimodal tasks.</li>
</ul>
</section>
<section id="recommendations-1" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-1">Recommendations</h3>
<ol type="1">
<li><strong>Validate the Need for Fine-Tuning</strong>: Before starting, ensure you have good prompts and effective RAG if applicable.</li>
<li><strong>Choose the Right Data</strong>: Use high-quality, successful examples for fine-tuning and filter out poor results.</li>
<li><strong>Start Small</strong>: Begin with smaller models and incrementally increase size based on performance needs.</li>
<li><strong>Leverage Synthetic Data</strong>: Generate and use synthetic data to supplement training data, especially in data-scarce domains.</li>
<li><strong>Understand Model Types</strong>: Choose between base models and instruction-tuned models based on the specific use case and desired control over fine-tuning.</li>
<li><strong>Explore Multimodal Capabilities</strong>: Consider multimodal fine-tuning for tasks that require handling both text and images, utilizing models like LLaVA.</li>
</ol>
</section>
</section>
<section id="chatbots" class="level2">
<h2 class="anchored" data-anchor-id="chatbots">Chatbots</h2>
<section id="overview-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-2">Overview</h3>
<ul>
<li><strong>Topic:</strong> Delves into the common pitfalls and considerations when working with LLM-powered chatbots.</li>
<li><strong>Takaways:</strong> Highlights why general-purpose chatbots are often a bad idea, with unrealistic expectations and overly broad scope leading to poor user experiences and significant challenges in development.</li>
</ul>
</section>
<section id="importance-of-saying-no-to-general-purpose-chatbots" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-saying-no-to-general-purpose-chatbots">Importance of Saying No to General-Purpose Chatbots</h3>
<ul>
<li><strong>Prevalence of Chatbot Requests</strong>: When working with LLMs, most clients will request a chatbot.</li>
<li><strong>Need for Caution</strong>: It’s often necessary to push back on these requests due to potential complications.</li>
</ul>
</section>
<section id="case-study-rechat-real-estate-crm-tool" class="level3">
<h3 class="anchored" data-anchor-id="case-study-rechat-real-estate-crm-tool">Case Study: Rechat Real Estate CRM Tool</h3>
<ul>
<li><strong>Initial Concept</strong>: A CRM tool for real estate that integrated multiple functionalities (appointments, listings, social media marketing).</li>
<li><strong>Initial Implementation</strong>: Started with a broad chat interface labeled “Ask Lucy anything.”
<ul>
<li><strong>Problems with Broad Scope</strong>:
<ul>
<li>Unmanageable surface area.</li>
<li>User expectations mismatched with capabilities.</li>
<li>Difficult to make progress on scoped tasks.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="lessons-from-rechat-case-study" class="level3">
<h3 class="anchored" data-anchor-id="lessons-from-rechat-case-study">Lessons from Rechat Case Study</h3>
<ul>
<li><strong>Scoped Interfaces</strong>: Guide users towards specific tasks.</li>
<li><strong>Fine-Tuning Challenges</strong>: Difficult to fine-tune against a large and varied set of functions.</li>
</ul>
</section>
<section id="managing-user-expectations" class="level3">
<h3 class="anchored" data-anchor-id="managing-user-expectations">Managing User Expectations</h3>
<ul>
<li><strong>High User Expectations</strong>: Users often assume chatbots can handle any request, leading to disappointment.</li>
<li><strong>Setting Realistic Boundaries</strong>: Important to guide users on what the chatbot can realistically do.</li>
</ul>
</section>
<section id="real-world-example-dpd-chatbot-incident" class="level3">
<h3 class="anchored" data-anchor-id="real-world-example-dpd-chatbot-incident">Real-World Example: DPD Chatbot Incident</h3>
<ul>
<li><strong>Background</strong>: A chatbot released for a package delivery company, DPD, faced issues on launch.</li>
<li><strong>Incident</strong>: The chatbot swore in response to a user’s prompt, leading to negative publicity.
<ul>
<li><strong>Media Coverage</strong>: The incident was widely reported, causing significant concern within the company.</li>
</ul></li>
<li><strong>Lesson Learned</strong>:
<ul>
<li><strong>Expectations vs.&nbsp;Reality</strong>: Even harmless errors can become major issues if they attract public attention.</li>
<li><strong>Guardrails</strong>: Conventional software has clear input validation; free-form text input in chatbots is harder to manage.</li>
</ul></li>
</ul>
</section>
<section id="guardrails-and-prompt-injections" class="level3">
<h3 class="anchored" data-anchor-id="guardrails-and-prompt-injections">Guardrails and Prompt Injections</h3>
<ul>
<li><strong>Challenges with Guardrails</strong>: Tools to check for prompt injections are imperfect.</li>
<li><strong>Importance of Reviewing Prompts</strong>: Critical to understand and review the prompts used by guardrails to ensure safety.</li>
</ul>
</section>
<section id="recommendations-2" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-2">Recommendations</h3>
<ol type="1">
<li><strong>Scoped Interfaces Over General Chatbots</strong>: Focus on integrating chatbot functionalities into specific parts of the application rather than creating a general-purpose chatbot.</li>
<li><strong>User Expectation Management</strong>: Clearly communicate what the chatbot can and cannot do to manage user expectations effectively.</li>
<li><strong>Modular Functionality</strong>: Break down the chatbot’s functionalities into specific modules that can be fine-tuned individually.</li>
<li><strong>Review Guardrails</strong>: Regularly review and understand the prompts and guardrails to ensure they are functioning correctly.</li>
<li><strong>Careful Rollout</strong>: Test chatbots extensively before public release to avoid unexpected behaviors that could lead to negative publicity.</li>
</ol>
</section>
</section>
<section id="preference-optimization" class="level2">
<h2 class="anchored" data-anchor-id="preference-optimization">Preference Optimization</h2>
<p>Discusses the effectiveness of Direct Preference Optimization (DPO) in fine-tuning LLMs to produce superior outputs. By leveraging human preferences in comparing two responses to the same prompt, DPO can significantly improve the quality of model outputs.</p>
<section id="preference-optimization-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="preference-optimization-algorithms">Preference Optimization Algorithms</h3>
<ul>
<li><strong>Challenge:</strong> Human-generated data is often imperfect, and training models solely on this data can lead to suboptimal results.</li>
<li><strong>Human Preference Evaluation</strong>: Humans excel at choosing between two options based on preference.</li>
<li><strong>Preference Optimization Algorithms</strong>: These techniques leverage human preferences to fine-tune models.</li>
</ul>
</section>
<section id="direct-preference-optimization-dpo" class="level3">
<h3 class="anchored" data-anchor-id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h3>
<ul>
<li><strong>Definition</strong>: DPO involves using human preference data to guide model fine-tuning.</li>
<li>Comparison to Supervised Fine-Tuning:
<ul>
<li><strong>Supervised Fine-Tuning</strong>: Model learns to imitate responses based on a prompt-response pair.</li>
<li><strong>DPO</strong>: Model learns from human preference data by comparing two responses to the same prompt and determining which is better.</li>
</ul></li>
</ul>
</section>
<section id="process-of-direct-preference-optimization" class="level3">
<h3 class="anchored" data-anchor-id="process-of-direct-preference-optimization">Process of Direct Preference Optimization</h3>
<ul>
<li>Data Collection:
<ul>
<li><strong>Prompt</strong>: Initial input or question.</li>
<li><strong>Responses</strong>: Two different responses to the prompt.</li>
<li><strong>Human Evaluation</strong>: Determining which response is better.</li>
</ul></li>
<li><strong>Model Update</strong>: Model adjusts weights to favor better responses, potentially exceeding the quality of the best human-generated responses.</li>
</ul>
</section>
<section id="case-study-customer-service-email-project" class="level3">
<h3 class="anchored" data-anchor-id="case-study-customer-service-email-project">Case Study: Customer Service Email Project</h3>
<ul>
<li>Project Overview:
<ul>
<li><strong>Data</strong>: 200 customer service emails.</li>
<li><strong>Responses</strong>: Two responses per email from different agents.</li>
<li><strong>Manager Evaluation</strong>: Manager chose the preferred response from each pair.</li>
</ul></li>
<li><strong>Model Used</strong>: Fine-tuned on Zephyr (base model).</li>
</ul>
</section>
<section id="performance-comparison" class="level3">
<h3 class="anchored" data-anchor-id="performance-comparison">Performance Comparison</h3>
<ul>
<li>Methods Compared:
<ol type="1">
<li><strong>GPT-4 Response Generation</strong>: Direct use of GPT-4 for generating responses.</li>
<li><strong>Supervised Fine-Tuning</strong>: Model fine-tuned on pairs of input-output data.</li>
<li><strong>Human Agents</strong>: Responses generated by human customer service agents.</li>
<li><strong>DPO Model</strong>: Model fine-tuned using direct preference optimization.</li>
</ol></li>
<li>Results:
<ul>
<li><strong>GPT-4</strong>: Produced the lowest quality responses.</li>
<li><strong>Supervised Fine-Tuning</strong>: Better than GPT-4 but worse than human agents.</li>
<li><strong>Human Agents</strong>: Better than the supervised fine-tuned model.</li>
<li><strong>DPO Model</strong>: Outperformed human agents, producing responses preferred 2 to 1 over human responses in blind comparisons.</li>
</ul></li>
</ul>
</section>
<section id="advantages-of-direct-preference-optimization" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-direct-preference-optimization">Advantages of Direct Preference Optimization</h3>
<ul>
<li><strong>Superhuman Performance</strong>: DPO models can generate responses superior to those of human experts.</li>
<li><strong>Flexibility with Data Quality</strong>: Effective even with imperfect or messy data.</li>
</ul>
</section>
<section id="recommendations-3" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-3">Recommendations</h3>
<ol type="1">
<li><strong>Adopt DPO for Fine-Tuning</strong>: Implement DPO in model fine-tuning processes to achieve superior performance.</li>
<li><strong>Leverage Human Preferences</strong>: Collect and utilize human preference data to guide model improvements.</li>
<li><strong>Evaluate Model Performance</strong>: Regularly compare DPO model outputs with human-generated outputs to ensure quality.</li>
<li><strong>Explore Variations of DPO</strong>: Investigate slight tweaks and alternative algorithms related to DPO to further enhance model performance.</li>
</ol>
</section>
</section>
<section id="evaluating-use-cases-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-use-cases-for-fine-tuning">Evaluating Use Cases for Fine-Tuning</h2>
<p>This discussion focuses on evaluating different use cases for fine-tuning large language models (LLMs). The primary aim is to determine when fine-tuning is beneficial for the target use case compared to using a general model like ChatGPT.</p>
<section id="customer-service-automation-for-a-fast-food-chain" class="level3">
<h3 class="anchored" data-anchor-id="customer-service-automation-for-a-fast-food-chain">1. Customer Service Automation for a Fast Food Chain</h3>
<ul>
<li><strong>Use Case</strong>: Automating responses to most customer service emails, with unusual requests routed to a human.</li>
<li>Evaluation:
<ul>
<li><strong>Fit for Fine-Tuning</strong>: Strong fit.</li>
<li><strong>Reasoning</strong>: The company likely has a substantial dataset from past customer interactions. Fine-tuning can capture the specific nuances of the company’s customer service style and common issues.</li>
<li><strong>Example</strong>: Handling specific inquiries about menu items, store locations, or promotions that are frequently encountered.</li>
</ul></li>
</ul>
</section>
<section id="classification-of-research-articles-for-a-medical-publisher" class="level3">
<h3 class="anchored" data-anchor-id="classification-of-research-articles-for-a-medical-publisher">2. Classification of Research Articles for a Medical Publisher</h3>
<ul>
<li><strong>Use Case</strong>: Classifying new research articles into a complex ontology, facilitating trend analysis for various organizations.</li>
<li>Evaluation:
<ul>
<li><strong>Fit for Fine-Tuning</strong>: Excellent fit.</li>
<li><strong>Reasoning</strong>: The ontology is complex with many subtle distinctions that are hard to convey in a prompt. The publisher likely has extensive historical data for training.</li>
<li><strong>Example</strong>: Classifying articles into one of 10,000 categories, focusing on the most common 500 categories initially for efficiency.</li>
<li><strong>Implementation Detail</strong>: Used a JSON array output for multi-class classification.</li>
</ul></li>
</ul>
</section>
<section id="short-fiction-generation-for-a-startup" class="level3">
<h3 class="anchored" data-anchor-id="short-fiction-generation-for-a-startup">3. Short Fiction Generation for a Startup</h3>
<ul>
<li><strong>Use Case</strong>: Creating the world’s best short fiction writer.</li>
<li>Evaluation:
<ul>
<li><strong>Fit for Fine-Tuning</strong>: Potentially good fit.</li>
<li><strong>Reasoning</strong>:
<ul>
<li>General models like ChatGPT can write good short stories.</li>
<li>Fine-tuning can help the model learn specific preferences in storytelling that go beyond what a general LLM can offer. The startup can gather user preferences on generated stories to continually improve the model.</li>
</ul></li>
<li><strong>Example</strong>: Generating two different story versions on a given topic and having users rate them to inform future fine-tuning.</li>
<li><strong>Considerations</strong>: The feedback loop involving user ratings can help refine and optimize the storytelling quality.</li>
</ul></li>
</ul>
</section>
<section id="automated-news-summarization-for-employees" class="level3">
<h3 class="anchored" data-anchor-id="automated-news-summarization-for-employees">4. Automated News Summarization for Employees</h3>
<ul>
<li><strong>Use Case</strong>: Providing employees with summaries of new articles on specific topics daily.</li>
<li>Evaluation:
<ul>
<li><strong>Fit for Fine-Tuning</strong>: Potentially unnecessary.</li>
<li><strong>Reasoning</strong>: General LLMs like ChatGPT can already provide high-quality summaries. The benefit of fine-tuning depends on the availability of unique internal data to improve the summarization process.</li>
<li><strong>Example</strong>: Summarizing a wide range of news articles without a significant internal dataset may not justify the effort of fine-tuning.</li>
<li><strong>Alternative</strong>: Using preference-based optimization (DPO) to gather feedback on summary quality and improve the model if news summarization is a critical business function.</li>
</ul></li>
</ul>
</section>
<section id="important-considerations" class="level3">
<h3 class="anchored" data-anchor-id="important-considerations">Important Considerations</h3>
<ul>
<li><strong>Data Availability</strong>: Fine-tuning is more effective when there is a large, high-quality dataset available from past interactions or classifications.</li>
<li><strong>Complexity and Specificity</strong>: Use cases with complex, nuanced requirements are better candidates for fine-tuning compared to general tasks.</li>
<li><strong>Resource Commitment</strong>: The decision to fine-tune should consider the resources required for collecting and annotating additional data, as well as the importance of the task within the organization.</li>
</ul>
</section>
<section id="recommendations-4" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-4">Recommendations</h3>
<ol type="1">
<li><strong>Assess Data Quality and Quantity</strong>: Ensure sufficient and relevant data is available for fine-tuning.</li>
<li><strong>Evaluate Task Complexity</strong>: Use fine-tuning for tasks that require specific knowledge or subtle distinctions that a general model might not capture.</li>
<li><strong>Consider Cost-Benefit</strong>: Weigh the benefits of improved performance against the costs of data collection and model training.</li>
<li><strong>Iterate and Improve</strong>: Continuously gather feedback to refine and improve the fine-tuned model, especially for user-preference-driven tasks.</li>
</ol>
</section>
</section>
<section id="qa-session-2" class="level2">
<h2 class="anchored" data-anchor-id="qa-session-2">Q&amp;A Session #2</h2>
<p>This Q&amp;A session addressed various questions related to model quantization, handling hallucinations in language models, and the importance of data annotation.</p>
<section id="quantization" class="level3">
<h3 class="anchored" data-anchor-id="quantization">Quantization</h3>
<ul>
<li><strong>Definition</strong>: Quantization is a technique used to reduce the precision of models.</li>
<li><strong>Performance Impact</strong>: Over-quantization can lead to performance degradation.</li>
<li><strong>Testing</strong>: It is crucial to test the quantized models to ensure performance is not adversely affected.</li>
</ul>
</section>
<section id="hallucination-in-language-models" class="level3">
<h3 class="anchored" data-anchor-id="hallucination-in-language-models">Hallucination in Language Models</h3>
<ul>
<li><strong>Issue</strong>: When classifying academic or scientific articles, ensuring that the language model (LM) only outputs valid classes is critical.</li>
<li><strong>Solution</strong>: Providing enough examples with specific sets of classes to train the model effectively.</li>
<li><strong>Metrics</strong>: Continuous monitoring and treating misclassifications as part of the expected process.</li>
</ul>
</section>
<section id="fine-tuning-large-language-models-1" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-large-language-models-1">Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Use Case Evaluation</strong>: The skill of evaluating use cases for fine-tuning is essential for data scientists.</li>
<li><strong>Example</strong>: Fine-tuning can outperform even human experts in specific, well-defined tasks, such as customer service for companies like McDonald’s.</li>
</ul>
</section>
<section id="optimizing-prompts" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-prompts">Optimizing Prompts</h3>
<ul>
<li><strong>Efficiency</strong>: Static elements in prompts that don’t change should be removed in favor of more dynamic elements.</li>
<li><strong>Few-Shot Examples</strong>: These should be minimized or eliminated with extensive fine-tuning.</li>
<li><strong>Prompt Engineering</strong>: A critical technique in making language models more efficient and effective.</li>
</ul>
</section>
<section id="data-annotation-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="data-annotation-and-evaluation">Data Annotation and Evaluation</h3>
<ul>
<li><strong>Human in the Loop</strong>: Essential for evaluating LLMs and curating data for training and fine-tuning.</li>
<li><strong>Tool Building</strong>: Custom tools are often more effective than generic ones for specific domains.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>notes</category>
  <category>llms</category>
  <guid>christianjmills.com/posts/mastering-llms-course-notes/workshop-001/</guid>
  <pubDate>Fri, 31 May 2024 07:00:00 GMT</pubDate>
  <media:content url="christianjmills.com/images/empty.gif" medium="image" type="image/gif"/>
</item>
</channel>
</rss>
